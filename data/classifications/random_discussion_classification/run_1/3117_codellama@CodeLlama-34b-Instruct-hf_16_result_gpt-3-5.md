## https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf/discussions/16

contains_question: yes
question_part: Is this a limitation on this HF implementation or am I using the inference API wrong. From the blog post I read that CodeLlama should support up to 100k tokens in the input. How to achieve that with this model