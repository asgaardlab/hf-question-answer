## https://huggingface.co/TehVenom/Pygmalion-Vicuna-1.1-7b/discussions/5

contains_question: yes
question_part: I tried to quantize this model to 4 bits, but I consistently run out of vram when quantizing layer 27/32. If somebody else could quantize the model, or suggest a way for me to do it myself, that would be great.