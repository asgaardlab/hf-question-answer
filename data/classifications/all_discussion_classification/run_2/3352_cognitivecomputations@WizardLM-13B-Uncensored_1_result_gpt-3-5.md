## https://huggingface.co/cognitivecomputations/WizardLM-13B-Uncensored/discussions/1

contains_question: yes

question_part: RAM (Not VRAM) requirements? I'm trying to run this with an RTX 3090 and 16GB RAM via oobabooga webgenui. When loading the model, it maxes out all 16GBs of RAM and freezes my computer. I understand that the model has to first load into the system RAM in order to be transferred into VRAM. My question is, how much more RAM will I need to load this model at 8-bit precision?