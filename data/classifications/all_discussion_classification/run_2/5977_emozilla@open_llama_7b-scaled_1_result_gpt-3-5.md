## https://huggingface.co/emozilla/open_llama_7b-scaled/discussions/1

contains_question: yes

question_part: I just wanted to check: this model is not re-trained for context, correct? It's the same as Open Llama 7B base model, but with the ROPE code added so it could be used as a basis for further training with increased context?  My understanding is that the code will work without re-training the model, but that responses are much improved by applying increased-context training on top?