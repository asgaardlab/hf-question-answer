## https://huggingface.co/kittn/mistral-7B-v0.1-hf/discussions/1

contains_question: yes

question_part: If I trained a model on mistral already, do I need to start from scratch due to difficulties of fine-tuning?
I heard rumors that there was a bug with the mistral 7B tokenizer. I was asking because I wanted to know if I should re-train from scratch again or if using my current checkpoint is ok. What do you suggest?