## https://huggingface.co/LoneStriker/lzlv_70b_fp16_hf-4.0bpw-h6-exl2-2/discussions/1

contains_question: yes  
question_part: I run a heterogeneous GPU setup with 36GB VRAM (3090+3060), and I wondered if you could provide a 3.50bpw exl2-2 quant of this LZLV model at least, and if possible, add it to the other 70b models that you quantize in exl2-2. It would be optimal for my setting at 8k context in fp8 (and at least for a few others folks I spotted on Reddit with similar setup to mine), and spare me the purchase of a second 3090! ^^