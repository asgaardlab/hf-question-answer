## https://huggingface.co/google/madlad400-10b-mt/discussions/3

contains_question: yes

question_part: I see gguf files model-q6k.gguf model-q4k.gguf, how to run it ? original llama.cpp looks like does not support madlad