## https://huggingface.co/tiiuae/falcon-40b/discussions/14

contains_question: yes

question_part: Quantizing the large (40 or even 7b) model on 4bit will help community a lot. And please fine tune it with large code database and on Wizard-Vicuna, Mega and other big chat databases as well so it can produce code during chat even.