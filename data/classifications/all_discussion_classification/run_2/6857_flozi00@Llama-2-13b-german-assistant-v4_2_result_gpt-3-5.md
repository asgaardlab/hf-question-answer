## https://huggingface.co/flozi00/Llama-2-13b-german-assistant-v4/discussions/2

contains_question: yes

question_part: A twitter contact suggested to me it could make sense to continue pretraining of one of the larger models, i.e. mpt-30b or falcon-40b on some German data.
What do you think about this?
Do you have ideas how to realize that? Perhaps continuing pretraining for some 50b tokens would cost 100k euros or so.