## https://huggingface.co/TheBloke/Yarn-Llama-2-7B-128K-AWQ/discussions/1

contains_question: yes

question_part: Can I use the quantized model with 128K context