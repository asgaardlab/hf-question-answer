## https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ/discussions/3

contains_question: yes

question_part: I see "It was created without group_size to minimise VRAM usage, and with --act-order to improve inference quality." 
I have been using koboldai for story creation as you can easierly influence how you want the next sentence to start etc....but I am a novice.... hell i only got kobold to even work with 4bit models a couple of days ago (needs a special branch and you have to rename the safetensor file to 4bit.safetensor).
Not at home yet but going to give it a try when I am...but how do I add --act.order? Also I note someone said it used almost all of thier 3090 vram, I have seen that slows generation down in some models i have tried as the vram maxes out, is there any tricks to getting it to use a bit less.