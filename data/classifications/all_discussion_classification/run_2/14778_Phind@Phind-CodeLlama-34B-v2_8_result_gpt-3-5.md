## https://huggingface.co/Phind/Phind-CodeLlama-34B-v2/discussions/8

contains_question: yes

question_part: Are there any plans to release smaller model sizes? For example 3B or 7B trained against the same data set could be useful for speculative decoding optimization.