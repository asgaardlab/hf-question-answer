## https://huggingface.co/google/ul2/discussions/4

contains_question: yes

question_part: Is there any documentation on splitting such models up for inferencing over multiple GPU's?