## https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions/17

contains_question: yes
question_part: Is it possible if you can provide a script so that we can replace standard attention with Flash Attention after we've loaded the model via model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1")