## https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/discussions/100

contains_question: yes

question_part: I am new to hugging face, so I was wondering if it is possible to use multiple instance of Inference Api with the same access token, all running at the same time, will that work, if yes then will there be any latency for all parallel requests. If No, then what is the possible solution to achieve this.