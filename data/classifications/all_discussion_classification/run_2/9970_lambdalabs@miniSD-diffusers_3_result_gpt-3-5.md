## https://huggingface.co/lambdalabs/miniSD-diffusers/discussions/3

contains_question: yes

question_part: How do I finetune the attention layers of the unet?
There are optimized dreambooth repositories, but I don't know if I have access to this feature. Do you have any published code that demonstrates this finetuning the attention layers? Thx