## https://huggingface.co/teknium/Phi-Hermes-1.3B/discussions/4

contains_question: yes

question_part: Since at fp16 it takes only 3.16 GB VRAM for inferencing Phi 1.5, can we run 24 copies (approximately) of Phi 1.5 on an A100-80GB GPU?