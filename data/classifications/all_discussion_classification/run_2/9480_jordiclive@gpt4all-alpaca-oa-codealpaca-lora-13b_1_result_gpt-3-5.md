## https://huggingface.co/jordiclive/gpt4all-alpaca-oa-codealpaca-lora-13b/discussions/1

contains_question: yes

question_part: I see you combined a few good instruction datasets. I was considering doing the same, but haven't found the time. How are the results? What's your impression. There seems to be a couple of school of thought, one is that is better to go smaller, better quality, and cleaner data. Another is to combine lower quality datasets to get a large amount of instruction. It seems like the alpaca datasets is the cleanest right now. So does this get better results that the alpaca_cleaned models?