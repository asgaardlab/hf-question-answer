## https://huggingface.co/state-spaces/mamba-2.8b/discussions/1

contains_question: yes

question_part: The model seems to be twice the size compared transformer based model for the same size (~5.9 GB for 3b transformer model vs 11.1GB Mamba model). 