## https://huggingface.co/tiiuae/falcon-7b/discussions/59

contains_question: yes

question_part: During inference it is taking too much time like it took 28 minutes on a single prompt when i assigned a token size of 700 how can I resolve this issue?