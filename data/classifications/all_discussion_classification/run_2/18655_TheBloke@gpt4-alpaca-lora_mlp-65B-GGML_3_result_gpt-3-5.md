## https://huggingface.co/TheBloke/gpt4-alpaca-lora_mlp-65B-GGML/discussions/3

contains_question: yes

question_part: I was wondering if it's possible to do 3-Bit quantization and if the trade-off in perplexity / speed might provide better output than the 30B models, while running at decent speed