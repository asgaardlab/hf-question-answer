## https://huggingface.co/TriadParty/deepmoney-34b-200k-base/discussions/1

contains_question: yes

question_part: Training parameters?
What context length did you train on?
Was it trained as a lora? What framework?
I noticed that this model in particular seems to retain its long context (40K+) performance. Some extensively trained Yi 200K models (like DPO Bagel) lost this in  their training, so I'm curious what you did to keep it.