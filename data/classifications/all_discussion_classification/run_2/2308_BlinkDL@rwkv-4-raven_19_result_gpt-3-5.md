## https://huggingface.co/BlinkDL/rwkv-4-raven/discussions/19

contains_question: yes

question_part: Hello I try to finetune the 7B rwkv model with LoRA, I have trained the 1.5B model successfully following the parameter you shared in train.py script. But for 7B model, I changed the layer to 32 and embedding dim to 4096, but I still cant load model correctly, just told me its a bad checkpoint. I just want to know the correct training parameters for 7B model. Any response is appreciated.