## https://huggingface.co/naver-clova-ix/donut-base/discussions/8

contains_question: yes

question_part: However, we noticed that the vocabulary in [DONUT's tokenizer.json](https://huggingface.co/naver-clova-ix/donut-base/blob/main/tokenizer.json) does not match the list of possible output tokens from the original [asian-bart-ecjk tokenizer.json](https://huggingface.co/hyunwoongko/asian-bart-ecjk/blob/main/tokenizer.json). Can someone explain why that is exactly? Where does the vocabulary of the donut-decoder come from if it has been pre-trained?