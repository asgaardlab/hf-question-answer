## https://huggingface.co/Mikael110/llama-2-13b-guanaco-fp16/discussions/1

contains_question: yes

question_part: I bet if you trained this model on one, multiple or even all of these coding datasets, it would be better than wizarscoder hands down. Not sure if it would be better to train llama-2-13b on these datasets first then train it on the guanaco qlora, or the other way around