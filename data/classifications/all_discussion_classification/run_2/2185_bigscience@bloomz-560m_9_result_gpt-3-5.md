## https://huggingface.co/bigscience/bloomz-560m/discussions/9

contains_question: yes

question_part: According to the BLOOM paper BLOOM: A 176B-Parameter Open-Access Multilingual Language Model, all bloom model was trained with a sequence length of 2048, but why the n_embed is only 1024 in config.json?