## https://huggingface.co/ctheodoris/Geneformer/discussions/262

contains_question: yes

question_part: Just wondering if there's any way to expand the maximum 2048 token length for Geneformer (e.g. for bigger inputs/datasets)? Or perhaps is there some easy way to use/pretrain a different (e.g. BERT-like) model that accommodates >2048 tokens in the input but still utilizes some of the same learned weights from the pretrained (6L/12L) Geneformer?