## https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-240k-503b/discussions/5

contains_question: yes
question_part: I guess Llama 7B was also trained with float32 but then float16 models are pushed to hub like [here](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)