## https://huggingface.co/LoneStriker/Nous-Capybara-limarpv3-34B-3.0bpw-h6-exl2-2/discussions/1

contains_question: yes

question_part: I am using the experimental branch of exllamav2.
I would like to understand the VRAM requirements for each quantization since I think it requires much more than the model size.