## https://huggingface.co/TheBloke/Llama-2-70B-Chat-GPTQ/discussions/15

contains_question: yes
question_part: Am I missing a config or environment variable that would improve the inference time or is this expected behavior with this quantization