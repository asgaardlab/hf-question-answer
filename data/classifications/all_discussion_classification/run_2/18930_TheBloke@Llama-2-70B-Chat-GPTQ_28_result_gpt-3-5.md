## https://huggingface.co/TheBloke/Llama-2-70B-Chat-GPTQ/discussions/28

contains_question: yes

question_part: The memory is not enough to load the model on 1 using cuda:0, is there a way both GPUs can be used?