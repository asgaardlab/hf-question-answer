## https://huggingface.co/NbAiLab/nb-sbert-base/discussions/2

contains_question: yes

question_part:
The second one is clearly incorrect, but is 75 the correct max sequence length for this model? If I remember correctly, BERT models have a sequence length of 512, or has that changed when finetuning this model?