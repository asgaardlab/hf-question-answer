## https://huggingface.co/Phind/Phind-CodeLlama-34B-v2/discussions/12

contains_question: yes

question_part: I noticed that you fine-tuned the dataset with a sequence length of 4,096. Does the language model (LLM) still maintain support for 100,000 tokens?