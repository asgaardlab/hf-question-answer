## https://huggingface.co/daryl149/llama-2-7b-chat-hf/discussions/1

contains_question: yes
question_part: I was wondering why I recieve this error message, since I thought this model would support context lengths of up to 4000 tokens