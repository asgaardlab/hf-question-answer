## https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF/discussions/6

contains_question: yes

question_part: @TheBloke: how did you use llama.cpp ./quantize to get those GGUFs when the tokenizer.model file isn't even uploaded by the model creator  If you managed to do a tokenizer.model file yourself, care to share the file on this repo 