## https://huggingface.co/bigscience/T0pp/discussions/5

contains_question: yes

question_part: To be able to simply run inference, I've had to create my own `device_map` so the model is uniformly sharded across GPUs. I wonder if there's a nicer way to do this.