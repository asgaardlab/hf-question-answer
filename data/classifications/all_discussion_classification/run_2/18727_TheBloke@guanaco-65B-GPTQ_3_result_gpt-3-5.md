## https://huggingface.co/TheBloke/guanaco-65B-GPTQ/discussions/3

contains_question: yes

question_part: Hey Bloke, I'm trying to run this on your runpod with a 3090 but I get the 
CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 23.69 GiB total capacity; 22.64 GiB already allocated; 41.81 MiB free; 22.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Any idea why that is