## https://huggingface.co/mosaicml/mpt-7b/discussions/7

contains_question: yes

question_part: 
1) Will you make a 3B model as well
2) do you think finetuning with lora adapter would work for this 7B model. (https://github.com/tloen/alpaca-lora)