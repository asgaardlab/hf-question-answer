## https://huggingface.co/TheBloke/llama2_70b_chat_uncensored-GGUF/discussions/2

contains_question: yes

question_part: The file llama2_70b_chat_uncensored.Q5_K_M.gguf won't open and appears to be the wrong size. Normally llama2_70b*.Q5_K_M.gguf files are 48.8GB, this one's 44.6 GB. Kobold.cpp won't load it, and says: Load Model OK: False. Could not load model: /Users/***/Documents/GitHub/koboldcpp/models/llama2_70b_chat_uncensored.Q5_K_M.gguf. The llama2_70b_chat_uncensored.Q5_K_S.gguf is also an odd size: I haven't tested it, but that's probably damaged too.