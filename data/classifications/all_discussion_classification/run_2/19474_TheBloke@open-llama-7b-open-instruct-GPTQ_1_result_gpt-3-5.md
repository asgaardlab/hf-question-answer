## https://huggingface.co/TheBloke/open-llama-7b-open-instruct-GPTQ/discussions/1

contains_question: yes

question_part: Hi, thank you for the model, this works great, any chance you could provide also the 4bit quantized version for the 13b model as in https://huggingface.co/openlm-research/open_llama_13b_600bt