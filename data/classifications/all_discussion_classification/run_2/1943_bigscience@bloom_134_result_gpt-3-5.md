## https://huggingface.co/bigscience/bloom/discussions/134

contains_question: yes

question_part: The hosted inference API mentions "The model is loaded and running on Intel Xeon Ice Lake CPU." and seems the latency is surprisingly low. Does anyone has pointers to where I could find more information about deploying to CPU only environment, or actually the service is still hosted with GPUs?