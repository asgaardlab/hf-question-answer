## https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/10

contains_question: yes

question_part: Maybe consider changing it to torch.finfo(torch.float16).max when the dtype is set to torch.float16 and keeping it to float(2**30) for torch.float32