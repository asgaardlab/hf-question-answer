## https://huggingface.co/lmsys/vicuna-13b-v1.5/discussions/3

contains_question: yes
question_part: During inference, should I set the `torch_dtype` to bf16 (like during finetuning) or to fp16 (which is found in `config.json`)