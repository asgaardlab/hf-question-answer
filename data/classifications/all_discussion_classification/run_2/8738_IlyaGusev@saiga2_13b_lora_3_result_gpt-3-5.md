## https://huggingface.co/IlyaGusev/saiga2_13b_lora/discussions/3

contains_question: yes
question_part: Please advise whether your model is prepared to run under 16bit (i.e. without flag load_in_8bit=true)? I wonder whether your LORA layer is only for 8bits and this layer takes extra time during inference? Do you see a configuration to run the model with good quality responses (as with 8bit) but fast as in 16bit?