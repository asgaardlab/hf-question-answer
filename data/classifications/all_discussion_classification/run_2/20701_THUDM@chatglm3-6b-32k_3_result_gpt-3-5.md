## https://huggingface.co/THUDM/chatglm3-6b-32k/discussions/3

contains_question: yes

question_part: Would you like to release int4/int8 quantizations for this model like previous ones?