## https://huggingface.co/Salesforce/blip-image-captioning-base/discussions/15

contains_question: yes

question_part: The pre-trained checkpoints are computed on a 14M images dataset (according to the paper made of images from COCO, VG, and Conceptual Captions), or on a 129M images dataset (previous dataset + LAION).
Is the HF checkpoint a new configuration computed only on COCO, or is it one of the fine-tuned checkpoints?