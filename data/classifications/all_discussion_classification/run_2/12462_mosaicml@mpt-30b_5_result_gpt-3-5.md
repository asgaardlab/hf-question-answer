## https://huggingface.co/mosaicml/mpt-30b/discussions/5

contains_question: yes

question_part: The model card states that "The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPUâ€”either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision." **What is the correct way of loading the model in 8-bit precision?**