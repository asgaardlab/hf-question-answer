## https://huggingface.co/swulling/bge-reranker-base-onnx-o4/discussions/1

contains_question: yes

question_part: Works great, much faster inference. Quantization possible? Thank you for this! As per title, getting good results (same rank output as original model) while running much faster. Memory usage is about the same as the original model. Is it possible to quantize these models to try to reduce size & memory footprint & further speed up inference?