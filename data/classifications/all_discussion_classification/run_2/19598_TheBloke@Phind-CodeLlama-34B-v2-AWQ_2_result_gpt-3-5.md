## https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v2-AWQ/discussions/2

contains_question: yes

question_part: Is there a fix to be able to use the AWQ model with vLLM instead of AutoAWQ