## https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf/discussions/7

contains_question: yes
question_part: 
What is right GPU to run this
I tried using 4 * 24 GB inference was very slow, Can you suggest the right gpu to run it on for fast inference