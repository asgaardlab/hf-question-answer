## https://huggingface.co/nomic-ai/gpt4all-j/discussions/3

contains_question: yes
question_part: Hi all, I've been trying to run gpt4all-j in huggingFace, both with a deploy in Spaces and Prod Inference endpoints. Tried with and A10G, and even nvdia T4.  The inference just take forever on the App, and ultimately fails. Any pointers on how I could run the inferences on a cloud?