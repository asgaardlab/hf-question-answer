## https://huggingface.co/tiiuae/falcon-40b/discussions/49

contains_question: yes

question_part: Is there a workaround to enable using these features?
Perhaps there should be an option to use a regular implementation of attention instead of `scaled_dot_product_attention`.