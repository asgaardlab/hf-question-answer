## https://huggingface.co/TheBloke/Llama-2-7B-32K-Instruct-GPTQ/discussions/3

contains_question: yes

question_part: I was wondering how you quantized the model using 32k sequence length.