## https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/13

contains_question: yes

question_part: Hi, I have fine-tuned a LLaMA-2 7B model using the Philip Schmid tutorial (https://www.philschmid.de/instruction-tune-llama-2) and have merged the LoRa weights back into the original weights. Now how can I further convert the weights into GGML format and 4 bit quantization, so I can run in llama.cpp?