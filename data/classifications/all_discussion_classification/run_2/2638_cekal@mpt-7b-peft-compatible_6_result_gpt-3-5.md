## https://huggingface.co/cekal/mpt-7b-peft-compatible/discussions/6

contains_question: yes

question_part: a. Is that possible to runs it with 'PromptTuningConfig' instead of 'LoraConfig' ? It need me to input the parameters of num_attention_heads and num_layers, but I don't know where to find it. 

b. for the stated trainable params, how do we know when the training data is larger the the 'trainable params' ?