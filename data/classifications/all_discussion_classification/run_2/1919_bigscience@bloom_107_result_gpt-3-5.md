## https://huggingface.co/bigscience/bloom/discussions/107

contains_question: yes

question_part: I want to know why the [hosted inference API](https://huggingface.co/bigscience/bloom) for BLOOM with the interactive playground on HuggingFace is so fast. Any tips for doing the inference faster as the Huggingface hosted API? Is the hosted inference API a quantized version of BLOOM (for example, the int8 version) or the runtime is powered by a different framework such as Microsoft `DeepSpeed`