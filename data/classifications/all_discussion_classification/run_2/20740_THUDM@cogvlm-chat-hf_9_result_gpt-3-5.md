## https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/9

contains_question: yes

question_part: How to run inference using multiple GPUs