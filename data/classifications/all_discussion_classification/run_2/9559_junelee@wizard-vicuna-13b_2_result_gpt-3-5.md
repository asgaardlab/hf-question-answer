## https://huggingface.co/junelee/wizard-vicuna-13b/discussions/2

contains_question: yes

question_part: Is it possible to convert this into ggml format and run it with llama.cpp?