## https://huggingface.co/cognitivecomputations/Wizard-Vicuna-13B-Uncensored/discussions/7

contains_question: yes
question_part: Any plans on expanding the context length with landmark attention
Do you think it was worth a shot for trying landmark attention on the wizard vicuna 13b model to see if we can expand its context length