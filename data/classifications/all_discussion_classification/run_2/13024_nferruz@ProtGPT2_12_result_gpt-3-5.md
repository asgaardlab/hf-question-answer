## https://huggingface.co/nferruz/ProtGPT2/discussions/12

contains_question: yes
question_part: However, I get the following error: ValueError: Asking to pad but the tokenizer does not have a padding token. This makes me think that padding was not used at training time, as the tokenizer does not have a padding token. How did you concatenate proteins of different lengths to create a batch at training time without padding?