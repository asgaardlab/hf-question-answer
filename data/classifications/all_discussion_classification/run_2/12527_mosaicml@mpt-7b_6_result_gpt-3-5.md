## https://huggingface.co/mosaicml/mpt-7b/discussions/6

contains_question: yes
question_part: config.json seems to say it's using torch attention, but switching it to flash attention says it's unimplemented with alibi.