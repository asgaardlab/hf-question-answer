## https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-PI-8192-GPTQ/discussions/3

contains_question: yes

question_part: How did you calcualte the perplexities for 2048 and 3072 contexts?
Aren't these context sizes expected to have poor perplexities?