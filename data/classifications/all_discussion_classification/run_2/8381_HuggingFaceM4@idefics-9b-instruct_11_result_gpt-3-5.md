## https://huggingface.co/HuggingFaceM4/idefics-9b-instruct/discussions/11

contains_question: yes

question_part: For padded tokens, the attention mask is set to 1 instead of 0. This behavior occurs when we set `padding='longest'` and does not occur otherwise. Any thoughts ?