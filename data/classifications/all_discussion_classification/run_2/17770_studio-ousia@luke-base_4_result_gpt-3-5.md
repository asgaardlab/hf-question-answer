## https://huggingface.co/studio-ousia/luke-base/discussions/4

contains_question: yes

question_part: I've tried updating transformers, torch, accelerate, reducing batch size. However, nothing seems to work. Could you tell me what could be the issue?
I tried changing from AutoModelForMaskedLM to LukeForMaskedLM, but I'm guessing we can't pretrain without entity tokens correct?