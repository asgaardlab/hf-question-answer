## https://huggingface.co/rwitz/go-bruins/discussions/1

contains_question: yes

question_part: It increased hallucinations at the fringes of knowledge, such as in my pop-culture questions. This often happens when LLMs are excessively fine-tuned. It made story telling far too stubborn to respect the user's prompt. Again, this happens when LLMs are excessive fine-tuned. That is, LLMs are in a battle to employ pre-packed story telling elements while respecting the user's prompt directives. Failing to do so results in absurd contradictions. Example: The pre-packaged need to build-suspense and knock on closed doors vs the user prompt stating he was caught stealing something.