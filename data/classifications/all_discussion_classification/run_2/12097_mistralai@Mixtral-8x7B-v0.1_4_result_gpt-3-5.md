## https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/4

contains_question: yes

question_part: No multi GPU inference support? RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat) Output generated in 2.42 seconds (0.00 tokens/s, 0 tokens, context 65, seed 459973075) seems to me like there is a total lack of multi GPU support for inference. I would appreciate it if this was addressed. best wishes and thank you so much for your hard work!