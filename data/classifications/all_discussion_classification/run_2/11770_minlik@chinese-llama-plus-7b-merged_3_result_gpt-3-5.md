## https://huggingface.co/minlik/chinese-llama-plus-7b-merged/discussions/3

contains_question: yes

question_part: I have one question: As in the original github repo, I noticed they use lora to train the model, while you directly load the model using LlamaForCausalLM (w/o lora params). So I wonder what's the difference between the model and the original one?