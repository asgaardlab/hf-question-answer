## https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/discussions/42

contains_question: yes

question_part: 
1. is there any comparison to show that which model is performed well? (all-miniLM-L6-v2 and en_core_web_lg).
2. If I am using all-miniLM-L6-v2, do I still need to do preprocessing? (all the NLP preprocessing task: tokenize, extract key word.
3. How can I look in-depth of how both model work?