## https://huggingface.co/TheBloke/Llama-2-7B-GGUF/discussions/2

contains_question: yes

question_part: I thought Llama2's maximum context length was 4,096 tokens. When I went to perform an inference through this model I saw that the maximum context length is 512. What is the reason for this modification?