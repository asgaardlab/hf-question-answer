## https://huggingface.co/cerebras/btlm-3b-8k-base/discussions/10

contains_question: yes
question_part: However, I can't run a training, because if I don't specify a target_modules in my lora config, I get an error, and if I specify 'q_proj' and  'v_proj' like with llama, I get an error. Could you tell me what should be put under target_modules for a LoraConfig object from the peft library?