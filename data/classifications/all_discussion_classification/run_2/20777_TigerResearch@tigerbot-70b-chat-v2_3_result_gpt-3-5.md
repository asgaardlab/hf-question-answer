## https://huggingface.co/TigerResearch/tigerbot-70b-chat-v2/discussions/3

contains_question: yes
question_part: WHY "max_position_embeddings": 2048 Typically, models based on LLaMA-2 have a parameter size of 4K, but why is it 2K here? Will this lead to a shorter effective understanding of the context by the model