## https://huggingface.co/malteos/bloom-1b5-clp-german/discussions/6

contains_question: yes
question_part: I gotta question by a CS student from Berlin (https://twitter.com/felix_red_panda), who was suggesting that one could continue pretraining falcon-40b, as it has seen 12b German tokens during pretraining. Are there plans to continue pretraining on German tokens such a model or another model after bloom-6b?