## https://huggingface.co/bigscience/bloom/discussions/118

contains_question: yes

question_part: 
1. What is the expected speed of inference on 8*A100 GPUs if everything works normally?
2. Is there any important detail missing?