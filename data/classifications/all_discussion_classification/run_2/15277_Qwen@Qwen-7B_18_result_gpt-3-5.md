## https://huggingface.co/Qwen/Qwen-7B/discussions/18

contains_question: yes

question_part: The model is a lot slower than Llama-2-7b even though I'm using the recommended packages in the Qwen modeling code  â€“ I installed the latest stable flash_attn version, and also installed the flash_attn RMS norm implementation from source. Do you know what could be wrong