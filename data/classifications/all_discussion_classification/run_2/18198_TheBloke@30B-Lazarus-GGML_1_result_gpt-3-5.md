## https://huggingface.co/TheBloke/30B-Lazarus-GGML/discussions/1

contains_question: yes

question_part: If I offload the model to GPU, will it require the same amount of VRAM?
And does the "max ram required" mean the ram required to use the context length (2048) fully?