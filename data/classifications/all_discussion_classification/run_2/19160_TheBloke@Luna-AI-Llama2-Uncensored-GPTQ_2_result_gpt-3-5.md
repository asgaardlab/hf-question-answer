## https://huggingface.co/TheBloke/Luna-AI-Llama2-Uncensored-GPTQ/discussions/2

contains_question: yes
question_part: is "max_position_embeddings": 2048 correct?
I only ask this because I have noticed that some updates in other GPTQ repos have changed max_position_embeddings to 4096.
I just set my position embedding compression factor to 2 (max length is 4096), since I just assumed it was a particular nuance of this quant.