## https://huggingface.co/MMG/xlm-roberta-large-ner-spanish/discussions/1

contains_question: yes

question_part: Could you share your finetuning parameters? Like epoch numbers. And are there empty examples in your loaded dataset? I load the huggingface conll2002 dataset and found many empty lines. When I try to replicate this Spanish ner fine-tuning I also got a similar score as you, but it is not as high as in the paper. Maybe the empty lines influence the result? What's your idea? :)