## https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/discussions/40

contains_question: yes
question_part: If 4-bit = 8GB model would 8-bit = 16GB? And how much better would it be? Curious, per the title. Would an 8-bit version of this model perform noticeably better and still fit into a 3090 / 4090?