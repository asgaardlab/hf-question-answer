## https://huggingface.co/internlm/internlm2-base-20b/discussions/1

contains_question: yes
question_part: On what amount of English tokens was the model trained on? Is it above 2.6 Trillion tokens?