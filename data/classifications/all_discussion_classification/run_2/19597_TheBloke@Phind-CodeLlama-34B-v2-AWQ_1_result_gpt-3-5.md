## https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v2-AWQ/discussions/1

contains_question: yes

question_part: just tried out the model with vllm and I am getting OOM errors. Shouldn't the 34B model run on a single RTX 3090? I also tried using two 3090s but I am still getting OOM with vllm 0.2.0