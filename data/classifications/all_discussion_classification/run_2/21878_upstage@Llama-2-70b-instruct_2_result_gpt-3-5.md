## https://huggingface.co/upstage/Llama-2-70b-instruct/discussions/2

contains_question: yes

question_part: Was wondering, checking the file `config.json`, the `max_position_embeddings` variable is set to 2048, while for llama-2 (https://huggingface.co/meta-llama/Llama-2-70b-hf/blob/main/config.json), this value is set to 4096. Would this model be able to do 4096 context, as llama-2-70b