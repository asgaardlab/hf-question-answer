## https://huggingface.co/tiiuae/falcon-40b/discussions/18

contains_question: yes

question_part: How to load Falcon-40B on Nvidia H100 GPU with 80GB VRAM? Even in `load_8_bit=True` setting, the model doesn't load on the GPU, how to load it for inference?