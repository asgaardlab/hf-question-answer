## https://huggingface.co/cognitivecomputations/laserxtral-exl2/discussions/1

contains_question: yes
question_part: does this model support 32k context withought NTK RoPE scaling?
From what I can see merged models have 8k context and some 16k, so my guess is this model rather be limited to 8k context.