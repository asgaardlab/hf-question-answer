## https://huggingface.co/TheBloke/MPT-7B-Instruct-GGML/discussions/2

contains_question: yes
question_part: Is it possible to load this quantised model for integration to a Langchain via langchain's HuggingFace Local Pipeline Integration? The original MPT-7B-Instruct could be loaded in a similar fashion.