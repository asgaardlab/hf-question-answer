## https://huggingface.co/bigscience/bloom/discussions/45

contains_question: yes
question_part: Hi guys, I'm very interested in run the 176B model locally. May I know how many A100-40GB GPUs are needed to host it? Thanks!