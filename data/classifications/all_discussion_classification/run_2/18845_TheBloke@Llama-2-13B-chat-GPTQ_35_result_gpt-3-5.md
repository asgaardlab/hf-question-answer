## https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ/discussions/35

contains_question: yes

question_part: When I am using 4bit or 8bit quantized LLaMa-2-13b models, I am getting same response from model even if I change temperature or top_p parameters for diversity. Can anyone tell me why this is happening