## https://huggingface.co/IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese/discussions/2

contains_question: yes
question_part: I found the tokenizer will tokenize many single word to two tokens,  then any text after tokenize will have double or more length . don't know why did you make this strange tokenizer?