## https://huggingface.co/TheBloke/llama2_7b_chat_uncensored-GGML/discussions/2

contains_question: yes

question_part: I know this might sound ridiculous to somebody who works with models on a daily basis, but I am a bloody beginner in machine learning, AI and such with a somewhat okay fundament in Python and was wondering how to deploy your models. Are there usually other files for the tokenizer etc. with the model that are missing here? Or if you could maybe point me to a good source of information how to deploy llama2 models locally using python, I would very much appreciate it. I am struggling to find a good entrance point into AI models, how to deploy, train and finetune them. Either the information sources are so advanced that I only understand gibberish and a lot of knowledge is assumed or it is dumbed down so much that it doesn't really help (pre-chewn solutions like web-UI and one-click-solutions are not what I am looking for) I know this might be the wrong place to ask, but I really would love to deploy this particular model locally within python. The goal is to create a STT > Conversational Model > TTS allowing me to communicate with my personal AI assistant running locally on my GPU. I got the STT running, now I am looking into how to deploy a slim model locally. Thank you for your time <3 and sorry for the inconvenience caused by my stupid questions!