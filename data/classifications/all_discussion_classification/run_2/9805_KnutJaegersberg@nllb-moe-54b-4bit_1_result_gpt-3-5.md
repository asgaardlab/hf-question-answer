## https://huggingface.co/KnutJaegersberg/nllb-moe-54b-4bit/discussions/1

contains_question: yes

question_part: I don't know about this, but since you got the mistral moe into gptq, is it possible to quantize this model, too? It's unique. I guess it outperforms the recent seamless translation models of meta. it's just huge. I guess there is no support in gptq nor gguf.