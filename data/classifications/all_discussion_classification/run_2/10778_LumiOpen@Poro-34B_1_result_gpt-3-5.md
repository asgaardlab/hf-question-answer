## https://huggingface.co/LumiOpen/Poro-34B/discussions/1

contains_question: yes  
question_part: In the model card you state that it was trained with a world size of 1024 and a micro batch size of 1 but in the training hyperparameters section you write effective batch size 4M tokens (2048x2048) instead of (1024x2048). Maybe there was a data entry error somewhere.