## https://huggingface.co/TheBloke/guanaco-65B-GPTQ/discussions/13

contains_question: yes

question_part: how to fine tune guanaco and run on multi core cpu (>32 cores)
question_part: can we fine tune guanaco using QLora and run the finetuned model on CPUs using llama.cpp
question_part: The missing piece is how to convert the fine tuned model to 4bit format which llama.cpp can run