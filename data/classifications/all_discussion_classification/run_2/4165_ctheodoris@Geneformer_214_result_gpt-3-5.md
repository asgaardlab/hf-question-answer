## https://huggingface.co/ctheodoris/Geneformer/discussions/214

contains_question: yes
question_part: In essence, my question is can I tokenize each dataset individually or in smaller batches? Does tk.tokenize_data() tokenize each .loom separately or is there some factor which gets calculated taking into account all .loom files in the directory? Could you please clarify the implementation.