## https://huggingface.co/nferruz/ProtGPT2/discussions/5

contains_question: yes

question_part: Is it possible to condition ProtGPT2 sequence generation with pre-computed embeddings from another model (eg: MSA Transformer) - so that we could generate a sequence of logits conditioned on the given embeddings (by another model)? If yes, then could you please guide with how to implement this?