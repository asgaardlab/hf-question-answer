## https://huggingface.co/upstage/llama-30b-instruct-2048/discussions/7

contains_question: yes
question_part: Which one is better, finetuning with a sequence length of 1024 or 2048 in LLaMA1 and LLaMA2? And what are the reasons behind the choice?