## https://huggingface.co/NickyNicky/togethercomputer-LLaMA-2-7B-32K-open-Orca-v2/discussions/1

contains_question: yes

question_part: I have a question: 
if you fine-tune the 
togethercomputer-LLaMA-2-7B-32K base model on a dataset with short contexts (length of input short), would you then expect it to perform well when given longer inputs ? 
(I am assuming you trained it with a dataset in which the length of input was less than 3000 tokens. )
Thanks in advance for your reply. 