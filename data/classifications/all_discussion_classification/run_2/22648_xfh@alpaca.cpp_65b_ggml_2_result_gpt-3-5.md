## https://huggingface.co/xfh/alpaca.cpp_65b_ggml/discussions/2

contains_question: yes

question_part: You're just git cloning Llama and then quantizing it with Alpaca.cpp. the quantization script is basically the same for both alpaca and llama.