## https://huggingface.co/FreedomIntelligence/AceGPT-13B-chat/discussions/1

contains_question: yes

question_part: Why max_length and max_position_embeddings are inconsistent? AceGPT is based on LLama-2, why not 4096.