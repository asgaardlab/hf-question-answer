## https://huggingface.co/beomi/OPEN-SOLAR-KO-10.7B/discussions/1

contains_question: yes

question_part: Hi,

First of all, great work on Korean models with TPUs. We at Finnish-NLP (https://huggingface.co/Finnish-NLP ) are inspired by your work and are planning on doing something similar. (Eg. continued pretraining from Mistral for example like here https://www.reddit.com/r/LocalLLaMA/comments/174i0vh/em_german_mistral_continous_pretraining/ or some other model)
We would be very pleased if you could share how you are doing continued pretraining on TPUs (Through TRC program as we also). Which framework are you using, how you modify tokenizer etch.

I asked this kind of question at EleutherAI discord and was adviced to look into your work.