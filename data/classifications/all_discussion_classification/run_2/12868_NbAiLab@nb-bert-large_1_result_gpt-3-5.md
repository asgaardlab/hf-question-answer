## https://huggingface.co/NbAiLab/nb-bert-large/discussions/1

contains_question: yes

question_part: Apologies in advance if this is a silly question. How come that the pretrained nb-bert-large tokenizer use different special_tokens_ids than most other BERT models?