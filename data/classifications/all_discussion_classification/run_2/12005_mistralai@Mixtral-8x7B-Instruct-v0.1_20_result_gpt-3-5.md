## https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/20

contains_question: yes

question_part: 
According to the blog, the value of this work is training a 47B model but it only cost 13B during inference. Somewhat "using a 47B model with 13B inference speed (but using 43B VRAM?)". Is there anything else I am missing?
Another question is related to the performance. I take this model something between 47B and 13B. And I justed checked the leaderboard, there are some 34B models have higher average score. I know that the real use is not equal to the scores of benchmarks but I would like to listen to those who have more experience using these models. Could you kindly provide some insights?