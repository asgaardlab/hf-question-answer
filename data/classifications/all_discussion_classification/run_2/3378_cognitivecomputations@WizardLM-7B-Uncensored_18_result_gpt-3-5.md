## https://huggingface.co/cognitivecomputations/WizardLM-7B-Uncensored/discussions/18

contains_question: yes

question_part: How does one go about training these models. I am looking to incorporate an internal codebase into the LLM. Here is my training code. I can't seem to get this to work as it always maxes out my GPU memory. I have even tried a H100 80 GB instance. I have tried the basics of reduced batch size. How does one go about training one of these from a checkpoint? Thanks.