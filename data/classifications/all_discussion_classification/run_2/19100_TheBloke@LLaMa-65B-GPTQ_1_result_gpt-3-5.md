## https://huggingface.co/TheBloke/LLaMa-65B-GPTQ/discussions/1

contains_question: yes

question_part: 
Do you know if it's possible to split vram usage between two GPUs with GPTQ's llama.py?