## https://huggingface.co/ctheodoris/Geneformer/discussions/73

contains_question: yes
question_part: When I modified the pretraining example's default parameters to the 12-layer version, i.e. num_embed_dim = 512, num_attn_heads = 8, and num_layers = 12, the RuntimeError: CUDA out of memory appeared, and I believe you may also face this problem before, so could you please provide some suggestions to avoid the error?