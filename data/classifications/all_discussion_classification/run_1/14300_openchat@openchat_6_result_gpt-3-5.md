## https://huggingface.co/openchat/openchat/discussions/6

contains_question: yes
question_part: Can I load this for local inference on an RTX 4090 with 24 GB dedicated memory somehow