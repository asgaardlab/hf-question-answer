## https://huggingface.co/microsoft/codebert-base/discussions/1

contains_question: yes

question_part: Upon closer inspection, it seems the tokenizer `roberta-base` tokenizer. 

In case this was by design, why was the tokenizer not trained on the training data?