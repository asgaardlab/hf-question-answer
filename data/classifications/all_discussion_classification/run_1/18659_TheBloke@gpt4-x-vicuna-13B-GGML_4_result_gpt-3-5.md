## https://huggingface.co/TheBloke/gpt4-x-vicuna-13B-GGML/discussions/4

contains_question: yes

question_part: the recent PR [https://github.com/ggerganov/llama.cpp/pull/1827] allows us to take advantage of the CUDA full GPU acceleration. Do you think it is worth the effort to re-quantize the model?