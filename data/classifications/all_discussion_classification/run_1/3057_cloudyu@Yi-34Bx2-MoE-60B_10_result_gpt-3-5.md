## https://huggingface.co/cloudyu/Yi-34Bx2-MoE-60B/discussions/10

contains_question: yes
question_part: Can the VLLM inference framework support running inference with this model? How can it be adjusted or modified to run on a setup with 8 Nvidia RTX 3090 GPUs?