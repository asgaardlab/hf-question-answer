## https://huggingface.co/microsoft/Orca-2-13b/discussions/12

contains_question: yes

question_part: I have a question, the model is trained using float32 data type, but due to resource constraints, I am performing inference with fp16. Does this significantly impact the performance of the model?