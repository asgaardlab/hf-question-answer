## https://huggingface.co/bigscience/bloom/discussions/134

contains_question: yes

question_part: The hosted inference API mentions "The model is loaded and running on Intel Xeon Ice Lake CPU." and seems the latency is surprisingly low.