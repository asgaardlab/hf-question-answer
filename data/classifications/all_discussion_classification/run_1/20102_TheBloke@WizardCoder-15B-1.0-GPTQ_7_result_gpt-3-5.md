## https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GPTQ/discussions/7

contains_question: yes

question_part: Can you quantize the rwkv-4-raven model in GPTQ since ggml has already been done.
I'd prefer the 14b if you can only do 1, and if you can do it in 8-bit that would be lovely, but I will take a 4-bit version too.
But if you can do all the models that would be awesome too as I'm sure other people would like them.
It's a really good coding model that supports 8k context out of the box and follows instructions better than wizard coder (at least that's what I've heard).