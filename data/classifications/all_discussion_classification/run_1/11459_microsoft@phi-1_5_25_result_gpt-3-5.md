## https://huggingface.co/microsoft/phi-1_5/discussions/25

contains_question: yes

question_part: However when I test with the sample code on a single A100-80G, the inference speed is around 28ms per token. My torch version is 2.0.1. 
May I know how to make the inference speed to be around 3ms?