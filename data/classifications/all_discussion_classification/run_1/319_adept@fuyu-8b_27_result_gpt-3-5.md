## https://huggingface.co/adept/fuyu-8b/discussions/27

contains_question: yes

question_part: What is the best way to lower response time from the model? currently I am ruining this on an laptop with a RTX 4080 so I dont have the 24 gigs of vram. i have used "torch_dtype=torch.float16" to even run any inference leaving me with generation times of over a minute. will lowering image resolution help?