## https://huggingface.co/TheBloke/WizardLM-30B-Uncensored-GPTQ/discussions/13

contains_question: yes

question_part: Is there some way of sharding this model to my two GPUs? I thought the model would be split automagically since I've seen other smaller models loads use both GPUs with the same code. But maybe this is a newbie question, I've just gotten started with this yesterday.