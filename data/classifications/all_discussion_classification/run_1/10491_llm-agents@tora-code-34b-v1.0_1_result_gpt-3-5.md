## https://huggingface.co/llm-agents/tora-code-34b-v1.0/discussions/1

contains_question: yes

question_part: Would you like to make quantized versions for this model which claims > is currently the first and only open-source model to achieve over 50% accuracy (pass@1) on the MATH dataset, which significantly outperforms GPT-4â€™s CoT result (51.0 vs. 42.5), and is competitive with GPT-4 solving problems with programs.