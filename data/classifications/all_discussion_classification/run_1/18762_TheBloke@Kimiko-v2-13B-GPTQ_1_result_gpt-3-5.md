## https://huggingface.co/TheBloke/Kimiko-v2-13B-GPTQ/discussions/1

contains_question: yes

question_part: from when the autogptq has merged into the transformers the system memory cunsumption has been increased too much. like before it only takes around 6-7 gb system ram to load a 13b model at 4bit but from now its taking too much and not fitting into the clould notebooks(kaagle and colab) system ram. is there any solution for that