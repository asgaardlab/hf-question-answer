## https://huggingface.co/adept/fuyu-8b/discussions/9

contains_question: yes

question_part: Is it possible to fine tune the Qlora model, flash-attention, add special tokens?
Is there an example in collaboration so I can reproduce it?