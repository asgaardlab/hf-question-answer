## https://huggingface.co/bigscience/mt0-xxl/discussions/6

contains_question: yes

question_part: 
1. How would I fine-tune an mt0 or bloomz model? Would something like LoRA work? Do I need to tokenize fine tuning inputs, and if so, how?
2. What is the most powerful model that fits in 24 gigs of VRAM for training? Iâ€™d prefer to use mt0-xxl for its impressive performance even against full Bloomz but I understand that might not be feasible, alternatives welcome!
3. Any recommendations for inference deployment? Should I switch AWS instances, change program, etc..?