## https://huggingface.co/LoneStriker/lzlv_70b_fp16_hf-4.0bpw-h6-exl2-2/discussions/1

contains_question: yes

question_part: I run a heterogeneous GPU setup with 36GB VRAM (3090+3060), and I wondered if you could provide a 3.50bpw exl2-2 quant of this LZLV model at least, and if possible, add it to the other 70b models that you quantize in exl2-2.