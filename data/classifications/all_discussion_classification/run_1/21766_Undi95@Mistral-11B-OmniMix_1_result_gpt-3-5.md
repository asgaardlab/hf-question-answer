## https://huggingface.co/Undi95/Mistral-11B-OmniMix/discussions/1

contains_question: yes
question_part: What is the context size of this model? According to Mistral documentation, the model was trained on 8192 tokens, while Llama2 has 4096 tokens.