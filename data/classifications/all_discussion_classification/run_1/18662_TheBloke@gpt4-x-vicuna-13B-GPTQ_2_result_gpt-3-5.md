## https://huggingface.co/TheBloke/gpt4-x-vicuna-13B-GPTQ/discussions/2

contains_question: yes
question_part: Does that mean the model itself supports 2048 tokens but the tokenizer can only encode 512 tokens max? Would there be a way to switch out the tokenizer so that it can feed the full 2048 tokens the model supports? Or am I missing something