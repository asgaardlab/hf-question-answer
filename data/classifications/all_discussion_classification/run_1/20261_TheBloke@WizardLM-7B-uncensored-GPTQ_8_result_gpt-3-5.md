## https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ/discussions/8

contains_question: yes

question_part: Can I use this model with 4 GB VRAM? Or I need 8 GB VRAM and more? 

When I run this model, I got this error with "pre_layer = 1" because if I turn off "pre_layer" I got run out of CUDA Core.

Can I use this modal or my GPU not strong enough to run this model?