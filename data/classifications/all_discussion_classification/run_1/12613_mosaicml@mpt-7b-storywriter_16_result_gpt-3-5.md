## https://huggingface.co/mosaicml/mpt-7b-storywriter/discussions/16

contains_question: yes

question_part: Using an embedding space, solve the context length problem?
If an embedding space for storing long-term dialogue memory (user input and model output) is added to the model architecture, can the problem of context length be solved?