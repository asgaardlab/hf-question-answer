## https://huggingface.co/facebook/opt-66b/discussions/6

contains_question: yes  
question_part: I would really like to use this model for inference, but I don't have a large enough GPU. T5 model has support for parallelization to multiple GPUs using: model.parallelize(). OPT doesn't have this feature. Please add this feature or tell me how I can just use your model on multiple GPUs without pain