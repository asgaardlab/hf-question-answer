## https://huggingface.co/microsoft/phi-2/discussions/94

contains_question: yes

question_part: Why inside `modeling_phi.py`, the output from Self Attention is not becoming the input of MLP?
What kind of transformers implementation is that?