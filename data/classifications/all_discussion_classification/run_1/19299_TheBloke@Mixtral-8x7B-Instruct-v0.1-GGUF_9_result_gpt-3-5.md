## https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/discussions/9

contains_question: yes
question_part: Why is the response slower than the 70B model? Is it unique to gguf or also to gptq? 