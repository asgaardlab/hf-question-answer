## https://huggingface.co/Locutusque/TinyMistral-248M/discussions/3

contains_question: yes
question_part: How did you manage to achieve a context length of around 32,768 tokens with only training with a Titan V ? Didn't you run out of memory ?