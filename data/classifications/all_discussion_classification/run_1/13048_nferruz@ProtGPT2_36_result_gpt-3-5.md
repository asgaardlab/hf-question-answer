## https://huggingface.co/nferruz/ProtGPT2/discussions/36

contains_question: yes

question_part: I have about 500k sequences and am thinking of starting with a small subset of these, say 2k sequences. I have access to two instances of NVIDIA A100 GPUs, each with 40 GB of memory. Will this be enough for training? Will I need any kind of memory saving tricks? Also, can someone give me an idea of how long it'll take to run the fine tuning for different number of sequences (between 2k and 500k)?