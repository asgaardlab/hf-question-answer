## https://huggingface.co/ctheodoris/Geneformer/discussions/154

contains_question: yes
question_part: When I used InSilicoPerturber with a fine-tuned model with the input below, it showed a warning "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked." Is it safe to ignore it, or where shall I find the `attention_mask` to avoid this message?  Thank you.