## https://huggingface.co/GodRain/WizardCoder-15B-V1.1-4bit/discussions/1

contains_question: yes

question_part: 
1) Will GPTQ-for-LLaMA be a better model loader than AutoGPTQ?
2) If so, how can I run it?  Will it run?  And what is the parameter for the --model_type argument?