## https://huggingface.co/Monero/WizardLM-SuperCOT-StoryTelling-30b-4bit/discussions/1

contains_question: yes

question_part: Do you happen to have a non-GPTQ version of this model so I could convert to ggml for use with llama.cpp
This is a GPTQ model, correct? Or am I wrong?