## https://huggingface.co/Trelis/Yi-6B-200K-Llamafied-chat-SFT-AWQ/discussions/1

contains_question: yes

question_part: Have the Yi 6B/34B models been fine-tuned to use EOS properly up to the 200k context length?
Did you use a fine-tuning dataset with long enough example to achieve that?
How did you test the quality of the context and that the model properly using EOS up to 200k context lengths? Any results?