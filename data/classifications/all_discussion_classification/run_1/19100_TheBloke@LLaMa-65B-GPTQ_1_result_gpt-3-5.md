## https://huggingface.co/TheBloke/LLaMa-65B-GPTQ/discussions/1

contains_question: yes

question_part: Do you know if it's possible to split vram usage between two GPUs with GPTQ's llama.py? Anyway thanks again, this was proving to be very difficult to do.