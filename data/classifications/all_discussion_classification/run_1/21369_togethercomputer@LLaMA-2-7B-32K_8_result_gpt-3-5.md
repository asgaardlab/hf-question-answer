## https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/8

contains_question: yes

question_part: How to training a llama-2-7B-32k from llama-2-7B? Just taking llama-2-7b as an example, I want to know how to train the context that can be extended to 32k. I saw that there is only fine-tuning llama-2-7b-32k code in openchatkit. If I want to train llama-2-7b to llama-2-7b-32k from scratch, what should I do?