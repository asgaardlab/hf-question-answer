## https://huggingface.co/Pi3141/alpaca-lora-30B-ggml/discussions/6

contains_question: yes
question_part: I'm guessing from the name, this is a LoRA finetuned model, but is it 4 bit? Or is there a different model I need if I want to run a 32B param at home?