## https://huggingface.co/TheBloke/Llama-2-70B-GGUF/discussions/1

contains_question: yes
question_part: Hi, I have a request. If you can upload a Q8_0 version with LOT option (--leave-output-tensor, the highest quality Quantized version) so that we can do the ./quantize  --allow-requantize model-q8_0-LOT.gguf Q4_0 or any other type ourselves without loss of quality (I guess).