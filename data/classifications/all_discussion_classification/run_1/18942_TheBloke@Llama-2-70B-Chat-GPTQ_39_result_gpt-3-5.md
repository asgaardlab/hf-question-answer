## https://huggingface.co/TheBloke/Llama-2-70B-Chat-GPTQ/discussions/39

contains_question: yes

question_part: Can I ask how I can deploy the model to perform batch inference across gpus.