## https://huggingface.co/fblgit/una-xaberius-34b-v1beta/discussions/1

contains_question: yes

question_part: 
Would you consider training on the 200K model instead of base Yi? Even if the training context is much shorter, some of the long context performance seems to be preserved. 
Also, is this a Lora or a native fintune? If the former, could you post the lora?