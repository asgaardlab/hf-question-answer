## https://huggingface.co/TheBloke/Llama-2-70B-Chat-AWQ/discussions/5

contains_question: yes  
question_part: Llama2 models have 4096 context length, is this something that can be configured during deployment?