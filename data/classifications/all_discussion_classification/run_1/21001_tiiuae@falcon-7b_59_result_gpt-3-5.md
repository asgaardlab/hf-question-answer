## https://huggingface.co/tiiuae/falcon-7b/discussions/59

contains_question: yes

question_part: Hello I have fintuned the falcon " ybelkada/falcon-7b-sharded-bf16 " but during inference it is taking too much time like it took 28 minutes on a single prompt when i assigned a token size of 700 how can I resolve this issue?