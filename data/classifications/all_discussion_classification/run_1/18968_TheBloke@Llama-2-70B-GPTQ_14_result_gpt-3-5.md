## https://huggingface.co/TheBloke/Llama-2-70B-GPTQ/discussions/14

contains_question: yes
question_part: Could the same way of fine-tuning be applied to this quantized model? Or should I use the base 70b llama2 model and quantize it after fine-tuning?