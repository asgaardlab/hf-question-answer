## https://huggingface.co/cognitivecomputations/WizardLM-1.0-Uncensored-CodeLlama-34b/discussions/2

contains_question: yes

question_part: Shouldn't it be like on the original CodeLlama model and Airoboros c34b 2.1? Also, wouldn't "rope_theta": 1000000, be useful, as per the original CodeLlama model?