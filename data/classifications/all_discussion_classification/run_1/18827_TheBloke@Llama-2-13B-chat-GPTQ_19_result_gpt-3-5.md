## https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ/discussions/19

contains_question: yes

question_part: Is it possible to fine-tune these GPTQ especially this llama2-13b-chat-gptq model on custom chat dataset if yes, what is the best structure to prepare the data for it I plan to use it with langchain to create a conversational bot (just like ChatGPT) so I might use the ConversationChain to achieve the same.