## https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-480k-1T/discussions/1

contains_question: yes

question_part: indicating that "Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 2048])." from layer0 to layer21.