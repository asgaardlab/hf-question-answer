## https://huggingface.co/zarakiquemparte/hermes-kimiko-7b-GGML/discussions/1

contains_question: yes

question_part: I like the idea of experimental merges of models, but it would be nice if you could add at least a few notes of details about the merges, like percentages of each model in the mix and reasoning behind such decision, if any at all. Also, I've noticed that for these 7B models you chose Q4_K_M. They are fairly small, it'd be nice if you could do Q6_K as well, I had good experience with those and they are still pretty fast. Just a suggestion.