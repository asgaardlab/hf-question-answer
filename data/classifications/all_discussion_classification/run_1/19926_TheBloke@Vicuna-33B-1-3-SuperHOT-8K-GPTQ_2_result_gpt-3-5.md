## https://huggingface.co/TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ/discussions/2

contains_question: yes
question_part: And I need orca_mini_7B-GPTQ with 8k context size, and I am not sure whether SuperHOT work on orca_mini_7B (As SuperHOT is a LORA for Llama while orca_mini_7B is Open-Llama) If it work, could you help me create orca_mini_7B-SuperHOT-8K-GPTQ?