## https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v2-AWQ/discussions/1

contains_question: yes

question_part: 
just tried out the model with vllm and I am getting OOM errors.
Shouldn't the 34B model run on a single RTX 3090?