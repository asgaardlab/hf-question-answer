## https://huggingface.co/TheBloke/CodeLlama-34B-GPTQ/discussions/3

contains_question: yes
question_part: It tries to allocate to CPU RAM and fails. There is enough space on my GPUs.. Could you may help with that issue? How do I force to load the model only to GPU or does is first load it to CPU RAM and then too GPU VRAM?