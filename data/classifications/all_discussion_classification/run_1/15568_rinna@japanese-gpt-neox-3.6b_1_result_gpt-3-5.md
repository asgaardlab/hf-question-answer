## https://huggingface.co/rinna/japanese-gpt-neox-3.6b/discussions/1

contains_question: yes

question_part: The "Training" section says that "The model was trained on around 312.5B tokens from Japanese CC-100, Japanese C4, and Japanese Wikipedia.." . I think the total number of tokens in these corpora is about 180B, and so this statement means the training epoch is 1.73 epochs (= 312.5 / 180)?