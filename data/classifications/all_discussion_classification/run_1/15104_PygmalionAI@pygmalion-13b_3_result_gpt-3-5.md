## https://huggingface.co/PygmalionAI/pygmalion-13b/discussions/3

contains_question: yes

question_part:
Is it possible the SHA256 hashes for both the vanilla LLaMA-13B and the PyTorch conversion could also be added to the description for troubleshooting purposes?

Problem is, I have no clue if the issue is with xor_codec.py, the output from convert_llama_weights_to_hf.py, or something else.