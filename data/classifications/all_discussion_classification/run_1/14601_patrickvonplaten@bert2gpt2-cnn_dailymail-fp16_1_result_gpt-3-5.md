## https://huggingface.co/patrickvonplaten/bert2gpt2-cnn_dailymail-fp16/discussions/1

contains_question: yes

question_part: I have a question, though. I followed a similar code to yours to machine translation task using a pre-trained BERT model on Arabic language named AraBERT as an encoder, and GPT2 as a decoder and fine-tune on a small dataset just to see what the results will look like. However, the model predicts a bunch of exclamation marks "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!". Do you know what the cause of the problem may be and how to solve it?