## https://huggingface.co/CausalLM/72B-preview-llamafied-qwen-llamafy/discussions/1

contains_question: yes

question_part: I thought you'd continue to pretrain the model on many tokens. Is this a fine-tune? What kinda data did it see? How much? Can we use it