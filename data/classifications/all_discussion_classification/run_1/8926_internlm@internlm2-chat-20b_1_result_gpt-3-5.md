## https://huggingface.co/internlm/internlm2-chat-20b/discussions/1

contains_question: yes

question_part: I'm not sure how different the architecture actually is, but if possible, could you change the config to be llama compatible instead of requiring custom runtime/tokenizer code?