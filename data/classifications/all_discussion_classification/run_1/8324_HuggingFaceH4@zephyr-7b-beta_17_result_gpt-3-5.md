## https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/discussions/17

contains_question: yes

question_part: Are there any plan's to adopt some of Amazon's tricks, such as the very large `rope_theta`, the 16K sliding window, and the 16K training? Whatever they did seems to work extremely well, better than other long context Llama finetunes/Loras I've tried.