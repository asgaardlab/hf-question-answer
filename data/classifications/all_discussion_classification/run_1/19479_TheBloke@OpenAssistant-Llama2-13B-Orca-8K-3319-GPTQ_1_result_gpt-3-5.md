## https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ/discussions/1

contains_question: yes

question_part: My question is, if we are quantizing 8k model with only 2k sequence length, are we going to lose performance