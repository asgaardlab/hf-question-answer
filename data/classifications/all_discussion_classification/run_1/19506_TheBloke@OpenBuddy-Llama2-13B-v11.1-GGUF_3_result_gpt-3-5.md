## https://huggingface.co/TheBloke/OpenBuddy-Llama2-13B-v11.1-GGUF/discussions/3

contains_question: yes

question_part: Whatever setting i use I get this error: $This model maximum context length is 2048 tokens. However, your messages resulted in over 1021 tokens and max_tokens is 2048. But it seems the model should be 4096. May I  must set some special parameter? n_ctx it's already 4096