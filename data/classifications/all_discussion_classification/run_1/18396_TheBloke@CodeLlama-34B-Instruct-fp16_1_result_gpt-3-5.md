## https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-fp16/discussions/1

contains_question: yes

question_part: Shouldn't the fp16 quantization reduce the model size?