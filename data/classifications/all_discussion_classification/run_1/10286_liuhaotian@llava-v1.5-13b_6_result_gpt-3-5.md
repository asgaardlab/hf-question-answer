## https://huggingface.co/liuhaotian/llava-v1.5-13b/discussions/6

contains_question: yes
question_part: Could you provide 4bit quantization model weights? The 32bit weights is too big to download. If you can provide 4bit quantization version like https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat-4bits it will be very convenient for model download and loading to GPU.