## https://huggingface.co/Helsinki-NLP/opus-mt-en-ROMANCE/discussions/5

contains_question: yes
question_part: Is there a limitation with the input token length with Helsinki-NLP/opus-mt-en-ROMANCE model Do we have more insights on the token length during training time Do you recommend breaking the input paragraphs into multiple lines using delimiters like period(.) and sending one line at a time for translation Any other inputs to come out from the max token limitations would be highly appreciated.