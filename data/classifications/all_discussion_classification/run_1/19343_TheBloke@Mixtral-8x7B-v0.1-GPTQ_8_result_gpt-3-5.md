## https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ/discussions/8

contains_question: yes

question_part: Hi @TheBloke, recently [vLLM](https://github.com/vllm-project/vllm/releases/tag/v0.2.6) has started supporting GPTQ models, this is the backend we use for serving our models, we are looking forward to using this model but seems that they only support "pt" weights as of now and seems like this repo only has "safetensors", any chance you can share the "pt" weights, while they roll out support for "safetensors"?