## https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ/discussions/5

contains_question: yes
question_part: has anyone tried deploying this through hf inference endpoint i get errors i know the inference engine command line has the option to pass in parameter to tell that it's an AWQ model but the deployment interface does not provide such thing i get errors and can't run