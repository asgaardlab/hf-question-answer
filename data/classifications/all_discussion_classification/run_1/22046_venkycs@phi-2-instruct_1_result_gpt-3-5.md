## https://huggingface.co/venkycs/phi-2-instruct/discussions/1

contains_question: yes

question_part: Why change the torch_dtype of phi-2 from float16 to float32? Thx for your great job! I've noticed that you changed the torch_dtype of phi-2 from float16 (the default value in the official version config.json) to float32. Why do that? Is that a common practice in LoRA finetuning?