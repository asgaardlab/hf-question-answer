## https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/5

contains_question: yes
question_part: It would be cool to use this model with `load_in_8bit=True` parameter and have it sharded in 2Gb weights for easy use with free tier Colab GPUs.