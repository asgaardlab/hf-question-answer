## https://huggingface.co/intfloat/multilingual-e5-large/discussions/21

contains_question: yes

question_part: 
1) How should I go through the fine-tuning process, do you recommend that I go through pre-training + fine-tuning or just do fine-tuning?
2) If only fine-tuning is required then section 4.2 in your paper, mentions that the MS-MARCO and NQ formats would be ideal formats to convert my corpus into, is my understanding correct and are there any other considerations or factors I should take into account when building my tuning set.
3) Which model do you recommend I start with? this multi-lingual model or the unsupervised bases?