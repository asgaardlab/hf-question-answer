## https://huggingface.co/microsoft/phi-1_5/discussions/45

contains_question: yes

question_part: I fine-tuned phi-1.5 but  when trying to infer a question I'm getting the answer and the model keep generating tokens until it reach max_lenght tokens. As a newbie, I wonder how to prevent such behavior. any insights ?