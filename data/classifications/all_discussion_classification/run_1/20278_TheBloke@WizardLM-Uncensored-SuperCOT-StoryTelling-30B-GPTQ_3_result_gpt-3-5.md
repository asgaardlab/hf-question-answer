## https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ/discussions/3

contains_question: yes
question_part: I see "It was created without group_size to minimise VRAM usage, and with --act-order to improve inference quality." 
Not at home yet but going to give it a try when I am...but how do I add --act.order? Also I note someone said it used almost all of thier 3090 vram, I have seen that slows generation down in some models i have tried as the vram maxes out, is there any tricks to getting it to use a bit less.