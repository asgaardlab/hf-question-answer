## https://huggingface.co/TheBloke/WizardLM-30B-Uncensored-GPTQ/discussions/7

contains_question: yes
question_part: As title. With a powerful enough card (for context, 24G VRAM), would it make more sense to run an unquantized 13B parameter model, or a 4bit quantized 30B model. I think this question actually translates to - how much does quantization affect performance.