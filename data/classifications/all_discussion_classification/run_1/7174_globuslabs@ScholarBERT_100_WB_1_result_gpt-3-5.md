## https://huggingface.co/globuslabs/ScholarBERT_100_WB/discussions/1

contains_question: yes

question_part: One thing I've realised is that the tokenization doesn't look quite right. In your paper you describe that you had to amend the 100_WB to include common words, the etc. as tokens, however this models tokenizer appears to split them. I'm wondering if this is the non-corrected version?