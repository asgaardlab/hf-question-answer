## https://huggingface.co/KnutJaegersberg/nllb-moe-54b-4bit/discussions/1

contains_question: yes
question_part: I don't know about this, but since you got the mistral moe into gptq, is it possible to quantize this model, too?