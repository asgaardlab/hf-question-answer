## https://huggingface.co/TheBloke/Llama-2-70B-Chat-GPTQ/discussions/15

contains_question: yes
question_part: However, when running the unquantized model sharded across 4 A100s I was able to get around 45ms/token. Am I missing a config or environment variable that would improve the inference time or is this expected behavior with this quantization