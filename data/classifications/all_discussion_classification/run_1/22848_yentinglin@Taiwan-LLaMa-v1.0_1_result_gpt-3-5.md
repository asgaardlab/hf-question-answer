## https://huggingface.co/yentinglin/Taiwan-LLaMa-v1.0/discussions/1

contains_question: yes

question_part: I've heard that LLAMA-2 from Meta was predominantly trained on English corpus. Consequently, the vanilla Meta LLAMA-2 struggles with Chinese reasoning and responses. It would be very grateful if you could share some of your experience or tips for training the English-based LLM to learn Chinese? And, if it's like going through the pre-training again. What's the advantage of using LLAMA2 rather than previous published model like Falcon or MPT ?