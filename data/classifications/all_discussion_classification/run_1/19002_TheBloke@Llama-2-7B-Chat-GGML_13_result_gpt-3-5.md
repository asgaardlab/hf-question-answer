## https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/13

contains_question: yes

question_part: Now how can I further convert the weights into GGML format and 4 bit quantization, so I can run in llama.cpp