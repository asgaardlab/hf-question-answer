## https://huggingface.co/TheBloke/guanaco-65B-GGML/discussions/2

contains_question: yes
question_part: Could you upload the fp16, non quantized version? I can then make the 6_K version or even try to load the 'native' fp16. I am interested in understanding how quantizing affects the quality of responses in my use cases, and unless @ehartford gets the hardware to make 65B Wizards or @allenai give us access to 65B Tulu, Guanaco is currently still the most consistently performant model. Thus, I want to try to get the most out of it on my hardware.