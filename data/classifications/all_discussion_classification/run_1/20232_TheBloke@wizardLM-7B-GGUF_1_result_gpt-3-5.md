## https://huggingface.co/TheBloke/wizardLM-7B-GGUF/discussions/1

contains_question: yes

question_part: However, I have tried the 4-bit GGUF models with llama.cpp and ctransformers and the result is always some nonsense. Do I need a specific version? What can I do to make them work properly?