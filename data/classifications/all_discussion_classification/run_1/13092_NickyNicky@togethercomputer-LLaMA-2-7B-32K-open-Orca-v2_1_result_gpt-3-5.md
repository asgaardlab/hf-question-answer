## https://huggingface.co/NickyNicky/togethercomputer-LLaMA-2-7B-32K-open-Orca-v2/discussions/1

contains_question: yes
question_part: I have a question: if you fine-tune the togethercomputer-LLaMA-2-7B-32K base model on a dataset with short contexts (length of input short), would you then expect it to perform well when given longer inputs ? 

P.S: also, would it be possible to share the script you use ( I guess QLoRa)
How many RAM did you used ? 