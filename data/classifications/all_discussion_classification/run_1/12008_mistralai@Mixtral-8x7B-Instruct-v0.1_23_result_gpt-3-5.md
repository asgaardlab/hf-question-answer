## https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/23

contains_question: yes

question_part: Do you have any insights if that intuition holds for a MoE? If during inference only 2 of the 7B experts are active based on the above, I'd expect the quality loss after quantization to be relatively higher than, say a 45B non-MoE quantized model.