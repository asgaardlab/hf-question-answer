## https://huggingface.co/kandinsky-community/kandinsky-3/discussions/5

contains_question: yes

question_part: But there is something with the unet to clearly understand the embedding from the flan model since it is based on a new biggan architecture with attention pooling, it has some issues with some things here and there. I also check the source, what is the actual meaning of attention mask? It is set to 128 . I don't what it is but maybe the projection? it is still good at complex prompts, but the generation quality is far behind that of its preceding models. Will sebr ai fix these issues.