## https://huggingface.co/abhishek/llama-2-7b-hf-small-shards/discussions/5

contains_question: yes

question_part: I have fine tuned the LlaMA and mistral sharded model for fine tuning on google colab and saved the same to hf. Now I am totally clueless about how can I run my fine tune model in google colab, also how can I conver the same into ggml/gguf format and quantize it into 4 bits.