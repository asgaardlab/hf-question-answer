## https://huggingface.co/OpenAssistant/llama2-70b-oasst-sft-v10/discussions/2

contains_question: yes
question_part: I'd like to ask if this model is finetuned with same 4k sequence length, and is there any possibility to extend to 8k length given that it performs better in coding tasks.