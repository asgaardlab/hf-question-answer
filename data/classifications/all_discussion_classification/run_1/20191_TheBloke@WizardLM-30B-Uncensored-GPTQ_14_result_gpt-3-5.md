## https://huggingface.co/TheBloke/WizardLM-30B-Uncensored-GPTQ/discussions/14

contains_question: yes

question_part: is it possible to use a continuous batching inference server with this model?
any other recommendations to achieve 10 calls per sec?
any other AWQ model similar/comparable to this bad boy?
