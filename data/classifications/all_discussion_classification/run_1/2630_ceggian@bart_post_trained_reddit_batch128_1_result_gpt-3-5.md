## https://huggingface.co/ceggian/bart_post_trained_reddit_batch128/discussions/1

contains_question: yes

question_part: 
* How much Reddit data was this BART model post-trained on? (e.g., session number, response number and length, etc.)
* What is the input format of this BART model? Suppose I have a multi-turn dialog context (u1, u2, u3) and would like to generate the next response u4. What format should I organize (u1, u2, u3) into to be fed into the encoder