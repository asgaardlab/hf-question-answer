## https://huggingface.co/matsuo-lab/weblab-10b/discussions/3

contains_question: yes

question_part: The tokenizer.json seems to be the same as for the original GPT-NeoX model. Is there a reason you didn't retrain the vocabulary so as to have more Japanese subtokens? I would have guessed that this would give even better performance in Japanese.