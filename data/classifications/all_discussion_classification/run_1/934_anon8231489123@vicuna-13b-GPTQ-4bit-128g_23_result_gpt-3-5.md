## https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/23

contains_question: yes

question_part: Can someone explain how to deploy this model ? Is it possible with llama.cpp ?