## https://huggingface.co/TheBloke/Llama-2-70B-GPTQ/discussions/7

contains_question: yes

question_part: I'm just curious why the `max_position_embeddings` in this quantization is set to `2048`, whereas the original model had `4096`.