## https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/2

contains_question: yes  
question_part: The HF version still requires at least 40gb VRAM and my attempts so far to split it across two 3090s have failed. There's also no requirements file, leaving you guessing which pytorch, einops, transformers, and sentencepiece to use.