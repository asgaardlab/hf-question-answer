## https://huggingface.co/mosaicml/mpt-7b-chat/discussions/34

contains_question: yes

question_part: 
How was fine-tuning done?
I'm wondering why this model is CC-By-NC-SA-4.0 (non-commercial use only)?
I suppose it's because of the alpaca and HC3 datasets used for fine tuning.
What was the thinking behind that? Were the datasets used for mpt-7b-instruct insufficient for being able to achieve chat-type fine tuning?
Is there information available on how the fine tuning was done and the prompt format? Were any multiple response formats used (i.e. user, then assistant, then user, then assistant). Thanks