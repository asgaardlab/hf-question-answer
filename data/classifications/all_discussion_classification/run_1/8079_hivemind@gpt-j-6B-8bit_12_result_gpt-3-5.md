## https://huggingface.co/hivemind/gpt-j-6B-8bit/discussions/12

contains_question: yes

question_part: So assuming I want train the model in 8-bit with mixed precision using your notebook, after the training is done, should I just convert it back to 32-bit or what? And if so, what's the proper way for a reverse_bnbfy? Or maybe I can somehow save and load the 8-bit weights back without resorting to saving in 32-bit, so can someone guide me how?