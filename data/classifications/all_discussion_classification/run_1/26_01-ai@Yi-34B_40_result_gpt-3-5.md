## https://huggingface.co/01-ai/Yi-34B/discussions/40

contains_question: yes  
question_part: I currently use a 4090, but the inference process is extremely slow. Is it impractical to expect this model to run efficiently on just a single 4090