## https://huggingface.co/segmind/SSD-1B/discussions/20

contains_question: yes
question_part: Is distilling an already distilled model superior to fine-tuning? I'm currently exploring various techniques to optimize and improve our model's performance, and one question arose in our discussions: If we take a model that has already undergone distillation and then distill it again, would this lead to better results compared to just fine-tuning the original model? However, it's unclear to me how this would work when applied repeatedly, especially when compared to fine-tuning. Has anyone here tried this approach before? If so, could you share your findings?