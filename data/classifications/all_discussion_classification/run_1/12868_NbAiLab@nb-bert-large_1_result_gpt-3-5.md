## https://huggingface.co/NbAiLab/nb-bert-large/discussions/1

contains_question: yes

question_part: How come that the pretrained nb-bert-large tokenizer use different special_tokens_ids than most other BERT models? Furthermore, any suggestions on how to overwride the current mapping to the normal special_token_ids for the nb-bert-large