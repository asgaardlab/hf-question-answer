## https://huggingface.co/01-ai/Yi-34B/discussions/35

contains_question: yes

question_part: 
I would like to ask recommendations on how to improve inference time for the Yi-34B models. Is this a good approach? What other [TGI parameters](https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher) should I check? And in the table where you compared the quantized versions based on a batch size, could you elaborate how to interpret that batch_size with respect to the MAX_INPUT_LENGTH?