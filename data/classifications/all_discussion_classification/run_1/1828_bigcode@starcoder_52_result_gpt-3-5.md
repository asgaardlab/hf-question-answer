## https://huggingface.co/bigcode/starcoder/discussions/52

contains_question: yes

question_part: 
Sicne there is a Paper about pretraining, can we have more details on the finetuning side? 
For example, tha Paper says: "StarCoder is the fine-tuned version of StarCoderBase, trained on another 35B Python tokens (roughly 2 epochs).", I have such questions: 
1. Does 'another' means they are not from the Stack? Is it open? 
2. Finetuing code and settings, can we find it somewhere?