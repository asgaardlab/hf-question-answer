## https://huggingface.co/witiko/mathberta/discussions/3

contains_question: yes
question_part: Hi, the model seems to output a tensor of size batchsize x sentence size x 78672 but the tokenizer vocab size is 50265. Any idea why there's this discrepancy?