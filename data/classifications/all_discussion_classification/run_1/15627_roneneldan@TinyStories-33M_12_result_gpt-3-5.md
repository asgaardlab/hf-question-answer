## https://huggingface.co/roneneldan/TinyStories-33M/discussions/12

contains_question: yes
question_part: In the footnote of page 2 of the paper, it is mentioned that the context length of the model is 512. However, when I look at the model config on HF, it says max_position_embeddings=2048. Am I missing something?