## https://huggingface.co/TheBloke/Llama-2-70B-Chat-GPTQ/discussions/29

contains_question: yes  
question_part: I saw from the config that `max_position_embeddings` is set to 2048, but the original llama2 model has 4096 maximum input length. Is there a particular reason to reduce the input length of these quantized model?