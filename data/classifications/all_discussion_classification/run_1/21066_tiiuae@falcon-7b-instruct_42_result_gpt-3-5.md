## https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/42

contains_question: yes

question_part: What was the lora_config used to create this fine-tuned version of falcon-7b? I'm trying to fine tune it with another instruct dataset and want to do it using bfp16 instead of 4-bit QLoRA. Can anyone please point me to an appropriate lora_config setup? Particularly the 'target_modules' parameter.