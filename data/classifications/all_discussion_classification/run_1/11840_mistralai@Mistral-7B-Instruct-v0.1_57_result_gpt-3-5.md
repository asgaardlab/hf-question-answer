## https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/discussions/57

contains_question: yes
question_part: Can you please clarify how you use the `max_position_embeddings` hyperparameter. The `config.json` file specifies that `max_position_embeddings=32768` while the paper claims an attention span of 131K tokens.