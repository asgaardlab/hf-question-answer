## https://huggingface.co/TheBloke/Llama-2-13B-chat-AWQ/discussions/1

contains_question: yes
question_part: any idea how to test this for inferencing using vllm, we tried every method we can think of, just keep getting error messaages saying AWS is not an option