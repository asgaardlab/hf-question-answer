## https://huggingface.co/cognitivecomputations/dolphin-2.1-mistral-7b/discussions/8

contains_question: yes

question_part: the training dataset is ~300k, epoch=4, per_device_train_batch=6, and gradient accumulation=4, GPU cards=4, but the global steps=1204, is the hyper-parameters correct?