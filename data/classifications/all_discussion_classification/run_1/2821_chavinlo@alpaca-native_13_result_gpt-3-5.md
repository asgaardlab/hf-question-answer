## https://huggingface.co/chavinlo/alpaca-native/discussions/13

contains_question: yes

question_part: I'm assuming I'm missing something in how I've set this up. I'm currently using AutoTokenizer and AutoModelForCausalLM using the files from this repo. These have has worked properly on llama models and some other alpaca-lora-merged models, so I'm unsure what the issue is. So far, only this model seems to have issues with the spaces - but the generation output is far better than any of the others. Any ideas as to what could cause this