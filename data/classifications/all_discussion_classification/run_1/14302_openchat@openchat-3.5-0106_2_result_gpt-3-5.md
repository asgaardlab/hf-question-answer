## https://huggingface.co/openchat/openchat-3.5-0106/discussions/2

contains_question: yes

question_part: Why don't you guys train mistral 7b 0.2 which has 32k context length on long context as well as short?