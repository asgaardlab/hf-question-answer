## https://huggingface.co/Jahaz/multi-lora-llama-7b-ggml-q5-1/discussions/1

contains_question: yes
question_part: What merging technique was used?
What merging script or technique was used for this? Since you claim to have mixed models from different architectures such as Bloom.
This can't be done with our own merging scripts to my knowledge, so I am curious to how this was performed.
Are there lora's representing those models, or is there a new technique that allows cross model merging?