## https://huggingface.co/elinas/alpaca-30b-lora-int4/discussions/9

contains_question: yes

question_part: Do you have plan to share your works with the public? As in how you are able to combine lora layers into the llama model and yet able to load the quantized weight into a Llama model. The standard "lora" added model would have extra lora-A and lora-B layers etc., which would not be able to fit into a Llama model from Transformer library, I would think. How is this accomplished if you do not mind sharing?