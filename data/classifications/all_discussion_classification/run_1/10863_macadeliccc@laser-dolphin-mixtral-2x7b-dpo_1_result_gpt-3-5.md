## https://huggingface.co/macadeliccc/laser-dolphin-mixtral-2x7b-dpo/discussions/1

contains_question: yes

question_part: @TheBloke would it be possible for you to quantize this model in GPTQ for llama.cpp. If not its no worries I just figured this would be the easiest way to ask