## https://huggingface.co/Q-bert/Merged-AGI-7B/discussions/1

contains_question: yes

question_part: It's incredible that you can merge 4 LLMs together yet the outputs remain coherent. I'm assuming that has something to do with how much better spherical linear interpolation is compared to weight averaging. This got me wondering if any combination of Mistrals can be merged? Are there compatibility issues (e.g. tokens)? Do you need to get permission first? Just wondering because the "smartest" Mistral I've come across is Dolphin 2.1, while the one that produced the most human-like responses is Starling alpha. Is there a reason these two couldn't be merged?