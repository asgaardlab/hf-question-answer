## https://huggingface.co/MetaIX/GPT4-X-Alpasta-30b-4bit/discussions/6

contains_question: yes  
question_part: llama.cpp now includes GPU offloading support, but it requires for model file to be represented in new GGML file format.