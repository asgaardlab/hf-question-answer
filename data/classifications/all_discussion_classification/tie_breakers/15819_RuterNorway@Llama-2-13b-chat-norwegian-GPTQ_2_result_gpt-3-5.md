## https://huggingface.co/RuterNorway/Llama-2-13b-chat-norwegian-GPTQ/discussions/2

contains_question: yes

question_part: Since ðŸ¤—transformers now has native support for GPTQ-quantized models, quantized models can now be loaded and used just by calling `AutoModelForCausalLM.from_pretrained('your_model')` TheBloke GPTQ models already support this but yours doesn't yet. It would be nice to see this change, as it then could be directly used in many scripts without much code alteration. I have already done this on a private repo, so I'll let you know the steps I took to make it work: 1. Rename the safetensors model file to model.safetensors 2. The safetensors file lacks metadata, which the ðŸ¤—transformers backend relies on. Therefore, add metadata. I used the safetensors util to add the metadata https://github.com/by321/safetensors_util I just added the metadata equivalent of TheBLoke's llama2 variant, which was the following config {"__metadata__": {"format": "pt", "quantized_by": "RuterNorway"}} If you'd like, I could make a pull reuqest, but I figured you might just do it youself so you don't have to spend time verifying everything.