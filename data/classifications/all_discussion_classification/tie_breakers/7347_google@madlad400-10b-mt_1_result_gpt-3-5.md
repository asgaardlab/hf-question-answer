## https://huggingface.co/google/madlad400-10b-mt/discussions/1

contains_question: yes

question_part: 
- Context ! 
- This would need to be integrated into llama.cpp, then the speed likely could approach 100 tokens/sec on the 10B model, also comes with a superior quantization.