!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nacs
conflicting_files: null
created_at: 2023-04-04 11:37:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
      fullname: nacs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nacs
      type: user
    createdAt: '2023-04-04T12:37:05.000Z'
    data:
      edited: false
      editors:
      - nacs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
          fullname: nacs
          isHf: false
          isPro: false
          name: nacs
          type: user
        html: '<p>Is there a 4-bit quantized version of this anywhere?</p>

          '
        raw: Is there a 4-bit quantized version of this anywhere?
        updatedAt: '2023-04-04T12:37:05.464Z'
      numEdits: 0
      reactions: []
    id: 642c19f1c01eb0e148004989
    type: comment
  author: nacs
  content: Is there a 4-bit quantized version of this anywhere?
  created_at: 2023-04-04 11:37:05+00:00
  edited: false
  hidden: false
  id: 642c19f1c01eb0e148004989
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
      fullname: nacs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nacs
      type: user
    createdAt: '2023-04-04T13:09:20.000Z'
    data:
      edited: true
      editors:
      - nacs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
          fullname: nacs
          isHf: false
          isPro: false
          name: nacs
          type: user
        html: '<p>Found one in case anyone needs: <a href="https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g">https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g</a>
          . Edit: Nevermind, that model didn''t load for me.</p>

          '
        raw: 'Found one in case anyone needs: https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
          . Edit: Nevermind, that model didn''t load for me.'
        updatedAt: '2023-04-04T13:15:27.105Z'
      numEdits: 2
      reactions: []
    id: 642c2180225f748ce30504a8
    type: comment
  author: nacs
  content: 'Found one in case anyone needs: https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
    . Edit: Nevermind, that model didn''t load for me.'
  created_at: 2023-04-04 12:09:20+00:00
  edited: true
  hidden: false
  id: 642c2180225f748ce30504a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-04-05T00:45:23.000Z'
    data:
      edited: false
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: '<p>Does anyone have a walkthrough for how to use GPTQ to 4-bit quantize
          the weights? I would like to know how to do this for future model releases.</p>

          '
        raw: Does anyone have a walkthrough for how to use GPTQ to 4-bit quantize
          the weights? I would like to know how to do this for future model releases.
        updatedAt: '2023-04-05T00:45:23.674Z'
      numEdits: 0
      reactions: []
    id: 642cc4a3d3340aee04449334
    type: comment
  author: disarmyouwitha
  content: Does anyone have a walkthrough for how to use GPTQ to 4-bit quantize the
    weights? I would like to know how to do this for future model releases.
  created_at: 2023-04-04 23:45:23+00:00
  edited: false
  hidden: false
  id: 642cc4a3d3340aee04449334
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
      fullname: nacs
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nacs
      type: user
    createdAt: '2023-04-05T03:50:59.000Z'
    data:
      edited: false
      editors:
      - nacs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8ca3771abd51458d8d3055d2b4668cf.svg
          fullname: nacs
          isHf: false
          isPro: false
          name: nacs
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;disarmyouwitha&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/disarmyouwitha\"\
          >@<span class=\"underline\">disarmyouwitha</span></a></span>\n\n\t</span></span>\
          \ The GPTQ-for-llama repo contains quantizing instructions</p>\n"
        raw: '@disarmyouwitha The GPTQ-for-llama repo contains quantizing instructions'
        updatedAt: '2023-04-05T03:50:59.128Z'
      numEdits: 0
      reactions: []
    id: 642cf0230d7975d942766fd7
    type: comment
  author: nacs
  content: '@disarmyouwitha The GPTQ-for-llama repo contains quantizing instructions'
  created_at: 2023-04-05 02:50:59+00:00
  edited: false
  hidden: false
  id: 642cf0230d7975d942766fd7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-04-05T04:20:06.000Z'
    data:
      edited: false
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: '<p>Thanks, not sure how I missed that ;0;</p>

          '
        raw: Thanks, not sure how I missed that ;0;
        updatedAt: '2023-04-05T04:20:06.345Z'
      numEdits: 0
      reactions: []
    id: 642cf6f6344576fa5eba85df
    type: comment
  author: disarmyouwitha
  content: Thanks, not sure how I missed that ;0;
  created_at: 2023-04-05 03:20:06+00:00
  edited: false
  hidden: false
  id: 642cf6f6344576fa5eba85df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/23aba778a7daae99348aeb0728cf4aec.svg
      fullname: Eric Kryski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ekryski
      type: user
    createdAt: '2023-04-07T20:42:34.000Z'
    data:
      edited: false
      editors:
      - ekryski
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/23aba778a7daae99348aeb0728cf4aec.svg
          fullname: Eric Kryski
          isHf: false
          isPro: false
          name: ekryski
          type: user
        html: '<blockquote>

          <p>Found one in case anyone needs: <a href="https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g">https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g</a>
          . Edit: Nevermind, that model didn''t load for me.</p>

          </blockquote>

          <p>I got this one running last night with the latest version of llama.cpp.
          A little slow (could have also me running a ton of stuff in parallel). Reasonable
          results: <a rel="nofollow" href="https://twitter.com/ekryski/status/1644275820805103617?s=20">https://twitter.com/ekryski/status/1644275820805103617?s=20</a>.</p>

          <p>Definitely promising. Gonna test out some embeddings on it to see how
          it handles things.</p>

          '
        raw: '> Found one in case anyone needs: https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
          . Edit: Nevermind, that model didn''t load for me.


          I got this one running last night with the latest version of llama.cpp.
          A little slow (could have also me running a ton of stuff in parallel). Reasonable
          results: https://twitter.com/ekryski/status/1644275820805103617?s=20.


          Definitely promising. Gonna test out some embeddings on it to see how it
          handles things.'
        updatedAt: '2023-04-07T20:42:34.310Z'
      numEdits: 0
      reactions: []
    id: 6430803a1a8652d76a3e68b5
    type: comment
  author: ekryski
  content: '> Found one in case anyone needs: https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g
    . Edit: Nevermind, that model didn''t load for me.


    I got this one running last night with the latest version of llama.cpp. A little
    slow (could have also me running a ton of stuff in parallel). Reasonable results:
    https://twitter.com/ekryski/status/1644275820805103617?s=20.


    Definitely promising. Gonna test out some embeddings on it to see how it handles
    things.'
  created_at: 2023-04-07 19:42:34+00:00
  edited: false
  hidden: false
  id: 6430803a1a8652d76a3e68b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a545728872bc7cc9d8f77dd67cd8090.svg
      fullname: Aviral Agarwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JackReacher23
      type: user
    createdAt: '2023-04-26T10:33:06.000Z'
    data:
      edited: false
      editors:
      - JackReacher23
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a545728872bc7cc9d8f77dd67cd8090.svg
          fullname: Aviral Agarwal
          isHf: false
          isPro: false
          name: JackReacher23
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ekryski&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ekryski\">@<span class=\"\
          underline\">ekryski</span></a></span>\n\n\t</span></span> can you please\
          \ tell the pre-requisites before loading the model you are mentioning? I\
          \ wanted to try it out myself as well in Python but I haven't been able\
          \ to make sense of all the things. It will be really helpful.</p>\n"
        raw: '@ekryski can you please tell the pre-requisites before loading the model
          you are mentioning? I wanted to try it out myself as well in Python but
          I haven''t been able to make sense of all the things. It will be really
          helpful.'
        updatedAt: '2023-04-26T10:33:06.626Z'
      numEdits: 0
      reactions: []
    id: 6448fde2f88f1495f094f276
    type: comment
  author: JackReacher23
  content: '@ekryski can you please tell the pre-requisites before loading the model
    you are mentioning? I wanted to try it out myself as well in Python but I haven''t
    been able to make sense of all the things. It will be really helpful.'
  created_at: 2023-04-26 09:33:06+00:00
  edited: false
  hidden: false
  id: 6448fde2f88f1495f094f276
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: chavinlo/gpt4-x-alpaca
repo_type: model
status: open
target_branch: null
title: 4-bit quantized?
