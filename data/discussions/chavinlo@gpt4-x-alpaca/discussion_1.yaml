!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cruiser
conflicting_files: null
created_at: 2023-03-31 12:45:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/10202572ef0bee95a185fa31144d0fa8.svg
      fullname: music lover
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cruiser
      type: user
    createdAt: '2023-03-31T13:45:09.000Z'
    data:
      edited: false
      editors:
      - cruiser
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/10202572ef0bee95a185fa31144d0fa8.svg
          fullname: music lover
          isHf: false
          isPro: false
          name: cruiser
          type: user
        html: '<p>gpt4all or something else? If gpt4all, hopefully it was on the unfiltered
          dataset with all the "as a large language model" removed. </p>

          '
        raw: 'gpt4all or something else? If gpt4all, hopefully it was on the unfiltered
          dataset with all the "as a large language model" removed. '
        updatedAt: '2023-03-31T13:45:09.866Z'
      numEdits: 0
      reactions: []
    id: 6426e3e59a7e77d6546663cd
    type: comment
  author: cruiser
  content: 'gpt4all or something else? If gpt4all, hopefully it was on the unfiltered
    dataset with all the "as a large language model" removed. '
  created_at: 2023-03-31 12:45:09+00:00
  edited: false
  hidden: false
  id: 6426e3e59a7e77d6546663cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-03-31T13:59:59.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>I hope it''s a gpt 4 dataset without some "I''m sorry, as a large
          language model" bullshit inside</p>

          '
        raw: I hope it's a gpt 4 dataset without some "I'm sorry, as a large language
          model" bullshit inside
        updatedAt: '2023-03-31T13:59:59.019Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - concedo
      - count: 1
        reaction: "\U0001F614"
        users:
        - concedo
    id: 6426e75f2db5108b10b682e7
    type: comment
  author: TheYuriLover
  content: I hope it's a gpt 4 dataset without some "I'm sorry, as a large language
    model" bullshit inside
  created_at: 2023-03-31 12:59:59+00:00
  edited: false
  hidden: false
  id: 6426e75f2db5108b10b682e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-03-31T15:41:09.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>It is gpt-4 self instruct.</p>

          '
        raw: It is gpt-4 self instruct.
        updatedAt: '2023-03-31T15:41:09.666Z'
      numEdits: 0
      reactions: []
    id: 6426ff15aef569700e8a6562
    type: comment
  author: teknium
  content: It is gpt-4 self instruct.
  created_at: 2023-03-31 14:41:09+00:00
  edited: false
  hidden: false
  id: 6426ff15aef569700e8a6562
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b766e76fddbd1fba03e763b5f601f73.svg
      fullname: Jonathan Marsh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JonOne
      type: user
    createdAt: '2023-03-31T20:46:35.000Z'
    data:
      edited: false
      editors:
      - JonOne
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3b766e76fddbd1fba03e763b5f601f73.svg
          fullname: Jonathan Marsh
          isHf: false
          isPro: false
          name: JonOne
          type: user
        html: "<p>I have Gpt4all installed. How do I install this?\u2019</p>\n"
        raw: "I have Gpt4all installed. How do I install this?\u2019"
        updatedAt: '2023-03-31T20:46:35.397Z'
      numEdits: 0
      reactions: []
    id: 642746ab188b9f01b17bb9f8
    type: comment
  author: JonOne
  content: "I have Gpt4all installed. How do I install this?\u2019"
  created_at: 2023-03-31 19:46:35+00:00
  edited: false
  hidden: false
  id: 642746ab188b9f01b17bb9f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-04-01T01:48:01.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>This has nothing to do with gpt4all, you pip install transformers
          and run inference on this model</p>

          '
        raw: This has nothing to do with gpt4all, you pip install transformers and
          run inference on this model
        updatedAt: '2023-04-01T01:48:01.475Z'
      numEdits: 0
      reactions: []
    id: 64278d51188b9f01b17d498c
    type: comment
  author: teknium
  content: This has nothing to do with gpt4all, you pip install transformers and run
    inference on this model
  created_at: 2023-04-01 00:48:01+00:00
  edited: false
  hidden: false
  id: 64278d51188b9f01b17d498c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/Ln3m7sK52hwGuMnPe4mfU.jpeg?w=200&h=200&f=face
      fullname: karan4d
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: karan4d
      type: user
    createdAt: '2023-04-02T04:45:05.000Z'
    data:
      edited: false
      editors:
      - karan4d
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/Ln3m7sK52hwGuMnPe4mfU.jpeg?w=200&h=200&f=face
          fullname: karan4d
          isHf: false
          isPro: false
          name: karan4d
          type: user
        html: '<blockquote>

          <p>It is gpt-4 self instruct.</p>

          </blockquote>

          <p>will you be releasing the dataset for further research?</p>

          '
        raw: '> It is gpt-4 self instruct.


          will you be releasing the dataset for further research?'
        updatedAt: '2023-04-02T04:45:05.377Z'
      numEdits: 0
      reactions: []
    id: 64290851fdd449b0d825570c
    type: comment
  author: karan4d
  content: '> It is gpt-4 self instruct.


    will you be releasing the dataset for further research?'
  created_at: 2023-04-02 03:45:05+00:00
  edited: false
  hidden: false
  id: 64290851fdd449b0d825570c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-04-02T06:29:20.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<blockquote>

          <blockquote>

          <p>It is gpt-4 self instruct.</p>

          </blockquote>

          <p>will you be releasing the dataset for further research?</p>

          </blockquote>

          <p>Here is the dataset: <a rel="nofollow" href="https://github.com/teknium1/GPTeacher">https://github.com/teknium1/GPTeacher</a><br>Its
          the general instruct set</p>

          '
        raw: "> > It is gpt-4 self instruct.\n> \n> will you be releasing the dataset\
          \ for further research?\n\nHere is the dataset: https://github.com/teknium1/GPTeacher\n\
          Its the general instruct set"
        updatedAt: '2023-04-02T06:29:20.358Z'
      numEdits: 0
      reactions: []
    id: 642920c0b20cdada12fa7d20
    type: comment
  author: teknium
  content: "> > It is gpt-4 self instruct.\n> \n> will you be releasing the dataset\
    \ for further research?\n\nHere is the dataset: https://github.com/teknium1/GPTeacher\n\
    Its the general instruct set"
  created_at: 2023-04-02 05:29:20+00:00
  edited: false
  hidden: false
  id: 642920c0b20cdada12fa7d20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7ea63d3651927cae95b4f812adce957.svg
      fullname: Hans Mayer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gustavoson
      type: user
    createdAt: '2023-04-02T09:57:19.000Z'
    data:
      edited: false
      editors:
      - Gustavoson
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7ea63d3651927cae95b4f812adce957.svg
          fullname: Hans Mayer
          isHf: false
          isPro: false
          name: Gustavoson
          type: user
        html: '<p>ValueError: Tokenizer class LlamaTokenizer does not exist or is
          not currently imported.</p>

          '
        raw: 'ValueError: Tokenizer class LlamaTokenizer does not exist or is not
          currently imported.'
        updatedAt: '2023-04-02T09:57:19.173Z'
      numEdits: 0
      reactions: []
    id: 6429517f4059ac9e6836628b
    type: comment
  author: Gustavoson
  content: 'ValueError: Tokenizer class LlamaTokenizer does not exist or is not currently
    imported.'
  created_at: 2023-04-02 08:57:19+00:00
  edited: false
  hidden: false
  id: 6429517f4059ac9e6836628b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8112b2c648c88e694b09fde069f37266.svg
      fullname: yeahman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mynamesisjames
      type: user
    createdAt: '2023-04-02T10:14:16.000Z'
    data:
      edited: false
      editors:
      - mynamesisjames
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8112b2c648c88e694b09fde069f37266.svg
          fullname: yeahman
          isHf: false
          isPro: false
          name: mynamesisjames
          type: user
        html: '<blockquote>

          <p>ValueError: Tokenizer class LlamaTokenizer does not exist or is not currently
          imported.</p>

          </blockquote>

          <p>You are not in the right thread ! Try ''pip install git+<a rel="nofollow"
          href="https://github.com/huggingface/transformers''">https://github.com/huggingface/transformers''</a>
          in your environment</p>

          '
        raw: '> ValueError: Tokenizer class LlamaTokenizer does not exist or is not
          currently imported.


          You are not in the right thread ! Try ''pip install git+https://github.com/huggingface/transformers''
          in your environment'
        updatedAt: '2023-04-02T10:14:16.174Z'
      numEdits: 0
      reactions: []
    id: 642955780132d430efdaf9c2
    type: comment
  author: mynamesisjames
  content: '> ValueError: Tokenizer class LlamaTokenizer does not exist or is not
    currently imported.


    You are not in the right thread ! Try ''pip install git+https://github.com/huggingface/transformers''
    in your environment'
  created_at: 2023-04-02 09:14:16+00:00
  edited: false
  hidden: false
  id: 642955780132d430efdaf9c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-04-02T14:45:35.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>Yes there is an issue with the class names in the model.config,
          open it up and change LLaMA to Llama</p>

          '
        raw: Yes there is an issue with the class names in the model.config, open
          it up and change LLaMA to Llama
        updatedAt: '2023-04-02T14:45:35.336Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - disarmyouwitha
        - ekryski
    id: 6429950fc9f3ac3c7549988a
    type: comment
  author: teknium
  content: Yes there is an issue with the class names in the model.config, open it
    up and change LLaMA to Llama
  created_at: 2023-04-02 13:45:35+00:00
  edited: false
  hidden: false
  id: 6429950fc9f3ac3c7549988a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-04-02T14:46:07.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>Also an issue in the tokenizer.config, change 512 to 2048 in the
          max token spot</p>

          '
        raw: Also an issue in the tokenizer.config, change 512 to 2048 in the max
          token spot
        updatedAt: '2023-04-02T14:46:07.074Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - disarmyouwitha
    id: 6429952f0132d430efdc4baf
    type: comment
  author: teknium
  content: Also an issue in the tokenizer.config, change 512 to 2048 in the max token
    spot
  created_at: 2023-04-02 13:46:07+00:00
  edited: false
  hidden: false
  id: 6429952f0132d430efdc4baf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12ec5e367e2eac1650b2961097f74fa2.svg
      fullname: Lim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: timlim123
      type: user
    createdAt: '2023-04-06T09:16:57.000Z'
    data:
      edited: false
      editors:
      - timlim123
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12ec5e367e2eac1650b2961097f74fa2.svg
          fullname: Lim
          isHf: false
          isPro: false
          name: timlim123
          type: user
        html: "<p>HI <span data-props=\"{&quot;user&quot;:&quot;teknium&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/teknium\">@<span class=\"\
          underline\">teknium</span></a></span>\n\n\t</span></span>,  where does it\
          \ show the fine-tuning code change the max sequence length to 2048?</p>\n"
        raw: HI @teknium,  where does it show the fine-tuning code change the max
          sequence length to 2048?
        updatedAt: '2023-04-06T09:16:57.702Z'
      numEdits: 0
      reactions: []
    id: 642e8e09bc17f2419d398ae1
    type: comment
  author: timlim123
  content: HI @teknium,  where does it show the fine-tuning code change the max sequence
    length to 2048?
  created_at: 2023-04-06 08:16:57+00:00
  edited: false
  hidden: false
  id: 642e8e09bc17f2419d398ae1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-04-06T10:15:50.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: "<blockquote>\n<p>HI <span data-props=\"{&quot;user&quot;:&quot;teknium&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/teknium\"\
          >@<span class=\"underline\">teknium</span></a></span>\n\n\t</span></span>,\
          \  where does it show the fine-tuning code change the max sequence length\
          \ to 2048?</p>\n</blockquote>\n<p>LLaMA is already set for 2048 tokens,\
          \ its just set wrong in the config here</p>\n"
        raw: '> HI @teknium,  where does it show the fine-tuning code change the max
          sequence length to 2048?


          LLaMA is already set for 2048 tokens, its just set wrong in the config here'
        updatedAt: '2023-04-06T10:15:50.119Z'
      numEdits: 0
      reactions: []
    id: 642e9bd6cf2c77d5e5a4dbc4
    type: comment
  author: teknium
  content: '> HI @teknium,  where does it show the fine-tuning code change the max
    sequence length to 2048?


    LLaMA is already set for 2048 tokens, its just set wrong in the config here'
  created_at: 2023-04-06 09:15:50+00:00
  edited: false
  hidden: false
  id: 642e9bd6cf2c77d5e5a4dbc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12ec5e367e2eac1650b2961097f74fa2.svg
      fullname: Lim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: timlim123
      type: user
    createdAt: '2023-04-06T13:22:14.000Z'
    data:
      edited: false
      editors:
      - timlim123
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12ec5e367e2eac1650b2961097f74fa2.svg
          fullname: Lim
          isHf: false
          isPro: false
          name: timlim123
          type: user
        html: '<p>Thank you, I got it now.</p>

          <p>It seems weird that Llama recommends changing the config to 512 to make
          it fit better with GPUs, I always thought that the input size into LLM are
          fixed and lower length input are always padded to the maximum length anyways.
          A question that does not relate to this repo but:</p>

          <p>How does reducing the sequence length to 512 during inference (like in
          Llama) help? Wouldn''t the model just pad to the maximum size that it was
          trained on?</p>

          '
        raw: 'Thank you, I got it now.


          It seems weird that Llama recommends changing the config to 512 to make
          it fit better with GPUs, I always thought that the input size into LLM are
          fixed and lower length input are always padded to the maximum length anyways.
          A question that does not relate to this repo but:


          How does reducing the sequence length to 512 during inference (like in Llama)
          help? Wouldn''t the model just pad to the maximum size that it was trained
          on?'
        updatedAt: '2023-04-06T13:22:14.773Z'
      numEdits: 0
      reactions: []
    id: 642ec786077daa71793d5781
    type: comment
  author: timlim123
  content: 'Thank you, I got it now.


    It seems weird that Llama recommends changing the config to 512 to make it fit
    better with GPUs, I always thought that the input size into LLM are fixed and
    lower length input are always padded to the maximum length anyways. A question
    that does not relate to this repo but:


    How does reducing the sequence length to 512 during inference (like in Llama)
    help? Wouldn''t the model just pad to the maximum size that it was trained on?'
  created_at: 2023-04-06 12:22:14+00:00
  edited: false
  hidden: false
  id: 642ec786077daa71793d5781
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: chavinlo/gpt4-x-alpaca
repo_type: model
status: open
target_branch: null
title: which dataset?
