!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tocsa
conflicting_files: null
created_at: 2023-09-22 20:31:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/723d2d2e084798a77a68179cf26390f0.svg
      fullname: Csaba Toth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tocsa
      type: user
    createdAt: '2023-09-22T21:31:32.000Z'
    data:
      edited: false
      editors:
      - tocsa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8373412489891052
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/723d2d2e084798a77a68179cf26390f0.svg
          fullname: Csaba Toth
          isHf: false
          isPro: false
          name: tocsa
          type: user
        html: "<p>The SgaeMaker deploy card shows this invocation:</p>\n<pre><code>predictor.predict({\n\
          \    \"inputs\": \"Can you please let us know more details about your \"\
          ,\n})\n</code></pre>\n<p>But this is an image+text multi modal model. How\
          \ do I pass in both the text prompt and the image?</p>\n"
        raw: "The SgaeMaker deploy card shows this invocation:\r\n```\r\npredictor.predict({\r\
          \n\t\"inputs\": \"Can you please let us know more details about your \"\
          ,\r\n})\r\n```\r\nBut this is an image+text multi modal model. How do I\
          \ pass in both the text prompt and the image?"
        updatedAt: '2023-09-22T21:31:32.444Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - tocsa
    id: 650e07b46620b0c57e148442
    type: comment
  author: tocsa
  content: "The SgaeMaker deploy card shows this invocation:\r\n```\r\npredictor.predict({\r\
    \n\t\"inputs\": \"Can you please let us know more details about your \",\r\n})\r\
    \n```\r\nBut this is an image+text multi modal model. How do I pass in both the\
    \ text prompt and the image?"
  created_at: 2023-09-22 20:31:32+00:00
  edited: false
  hidden: false
  id: 650e07b46620b0c57e148442
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/723d2d2e084798a77a68179cf26390f0.svg
      fullname: Csaba Toth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tocsa
      type: user
    createdAt: '2023-09-28T07:50:57.000Z'
    data:
      edited: false
      editors:
      - tocsa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5926152467727661
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/723d2d2e084798a77a68179cf26390f0.svg
          fullname: Csaba Toth
          isHf: false
          isPro: false
          name: tocsa
          type: user
        html: '<p>HF Discussion 1: <a rel="nofollow" href="https://discuss.huggingface.co/t/can-text-to-image-models-be-deployed-to-a-sagemaker-endpoint/20120">https://discuss.huggingface.co/t/can-text-to-image-models-be-deployed-to-a-sagemaker-endpoint/20120</a><br>HF
          Discussion 2: <a rel="nofollow" href="https://discuss.huggingface.co/t/how-to-use-llava-with-huggingface/52315">https://discuss.huggingface.co/t/how-to-use-llava-with-huggingface/52315</a><br>My
          SO entry: <a rel="nofollow" href="https://stackoverflow.com/questions/77193088/how-to-perform-an-inference-on-a-llava-llama-model-deployed-to-sagemake-from-hug">https://stackoverflow.com/questions/77193088/how-to-perform-an-inference-on-a-llava-llama-model-deployed-to-sagemake-from-hug</a><br>SO
          entry about a serverless deployment: <a rel="nofollow" href="https://stackoverflow.com/questions/76197446/how-to-do-model-inference-on-a-multimodal-model-from-hugginface-using-sagemaker">https://stackoverflow.com/questions/76197446/how-to-do-model-inference-on-a-multimodal-model-from-hugginface-using-sagemaker</a><br>GitHub
          Discussion: <a rel="nofollow" href="https://github.com/haotian-liu/LLaVA/discussions/454">https://github.com/haotian-liu/LLaVA/discussions/454</a></p>

          '
        raw: 'HF Discussion 1: https://discuss.huggingface.co/t/can-text-to-image-models-be-deployed-to-a-sagemaker-endpoint/20120

          HF Discussion 2: https://discuss.huggingface.co/t/how-to-use-llava-with-huggingface/52315

          My SO entry: https://stackoverflow.com/questions/77193088/how-to-perform-an-inference-on-a-llava-llama-model-deployed-to-sagemake-from-hug

          SO entry about a serverless deployment: https://stackoverflow.com/questions/76197446/how-to-do-model-inference-on-a-multimodal-model-from-hugginface-using-sagemaker

          GitHub Discussion: https://github.com/haotian-liu/LLaVA/discussions/454'
        updatedAt: '2023-09-28T07:50:57.928Z'
      numEdits: 0
      reactions: []
    id: 651530610885410769b2d880
    type: comment
  author: tocsa
  content: 'HF Discussion 1: https://discuss.huggingface.co/t/can-text-to-image-models-be-deployed-to-a-sagemaker-endpoint/20120

    HF Discussion 2: https://discuss.huggingface.co/t/how-to-use-llava-with-huggingface/52315

    My SO entry: https://stackoverflow.com/questions/77193088/how-to-perform-an-inference-on-a-llava-llama-model-deployed-to-sagemake-from-hug

    SO entry about a serverless deployment: https://stackoverflow.com/questions/76197446/how-to-do-model-inference-on-a-multimodal-model-from-hugginface-using-sagemaker

    GitHub Discussion: https://github.com/haotian-liu/LLaVA/discussions/454'
  created_at: 2023-09-28 06:50:57+00:00
  edited: false
  hidden: false
  id: 651530610885410769b2d880
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/723d2d2e084798a77a68179cf26390f0.svg
      fullname: Csaba Toth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tocsa
      type: user
    createdAt: '2023-10-26T05:07:48.000Z'
    data:
      edited: false
      editors:
      - tocsa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7455698251724243
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/723d2d2e084798a77a68179cf26390f0.svg
          fullname: Csaba Toth
          isHf: false
          isPro: false
          name: tocsa
          type: user
        html: '<p>I got fed up and used replicate.com: <a rel="nofollow" href="https://stackoverflow.com/a/77364236/292502">https://stackoverflow.com/a/77364236/292502</a></p>

          '
        raw: 'I got fed up and used replicate.com: https://stackoverflow.com/a/77364236/292502'
        updatedAt: '2023-10-26T05:07:48.102Z'
      numEdits: 0
      reactions: []
    id: 6539f42426df26ecd138db3f
    type: comment
  author: tocsa
  content: 'I got fed up and used replicate.com: https://stackoverflow.com/a/77364236/292502'
  created_at: 2023-10-26 04:07:48+00:00
  edited: false
  hidden: false
  id: 6539f42426df26ecd138db3f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: liuhaotian/llava-llama-2-13b-chat-lightning-preview
repo_type: model
status: open
target_branch: null
title: How do I pass the text prompt and the image parameters to the predictor?
