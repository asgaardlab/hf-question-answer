!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sebastian651
conflicting_files: null
created_at: 2024-01-09 19:31:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/757fe741e59658e095877f88245593ea.svg
      fullname: Sebastian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sebastian651
      type: user
    createdAt: '2024-01-09T19:31:05.000Z'
    data:
      edited: false
      editors:
      - Sebastian651
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.88418048620224
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/757fe741e59658e095877f88245593ea.svg
          fullname: Sebastian
          isHf: false
          isPro: false
          name: Sebastian651
          type: user
        html: '<p>Hello bartowski,<br>would it be possible to get a 3bpw version of
          this model? At this size, 8GB GPUs are able to run 13B models at 4k context.</p>

          <p>Thanks,<br>Sebastian</p>

          '
        raw: "Hello bartowski,\r\nwould it be possible to get a 3bpw version of this\
          \ model? At this size, 8GB GPUs are able to run 13B models at 4k context.\r\
          \n\r\nThanks,\r\nSebastian"
        updatedAt: '2024-01-09T19:31:05.967Z'
      numEdits: 0
      reactions: []
    id: 659d9ef901805191e5df5a14
    type: comment
  author: Sebastian651
  content: "Hello bartowski,\r\nwould it be possible to get a 3bpw version of this\
    \ model? At this size, 8GB GPUs are able to run 13B models at 4k context.\r\n\r\
    \nThanks,\r\nSebastian"
  created_at: 2024-01-09 19:31:05+00:00
  edited: false
  hidden: false
  id: 659d9ef901805191e5df5a14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
      fullname: Bartowski
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bartowski
      type: user
    createdAt: '2024-01-10T04:11:04.000Z'
    data:
      edited: false
      editors:
      - bartowski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8779328465461731
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
          fullname: Bartowski
          isHf: false
          isPro: false
          name: bartowski
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sebastian651&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Sebastian651\"\
          >@<span class=\"underline\">Sebastian651</span></a></span>\n\n\t</span></span>\
          \ oh sure, i'll add 3.0 to all my future 13B as well if that's a nice target</p>\n"
        raw: '@Sebastian651 oh sure, i''ll add 3.0 to all my future 13B as well if
          that''s a nice target'
        updatedAt: '2024-01-10T04:11:04.181Z'
      numEdits: 0
      reactions: []
    id: 659e18d8f2e0e03e45939544
    type: comment
  author: bartowski
  content: '@Sebastian651 oh sure, i''ll add 3.0 to all my future 13B as well if that''s
    a nice target'
  created_at: 2024-01-10 04:11:04+00:00
  edited: false
  hidden: false
  id: 659e18d8f2e0e03e45939544
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
      fullname: Bartowski
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bartowski
      type: user
    createdAt: '2024-01-10T18:09:34.000Z'
    data:
      edited: false
      editors:
      - bartowski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8750385046005249
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
          fullname: Bartowski
          isHf: false
          isPro: false
          name: bartowski
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Sebastian651&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Sebastian651\"\
          >@<span class=\"underline\">Sebastian651</span></a></span>\n\n\t</span></span>\
          \ , I made 3.0, 3.25, and 3.5 so you can experiment and find the sweet spot!\
          \ Let me know and whichever works best I'll add to 13B quants :)</p>\n"
        raw: Hey @Sebastian651 , I made 3.0, 3.25, and 3.5 so you can experiment and
          find the sweet spot! Let me know and whichever works best I'll add to 13B
          quants :)
        updatedAt: '2024-01-10T18:09:34.644Z'
      numEdits: 0
      reactions: []
    id: 659edd5e96cd935fab603533
    type: comment
  author: bartowski
  content: Hey @Sebastian651 , I made 3.0, 3.25, and 3.5 so you can experiment and
    find the sweet spot! Let me know and whichever works best I'll add to 13B quants
    :)
  created_at: 2024-01-10 18:09:34+00:00
  edited: false
  hidden: false
  id: 659edd5e96cd935fab603533
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/757fe741e59658e095877f88245593ea.svg
      fullname: Sebastian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sebastian651
      type: user
    createdAt: '2024-01-11T17:09:46.000Z'
    data:
      edited: false
      editors:
      - Sebastian651
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9198205471038818
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/757fe741e59658e095877f88245593ea.svg
          fullname: Sebastian
          isHf: false
          isPro: false
          name: Sebastian651
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;bartowski&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bartowski\">@<span class=\"\
          underline\">bartowski</span></a></span>\n\n\t</span></span> thanks for the\
          \ quick reaction!<br>I tested the models today, the 3.0bpw is the best I\
          \ can do with my 8GB RTX 3050 at 4k context and 8bit cache. With this settings,\
          \ it's using 7.4GB VRAM and generates ~9 token/s. Which is a huge step up\
          \ and actually usable compared to about 2.7 token/s using a GGUF Q4_K_M\
          \ model only partially loaded to the GPU.<br>Using the 3.25bpw I can go\
          \ up to 3k context with 8-bit cache, but it gave me a bluescreen with a\
          \ memory management error at the first try. After reboot, it's working also\
          \ with 7.4GB of VRAM. For me, 4k context is a lower bound I like to use,\
          \ so I will stick with the 3.0bpw version :)</p>\n"
        raw: '@bartowski thanks for the quick reaction!

          I tested the models today, the 3.0bpw is the best I can do with my 8GB RTX
          3050 at 4k context and 8bit cache. With this settings, it''s using 7.4GB
          VRAM and generates ~9 token/s. Which is a huge step up and actually usable
          compared to about 2.7 token/s using a GGUF Q4_K_M model only partially loaded
          to the GPU.

          Using the 3.25bpw I can go up to 3k context with 8-bit cache, but it gave
          me a bluescreen with a memory management error at the first try. After reboot,
          it''s working also with 7.4GB of VRAM. For me, 4k context is a lower bound
          I like to use, so I will stick with the 3.0bpw version :)'
        updatedAt: '2024-01-11T17:09:46.477Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ramzeez88
    id: 65a020da44d2a6d6fb7768c7
    type: comment
  author: Sebastian651
  content: '@bartowski thanks for the quick reaction!

    I tested the models today, the 3.0bpw is the best I can do with my 8GB RTX 3050
    at 4k context and 8bit cache. With this settings, it''s using 7.4GB VRAM and generates
    ~9 token/s. Which is a huge step up and actually usable compared to about 2.7
    token/s using a GGUF Q4_K_M model only partially loaded to the GPU.

    Using the 3.25bpw I can go up to 3k context with 8-bit cache, but it gave me a
    bluescreen with a memory management error at the first try. After reboot, it''s
    working also with 7.4GB of VRAM. For me, 4k context is a lower bound I like to
    use, so I will stick with the 3.0bpw version :)'
  created_at: 2024-01-11 17:09:46+00:00
  edited: false
  hidden: false
  id: 65a020da44d2a6d6fb7768c7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: bartowski/LLaMA2-13B-Tiefighter-exl2
repo_type: model
status: open
target_branch: null
title: 'Request for 3bpw model (target: 8GB VRAM Cards)'
