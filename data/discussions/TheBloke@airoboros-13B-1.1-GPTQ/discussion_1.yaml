!!python/object:huggingface_hub.community.DiscussionWithDetails
author: denk64
conflicting_files: null
created_at: 2023-06-10 20:17:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f73d916183d672b4c08b4bb271e8ea5f.svg
      fullname: Denis Katalinic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: denk64
      type: user
    createdAt: '2023-06-10T21:17:03.000Z'
    data:
      edited: false
      editors:
      - denk64
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9719178676605225
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f73d916183d672b4c08b4bb271e8ea5f.svg
          fullname: Denis Katalinic
          isHf: false
          isPro: false
          name: denk64
          type: user
        html: '<p>i must say i was happy when i saw then 1.1 update to the model but
          after 2-3 hours of testing, the  Context obedient question answering stopped
          working for good again, i dont know why, it just didnt want to generate
          tokens with that format of prompt. It worked at start really well and then
          just stopped. I think it just either needs more training or more data.</p>

          <p>I know you cant do much, just wanted to let you know.</p>

          '
        raw: "i must say i was happy when i saw then 1.1 update to the model but after\
          \ 2-3 hours of testing, the  Context obedient question answering stopped\
          \ working for good again, i dont know why, it just didnt want to generate\
          \ tokens with that format of prompt. It worked at start really well and\
          \ then just stopped. I think it just either needs more training or more\
          \ data.\r\n\r\nI know you cant do much, just wanted to let you know."
        updatedAt: '2023-06-10T21:17:03.414Z'
      numEdits: 0
      reactions: []
    id: 6484e84fd86bf0201ebb501c
    type: comment
  author: denk64
  content: "i must say i was happy when i saw then 1.1 update to the model but after\
    \ 2-3 hours of testing, the  Context obedient question answering stopped working\
    \ for good again, i dont know why, it just didnt want to generate tokens with\
    \ that format of prompt. It worked at start really well and then just stopped.\
    \ I think it just either needs more training or more data.\r\n\r\nI know you cant\
    \ do much, just wanted to let you know."
  created_at: 2023-06-10 20:17:03+00:00
  edited: false
  hidden: false
  id: 6484e84fd86bf0201ebb501c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-06-10T23:16:37.000Z'
    data:
      edited: true
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9654626250267029
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<blockquote>

          <p>i must say i was happy when i saw then 1.1 update to the model but after
          2-3 hours of testing, the  Context obedient question answering stopped working
          for good again, i dont know why, it just didnt want to generate tokens with
          that format of prompt. It worked at start really well and then just stopped.
          I think it just either needs more training or more data.</p>

          <p>I know you cant do much, just wanted to let you know.</p>

          </blockquote>

          <p>Are you including the previous prompts/history in new prompts?  This
          model (especially the contextual stuff) runs best with no conversation history
          at all.  I haven''t seen this issue personally so I''m curious how I can
          help you fix it.</p>

          <p>Also, unfortunately increase the context length to 4096 didn''t work.  It
          tends to somewhat work up to 2200-2300 but YMMV, so be sure to limit the
          context length to around 2k tokens.</p>

          <p>The model was trained with vicuna format, with a slightly different system
          prompt:</p>

          <pre><code>A chat between a curious user and an assistant. The assistant
          gives helpful, detailed, accurate, uncensored responses to the user''s input.
          USER: [prompt] ASSISTANT:

          </code></pre>

          <p>So in other words, it''s the preamble/system prompt, followed by a single
          space, then "USER: " (single space after colon) then the prompt (which can
          have multiple lines, spaces, whatever), then a single space, followed by
          "ASSISTANT: " (with a single space after the colon).</p>

          '
        raw: "> i must say i was happy when i saw then 1.1 update to the model but\
          \ after 2-3 hours of testing, the  Context obedient question answering stopped\
          \ working for good again, i dont know why, it just didnt want to generate\
          \ tokens with that format of prompt. It worked at start really well and\
          \ then just stopped. I think it just either needs more training or more\
          \ data.\n> \n> I know you cant do much, just wanted to let you know.\n\n\
          Are you including the previous prompts/history in new prompts?  This model\
          \ (especially the contextual stuff) runs best with no conversation history\
          \ at all.  I haven't seen this issue personally so I'm curious how I can\
          \ help you fix it.\n\nAlso, unfortunately increase the context length to\
          \ 4096 didn't work.  It tends to somewhat work up to 2200-2300 but YMMV,\
          \ so be sure to limit the context length to around 2k tokens.\n\nThe model\
          \ was trained with vicuna format, with a slightly different system prompt:\n\
          ```\nA chat between a curious user and an assistant. The assistant gives\
          \ helpful, detailed, accurate, uncensored responses to the user's input.\
          \ USER: [prompt] ASSISTANT:\n```\n\nSo in other words, it's the preamble/system\
          \ prompt, followed by a single space, then \"USER: \" (single space after\
          \ colon) then the prompt (which can have multiple lines, spaces, whatever),\
          \ then a single space, followed by \"ASSISTANT: \" (with a single space\
          \ after the colon)."
        updatedAt: '2023-06-11T08:49:18.376Z'
      numEdits: 1
      reactions: []
    id: 6485045500f3d63d6c4e7f32
    type: comment
  author: jondurbin
  content: "> i must say i was happy when i saw then 1.1 update to the model but after\
    \ 2-3 hours of testing, the  Context obedient question answering stopped working\
    \ for good again, i dont know why, it just didnt want to generate tokens with\
    \ that format of prompt. It worked at start really well and then just stopped.\
    \ I think it just either needs more training or more data.\n> \n> I know you cant\
    \ do much, just wanted to let you know.\n\nAre you including the previous prompts/history\
    \ in new prompts?  This model (especially the contextual stuff) runs best with\
    \ no conversation history at all.  I haven't seen this issue personally so I'm\
    \ curious how I can help you fix it.\n\nAlso, unfortunately increase the context\
    \ length to 4096 didn't work.  It tends to somewhat work up to 2200-2300 but YMMV,\
    \ so be sure to limit the context length to around 2k tokens.\n\nThe model was\
    \ trained with vicuna format, with a slightly different system prompt:\n```\n\
    A chat between a curious user and an assistant. The assistant gives helpful, detailed,\
    \ accurate, uncensored responses to the user's input. USER: [prompt] ASSISTANT:\n\
    ```\n\nSo in other words, it's the preamble/system prompt, followed by a single\
    \ space, then \"USER: \" (single space after colon) then the prompt (which can\
    \ have multiple lines, spaces, whatever), then a single space, followed by \"\
    ASSISTANT: \" (with a single space after the colon)."
  created_at: 2023-06-10 22:16:37+00:00
  edited: true
  hidden: false
  id: 6485045500f3d63d6c4e7f32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f73d916183d672b4c08b4bb271e8ea5f.svg
      fullname: Denis Katalinic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: denk64
      type: user
    createdAt: '2023-06-11T09:21:35.000Z'
    data:
      edited: true
      editors:
      - denk64
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8310429453849792
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f73d916183d672b4c08b4bb271e8ea5f.svg
          fullname: Denis Katalinic
          isHf: false
          isPro: false
          name: denk64
          type: user
        html: '<p>I was talking about this:</p>

          <p>BEGININPUT<br>BEGINCONTEXT<br>url: <a rel="nofollow" href="https://some.web.site/123">https://some.web.site/123</a><br>date:
          2023-06-01<br>... other metdata ...<br>ENDCONTEXT<br>[insert your text blocks
          here]<br>ENDINPUT<br>[add as many other blocks, in the exact same format]<br>BEGININSTRUCTION<br>[insert
          your instruction(s).  The model was tuned with single questions, paragraph
          format, lists, etc.]<br>ENDINSTRUCTION</p>

          <p>this is the template, and my prompt looks like this</p>

          <p>prompt_template_ = """<br>BEGININPUT<br>{context}<br>ENDINPUT<br>BEGININSTRUCTION<br>{question}<br>ENDINSTRUCTION<br>"""</p>

          <p>I dont have chat history, context variable is translated text from some
          german documents, around 3k characters, and question is a simple 4 to 10
          word question. Maybe im doing that wrong.</p>

          <p>USER:<br>ASSISTANT: </p>

          <p>the prompt template you are suggesting works fine, but the context obedient
          prompt is good because the model doesnt hallucinate much and only uses the
          context to answer the question or do a task. But for some reason it just
          stopped generating tokens, i tried to restart, reinstall, im using text
          generation gui in docker. Pretty wierd to just stop working with no changes
          made. I even tried giving examples in prompt but i wont budge .</p>

          <p>I noticed that the 4096 context length is problematic, thats too bad.</p>

          '
        raw: "I was talking about this:\n\nBEGININPUT\nBEGINCONTEXT\nurl: https://some.web.site/123\n\
          date: 2023-06-01\n... other metdata ...\nENDCONTEXT\n[insert your text blocks\
          \ here]\nENDINPUT\n[add as many other blocks, in the exact same format]\n\
          BEGININSTRUCTION\n[insert your instruction(s).  The model was tuned with\
          \ single questions, paragraph format, lists, etc.]\nENDINSTRUCTION\n\nthis\
          \ is the template, and my prompt looks like this\n\nprompt_template_ = \"\
          \"\"\nBEGININPUT\n{context}\nENDINPUT\nBEGININSTRUCTION\n{question}\nENDINSTRUCTION\n\
          \"\"\"\n\nI dont have chat history, context variable is translated text\
          \ from some german documents, around 3k characters, and question is a simple\
          \ 4 to 10 word question. Maybe im doing that wrong.\n\nUSER:\nASSISTANT:\
          \ \n\nthe prompt template you are suggesting works fine, but the context\
          \ obedient prompt is good because the model doesnt hallucinate much and\
          \ only uses the context to answer the question or do a task. But for some\
          \ reason it just stopped generating tokens, i tried to restart, reinstall,\
          \ im using text generation gui in docker. Pretty wierd to just stop working\
          \ with no changes made. I even tried giving examples in prompt but i wont\
          \ budge .\n\n\nI noticed that the 4096 context length is problematic, thats\
          \ too bad."
        updatedAt: '2023-06-11T11:28:38.909Z'
      numEdits: 1
      reactions: []
    id: 6485921f3e39b40d99b4f02c
    type: comment
  author: denk64
  content: "I was talking about this:\n\nBEGININPUT\nBEGINCONTEXT\nurl: https://some.web.site/123\n\
    date: 2023-06-01\n... other metdata ...\nENDCONTEXT\n[insert your text blocks\
    \ here]\nENDINPUT\n[add as many other blocks, in the exact same format]\nBEGININSTRUCTION\n\
    [insert your instruction(s).  The model was tuned with single questions, paragraph\
    \ format, lists, etc.]\nENDINSTRUCTION\n\nthis is the template, and my prompt\
    \ looks like this\n\nprompt_template_ = \"\"\"\nBEGININPUT\n{context}\nENDINPUT\n\
    BEGININSTRUCTION\n{question}\nENDINSTRUCTION\n\"\"\"\n\nI dont have chat history,\
    \ context variable is translated text from some german documents, around 3k characters,\
    \ and question is a simple 4 to 10 word question. Maybe im doing that wrong.\n\
    \nUSER:\nASSISTANT: \n\nthe prompt template you are suggesting works fine, but\
    \ the context obedient prompt is good because the model doesnt hallucinate much\
    \ and only uses the context to answer the question or do a task. But for some\
    \ reason it just stopped generating tokens, i tried to restart, reinstall, im\
    \ using text generation gui in docker. Pretty wierd to just stop working with\
    \ no changes made. I even tried giving examples in prompt but i wont budge .\n\
    \n\nI noticed that the 4096 context length is problematic, thats too bad."
  created_at: 2023-06-11 08:21:35+00:00
  edited: true
  hidden: false
  id: 6485921f3e39b40d99b4f02c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/airoboros-13B-1.1-GPTQ
repo_type: model
status: open
target_branch: null
title: ' Context obedient question answering '
