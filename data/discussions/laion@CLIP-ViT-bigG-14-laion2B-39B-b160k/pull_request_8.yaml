!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Geonmo
conflicting_files: []
created_at: 2023-06-27 23:42:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660103241634-61839579a658bd70108e88c0.jpeg?w=200&h=200&f=face
      fullname: Geonmo Gu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Geonmo
      type: user
    createdAt: '2023-06-28T00:42:07.000Z'
    data:
      edited: false
      editors:
      - Geonmo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8484135270118713
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660103241634-61839579a658bd70108e88c0.jpeg?w=200&h=200&f=face
          fullname: Geonmo Gu
          isHf: false
          isPro: false
          name: Geonmo
          type: user
        html: '<p>The presence of <code>projection_dim</code> is required but missing
          in the current implementation. Consequently, the utilization of <code>CLIPVisionModelWithProjection</code>
          and <code>CLIPTextModelWithProjection</code> is currently unavailable.</p>

          '
        raw: The presence of `projection_dim` is required but missing in the current
          implementation. Consequently, the utilization of `CLIPVisionModelWithProjection`
          and `CLIPTextModelWithProjection` is currently unavailable.
        updatedAt: '2023-06-28T00:42:07.632Z'
      numEdits: 0
      reactions: []
    id: 649b81df6d4d47d5f0293a70
    type: comment
  author: Geonmo
  content: The presence of `projection_dim` is required but missing in the current
    implementation. Consequently, the utilization of `CLIPVisionModelWithProjection`
    and `CLIPTextModelWithProjection` is currently unavailable.
  created_at: 2023-06-27 23:42:07+00:00
  edited: false
  hidden: false
  id: 649b81df6d4d47d5f0293a70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660103241634-61839579a658bd70108e88c0.jpeg?w=200&h=200&f=face
      fullname: Geonmo Gu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Geonmo
      type: user
    createdAt: '2023-06-28T00:42:07.000Z'
    data:
      oid: 01e89cc4f6c5ab7bbebaa693a7f0a265ce06576f
      parents:
      - 8c7a3583335de4dba1b07182dbf81c75137ce67b
      subject: Update config.json
    id: 649b81df0000000000000000
    type: commit
  author: Geonmo
  created_at: 2023-06-27 23:42:07+00:00
  id: 649b81df0000000000000000
  oid: 01e89cc4f6c5ab7bbebaa693a7f0a265ce06576f
  summary: Update config.json
  type: commit
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660103241634-61839579a658bd70108e88c0.jpeg?w=200&h=200&f=face
      fullname: Geonmo Gu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Geonmo
      type: user
    createdAt: '2023-06-28T00:43:09.000Z'
    data:
      from: Update config.json
      to: Please correct config.json
    id: 649b821d2ddbc93209ff6ce1
    type: title-change
  author: Geonmo
  created_at: 2023-06-27 23:43:09+00:00
  id: 649b821d2ddbc93209ff6ce1
  new_title: Please correct config.json
  old_title: Update config.json
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660103241634-61839579a658bd70108e88c0.jpeg?w=200&h=200&f=face
      fullname: Geonmo Gu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Geonmo
      type: user
    createdAt: '2023-06-28T00:45:27.000Z'
    data:
      edited: true
      editors:
      - Geonmo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.12606607377529144
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660103241634-61839579a658bd70108e88c0.jpeg?w=200&h=200&f=face
          fullname: Geonmo Gu
          isHf: false
          isPro: false
          name: Geonmo
          type: user
        html: "<pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"\
          hljs-keyword\">import</span> CLIPVisionModelWithProjection\n\nmodel_name\
          \ = <span class=\"hljs-string\">'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'</span>\n\
          \nclip_vision_model = CLIPVisionModelWithProjection.from_pretrained(model_name,\
          \ torch_dtype=torch.float16)\n</code></pre>\n<p>then it will happen</p>\n\
          <pre><code class=\"language-bash\">`text_config_dict` is provided <span\
          \ class=\"hljs-built_in\">which</span> will be used to initialize `CLIPTextConfig`.\
          \ The value `text_config[<span class=\"hljs-string\">\"id2label\"</span>]`\
          \ will be overriden.\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:07&lt;00:00, \
          \ 3.51s/it]\nRuntimeError: Error(s) <span class=\"hljs-keyword\">in</span>\
          \ loading state_dict <span class=\"hljs-keyword\">for</span> CLIPVisionModelWithProjection:\n\
          \        size mismatch <span class=\"hljs-keyword\">for</span> visual_projection.weight:\
          \ copying a param with shape torch.Size([1280, 1664]) from checkpoint, the\
          \ shape <span class=\"hljs-keyword\">in</span> current model is torch.Size([512,\
          \ 1664]).\n        You may consider adding `ignore_mismatched_sizes=True`\
          \ <span class=\"hljs-keyword\">in</span> the model `from_pretrained` method.\n\
          </code></pre>\n"
        raw: "```python\nimport torch\nfrom transformers import CLIPVisionModelWithProjection\n\
          \nmodel_name = 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'\n\nclip_vision_model\
          \ = CLIPVisionModelWithProjection.from_pretrained(model_name, torch_dtype=torch.float16)\n\
          ```\n\nthen it will happen\n\n```bash\n`text_config_dict` is provided which\
          \ will be used to initialize `CLIPTextConfig`. The value `text_config[\"\
          id2label\"]` will be overriden.\nLoading checkpoint shards: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:07<00:00,\
          \  3.51s/it]\nRuntimeError: Error(s) in loading state_dict for CLIPVisionModelWithProjection:\n\
          \        size mismatch for visual_projection.weight: copying a param with\
          \ shape torch.Size([1280, 1664]) from checkpoint, the shape in current model\
          \ is torch.Size([512, 1664]).\n        You may consider adding `ignore_mismatched_sizes=True`\
          \ in the model `from_pretrained` method.\n```"
        updatedAt: '2023-06-28T00:58:47.302Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AisingioroHao0
    id: 649b82a7d6897b1e0afba354
    type: comment
  author: Geonmo
  content: "```python\nimport torch\nfrom transformers import CLIPVisionModelWithProjection\n\
    \nmodel_name = 'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'\n\nclip_vision_model\
    \ = CLIPVisionModelWithProjection.from_pretrained(model_name, torch_dtype=torch.float16)\n\
    ```\n\nthen it will happen\n\n```bash\n`text_config_dict` is provided which will\
    \ be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"\
    ]` will be overriden.\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 2/2 [00:07<00:00,  3.51s/it]\nRuntimeError: Error(s)\
    \ in loading state_dict for CLIPVisionModelWithProjection:\n        size mismatch\
    \ for visual_projection.weight: copying a param with shape torch.Size([1280, 1664])\
    \ from checkpoint, the shape in current model is torch.Size([512, 1664]).\n  \
    \      You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained`\
    \ method.\n```"
  created_at: 2023-06-27 23:45:27+00:00
  edited: true
  hidden: false
  id: 649b82a7d6897b1e0afba354
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660103241634-61839579a658bd70108e88c0.jpeg?w=200&h=200&f=face
      fullname: Geonmo Gu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Geonmo
      type: user
    createdAt: '2023-06-28T00:48:06.000Z'
    data:
      oid: b1e107bce09979ff77c0508a48d0e35320dcbed0
      parents:
      - 01e89cc4f6c5ab7bbebaa693a7f0a265ce06576f
      subject: Update config.json
    id: 649b83460000000000000000
    type: commit
  author: Geonmo
  created_at: 2023-06-27 23:48:06+00:00
  id: 649b83460000000000000000
  oid: b1e107bce09979ff77c0508a48d0e35320dcbed0
  summary: Update config.json
  type: commit
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660103241634-61839579a658bd70108e88c0.jpeg?w=200&h=200&f=face
      fullname: Geonmo Gu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Geonmo
      type: user
    createdAt: '2023-06-28T02:13:22.000Z'
    data:
      oid: 1334e79e3e45e36de55788316cfd6f1175fff48e
      parents:
      - b1e107bce09979ff77c0508a48d0e35320dcbed0
      subject: Update config.json
    id: 649b97420000000000000000
    type: commit
  author: Geonmo
  created_at: 2023-06-28 01:13:22+00:00
  id: 649b97420000000000000000
  oid: 1334e79e3e45e36de55788316cfd6f1175fff48e
  summary: Update config.json
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2024-01-16T21:25:16.000Z'
    data:
      edited: false
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6151100993156433
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: '<p>fixed</p>

          '
        raw: fixed
        updatedAt: '2024-01-16T21:25:16.342Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Geonmo
      relatedEventId: 65a6f43c5c58475cf9f179ba
    id: 65a6f43c5c58475cf9f179b7
    type: comment
  author: rwightman
  content: fixed
  created_at: 2024-01-16 21:25:16+00:00
  edited: false
  hidden: false
  id: 65a6f43c5c58475cf9f179b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2024-01-16T21:25:16.000Z'
    data:
      status: closed
    id: 65a6f43c5c58475cf9f179ba
    type: status-change
  author: rwightman
  created_at: 2024-01-16 21:25:16+00:00
  id: 65a6f43c5c58475cf9f179ba
  new_status: closed
  type: status-change
is_pull_request: true
merge_commit_oid: null
num: 8
repo_id: laion/CLIP-ViT-bigG-14-laion2B-39B-b160k
repo_type: model
status: closed
target_branch: refs/heads/main
title: Please correct config.json
