!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aminedjeghri
conflicting_files: null
created_at: 2023-06-09 08:05:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/378b502684b781cc061709f1052d047e.svg
      fullname: Amine Djeghri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aminedjeghri
      type: user
    createdAt: '2023-06-09T09:05:05.000Z'
    data:
      edited: true
      editors:
      - aminedjeghri
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3977499306201935
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/378b502684b781cc061709f1052d047e.svg
          fullname: Amine Djeghri
          isHf: false
          isPro: false
          name: aminedjeghri
          type: user
        html: "<p>AttributeError: 'LlamaCppModel' object has no attribute 'model'</p>\n\
          <pre><code>llama.cpp: loading model from models\\starcoder\\starcoder.ggmlv3.q4_1.bin\n\
          error loading model: missing tok_embeddings.weight\nllama_init_from_file:\
          \ failed to load model\nTraceback (most recent call last):\n  File \"C:\\\
          Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
          server.py\", line 1079, in &lt;module&gt;\n    shared.model, shared.tokenizer\
          \ = load_model(shared.model_name)\n  File \"C:\\Users\\AmineDjeghri\\Downloads\\\
          oobabooga_windows\\oobabooga_windows\\text-generation-webui\\modules\\models.py\"\
          , line 94, in load_model\n    output = load_func(model_name)\n  File \"\
          C:\\Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 271, in llamacpp_loader\n\
          \    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n  File\
          \ \"C:\\Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\llamacpp_model.py\", line 49, in from_pretrained\n\
          \    self.model = Llama(**params)\n  File \"C:\\Users\\AmineDjeghri\\Downloads\\\
          oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          llama_cpp\\llama.py\", line 197, in __init__\n    assert self.ctx is not\
          \ None\nAssertionError\nException ignored in: &lt;function LlamaCppModel.__del__\
          \ at 0x0000018E453C2A70&gt;\nTraceback (most recent call last):\n  File\
          \ \"C:\\Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\llamacpp_model.py\", line 23, in __del__\n\
          \    self.model.__del__()\nAttributeError: 'LlamaCppModel' object has no\
          \ attribute 'model'\n</code></pre>\n"
        raw: "AttributeError: 'LlamaCppModel' object has no attribute 'model'\n\n\n\
          ```\nllama.cpp: loading model from models\\starcoder\\starcoder.ggmlv3.q4_1.bin\n\
          error loading model: missing tok_embeddings.weight\nllama_init_from_file:\
          \ failed to load model\nTraceback (most recent call last):\n  File \"C:\\\
          Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
          server.py\", line 1079, in <module>\n    shared.model, shared.tokenizer\
          \ = load_model(shared.model_name)\n  File \"C:\\Users\\AmineDjeghri\\Downloads\\\
          oobabooga_windows\\oobabooga_windows\\text-generation-webui\\modules\\models.py\"\
          , line 94, in load_model\n    output = load_func(model_name)\n  File \"\
          C:\\Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 271, in llamacpp_loader\n\
          \    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n  File\
          \ \"C:\\Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\llamacpp_model.py\", line 49, in from_pretrained\n\
          \    self.model = Llama(**params)\n  File \"C:\\Users\\AmineDjeghri\\Downloads\\\
          oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          llama_cpp\\llama.py\", line 197, in __init__\n    assert self.ctx is not\
          \ None\nAssertionError\nException ignored in: <function LlamaCppModel.__del__\
          \ at 0x0000018E453C2A70>\nTraceback (most recent call last):\n  File \"\
          C:\\Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\llamacpp_model.py\", line 23, in __del__\n\
          \    self.model.__del__()\nAttributeError: 'LlamaCppModel' object has no\
          \ attribute 'model'\n\n```"
        updatedAt: '2023-06-09T09:09:50.841Z'
      numEdits: 1
      reactions:
      - count: 4
        reaction: "\U0001F614"
        users:
        - dzupin
        - Kevinzyc
        - Vaktpost
        - florestankorp
      - count: 1
        reaction: "\U0001F44D"
        users:
        - florestankorp
    id: 6482eb415e72c4eee9c46abb
    type: comment
  author: aminedjeghri
  content: "AttributeError: 'LlamaCppModel' object has no attribute 'model'\n\n\n\
    ```\nllama.cpp: loading model from models\\starcoder\\starcoder.ggmlv3.q4_1.bin\n\
    error loading model: missing tok_embeddings.weight\nllama_init_from_file: failed\
    \ to load model\nTraceback (most recent call last):\n  File \"C:\\Users\\AmineDjeghri\\\
    Downloads\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\server.py\"\
    , line 1079, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"C:\\Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\", line 94, in load_model\n    output\
    \ = load_func(model_name)\n  File \"C:\\Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\\
    oobabooga_windows\\text-generation-webui\\modules\\models.py\", line 271, in llamacpp_loader\n\
    \    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n  File \"C:\\\
    Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
    modules\\llamacpp_model.py\", line 49, in from_pretrained\n    self.model = Llama(**params)\n\
    \  File \"C:\\Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\llama_cpp\\llama.py\", line 197, in\
    \ __init__\n    assert self.ctx is not None\nAssertionError\nException ignored\
    \ in: <function LlamaCppModel.__del__ at 0x0000018E453C2A70>\nTraceback (most\
    \ recent call last):\n  File \"C:\\Users\\AmineDjeghri\\Downloads\\oobabooga_windows\\\
    oobabooga_windows\\text-generation-webui\\modules\\llamacpp_model.py\", line 23,\
    \ in __del__\n    self.model.__del__()\nAttributeError: 'LlamaCppModel' object\
    \ has no attribute 'model'\n\n```"
  created_at: 2023-06-09 08:05:05+00:00
  edited: true
  hidden: false
  id: 6482eb415e72c4eee9c46abb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-09T09:06:01.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8714317679405212
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes. Please see the README for supported clients/libraries.  text-generation-ui
          can not load it at this time.</p>

          '
        raw: Yes. Please see the README for supported clients/libraries.  text-generation-ui
          can not load it at this time.
        updatedAt: '2023-06-09T09:06:01.677Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - aminedjeghri
        - florestankorp
    id: 6482eb7904f67f5f605896c5
    type: comment
  author: TheBloke
  content: Yes. Please see the README for supported clients/libraries.  text-generation-ui
    can not load it at this time.
  created_at: 2023-06-09 08:06:01+00:00
  edited: false
  hidden: false
  id: 6482eb7904f67f5f605896c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/378b502684b781cc061709f1052d047e.svg
      fullname: Amine Djeghri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aminedjeghri
      type: user
    createdAt: '2023-06-09T09:18:49.000Z'
    data:
      edited: false
      editors:
      - aminedjeghri
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.845034122467041
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/378b502684b781cc061709f1052d047e.svg
          fullname: Amine Djeghri
          isHf: false
          isPro: false
          name: aminedjeghri
          type: user
        html: '<p>Thanks !<br>from the readme : </p>

          <p>These files are not compatible with llama.cpp.<br>Currently they can
          be used with:</p>

          <ul>

          <li>KoboldCpp, a powerful inference engine based on llama.cpp, with good
          UI: KoboldCpp</li>

          <li>The ctransformers Python library, which includes LangChain support:
          ctransformers</li>

          <li>The GPT4All-UI which uses ctransformers: GPT4All-UI</li>

          <li>rustformers'' llm</li>

          </ul>

          '
        raw: "Thanks !\nfrom the readme : \n\nThese files are not compatible with\
          \ llama.cpp.\nCurrently they can be used with:\n- KoboldCpp, a powerful\
          \ inference engine based on llama.cpp, with good UI: KoboldCpp\n- The ctransformers\
          \ Python library, which includes LangChain support: ctransformers\n- The\
          \ GPT4All-UI which uses ctransformers: GPT4All-UI\n- rustformers' llm\n"
        updatedAt: '2023-06-09T09:18:49.151Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6482ee797e165f4007d3536d
    id: 6482ee797e165f4007d3536b
    type: comment
  author: aminedjeghri
  content: "Thanks !\nfrom the readme : \n\nThese files are not compatible with llama.cpp.\n\
    Currently they can be used with:\n- KoboldCpp, a powerful inference engine based\
    \ on llama.cpp, with good UI: KoboldCpp\n- The ctransformers Python library, which\
    \ includes LangChain support: ctransformers\n- The GPT4All-UI which uses ctransformers:\
    \ GPT4All-UI\n- rustformers' llm\n"
  created_at: 2023-06-09 08:18:49+00:00
  edited: false
  hidden: false
  id: 6482ee797e165f4007d3536b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/378b502684b781cc061709f1052d047e.svg
      fullname: Amine Djeghri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aminedjeghri
      type: user
    createdAt: '2023-06-09T09:18:49.000Z'
    data:
      status: closed
    id: 6482ee797e165f4007d3536d
    type: status-change
  author: aminedjeghri
  created_at: 2023-06-09 08:18:49+00:00
  id: 6482ee797e165f4007d3536d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-12-12T02:20:11.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9529721736907959
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>You say it works with KoboldCpp but it does not.. gives an error
          which someone pointed out and you then say you need to use <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp.git">https://github.com/ggerganov/llama.cpp.git</a>
          which is not even compiled for Windows.</p>

          '
        raw: You say it works with KoboldCpp but it does not.. gives an error which
          someone pointed out and you then say you need to use https://github.com/ggerganov/llama.cpp.git
          which is not even compiled for Windows.
        updatedAt: '2023-12-12T02:20:11.349Z'
      numEdits: 0
      reactions: []
    id: 6577c35b8628ec00e9d7dabc
    type: comment
  author: jdc4429
  content: You say it works with KoboldCpp but it does not.. gives an error which
    someone pointed out and you then say you need to use https://github.com/ggerganov/llama.cpp.git
    which is not even compiled for Windows.
  created_at: 2023-12-12 02:20:11+00:00
  edited: false
  hidden: false
  id: 6577c35b8628ec00e9d7dabc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/starcoder-GGML
repo_type: model
status: closed
target_branch: null
title: error when loading the model on text generation web-ui
