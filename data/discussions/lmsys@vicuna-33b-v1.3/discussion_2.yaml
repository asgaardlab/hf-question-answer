!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ruradium
conflicting_files: null
created_at: 2023-06-23 11:00:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3f0ee918665e92561ddeb895ba15f24.svg
      fullname: ray zhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ruradium
      type: user
    createdAt: '2023-06-23T12:00:38.000Z'
    data:
      edited: false
      editors:
      - ruradium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6310147643089294
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3f0ee918665e92561ddeb895ba15f24.svg
          fullname: ray zhong
          isHf: false
          isPro: false
          name: ruradium
          type: user
        html: '<p>I tried TheBloke/vicuna-33B-preview-GPTQ, but get error int the
          loading state</p>

          <p>showing Error(s) in loading state_dict for LlamaForCausalLM:<br>Missing
          key(s) in state_dict: "model.layers.0.self_attn.k_proj.g_idx" etc</p>

          '
        raw: "I tried TheBloke/vicuna-33B-preview-GPTQ, but get error int the loading\
          \ state\r\n\r\nshowing Error(s) in loading state_dict for LlamaForCausalLM:\r\
          \nMissing key(s) in state_dict: \"model.layers.0.self_attn.k_proj.g_idx\"\
          \ etc"
        updatedAt: '2023-06-23T12:00:38.605Z'
      numEdits: 0
      reactions: []
    id: 64958966acca217c1d2da7da
    type: comment
  author: ruradium
  content: "I tried TheBloke/vicuna-33B-preview-GPTQ, but get error int the loading\
    \ state\r\n\r\nshowing Error(s) in loading state_dict for LlamaForCausalLM:\r\n\
    Missing key(s) in state_dict: \"model.layers.0.self_attn.k_proj.g_idx\" etc"
  created_at: 2023-06-23 11:00:38+00:00
  edited: false
  hidden: false
  id: 64958966acca217c1d2da7da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-23T12:02:01.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8827787041664124
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>FastChat uses an older and, in my opinion, no-longer-recommended
          GPTQ implementation called GPTQ-for-LLaMa which requires manually setting
          GPTQ parameters. If it used AutoGPTQ instead, it would auto detect the parameters
          and it would work automatically.</p>

          <p>As I said to you on your post in my repo, you need to set group_size
          to None or -1.</p>

          '
        raw: 'FastChat uses an older and, in my opinion, no-longer-recommended GPTQ
          implementation called GPTQ-for-LLaMa which requires manually setting GPTQ
          parameters. If it used AutoGPTQ instead, it would auto detect the parameters
          and it would work automatically.


          As I said to you on your post in my repo, you need to set group_size to
          None or -1.'
        updatedAt: '2023-06-23T12:02:47.468Z'
      numEdits: 1
      reactions: []
    id: 649589b914d5904de38e3410
    type: comment
  author: TheBloke
  content: 'FastChat uses an older and, in my opinion, no-longer-recommended GPTQ
    implementation called GPTQ-for-LLaMa which requires manually setting GPTQ parameters.
    If it used AutoGPTQ instead, it would auto detect the parameters and it would
    work automatically.


    As I said to you on your post in my repo, you need to set group_size to None or
    -1.'
  created_at: 2023-06-23 11:02:01+00:00
  edited: true
  hidden: false
  id: 649589b914d5904de38e3410
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3f0ee918665e92561ddeb895ba15f24.svg
      fullname: ray zhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ruradium
      type: user
    createdAt: '2023-06-23T12:05:27.000Z'
    data:
      edited: false
      editors:
      - ruradium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8062338829040527
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3f0ee918665e92561ddeb895ba15f24.svg
          fullname: ray zhong
          isHf: false
          isPro: false
          name: ruradium
          type: user
        html: '<p>tried, but still the same error</p>

          '
        raw: tried, but still the same error
        updatedAt: '2023-06-23T12:05:27.061Z'
      numEdits: 0
      reactions: []
    id: 64958a872d3da5b869ff4a15
    type: comment
  author: ruradium
  content: tried, but still the same error
  created_at: 2023-06-23 11:05:27+00:00
  edited: false
  hidden: false
  id: 64958a872d3da5b869ff4a15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-23T12:46:13.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6352202296257019
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>OK I just tested it and yes I see the same. It is because recent\
          \ GPTQ-for-LLaMa branches have broken compatibility again.  This is why\
          \ I don't recommend using it.</p>\n<p>However you can get it to work if\
          \ you use the <code>old-cuda</code> branch of GPTQ-for-LLaMa. </p>\n<p>To\
          \ do that:</p>\n<pre><code>cd FastChat/repositories/GPTQ-for-LLaMa\npip3\
          \ uninstall -y quant-cuda\ngit switch old-cuda\npython3 setup_cuda.py install\n\
          </code></pre>\n<p>Then test again, without <code>--gptq-groupsize </code>\
          \ parameter:</p>\n<pre><code> python3 -m fastchat.serve.cli \\\n    --model-path\
          \ /workspace/process/vicuna-33b/gptq \\\n    --gptq-wbits 4\n</code></pre>\n\
          <p>Here is a log of me making that change and successfully using FastChat\
          \ with my Vicuna 33B Preview GPTQ:</p>\n<pre><code> [fastchat] ubuntu@h100:/workspace/git/FastChat\
          \ (main \u2718)\u272D \u1405 cd repositories/GPTQ-for-LLaMa\n [fastchat]\
          \ ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa (fastest-inference-4bit\
          \ \u2718)\u272D \u1405 ll\ntotal 132K\n-rw-rw-r-- 1 ubuntu ubuntu  12K Jun\
          \ 23 12:33 LICENSE.txt\n-rw-rw-r-- 1 ubuntu ubuntu 7.9K Jun 23 12:33 README.md\n\
          drwxrwxr-x 2 ubuntu ubuntu 4.0K Jun 23 12:35 __pycache__\ndrwxrwxr-x 5 ubuntu\
          \ ubuntu 4.0K Jun 23 12:34 build\n-rw-rw-r-- 1 ubuntu ubuntu 1.1K Jun 23\
          \ 12:33 convert_llama_weights_to_hf.py\ndrwxrwxr-x 2 ubuntu ubuntu 4.0K\
          \ Jun 23 12:34 dist\n-rw-rw-r-- 1 ubuntu ubuntu 7.6K Jun 23 12:33 gptq.py\n\
          -rw-rw-r-- 1 ubuntu ubuntu  20K Jun 23 12:33 llama.py\n-rw-rw-r-- 1 ubuntu\
          \ ubuntu  16K Jun 23 12:33 neox.py\n-rw-rw-r-- 1 ubuntu ubuntu  18K Jun\
          \ 23 12:33 opt.py\ndrwxrwxr-x 3 ubuntu ubuntu 4.0K Jun 23 12:35 quant\n\
          -rw-rw-r-- 1 ubuntu ubuntu 1.3K Jun 23 12:33 quant_cuda.cpp\ndrwxrwxr-x\
          \ 2 ubuntu ubuntu 4.0K Jun 23 12:33 quant_cuda.egg-info\n-rw-rw-r-- 1 ubuntu\
          \ ubuntu 7.9K Jun 23 12:33 quant_cuda_kernel.cu\n-rw-rw-r-- 1 ubuntu ubuntu\
          \  170 Jun 23 12:33 requirements.txt\n-rw-rw-r-- 1 ubuntu ubuntu  333 Jun\
          \ 23 12:33 setup_cuda.py\ndrwxrwxr-x 3 ubuntu ubuntu 4.0K Jun 23 12:35 utils\n\
          \ [fastchat] ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa\
          \ (fastest-inference-4bit \u2718)\u272D \u1405 pip3 uninstall quant-cuda\n\
          Found existing installation: quant-cuda 0.0.0\nUninstalling quant-cuda-0.0.0:\n\
          \  Would remove:\n    /workspace/venv/fastchat/lib/python3.10/site-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n\
          Proceed (Y/n)? Y\n  Successfully uninstalled quant-cuda-0.0.0\n [fastchat]\
          \ ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa (fastest-inference-4bit\
          \ \u2718)\u272D \u1405 git switch old-cuda\nBranch 'old-cuda' set up to\
          \ track remote branch 'old-cuda' from 'origin'.\nSwitched to a new branch\
          \ 'old-cuda'\n [fastchat] ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa\
          \ (old-cuda \u2718)\u272D \u1405 python3 setup_cuda.py install\nrunning\
          \ install\n/workspace/venv/fastchat/lib/python3.10/site-packages/setuptools/command/install.py:34:\
          \ SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build\
          \ and pip and other standards-based tools.\n  warnings.warn(\n/workspace/venv/fastchat/lib/python3.10/site-packages/setuptools/command/easy_install.py:144:\
          \ EasyInstallDeprecationWarning: easy_install command is deprecated. Use\
          \ build and pip and other standards-based tools.\n  warnings.warn(\nrunning\
          \ bdist_egg\nrunning egg_info\nwriting quant_cuda.egg-info/PKG-INFO\nwriting\
          \ dependency_links to quant_cuda.egg-info/dependency_links.txt\nwriting\
          \ top-level names to quant_cuda.egg-info/top_level.txt\nreading manifest\
          \ file 'quant_cuda.egg-info/SOURCES.txt'\nwriting manifest file 'quant_cuda.egg-info/SOURCES.txt'\n\
          installing library code to build/bdist.linux-x86_64/egg\nrunning install_lib\n\
          running build_ext\nbuilding 'quant_cuda' extension\nEmitting ninja build\
          \ file /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/build.ninja...\n\
          Compiling objects...\nAllowing ninja to set a default number of workers...\
          \ (overridable by setting the environment variable MAX_JOBS=N)\n[1/2] c++\
          \ -MMD -MF /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda.o.d\
          \ -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall\
          \ -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv\
          \ -O2 -fPIC -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include\
          \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\
          \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/TH\
          \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/THC\
          \ -I/usr/local/cuda/include -I/workspace/venv/fastchat/include -I/usr/include/python3.10\
          \ -c -c /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/quant_cuda.cpp\
          \ -o /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda.o\
          \ -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"\
          _libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1013\"' -DTORCH_EXTENSION_NAME=quant_cuda\
          \ -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n[2/2] /usr/local/cuda/bin/nvcc\
          \  -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include\
          \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\
          \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/TH\
          \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/THC\
          \ -I/usr/local/cuda/include -I/workspace/venv/fastchat/include -I/usr/include/python3.10\
          \ -c -c /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/quant_cuda_kernel.cu\
          \ -o /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda_kernel.o\
          \ -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__\
          \ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options\
          \ ''\"'\"'-fPIC'\"'\"'' -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"\
          _gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1013\"\
          ' -DTORCH_EXTENSION_NAME=quant_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_90,code=compute_90\
          \ -gencode=arch=compute_90,code=sm_90 -std=c++17\n\n... log trimmed here\
          \ ...\n\nx86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions\
          \ -Wl,-Bsymbolic-functions -g -fwrapv -O2 /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda.o\
          \ /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda_kernel.o\
          \ -L/workspace/venv/fastchat/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64\
          \ -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart\
          \ -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/quant_cuda.cpython-310-x86_64-linux-gnu.so\n\
          creating build/bdist.linux-x86_64/egg\ncopying build/lib.linux-x86_64-cpython-310/quant_cuda.cpython-310-x86_64-linux-gnu.so\
          \ -&gt; build/bdist.linux-x86_64/egg\ncreating stub loader for quant_cuda.cpython-310-x86_64-linux-gnu.so\n\
          byte-compiling build/bdist.linux-x86_64/egg/quant_cuda.py to quant_cuda.cpython-310.pyc\n\
          creating build/bdist.linux-x86_64/egg/EGG-INFO\ncopying quant_cuda.egg-info/PKG-INFO\
          \ -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\ncopying quant_cuda.egg-info/SOURCES.txt\
          \ -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\ncopying quant_cuda.egg-info/dependency_links.txt\
          \ -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\ncopying quant_cuda.egg-info/top_level.txt\
          \ -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\nwriting build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n\
          zip_safe flag not set; analyzing archive contents...\n__pycache__.quant_cuda.cpython-310:\
          \ module references __file__\ncreating 'dist/quant_cuda-0.0.0-py3.10-linux-x86_64.egg'\
          \ and adding 'build/bdist.linux-x86_64/egg' to it\nremoving 'build/bdist.linux-x86_64/egg'\
          \ (and everything under it)\nProcessing quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n\
          creating /workspace/venv/fastchat/lib/python3.10/site-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n\
          Extracting quant_cuda-0.0.0-py3.10-linux-x86_64.egg to /workspace/venv/fastchat/lib/python3.10/site-packages\n\
          Adding quant-cuda 0.0.0 to easy-install.pth file\n\nInstalled /workspace/venv/fastchat/lib/python3.10/site-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n\
          Processing dependencies for quant-cuda==0.0.0\nFinished processing dependencies\
          \ for quant-cuda==0.0.0\n [fastchat] ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa\
          \ (old-cuda \u2718)\u272D \u1405 cd ../..\n [fastchat] ubuntu@h100:/workspace/git/FastChat\
          \ (main \u2718)\u272D \u1405 python3 -m fastchat.serve.cli \\\n    --model-path\
          \ /workspace/process/vicuna-33b/gptq \\\n    --gptq-wbits 4\nLoading GPTQ\
          \ quantized model...\nLoading model ...\nDone.\nUSER: write a story about\
          \ llamas\nASSISTANT: Once upon a time, in the picturesque mountains of Peru,\
          \ there was a magical land called Llama Valley. This valley was home to\
          \ a diverse and vibrant community of llamas, each with their unique personalities\
          \ and abilities. The llamas lived in harmony, taking turns watching over\
          \ the valley and protecting it from any dangers that may emerge.\n\nThe\
          \ story revolves around a young llama named Llama League. Llama League was\
          \ a curious and adventurous soul, always eager to learn about the world\
          \ and the powers that existed within it. One day, Llama League was exploring\
          \ the mountainside when they stumbled upon a mysterious, ancient temple\
          \ hidden deep within the mountains. This temple was known as the Temple\
          \ of the Stars, and it was said to contain the secret to unlocking the true\
          \ power of the llama community.\n\nInside the temple, Llama League discovered\
          \ a vast chamber with a massive mosaic on the floor, depicting a constellation\
          \ of llamas surrounded by a celestial circle. In the center of the mosaic\
          \ stood a pedestal with a peculiar artifact: the Star Orb. The Star Orb\
          \ seemed to pulse with an ethereal energy, and Llama League could feel its\
          \ power resonating within them.\n\nAs Llama League touched the Star Orb,\
          \ they were suddenly overcome by a surge of ancient knowledge and mystical\
          \ abilities. The young llama discovered they could harness the power of\
          \ the stars, using it to protect their valley and help their fellow llamas.\
          \ With this newfound power, Llama League became the leader of the Llama\
          \ League, a group of talented llamas who swore to protect their home and\
          \ uphold the principles of unity and harmony.\n\nNot all was well in the\
          \ Llama Valley, however. An ancient evil, known as the Shadow Beast, had\
          \ been locked away for centuries, but it was now stirring once again. The\
          \ Shadow Beast sought to destroy the valley and its inhabitants, hoping\
          \ to plunge the world into darkness.\n\nArmed with their newfound abilities,\
          \ Llama League and their companions set out on an epic journey to gather\
          \ their fellow llamas and prepare them for the incoming threat. They traversed\
          \ the valley, meeting with the wisest elders and the strongest warriors,\
          \ each of\nUSER:\n</code></pre>\n"
        raw: "OK I just tested it and yes I see the same. It is because recent GPTQ-for-LLaMa\
          \ branches have broken compatibility again.  This is why I don't recommend\
          \ using it.\n\nHowever you can get it to work if you use the `old-cuda`\
          \ branch of GPTQ-for-LLaMa. \n\nTo do that:\n```\ncd FastChat/repositories/GPTQ-for-LLaMa\n\
          pip3 uninstall -y quant-cuda\ngit switch old-cuda\npython3 setup_cuda.py\
          \ install\n```\n\nThen test again, without `--gptq-groupsize ` parameter:\n\
          ```\n python3 -m fastchat.serve.cli \\\n    --model-path /workspace/process/vicuna-33b/gptq\
          \ \\\n    --gptq-wbits 4\n```\n\nHere is a log of me making that change\
          \ and successfully using FastChat with my Vicuna 33B Preview GPTQ:\n```\n\
          \ [fastchat] ubuntu@h100:/workspace/git/FastChat (main \u2718)\u272D \u1405\
          \ cd repositories/GPTQ-for-LLaMa\n [fastchat] ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa\
          \ (fastest-inference-4bit \u2718)\u272D \u1405 ll\ntotal 132K\n-rw-rw-r--\
          \ 1 ubuntu ubuntu  12K Jun 23 12:33 LICENSE.txt\n-rw-rw-r-- 1 ubuntu ubuntu\
          \ 7.9K Jun 23 12:33 README.md\ndrwxrwxr-x 2 ubuntu ubuntu 4.0K Jun 23 12:35\
          \ __pycache__\ndrwxrwxr-x 5 ubuntu ubuntu 4.0K Jun 23 12:34 build\n-rw-rw-r--\
          \ 1 ubuntu ubuntu 1.1K Jun 23 12:33 convert_llama_weights_to_hf.py\ndrwxrwxr-x\
          \ 2 ubuntu ubuntu 4.0K Jun 23 12:34 dist\n-rw-rw-r-- 1 ubuntu ubuntu 7.6K\
          \ Jun 23 12:33 gptq.py\n-rw-rw-r-- 1 ubuntu ubuntu  20K Jun 23 12:33 llama.py\n\
          -rw-rw-r-- 1 ubuntu ubuntu  16K Jun 23 12:33 neox.py\n-rw-rw-r-- 1 ubuntu\
          \ ubuntu  18K Jun 23 12:33 opt.py\ndrwxrwxr-x 3 ubuntu ubuntu 4.0K Jun 23\
          \ 12:35 quant\n-rw-rw-r-- 1 ubuntu ubuntu 1.3K Jun 23 12:33 quant_cuda.cpp\n\
          drwxrwxr-x 2 ubuntu ubuntu 4.0K Jun 23 12:33 quant_cuda.egg-info\n-rw-rw-r--\
          \ 1 ubuntu ubuntu 7.9K Jun 23 12:33 quant_cuda_kernel.cu\n-rw-rw-r-- 1 ubuntu\
          \ ubuntu  170 Jun 23 12:33 requirements.txt\n-rw-rw-r-- 1 ubuntu ubuntu\
          \  333 Jun 23 12:33 setup_cuda.py\ndrwxrwxr-x 3 ubuntu ubuntu 4.0K Jun 23\
          \ 12:35 utils\n [fastchat] ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa\
          \ (fastest-inference-4bit \u2718)\u272D \u1405 pip3 uninstall quant-cuda\n\
          Found existing installation: quant-cuda 0.0.0\nUninstalling quant-cuda-0.0.0:\n\
          \  Would remove:\n    /workspace/venv/fastchat/lib/python3.10/site-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n\
          Proceed (Y/n)? Y\n  Successfully uninstalled quant-cuda-0.0.0\n [fastchat]\
          \ ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa (fastest-inference-4bit\
          \ \u2718)\u272D \u1405 git switch old-cuda\nBranch 'old-cuda' set up to\
          \ track remote branch 'old-cuda' from 'origin'.\nSwitched to a new branch\
          \ 'old-cuda'\n [fastchat] ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa\
          \ (old-cuda \u2718)\u272D \u1405 python3 setup_cuda.py install\nrunning\
          \ install\n/workspace/venv/fastchat/lib/python3.10/site-packages/setuptools/command/install.py:34:\
          \ SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build\
          \ and pip and other standards-based tools.\n  warnings.warn(\n/workspace/venv/fastchat/lib/python3.10/site-packages/setuptools/command/easy_install.py:144:\
          \ EasyInstallDeprecationWarning: easy_install command is deprecated. Use\
          \ build and pip and other standards-based tools.\n  warnings.warn(\nrunning\
          \ bdist_egg\nrunning egg_info\nwriting quant_cuda.egg-info/PKG-INFO\nwriting\
          \ dependency_links to quant_cuda.egg-info/dependency_links.txt\nwriting\
          \ top-level names to quant_cuda.egg-info/top_level.txt\nreading manifest\
          \ file 'quant_cuda.egg-info/SOURCES.txt'\nwriting manifest file 'quant_cuda.egg-info/SOURCES.txt'\n\
          installing library code to build/bdist.linux-x86_64/egg\nrunning install_lib\n\
          running build_ext\nbuilding 'quant_cuda' extension\nEmitting ninja build\
          \ file /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/build.ninja...\n\
          Compiling objects...\nAllowing ninja to set a default number of workers...\
          \ (overridable by setting the environment variable MAX_JOBS=N)\n[1/2] c++\
          \ -MMD -MF /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda.o.d\
          \ -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall\
          \ -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv\
          \ -O2 -fPIC -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include\
          \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\
          \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/TH\
          \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/THC\
          \ -I/usr/local/cuda/include -I/workspace/venv/fastchat/include -I/usr/include/python3.10\
          \ -c -c /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/quant_cuda.cpp\
          \ -o /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda.o\
          \ -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"\
          _libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1013\"' -DTORCH_EXTENSION_NAME=quant_cuda\
          \ -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n[2/2] /usr/local/cuda/bin/nvcc\
          \  -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include\
          \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\
          \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/TH\
          \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/THC\
          \ -I/usr/local/cuda/include -I/workspace/venv/fastchat/include -I/usr/include/python3.10\
          \ -c -c /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/quant_cuda_kernel.cu\
          \ -o /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda_kernel.o\
          \ -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__\
          \ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options\
          \ ''\"'\"'-fPIC'\"'\"'' -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"\
          _gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1013\"\
          ' -DTORCH_EXTENSION_NAME=quant_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_90,code=compute_90\
          \ -gencode=arch=compute_90,code=sm_90 -std=c++17\n\n... log trimmed here\
          \ ...\n\nx86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions\
          \ -Wl,-Bsymbolic-functions -g -fwrapv -O2 /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda.o\
          \ /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda_kernel.o\
          \ -L/workspace/venv/fastchat/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64\
          \ -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart\
          \ -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/quant_cuda.cpython-310-x86_64-linux-gnu.so\n\
          creating build/bdist.linux-x86_64/egg\ncopying build/lib.linux-x86_64-cpython-310/quant_cuda.cpython-310-x86_64-linux-gnu.so\
          \ -> build/bdist.linux-x86_64/egg\ncreating stub loader for quant_cuda.cpython-310-x86_64-linux-gnu.so\n\
          byte-compiling build/bdist.linux-x86_64/egg/quant_cuda.py to quant_cuda.cpython-310.pyc\n\
          creating build/bdist.linux-x86_64/egg/EGG-INFO\ncopying quant_cuda.egg-info/PKG-INFO\
          \ -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying quant_cuda.egg-info/SOURCES.txt\
          \ -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying quant_cuda.egg-info/dependency_links.txt\
          \ -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying quant_cuda.egg-info/top_level.txt\
          \ -> build/bdist.linux-x86_64/egg/EGG-INFO\nwriting build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n\
          zip_safe flag not set; analyzing archive contents...\n__pycache__.quant_cuda.cpython-310:\
          \ module references __file__\ncreating 'dist/quant_cuda-0.0.0-py3.10-linux-x86_64.egg'\
          \ and adding 'build/bdist.linux-x86_64/egg' to it\nremoving 'build/bdist.linux-x86_64/egg'\
          \ (and everything under it)\nProcessing quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n\
          creating /workspace/venv/fastchat/lib/python3.10/site-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n\
          Extracting quant_cuda-0.0.0-py3.10-linux-x86_64.egg to /workspace/venv/fastchat/lib/python3.10/site-packages\n\
          Adding quant-cuda 0.0.0 to easy-install.pth file\n\nInstalled /workspace/venv/fastchat/lib/python3.10/site-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n\
          Processing dependencies for quant-cuda==0.0.0\nFinished processing dependencies\
          \ for quant-cuda==0.0.0\n [fastchat] ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa\
          \ (old-cuda \u2718)\u272D \u1405 cd ../..\n [fastchat] ubuntu@h100:/workspace/git/FastChat\
          \ (main \u2718)\u272D \u1405 python3 -m fastchat.serve.cli \\\n    --model-path\
          \ /workspace/process/vicuna-33b/gptq \\\n    --gptq-wbits 4\nLoading GPTQ\
          \ quantized model...\nLoading model ...\nDone.\nUSER: write a story about\
          \ llamas\nASSISTANT: Once upon a time, in the picturesque mountains of Peru,\
          \ there was a magical land called Llama Valley. This valley was home to\
          \ a diverse and vibrant community of llamas, each with their unique personalities\
          \ and abilities. The llamas lived in harmony, taking turns watching over\
          \ the valley and protecting it from any dangers that may emerge.\n\nThe\
          \ story revolves around a young llama named Llama League. Llama League was\
          \ a curious and adventurous soul, always eager to learn about the world\
          \ and the powers that existed within it. One day, Llama League was exploring\
          \ the mountainside when they stumbled upon a mysterious, ancient temple\
          \ hidden deep within the mountains. This temple was known as the Temple\
          \ of the Stars, and it was said to contain the secret to unlocking the true\
          \ power of the llama community.\n\nInside the temple, Llama League discovered\
          \ a vast chamber with a massive mosaic on the floor, depicting a constellation\
          \ of llamas surrounded by a celestial circle. In the center of the mosaic\
          \ stood a pedestal with a peculiar artifact: the Star Orb. The Star Orb\
          \ seemed to pulse with an ethereal energy, and Llama League could feel its\
          \ power resonating within them.\n\nAs Llama League touched the Star Orb,\
          \ they were suddenly overcome by a surge of ancient knowledge and mystical\
          \ abilities. The young llama discovered they could harness the power of\
          \ the stars, using it to protect their valley and help their fellow llamas.\
          \ With this newfound power, Llama League became the leader of the Llama\
          \ League, a group of talented llamas who swore to protect their home and\
          \ uphold the principles of unity and harmony.\n\nNot all was well in the\
          \ Llama Valley, however. An ancient evil, known as the Shadow Beast, had\
          \ been locked away for centuries, but it was now stirring once again. The\
          \ Shadow Beast sought to destroy the valley and its inhabitants, hoping\
          \ to plunge the world into darkness.\n\nArmed with their newfound abilities,\
          \ Llama League and their companions set out on an epic journey to gather\
          \ their fellow llamas and prepare them for the incoming threat. They traversed\
          \ the valley, meeting with the wisest elders and the strongest warriors,\
          \ each of\nUSER:\n```"
        updatedAt: '2023-06-23T12:46:13.867Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - kjerk
    id: 64959415010732e4861495f0
    type: comment
  author: TheBloke
  content: "OK I just tested it and yes I see the same. It is because recent GPTQ-for-LLaMa\
    \ branches have broken compatibility again.  This is why I don't recommend using\
    \ it.\n\nHowever you can get it to work if you use the `old-cuda` branch of GPTQ-for-LLaMa.\
    \ \n\nTo do that:\n```\ncd FastChat/repositories/GPTQ-for-LLaMa\npip3 uninstall\
    \ -y quant-cuda\ngit switch old-cuda\npython3 setup_cuda.py install\n```\n\nThen\
    \ test again, without `--gptq-groupsize ` parameter:\n```\n python3 -m fastchat.serve.cli\
    \ \\\n    --model-path /workspace/process/vicuna-33b/gptq \\\n    --gptq-wbits\
    \ 4\n```\n\nHere is a log of me making that change and successfully using FastChat\
    \ with my Vicuna 33B Preview GPTQ:\n```\n [fastchat] ubuntu@h100:/workspace/git/FastChat\
    \ (main \u2718)\u272D \u1405 cd repositories/GPTQ-for-LLaMa\n [fastchat] ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa\
    \ (fastest-inference-4bit \u2718)\u272D \u1405 ll\ntotal 132K\n-rw-rw-r-- 1 ubuntu\
    \ ubuntu  12K Jun 23 12:33 LICENSE.txt\n-rw-rw-r-- 1 ubuntu ubuntu 7.9K Jun 23\
    \ 12:33 README.md\ndrwxrwxr-x 2 ubuntu ubuntu 4.0K Jun 23 12:35 __pycache__\n\
    drwxrwxr-x 5 ubuntu ubuntu 4.0K Jun 23 12:34 build\n-rw-rw-r-- 1 ubuntu ubuntu\
    \ 1.1K Jun 23 12:33 convert_llama_weights_to_hf.py\ndrwxrwxr-x 2 ubuntu ubuntu\
    \ 4.0K Jun 23 12:34 dist\n-rw-rw-r-- 1 ubuntu ubuntu 7.6K Jun 23 12:33 gptq.py\n\
    -rw-rw-r-- 1 ubuntu ubuntu  20K Jun 23 12:33 llama.py\n-rw-rw-r-- 1 ubuntu ubuntu\
    \  16K Jun 23 12:33 neox.py\n-rw-rw-r-- 1 ubuntu ubuntu  18K Jun 23 12:33 opt.py\n\
    drwxrwxr-x 3 ubuntu ubuntu 4.0K Jun 23 12:35 quant\n-rw-rw-r-- 1 ubuntu ubuntu\
    \ 1.3K Jun 23 12:33 quant_cuda.cpp\ndrwxrwxr-x 2 ubuntu ubuntu 4.0K Jun 23 12:33\
    \ quant_cuda.egg-info\n-rw-rw-r-- 1 ubuntu ubuntu 7.9K Jun 23 12:33 quant_cuda_kernel.cu\n\
    -rw-rw-r-- 1 ubuntu ubuntu  170 Jun 23 12:33 requirements.txt\n-rw-rw-r-- 1 ubuntu\
    \ ubuntu  333 Jun 23 12:33 setup_cuda.py\ndrwxrwxr-x 3 ubuntu ubuntu 4.0K Jun\
    \ 23 12:35 utils\n [fastchat] ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa\
    \ (fastest-inference-4bit \u2718)\u272D \u1405 pip3 uninstall quant-cuda\nFound\
    \ existing installation: quant-cuda 0.0.0\nUninstalling quant-cuda-0.0.0:\n  Would\
    \ remove:\n    /workspace/venv/fastchat/lib/python3.10/site-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n\
    Proceed (Y/n)? Y\n  Successfully uninstalled quant-cuda-0.0.0\n [fastchat] ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa\
    \ (fastest-inference-4bit \u2718)\u272D \u1405 git switch old-cuda\nBranch 'old-cuda'\
    \ set up to track remote branch 'old-cuda' from 'origin'.\nSwitched to a new branch\
    \ 'old-cuda'\n [fastchat] ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa\
    \ (old-cuda \u2718)\u272D \u1405 python3 setup_cuda.py install\nrunning install\n\
    /workspace/venv/fastchat/lib/python3.10/site-packages/setuptools/command/install.py:34:\
    \ SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and\
    \ pip and other standards-based tools.\n  warnings.warn(\n/workspace/venv/fastchat/lib/python3.10/site-packages/setuptools/command/easy_install.py:144:\
    \ EasyInstallDeprecationWarning: easy_install command is deprecated. Use build\
    \ and pip and other standards-based tools.\n  warnings.warn(\nrunning bdist_egg\n\
    running egg_info\nwriting quant_cuda.egg-info/PKG-INFO\nwriting dependency_links\
    \ to quant_cuda.egg-info/dependency_links.txt\nwriting top-level names to quant_cuda.egg-info/top_level.txt\n\
    reading manifest file 'quant_cuda.egg-info/SOURCES.txt'\nwriting manifest file\
    \ 'quant_cuda.egg-info/SOURCES.txt'\ninstalling library code to build/bdist.linux-x86_64/egg\n\
    running install_lib\nrunning build_ext\nbuilding 'quant_cuda' extension\nEmitting\
    \ ninja build file /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/build.ninja...\n\
    Compiling objects...\nAllowing ninja to set a default number of workers... (overridable\
    \ by setting the environment variable MAX_JOBS=N)\n[1/2] c++ -MMD -MF /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda.o.d\
    \ -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g\
    \ -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC\
    \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\
    \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/TH -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/THC\
    \ -I/usr/local/cuda/include -I/workspace/venv/fastchat/include -I/usr/include/python3.10\
    \ -c -c /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/quant_cuda.cpp -o\
    \ /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda.o\
    \ -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"\
    _libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1013\"' -DTORCH_EXTENSION_NAME=quant_cuda\
    \ -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n[2/2] /usr/local/cuda/bin/nvcc  -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include\
    \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/torch/csrc/api/include\
    \ -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/TH -I/workspace/venv/fastchat/lib/python3.10/site-packages/torch/include/THC\
    \ -I/usr/local/cuda/include -I/workspace/venv/fastchat/include -I/usr/include/python3.10\
    \ -c -c /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/quant_cuda_kernel.cu\
    \ -o /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda_kernel.o\
    \ -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__\
    \ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"\
    '\"'-fPIC'\"'\"'' -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"\
    _gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1013\"\
    ' -DTORCH_EXTENSION_NAME=quant_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_90,code=compute_90\
    \ -gencode=arch=compute_90,code=sm_90 -std=c++17\n\n... log trimmed here ...\n\
    \nx86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions\
    \ -g -fwrapv -O2 /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda.o\
    \ /workspace/git/FastChat/repositories/GPTQ-for-LLaMa/build/temp.linux-x86_64-cpython-310/quant_cuda_kernel.o\
    \ -L/workspace/venv/fastchat/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64\
    \ -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart\
    \ -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/quant_cuda.cpython-310-x86_64-linux-gnu.so\n\
    creating build/bdist.linux-x86_64/egg\ncopying build/lib.linux-x86_64-cpython-310/quant_cuda.cpython-310-x86_64-linux-gnu.so\
    \ -> build/bdist.linux-x86_64/egg\ncreating stub loader for quant_cuda.cpython-310-x86_64-linux-gnu.so\n\
    byte-compiling build/bdist.linux-x86_64/egg/quant_cuda.py to quant_cuda.cpython-310.pyc\n\
    creating build/bdist.linux-x86_64/egg/EGG-INFO\ncopying quant_cuda.egg-info/PKG-INFO\
    \ -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying quant_cuda.egg-info/SOURCES.txt\
    \ -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying quant_cuda.egg-info/dependency_links.txt\
    \ -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying quant_cuda.egg-info/top_level.txt\
    \ -> build/bdist.linux-x86_64/egg/EGG-INFO\nwriting build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n\
    zip_safe flag not set; analyzing archive contents...\n__pycache__.quant_cuda.cpython-310:\
    \ module references __file__\ncreating 'dist/quant_cuda-0.0.0-py3.10-linux-x86_64.egg'\
    \ and adding 'build/bdist.linux-x86_64/egg' to it\nremoving 'build/bdist.linux-x86_64/egg'\
    \ (and everything under it)\nProcessing quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n\
    creating /workspace/venv/fastchat/lib/python3.10/site-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n\
    Extracting quant_cuda-0.0.0-py3.10-linux-x86_64.egg to /workspace/venv/fastchat/lib/python3.10/site-packages\n\
    Adding quant-cuda 0.0.0 to easy-install.pth file\n\nInstalled /workspace/venv/fastchat/lib/python3.10/site-packages/quant_cuda-0.0.0-py3.10-linux-x86_64.egg\n\
    Processing dependencies for quant-cuda==0.0.0\nFinished processing dependencies\
    \ for quant-cuda==0.0.0\n [fastchat] ubuntu@h100:/workspace/git/FastChat/repositories/GPTQ-for-LLaMa\
    \ (old-cuda \u2718)\u272D \u1405 cd ../..\n [fastchat] ubuntu@h100:/workspace/git/FastChat\
    \ (main \u2718)\u272D \u1405 python3 -m fastchat.serve.cli \\\n    --model-path\
    \ /workspace/process/vicuna-33b/gptq \\\n    --gptq-wbits 4\nLoading GPTQ quantized\
    \ model...\nLoading model ...\nDone.\nUSER: write a story about llamas\nASSISTANT:\
    \ Once upon a time, in the picturesque mountains of Peru, there was a magical\
    \ land called Llama Valley. This valley was home to a diverse and vibrant community\
    \ of llamas, each with their unique personalities and abilities. The llamas lived\
    \ in harmony, taking turns watching over the valley and protecting it from any\
    \ dangers that may emerge.\n\nThe story revolves around a young llama named Llama\
    \ League. Llama League was a curious and adventurous soul, always eager to learn\
    \ about the world and the powers that existed within it. One day, Llama League\
    \ was exploring the mountainside when they stumbled upon a mysterious, ancient\
    \ temple hidden deep within the mountains. This temple was known as the Temple\
    \ of the Stars, and it was said to contain the secret to unlocking the true power\
    \ of the llama community.\n\nInside the temple, Llama League discovered a vast\
    \ chamber with a massive mosaic on the floor, depicting a constellation of llamas\
    \ surrounded by a celestial circle. In the center of the mosaic stood a pedestal\
    \ with a peculiar artifact: the Star Orb. The Star Orb seemed to pulse with an\
    \ ethereal energy, and Llama League could feel its power resonating within them.\n\
    \nAs Llama League touched the Star Orb, they were suddenly overcome by a surge\
    \ of ancient knowledge and mystical abilities. The young llama discovered they\
    \ could harness the power of the stars, using it to protect their valley and help\
    \ their fellow llamas. With this newfound power, Llama League became the leader\
    \ of the Llama League, a group of talented llamas who swore to protect their home\
    \ and uphold the principles of unity and harmony.\n\nNot all was well in the Llama\
    \ Valley, however. An ancient evil, known as the Shadow Beast, had been locked\
    \ away for centuries, but it was now stirring once again. The Shadow Beast sought\
    \ to destroy the valley and its inhabitants, hoping to plunge the world into darkness.\n\
    \nArmed with their newfound abilities, Llama League and their companions set out\
    \ on an epic journey to gather their fellow llamas and prepare them for the incoming\
    \ threat. They traversed the valley, meeting with the wisest elders and the strongest\
    \ warriors, each of\nUSER:\n```"
  created_at: 2023-06-23 11:46:13+00:00
  edited: false
  hidden: false
  id: 64959415010732e4861495f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3f0ee918665e92561ddeb895ba15f24.svg
      fullname: ray zhong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ruradium
      type: user
    createdAt: '2023-06-23T13:21:21.000Z'
    data:
      edited: false
      editors:
      - ruradium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9247070550918579
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3f0ee918665e92561ddeb895ba15f24.svg
          fullname: ray zhong
          isHf: false
          isPro: false
          name: ruradium
          type: user
        html: '<p>Thanks, got them work, but extreamly slow on my side,  I can see
          that the model is loaded into GPU VRAM, but looks like it is inferencing
          in the CPU.<br>I also got such error message when loading:<br>2023-06-23
          21:18:57.809967: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable
          to register cuBLAS factory: Attempting to register factory for plugin cuBLAS
          when one has already been registered</p>

          '
        raw: 'Thanks, got them work, but extreamly slow on my side,  I can see that
          the model is loaded into GPU VRAM, but looks like it is inferencing in the
          CPU.

          I also got such error message when loading:

          2023-06-23 21:18:57.809967: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981]
          Unable to register cuBLAS factory: Attempting to register factory for plugin
          cuBLAS when one has already been registered'
        updatedAt: '2023-06-23T13:21:21.419Z'
      numEdits: 0
      reactions: []
    id: 64959c51a4428a51351b3cb5
    type: comment
  author: ruradium
  content: 'Thanks, got them work, but extreamly slow on my side,  I can see that
    the model is loaded into GPU VRAM, but looks like it is inferencing in the CPU.

    I also got such error message when loading:

    2023-06-23 21:18:57.809967: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981]
    Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS
    when one has already been registered'
  created_at: 2023-06-23 12:21:21+00:00
  edited: false
  hidden: false
  id: 64959c51a4428a51351b3cb5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
      fullname: Lianmin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lmzheng
      type: user
    createdAt: '2023-06-23T19:28:53.000Z'
    data:
      edited: false
      editors:
      - lmzheng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9042330384254456
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d35f3ceaf3858ce253ab7a/3yiiMriltXJBi242FZY-W.jpeg?w=200&h=200&f=face
          fullname: Lianmin
          isHf: false
          isPro: false
          name: lmzheng
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ruradium&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ruradium\">@<span class=\"\
          underline\">ruradium</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/TheBloke\">@<span class=\"underline\">TheBloke</span></a></span>\n\
          \n\t</span></span> Thanks for the discussion here. Please help us update\
          \ the gptq support in FastChat. It is a community-contributed feature that\
          \ we do not have the bandwidth to maintain.</p>\n"
        raw: '@ruradium @TheBloke Thanks for the discussion here. Please help us update
          the gptq support in FastChat. It is a community-contributed feature that
          we do not have the bandwidth to maintain.'
        updatedAt: '2023-06-23T19:28:53.579Z'
      numEdits: 0
      reactions: []
    id: 6495f2753d59ba7456be27b6
    type: comment
  author: lmzheng
  content: '@ruradium @TheBloke Thanks for the discussion here. Please help us update
    the gptq support in FastChat. It is a community-contributed feature that we do
    not have the bandwidth to maintain.'
  created_at: 2023-06-23 18:28:53+00:00
  edited: false
  hidden: false
  id: 6495f2753d59ba7456be27b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-23T19:55:03.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9258092045783997
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;lmzheng&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lmzheng\">@<span class=\"\
          underline\">lmzheng</span></a></span>\n\n\t</span></span> OK, I understand.\
          \ Sorry.  </p>\n<p>I'll have a look to see if I can PR an AutoGPTQ implementation.</p>\n"
        raw: "@lmzheng OK, I understand. Sorry.  \n\nI'll have a look to see if I\
          \ can PR an AutoGPTQ implementation."
        updatedAt: '2023-06-23T19:55:03.006Z'
      numEdits: 0
      reactions: []
    id: 6495f89795aca460d0723462
    type: comment
  author: TheBloke
  content: "@lmzheng OK, I understand. Sorry.  \n\nI'll have a look to see if I can\
    \ PR an AutoGPTQ implementation."
  created_at: 2023-06-23 18:55:03+00:00
  edited: false
  hidden: false
  id: 6495f89795aca460d0723462
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: lmsys/vicuna-33b-v1.3
repo_type: model
status: open
target_branch: null
title: Is there a 4bit quantize version for the FastChat?
