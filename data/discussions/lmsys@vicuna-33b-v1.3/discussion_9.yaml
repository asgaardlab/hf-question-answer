!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Shivam1410
conflicting_files: null
created_at: 2023-09-15 10:12:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/051619a9693793d2761dea810550301e.svg
      fullname: Shivam garg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shivam1410
      type: user
    createdAt: '2023-09-15T11:12:09.000Z'
    data:
      edited: false
      editors:
      - Shivam1410
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.31090351939201355
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/051619a9693793d2761dea810550301e.svg
          fullname: Shivam garg
          isHf: false
          isPro: false
          name: Shivam1410
          type: user
        html: '<hr>

          <p>UnexpectedStatusException                 Traceback (most recent call
          last)<br>Cell In[6], line 2<br>      1 # deploy model to SageMaker Inference<br>----&gt;
          2 predictor = huggingface_model.deploy(<br>      3     initial_instance_count=1,<br>      4     instance_type="ml.g5.2xlarge",<br>      5     container_startup_health_check_timeout=300,<br>      6   )</p>

          <p>File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/huggingface/model.py:313,
          in HuggingFaceModel.deploy(self, initial_instance_count, instance_type,
          serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key,
          wait, data_capture_config, async_inference_config, serverless_inference_config,
          volume_size, model_data_download_timeout, container_startup_health_check_timeout,
          inference_recommendation_id, explainer_config, **kwargs)<br>    306     inference_tool
          = "neuron" if instance_type.startswith("ml.inf1") else "neuronx"<br>    307     self.image_uri
          = self.serving_image_uri(<br>    308         region_name=self.sagemaker_session.boto_session.region_name,<br>    309         instance_type=instance_type,<br>    310         inference_tool=inference_tool,<br>    311     )<br>--&gt;
          313 return super(HuggingFaceModel, self).deploy(<br>    314     initial_instance_count,<br>    315     instance_type,<br>    316     serializer,<br>    317     deserializer,<br>    318     accelerator_type,<br>    319     endpoint_name,<br>    320     tags,<br>    321     kms_key,<br>    322     wait,<br>    323     data_capture_config,<br>    324     async_inference_config,<br>    325     serverless_inference_config,<br>    326     volume_size=volume_size,<br>    327     model_data_download_timeout=model_data_download_timeout,<br>    328     container_startup_health_check_timeout=container_startup_health_check_timeout,<br>    329     inference_recommendation_id=inference_recommendation_id,<br>    330     explainer_config=explainer_config,<br>    331
          )</p>

          <p>File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/model.py:1430,
          in Model.deploy(self, initial_instance_count, instance_type, serializer,
          deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config,
          async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout,
          container_startup_health_check_timeout, inference_recommendation_id, explainer_config,
          **kwargs)<br>   1427 if is_explainer_enabled:<br>   1428     explainer_config_dict
          = explainer_config._to_request_dict()<br>-&gt; 1430 self.sagemaker_session.endpoint_from_production_variants(<br>   1431     name=self.endpoint_name,<br>   1432     production_variants=[production_variant],<br>   1433     tags=tags,<br>   1434     kms_key=kms_key,<br>   1435     wait=wait,<br>   1436     data_capture_config_dict=data_capture_config_dict,<br>   1437     explainer_config_dict=explainer_config_dict,<br>   1438     async_inference_config_dict=async_inference_config_dict,<br>   1439
          )<br>   1441 if self.predictor_cls:<br>   1442     predictor = self.predictor_cls(self.endpoint_name,
          self.sagemaker_session)</p>

          <p>File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:4727,
          in Session.endpoint_from_production_variants(self, name, production_variants,
          tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict,
          explainer_config_dict)<br>   4724 LOGGER.info("Creating endpoint-config
          with name %s", name)<br>   4725 self.sagemaker_client.create_endpoint_config(**config_options)<br>-&gt;
          4727 return self.create_endpoint(<br>   4728     endpoint_name=name, config_name=name,
          tags=endpoint_tags, wait=wait<br>   4729 )</p>

          <p>File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:4072,
          in Session.create_endpoint(self, endpoint_name, config_name, tags, wait)<br>   4068
          self.sagemaker_client.create_endpoint(<br>   4069     EndpointName=endpoint_name,
          EndpointConfigName=config_name, Tags=tags<br>   4070 )<br>   4071 if wait:<br>-&gt;
          4072     self.wait_for_endpoint(endpoint_name)<br>   4073 return endpoint_name</p>

          <p>File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:4424,
          in Session.wait_for_endpoint(self, endpoint, poll)<br>   4418     if "CapacityError"
          in str(reason):<br>   4419         raise exceptions.CapacityError(<br>   4420             message=message,<br>   4421             allowed_statuses=["InService"],<br>   4422             actual_status=status,<br>   4423         )<br>-&gt;
          4424     raise exceptions.UnexpectedStatusException(<br>   4425         message=message,<br>   4426         allowed_statuses=["InService"],<br>   4427         actual_status=status,<br>   4428     )<br>   4429
          return desc</p>

          '
        raw: "---------------------------------------------------------------------------\r\
          \nUnexpectedStatusException                 Traceback (most recent call\
          \ last)\r\nCell In[6], line 2\r\n      1 # deploy model to SageMaker Inference\r\
          \n----> 2 predictor = huggingface_model.deploy(\r\n      3     initial_instance_count=1,\r\
          \n      4     instance_type=\"ml.g5.2xlarge\",\r\n      5     container_startup_health_check_timeout=300,\r\
          \n      6   )\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/huggingface/model.py:313,\
          \ in HuggingFaceModel.deploy(self, initial_instance_count, instance_type,\
          \ serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key,\
          \ wait, data_capture_config, async_inference_config, serverless_inference_config,\
          \ volume_size, model_data_download_timeout, container_startup_health_check_timeout,\
          \ inference_recommendation_id, explainer_config, **kwargs)\r\n    306  \
          \   inference_tool = \"neuron\" if instance_type.startswith(\"ml.inf1\"\
          ) else \"neuronx\"\r\n    307     self.image_uri = self.serving_image_uri(\r\
          \n    308         region_name=self.sagemaker_session.boto_session.region_name,\r\
          \n    309         instance_type=instance_type,\r\n    310         inference_tool=inference_tool,\r\
          \n    311     )\r\n--> 313 return super(HuggingFaceModel, self).deploy(\r\
          \n    314     initial_instance_count,\r\n    315     instance_type,\r\n\
          \    316     serializer,\r\n    317     deserializer,\r\n    318     accelerator_type,\r\
          \n    319     endpoint_name,\r\n    320     tags,\r\n    321     kms_key,\r\
          \n    322     wait,\r\n    323     data_capture_config,\r\n    324     async_inference_config,\r\
          \n    325     serverless_inference_config,\r\n    326     volume_size=volume_size,\r\
          \n    327     model_data_download_timeout=model_data_download_timeout,\r\
          \n    328     container_startup_health_check_timeout=container_startup_health_check_timeout,\r\
          \n    329     inference_recommendation_id=inference_recommendation_id,\r\
          \n    330     explainer_config=explainer_config,\r\n    331 )\r\n\r\nFile\
          \ ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/model.py:1430,\
          \ in Model.deploy(self, initial_instance_count, instance_type, serializer,\
          \ deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config,\
          \ async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout,\
          \ container_startup_health_check_timeout, inference_recommendation_id, explainer_config,\
          \ **kwargs)\r\n   1427 if is_explainer_enabled:\r\n   1428     explainer_config_dict\
          \ = explainer_config._to_request_dict()\r\n-> 1430 self.sagemaker_session.endpoint_from_production_variants(\r\
          \n   1431     name=self.endpoint_name,\r\n   1432     production_variants=[production_variant],\r\
          \n   1433     tags=tags,\r\n   1434     kms_key=kms_key,\r\n   1435    \
          \ wait=wait,\r\n   1436     data_capture_config_dict=data_capture_config_dict,\r\
          \n   1437     explainer_config_dict=explainer_config_dict,\r\n   1438  \
          \   async_inference_config_dict=async_inference_config_dict,\r\n   1439\
          \ )\r\n   1441 if self.predictor_cls:\r\n   1442     predictor = self.predictor_cls(self.endpoint_name,\
          \ self.sagemaker_session)\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:4727,\
          \ in Session.endpoint_from_production_variants(self, name, production_variants,\
          \ tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict,\
          \ explainer_config_dict)\r\n   4724 LOGGER.info(\"Creating endpoint-config\
          \ with name %s\", name)\r\n   4725 self.sagemaker_client.create_endpoint_config(**config_options)\r\
          \n-> 4727 return self.create_endpoint(\r\n   4728     endpoint_name=name,\
          \ config_name=name, tags=endpoint_tags, wait=wait\r\n   4729 )\r\n\r\nFile\
          \ ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:4072,\
          \ in Session.create_endpoint(self, endpoint_name, config_name, tags, wait)\r\
          \n   4068 self.sagemaker_client.create_endpoint(\r\n   4069     EndpointName=endpoint_name,\
          \ EndpointConfigName=config_name, Tags=tags\r\n   4070 )\r\n   4071 if wait:\r\
          \n-> 4072     self.wait_for_endpoint(endpoint_name)\r\n   4073 return endpoint_name\r\
          \n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:4424,\
          \ in Session.wait_for_endpoint(self, endpoint, poll)\r\n   4418     if \"\
          CapacityError\" in str(reason):\r\n   4419         raise exceptions.CapacityError(\r\
          \n   4420             message=message,\r\n   4421             allowed_statuses=[\"\
          InService\"],\r\n   4422             actual_status=status,\r\n   4423  \
          \       )\r\n-> 4424     raise exceptions.UnexpectedStatusException(\r\n\
          \   4425         message=message,\r\n   4426         allowed_statuses=[\"\
          InService\"],\r\n   4427         actual_status=status,\r\n   4428     )\r\
          \n   4429 return desc\r\n"
        updatedAt: '2023-09-15T11:12:09.317Z'
      numEdits: 0
      reactions: []
    id: 65043c090c120aa7315131a2
    type: comment
  author: Shivam1410
  content: "---------------------------------------------------------------------------\r\
    \nUnexpectedStatusException                 Traceback (most recent call last)\r\
    \nCell In[6], line 2\r\n      1 # deploy model to SageMaker Inference\r\n---->\
    \ 2 predictor = huggingface_model.deploy(\r\n      3     initial_instance_count=1,\r\
    \n      4     instance_type=\"ml.g5.2xlarge\",\r\n      5     container_startup_health_check_timeout=300,\r\
    \n      6   )\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/huggingface/model.py:313,\
    \ in HuggingFaceModel.deploy(self, initial_instance_count, instance_type, serializer,\
    \ deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config,\
    \ async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout,\
    \ container_startup_health_check_timeout, inference_recommendation_id, explainer_config,\
    \ **kwargs)\r\n    306     inference_tool = \"neuron\" if instance_type.startswith(\"\
    ml.inf1\") else \"neuronx\"\r\n    307     self.image_uri = self.serving_image_uri(\r\
    \n    308         region_name=self.sagemaker_session.boto_session.region_name,\r\
    \n    309         instance_type=instance_type,\r\n    310         inference_tool=inference_tool,\r\
    \n    311     )\r\n--> 313 return super(HuggingFaceModel, self).deploy(\r\n  \
    \  314     initial_instance_count,\r\n    315     instance_type,\r\n    316  \
    \   serializer,\r\n    317     deserializer,\r\n    318     accelerator_type,\r\
    \n    319     endpoint_name,\r\n    320     tags,\r\n    321     kms_key,\r\n\
    \    322     wait,\r\n    323     data_capture_config,\r\n    324     async_inference_config,\r\
    \n    325     serverless_inference_config,\r\n    326     volume_size=volume_size,\r\
    \n    327     model_data_download_timeout=model_data_download_timeout,\r\n   \
    \ 328     container_startup_health_check_timeout=container_startup_health_check_timeout,\r\
    \n    329     inference_recommendation_id=inference_recommendation_id,\r\n   \
    \ 330     explainer_config=explainer_config,\r\n    331 )\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/model.py:1430,\
    \ in Model.deploy(self, initial_instance_count, instance_type, serializer, deserializer,\
    \ accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config,\
    \ serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout,\
    \ inference_recommendation_id, explainer_config, **kwargs)\r\n   1427 if is_explainer_enabled:\r\
    \n   1428     explainer_config_dict = explainer_config._to_request_dict()\r\n\
    -> 1430 self.sagemaker_session.endpoint_from_production_variants(\r\n   1431 \
    \    name=self.endpoint_name,\r\n   1432     production_variants=[production_variant],\r\
    \n   1433     tags=tags,\r\n   1434     kms_key=kms_key,\r\n   1435     wait=wait,\r\
    \n   1436     data_capture_config_dict=data_capture_config_dict,\r\n   1437  \
    \   explainer_config_dict=explainer_config_dict,\r\n   1438     async_inference_config_dict=async_inference_config_dict,\r\
    \n   1439 )\r\n   1441 if self.predictor_cls:\r\n   1442     predictor = self.predictor_cls(self.endpoint_name,\
    \ self.sagemaker_session)\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:4727,\
    \ in Session.endpoint_from_production_variants(self, name, production_variants,\
    \ tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict,\
    \ explainer_config_dict)\r\n   4724 LOGGER.info(\"Creating endpoint-config with\
    \ name %s\", name)\r\n   4725 self.sagemaker_client.create_endpoint_config(**config_options)\r\
    \n-> 4727 return self.create_endpoint(\r\n   4728     endpoint_name=name, config_name=name,\
    \ tags=endpoint_tags, wait=wait\r\n   4729 )\r\n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:4072,\
    \ in Session.create_endpoint(self, endpoint_name, config_name, tags, wait)\r\n\
    \   4068 self.sagemaker_client.create_endpoint(\r\n   4069     EndpointName=endpoint_name,\
    \ EndpointConfigName=config_name, Tags=tags\r\n   4070 )\r\n   4071 if wait:\r\
    \n-> 4072     self.wait_for_endpoint(endpoint_name)\r\n   4073 return endpoint_name\r\
    \n\r\nFile ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:4424,\
    \ in Session.wait_for_endpoint(self, endpoint, poll)\r\n   4418     if \"CapacityError\"\
    \ in str(reason):\r\n   4419         raise exceptions.CapacityError(\r\n   4420\
    \             message=message,\r\n   4421             allowed_statuses=[\"InService\"\
    ],\r\n   4422             actual_status=status,\r\n   4423         )\r\n-> 4424\
    \     raise exceptions.UnexpectedStatusException(\r\n   4425         message=message,\r\
    \n   4426         allowed_statuses=[\"InService\"],\r\n   4427         actual_status=status,\r\
    \n   4428     )\r\n   4429 return desc\r\n"
  created_at: 2023-09-15 10:12:09+00:00
  edited: false
  hidden: false
  id: 65043c090c120aa7315131a2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: lmsys/vicuna-33b-v1.3
repo_type: model
status: open
target_branch: null
title: 'Failed. Reason: The primary container for production variant AllTraffic did
  not pass the ping health check'
