!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ericanthonymitchell
conflicting_files: null
created_at: 2023-03-05 05:38:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2354878397a97d81262ea00a89b175e3.svg
      fullname: Eric Anthony Mitchell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ericanthonymitchell
      type: user
    createdAt: '2023-03-05T05:38:45.000Z'
    data:
      edited: false
      editors:
      - ericanthonymitchell
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2354878397a97d81262ea00a89b175e3.svg
          fullname: Eric Anthony Mitchell
          isHf: false
          isPro: false
          name: ericanthonymitchell
          type: user
        html: "<p>I'm trying to implement a custom sampling loop for GPT-JT, because\
          \ I need some features not supported by <code>model.generate</code>. However,\
          \ I'm a bit confused about how the bidirectional attention mask is tracked.\
          \ <strong>Can someone point me to the code when GPT-JT bidirectional vs\
          \ causal masking is controlled?</strong></p>\n<p>In <a href=\"https://huggingface.co/togethercomputer/GPT-JT-6B-v1/discussions/1#63873d5388b39a64e1ea227b\"\
          >this answer</a>, <span data-props=\"{&quot;user&quot;:&quot;juewang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/juewang\"\
          >@<span class=\"underline\">juewang</span></a></span>\n\n\t</span></span>\
          \ mentions that the causal attention mask for GPT-JT is set to 1 by default.\
          \ However, loading GPT-JT with <code>transformers.AutoModelForCausalLM.from_pretrained</code>\
          \ just loads a normal GPT-J model, and the attention bias for GPT-J defaults\
          \ to causal attention, as far as I can tell from <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/blob/main/src/transformers/models/gptj/modeling_gptj.py#L93\"\
          >here</a>.</p>\n<p>Could someone explain what I'm missing? I'm confused\
          \ about how GPT-JT can implement custom attention masking, when there doesn't\
          \ seem to be any GPT-JT-specific code in HuggingFace (just relying on GPT-J).</p>\n\
          <p>Thanks!</p>\n"
        raw: "I'm trying to implement a custom sampling loop for GPT-JT, because I\
          \ need some features not supported by `model.generate`. However, I'm a bit\
          \ confused about how the bidirectional attention mask is tracked. **Can\
          \ someone point me to the code when GPT-JT bidirectional vs causal masking\
          \ is controlled?**\r\n\r\nIn [this answer](https://huggingface.co/togethercomputer/GPT-JT-6B-v1/discussions/1#63873d5388b39a64e1ea227b),\
          \ @juewang mentions that the causal attention mask for GPT-JT is set to\
          \ 1 by default. However, loading GPT-JT with `transformers.AutoModelForCausalLM.from_pretrained`\
          \ just loads a normal GPT-J model, and the attention bias for GPT-J defaults\
          \ to causal attention, as far as I can tell from [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gptj/modeling_gptj.py#L93).\r\
          \n\r\nCould someone explain what I'm missing? I'm confused about how GPT-JT\
          \ can implement custom attention masking, when there doesn't seem to be\
          \ any GPT-JT-specific code in HuggingFace (just relying on GPT-J).\r\n\r\
          \nThanks!"
        updatedAt: '2023-03-05T05:38:45.605Z'
      numEdits: 0
      reactions: []
    id: 64042ae5dbfbea2a05443a7b
    type: comment
  author: ericanthonymitchell
  content: "I'm trying to implement a custom sampling loop for GPT-JT, because I need\
    \ some features not supported by `model.generate`. However, I'm a bit confused\
    \ about how the bidirectional attention mask is tracked. **Can someone point me\
    \ to the code when GPT-JT bidirectional vs causal masking is controlled?**\r\n\
    \r\nIn [this answer](https://huggingface.co/togethercomputer/GPT-JT-6B-v1/discussions/1#63873d5388b39a64e1ea227b),\
    \ @juewang mentions that the causal attention mask for GPT-JT is set to 1 by default.\
    \ However, loading GPT-JT with `transformers.AutoModelForCausalLM.from_pretrained`\
    \ just loads a normal GPT-J model, and the attention bias for GPT-J defaults to\
    \ causal attention, as far as I can tell from [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gptj/modeling_gptj.py#L93).\r\
    \n\r\nCould someone explain what I'm missing? I'm confused about how GPT-JT can\
    \ implement custom attention masking, when there doesn't seem to be any GPT-JT-specific\
    \ code in HuggingFace (just relying on GPT-J).\r\n\r\nThanks!"
  created_at: 2023-03-05 05:38:45+00:00
  edited: false
  hidden: false
  id: 64042ae5dbfbea2a05443a7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2354878397a97d81262ea00a89b175e3.svg
      fullname: Eric Anthony Mitchell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ericanthonymitchell
      type: user
    createdAt: '2023-03-08T00:55:36.000Z'
    data:
      edited: false
      editors:
      - ericanthonymitchell
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2354878397a97d81262ea00a89b175e3.svg
          fullname: Eric Anthony Mitchell
          isHf: false
          isPro: false
          name: ericanthonymitchell
          type: user
        html: "<p>I was confused because I didn't realize that the attention_mask\
          \ is actually a PyTorch registered buffer, i.e., part of the weights checkpoint;\
          \ it's not controlled in code. The mask is in <code>model.transformer.h[i].attn.bias.data[:]</code>.</p>\n\
          <p>My simple sampling loop looks like this, for reference:</p>\n<pre><code>def\
          \ gptjt_sample(model, tokenizer, prompt_text, max_length=100, eos_token_id=None,\
          \ do_sample=False):\n    dev = list(model.parameters())[0].device\n    input_ids\
          \ = tokenizer(prompt_text, return_tensors='pt').input_ids.to(dev)\n    past_key_values\
          \ = None\n    output_ids = input_ids\n    for i in range(max_length):\n\
          \        possibly_only_last_token = output_ids[:, -1:] if past_key_values\
          \ is not None else output_ids\n        outputs = model(possibly_only_last_token,\
          \ use_cache=True, past_key_values=past_key_values, output_hidden_states=True)\n\
          \        past_key_values = outputs.past_key_values\n    \n        next_token_logits\
          \ = outputs.logits[:, -1, :]\n        if do_sample:\n            next_token\
          \ = torch.multinomial(torch.softmax(next_token_logits, dim=-1), num_samples=1)\n\
          \        else:\n            next_token = torch.argmax(next_token_logits,\
          \ dim=-1, keepdim=True)\n        output_ids = torch.cat([output_ids, next_token],\
          \ dim=-1)\n        if eos_token_id is not None and next_token == eos_token_id:\n\
          \            break\n    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\
          </code></pre>\n"
        raw: "I was confused because I didn't realize that the attention_mask is actually\
          \ a PyTorch registered buffer, i.e., part of the weights checkpoint; it's\
          \ not controlled in code. The mask is in `model.transformer.h[i].attn.bias.data[:]`.\n\
          \nMy simple sampling loop looks like this, for reference:\n\n    def gptjt_sample(model,\
          \ tokenizer, prompt_text, max_length=100, eos_token_id=None, do_sample=False):\n\
          \        dev = list(model.parameters())[0].device\n        input_ids = tokenizer(prompt_text,\
          \ return_tensors='pt').input_ids.to(dev)\n        past_key_values = None\n\
          \        output_ids = input_ids\n        for i in range(max_length):\n \
          \           possibly_only_last_token = output_ids[:, -1:] if past_key_values\
          \ is not None else output_ids\n            outputs = model(possibly_only_last_token,\
          \ use_cache=True, past_key_values=past_key_values, output_hidden_states=True)\n\
          \            past_key_values = outputs.past_key_values\n        \n     \
          \       next_token_logits = outputs.logits[:, -1, :]\n            if do_sample:\n\
          \                next_token = torch.multinomial(torch.softmax(next_token_logits,\
          \ dim=-1), num_samples=1)\n            else:\n                next_token\
          \ = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n            output_ids\
          \ = torch.cat([output_ids, next_token], dim=-1)\n            if eos_token_id\
          \ is not None and next_token == eos_token_id:\n                break\n \
          \       return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
        updatedAt: '2023-03-08T00:55:36.649Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6407dd087bfb92e5faf9a1ca
    id: 6407dd087bfb92e5faf9a1c9
    type: comment
  author: ericanthonymitchell
  content: "I was confused because I didn't realize that the attention_mask is actually\
    \ a PyTorch registered buffer, i.e., part of the weights checkpoint; it's not\
    \ controlled in code. The mask is in `model.transformer.h[i].attn.bias.data[:]`.\n\
    \nMy simple sampling loop looks like this, for reference:\n\n    def gptjt_sample(model,\
    \ tokenizer, prompt_text, max_length=100, eos_token_id=None, do_sample=False):\n\
    \        dev = list(model.parameters())[0].device\n        input_ids = tokenizer(prompt_text,\
    \ return_tensors='pt').input_ids.to(dev)\n        past_key_values = None\n   \
    \     output_ids = input_ids\n        for i in range(max_length):\n          \
    \  possibly_only_last_token = output_ids[:, -1:] if past_key_values is not None\
    \ else output_ids\n            outputs = model(possibly_only_last_token, use_cache=True,\
    \ past_key_values=past_key_values, output_hidden_states=True)\n            past_key_values\
    \ = outputs.past_key_values\n        \n            next_token_logits = outputs.logits[:,\
    \ -1, :]\n            if do_sample:\n                next_token = torch.multinomial(torch.softmax(next_token_logits,\
    \ dim=-1), num_samples=1)\n            else:\n                next_token = torch.argmax(next_token_logits,\
    \ dim=-1, keepdim=True)\n            output_ids = torch.cat([output_ids, next_token],\
    \ dim=-1)\n            if eos_token_id is not None and next_token == eos_token_id:\n\
    \                break\n        return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
  created_at: 2023-03-08 00:55:36+00:00
  edited: false
  hidden: false
  id: 6407dd087bfb92e5faf9a1c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/2354878397a97d81262ea00a89b175e3.svg
      fullname: Eric Anthony Mitchell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ericanthonymitchell
      type: user
    createdAt: '2023-03-08T00:55:36.000Z'
    data:
      status: closed
    id: 6407dd087bfb92e5faf9a1ca
    type: status-change
  author: ericanthonymitchell
  created_at: 2023-03-08 00:55:36+00:00
  id: 6407dd087bfb92e5faf9a1ca
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-03-10T06:46:09.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: '<p>Yeah, you are right, <code>attention_mask</code> is a registered
          buffer and will be overwritten after loading the ckpt.</p>

          '
        raw: Yeah, you are right, `attention_mask` is a registered buffer and will
          be overwritten after loading the ckpt.
        updatedAt: '2023-03-10T06:46:09.032Z'
      numEdits: 0
      reactions: []
    id: 640ad23151028b5b07be08b6
    type: comment
  author: juewang
  content: Yeah, you are right, `attention_mask` is a registered buffer and will be
    overwritten after loading the ckpt.
  created_at: 2023-03-10 06:46:09+00:00
  edited: false
  hidden: false
  id: 640ad23151028b5b07be08b6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: togethercomputer/GPT-JT-6B-v1
repo_type: model
status: closed
target_branch: null
title: Confused about bidirectional attention when implementing custom sampling loop
