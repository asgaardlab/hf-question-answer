!!python/object:huggingface_hub.community.DiscussionWithDetails
author: idop11
conflicting_files: null
created_at: 2023-02-03 18:44:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76ddda68ee8359da998885b42e06058f.svg
      fullname: Ido Pesok
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: idop11
      type: user
    createdAt: '2023-02-03T18:44:55.000Z'
    data:
      edited: false
      editors:
      - idop11
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76ddda68ee8359da998885b42e06058f.svg
          fullname: Ido Pesok
          isHf: false
          isPro: false
          name: idop11
          type: user
        html: '<p>Hi,</p>

          <p>Great work on this model. Initial results are very impressive. Is there
          any chance to be able to download the full weights of the model (70GB) to
          be able to run fine tuning on using a TPU? Want to fine tune GPT JT on custom
          prompt dataset.</p>

          <p>Looking to run fine tuning following this guide <a rel="nofollow" href="https://github.com/kingoflolz/mesh-transformer-jax">https://github.com/kingoflolz/mesh-transformer-jax</a></p>

          <p>Thanks!</p>

          '
        raw: "Hi,\r\n\r\nGreat work on this model. Initial results are very impressive.\
          \ Is there any chance to be able to download the full weights of the model\
          \ (70GB) to be able to run fine tuning on using a TPU? Want to fine tune\
          \ GPT JT on custom prompt dataset.\r\n\r\nLooking to run fine tuning following\
          \ this guide https://github.com/kingoflolz/mesh-transformer-jax\r\n\r\n\
          Thanks!"
        updatedAt: '2023-02-03T18:44:55.073Z'
      numEdits: 0
      reactions: []
    id: 63dd5627422ca8d7f7e20c66
    type: comment
  author: idop11
  content: "Hi,\r\n\r\nGreat work on this model. Initial results are very impressive.\
    \ Is there any chance to be able to download the full weights of the model (70GB)\
    \ to be able to run fine tuning on using a TPU? Want to fine tune GPT JT on custom\
    \ prompt dataset.\r\n\r\nLooking to run fine tuning following this guide https://github.com/kingoflolz/mesh-transformer-jax\r\
    \n\r\nThanks!"
  created_at: 2023-02-03 18:44:55+00:00
  edited: false
  hidden: false
  id: 63dd5627422ca8d7f7e20c66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37f8541bb895fdf4742031e9632a0d32.svg
      fullname: C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kobalsky
      type: user
    createdAt: '2023-02-05T03:28:53.000Z'
    data:
      edited: true
      editors:
      - kobalsky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37f8541bb895fdf4742031e9632a0d32.svg
          fullname: C
          isHf: false
          isPro: false
          name: kobalsky
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;idop11&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/idop11\">@<span class=\"\
          underline\">idop11</span></a></span>\n\n\t</span></span> apparently the\
          \ model cannot be fine-tuned <a href=\"https://huggingface.co/togethercomputer/GPT-JT-6B-v1/discussions/15\"\
          >https://huggingface.co/togethercomputer/GPT-JT-6B-v1/discussions/15</a>\
          \ at this time</p>\n"
        raw: '@idop11 apparently the model cannot be fine-tuned https://huggingface.co/togethercomputer/GPT-JT-6B-v1/discussions/15
          at this time'
        updatedAt: '2023-02-05T03:29:14.949Z'
      numEdits: 1
      reactions: []
    id: 63df22756ddb7f3a88d1d55f
    type: comment
  author: kobalsky
  content: '@idop11 apparently the model cannot be fine-tuned https://huggingface.co/togethercomputer/GPT-JT-6B-v1/discussions/15
    at this time'
  created_at: 2023-02-05 03:28:53+00:00
  edited: true
  hidden: false
  id: 63df22756ddb7f3a88d1d55f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-02-05T05:39:17.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;idop11&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/idop11\">@<span class=\"\
          underline\">idop11</span></a></span>\n\n\t</span></span> Thanks for your\
          \ interest in fine-tuning our model! Unfortunately, our model was not trained\
          \ using <code>mesh-transformer-jax</code>, and the format of full weights\
          \ (including optimizer states) might not be compatible with their code base.<br><span\
          \ data-props=\"{&quot;user&quot;:&quot;kobalsky&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/kobalsky\">@<span class=\"\
          underline\">kobalsky</span></a></span>\n\n\t</span></span> The model can\
          \ be fine-tuned, but necessary adjustments are required, check out <a href=\"\
          https://huggingface.co/togethercomputer/GPT-JT-6B-v1/discussions/22#63df3cf7fb0ee94683f37fd4\"\
          >this</a>~</p>\n"
        raw: '@idop11 Thanks for your interest in fine-tuning our model! Unfortunately,
          our model was not trained using `mesh-transformer-jax`, and the format of
          full weights (including optimizer states) might not be compatible with their
          code base.

          @kobalsky The model can be fine-tuned, but necessary adjustments are required,
          check out [this](https://huggingface.co/togethercomputer/GPT-JT-6B-v1/discussions/22#63df3cf7fb0ee94683f37fd4)~'
        updatedAt: '2023-02-05T05:39:17.559Z'
      numEdits: 0
      reactions: []
    id: 63df4105edf4471493a5de0a
    type: comment
  author: juewang
  content: '@idop11 Thanks for your interest in fine-tuning our model! Unfortunately,
    our model was not trained using `mesh-transformer-jax`, and the format of full
    weights (including optimizer states) might not be compatible with their code base.

    @kobalsky The model can be fine-tuned, but necessary adjustments are required,
    check out [this](https://huggingface.co/togethercomputer/GPT-JT-6B-v1/discussions/22#63df3cf7fb0ee94683f37fd4)~'
  created_at: 2023-02-05 05:39:17+00:00
  edited: false
  hidden: false
  id: 63df4105edf4471493a5de0a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: togethercomputer/GPT-JT-6B-v1
repo_type: model
status: open
target_branch: null
title: Fine Tuning // Download Full Weights
