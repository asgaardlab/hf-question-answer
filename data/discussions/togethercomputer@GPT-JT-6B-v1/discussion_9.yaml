!!python/object:huggingface_hub.community.DiscussionWithDetails
author: spartanml
conflicting_files: null
created_at: 2022-12-05 17:47:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/deaeb1e80603518efdb450b059f530a3.svg
      fullname: Spar Tan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spartanml
      type: user
    createdAt: '2022-12-05T17:47:44.000Z'
    data:
      edited: false
      editors:
      - spartanml
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/deaeb1e80603518efdb450b059f530a3.svg
          fullname: Spar Tan
          isHf: false
          isPro: false
          name: spartanml
          type: user
        html: '<p>Where can I find the hardware requirements for this model? (Specifically,
          can it run on 3060/12GB)?</p>

          '
        raw: Where can I find the hardware requirements for this model? (Specifically,
          can it run on 3060/12GB)?
        updatedAt: '2022-12-05T17:47:44.915Z'
      numEdits: 0
      reactions: []
    id: 638e2ec03885c7babdbcbfb0
    type: comment
  author: spartanml
  content: Where can I find the hardware requirements for this model? (Specifically,
    can it run on 3060/12GB)?
  created_at: 2022-12-05 17:47:44+00:00
  edited: false
  hidden: false
  id: 638e2ec03885c7babdbcbfb0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2022-12-07T03:47:51.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p>Theoretically, GPT-JT cannot run on one single 3060 12GB as the\
          \ model itself takes up ~12GB and thus so there is not enough memory for\
          \ inference. I'll recommend VRAM &gt;= 16GB. An alternative is to use multiple\
          \ 3060 GPUs with <code>accelerate</code>:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, AutoModelForCausalLM\n<span class=\"hljs-keyword\"\
          >from</span> accelerate <span class=\"hljs-keyword\">import</span> dispatch_model,\
          \ infer_auto_device_map\n<span class=\"hljs-keyword\">from</span> accelerate.utils\
          \ <span class=\"hljs-keyword\">import</span> get_balanced_memory\n\n<span\
          \ class=\"hljs-comment\"># Load model to CPU</span>\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"togethercomputer/GPT-JT-6B-v1\"</span>)\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(<span class=\"hljs-string\">\"\
          togethercomputer/GPT-JT-6B-v1\"</span>)\n\nmax_memory = get_balanced_memory(\n\
          \    model,\n    max_memory=<span class=\"hljs-literal\">None</span>,\n\
          \    no_split_module_classes=[<span class=\"hljs-string\">\"GPTJBlock\"\
          </span>],\n    dtype=<span class=\"hljs-string\">'float16'</span>,\n   \
          \ low_zero=<span class=\"hljs-literal\">False</span>,\n)\n\ndevice_map =\
          \ infer_auto_device_map(\n    model, \n    max_memory=max_memory,\n    no_split_module_classes=[<span\
          \ class=\"hljs-string\">\"GPTJBlock\"</span>], \n    dtype=<span class=\"\
          hljs-string\">'float16'</span>\n)\n\nmodel = dispatch_model(model, device_map=device_map)\n\
          </code></pre>\n"
        raw: "Theoretically, GPT-JT cannot run on one single 3060 12GB as the model\
          \ itself takes up ~12GB and thus so there is not enough memory for inference.\
          \ I'll recommend VRAM >= 16GB. An alternative is to use multiple 3060 GPUs\
          \ with `accelerate`:\n\n```python\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\nfrom accelerate import dispatch_model, infer_auto_device_map\n\
          from accelerate.utils import get_balanced_memory\n\n# Load model to CPU\n\
          tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
          )\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
          )\n\nmax_memory = get_balanced_memory(\n    model,\n    max_memory=None,\n\
          \    no_split_module_classes=[\"GPTJBlock\"],\n    dtype='float16',\n  \
          \  low_zero=False,\n)\n\ndevice_map = infer_auto_device_map(\n    model,\
          \ \n    max_memory=max_memory,\n    no_split_module_classes=[\"GPTJBlock\"\
          ], \n    dtype='float16'\n)\n\nmodel = dispatch_model(model, device_map=device_map)\n\
          ```"
        updatedAt: '2022-12-07T03:47:51.298Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - spartanml
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ehalit
    id: 63900ce7888447611c245c11
    type: comment
  author: juewang
  content: "Theoretically, GPT-JT cannot run on one single 3060 12GB as the model\
    \ itself takes up ~12GB and thus so there is not enough memory for inference.\
    \ I'll recommend VRAM >= 16GB. An alternative is to use multiple 3060 GPUs with\
    \ `accelerate`:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    from accelerate import dispatch_model, infer_auto_device_map\nfrom accelerate.utils\
    \ import get_balanced_memory\n\n# Load model to CPU\ntokenizer = AutoTokenizer.from_pretrained(\"\
    togethercomputer/GPT-JT-6B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    togethercomputer/GPT-JT-6B-v1\")\n\nmax_memory = get_balanced_memory(\n    model,\n\
    \    max_memory=None,\n    no_split_module_classes=[\"GPTJBlock\"],\n    dtype='float16',\n\
    \    low_zero=False,\n)\n\ndevice_map = infer_auto_device_map(\n    model, \n\
    \    max_memory=max_memory,\n    no_split_module_classes=[\"GPTJBlock\"], \n \
    \   dtype='float16'\n)\n\nmodel = dispatch_model(model, device_map=device_map)\n\
    ```"
  created_at: 2022-12-07 03:47:51+00:00
  edited: false
  hidden: false
  id: 63900ce7888447611c245c11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7336691cb4560dd9a9150845523e6030.svg
      fullname: Guillermo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: billy-ai
      type: user
    createdAt: '2022-12-07T16:31:15.000Z'
    data:
      edited: false
      editors:
      - billy-ai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7336691cb4560dd9a9150845523e6030.svg
          fullname: Guillermo
          isHf: false
          isPro: false
          name: billy-ai
          type: user
        html: '<p>I''m using this code and inference still takes ~12 seconds. I use
          <code>NVIDIA T4 x 2</code>.  For inference I use the command <code>model.generate</code>,
          do you know if I need to do anything else to make it use GPU?</p>

          <p>Do you have a code snippet with an inference example, which uses GPU?
          :) That would be awesome.</p>

          <p>Thanks for the good work!</p>

          '
        raw: 'I''m using this code and inference still takes ~12 seconds. I use `NVIDIA
          T4 x 2`.  For inference I use the command `model.generate`, do you know
          if I need to do anything else to make it use GPU?


          Do you have a code snippet with an inference example, which uses GPU? :)
          That would be awesome.


          Thanks for the good work!'
        updatedAt: '2022-12-07T16:31:15.842Z'
      numEdits: 0
      reactions: []
    id: 6390bfd3d00f25601f43914c
    type: comment
  author: billy-ai
  content: 'I''m using this code and inference still takes ~12 seconds. I use `NVIDIA
    T4 x 2`.  For inference I use the command `model.generate`, do you know if I need
    to do anything else to make it use GPU?


    Do you have a code snippet with an inference example, which uses GPU? :) That
    would be awesome.


    Thanks for the good work!'
  created_at: 2022-12-07 16:31:15+00:00
  edited: false
  hidden: false
  id: 6390bfd3d00f25601f43914c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2022-12-09T14:05:55.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;billy-ai&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/billy-ai\">@<span class=\"\
          underline\">billy-ai</span></a></span>\n\n\t</span></span> Sorry for the\
          \ late reply. If you use this code, the inference should run on GPU.<br>--\
          \ How many tokens were you trying to generate? It's possible to be slow\
          \ if <code>max_new_tokens</code> is large.</p>\n<p>If you use T4 with 16GB\
          \ VRAM, simply moving the model to GPU<code>model = model.half().to('cuda:0')</code>\
          \ and calling <code>output = model.generate(input_ids, max_new_tokens=10)</code>\
          \ are enough to GPU.</p>\n"
        raw: '@billy-ai Sorry for the late reply. If you use this code, the inference
          should run on GPU.

          -- How many tokens were you trying to generate? It''s possible to be slow
          if `max_new_tokens` is large.


          If you use T4 with 16GB VRAM, simply moving the model to GPU`model = model.half().to(''cuda:0'')`
          and calling `output = model.generate(input_ids, max_new_tokens=10)` are
          enough to GPU.'
        updatedAt: '2022-12-09T14:05:55.740Z'
      numEdits: 0
      reactions: []
    id: 639340c320110aaf13e81ab4
    type: comment
  author: juewang
  content: '@billy-ai Sorry for the late reply. If you use this code, the inference
    should run on GPU.

    -- How many tokens were you trying to generate? It''s possible to be slow if `max_new_tokens`
    is large.


    If you use T4 with 16GB VRAM, simply moving the model to GPU`model = model.half().to(''cuda:0'')`
    and calling `output = model.generate(input_ids, max_new_tokens=10)` are enough
    to GPU.'
  created_at: 2022-12-09 14:05:55+00:00
  edited: false
  hidden: false
  id: 639340c320110aaf13e81ab4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49dcdc289570c4f1e8d81d7d3f6ab736.svg
      fullname: Stoic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ascendant
      type: user
    createdAt: '2023-01-23T10:59:15.000Z'
    data:
      edited: false
      editors:
      - Ascendant
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49dcdc289570c4f1e8d81d7d3f6ab736.svg
          fullname: Stoic
          isHf: false
          isPro: false
          name: Ascendant
          type: user
        html: '<p>If I only have a 3070 with only 8 VRAM but has a lot of regular
          RAM (46) can I get away with running it on the CPU instead,  don''t mind
          if it''s much slower?</p>

          '
        raw: If I only have a 3070 with only 8 VRAM but has a lot of regular RAM (46)
          can I get away with running it on the CPU instead,  don't mind if it's much
          slower?
        updatedAt: '2023-01-23T10:59:15.754Z'
      numEdits: 0
      reactions: []
    id: 63ce68831378c94f84bd905a
    type: comment
  author: Ascendant
  content: If I only have a 3070 with only 8 VRAM but has a lot of regular RAM (46)
    can I get away with running it on the CPU instead,  don't mind if it's much slower?
  created_at: 2023-01-23 10:59:15+00:00
  edited: false
  hidden: false
  id: 63ce68831378c94f84bd905a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-01-26T17:22:36.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: '<blockquote>

          <p>If I only have a 3070 with only 8 VRAM but has a lot of regular RAM (46)
          can I get away with running it on the CPU instead,  don''t mind if it''s
          much slower?</p>

          </blockquote>

          <p>Sure, you can run it on CPU without any problem. You can also try quantization:
          <code>model = AutoModelForCausalLM.from_pretrained(''togethercomputer/GPT-JT-6B-v1'',
          device_map=''auto'', load_in_8bit=True, int8_threshold=6.0)</code> :)</p>

          '
        raw: '> If I only have a 3070 with only 8 VRAM but has a lot of regular RAM
          (46) can I get away with running it on the CPU instead,  don''t mind if
          it''s much slower?


          Sure, you can run it on CPU without any problem. You can also try quantization:
          `model = AutoModelForCausalLM.from_pretrained(''togethercomputer/GPT-JT-6B-v1'',
          device_map=''auto'', load_in_8bit=True, int8_threshold=6.0)` :)'
        updatedAt: '2023-01-26T17:22:36.495Z'
      numEdits: 0
      reactions: []
    id: 63d2b6dc13d86bc148045578
    type: comment
  author: juewang
  content: '> If I only have a 3070 with only 8 VRAM but has a lot of regular RAM
    (46) can I get away with running it on the CPU instead,  don''t mind if it''s
    much slower?


    Sure, you can run it on CPU without any problem. You can also try quantization:
    `model = AutoModelForCausalLM.from_pretrained(''togethercomputer/GPT-JT-6B-v1'',
    device_map=''auto'', load_in_8bit=True, int8_threshold=6.0)` :)'
  created_at: 2023-01-26 17:22:36+00:00
  edited: false
  hidden: false
  id: 63d2b6dc13d86bc148045578
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/deaeb1e80603518efdb450b059f530a3.svg
      fullname: Spar Tan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spartanml
      type: user
    createdAt: '2023-02-04T07:41:46.000Z'
    data:
      edited: false
      editors:
      - spartanml
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/deaeb1e80603518efdb450b059f530a3.svg
          fullname: Spar Tan
          isHf: false
          isPro: false
          name: spartanml
          type: user
        html: "<blockquote>\n<p>Theoretically, GPT-JT cannot run on one single 3060\
          \ 12GB as the model itself takes up ~12GB and thus so there is not enough\
          \ memory for inference. I'll recommend VRAM &gt;= 16GB. An alternative is\
          \ to use multiple 3060 GPUs with <code>accelerate</code>:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer, AutoModelForCausalLM\n\
          <span class=\"hljs-keyword\">from</span> accelerate <span class=\"hljs-keyword\"\
          >import</span> dispatch_model, infer_auto_device_map\n<span class=\"hljs-keyword\"\
          >from</span> accelerate.utils <span class=\"hljs-keyword\">import</span>\
          \ get_balanced_memory\n\n<span class=\"hljs-comment\"># Load model to CPU</span>\n\
          tokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">\"\
          togethercomputer/GPT-JT-6B-v1\"</span>)\nmodel = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"togethercomputer/GPT-JT-6B-v1\"</span>)\n\nmax_memory\
          \ = get_balanced_memory(\n    model,\n    max_memory=<span class=\"hljs-literal\"\
          >None</span>,\n    no_split_module_classes=[<span class=\"hljs-string\"\
          >\"GPTJBlock\"</span>],\n    dtype=<span class=\"hljs-string\">'float16'</span>,\n\
          \    low_zero=<span class=\"hljs-literal\">False</span>,\n)\n\ndevice_map\
          \ = infer_auto_device_map(\n    model, \n    max_memory=max_memory,\n  \
          \  no_split_module_classes=[<span class=\"hljs-string\">\"GPTJBlock\"</span>],\
          \ \n    dtype=<span class=\"hljs-string\">'float16'</span>\n)\n\nmodel =\
          \ dispatch_model(model, device_map=device_map)\n</code></pre>\n</blockquote>\n\
          <p>Thanks! Sadly, won't be able to get another GPU soon!</p>\n"
        raw: "> Theoretically, GPT-JT cannot run on one single 3060 12GB as the model\
          \ itself takes up ~12GB and thus so there is not enough memory for inference.\
          \ I'll recommend VRAM >= 16GB. An alternative is to use multiple 3060 GPUs\
          \ with `accelerate`:\n> \n> ```python\n> from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\n> from accelerate import dispatch_model, infer_auto_device_map\n\
          > from accelerate.utils import get_balanced_memory\n> \n> # Load model to\
          \ CPU\n> tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
          )\n> model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
          )\n> \n> max_memory = get_balanced_memory(\n>     model,\n>     max_memory=None,\n\
          >     no_split_module_classes=[\"GPTJBlock\"],\n>     dtype='float16',\n\
          >     low_zero=False,\n> )\n> \n> device_map = infer_auto_device_map(\n\
          >     model, \n>     max_memory=max_memory,\n>     no_split_module_classes=[\"\
          GPTJBlock\"], \n>     dtype='float16'\n> )\n> \n> model = dispatch_model(model,\
          \ device_map=device_map)\n> ```\n\nThanks! Sadly, won't be able to get another\
          \ GPU soon!"
        updatedAt: '2023-02-04T07:41:46.665Z'
      numEdits: 0
      reactions: []
    id: 63de0c3aedf44714938c16f6
    type: comment
  author: spartanml
  content: "> Theoretically, GPT-JT cannot run on one single 3060 12GB as the model\
    \ itself takes up ~12GB and thus so there is not enough memory for inference.\
    \ I'll recommend VRAM >= 16GB. An alternative is to use multiple 3060 GPUs with\
    \ `accelerate`:\n> \n> ```python\n> from transformers import AutoTokenizer, AutoModelForCausalLM\n\
    > from accelerate import dispatch_model, infer_auto_device_map\n> from accelerate.utils\
    \ import get_balanced_memory\n> \n> # Load model to CPU\n> tokenizer = AutoTokenizer.from_pretrained(\"\
    togethercomputer/GPT-JT-6B-v1\")\n> model = AutoModelForCausalLM.from_pretrained(\"\
    togethercomputer/GPT-JT-6B-v1\")\n> \n> max_memory = get_balanced_memory(\n> \
    \    model,\n>     max_memory=None,\n>     no_split_module_classes=[\"GPTJBlock\"\
    ],\n>     dtype='float16',\n>     low_zero=False,\n> )\n> \n> device_map = infer_auto_device_map(\n\
    >     model, \n>     max_memory=max_memory,\n>     no_split_module_classes=[\"\
    GPTJBlock\"], \n>     dtype='float16'\n> )\n> \n> model = dispatch_model(model,\
    \ device_map=device_map)\n> ```\n\nThanks! Sadly, won't be able to get another\
    \ GPU soon!"
  created_at: 2023-02-04 07:41:46+00:00
  edited: false
  hidden: false
  id: 63de0c3aedf44714938c16f6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: togethercomputer/GPT-JT-6B-v1
repo_type: model
status: open
target_branch: null
title: Hardware requirements for inference?
