!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BigSalmon
conflicting_files: null
created_at: 2022-11-25 03:07:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b14483dce02604e3683e53bc29ea309c.svg
      fullname: Simon Salmon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BigSalmon
      type: user
    createdAt: '2022-11-25T03:07:14.000Z'
    data:
      edited: false
      editors:
      - BigSalmon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b14483dce02604e3683e53bc29ea309c.svg
          fullname: Simon Salmon
          isHf: false
          isPro: false
          name: BigSalmon
          type: user
        html: '<p>How do you use the bidirectional aspect of the model?</p>

          '
        raw: How do you use the bidirectional aspect of the model?
        updatedAt: '2022-11-25T03:07:14.708Z'
      numEdits: 0
      reactions: []
    id: 6380316296f995a21717dd7a
    type: comment
  author: BigSalmon
  content: How do you use the bidirectional aspect of the model?
  created_at: 2022-11-25 03:07:14+00:00
  edited: false
  hidden: false
  id: 6380316296f995a21717dd7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2022-11-25T03:20:36.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: '<p>Hi. It is basically a Prefix-LM, where the prompt phase uses bidirectional
          attention, and the token generation phase uses causal attention.<br>We mainly
          follow the setup of <a rel="nofollow" href="https://arxiv.org/abs/2210.11399">UL2R</a>.
          Have fun :)</p>

          '
        raw: 'Hi. It is basically a Prefix-LM, where the prompt phase uses bidirectional
          attention, and the token generation phase uses causal attention.

          We mainly follow the setup of [UL2R](https://arxiv.org/abs/2210.11399).
          Have fun :)'
        updatedAt: '2022-11-25T03:20:36.054Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - BigSalmon
    id: 638034848a841ab26d7c1954
    type: comment
  author: juewang
  content: 'Hi. It is basically a Prefix-LM, where the prompt phase uses bidirectional
    attention, and the token generation phase uses causal attention.

    We mainly follow the setup of [UL2R](https://arxiv.org/abs/2210.11399). Have fun
    :)'
  created_at: 2022-11-25 03:20:36+00:00
  edited: false
  hidden: false
  id: 638034848a841ab26d7c1954
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2022-11-29T19:47:08.000Z'
    data:
      edited: true
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>I''m curious about getting this to load right using <code>transformers</code>.  If
          we load it with <code>AutoModelForCausalLM</code>, won''t the prompt not
          get bidirectional attention? Is the idea that PrefixLM is only needed during
          training and you go back to CLM at inference?</p>

          '
        raw: I'm curious about getting this to load right using `transformers`.  If
          we load it with `AutoModelForCausalLM`, won't the prompt not get bidirectional
          attention? Is the idea that PrefixLM is only needed during training and
          you go back to CLM at inference?
        updatedAt: '2022-11-29T19:51:17.654Z'
      numEdits: 1
      reactions: []
    id: 638661bc06858a85f59396c8
    type: comment
  author: sam-mosaic
  content: I'm curious about getting this to load right using `transformers`.  If
    we load it with `AutoModelForCausalLM`, won't the prompt not get bidirectional
    attention? Is the idea that PrefixLM is only needed during training and you go
    back to CLM at inference?
  created_at: 2022-11-29 19:47:08+00:00
  edited: true
  hidden: false
  id: 638661bc06858a85f59396c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2022-11-30T01:24:51.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: '<p>Great question! We achieve this by setting <code>model.transformer.h[i].attn.bias.data[:]
          = 1</code><br>Therefore, during the prompt encoding phase, the <a rel="nofollow"
          href="https://github.com/huggingface/transformers/blob/ab9fe45236cd99b8797df78219438f8f6662bb42/src/transformers/models/gptj/modeling_gptj.py#L166">causal
          mask</a> will be all one thus none will be masked; And during token generation
          phase, each generated token naturally only sees the tokens before it, so
          no special handling is required. :)</p>

          '
        raw: 'Great question! We achieve this by setting `model.transformer.h[i].attn.bias.data[:]
          = 1`

          Therefore, during the prompt encoding phase, the [causal mask](https://github.com/huggingface/transformers/blob/ab9fe45236cd99b8797df78219438f8f6662bb42/src/transformers/models/gptj/modeling_gptj.py#L166)
          will be all one thus none will be masked; And during token generation phase,
          each generated token naturally only sees the tokens before it, so no special
          handling is required. :)'
        updatedAt: '2022-11-30T01:24:51.506Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - khaimaitien
    id: 6386b0e35ea0bcbedb161a75
    type: comment
  author: juewang
  content: 'Great question! We achieve this by setting `model.transformer.h[i].attn.bias.data[:]
    = 1`

    Therefore, during the prompt encoding phase, the [causal mask](https://github.com/huggingface/transformers/blob/ab9fe45236cd99b8797df78219438f8f6662bb42/src/transformers/models/gptj/modeling_gptj.py#L166)
    will be all one thus none will be masked; And during token generation phase, each
    generated token naturally only sees the tokens before it, so no special handling
    is required. :)'
  created_at: 2022-11-30 01:24:51+00:00
  edited: false
  hidden: false
  id: 6386b0e35ea0bcbedb161a75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
      fullname: Khai Mai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: khaimaitien
      type: user
    createdAt: '2022-11-30T04:47:45.000Z'
    data:
      edited: false
      editors:
      - khaimaitien
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
          fullname: Khai Mai
          isHf: false
          isPro: false
          name: khaimaitien
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;juewang&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/juewang\">@<span class=\"\
          underline\">juewang</span></a></span>\n\n\t</span></span> Now I want to\
          \ continue to fintune GPT-JT on dialog datasets, do you also release  the\
          \ finetuning  source code or any instruction for it ?</p>\n"
        raw: '@juewang Now I want to continue to fintune GPT-JT on dialog datasets,
          do you also release  the finetuning  source code or any instruction for
          it ?'
        updatedAt: '2022-11-30T04:47:45.775Z'
      numEdits: 0
      reactions: []
    id: 6386e071a38ac48afd92f9f9
    type: comment
  author: khaimaitien
  content: '@juewang Now I want to continue to fintune GPT-JT on dialog datasets,
    do you also release  the finetuning  source code or any instruction for it ?'
  created_at: 2022-11-30 04:47:45+00:00
  edited: false
  hidden: false
  id: 6386e071a38ac48afd92f9f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
      fullname: Khai Mai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: khaimaitien
      type: user
    createdAt: '2022-11-30T09:08:16.000Z'
    data:
      edited: false
      editors:
      - khaimaitien
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
          fullname: Khai Mai
          isHf: false
          isPro: false
          name: khaimaitien
          type: user
        html: '<blockquote>

          <p>Great question! We achieve this by setting <code>model.transformer.h[i].attn.bias.data[:]
          = 1</code><br>Therefore, during the prompt encoding phase, the <a rel="nofollow"
          href="https://github.com/huggingface/transformers/blob/ab9fe45236cd99b8797df78219438f8f6662bb42/src/transformers/models/gptj/modeling_gptj.py#L166">causal
          mask</a> will be all one thus none will be masked; And during token generation
          phase, each generated token naturally only sees the tokens before it, so
          no special handling is required. :)</p>

          </blockquote>

          <p>So by using this:<br>model = AutoModelForCausalLM.from_pretrained("togethercomputer/GPT-JT-6B-v1")<br>model.generate(...)<br>Do
          I need to set model.transformer.h[i].attn.bias.data[:] = 1 at the first
          time of encoding the prompt or you have already handled it automatically
          in the code. If you''ve already handled it, can you show me that line of
          code ? Thank you !</p>

          '
        raw: "> Great question! We achieve this by setting `model.transformer.h[i].attn.bias.data[:]\
          \ = 1`\n> Therefore, during the prompt encoding phase, the [causal mask](https://github.com/huggingface/transformers/blob/ab9fe45236cd99b8797df78219438f8f6662bb42/src/transformers/models/gptj/modeling_gptj.py#L166)\
          \ will be all one thus none will be masked; And during token generation\
          \ phase, each generated token naturally only sees the tokens before it,\
          \ so no special handling is required. :)\n\nSo by using this: \nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
          )\nmodel.generate(...) \nDo I need to set model.transformer.h[i].attn.bias.data[:]\
          \ = 1 at the first time of encoding the prompt or you have already handled\
          \ it automatically in the code. If you've already handled it, can you show\
          \ me that line of code ? Thank you !"
        updatedAt: '2022-11-30T09:08:16.383Z'
      numEdits: 0
      reactions: []
    id: 63871d8023da90491eb7ef2d
    type: comment
  author: khaimaitien
  content: "> Great question! We achieve this by setting `model.transformer.h[i].attn.bias.data[:]\
    \ = 1`\n> Therefore, during the prompt encoding phase, the [causal mask](https://github.com/huggingface/transformers/blob/ab9fe45236cd99b8797df78219438f8f6662bb42/src/transformers/models/gptj/modeling_gptj.py#L166)\
    \ will be all one thus none will be masked; And during token generation phase,\
    \ each generated token naturally only sees the tokens before it, so no special\
    \ handling is required. :)\n\nSo by using this: \nmodel = AutoModelForCausalLM.from_pretrained(\"\
    togethercomputer/GPT-JT-6B-v1\")\nmodel.generate(...) \nDo I need to set model.transformer.h[i].attn.bias.data[:]\
    \ = 1 at the first time of encoding the prompt or you have already handled it\
    \ automatically in the code. If you've already handled it, can you show me that\
    \ line of code ? Thank you !"
  created_at: 2022-11-30 09:08:16+00:00
  edited: false
  hidden: false
  id: 63871d8023da90491eb7ef2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2022-11-30T11:19:13.000Z'
    data:
      edited: true
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;juewang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/juewang\"\
          >@<span class=\"underline\">juewang</span></a></span>\n\n\t</span></span>\
          \ Now I want to continue to fintune GPT-JT on dialog datasets, do you also\
          \ release  the finetuning  source code or any instruction for it ?</p>\n\
          </blockquote>\n<p>We used a fork of <a rel=\"nofollow\" href=\"https://github.com/DS3Lab/DT-FM\"\
          >DS3Lab/DT-FM</a>, and I think we will release the source code of training\
          \ GPT-JT soon!</p>\n"
        raw: '> @juewang Now I want to continue to fintune GPT-JT on dialog datasets,
          do you also release  the finetuning  source code or any instruction for
          it ?


          We used a fork of [DS3Lab/DT-FM](https://github.com/DS3Lab/DT-FM), and I
          think we will release the source code of training GPT-JT soon!'
        updatedAt: '2022-11-30T11:23:36.809Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - khaimaitien
    id: 63873c3185f406f24f53d592
    type: comment
  author: juewang
  content: '> @juewang Now I want to continue to fintune GPT-JT on dialog datasets,
    do you also release  the finetuning  source code or any instruction for it ?


    We used a fork of [DS3Lab/DT-FM](https://github.com/DS3Lab/DT-FM), and I think
    we will release the source code of training GPT-JT soon!'
  created_at: 2022-11-30 11:19:13+00:00
  edited: true
  hidden: false
  id: 63873c3185f406f24f53d592
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2022-11-30T11:24:03.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Great question! We achieve this by setting\
          \ <code>model.transformer.h[i].attn.bias.data[:] = 1</code><br>Therefore,\
          \ during the prompt encoding phase, the <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/ab9fe45236cd99b8797df78219438f8f6662bb42/src/transformers/models/gptj/modeling_gptj.py#L166\"\
          >causal mask</a> will be all one thus none will be masked; And during token\
          \ generation phase, each generated token naturally only sees the tokens\
          \ before it, so no special handling is required. :)</p>\n</blockquote>\n\
          <p>So by using this:<br>model = AutoModelForCausalLM.from_pretrained(\"\
          togethercomputer/GPT-JT-6B-v1\")<br>model.generate(...)<br>Do I need to\
          \ set model.transformer.h[i].attn.bias.data[:] = 1 at the first time of\
          \ encoding the prompt or you have already handled it automatically in the\
          \ code. If you've already handled it, can you show me that line of code\
          \ ? Thank you !</p>\n</blockquote>\n<p>You don't have to set it manually.\
          \ After loading, it defaults to all 1. </p>\n<p>So in short, the following\
          \ code should work:</p>\n<pre><code class=\"language-python\">model = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"togethercomputer/GPT-JT-6B-v1\"</span>)\nmodel.generate(...)\
          \ \n</code></pre>\n"
        raw: "> > Great question! We achieve this by setting `model.transformer.h[i].attn.bias.data[:]\
          \ = 1`\n> > Therefore, during the prompt encoding phase, the [causal mask](https://github.com/huggingface/transformers/blob/ab9fe45236cd99b8797df78219438f8f6662bb42/src/transformers/models/gptj/modeling_gptj.py#L166)\
          \ will be all one thus none will be masked; And during token generation\
          \ phase, each generated token naturally only sees the tokens before it,\
          \ so no special handling is required. :)\n> \n> So by using this: \n> model\
          \ = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
          )\n> model.generate(...) \n> Do I need to set model.transformer.h[i].attn.bias.data[:]\
          \ = 1 at the first time of encoding the prompt or you have already handled\
          \ it automatically in the code. If you've already handled it, can you show\
          \ me that line of code ? Thank you !\n\nYou don't have to set it manually.\
          \ After loading, it defaults to all 1. \n\nSo in short, the following code\
          \ should work:\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          togethercomputer/GPT-JT-6B-v1\")\nmodel.generate(...) \n```"
        updatedAt: '2022-11-30T11:24:03.030Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - BigSalmon
        - khaimaitien
        - vonjack
    id: 63873d5388b39a64e1ea227b
    type: comment
  author: juewang
  content: "> > Great question! We achieve this by setting `model.transformer.h[i].attn.bias.data[:]\
    \ = 1`\n> > Therefore, during the prompt encoding phase, the [causal mask](https://github.com/huggingface/transformers/blob/ab9fe45236cd99b8797df78219438f8f6662bb42/src/transformers/models/gptj/modeling_gptj.py#L166)\
    \ will be all one thus none will be masked; And during token generation phase,\
    \ each generated token naturally only sees the tokens before it, so no special\
    \ handling is required. :)\n> \n> So by using this: \n> model = AutoModelForCausalLM.from_pretrained(\"\
    togethercomputer/GPT-JT-6B-v1\")\n> model.generate(...) \n> Do I need to set model.transformer.h[i].attn.bias.data[:]\
    \ = 1 at the first time of encoding the prompt or you have already handled it\
    \ automatically in the code. If you've already handled it, can you show me that\
    \ line of code ? Thank you !\n\nYou don't have to set it manually. After loading,\
    \ it defaults to all 1. \n\nSo in short, the following code should work:\n```python\n\
    model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
    )\nmodel.generate(...) \n```"
  created_at: 2022-11-30 11:24:03+00:00
  edited: false
  hidden: false
  id: 63873d5388b39a64e1ea227b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
      fullname: Khai Mai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: khaimaitien
      type: user
    createdAt: '2022-11-30T16:49:56.000Z'
    data:
      edited: false
      editors:
      - khaimaitien
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
          fullname: Khai Mai
          isHf: false
          isPro: false
          name: khaimaitien
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p>Great question! We achieve\
          \ this by setting <code>model.transformer.h[i].attn.bias.data[:] = 1</code><br>Therefore,\
          \ during the prompt encoding phase, the <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/ab9fe45236cd99b8797df78219438f8f6662bb42/src/transformers/models/gptj/modeling_gptj.py#L166\"\
          >causal mask</a> will be all one thus none will be masked; And during token\
          \ generation phase, each generated token naturally only sees the tokens\
          \ before it, so no special handling is required. :)</p>\n</blockquote>\n\
          <p>So by using this:<br>model = AutoModelForCausalLM.from_pretrained(\"\
          togethercomputer/GPT-JT-6B-v1\")<br>model.generate(...)<br>Do I need to\
          \ set model.transformer.h[i].attn.bias.data[:] = 1 at the first time of\
          \ encoding the prompt or you have already handled it automatically in the\
          \ code. If you've already handled it, can you show me that line of code\
          \ ? Thank you !</p>\n</blockquote>\n<p>You don't have to set it manually.\
          \ After loading, it defaults to all 1. </p>\n<p>So in short, the following\
          \ code should work:</p>\n<pre><code class=\"language-python\">model = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"togethercomputer/GPT-JT-6B-v1\"</span>)\nmodel.generate(...)\
          \ \n</code></pre>\n</blockquote>\n<p>Thank you for your answer. I also have\
          \ another question about Padding during training. In the paper: \"Transcending\
          \ Scaling Laws with 0.1% Extra Compute\",<br><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/1669826844782-635baca137c6a2c12e2e2af7.png\"\
          ><img alt=\"Screen Shot 2022-11-30 at 23.45.52.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1669826844782-635baca137c6a2c12e2e2af7.png\"\
          ></a><br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1669826220216-635baca137c6a2c12e2e2af7.png\"\
          ><img alt=\"Screen Shot 2022-11-30 at 23.36.56.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1669826220216-635baca137c6a2c12e2e2af7.png\"\
          ></a><br>Can you explain more about this prefix optimization ? Such as giving\
          \ examples ! Thank you !</p>\n"
        raw: "> > > Great question! We achieve this by setting `model.transformer.h[i].attn.bias.data[:]\
          \ = 1`\n> > > Therefore, during the prompt encoding phase, the [causal mask](https://github.com/huggingface/transformers/blob/ab9fe45236cd99b8797df78219438f8f6662bb42/src/transformers/models/gptj/modeling_gptj.py#L166)\
          \ will be all one thus none will be masked; And during token generation\
          \ phase, each generated token naturally only sees the tokens before it,\
          \ so no special handling is required. :)\n> > \n> > So by using this: \n\
          > > model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
          )\n> > model.generate(...) \n> > Do I need to set model.transformer.h[i].attn.bias.data[:]\
          \ = 1 at the first time of encoding the prompt or you have already handled\
          \ it automatically in the code. If you've already handled it, can you show\
          \ me that line of code ? Thank you !\n> \n> You don't have to set it manually.\
          \ After loading, it defaults to all 1. \n> \n> So in short, the following\
          \ code should work:\n> ```python\n> model = AutoModelForCausalLM.from_pretrained(\"\
          togethercomputer/GPT-JT-6B-v1\")\n> model.generate(...) \n> ```\n\nThank\
          \ you for your answer. I also have another question about Padding during\
          \ training. In the paper: \"Transcending Scaling Laws with 0.1% Extra Compute\"\
          , \n![Screen Shot 2022-11-30 at 23.45.52.png](https://cdn-uploads.huggingface.co/production/uploads/1669826844782-635baca137c6a2c12e2e2af7.png)\n\
          ![Screen Shot 2022-11-30 at 23.36.56.png](https://cdn-uploads.huggingface.co/production/uploads/1669826220216-635baca137c6a2c12e2e2af7.png)\n\
          Can you explain more about this prefix optimization ? Such as giving examples\
          \ ! Thank you !"
        updatedAt: '2022-11-30T16:49:56.650Z'
      numEdits: 0
      reactions: []
    id: 638789b4f2517225ae257107
    type: comment
  author: khaimaitien
  content: "> > > Great question! We achieve this by setting `model.transformer.h[i].attn.bias.data[:]\
    \ = 1`\n> > > Therefore, during the prompt encoding phase, the [causal mask](https://github.com/huggingface/transformers/blob/ab9fe45236cd99b8797df78219438f8f6662bb42/src/transformers/models/gptj/modeling_gptj.py#L166)\
    \ will be all one thus none will be masked; And during token generation phase,\
    \ each generated token naturally only sees the tokens before it, so no special\
    \ handling is required. :)\n> > \n> > So by using this: \n> > model = AutoModelForCausalLM.from_pretrained(\"\
    togethercomputer/GPT-JT-6B-v1\")\n> > model.generate(...) \n> > Do I need to set\
    \ model.transformer.h[i].attn.bias.data[:] = 1 at the first time of encoding the\
    \ prompt or you have already handled it automatically in the code. If you've already\
    \ handled it, can you show me that line of code ? Thank you !\n> \n> You don't\
    \ have to set it manually. After loading, it defaults to all 1. \n> \n> So in\
    \ short, the following code should work:\n> ```python\n> model = AutoModelForCausalLM.from_pretrained(\"\
    togethercomputer/GPT-JT-6B-v1\")\n> model.generate(...) \n> ```\n\nThank you for\
    \ your answer. I also have another question about Padding during training. In\
    \ the paper: \"Transcending Scaling Laws with 0.1% Extra Compute\", \n![Screen\
    \ Shot 2022-11-30 at 23.45.52.png](https://cdn-uploads.huggingface.co/production/uploads/1669826844782-635baca137c6a2c12e2e2af7.png)\n\
    ![Screen Shot 2022-11-30 at 23.36.56.png](https://cdn-uploads.huggingface.co/production/uploads/1669826220216-635baca137c6a2c12e2e2af7.png)\n\
    Can you explain more about this prefix optimization ? Such as giving examples\
    \ ! Thank you !"
  created_at: 2022-11-30 16:49:56+00:00
  edited: false
  hidden: false
  id: 638789b4f2517225ae257107
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2022-12-01T03:02:09.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: '<p>Our setting is slightly different to the UL2R paper -- we didn''t
          add padding to the prefix, so the prefix length is variable.<br>We manipulated
          the causal mask to indicate the prefix part and target part, and we only
          calculate loss on the target part.</p>

          <p>In the first 2.6B tokens, we trained with UL2''s mixture-of-denoiser
          objective (see 3.3 in "Transcending Scaling Laws with 0.1% Extra Compute").<br>The
          training samples are like:</p>

          <ul>

          <li>[S2S] [prefix] [target]</li>

          <li>[NLU] [prefix] [target]</li>

          <li>[NLG] [prefix] [target]</li>

          </ul>

          <p>And in the following 0.92B tokens, we trained in pure PrefixLM (sequential
          denoising in UL2) and get rid of the [S2S] tag, since this is what we planned
          to evaluate (and can be used with HF''s transformers out-of-the-box). So
          the training samples are like:</p>

          <ul>

          <li>[prefix] [target]</li>

          </ul>

          '
        raw: "Our setting is slightly different to the UL2R paper -- we didn't add\
          \ padding to the prefix, so the prefix length is variable. \nWe manipulated\
          \ the causal mask to indicate the prefix part and target part, and we only\
          \ calculate loss on the target part.\n\nIn the first 2.6B tokens, we trained\
          \ with UL2's mixture-of-denoiser objective (see 3.3 in \"Transcending Scaling\
          \ Laws with 0.1% Extra Compute\"). \nThe training samples are like:\n- [S2S]\
          \ [prefix] [target]\n- [NLU] [prefix] [target]\n- [NLG] [prefix] [target]\n\
          \nAnd in the following 0.92B tokens, we trained in pure PrefixLM (sequential\
          \ denoising in UL2) and get rid of the [S2S] tag, since this is what we\
          \ planned to evaluate (and can be used with HF's transformers out-of-the-box).\
          \ So the training samples are like:\n- [prefix] [target]"
        updatedAt: '2022-12-01T03:02:09.402Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - khaimaitien
        - kellyhuis
    id: 638819318b3c55fea3e7dcf7
    type: comment
  author: juewang
  content: "Our setting is slightly different to the UL2R paper -- we didn't add padding\
    \ to the prefix, so the prefix length is variable. \nWe manipulated the causal\
    \ mask to indicate the prefix part and target part, and we only calculate loss\
    \ on the target part.\n\nIn the first 2.6B tokens, we trained with UL2's mixture-of-denoiser\
    \ objective (see 3.3 in \"Transcending Scaling Laws with 0.1% Extra Compute\"\
    ). \nThe training samples are like:\n- [S2S] [prefix] [target]\n- [NLU] [prefix]\
    \ [target]\n- [NLG] [prefix] [target]\n\nAnd in the following 0.92B tokens, we\
    \ trained in pure PrefixLM (sequential denoising in UL2) and get rid of the [S2S]\
    \ tag, since this is what we planned to evaluate (and can be used with HF's transformers\
    \ out-of-the-box). So the training samples are like:\n- [prefix] [target]"
  created_at: 2022-12-01 03:02:09+00:00
  edited: false
  hidden: false
  id: 638819318b3c55fea3e7dcf7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4acab7d1e48d57fa6ecf9f4d09744264.svg
      fullname: Jonathan Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jlli
      type: user
    createdAt: '2023-01-21T01:40:36.000Z'
    data:
      edited: false
      editors:
      - jlli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4acab7d1e48d57fa6ecf9f4d09744264.svg
          fullname: Jonathan Li
          isHf: false
          isPro: false
          name: jlli
          type: user
        html: '<p>Some further questions here! </p>

          <ol>

          <li>I noticed there are extra tokens in the tokenizer ( e.g. &lt;|extratoken_1|&gt;)
          that correspond to the infill tokens for the R- and X-denoising parts of
          the UL2 loss. However, there are no special tokens for the sentinels [S2S],
          [NLU], and [NLG]. Do you just let the tokenizer treat [S2S] as 5 separate
          tokens? Or is a different representation used?</li>

          <li>For R- and X-denoising, do you use special <b> and  tokens to bookend
          the  part? (see attached image) </b></li><b>

          <li>What was the weight decay used during UL2 training?<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/1674264764091-63cb1d0a1b705cc951e72e14.png"><img
          alt="Screenshot 2023-01-20 at 5.32.34 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/1674264764091-63cb1d0a1b705cc951e72e14.png"></a></li>

          </b></ol><b>

          </b>'
        raw: "Some further questions here! \n1. I noticed there are extra tokens in\
          \ the tokenizer ( e.g. <|extratoken_1|>) that correspond to the infill tokens\
          \ for the R- and X-denoising parts of the UL2 loss. However, there are no\
          \ special tokens for the sentinels [S2S], [NLU], and [NLG]. Do you just\
          \ let the tokenizer treat [S2S] as 5 separate tokens? Or is a different\
          \ representation used?\n2. For R- and X-denoising, do you use special <B>\
          \ and <E> tokens to bookend the <target> part? (see attached image) \n3.\
          \ What was the weight decay used during UL2 training?\n![Screenshot 2023-01-20\
          \ at 5.32.34 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1674264764091-63cb1d0a1b705cc951e72e14.png)"
        updatedAt: '2023-01-21T01:40:36.861Z'
      numEdits: 0
      reactions: []
    id: 63cb4294312febded98b2029
    type: comment
  author: jlli
  content: "Some further questions here! \n1. I noticed there are extra tokens in\
    \ the tokenizer ( e.g. <|extratoken_1|>) that correspond to the infill tokens\
    \ for the R- and X-denoising parts of the UL2 loss. However, there are no special\
    \ tokens for the sentinels [S2S], [NLU], and [NLG]. Do you just let the tokenizer\
    \ treat [S2S] as 5 separate tokens? Or is a different representation used?\n2.\
    \ For R- and X-denoising, do you use special <B> and <E> tokens to bookend the\
    \ <target> part? (see attached image) \n3. What was the weight decay used during\
    \ UL2 training?\n![Screenshot 2023-01-20 at 5.32.34 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1674264764091-63cb1d0a1b705cc951e72e14.png)"
  created_at: 2023-01-21 01:40:36+00:00
  edited: false
  hidden: false
  id: 63cb4294312febded98b2029
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-01-26T17:11:05.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;jlli&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jlli\">@<span class=\"\
          underline\">jlli</span></a></span>\n\n\t</span></span> !<br>Regarding your\
          \ first question, we handle [S2S], [NLU], and [NLG] as plain text, i.e.\
          \ with multiple tokens designated for each sentinel.<br>For the second question,\
          \ we adopt the strategy outlined in the paper \"<a rel=\"nofollow\" href=\"\
          https://arxiv.org/abs/2210.11399\">Transcending Scaling Laws with 0.1% Extra\
          \ Compute</a>\" by utilizing the last 100 tokens in the vocabulary as additional\
          \ identifiers (i.e. ).<br>Lastly, in regards to your third question, we\
          \ use AdamW optimization algorithm with a weight decay of 0.01.<br>Please\
          \ let me know if you have any further question :)</p>\n"
        raw: 'Hi, @jlli !

          Regarding your first question, we handle [S2S], [NLU], and [NLG] as plain
          text, i.e. with multiple tokens designated for each sentinel.

          For the second question, we adopt the strategy outlined in the paper "[Transcending
          Scaling Laws with 0.1% Extra Compute](https://arxiv.org/abs/2210.11399)"
          by utilizing the last 100 tokens in the vocabulary as additional identifiers
          (i.e. <extra_id_XX>).

          Lastly, in regards to your third question, we use AdamW optimization algorithm
          with a weight decay of 0.01.

          Please let me know if you have any further question :)'
        updatedAt: '2023-01-26T17:11:05.050Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jlli
    id: 63d2b42913d86bc148040c57
    type: comment
  author: juewang
  content: 'Hi, @jlli !

    Regarding your first question, we handle [S2S], [NLU], and [NLG] as plain text,
    i.e. with multiple tokens designated for each sentinel.

    For the second question, we adopt the strategy outlined in the paper "[Transcending
    Scaling Laws with 0.1% Extra Compute](https://arxiv.org/abs/2210.11399)" by utilizing
    the last 100 tokens in the vocabulary as additional identifiers (i.e. <extra_id_XX>).

    Lastly, in regards to your third question, we use AdamW optimization algorithm
    with a weight decay of 0.01.

    Please let me know if you have any further question :)'
  created_at: 2023-01-26 17:11:05+00:00
  edited: false
  hidden: false
  id: 63d2b42913d86bc148040c57
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: togethercomputer/GPT-JT-6B-v1
repo_type: model
status: open
target_branch: null
title: How do you use the bidirectional aspect of the model?
