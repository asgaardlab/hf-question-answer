!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hansintheair
conflicting_files: null
created_at: 2022-12-29 21:32:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e483b59c2c2b981101f737d28bd9bca.svg
      fullname: Hannes Ziegler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hansintheair
      type: user
    createdAt: '2022-12-29T21:32:17.000Z'
    data:
      edited: false
      editors:
      - hansintheair
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e483b59c2c2b981101f737d28bd9bca.svg
          fullname: Hannes Ziegler
          isHf: false
          isPro: false
          name: hansintheair
          type: user
        html: '<p>I''m a complete newcomer to hugging face, and would like to try
          running this model on my machine.</p>

          <p>I created a new environment with:</p>

          <p><code>conda create -n model-env python=3.10 transformers pytorch</code></p>

          <p>activated the environment, read through the readme, and copied the quick
          start code given there into a new .py file.</p>

          <p>It seems the GPT-JT-6B-v1 folder contains all the files necessary to
          run this model, so modified the path to point to my local directory:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM,
          AutoTokenizer


          model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">r"C:\&lt;path&gt;\test\GPT-JT-6B-v1"</span>)

          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">r"C:\&lt;path&gt;\test\GPT-JT-6B-v1"</span>)

          </code></pre>

          <p>However, when I run this I get the following error:</p>

          <pre><code>OSError: Unable to load weights from pytorch checkpoint file
          for ''C:\&lt;path&gt;\test\GPT-JT-6B-v1\pytorch_model.bin'' at ''C:\&lt;path&gt;\test\GPT-JT-6B-v1\pytorch_model.bin''.
          If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set
          from_tf=True.

          </code></pre>

          <p>Am I doing it wrong??</p>

          '
        raw: "I'm a complete newcomer to hugging face, and would like to try running\
          \ this model on my machine.\r\n\r\nI created a new environment with:\r\n\
          \r\n`conda create -n model-env python=3.10 transformers pytorch`\r\n\r\n\
          activated the environment, read through the readme, and copied the quick\
          \ start code given there into a new .py file.\r\n\r\nIt seems the GPT-JT-6B-v1\
          \ folder contains all the files necessary to run this model, so modified\
          \ the path to point to my local directory:\r\n\r\n```python\r\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(r\"\
          C:\\<path>\\test\\GPT-JT-6B-v1\")\r\ntokenizer = AutoTokenizer.from_pretrained(r\"\
          C:\\<path>\\test\\GPT-JT-6B-v1\")\r\n```\r\n\r\nHowever, when I run this\
          \ I get the following error:\r\n\r\n```\r\nOSError: Unable to load weights\
          \ from pytorch checkpoint file for 'C:\\<path>\\test\\GPT-JT-6B-v1\\pytorch_model.bin'\
          \ at 'C:\\<path>\\test\\GPT-JT-6B-v1\\pytorch_model.bin'. If you tried to\
          \ load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\
          \n```\r\n\r\nAm I doing it wrong??\r\n"
        updatedAt: '2022-12-29T21:32:17.546Z'
      numEdits: 0
      reactions: []
    id: 63ae076142fd3b8dbaebf543
    type: comment
  author: hansintheair
  content: "I'm a complete newcomer to hugging face, and would like to try running\
    \ this model on my machine.\r\n\r\nI created a new environment with:\r\n\r\n`conda\
    \ create -n model-env python=3.10 transformers pytorch`\r\n\r\nactivated the environment,\
    \ read through the readme, and copied the quick start code given there into a\
    \ new .py file.\r\n\r\nIt seems the GPT-JT-6B-v1 folder contains all the files\
    \ necessary to run this model, so modified the path to point to my local directory:\r\
    \n\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
    \n\r\nmodel = AutoModelForCausalLM.from_pretrained(r\"C:\\<path>\\test\\GPT-JT-6B-v1\"\
    )\r\ntokenizer = AutoTokenizer.from_pretrained(r\"C:\\<path>\\test\\GPT-JT-6B-v1\"\
    )\r\n```\r\n\r\nHowever, when I run this I get the following error:\r\n\r\n```\r\
    \nOSError: Unable to load weights from pytorch checkpoint file for 'C:\\<path>\\\
    test\\GPT-JT-6B-v1\\pytorch_model.bin' at 'C:\\<path>\\test\\GPT-JT-6B-v1\\pytorch_model.bin'.\
    \ If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\
    \n```\r\n\r\nAm I doing it wrong??\r\n"
  created_at: 2022-12-29 21:32:17+00:00
  edited: false
  hidden: false
  id: 63ae076142fd3b8dbaebf543
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ed4ae9be9492e21d8296ccb2d2629fd.svg
      fullname: Max
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DarwinAnim8or
      type: user
    createdAt: '2023-01-05T03:15:05.000Z'
    data:
      edited: false
      editors:
      - DarwinAnim8or
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ed4ae9be9492e21d8296ccb2d2629fd.svg
          fullname: Max
          isHf: false
          isPro: false
          name: DarwinAnim8or
          type: user
        html: '<p>Have you tried letting the transformers model handle the installation
          of the model? </p>

          <p>For example:<br>from transformers import AutoTokenizer, AutoModelForCausalLM</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("togethercomputer/GPT-JT-6B-v1")</p>

          <p>model = AutoModelForCausalLM.from_pretrained("togethercomputer/GPT-JT-6B-v1")</p>

          <p>This code will download all the files itself and manage them</p>

          '
        raw: "Have you tried letting the transformers model handle the installation\
          \ of the model? \n\nFor example:\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
          )\n\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
          )\n\nThis code will download all the files itself and manage them"
        updatedAt: '2023-01-05T03:15:05.719Z'
      numEdits: 0
      reactions: []
    id: 63b640b9c5a5432fd862ef9e
    type: comment
  author: DarwinAnim8or
  content: "Have you tried letting the transformers model handle the installation\
    \ of the model? \n\nFor example:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    \ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
    )\n\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
    )\n\nThis code will download all the files itself and manage them"
  created_at: 2023-01-05 03:15:05+00:00
  edited: false
  hidden: false
  id: 63b640b9c5a5432fd862ef9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-01-23T02:55:18.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p>Hello, as <span data-props=\"{&quot;user&quot;:&quot;DarwinAnim8or&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/DarwinAnim8or\"\
          >@<span class=\"underline\">DarwinAnim8or</span></a></span>\n\n\t</span></span>\
          \ mentioned, the transformers library can handle loading the model for you.<br>However,\
          \ if you prefer to load the model from a local folder, please ensure that\
          \ the folder <code>C:\\&lt;path&gt;\\test\\GPT-JT-6B-v1</code> directly\
          \ contains all of the necessary files from this repository, and that there\
          \ are no nested folders within it. Thank you \U0001F60A</p>\n"
        raw: "Hello, as @DarwinAnim8or mentioned, the transformers library can handle\
          \ loading the model for you. \nHowever, if you prefer to load the model\
          \ from a local folder, please ensure that the folder `C:\\<path>\\test\\\
          GPT-JT-6B-v1` directly contains all of the necessary files from this repository,\
          \ and that there are no nested folders within it. Thank you \U0001F60A"
        updatedAt: '2023-01-23T02:55:18.265Z'
      numEdits: 0
      reactions: []
    id: 63cdf716de1a04e05738adba
    type: comment
  author: juewang
  content: "Hello, as @DarwinAnim8or mentioned, the transformers library can handle\
    \ loading the model for you. \nHowever, if you prefer to load the model from a\
    \ local folder, please ensure that the folder `C:\\<path>\\test\\GPT-JT-6B-v1`\
    \ directly contains all of the necessary files from this repository, and that\
    \ there are no nested folders within it. Thank you \U0001F60A"
  created_at: 2023-01-23 02:55:18+00:00
  edited: false
  hidden: false
  id: 63cdf716de1a04e05738adba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9e483b59c2c2b981101f737d28bd9bca.svg
      fullname: Hannes Ziegler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hansintheair
      type: user
    createdAt: '2023-02-04T02:04:23.000Z'
    data:
      status: closed
    id: 63ddbd27a05f19fc14e0bb90
    type: status-change
  author: hansintheair
  created_at: 2023-02-04 02:04:23+00:00
  id: 63ddbd27a05f19fc14e0bb90
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e483b59c2c2b981101f737d28bd9bca.svg
      fullname: Hannes Ziegler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hansintheair
      type: user
    createdAt: '2023-02-04T03:59:50.000Z'
    data:
      edited: false
      editors:
      - hansintheair
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e483b59c2c2b981101f737d28bd9bca.svg
          fullname: Hannes Ziegler
          isHf: false
          isPro: false
          name: hansintheair
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;juewang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/juewang\"\
          >@<span class=\"underline\">juewang</span></a></span>\n\n\t</span></span>\
          \ , it turns out the pytorch_model.bin file was corrupted. After I downloaded\
          \ it independently and replaced the corrupted file, I got it running!</p>\n\
          <p>However, I ended up getting a bunch of weird jibberish to my prompt \"\
          once upon a time\":</p>\n<pre><code>once upon a timeonceonceonceonlyonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonce\
          \ onceonce once onceonce once once a timeonceonce\n once a timeonceonce\
          \ once once a timeonceonce once a timeonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonce\n\
          </code></pre>\n<p>Along with a warning:</p>\n<pre><code>The attention mask\
          \ and the pad token id were not set. As a consequence, you may observe unexpected\
          \ behavior. Please pass your input's `attention_mask` to obtain reliable\
          \ results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end\
          \ generation.`\n</code></pre>\n<p>Looking into this now, but any pointers\
          \ are much appreciated =]</p>\n"
        raw: "Thank you @juewang , it turns out the pytorch_model.bin file was corrupted.\
          \ After I downloaded it independently and replaced the corrupted file, I\
          \ got it running!\n\nHowever, I ended up getting a bunch of weird jibberish\
          \ to my prompt \"once upon a time\":\n\n```\nonce upon a timeonceonceonceonlyonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonce\
          \ onceonce once onceonce once once a timeonceonce\n once a timeonceonce\
          \ once once a timeonceonce once a timeonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonce\n\
          ```\n\nAlong with a warning:\n\n```\nThe attention mask and the pad token\
          \ id were not set. As a consequence, you may observe unexpected behavior.\
          \ Please pass your input's `attention_mask` to obtain reliable results.\n\
          Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.`\n\
          ```\n\nLooking into this now, but any pointers are much appreciated =]\n\
          \n\n"
        updatedAt: '2023-02-04T03:59:50.177Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63ddd8364541ab470d00156f
    id: 63ddd8364541ab470d00156e
    type: comment
  author: hansintheair
  content: "Thank you @juewang , it turns out the pytorch_model.bin file was corrupted.\
    \ After I downloaded it independently and replaced the corrupted file, I got it\
    \ running!\n\nHowever, I ended up getting a bunch of weird jibberish to my prompt\
    \ \"once upon a time\":\n\n```\nonce upon a timeonceonceonceonlyonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonce\
    \ onceonce once onceonce once once a timeonceonce\n once a timeonceonce once once\
    \ a timeonceonce once a timeonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonceonce\n\
    ```\n\nAlong with a warning:\n\n```\nThe attention mask and the pad token id were\
    \ not set. As a consequence, you may observe unexpected behavior. Please pass\
    \ your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id`\
    \ to `eos_token_id`:50256 for open-end generation.`\n```\n\nLooking into this\
    \ now, but any pointers are much appreciated =]\n\n\n"
  created_at: 2023-02-04 03:59:50+00:00
  edited: false
  hidden: false
  id: 63ddd8364541ab470d00156e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9e483b59c2c2b981101f737d28bd9bca.svg
      fullname: Hannes Ziegler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hansintheair
      type: user
    createdAt: '2023-02-04T03:59:50.000Z'
    data:
      status: open
    id: 63ddd8364541ab470d00156f
    type: status-change
  author: hansintheair
  created_at: 2023-02-04 03:59:50+00:00
  id: 63ddd8364541ab470d00156f
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: togethercomputer/GPT-JT-6B-v1
repo_type: model
status: open
target_branch: null
title: Complete noob question - cloned the repository, now what?
