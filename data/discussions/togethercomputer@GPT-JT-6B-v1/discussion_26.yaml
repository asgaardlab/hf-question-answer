!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jswowah
conflicting_files: null
created_at: 2023-03-19 23:26:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/21e28693ed7bf03af06c5de5c5b77131.svg
      fullname: Jade sage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jswowah
      type: user
    createdAt: '2023-03-20T00:26:45.000Z'
    data:
      edited: false
      editors:
      - jswowah
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/21e28693ed7bf03af06c5de5c5b77131.svg
          fullname: Jade sage
          isHf: false
          isPro: false
          name: jswowah
          type: user
        html: "<p>Hello,  I've been trying to replicate the GPT-JT spaces demo results\
          \ locally. So far the results are getting close to the demo but seems to\
          \ be struggling for tasks that require deeper contextual understanding like\
          \ answering questions from give examples. I've noticed by changing the \"\
          stop, split by\" parameter on the web demo from the default, the model also\
          \ seems to struggle on tasks it previously was performing well at. However,\
          \ there doesn't seem to be an argument that is settable in the Transformers\
          \ pipeline. Could anyone provide an explanation on how or if setting \"\
          stop, split by\" is possible locally?<br>I am using the following code to\
          \ load and run the model:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, AutoModelForCausalLM\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> pipeline\n\
          \ntokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\"\
          >\"togethercomputer/GPT-JT-6B-v1\"</span>)\n\nmodel = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"togethercomputer/GPT-JT-6B-v1\"</span>, device_map=<span\
          \ class=\"hljs-string\">\"auto\"</span>, load_in_8bit=<span class=\"hljs-literal\"\
          >True</span>)\n\npipe = pipeline(<span class=\"hljs-string\">\"text-generation\"\
          </span>,model=model, tokenizer=tokenizer)\n\ngen_args = {\n    <span class=\"\
          hljs-string\">'top_p'</span>:<span class=\"hljs-number\">1.0</span>,\n \
          \   <span class=\"hljs-string\">'top_k'</span> : <span class=\"hljs-number\"\
          >40</span>,\n    <span class=\"hljs-string\">'temperature'</span>:<span\
          \ class=\"hljs-number\">0.01</span>,\n    <span class=\"hljs-string\">'repetition_penalty'</span>\
          \ : <span class=\"hljs-number\">1.0</span>,\n    <span class=\"hljs-string\"\
          >'max_new_tokens'</span> : <span class=\"hljs-number\">2</span>,\n    <span\
          \ class=\"hljs-string\">'do_sample'</span> : <span class=\"hljs-literal\"\
          >True</span>,\n    <span class=\"hljs-comment\">#'stop' : '\\n' # This doesn't\
          \ work and raises an exception about 'model_kwargs' not being used</span>\n\
          }\n\noutput = pipe(prompt, **gen_args)\n<span class=\"hljs-built_in\">print</span>(output[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'generated_text'</span>])\n\
          </code></pre>\n"
        raw: "Hello,  I've been trying to replicate the GPT-JT spaces demo results\
          \ locally. So far the results are getting close to the demo but seems to\
          \ be struggling for tasks that require deeper contextual understanding like\
          \ answering questions from give examples. I've noticed by changing the \"\
          stop, split by\" parameter on the web demo from the default, the model also\
          \ seems to struggle on tasks it previously was performing well at. However,\
          \ there doesn't seem to be an argument that is settable in the Transformers\
          \ pipeline. Could anyone provide an explanation on how or if setting \"\
          stop, split by\" is possible locally? \r\nI am using the following code\
          \ to load and run the model:\r\n```python\r\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\r\nfrom transformers import pipeline\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\")\r\n\
          \r\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
          , device_map=\"auto\", load_in_8bit=True)\r\n\r\npipe = pipeline(\"text-generation\"\
          ,model=model, tokenizer=tokenizer)\r\n\r\ngen_args = {\r\n    'top_p':1.0,\r\
          \n    'top_k' : 40,\r\n    'temperature':0.01,\r\n    'repetition_penalty'\
          \ : 1.0,\r\n    'max_new_tokens' : 2,\r\n    'do_sample' : True,\r\n   \
          \ #'stop' : '\\n' # This doesn't work and raises an exception about 'model_kwargs'\
          \ not being used\r\n}\r\n\r\noutput = pipe(prompt, **gen_args)\r\nprint(output[0]['generated_text'])\r\
          \n\r\n```"
        updatedAt: '2023-03-20T00:26:45.643Z'
      numEdits: 0
      reactions: []
    id: 6417a845058f65de431ead37
    type: comment
  author: jswowah
  content: "Hello,  I've been trying to replicate the GPT-JT spaces demo results locally.\
    \ So far the results are getting close to the demo but seems to be struggling\
    \ for tasks that require deeper contextual understanding like answering questions\
    \ from give examples. I've noticed by changing the \"stop, split by\" parameter\
    \ on the web demo from the default, the model also seems to struggle on tasks\
    \ it previously was performing well at. However, there doesn't seem to be an argument\
    \ that is settable in the Transformers pipeline. Could anyone provide an explanation\
    \ on how or if setting \"stop, split by\" is possible locally? \r\nI am using\
    \ the following code to load and run the model:\r\n```python\r\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM\r\nfrom transformers import pipeline\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
    )\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-JT-6B-v1\"\
    , device_map=\"auto\", load_in_8bit=True)\r\n\r\npipe = pipeline(\"text-generation\"\
    ,model=model, tokenizer=tokenizer)\r\n\r\ngen_args = {\r\n    'top_p':1.0,\r\n\
    \    'top_k' : 40,\r\n    'temperature':0.01,\r\n    'repetition_penalty' : 1.0,\r\
    \n    'max_new_tokens' : 2,\r\n    'do_sample' : True,\r\n    #'stop' : '\\n'\
    \ # This doesn't work and raises an exception about 'model_kwargs' not being used\r\
    \n}\r\n\r\noutput = pipe(prompt, **gen_args)\r\nprint(output[0]['generated_text'])\r\
    \n\r\n```"
  created_at: 2023-03-19 23:26:45+00:00
  edited: false
  hidden: false
  id: 6417a845058f65de431ead37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-03-23T02:39:55.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jswowah&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jswowah\">@<span class=\"\
          underline\">jswowah</span></a></span>\n\n\t</span></span> The \"stop,\"\
          \ parameter is not a configurable setting in the Transformers library. You\
          \ can do a post-process yourself after generate the text or define a custom\
          \ stop criteria:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> StoppingCriteria,\
          \ StoppingCriteriaList\n\n<span class=\"hljs-keyword\">class</span> <span\
          \ class=\"hljs-title class_\">StopWordsCriteria</span>(<span class=\"hljs-title\
          \ class_ inherited__\">StoppingCriteria</span>):\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"\
          hljs-params\">self, stop_words, tokenizer</span>):\n        self.tokenizer\
          \ = tokenizer\n        self.stop_words = stop_words\n        self._cache_str\
          \ = <span class=\"hljs-string\">''</span>\n\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">__call__</span>(<span class=\"\
          hljs-params\">self, input_ids: torch.LongTensor, scores: torch.FloatTensor,\
          \ **kwargs</span>) -&gt; <span class=\"hljs-built_in\">bool</span>:\n  \
          \      self._cache_str += self.tokenizer.decode(input_ids[<span class=\"\
          hljs-number\">0</span>, -<span class=\"hljs-number\">1</span>])\n      \
          \  <span class=\"hljs-keyword\">for</span> stop_words <span class=\"hljs-keyword\"\
          >in</span> self.stop_words:\n            <span class=\"hljs-keyword\">if</span>\
          \ stop_words <span class=\"hljs-keyword\">in</span> self._cache_str:\n \
          \               <span class=\"hljs-keyword\">return</span> <span class=\"\
          hljs-literal\">True</span>\n        <span class=\"hljs-keyword\">return</span>\
          \ <span class=\"hljs-literal\">False</span>\n</code></pre>\n<p>And then\
          \ do</p>\n<pre><code>pipe(\"Question: What currency is used in Zurich?\\\
          n\\nAnswer:\", stopping_criteria=StoppingCriteriaList([StopWordsCriteria(['\\\
          n', 'other stop words'], tokenizer)]))\n</code></pre>\n"
        raw: "@jswowah The \"stop,\" parameter is not a configurable setting in the\
          \ Transformers library. You can do a post-process yourself after generate\
          \ the text or define a custom stop criteria:\n```python\nimport torch\n\
          from transformers import StoppingCriteria, StoppingCriteriaList\n\nclass\
          \ StopWordsCriteria(StoppingCriteria):\n    def __init__(self, stop_words,\
          \ tokenizer):\n        self.tokenizer = tokenizer\n        self.stop_words\
          \ = stop_words\n        self._cache_str = ''\n\n    def __call__(self, input_ids:\
          \ torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n    \
          \    self._cache_str += self.tokenizer.decode(input_ids[0, -1])\n      \
          \  for stop_words in self.stop_words:\n            if stop_words in self._cache_str:\n\
          \                return True\n        return False\n```\nAnd then do\n```\n\
          pipe(\"Question: What currency is used in Zurich?\\n\\nAnswer:\", stopping_criteria=StoppingCriteriaList([StopWordsCriteria(['\\\
          n', 'other stop words'], tokenizer)]))\n```"
        updatedAt: '2023-03-23T02:39:55.033Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jswowah
    id: 641bbbfb4723a2b0aa521e18
    type: comment
  author: juewang
  content: "@jswowah The \"stop,\" parameter is not a configurable setting in the\
    \ Transformers library. You can do a post-process yourself after generate the\
    \ text or define a custom stop criteria:\n```python\nimport torch\nfrom transformers\
    \ import StoppingCriteria, StoppingCriteriaList\n\nclass StopWordsCriteria(StoppingCriteria):\n\
    \    def __init__(self, stop_words, tokenizer):\n        self.tokenizer = tokenizer\n\
    \        self.stop_words = stop_words\n        self._cache_str = ''\n\n    def\
    \ __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs)\
    \ -> bool:\n        self._cache_str += self.tokenizer.decode(input_ids[0, -1])\n\
    \        for stop_words in self.stop_words:\n            if stop_words in self._cache_str:\n\
    \                return True\n        return False\n```\nAnd then do\n```\npipe(\"\
    Question: What currency is used in Zurich?\\n\\nAnswer:\", stopping_criteria=StoppingCriteriaList([StopWordsCriteria(['\\\
    n', 'other stop words'], tokenizer)]))\n```"
  created_at: 2023-03-23 01:39:55+00:00
  edited: false
  hidden: false
  id: 641bbbfb4723a2b0aa521e18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/21e28693ed7bf03af06c5de5c5b77131.svg
      fullname: Jade sage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jswowah
      type: user
    createdAt: '2023-03-23T21:57:57.000Z'
    data:
      edited: false
      editors:
      - jswowah
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/21e28693ed7bf03af06c5de5c5b77131.svg
          fullname: Jade sage
          isHf: false
          isPro: false
          name: jswowah
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;juewang&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/juewang\">@<span class=\"\
          underline\">juewang</span></a></span>\n\n\t</span></span><br>Thanks for\
          \ the clarification, I am getting much better results after implementing\
          \ the <code>StopWordsCriteria</code>.  Does the order of the list of stopping\
          \ words affect  how and when the model decides to stop?</p>\n"
        raw: "@juewang  \nThanks for the clarification, I am getting much better results\
          \ after implementing the `StopWordsCriteria`.  Does the order of the list\
          \ of stopping words affect  how and when the model decides to stop?"
        updatedAt: '2023-03-23T21:57:57.211Z'
      numEdits: 0
      reactions: []
    id: 641ccb653a58d3b736a7fb0d
    type: comment
  author: jswowah
  content: "@juewang  \nThanks for the clarification, I am getting much better results\
    \ after implementing the `StopWordsCriteria`.  Does the order of the list of stopping\
    \ words affect  how and when the model decides to stop?"
  created_at: 2023-03-23 20:57:57+00:00
  edited: false
  hidden: false
  id: 641ccb653a58d3b736a7fb0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-03-24T04:36:18.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jswowah&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jswowah\">@<span class=\"\
          underline\">jswowah</span></a></span>\n\n\t</span></span> There is no difference\
          \ I think. You can customize this part if you want to.</p>\n"
        raw: '@jswowah There is no difference I think. You can customize this part
          if you want to.'
        updatedAt: '2023-03-24T04:36:18.840Z'
      numEdits: 0
      reactions: []
    id: 641d28c2b54f6cf02b05efb2
    type: comment
  author: juewang
  content: '@jswowah There is no difference I think. You can customize this part if
    you want to.'
  created_at: 2023-03-24 03:36:18+00:00
  edited: false
  hidden: false
  id: 641d28c2b54f6cf02b05efb2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/21e28693ed7bf03af06c5de5c5b77131.svg
      fullname: Jade sage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jswowah
      type: user
    createdAt: '2023-03-25T03:53:24.000Z'
    data:
      edited: false
      editors:
      - jswowah
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/21e28693ed7bf03af06c5de5c5b77131.svg
          fullname: Jade sage
          isHf: false
          isPro: false
          name: jswowah
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;juewang&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/juewang\">@<span class=\"\
          underline\">juewang</span></a></span>\n\n\t</span></span><br>Great, thanks\
          \ again for the help.</p>\n"
        raw: "@juewang \nGreat, thanks again for the help."
        updatedAt: '2023-03-25T03:53:24.316Z'
      numEdits: 0
      reactions: []
      relatedEventId: 641e703463d9d99d8c195567
    id: 641e703463d9d99d8c195566
    type: comment
  author: jswowah
  content: "@juewang \nGreat, thanks again for the help."
  created_at: 2023-03-25 02:53:24+00:00
  edited: false
  hidden: false
  id: 641e703463d9d99d8c195566
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/21e28693ed7bf03af06c5de5c5b77131.svg
      fullname: Jade sage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jswowah
      type: user
    createdAt: '2023-03-25T03:53:24.000Z'
    data:
      status: closed
    id: 641e703463d9d99d8c195567
    type: status-change
  author: jswowah
  created_at: 2023-03-25 02:53:24+00:00
  id: 641e703463d9d99d8c195567
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 26
repo_id: togethercomputer/GPT-JT-6B-v1
repo_type: model
status: closed
target_branch: null
title: Any way to set the "stop, split by" when running the model locally?
