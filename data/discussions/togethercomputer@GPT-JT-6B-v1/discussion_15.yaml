!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MukeshSharma
conflicting_files: null
created_at: 2022-12-22 10:47:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
      fullname: Mukesh Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MukeshSharma
      type: user
    createdAt: '2022-12-22T10:47:53.000Z'
    data:
      edited: false
      editors:
      - MukeshSharma
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
          fullname: Mukesh Sharma
          isHf: false
          isPro: false
          name: MukeshSharma
          type: user
        html: '<p>Any python  notebook available to fine tune GPT-JT-6B-v1  On my
          personal dataset. </p>

          <p>Is it good for Code Generation ? Can it perform better than original
          GPT-J ? </p>

          '
        raw: "Any python  notebook available to fine tune GPT-JT-6B-v1  On my personal\
          \ dataset. \r\n\r\nIs it good for Code Generation ? Can it perform better\
          \ than original GPT-J ? \r\n"
        updatedAt: '2022-12-22T10:47:53.973Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - yahma
        - kobalsky
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - yahma
    id: 63a435d9412fd71fb7ef71ea
    type: comment
  author: MukeshSharma
  content: "Any python  notebook available to fine tune GPT-JT-6B-v1  On my personal\
    \ dataset. \r\n\r\nIs it good for Code Generation ? Can it perform better than\
    \ original GPT-J ? \r\n"
  created_at: 2022-12-22 10:47:53+00:00
  edited: false
  hidden: false
  id: 63a435d9412fd71fb7ef71ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671752917839-635157f885bdb764f6ac41bc.png?w=200&h=200&f=face
      fullname: Aivan Monceller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: geocine
      type: user
    createdAt: '2022-12-29T14:16:05.000Z'
    data:
      edited: false
      editors:
      - geocine
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671752917839-635157f885bdb764f6ac41bc.png?w=200&h=200&f=face
          fullname: Aivan Monceller
          isHf: false
          isPro: false
          name: geocine
          type: user
        html: '<p>Want to know as well</p>

          '
        raw: Want to know as well
        updatedAt: '2022-12-29T14:16:05.092Z'
      numEdits: 0
      reactions: []
    id: 63ada125554f7aa0804a4462
    type: comment
  author: geocine
  content: Want to know as well
  created_at: 2022-12-29 14:16:05+00:00
  edited: false
  hidden: false
  id: 63ada125554f7aa0804a4462
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672330023435-62c6faed53c7156f5bf767ed.png?w=200&h=200&f=face
      fullname: Gene Ruebsamen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yahma
      type: user
    createdAt: '2023-01-07T05:35:22.000Z'
    data:
      edited: true
      editors:
      - yahma
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672330023435-62c6faed53c7156f5bf767ed.png?w=200&h=200&f=face
          fullname: Gene Ruebsamen
          isHf: false
          isPro: false
          name: yahma
          type: user
        html: '<p>I tried fine-tuning an 8-bit quantized version of GPT-JT and failed
          to get any output.. I''ve fine-tuned 8-bit quantized regular GPT-J without
          issue. I''m wondering if there are differences in fine-tuning the models.</p>

          '
        raw: I tried fine-tuning an 8-bit quantized version of GPT-JT and failed to
          get any output.. I've fine-tuned 8-bit quantized regular GPT-J without issue.
          I'm wondering if there are differences in fine-tuning the models.
        updatedAt: '2023-01-07T05:36:04.491Z'
      numEdits: 1
      reactions: []
    id: 63b9049a491867c464807e07
    type: comment
  author: yahma
  content: I tried fine-tuning an 8-bit quantized version of GPT-JT and failed to
    get any output.. I've fine-tuned 8-bit quantized regular GPT-J without issue.
    I'm wondering if there are differences in fine-tuning the models.
  created_at: 2023-01-07 05:35:22+00:00
  edited: true
  hidden: false
  id: 63b9049a491867c464807e07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37f8541bb895fdf4742031e9632a0d32.svg
      fullname: C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kobalsky
      type: user
    createdAt: '2023-01-27T18:19:47.000Z'
    data:
      edited: false
      editors:
      - kobalsky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37f8541bb895fdf4742031e9632a0d32.svg
          fullname: C
          isHf: false
          isPro: false
          name: kobalsky
          type: user
        html: '<p>Looks like this model cannot be fine-tuned</p>

          '
        raw: Looks like this model cannot be fine-tuned
        updatedAt: '2023-01-27T18:19:47.169Z'
      numEdits: 0
      reactions: []
    id: 63d415c3ce1a38ea4ea7aebd
    type: comment
  author: kobalsky
  content: Looks like this model cannot be fine-tuned
  created_at: 2023-01-27 18:19:47+00:00
  edited: false
  hidden: false
  id: 63d415c3ce1a38ea4ea7aebd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-01-28T14:55:10.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p>Sorry for the late reply <span data-props=\"{&quot;user&quot;:&quot;yahma&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yahma\"\
          >@<span class=\"underline\">yahma</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;kobalsky&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/kobalsky\">@<span class=\"\
          underline\">kobalsky</span></a></span>\n\n\t</span></span><br>I finally\
          \ realize that to achieve bidirectional attention for inference, we set\
          \ zeros the <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/48d4e147d824efab97637947709d5aa67c809b3d/src/transformers/models/gptj/modeling_gptj.py#L98-L103\"\
          >causal mask</a> , by <code>layer.bias[:] = 0</code>. This is fine because\
          \ during inference, the model naturally cannot see future tokens. So removing\
          \ causal mask won't cause any problem.</p>\n<p>In order to do training /\
          \ fine-tuning, we should revert this back, and manually control the causal\
          \ mask for each sequence \u2013 the prompt part should be all zeros, and\
          \ the generation part should be causal mask. Otherwise, there will be information\
          \ leakage (each token can see the entire sequence) in training so the model\
          \ won't learn meaningful things.</p>\n"
        raw: "Sorry for the late reply @yahma @kobalsky \nI finally realize that to\
          \ achieve bidirectional attention for inference, we set zeros the [causal\
          \ mask](https://github.com/huggingface/transformers/blob/48d4e147d824efab97637947709d5aa67c809b3d/src/transformers/models/gptj/modeling_gptj.py#L98-L103)\
          \ , by `layer.bias[:] = 0`. This is fine because during inference, the model\
          \ naturally cannot see future tokens. So removing causal mask won't cause\
          \ any problem.\n\nIn order to do training / fine-tuning, we should revert\
          \ this back, and manually control the causal mask for each sequence \u2013\
          \ the prompt part should be all zeros, and the generation part should be\
          \ causal mask. Otherwise, there will be information leakage (each token\
          \ can see the entire sequence) in training so the model won't learn meaningful\
          \ things."
        updatedAt: '2023-01-28T14:55:10.198Z'
      numEdits: 0
      reactions: []
    id: 63d5374e963de177b60f490b
    type: comment
  author: juewang
  content: "Sorry for the late reply @yahma @kobalsky \nI finally realize that to\
    \ achieve bidirectional attention for inference, we set zeros the [causal mask](https://github.com/huggingface/transformers/blob/48d4e147d824efab97637947709d5aa67c809b3d/src/transformers/models/gptj/modeling_gptj.py#L98-L103)\
    \ , by `layer.bias[:] = 0`. This is fine because during inference, the model naturally\
    \ cannot see future tokens. So removing causal mask won't cause any problem.\n\
    \nIn order to do training / fine-tuning, we should revert this back, and manually\
    \ control the causal mask for each sequence \u2013 the prompt part should be all\
    \ zeros, and the generation part should be causal mask. Otherwise, there will\
    \ be information leakage (each token can see the entire sequence) in training\
    \ so the model won't learn meaningful things."
  created_at: 2023-01-28 14:55:10+00:00
  edited: false
  hidden: false
  id: 63d5374e963de177b60f490b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ff5fc4fe6383d50b29052e/Vk9R5rKqG-Z_ou-55J9x-.jpeg?w=200&h=200&f=face
      fullname: AayushShah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AayushShah
      type: user
    createdAt: '2023-03-09T02:36:52.000Z'
    data:
      edited: false
      editors:
      - AayushShah
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ff5fc4fe6383d50b29052e/Vk9R5rKqG-Z_ou-55J9x-.jpeg?w=200&h=200&f=face
          fullname: AayushShah
          isHf: false
          isPro: false
          name: AayushShah
          type: user
        html: "<p>Hello, <span data-props=\"{&quot;user&quot;:&quot;juewang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/juewang\"\
          >@<span class=\"underline\">juewang</span></a></span>\n\n\t</span></span>\
          \ does that mean we can not fine-tune it on the new (specific) dataset?<br>Like\
          \ I want to tune the model on just a medical dataset (say) for the purpose\
          \ of question answering, I want the model to \"generate\" the answers from\
          \ its knowledge. </p>\n<p>In this case, what procedure should I follow to\
          \ tune the model with just the medical dataset and then say I should be\
          \ able to ask the question like: \"What are top 5 causes of diarrhea?\"\
          \ and it should return the \"generated\" answer.</p>\n<p>Please help, thanks.</p>\n"
        raw: "Hello, @juewang does that mean we can not fine-tune it on the new (specific)\
          \ dataset? \nLike I want to tune the model on just a medical dataset (say)\
          \ for the purpose of question answering, I want the model to \"generate\"\
          \ the answers from its knowledge. \n\nIn this case, what procedure should\
          \ I follow to tune the model with just the medical dataset and then say\
          \ I should be able to ask the question like: \"What are top 5 causes of\
          \ diarrhea?\" and it should return the \"generated\" answer.\n\nPlease help,\
          \ thanks."
        updatedAt: '2023-03-09T02:36:52.753Z'
      numEdits: 0
      reactions: []
    id: 6409464460fc65165c5198ba
    type: comment
  author: AayushShah
  content: "Hello, @juewang does that mean we can not fine-tune it on the new (specific)\
    \ dataset? \nLike I want to tune the model on just a medical dataset (say) for\
    \ the purpose of question answering, I want the model to \"generate\" the answers\
    \ from its knowledge. \n\nIn this case, what procedure should I follow to tune\
    \ the model with just the medical dataset and then say I should be able to ask\
    \ the question like: \"What are top 5 causes of diarrhea?\" and it should return\
    \ the \"generated\" answer.\n\nPlease help, thanks."
  created_at: 2023-03-09 02:36:52+00:00
  edited: false
  hidden: false
  id: 6409464460fc65165c5198ba
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: togethercomputer/GPT-JT-6B-v1
repo_type: model
status: open
target_branch: null
title: 'What is the fine tuning process of GPT-JT-6B-v1 Copied ? Any Docs available
  ? '
