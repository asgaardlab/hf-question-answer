!!python/object:huggingface_hub.community.DiscussionWithDetails
author: spartanml
conflicting_files: null
created_at: 2022-12-05 17:52:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/deaeb1e80603518efdb450b059f530a3.svg
      fullname: Spar Tan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spartanml
      type: user
    createdAt: '2022-12-05T17:52:39.000Z'
    data:
      edited: true
      editors:
      - spartanml
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/deaeb1e80603518efdb450b059f530a3.svg
          fullname: Spar Tan
          isHf: false
          isPro: false
          name: spartanml
          type: user
        html: '<p>Will it be possible to create/train a model that doesn''t train
          with facts. But only on language structure. For instance:</p>

          <p>Instead of training using: "Issac Newton formulated the theory of Gravity".<br>Can''t
          we train using: "&lt;PERSON_NAME&gt; formulated the theory of &lt;CONCEPT&gt;"</p>

          <p>If possible, I think we can still capture the language structure but
          dramatically reduce the model size?</p>

          '
        raw: 'Will it be possible to create/train a model that doesn''t train with
          facts. But only on language structure. For instance:


          Instead of training using: "Issac Newton formulated the theory of Gravity".

          Can''t we train using: "\<PERSON_NAME\> formulated the theory of \<CONCEPT\>"


          If possible, I think we can still capture the language structure but dramatically
          reduce the model size?'
        updatedAt: '2022-12-05T17:53:30.117Z'
      numEdits: 1
      reactions: []
    id: 638e2fe777a19e8a1fda8bd6
    type: comment
  author: spartanml
  content: 'Will it be possible to create/train a model that doesn''t train with facts.
    But only on language structure. For instance:


    Instead of training using: "Issac Newton formulated the theory of Gravity".

    Can''t we train using: "\<PERSON_NAME\> formulated the theory of \<CONCEPT\>"


    If possible, I think we can still capture the language structure but dramatically
    reduce the model size?'
  created_at: 2022-12-05 17:52:39+00:00
  edited: true
  hidden: false
  id: 638e2fe777a19e8a1fda8bd6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2022-12-07T03:47:00.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: '<p>That would a very interesting idea! Thank you!<br>An issue is that
          there might be inconsistencies between training and inference. E.g. the
          user might enter a prompt that includes facts, such as "Issac Newton formulated
          the theory of", , and the model here would answer " ", which however, might
          not be what the user expects. So we might need additional mechanisms to
          handle the factual knowledge part, e.g. <a rel="nofollow" href="https://arxiv.org/abs/2112.04426">retrofit</a>
          GPT-J.<br>Thank you again for your suggestion. We would definitely look
          into this direction!</p>

          '
        raw: 'That would a very interesting idea! Thank you!

          An issue is that there might be inconsistencies between training and inference.
          E.g. the user might enter a prompt that includes facts, such as "Issac Newton
          formulated the theory of", , and the model here would answer " <CONCEPT>",
          which however, might not be what the user expects. So we might need additional
          mechanisms to handle the factual knowledge part, e.g. [retrofit](https://arxiv.org/abs/2112.04426)
          GPT-J.

          Thank you again for your suggestion. We would definitely look into this
          direction!'
        updatedAt: '2022-12-07T03:47:00.235Z'
      numEdits: 0
      reactions: []
    id: 63900cb4888447611c245816
    type: comment
  author: juewang
  content: 'That would a very interesting idea! Thank you!

    An issue is that there might be inconsistencies between training and inference.
    E.g. the user might enter a prompt that includes facts, such as "Issac Newton
    formulated the theory of", , and the model here would answer " <CONCEPT>", which
    however, might not be what the user expects. So we might need additional mechanisms
    to handle the factual knowledge part, e.g. [retrofit](https://arxiv.org/abs/2112.04426)
    GPT-J.

    Thank you again for your suggestion. We would definitely look into this direction!'
  created_at: 2022-12-07 03:47:00+00:00
  edited: false
  hidden: false
  id: 63900cb4888447611c245816
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/deaeb1e80603518efdb450b059f530a3.svg
      fullname: Spar Tan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spartanml
      type: user
    createdAt: '2023-02-04T16:41:58.000Z'
    data:
      edited: false
      editors:
      - spartanml
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/deaeb1e80603518efdb450b059f530a3.svg
          fullname: Spar Tan
          isHf: false
          isPro: false
          name: spartanml
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;JEU007&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/JEU007\">@<span class=\"\
          underline\">JEU007</span></a></span>\n\n\t</span></span> thanks for replying!\
          \ Missed the notification!</p>\n<p>I was thinking along these lines for\
          \ a model thats significantly smaller but can do two things really well:\
          \ \"Language semantics &amp; say do JS scripts\". If this sort of a model\
          \ can run on a commercial grade GPU, this would be wild!</p>\n<p>Because\
          \ then it opens up for natural language automation: This model + something\
          \ like ScriptKit will do almost anything that can be done with computers\
          \ &amp; a bunch of scripts!</p>\n"
        raw: '@JEU007 thanks for replying! Missed the notification!


          I was thinking along these lines for a model thats significantly smaller
          but can do two things really well: "Language semantics & say do JS scripts".
          If this sort of a model can run on a commercial grade GPU, this would be
          wild!


          Because then it opens up for natural language automation: This model + something
          like ScriptKit will do almost anything that can be done with computers &
          a bunch of scripts!'
        updatedAt: '2023-02-04T16:41:58.386Z'
      numEdits: 0
      reactions: []
    id: 63de8ad6edf4471493974a95
    type: comment
  author: spartanml
  content: '@JEU007 thanks for replying! Missed the notification!


    I was thinking along these lines for a model thats significantly smaller but can
    do two things really well: "Language semantics & say do JS scripts". If this sort
    of a model can run on a commercial grade GPU, this would be wild!


    Because then it opens up for natural language automation: This model + something
    like ScriptKit will do almost anything that can be done with computers & a bunch
    of scripts!'
  created_at: 2023-02-04 16:41:58+00:00
  edited: false
  hidden: false
  id: 63de8ad6edf4471493974a95
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: togethercomputer/GPT-JT-6B-v1
repo_type: model
status: open
target_branch: null
title: Model sans facts?
