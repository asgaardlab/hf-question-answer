!!python/object:huggingface_hub.community.DiscussionWithDetails
author: asifhugs
conflicting_files: null
created_at: 2023-02-01 11:19:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669879497278-6261c62c0568e418d693090a.png?w=200&h=200&f=face
      fullname: Asif Ahmad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asifhugs
      type: user
    createdAt: '2023-02-01T11:19:29.000Z'
    data:
      edited: false
      editors:
      - asifhugs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669879497278-6261c62c0568e418d693090a.png?w=200&h=200&f=face
          fullname: Asif Ahmad
          isHf: false
          isPro: false
          name: asifhugs
          type: user
        html: '<p>Hi, thanks for the work. I have fine-tuned this model like we do
          other casual language models for example  <a href="https://huggingface.co/EleutherAI/gpt-j-6B">EleutherAI/gpt-j-6B</a>,  <a
          href="https://huggingface.co/EleutherAI/gpt-neo-2.7B">EleutherAI/gpt-neo-2.7B</a>
          etc. using my own data set. But the generated texts are only numbers like
          0, 1, etc.</p>

          <p>While the models have no such issues.</p>

          <p>I would highly appreciate any suggestion/help in this regard.</p>

          '
        raw: "Hi, thanks for the work. I have fine-tuned this model like we do other\
          \ casual language models for example  [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B),\
          \  [EleutherAI/gpt-neo-2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B)\
          \ etc. using my own data set. But the generated texts are only numbers like\
          \ 0, 1, etc.\r\n\r\nWhile the models have no such issues.\r\n\r\nI would\
          \ highly appreciate any suggestion/help in this regard."
        updatedAt: '2023-02-01T11:19:29.482Z'
      numEdits: 0
      reactions: []
    id: 63da4ac1e697e5898cb90b42
    type: comment
  author: asifhugs
  content: "Hi, thanks for the work. I have fine-tuned this model like we do other\
    \ casual language models for example  [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B),\
    \  [EleutherAI/gpt-neo-2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B) etc.\
    \ using my own data set. But the generated texts are only numbers like 0, 1, etc.\r\
    \n\r\nWhile the models have no such issues.\r\n\r\nI would highly appreciate any\
    \ suggestion/help in this regard."
  created_at: 2023-02-01 11:19:29+00:00
  edited: false
  hidden: false
  id: 63da4ac1e697e5898cb90b42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669879497278-6261c62c0568e418d693090a.png?w=200&h=200&f=face
      fullname: Asif Ahmad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asifhugs
      type: user
    createdAt: '2023-02-01T11:56:29.000Z'
    data:
      edited: false
      editors:
      - asifhugs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669879497278-6261c62c0568e418d693090a.png?w=200&h=200&f=face
          fullname: Asif Ahmad
          isHf: false
          isPro: false
          name: asifhugs
          type: user
        html: '<p>Some of the results are:</p>

          <p>GeneratedText:</p>

          <p> ................................................................................................................................</p>

          '
        raw: "Some of the results are:\n\nGeneratedText:\n\n ................................................................................................................................"
        updatedAt: '2023-02-01T11:56:29.400Z'
      numEdits: 0
      reactions: []
    id: 63da536de88c619bee29c0d6
    type: comment
  author: asifhugs
  content: "Some of the results are:\n\nGeneratedText:\n\n ................................................................................................................................"
  created_at: 2023-02-01 11:56:29+00:00
  edited: false
  hidden: false
  id: 63da536de88c619bee29c0d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669879497278-6261c62c0568e418d693090a.png?w=200&h=200&f=face
      fullname: Asif Ahmad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asifhugs
      type: user
    createdAt: '2023-02-01T18:01:29.000Z'
    data:
      edited: false
      editors:
      - asifhugs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669879497278-6261c62c0568e418d693090a.png?w=200&h=200&f=face
          fullname: Asif Ahmad
          isHf: false
          isPro: false
          name: asifhugs
          type: user
        html: '<p>InputText = "Bitcoin is "<br>{''Generated Text:'', ''Bitcoin is
          3 3 3 3 3 3 3 3 3 3 3 3 3 3 3  3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
          3 3 3 ''}</p>

          '
        raw: 'InputText = "Bitcoin is "

          {''Generated Text:'', ''Bitcoin is 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3  3 3 3
          3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ''}'
        updatedAt: '2023-02-01T18:01:29.746Z'
      numEdits: 0
      reactions: []
    id: 63daa8f9ca02a16f04fcfa1f
    type: comment
  author: asifhugs
  content: 'InputText = "Bitcoin is "

    {''Generated Text:'', ''Bitcoin is 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3  3 3 3 3 3 3
    3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ''}'
  created_at: 2023-02-01 18:01:29+00:00
  edited: false
  hidden: false
  id: 63daa8f9ca02a16f04fcfa1f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-02-02T02:24:15.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;asifhugs&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/asifhugs\">@<span class=\"\
          underline\">asifhugs</span></a></span>\n\n\t</span></span> Thank you for\
          \ reaching out! I believe this is the causal mask issue for training.</p>\n\
          <p>To achieve bidirectional attention for inference, we set zeros the causal\
          \ mask , by <code>layer.bias[:] = 0</code>. This is fine for inference,\
          \ as the model naturally cannot see future tokens. So removing causal mask\
          \ won't cause any problem.</p>\n<p>In order to do training / fine-tuning,\
          \ we should revert this back, and manually control the causal mask for each\
          \ sequence \u2013 the prompt part should be all zeros, and the generation\
          \ part should be causal mask. Otherwise, there will be information leakage\
          \ (each token can see the entire sequence) in training so the model won't\
          \ learn any meaningful things.</p>\n"
        raw: "@asifhugs Thank you for reaching out! I believe this is the causal mask\
          \ issue for training.\n\nTo achieve bidirectional attention for inference,\
          \ we set zeros the causal mask , by `layer.bias[:] = 0`. This is fine for\
          \ inference, as the model naturally cannot see future tokens. So removing\
          \ causal mask won't cause any problem.\n\nIn order to do training / fine-tuning,\
          \ we should revert this back, and manually control the causal mask for each\
          \ sequence \u2013 the prompt part should be all zeros, and the generation\
          \ part should be causal mask. Otherwise, there will be information leakage\
          \ (each token can see the entire sequence) in training so the model won't\
          \ learn any meaningful things."
        updatedAt: '2023-02-02T02:24:15.199Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - asifhugs
        - KaiserWhoLearns
    id: 63db1ecfa175b8d7f2a50718
    type: comment
  author: juewang
  content: "@asifhugs Thank you for reaching out! I believe this is the causal mask\
    \ issue for training.\n\nTo achieve bidirectional attention for inference, we\
    \ set zeros the causal mask , by `layer.bias[:] = 0`. This is fine for inference,\
    \ as the model naturally cannot see future tokens. So removing causal mask won't\
    \ cause any problem.\n\nIn order to do training / fine-tuning, we should revert\
    \ this back, and manually control the causal mask for each sequence \u2013 the\
    \ prompt part should be all zeros, and the generation part should be causal mask.\
    \ Otherwise, there will be information leakage (each token can see the entire\
    \ sequence) in training so the model won't learn any meaningful things."
  created_at: 2023-02-02 02:24:15+00:00
  edited: false
  hidden: false
  id: 63db1ecfa175b8d7f2a50718
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669879497278-6261c62c0568e418d693090a.png?w=200&h=200&f=face
      fullname: Asif Ahmad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asifhugs
      type: user
    createdAt: '2023-02-03T09:52:30.000Z'
    data:
      edited: false
      editors:
      - asifhugs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669879497278-6261c62c0568e418d693090a.png?w=200&h=200&f=face
          fullname: Asif Ahmad
          isHf: false
          isPro: false
          name: asifhugs
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;juewang&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/juewang\">@<span class=\"\
          underline\">juewang</span></a></span>\n\n\t</span></span>,<br>Thank you\
          \ so much for the response and identifying the issue.<br>Currently, I am\
          \ using this <a rel=\"nofollow\" href=\"https://github.com/asifehmad/Test3/blob/main/Fine_Tune.py\"\
          >script</a> for the task, could you please check it and mention/identify\
          \ the possible changes in it which will lead to avoiding the issue?<br>Would\
          \ be very much thankful!</p>\n<p>Thanks a lot again,<br>Asif</p>\n"
        raw: "Hi @juewang,\nThank you so much for the response and identifying the\
          \ issue. \nCurrently, I am using this [script](https://github.com/asifehmad/Test3/blob/main/Fine_Tune.py)\
          \ for the task, could you please check it and mention/identify the possible\
          \ changes in it which will lead to avoiding the issue?\nWould be very much\
          \ thankful!\n\nThanks a lot again,\nAsif"
        updatedAt: '2023-02-03T09:52:30.996Z'
      numEdits: 0
      reactions: []
    id: 63dcd95e02895390662d51b7
    type: comment
  author: asifhugs
  content: "Hi @juewang,\nThank you so much for the response and identifying the issue.\
    \ \nCurrently, I am using this [script](https://github.com/asifehmad/Test3/blob/main/Fine_Tune.py)\
    \ for the task, could you please check it and mention/identify the possible changes\
    \ in it which will lead to avoiding the issue?\nWould be very much thankful!\n\
    \nThanks a lot again,\nAsif"
  created_at: 2023-02-03 09:52:30+00:00
  edited: false
  hidden: false
  id: 63dcd95e02895390662d51b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-02-05T05:21:59.000Z'
    data:
      edited: true
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;asifhugs&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/asifhugs\"\
          >@<span class=\"underline\">asifhugs</span></a></span>\n\n\t</span></span>,\
          \ a quick fix is to reset the causal mask after loading the trained model,\
          \ e.g.:</p>\n<pre><code class=\"language-python\">model = ...\nmax_positions\
          \ = <span class=\"hljs-number\">2048</span>\n<span class=\"hljs-keyword\"\
          >for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\"\
          >range</span>(<span class=\"hljs-built_in\">len</span>(model.transformer.h)):\n\
          \    model.transformer.h[i].attn.bias[:] = torch.tril(torch.ones((max_positions,\
          \ max_positions), dtype=torch.uint8)).view(\n        <span class=\"hljs-number\"\
          >1</span>, <span class=\"hljs-number\">1</span>, max_positions, max_positions\n\
          \    )\n</code></pre>\n<p>After doing this, this model becomes a <strong>pure\
          \ causal language model</strong>.</p>\n<p>If you want to keep a PrefixLM-style\
          \ training. You should pass a <code>prefix_mask</code>as an argument to\
          \ tell the model which part is the prefix/prompt, and write your custom\
          \ model class to let the model attend to the whole context of prompt. For\
          \ example, we can insert the following code after <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/blob/59d5edef34ae0fa56065a2e863736d4f133c558b/src/transformers/models/gptj/modeling_gptj.py#L158\"\
          >here</a> :</p>\n<pre><code># `prefix_mask` is passed as an argument with\
          \ a shape of (bsz, seqlen)\nif prefix_mask is not None:\n    bsz = query.size(0)\n\
          \    causal_mask = causal_mask.repeat(bsz, 1, 1, 1) # (bsz, 1, src_len,\
          \ tgt_len)\n    causal_mask = causal_mask.permute(0, 3, 1, 2) # (bsz, tgt_len,\
          \ 1, src_len)\n    causal_mask[prefix_mask.bool()] = 1\n    causal_mask\
          \ = causal_mask.permute(0, 2, 3, 1) # (bsz, 1, src_len, tgt_len)\n</code></pre>\n"
        raw: "Hi @asifhugs, a quick fix is to reset the causal mask after loading\
          \ the trained model, e.g.:\n```python\nmodel = ...\nmax_positions = 2048\n\
          for i in range(len(model.transformer.h)):\n    model.transformer.h[i].attn.bias[:]\
          \ = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(\n\
          \        1, 1, max_positions, max_positions\n    )\n``` \nAfter doing this,\
          \ this model becomes a **pure causal language model**.\n\nIf you want to\
          \ keep a PrefixLM-style training. You should pass a `prefix_mask`as an argument\
          \ to tell the model which part is the prefix/prompt, and write your custom\
          \ model class to let the model attend to the whole context of prompt. For\
          \ example, we can insert the following code after [here](https://github.com/huggingface/transformers/blob/59d5edef34ae0fa56065a2e863736d4f133c558b/src/transformers/models/gptj/modeling_gptj.py#L158)\
          \ :\n```\n# `prefix_mask` is passed as an argument with a shape of (bsz,\
          \ seqlen)\nif prefix_mask is not None:\n    bsz = query.size(0)\n    causal_mask\
          \ = causal_mask.repeat(bsz, 1, 1, 1) # (bsz, 1, src_len, tgt_len)\n    causal_mask\
          \ = causal_mask.permute(0, 3, 1, 2) # (bsz, tgt_len, 1, src_len)\n    causal_mask[prefix_mask.bool()]\
          \ = 1\n    causal_mask = causal_mask.permute(0, 2, 3, 1) # (bsz, 1, src_len,\
          \ tgt_len)\n```"
        updatedAt: '2023-02-06T13:31:22.567Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - asifhugs
    id: 63df3cf7fb0ee94683f37fd4
    type: comment
  author: juewang
  content: "Hi @asifhugs, a quick fix is to reset the causal mask after loading the\
    \ trained model, e.g.:\n```python\nmodel = ...\nmax_positions = 2048\nfor i in\
    \ range(len(model.transformer.h)):\n    model.transformer.h[i].attn.bias[:] =\
    \ torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(\n\
    \        1, 1, max_positions, max_positions\n    )\n``` \nAfter doing this, this\
    \ model becomes a **pure causal language model**.\n\nIf you want to keep a PrefixLM-style\
    \ training. You should pass a `prefix_mask`as an argument to tell the model which\
    \ part is the prefix/prompt, and write your custom model class to let the model\
    \ attend to the whole context of prompt. For example, we can insert the following\
    \ code after [here](https://github.com/huggingface/transformers/blob/59d5edef34ae0fa56065a2e863736d4f133c558b/src/transformers/models/gptj/modeling_gptj.py#L158)\
    \ :\n```\n# `prefix_mask` is passed as an argument with a shape of (bsz, seqlen)\n\
    if prefix_mask is not None:\n    bsz = query.size(0)\n    causal_mask = causal_mask.repeat(bsz,\
    \ 1, 1, 1) # (bsz, 1, src_len, tgt_len)\n    causal_mask = causal_mask.permute(0,\
    \ 3, 1, 2) # (bsz, tgt_len, 1, src_len)\n    causal_mask[prefix_mask.bool()] =\
    \ 1\n    causal_mask = causal_mask.permute(0, 2, 3, 1) # (bsz, 1, src_len, tgt_len)\n\
    ```"
  created_at: 2023-02-05 05:21:59+00:00
  edited: true
  hidden: false
  id: 63df3cf7fb0ee94683f37fd4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669879497278-6261c62c0568e418d693090a.png?w=200&h=200&f=face
      fullname: Asif Ahmad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asifhugs
      type: user
    createdAt: '2023-02-05T22:22:34.000Z'
    data:
      edited: false
      editors:
      - asifhugs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669879497278-6261c62c0568e418d693090a.png?w=200&h=200&f=face
          fullname: Asif Ahmad
          isHf: false
          isPro: false
          name: asifhugs
          type: user
        html: "<blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;asifhugs&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/asifhugs\"\
          >@<span class=\"underline\">asifhugs</span></a></span>\n\n\t</span></span>,\
          \ a quick fix is to reset the causal mask after loading the trained model,\
          \ e.g.:</p>\n<pre><code class=\"language-python\">model = ...\nmax_positions\
          \ = <span class=\"hljs-number\">2048</span>\n<span class=\"hljs-keyword\"\
          >for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\"\
          >range</span>(<span class=\"hljs-built_in\">len</span>(model.transformer.h)):\n\
          \    model.transformer.h[i].bias[:] = torch.tril(torch.ones((max_positions,\
          \ max_positions), dtype=torch.uint8)).view(\n        <span class=\"hljs-number\"\
          >1</span>, <span class=\"hljs-number\">1</span>, max_positions, max_positions\n\
          \    )\n</code></pre>\n<p>After doing this, this model becomes a <strong>pure\
          \ causal language model</strong>.</p>\n<p>If you want to keep a PrefixLM-style\
          \ training. You should pass a <code>prefix_mask</code>as an argument to\
          \ tell the model which part is the prefix/prompt, and write your custom\
          \ model class to let the model attend to the whole context of prompt. For\
          \ example, we can insert the following code after <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/blob/59d5edef34ae0fa56065a2e863736d4f133c558b/src/transformers/models/gptj/modeling_gptj.py#L158\"\
          >here</a> :</p>\n<pre><code># `prefix_mask` is passed as an argument with\
          \ a shape of (bsz, seqlen)\nif prefix_mask is not None:\n    bsz = query.size(0)\n\
          \    causal_mask = causal_mask.repeat(bsz, 1, 1, 1) # (bsz, 1, src_len,\
          \ tgt_len)\n    causal_mask = causal_mask.permute(0, 3, 1, 2) # (bsz, tgt_len,\
          \ 1, src_len)\n    causal_mask[prefix_mask.bool()] = 1\n    causal_mask\
          \ = causal_mask.permute(0, 2, 3, 1) # (bsz, 1, src_len, tgt_len)\n</code></pre>\n\
          </blockquote>\n<p>Thanks a lot <span data-props=\"{&quot;user&quot;:&quot;juewang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/juewang\"\
          >@<span class=\"underline\">juewang</span></a></span>\n\n\t</span></span>\
          \ for the detailed comment. I will try these and will let you know.<br>Thanks\
          \ again!</p>\n"
        raw: "> Hi @asifhugs, a quick fix is to reset the causal mask after loading\
          \ the trained model, e.g.:\n> ```python\n> model = ...\n> max_positions\
          \ = 2048\n> for i in range(len(model.transformer.h)):\n>     model.transformer.h[i].bias[:]\
          \ = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(\n\
          >         1, 1, max_positions, max_positions\n>     )\n> ``` \n> After doing\
          \ this, this model becomes a **pure causal language model**.\n> \n> If you\
          \ want to keep a PrefixLM-style training. You should pass a `prefix_mask`as\
          \ an argument to tell the model which part is the prefix/prompt, and write\
          \ your custom model class to let the model attend to the whole context of\
          \ prompt. For example, we can insert the following code after [here](https://github.com/huggingface/transformers/blob/59d5edef34ae0fa56065a2e863736d4f133c558b/src/transformers/models/gptj/modeling_gptj.py#L158)\
          \ :\n> ```\n> # `prefix_mask` is passed as an argument with a shape of (bsz,\
          \ seqlen)\n> if prefix_mask is not None:\n>     bsz = query.size(0)\n> \
          \    causal_mask = causal_mask.repeat(bsz, 1, 1, 1) # (bsz, 1, src_len,\
          \ tgt_len)\n>     causal_mask = causal_mask.permute(0, 3, 1, 2) # (bsz,\
          \ tgt_len, 1, src_len)\n>     causal_mask[prefix_mask.bool()] = 1\n>   \
          \  causal_mask = causal_mask.permute(0, 2, 3, 1) # (bsz, 1, src_len, tgt_len)\n\
          > ```\n\nThanks a lot @juewang for the detailed comment. I will try these\
          \ and will let you know.\nThanks again!"
        updatedAt: '2023-02-05T22:22:34.289Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - amaliak
        - yahma
    id: 63e02c2a809a891ee93cf0a2
    type: comment
  author: asifhugs
  content: "> Hi @asifhugs, a quick fix is to reset the causal mask after loading\
    \ the trained model, e.g.:\n> ```python\n> model = ...\n> max_positions = 2048\n\
    > for i in range(len(model.transformer.h)):\n>     model.transformer.h[i].bias[:]\
    \ = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(\n\
    >         1, 1, max_positions, max_positions\n>     )\n> ``` \n> After doing this,\
    \ this model becomes a **pure causal language model**.\n> \n> If you want to keep\
    \ a PrefixLM-style training. You should pass a `prefix_mask`as an argument to\
    \ tell the model which part is the prefix/prompt, and write your custom model\
    \ class to let the model attend to the whole context of prompt. For example, we\
    \ can insert the following code after [here](https://github.com/huggingface/transformers/blob/59d5edef34ae0fa56065a2e863736d4f133c558b/src/transformers/models/gptj/modeling_gptj.py#L158)\
    \ :\n> ```\n> # `prefix_mask` is passed as an argument with a shape of (bsz, seqlen)\n\
    > if prefix_mask is not None:\n>     bsz = query.size(0)\n>     causal_mask =\
    \ causal_mask.repeat(bsz, 1, 1, 1) # (bsz, 1, src_len, tgt_len)\n>     causal_mask\
    \ = causal_mask.permute(0, 3, 1, 2) # (bsz, tgt_len, 1, src_len)\n>     causal_mask[prefix_mask.bool()]\
    \ = 1\n>     causal_mask = causal_mask.permute(0, 2, 3, 1) # (bsz, 1, src_len,\
    \ tgt_len)\n> ```\n\nThanks a lot @juewang for the detailed comment. I will try\
    \ these and will let you know.\nThanks again!"
  created_at: 2023-02-05 22:22:34+00:00
  edited: false
  hidden: false
  id: 63e02c2a809a891ee93cf0a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c5d990463eedcfcc1c07be863a5219d.svg
      fullname: Jacopo Bandoni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JacopoBandoni
      type: user
    createdAt: '2023-03-13T10:25:22.000Z'
    data:
      edited: false
      editors:
      - JacopoBandoni
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c5d990463eedcfcc1c07be863a5219d.svg
          fullname: Jacopo Bandoni
          isHf: false
          isPro: false
          name: JacopoBandoni
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;asifhugs&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/asifhugs\"\
          >@<span class=\"underline\">asifhugs</span></a></span>\n\n\t</span></span>,\
          \ did you succeed in training?</p>\n"
        raw: Hi @asifhugs, did you succeed in training?
        updatedAt: '2023-03-13T10:25:22.664Z'
      numEdits: 0
      reactions: []
    id: 640efa123282336aadb91dfa
    type: comment
  author: JacopoBandoni
  content: Hi @asifhugs, did you succeed in training?
  created_at: 2023-03-13 09:25:22+00:00
  edited: false
  hidden: false
  id: 640efa123282336aadb91dfa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c5d990463eedcfcc1c07be863a5219d.svg
      fullname: Jacopo Bandoni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JacopoBandoni
      type: user
    createdAt: '2023-03-13T10:56:07.000Z'
    data:
      edited: true
      editors:
      - JacopoBandoni
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c5d990463eedcfcc1c07be863a5219d.svg
          fullname: Jacopo Bandoni
          isHf: false
          isPro: false
          name: JacopoBandoni
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;juewang&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/juewang\">@<span class=\"\
          underline\">juewang</span></a></span>\n\n\t</span></span></p>\n<blockquote>\n\
          <p>Hi <span data-props=\"{&quot;user&quot;:&quot;asifhugs&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/asifhugs\">@<span class=\"\
          underline\">asifhugs</span></a></span>\n\n\t</span></span>, a quick fix\
          \ is to reset the causal mask after loading the trained model, e.g.:</p>\n\
          <pre><code class=\"language-python\">model = ...\nmax_positions = <span\
          \ class=\"hljs-number\">2048</span>\n<span class=\"hljs-keyword\">for</span>\
          \ i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\"\
          >range</span>(<span class=\"hljs-built_in\">len</span>(model.transformer.h)):\n\
          \    model.transformer.h[i].attn.bias[:] = torch.tril(torch.ones((max_positions,\
          \ max_positions), dtype=torch.uint8)).view(\n        <span class=\"hljs-number\"\
          >1</span>, <span class=\"hljs-number\">1</span>, max_positions, max_positions\n\
          \    )\n</code></pre>\n<p>After doing this, this model becomes a <strong>pure\
          \ causal language model</strong>.</p>\n<p>If you want to keep a PrefixLM-style\
          \ training. You should pass a <code>prefix_mask</code>as an argument to\
          \ tell the model which part is the prefix/prompt, and write your custom\
          \ model class to let the model attend to the whole context of prompt. For\
          \ example, we can insert the following code after <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/blob/59d5edef34ae0fa56065a2e863736d4f133c558b/src/transformers/models/gptj/modeling_gptj.py#L158\"\
          >here</a> :</p>\n<pre><code># `prefix_mask` is passed as an argument with\
          \ a shape of (bsz, seqlen)\nif prefix_mask is not None:\n    bsz = query.size(0)\n\
          \    causal_mask = causal_mask.repeat(bsz, 1, 1, 1) # (bsz, 1, src_len,\
          \ tgt_len)\n    causal_mask = causal_mask.permute(0, 3, 1, 2) # (bsz, tgt_len,\
          \ 1, src_len)\n    causal_mask[prefix_mask.bool()] = 1\n    causal_mask\
          \ = causal_mask.permute(0, 2, 3, 1) # (bsz, 1, src_len, tgt_len)\n</code></pre>\n\
          </blockquote>\n<p>Would it be possible to just pass the attention_mask in\
          \ the forward pass during training.<br>In such a way that is possible to\
          \ train prefix style without having to change the underlying code?<br>Thank\
          \ you!</p>\n"
        raw: "@juewang\n> Hi @asifhugs, a quick fix is to reset the causal mask after\
          \ loading the trained model, e.g.:\n> ```python\n> model = ...\n> max_positions\
          \ = 2048\n> for i in range(len(model.transformer.h)):\n>     model.transformer.h[i].attn.bias[:]\
          \ = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(\n\
          >         1, 1, max_positions, max_positions\n>     )\n> ``` \n> After doing\
          \ this, this model becomes a **pure causal language model**.\n> \n> If you\
          \ want to keep a PrefixLM-style training. You should pass a `prefix_mask`as\
          \ an argument to tell the model which part is the prefix/prompt, and write\
          \ your custom model class to let the model attend to the whole context of\
          \ prompt. For example, we can insert the following code after [here](https://github.com/huggingface/transformers/blob/59d5edef34ae0fa56065a2e863736d4f133c558b/src/transformers/models/gptj/modeling_gptj.py#L158)\
          \ :\n> ```\n> # `prefix_mask` is passed as an argument with a shape of (bsz,\
          \ seqlen)\n> if prefix_mask is not None:\n>     bsz = query.size(0)\n> \
          \    causal_mask = causal_mask.repeat(bsz, 1, 1, 1) # (bsz, 1, src_len,\
          \ tgt_len)\n>     causal_mask = causal_mask.permute(0, 3, 1, 2) # (bsz,\
          \ tgt_len, 1, src_len)\n>     causal_mask[prefix_mask.bool()] = 1\n>   \
          \  causal_mask = causal_mask.permute(0, 2, 3, 1) # (bsz, 1, src_len, tgt_len)\n\
          > ```\n\nWould it be possible to just pass the attention_mask in the forward\
          \ pass during training.\nIn such a way that is possible to train prefix\
          \ style without having to change the underlying code?\nThank you!"
        updatedAt: '2023-03-13T10:56:37.960Z'
      numEdits: 2
      reactions: []
    id: 640f0147b99903b2a4425b21
    type: comment
  author: JacopoBandoni
  content: "@juewang\n> Hi @asifhugs, a quick fix is to reset the causal mask after\
    \ loading the trained model, e.g.:\n> ```python\n> model = ...\n> max_positions\
    \ = 2048\n> for i in range(len(model.transformer.h)):\n>     model.transformer.h[i].attn.bias[:]\
    \ = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(\n\
    >         1, 1, max_positions, max_positions\n>     )\n> ``` \n> After doing this,\
    \ this model becomes a **pure causal language model**.\n> \n> If you want to keep\
    \ a PrefixLM-style training. You should pass a `prefix_mask`as an argument to\
    \ tell the model which part is the prefix/prompt, and write your custom model\
    \ class to let the model attend to the whole context of prompt. For example, we\
    \ can insert the following code after [here](https://github.com/huggingface/transformers/blob/59d5edef34ae0fa56065a2e863736d4f133c558b/src/transformers/models/gptj/modeling_gptj.py#L158)\
    \ :\n> ```\n> # `prefix_mask` is passed as an argument with a shape of (bsz, seqlen)\n\
    > if prefix_mask is not None:\n>     bsz = query.size(0)\n>     causal_mask =\
    \ causal_mask.repeat(bsz, 1, 1, 1) # (bsz, 1, src_len, tgt_len)\n>     causal_mask\
    \ = causal_mask.permute(0, 3, 1, 2) # (bsz, tgt_len, 1, src_len)\n>     causal_mask[prefix_mask.bool()]\
    \ = 1\n>     causal_mask = causal_mask.permute(0, 2, 3, 1) # (bsz, 1, src_len,\
    \ tgt_len)\n> ```\n\nWould it be possible to just pass the attention_mask in the\
    \ forward pass during training.\nIn such a way that is possible to train prefix\
    \ style without having to change the underlying code?\nThank you!"
  created_at: 2023-03-13 09:56:07+00:00
  edited: true
  hidden: false
  id: 640f0147b99903b2a4425b21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-03-16T15:30:26.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;JacopoBandoni&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/JacopoBandoni\"\
          >@<span class=\"underline\">JacopoBandoni</span></a></span>\n\n\t</span></span>\
          \  I am afraid no.. <code>attention_mask</code> is used to indicate padding\
          \ tokens, which should be masked; <code>prefix_mask</code> is used to indicate\
          \ the bidirectional context.<br>You might want to have a look at this as\
          \ a reference for fine-tuning :)</p>\n<ul>\n<li><a rel=\"nofollow\" href=\"\
          https://github.com/togethercomputer/OpenChatKit/blob/main/training/dist_prefixlm_train.py\"\
          >https://github.com/togethercomputer/OpenChatKit/blob/main/training/dist_prefixlm_train.py</a></li>\n\
          <li><a rel=\"nofollow\" href=\"https://github.com/togethercomputer/OpenChatKit/blob/a6f652882fa87ee69dc011b74a52c0064f169f3d/training/modules/hf_gptj_modules.py#L101-L106\"\
          >https://github.com/togethercomputer/OpenChatKit/blob/a6f652882fa87ee69dc011b74a52c0064f169f3d/training/modules/hf_gptj_modules.py#L101-L106</a></li>\n\
          </ul>\n"
        raw: '@JacopoBandoni  I am afraid no.. `attention_mask` is used to indicate
          padding tokens, which should be masked; `prefix_mask` is used to indicate
          the bidirectional context.

          You might want to have a look at this as a reference for fine-tuning :)

          - https://github.com/togethercomputer/OpenChatKit/blob/main/training/dist_prefixlm_train.py

          - https://github.com/togethercomputer/OpenChatKit/blob/a6f652882fa87ee69dc011b74a52c0064f169f3d/training/modules/hf_gptj_modules.py#L101-L106'
        updatedAt: '2023-03-16T15:30:26.118Z'
      numEdits: 0
      reactions: []
    id: 64133612b0913410906feeac
    type: comment
  author: juewang
  content: '@JacopoBandoni  I am afraid no.. `attention_mask` is used to indicate
    padding tokens, which should be masked; `prefix_mask` is used to indicate the
    bidirectional context.

    You might want to have a look at this as a reference for fine-tuning :)

    - https://github.com/togethercomputer/OpenChatKit/blob/main/training/dist_prefixlm_train.py

    - https://github.com/togethercomputer/OpenChatKit/blob/a6f652882fa87ee69dc011b74a52c0064f169f3d/training/modules/hf_gptj_modules.py#L101-L106'
  created_at: 2023-03-16 14:30:26+00:00
  edited: false
  hidden: false
  id: 64133612b0913410906feeac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669879497278-6261c62c0568e418d693090a.png?w=200&h=200&f=face
      fullname: Asif Ahmad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asifhugs
      type: user
    createdAt: '2023-03-16T15:32:04.000Z'
    data:
      edited: false
      editors:
      - asifhugs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669879497278-6261c62c0568e418d693090a.png?w=200&h=200&f=face
          fullname: Asif Ahmad
          isHf: false
          isPro: false
          name: asifhugs
          type: user
        html: "<blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;asifhugs&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/asifhugs\"\
          >@<span class=\"underline\">asifhugs</span></a></span>\n\n\t</span></span>,\
          \ did you succeed in training?</p>\n</blockquote>\n<p>Hi, no not yet!</p>\n"
        raw: '> Hi @asifhugs, did you succeed in training?


          Hi, no not yet!'
        updatedAt: '2023-03-16T15:32:04.941Z'
      numEdits: 0
      reactions: []
    id: 64133674ce07729a7cf9ee45
    type: comment
  author: asifhugs
  content: '> Hi @asifhugs, did you succeed in training?


    Hi, no not yet!'
  created_at: 2023-03-16 14:32:04+00:00
  edited: false
  hidden: false
  id: 64133674ce07729a7cf9ee45
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: togethercomputer/GPT-JT-6B-v1
repo_type: model
status: open
target_branch: null
title: Generated Text have issues
