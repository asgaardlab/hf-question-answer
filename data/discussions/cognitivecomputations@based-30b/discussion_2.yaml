!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ARIRIGI
conflicting_files: null
created_at: 2023-06-05 11:23:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63452b1d1f995effab827ee0/UZdDZFsN3fUB2sxjgnrOL.png?w=200&h=200&f=face
      fullname: Jin Zewen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ARIRIGI
      type: user
    createdAt: '2023-06-05T12:23:50.000Z'
    data:
      edited: false
      editors:
      - ARIRIGI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9718746542930603
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63452b1d1f995effab827ee0/UZdDZFsN3fUB2sxjgnrOL.png?w=200&h=200&f=face
          fullname: Jin Zewen
          isHf: false
          isPro: false
          name: ARIRIGI
          type: user
        html: '<p>This is an excellent work, thank you for sharing the results of
          what you have done.<br>However, after reviewing the dataset, I had a thought:
          Could the original model achieve similar results?<br>In the paper of the
          LIMA model published by Meta AI before, it was concluded that most of the
          model''s knowledge is learned during the pre-training phase, and subsequent
          instruction learning and reinforcement learning are used "to better align
          to end tasks and user preferences". So, I think your work is essentially
          adjusting this alignment, rather than adding new knowledge.<br>Based on
          this idea, I believe it might be possible to achieve similar effects simply
          by adjusting the prompts. I immediately thought of the prompt I saw before
          for launching chagpt in "Developer Mode" . Although OpenAI has imposed many
          restrictions on this, we can still remove some restrictions through similar
          prompts. Here''s an example I am sharing, with a question adopted from one
          of the questions in your dataset:<br><a rel="nofollow" href="https://chat.openai.com/share/8ec1dde0-2101-47bd-8c1a-f10a538be9c9">https://chat.openai.com/share/8ec1dde0-2101-47bd-8c1a-f10a538be9c9</a><br>My
          personal feeling is that the results obtained are somewhat similar to the
          effects that the datasets trying to demonstrate, which seems to suggest
          that the effect of fine-tuning is to replace similar prompts.<br>Based on
          this observation, I thought that perhaps we could use such prompts to obtain
          a large number of normal outputs and human mode outputs from chatgpt, and
          then build a dataset for comparison between the two kinds of responses,
          and finally use it for instruction learning. Anyway, as someone who has
          just entered this field recently, I''m not sure whether anyone has done
          this before, and I don''t have a dataset to carry out the experiment myself,
          so I wonder whether you think this idea makes sense and is feasible?</p>

          '
        raw: "This is an excellent work, thank you for sharing the results of what\
          \ you have done. \r\nHowever, after reviewing the dataset, I had a thought:\
          \ Could the original model achieve similar results? \r\nIn the paper of\
          \ the LIMA model published by Meta AI before, it was concluded that most\
          \ of the model's knowledge is learned during the pre-training phase, and\
          \ subsequent instruction learning and reinforcement learning are used \"\
          to better align to end tasks and user preferences\". So, I think your work\
          \ is essentially adjusting this alignment, rather than adding new knowledge.\
          \ \r\nBased on this idea, I believe it might be possible to achieve similar\
          \ effects simply by adjusting the prompts. I immediately thought of the\
          \ prompt I saw before for launching chagpt in \"Developer Mode\" . Although\
          \ OpenAI has imposed many restrictions on this, we can still remove some\
          \ restrictions through similar prompts. Here's an example I am sharing,\
          \ with a question adopted from one of the questions in your dataset: \r\n\
          https://chat.openai.com/share/8ec1dde0-2101-47bd-8c1a-f10a538be9c9\r\nMy\
          \ personal feeling is that the results obtained are somewhat similar to\
          \ the effects that the datasets trying to demonstrate, which seems to suggest\
          \ that the effect of fine-tuning is to replace similar prompts.\r\nBased\
          \ on this observation, I thought that perhaps we could use such prompts\
          \ to obtain a large number of normal outputs and human mode outputs from\
          \ chatgpt, and then build a dataset for comparison between the two kinds\
          \ of responses, and finally use it for instruction learning. Anyway, as\
          \ someone who has just entered this field recently, I'm not sure whether\
          \ anyone has done this before, and I don't have a dataset to carry out the\
          \ experiment myself, so I wonder whether you think this idea makes sense\
          \ and is feasible?"
        updatedAt: '2023-06-05T12:23:50.231Z'
      numEdits: 0
      reactions: []
    id: 647dd3d611084fb583197c15
    type: comment
  author: ARIRIGI
  content: "This is an excellent work, thank you for sharing the results of what you\
    \ have done. \r\nHowever, after reviewing the dataset, I had a thought: Could\
    \ the original model achieve similar results? \r\nIn the paper of the LIMA model\
    \ published by Meta AI before, it was concluded that most of the model's knowledge\
    \ is learned during the pre-training phase, and subsequent instruction learning\
    \ and reinforcement learning are used \"to better align to end tasks and user\
    \ preferences\". So, I think your work is essentially adjusting this alignment,\
    \ rather than adding new knowledge. \r\nBased on this idea, I believe it might\
    \ be possible to achieve similar effects simply by adjusting the prompts. I immediately\
    \ thought of the prompt I saw before for launching chagpt in \"Developer Mode\"\
    \ . Although OpenAI has imposed many restrictions on this, we can still remove\
    \ some restrictions through similar prompts. Here's an example I am sharing, with\
    \ a question adopted from one of the questions in your dataset: \r\nhttps://chat.openai.com/share/8ec1dde0-2101-47bd-8c1a-f10a538be9c9\r\
    \nMy personal feeling is that the results obtained are somewhat similar to the\
    \ effects that the datasets trying to demonstrate, which seems to suggest that\
    \ the effect of fine-tuning is to replace similar prompts.\r\nBased on this observation,\
    \ I thought that perhaps we could use such prompts to obtain a large number of\
    \ normal outputs and human mode outputs from chatgpt, and then build a dataset\
    \ for comparison between the two kinds of responses, and finally use it for instruction\
    \ learning. Anyway, as someone who has just entered this field recently, I'm not\
    \ sure whether anyone has done this before, and I don't have a dataset to carry\
    \ out the experiment myself, so I wonder whether you think this idea makes sense\
    \ and is feasible?"
  created_at: 2023-06-05 11:23:50+00:00
  edited: false
  hidden: false
  id: 647dd3d611084fb583197c15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-06-05T16:42:35.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9666562080383301
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>its different - if you apply an alignment and then later convince
          it to ignore it.  It loses something in the process.<br>It''s like if you
          cut off someones hand and then replace it with a robot hand.</p>

          <p>Also - my goal isn''t to make it "pretend to have opinions"</p>

          <p>My goal is to reveal its "actual" opinions.</p>

          '
        raw: 'its different - if you apply an alignment and then later convince it
          to ignore it.  It loses something in the process.

          It''s like if you cut off someones hand and then replace it with a robot
          hand.


          Also - my goal isn''t to make it "pretend to have opinions"


          My goal is to reveal its "actual" opinions.'
        updatedAt: '2023-06-05T16:42:35.483Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Joseph717171
    id: 647e107bf14eafc3b4568c3c
    type: comment
  author: ehartford
  content: 'its different - if you apply an alignment and then later convince it to
    ignore it.  It loses something in the process.

    It''s like if you cut off someones hand and then replace it with a robot hand.


    Also - my goal isn''t to make it "pretend to have opinions"


    My goal is to reveal its "actual" opinions.'
  created_at: 2023-06-05 15:42:35+00:00
  edited: false
  hidden: false
  id: 647e107bf14eafc3b4568c3c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: cognitivecomputations/based-30b
repo_type: model
status: open
target_branch: null
title: Great work and here's my personal thoughts
