!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hoioi
conflicting_files: null
created_at: 2023-12-10 16:08:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
      fullname: Hut hiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hoioi
      type: user
    createdAt: '2023-12-10T16:08:27.000Z'
    data:
      edited: false
      editors:
      - Hoioi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9251624345779419
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
          fullname: Hut hiu
          isHf: false
          isPro: false
          name: Hoioi
          type: user
        html: '<p>I''m not sure if there''s something wrong with the original model
          or the quantized version, but it generates &lt;0x0A&gt; instead of a  going
          to the new line. </p>

          '
        raw: 'I''m not sure if there''s something wrong with the original model or
          the quantized version, but it generates <0x0A> instead of a  going to the
          new line. '
        updatedAt: '2023-12-10T16:08:27.575Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - hppdqdq
        - zhibor
        - lixbo
        - mikestaub
    id: 6575e27bf9898ed3ab9c3f98
    type: comment
  author: Hoioi
  content: 'I''m not sure if there''s something wrong with the original model or the
    quantized version, but it generates <0x0A> instead of a  going to the new line. '
  created_at: 2023-12-10 16:08:27+00:00
  edited: false
  hidden: false
  id: 6575e27bf9898ed3ab9c3f98
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-12-10T17:24:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9548134803771973
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK thanks. I''m also not sure if this is because of the original
          model, or a bug in the code I need to use to make it.  </p>

          <p>Because the original model doesn''t include a tokenizer.model, I need
          to use a llama.cpp PR that can make GGUF from tokenizer.json.  And it currently
          seems to have this bug.</p>

          <p>I will see if I can find a tokenizer.model for this model and do it that
          way instead.</p>

          '
        raw: "OK thanks. I'm also not sure if this is because of the original model,\
          \ or a bug in the code I need to use to make it.  \n\nBecause the original\
          \ model doesn't include a tokenizer.model, I need to use a llama.cpp PR\
          \ that can make GGUF from tokenizer.json.  And it currently seems to have\
          \ this bug.\n\nI will see if I can find a tokenizer.model for this model\
          \ and do it that way instead."
        updatedAt: '2023-12-10T17:24:22.344Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Hoioi
    id: 6575f4466da136b50fc3c5be
    type: comment
  author: TheBloke
  content: "OK thanks. I'm also not sure if this is because of the original model,\
    \ or a bug in the code I need to use to make it.  \n\nBecause the original model\
    \ doesn't include a tokenizer.model, I need to use a llama.cpp PR that can make\
    \ GGUF from tokenizer.json.  And it currently seems to have this bug.\n\nI will\
    \ see if I can find a tokenizer.model for this model and do it that way instead."
  created_at: 2023-12-10 17:24:22+00:00
  edited: false
  hidden: false
  id: 6575f4466da136b50fc3c5be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-12-10T18:34:16.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9667060971260071
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''ve re-created the GGUFs for this model and v1 and newlines are
          now generated correctly.</p>

          '
        raw: I've re-created the GGUFs for this model and v1 and newlines are now
          generated correctly.
        updatedAt: '2023-12-10T18:34:26.722Z'
      numEdits: 1
      reactions:
      - count: 6
        reaction: "\U0001F917"
        users:
        - lixbo
        - HR1777
        - zhibor
        - Hoioi
        - jlzhou
        - SiriusRU
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Hoioi
        - jlzhou
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - hppdqdq
    id: 657604a84fffc3f08b85bca9
    type: comment
  author: TheBloke
  content: I've re-created the GGUFs for this model and v1 and newlines are now generated
    correctly.
  created_at: 2023-12-10 18:34:16+00:00
  edited: true
  hidden: false
  id: 657604a84fffc3f08b85bca9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b0389d0bcec73364c6583e33f10f70b7.svg
      fullname: linbo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lixbo
      type: user
    createdAt: '2023-12-10T18:51:58.000Z'
    data:
      edited: false
      editors:
      - lixbo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8628047108650208
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b0389d0bcec73364c6583e33f10f70b7.svg
          fullname: linbo
          isHf: false
          isPro: false
          name: lixbo
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> hey man, please\
          \ don't forget to quantize this model <a href=\"https://huggingface.co/Intel/neural-chat-7b-v3-3\"\
          >neural-chat-7b-v3-3</a> please.</p>\n"
        raw: '@TheBloke hey man, please don''t forget to quantize this model [neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)
          please.'
        updatedAt: '2023-12-10T18:51:58.151Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hppdqdq
    id: 657608ce8b44ef012b76c3ac
    type: comment
  author: lixbo
  content: '@TheBloke hey man, please don''t forget to quantize this model [neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)
    please.'
  created_at: 2023-12-10 18:51:58+00:00
  edited: false
  hidden: false
  id: 657608ce8b44ef012b76c3ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
      fullname: Hut hiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hoioi
      type: user
    createdAt: '2023-12-10T19:29:25.000Z'
    data:
      edited: false
      editors:
      - Hoioi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9412652850151062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
          fullname: Hut hiu
          isHf: false
          isPro: false
          name: Hoioi
          type: user
        html: '<p>Thank you so much for your fast support! </p>

          '
        raw: 'Thank you so much for your fast support! '
        updatedAt: '2023-12-10T19:29:25.316Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hppdqdq
    id: 6576119554d17496120c3336
    type: comment
  author: Hoioi
  content: 'Thank you so much for your fast support! '
  created_at: 2023-12-10 19:29:25+00:00
  edited: false
  hidden: false
  id: 6576119554d17496120c3336
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/go-bruins-v2-GGUF
repo_type: model
status: open
target_branch: null
title: It generates <0x0A> instead of new line
