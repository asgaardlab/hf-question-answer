!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brucethemoose
conflicting_files: null
created_at: 2023-12-06 03:29:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-06T03:29:24.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9595953226089478
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Would you consider training on the 200K model instead of base Yi?
          Even if the training context is much shorter, some of the long context performance
          seems to be preserved.</p>

          <p>Also, is this a Lora or a native fintune? If the former, could you post
          the lora?</p>

          '
        raw: 'Would you consider training on the 200K model instead of base Yi? Even
          if the training context is much shorter, some of the long context performance
          seems to be preserved.


          Also, is this a Lora or a native fintune? If the former, could you post
          the lora?'
        updatedAt: '2023-12-06T03:29:56.418Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - distantquant
        - dirkson
        - adamo1139
    id: 656fea9415709277eddf116b
    type: comment
  author: brucethemoose
  content: 'Would you consider training on the 200K model instead of base Yi? Even
    if the training context is much shorter, some of the long context performance
    seems to be preserved.


    Also, is this a Lora or a native fintune? If the former, could you post the lora?'
  created_at: 2023-12-06 03:29:24+00:00
  edited: true
  hidden: false
  id: 656fea9415709277eddf116b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6401c8c9f98fbc64bcd7dca1/MOSgc_mPbfUZ-354osy1v.png?w=200&h=200&f=face
      fullname: FBL
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: fblgit
      type: user
    createdAt: '2023-12-12T14:36:39.000Z'
    data:
      edited: false
      editors:
      - fblgit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8553890585899353
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6401c8c9f98fbc64bcd7dca1/MOSgc_mPbfUZ-354osy1v.png?w=200&h=200&f=face
          fullname: FBL
          isHf: false
          isPro: true
          name: fblgit
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;brucethemoose&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/brucethemoose\"\
          >@<span class=\"underline\">brucethemoose</span></a></span>\n\n\t</span></span><br><a\
          \ href=\"https://huggingface.co/01-ai/Yi-34B-Chat/discussions/12\">Raised</a><br>If\
          \ u share the train script for the 200K i can give it a shot as it is right\
          \ now, im not sure how to expand such a context.. the limit is 8xH100..\
          \ if thats not enough than I wont be able to run it.</p>\n"
        raw: "@brucethemoose \n[Raised](https://huggingface.co/01-ai/Yi-34B-Chat/discussions/12)\n\
          If u share the train script for the 200K i can give it a shot as it is right\
          \ now, im not sure how to expand such a context.. the limit is 8xH100..\
          \ if thats not enough than I wont be able to run it."
        updatedAt: '2023-12-12T14:36:39.886Z'
      numEdits: 0
      reactions: []
    id: 65786ff76014c0727c69ac65
    type: comment
  author: fblgit
  content: "@brucethemoose \n[Raised](https://huggingface.co/01-ai/Yi-34B-Chat/discussions/12)\n\
    If u share the train script for the 200K i can give it a shot as it is right now,\
    \ im not sure how to expand such a context.. the limit is 8xH100.. if thats not\
    \ enough than I wont be able to run it."
  created_at: 2023-12-12 14:36:39+00:00
  edited: false
  hidden: false
  id: 65786ff76014c0727c69ac65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-12T15:53:49.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9845808148384094
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Oh it doesn''t have to be trained natively at 200k, training at
          lower context still preserves some of the higher context.</p>

          <p>That being said, the training repo you want is probably unsloth, which
          now has a DPO script and should save quite a bit of VRAM.</p>

          '
        raw: 'Oh it doesn''t have to be trained natively at 200k, training at lower
          context still preserves some of the higher context.


          That being said, the training repo you want is probably unsloth, which now
          has a DPO script and should save quite a bit of VRAM.'
        updatedAt: '2023-12-12T15:53:49.464Z'
      numEdits: 0
      reactions: []
    id: 6578820d1e4ac8627fadb49b
    type: comment
  author: brucethemoose
  content: 'Oh it doesn''t have to be trained natively at 200k, training at lower
    context still preserves some of the higher context.


    That being said, the training repo you want is probably unsloth, which now has
    a DPO script and should save quite a bit of VRAM.'
  created_at: 2023-12-12 15:53:49+00:00
  edited: false
  hidden: false
  id: 6578820d1e4ac8627fadb49b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-12T20:39:31.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8562729358673096
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Also, see this concise PEFT issue for LongLora, for higher quality
          training at long context: </p>

          <p><a rel="nofollow" href="https://github.com/huggingface/peft/issues/958">https://github.com/huggingface/peft/issues/958</a></p>

          <p>TBH I have no idea what Yi did on their end to train at such an extreme
          context.</p>

          '
        raw: "Also, see this concise PEFT issue for LongLora, for higher quality training\
          \ at long context: \n\nhttps://github.com/huggingface/peft/issues/958\n\n\
          TBH I have no idea what Yi did on their end to train at such an extreme\
          \ context."
        updatedAt: '2023-12-12T20:41:22.732Z'
      numEdits: 1
      reactions: []
    id: 6578c503224758a1fc908b8b
    type: comment
  author: brucethemoose
  content: "Also, see this concise PEFT issue for LongLora, for higher quality training\
    \ at long context: \n\nhttps://github.com/huggingface/peft/issues/958\n\nTBH I\
    \ have no idea what Yi did on their end to train at such an extreme context."
  created_at: 2023-12-12 20:39:31+00:00
  edited: true
  hidden: false
  id: 6578c503224758a1fc908b8b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: fblgit/una-xaberius-34b-v1beta
repo_type: model
status: open
target_branch: null
title: 200K?
