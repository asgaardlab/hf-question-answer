!!python/object:huggingface_hub.community.DiscussionWithDetails
author: viral1008
conflicting_files: null
created_at: 2023-11-16 03:44:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cbab98b916957da8bbcb6910b04ebbf3.svg
      fullname: Viral Sherathiya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viral1008
      type: user
    createdAt: '2023-11-16T03:44:49.000Z'
    data:
      edited: false
      editors:
      - viral1008
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.19282886385917664
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cbab98b916957da8bbcb6910b04ebbf3.svg
          fullname: Viral Sherathiya
          isHf: false
          isPro: false
          name: viral1008
          type: user
        html: "<p>Can Anyone Help me with Prompt Template</p>\n<p>from langchain.memory\
          \ import ConversationBufferMemory<br>from langchain.prompts import PromptTemplate</p>\n\
          <p>def get_prompt_template(promptTemplate_type=None, history=False):</p>\n\
          <pre><code>prompt_template = \"Verwenden Sie die folgenden Kontextelemente,\
          \ um die Frage am Ende zu beantworten. Wenn Sie die Antwort nicht kennen,\
          \ sagen Sie einfach, dass Sie es nicht wissen, und versuchen Sie nicht,\
          \ eine Antwort zu erfinden. USER: {instruction} ASSISTANT:\"\n\nprompt =\
          \ PromptTemplate(\n    template=prompt_template, input_variables=[\"instruction\"\
          ]\n)\n\nmemory = ConversationBufferMemory(input_key=\"instruction\", memory_key=\"\
          history\")\n\nreturn (\n    prompt,\n    memory,\n)\n</code></pre>\n<p>|<br>error</p>\n\
          <p>3 = 0 | VSX = 0 |<br>ERROR:app:Exception on /ChatBotAPI/get-response-from-chatbot\
          \ [POST]<br>Traceback (most recent call last):<br>  File \"D:\\Zoo_Chatbot\\\
          venv\\Lib\\site-packages\\flask\\app.py\", line 1455, in wsgi_app<br>  \
          \  response = self.full_dispatch_request()<br>               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>\
          \  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\flask\\app.py\", line\
          \ 869, in full_dispatch_request<br>    rv = self.handle_user_exception(e)<br>\
          \         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File \"D:\\Zoo_Chatbot\\venv\\\
          Lib\\site-packages\\flask_cors\\extension.py\", line 176, in wrapped_function<br>\
          \    return cors_after_request(app.make_response(f(*args, **kwargs)))<br>\
          \                                                ^^^^^^^^^^^^^^^^^^<br>\
          \  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\flask\\app.py\", line\
          \ 867, in full_dispatch_request<br>    rv = self.dispatch_request()<br>\
          \         ^^^^^^^^^^^^^^^^^^^^^^^<br>  File \"D:\\Zoo_Chatbot\\venv\\Lib\\\
          site-packages\\flask\\app.py\", line 852, in dispatch_request<br>    return\
          \ self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)<br>\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>\
          \  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\flask_cors\\decorator.py\"\
          , line 130, in wrapped_function<br>    resp = make_response(f(*args, **kwargs))<br>\
          \                         ^^^^^^^^^^^^^^^^^^<br>  File \"D:\\Zoo_Chatbot\\\
          app.py\", line 18, in get_response_from_chatbot<br>    response = main(user_input)<br>\
          \               ^^^^^^^^^^^^^^^^<br>  File \"D:\\Zoo_Chatbot\\run_localGPT.py\"\
          , line 119, in main<br>    QA = RetrievalQA.from_chain_type(<br>       \
          \  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File \"D:\\Zoo_Chatbot\\venv\\Lib\\\
          site-packages\\langchain\\chains\\retrieval_qa\\base.py\", line 100, in\
          \ from_chain_type<br>    combine_documents_chain = load_qa_chain(<br>  \
          \                            ^^^^^^^^^^^^^^<br>  File \"D:\\Zoo_Chatbot\\\
          venv\\Lib\\site-packages\\langchain\\chains\\question_answering_<em>init</em>_.py\"\
          , line 249, in load_qa_chain<br>    return loader_mapping[chain_type](<br>\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File \"D:\\Zoo_Chatbot\\venv\\\
          Lib\\site-packages\\langchain\\chains\\question_answering__init__.py\",\
          \ line 73, in _load_stuff_chain<br>    llm_chain = LLMChain(<br>       \
          \         ^^^^^^^^^<br>  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\\
          langchain\\load\\serializable.py\", line 74, in __init__<br>    super().<strong>init</strong>(**kwargs)<br>\
          \  File \"pydantic\\main.py\", line 341, in pydantic.main.BaseModel.<strong>init</strong><br>pydantic.error_wrappers.ValidationError:\
          \ 1 validation error for LLMChain<br>prompt<br>  value is not a valid dict\
          \ (type=type_error.dict)</p>\n"
        raw: "Can Anyone Help me with Prompt Template\r\n\r\nfrom langchain.memory\
          \ import ConversationBufferMemory\r\nfrom langchain.prompts import PromptTemplate\r\
          \n\r\ndef get_prompt_template(promptTemplate_type=None, history=False):\r\
          \n    \r\n    prompt_template = \"Verwenden Sie die folgenden Kontextelemente,\
          \ um die Frage am Ende zu beantworten. Wenn Sie die Antwort nicht kennen,\
          \ sagen Sie einfach, dass Sie es nicht wissen, und versuchen Sie nicht,\
          \ eine Antwort zu erfinden. USER: {instruction} ASSISTANT:\"\r\n    \r\n\
          \    prompt = PromptTemplate(\r\n        template=prompt_template, input_variables=[\"\
          instruction\"]\r\n    )\r\n    \r\n    memory = ConversationBufferMemory(input_key=\"\
          instruction\", memory_key=\"history\")\r\n\r\n    return (\r\n        prompt,\r\
          \n        memory,\r\n    )\r\n|\r\nerror\r\n\r\n3 = 0 | VSX = 0 |\r\nERROR:app:Exception\
          \ on /ChatBotAPI/get-response-from-chatbot [POST]\r\nTraceback (most recent\
          \ call last):\r\n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\flask\\\
          app.py\", line 1455, in wsgi_app\r\n    response = self.full_dispatch_request()\r\
          \n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\\
          venv\\Lib\\site-packages\\flask\\app.py\", line 869, in full_dispatch_request\r\
          \n    rv = self.handle_user_exception(e)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\flask_cors\\extension.py\"\
          , line 176, in wrapped_function\r\n    return cors_after_request(app.make_response(f(*args,\
          \ **kwargs)))\r\n                                                ^^^^^^^^^^^^^^^^^^\r\
          \n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\flask\\app.py\", line\
          \ 867, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n\
          \         ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\\
          site-packages\\flask\\app.py\", line 852, in dispatch_request\r\n    return\
          \ self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\r\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\flask_cors\\decorator.py\"\
          , line 130, in wrapped_function\r\n    resp = make_response(f(*args, **kwargs))\r\
          \n                         ^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\\
          app.py\", line 18, in get_response_from_chatbot\r\n    response = main(user_input)\r\
          \n               ^^^^^^^^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\run_localGPT.py\"\
          , line 119, in main\r\n    QA = RetrievalQA.from_chain_type(\r\n       \
          \  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\\
          site-packages\\langchain\\chains\\retrieval_qa\\base.py\", line 100, in\
          \ from_chain_type\r\n    combine_documents_chain = load_qa_chain(\r\n  \
          \                            ^^^^^^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\\
          venv\\Lib\\site-packages\\langchain\\chains\\question_answering\\__init__.py\"\
          , line 249, in load_qa_chain\r\n    return loader_mapping[chain_type](\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\venv\\\
          Lib\\site-packages\\langchain\\chains\\question_answering\\__init__.py\"\
          , line 73, in _load_stuff_chain\r\n    llm_chain = LLMChain(\r\n       \
          \         ^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\\
          langchain\\load\\serializable.py\", line 74, in __init__\r\n    super().__init__(**kwargs)\r\
          \n  File \"pydantic\\main.py\", line 341, in pydantic.main.BaseModel.__init__\r\
          \npydantic.error_wrappers.ValidationError: 1 validation error for LLMChain\r\
          \nprompt\r\n  value is not a valid dict (type=type_error.dict)\r\n\r\n"
        updatedAt: '2023-11-16T03:44:49.384Z'
      numEdits: 0
      reactions: []
    id: 65559031a0c34cd61a59a8dc
    type: comment
  author: viral1008
  content: "Can Anyone Help me with Prompt Template\r\n\r\nfrom langchain.memory import\
    \ ConversationBufferMemory\r\nfrom langchain.prompts import PromptTemplate\r\n\
    \r\ndef get_prompt_template(promptTemplate_type=None, history=False):\r\n    \r\
    \n    prompt_template = \"Verwenden Sie die folgenden Kontextelemente, um die\
    \ Frage am Ende zu beantworten. Wenn Sie die Antwort nicht kennen, sagen Sie einfach,\
    \ dass Sie es nicht wissen, und versuchen Sie nicht, eine Antwort zu erfinden.\
    \ USER: {instruction} ASSISTANT:\"\r\n    \r\n    prompt = PromptTemplate(\r\n\
    \        template=prompt_template, input_variables=[\"instruction\"]\r\n    )\r\
    \n    \r\n    memory = ConversationBufferMemory(input_key=\"instruction\", memory_key=\"\
    history\")\r\n\r\n    return (\r\n        prompt,\r\n        memory,\r\n    )\r\
    \n|\r\nerror\r\n\r\n3 = 0 | VSX = 0 |\r\nERROR:app:Exception on /ChatBotAPI/get-response-from-chatbot\
    \ [POST]\r\nTraceback (most recent call last):\r\n  File \"D:\\Zoo_Chatbot\\venv\\\
    Lib\\site-packages\\flask\\app.py\", line 1455, in wsgi_app\r\n    response =\
    \ self.full_dispatch_request()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\flask\\app.py\", line 869,\
    \ in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n     \
    \    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\\
    flask_cors\\extension.py\", line 176, in wrapped_function\r\n    return cors_after_request(app.make_response(f(*args,\
    \ **kwargs)))\r\n                                                ^^^^^^^^^^^^^^^^^^\r\
    \n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\flask\\app.py\", line 867,\
    \ in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n         ^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\flask\\app.py\", line 852,\
    \ in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\flask_cors\\decorator.py\"\
    , line 130, in wrapped_function\r\n    resp = make_response(f(*args, **kwargs))\r\
    \n                         ^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\app.py\"\
    , line 18, in get_response_from_chatbot\r\n    response = main(user_input)\r\n\
    \               ^^^^^^^^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\run_localGPT.py\"\
    , line 119, in main\r\n    QA = RetrievalQA.from_chain_type(\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\langchain\\chains\\retrieval_qa\\\
    base.py\", line 100, in from_chain_type\r\n    combine_documents_chain = load_qa_chain(\r\
    \n                              ^^^^^^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\venv\\\
    Lib\\site-packages\\langchain\\chains\\question_answering\\__init__.py\", line\
    \ 249, in load_qa_chain\r\n    return loader_mapping[chain_type](\r\n        \
    \   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\venv\\Lib\\site-packages\\\
    langchain\\chains\\question_answering\\__init__.py\", line 73, in _load_stuff_chain\r\
    \n    llm_chain = LLMChain(\r\n                ^^^^^^^^^\r\n  File \"D:\\Zoo_Chatbot\\\
    venv\\Lib\\site-packages\\langchain\\load\\serializable.py\", line 74, in __init__\r\
    \n    super().__init__(**kwargs)\r\n  File \"pydantic\\main.py\", line 341, in\
    \ pydantic.main.BaseModel.__init__\r\npydantic.error_wrappers.ValidationError:\
    \ 1 validation error for LLMChain\r\nprompt\r\n  value is not a valid dict (type=type_error.dict)\r\
    \n\r\n"
  created_at: 2023-11-16 03:44:49+00:00
  edited: false
  hidden: false
  id: 65559031a0c34cd61a59a8dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/cbab98b916957da8bbcb6910b04ebbf3.svg
      fullname: Viral Sherathiya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viral1008
      type: user
    createdAt: '2023-11-16T04:56:53.000Z'
    data:
      from: Prompt Templatee
      to: Prompt Template
    id: 6555a1155f7b57f5851e5a2f
    type: title-change
  author: viral1008
  created_at: 2023-11-16 04:56:53+00:00
  id: 6555a1155f7b57f5851e5a2f
  new_title: Prompt Template
  old_title: Prompt Templatee
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/em_german_leo_mistral-GGUF
repo_type: model
status: open
target_branch: null
title: Prompt Template
