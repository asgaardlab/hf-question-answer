!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nexesenex
conflicting_files: null
created_at: 2023-12-30 12:11:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-12-30T12:11:26.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.815358579158783
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Hey Perlthoughts.</p>

          <p>First, congrats for this successful MOE setup. I made an exl2 6.0bpw
          quant and tested the perplexity at 512 tokens, and :</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/CdMNnk4TIZpFcBlWmyJos.png"><img
          alt="Screenshot 2023-12-30 at 13-08-43 Text generation web UI.png" src="https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/CdMNnk4TIZpFcBlWmyJos.png"></a></p>

          <p>Your model doesn''t alter the perplexity negatively.</p>

          <p>Then, beyond this achievement, does such a MOE bring benefit compared
          to the original model Mistral Instruct v0.2, if the weights of each experts
          are the same?</p>

          '
        raw: 'Hey Perlthoughts.


          First, congrats for this successful MOE setup. I made an exl2 6.0bpw quant
          and tested the perplexity at 512 tokens, and :


          ![Screenshot 2023-12-30 at 13-08-43 Text generation web UI.png](https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/CdMNnk4TIZpFcBlWmyJos.png)


          Your model doesn''t alter the perplexity negatively.


          Then, beyond this achievement, does such a MOE bring benefit compared to
          the original model Mistral Instruct v0.2, if the weights of each experts
          are the same?

          '
        updatedAt: '2023-12-30T12:42:28.950Z'
      numEdits: 1
      reactions: []
    id: 659008ee35331883752daed6
    type: comment
  author: Nexesenex
  content: 'Hey Perlthoughts.


    First, congrats for this successful MOE setup. I made an exl2 6.0bpw quant and
    tested the perplexity at 512 tokens, and :


    ![Screenshot 2023-12-30 at 13-08-43 Text generation web UI.png](https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/CdMNnk4TIZpFcBlWmyJos.png)


    Your model doesn''t alter the perplexity negatively.


    Then, beyond this achievement, does such a MOE bring benefit compared to the original
    model Mistral Instruct v0.2, if the weights of each experts are the same?

    '
  created_at: 2023-12-30 12:11:26+00:00
  edited: true
  hidden: false
  id: 659008ee35331883752daed6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
      fullname: Ray Hernandez
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: perlthoughts
      type: user
    createdAt: '2023-12-30T13:32:24.000Z'
    data:
      edited: false
      editors:
      - perlthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9604950547218323
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
          fullname: Ray Hernandez
          isHf: false
          isPro: false
          name: perlthoughts
          type: user
        html: '<p>i believe it offers more perceptrons to be finetuned which in theory
          should produce better outputs. Thanks for trying it!</p>

          '
        raw: i believe it offers more perceptrons to be finetuned which in theory
          should produce better outputs. Thanks for trying it!
        updatedAt: '2023-12-30T13:32:24.490Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65901be8075245eadda4b160
    type: comment
  author: perlthoughts
  content: i believe it offers more perceptrons to be finetuned which in theory should
    produce better outputs. Thanks for trying it!
  created_at: 2023-12-30 13:32:24+00:00
  edited: false
  hidden: false
  id: 65901be8075245eadda4b160
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-11T02:08:54.000Z'
    data:
      from: Benedits of this model
      to: Benefits of this model
    id: 659f4db6430ffa77ac1fbec3
    type: title-change
  author: Nexesenex
  created_at: 2024-01-11 02:08:54+00:00
  id: 659f4db6430ffa77ac1fbec3
  new_title: Benefits of this model
  old_title: Benedits of this model
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: perlthoughts/Mistral-7B-Instruct-v0.2-2x7B-MoE
repo_type: model
status: open
target_branch: null
title: Benefits of this model
