!!python/object:huggingface_hub.community.DiscussionWithDetails
author: praxis-dev
conflicting_files: null
created_at: 2023-09-29 19:18:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lGAQrPgJMuqdtQAwmlIa7.jpeg?w=200&h=200&f=face
      fullname: Igor Chesnokov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: praxis-dev
      type: user
    createdAt: '2023-09-29T20:18:13.000Z'
    data:
      edited: true
      editors:
      - praxis-dev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7291549444198608
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lGAQrPgJMuqdtQAwmlIa7.jpeg?w=200&h=200&f=face
          fullname: Igor Chesnokov
          isHf: false
          isPro: false
          name: praxis-dev
          type: user
        html: '<p>Hi, getting this error from time to time with Llama-2-13B-Chat-GGML.
          </p>

          <p>Can I change maximum context length? </p>

          '
        raw: "Hi, getting this error from time to time with Llama-2-13B-Chat-GGML.\
          \ \n\nCan I change maximum context length? \n\n"
        updatedAt: '2023-09-29T20:28:17.347Z'
      numEdits: 1
      reactions: []
    id: 651731054965add2b0638059
    type: comment
  author: praxis-dev
  content: "Hi, getting this error from time to time with Llama-2-13B-Chat-GGML. \n\
    \nCan I change maximum context length? \n\n"
  created_at: 2023-09-29 19:18:13+00:00
  edited: true
  hidden: false
  id: 651731054965add2b0638059
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lGAQrPgJMuqdtQAwmlIa7.jpeg?w=200&h=200&f=face
      fullname: Igor Chesnokov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: praxis-dev
      type: user
    createdAt: '2023-09-29T20:23:54.000Z'
    data:
      edited: false
      editors:
      - praxis-dev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8013966679573059
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lGAQrPgJMuqdtQAwmlIa7.jpeg?w=200&h=200&f=face
          fullname: Igor Chesnokov
          isHf: false
          isPro: false
          name: praxis-dev
          type: user
        html: '<p>every once in a while I see recommendation like this: Try passing
          n_ctx=4096 to LLama() and it seems to work, but where exactly should I pass
          it ?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/651720eb8024217c81d78de3/baF7wxHD_pXo2TD6S6Z8i.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/651720eb8024217c81d78de3/baF7wxHD_pXo2TD6S6Z8i.png"></a></p>

          '
        raw: 'every once in a while I see recommendation like this: Try passing n_ctx=4096
          to LLama() and it seems to work, but where exactly should I pass it ?



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/651720eb8024217c81d78de3/baF7wxHD_pXo2TD6S6Z8i.png)

          '
        updatedAt: '2023-09-29T20:23:54.761Z'
      numEdits: 0
      reactions: []
    id: 6517325a42097d8c59edfd37
    type: comment
  author: praxis-dev
  content: 'every once in a while I see recommendation like this: Try passing n_ctx=4096
    to LLama() and it seems to work, but where exactly should I pass it ?



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/651720eb8024217c81d78de3/baF7wxHD_pXo2TD6S6Z8i.png)

    '
  created_at: 2023-09-29 19:23:54+00:00
  edited: false
  hidden: false
  id: 6517325a42097d8c59edfd37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/55d1f0f36cc84d4a07cbf6cd5336c1ab.svg
      fullname: Aryaman Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gupta-Aryaman
      type: user
    createdAt: '2023-10-29T19:31:59.000Z'
    data:
      edited: true
      editors:
      - Gupta-Aryaman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5626199245452881
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/55d1f0f36cc84d4a07cbf6cd5336c1ab.svg
          fullname: Aryaman Gupta
          isHf: false
          isPro: false
          name: Gupta-Aryaman
          type: user
        html: "<pre><code>def load_llm():\n    llm = LlamaCpp(\n        model_path\
          \ = model_path,\n        n_gpu_layers=5,\n        n_batch=128,\n       \
          \ verbose=True,\n        f16_kv=True,\n        n_ctx=2048\n    )\n    return\
          \ llm\n</code></pre>\n<p>I think using LlamaCpp(like so) instead of CTransformers\
          \ might help.</p>\n"
        raw: "```\ndef load_llm():\n    llm = LlamaCpp(\n        model_path = model_path,\n\
          \        n_gpu_layers=5,\n        n_batch=128,\n        verbose=True,\n\
          \        f16_kv=True,\n        n_ctx=2048\n    )\n    return llm\n```\n\
          I think using LlamaCpp(like so) instead of CTransformers might help."
        updatedAt: '2023-10-29T19:32:48.979Z'
      numEdits: 1
      reactions: []
    id: 653eb32fb424289c5f7549ad
    type: comment
  author: Gupta-Aryaman
  content: "```\ndef load_llm():\n    llm = LlamaCpp(\n        model_path = model_path,\n\
    \        n_gpu_layers=5,\n        n_batch=128,\n        verbose=True,\n      \
    \  f16_kv=True,\n        n_ctx=2048\n    )\n    return llm\n```\nI think using\
    \ LlamaCpp(like so) instead of CTransformers might help."
  created_at: 2023-10-29 18:31:59+00:00
  edited: true
  hidden: false
  id: 653eb32fb424289c5f7549ad
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: TheBloke/Llama-2-13B-chat-GGML
repo_type: model
status: open
target_branch: null
title: Number of tokens  exceeded maximum context length
