!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rachelshalom
conflicting_files: null
created_at: 2023-11-06 19:36:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668524739440-6373a9542d4eccfa6f90e97c.jpeg?w=200&h=200&f=face
      fullname: Rachel Shalom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rachelshalom
      type: user
    createdAt: '2023-11-06T19:36:39.000Z'
    data:
      edited: true
      editors:
      - rachelshalom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5009680390357971
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668524739440-6373a9542d4eccfa6f90e97c.jpeg?w=200&h=200&f=face
          fullname: Rachel Shalom
          isHf: false
          isPro: false
          name: rachelshalom
          type: user
        html: '<p>hi<br>I am using the latest langchain to load llama cpp<br>installed
          llama cpp python with: CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip
          install --upgrade --force-reinstall llama-cpp-python --no-cache-dir<br>nvcc
          --version<br>nvcc: NVIDIA (R) Cuda compiler driver<br>Copyright (c) 2005-2023
          NVIDIA Corporation<br>Built on Tue_Jul_11_02:20:44_PDT_2023<br>Cuda compilation
          tools, release 12.2, V12.2.128<br>Build cuda_12.2.r12.2/compiler.33053471_0</p>

          <p>nvidia-smi:<br> NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA
          Version: 12.1     |<br>|-----------------------------------------+----------------------+----------------------+<br>|
          GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile
          Uncorr. ECC |<br>| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage
          | GPU-Util  Compute M. |<br>|                                         |                      |               MIG
          M. |<br>|=========================================+======================+======================|<br>|   0  NVIDIA
          H100 PCIe                On | 00000000:3F:00.0 Off |                    0
          |<br>| N/A   29C    P0               47W / 350W|      0MiB / 81559MiB |      0%      Default
          |<br>|                                         |                      |             Disabled
          |<br>+-----------------------------------------+----------------------+----------------------+<br>|   1  NVIDIA
          H100 PCIe                On | 00000000:56:00.0 Off |                    0
          |<br>| N/A   30C    P0               51W / 350W|      0MiB / 81559MiB |      0%      Default
          |<br>|                                         |                      |             Disabled
          |<br>+-----------------------------------------+----------------------+----------------------+<br>|   2  NVIDIA
          H100 PCIe                On | 00000000:C3:00.0 Off |                    0
          |<br>| N/A   31C    P0               49W / 350W|      0MiB / 81559MiB |      0%      Default
          |<br>|                                         |                      |             Disabled
          |<br>+-----------------------------------------+----------------------+----------------------+<br>|   3  NVIDIA
          H100 PCIe                On | 00000000:DA:00.0 Off |                    0
          |<br>| N/A   32C    P0               51W / 350W|      0MiB / 81559MiB |      0%      Default
          |<br>|                                         |                      |             Disabled
          |<br>+-----------------------------------------+----------------------+----------------------+</p>

          <p>I manage to load the model with cblas=1  </p>

          <p>llm_load_tensors: using CUDA for GPU acceleration<br>ggml_cuda_set_main_device:
          using device 0 (NVIDIA H100 PCIe) as main device<br>llm_load_tensors: mem
          required  = 5114.10 MB<br>llm_load_tensors: offloading 1 repeating layers
          to GPU<br>llm_load_tensors: offloaded 1/35 layers to GPU<br>llm_load_tensors:
          VRAM used: 158.35 MB<br>....................................................................................................<br>llama_new_context_with_model:
          n_ctx      = 3000<br>llama_new_context_with_model: freq_base  = 10000.0<br>llama_new_context_with_model:
          freq_scale = 1<br>llama_new_context_with_model: kv self size  = 1500.00
          MB<br>llama_build_graph: non-view tensors processed: 740/740<br>llama_new_context_with_model:
          compute buffer total size = 223.99 MB<br>llama_new_context_with_model: VRAM
          scratch buffer: 217.36 MB<br>llama_new_context_with_model: total VRAM used:
          375.71 MB (model: 158.35 MB, context: 217.36 MB)<br>AVX = 1 | AVX2 = 1 |
          AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA
          = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3
          = 1 | VSX = 0 | </p>

          <p>but when the model is processing a prompt I get a cuda error:</p>

          <p>CUDA error 222 at /tmp/pip-install-uq8lpx95/llama-cpp-python_c2bd3bc9a27b49f3805443a95df1ea3d/vendor/llama.cpp/ggml-cuda.cu:7043:
          the provided PTX was compiled with an unsupported toolchain.<br>current
          device: 0</p>

          <p>any advice to solve this?</p>

          '
        raw: "hi \nI am using the latest langchain to load llama cpp\ninstalled llama\
          \ cpp python with: CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install\
          \ --upgrade --force-reinstall llama-cpp-python --no-cache-dir\nnvcc --version\n\
          nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\n\
          Built on Tue_Jul_11_02:20:44_PDT_2023\nCuda compilation tools, release 12.2,\
          \ V12.2.128\nBuild cuda_12.2.r12.2/compiler.33053471_0\n\nnvidia-smi:\n\
          \ NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version:\
          \ 12.1     |\n|-----------------------------------------+----------------------+----------------------+\n\
          | GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile\
          \ Uncorr. ECC |\n| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage\
          \ | GPU-Util  Compute M. |\n|                                         |\
          \                      |               MIG M. |\n|=========================================+======================+======================|\n\
          |   0  NVIDIA H100 PCIe                On | 00000000:3F:00.0 Off |     \
          \               0 |\n| N/A   29C    P0               47W / 350W|      0MiB\
          \ / 81559MiB |      0%      Default |\n|                               \
          \          |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n\
          |   1  NVIDIA H100 PCIe                On | 00000000:56:00.0 Off |     \
          \               0 |\n| N/A   30C    P0               51W / 350W|      0MiB\
          \ / 81559MiB |      0%      Default |\n|                               \
          \          |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n\
          |   2  NVIDIA H100 PCIe                On | 00000000:C3:00.0 Off |     \
          \               0 |\n| N/A   31C    P0               49W / 350W|      0MiB\
          \ / 81559MiB |      0%      Default |\n|                               \
          \          |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n\
          |   3  NVIDIA H100 PCIe                On | 00000000:DA:00.0 Off |     \
          \               0 |\n| N/A   32C    P0               51W / 350W|      0MiB\
          \ / 81559MiB |      0%      Default |\n|                               \
          \          |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n\
          \                                                                      \
          \                   \n\nI manage to load the model with cblas=1  \n\nllm_load_tensors:\
          \ using CUDA for GPU acceleration\nggml_cuda_set_main_device: using device\
          \ 0 (NVIDIA H100 PCIe) as main device\nllm_load_tensors: mem required  =\
          \ 5114.10 MB\nllm_load_tensors: offloading 1 repeating layers to GPU\nllm_load_tensors:\
          \ offloaded 1/35 layers to GPU\nllm_load_tensors: VRAM used: 158.35 MB\n\
          ....................................................................................................\n\
          llama_new_context_with_model: n_ctx      = 3000\nllama_new_context_with_model:\
          \ freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model:\
          \ kv self size  = 1500.00 MB\nllama_build_graph: non-view tensors processed:\
          \ 740/740\nllama_new_context_with_model: compute buffer total size = 223.99\
          \ MB\nllama_new_context_with_model: VRAM scratch buffer: 217.36 MB\nllama_new_context_with_model:\
          \ total VRAM used: 375.71 MB (model: 158.35 MB, context: 217.36 MB)\nAVX\
          \ = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA\
          \ = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0\
          \ | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n\nbut when the model is\
          \ processing a prompt I get a cuda error:\n\nCUDA error 222 at /tmp/pip-install-uq8lpx95/llama-cpp-python_c2bd3bc9a27b49f3805443a95df1ea3d/vendor/llama.cpp/ggml-cuda.cu:7043:\
          \ the provided PTX was compiled with an unsupported toolchain.\ncurrent\
          \ device: 0\n\nany advice to solve this?"
        updatedAt: '2023-11-07T15:20:42.599Z'
      numEdits: 2
      reactions: []
    id: 6549404769da1e356c707716
    type: comment
  author: rachelshalom
  content: "hi \nI am using the latest langchain to load llama cpp\ninstalled llama\
    \ cpp python with: CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install\
    \ --upgrade --force-reinstall llama-cpp-python --no-cache-dir\nnvcc --version\n\
    nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\n\
    Built on Tue_Jul_11_02:20:44_PDT_2023\nCuda compilation tools, release 12.2, V12.2.128\n\
    Build cuda_12.2.r12.2/compiler.33053471_0\n\nnvidia-smi:\n NVIDIA-SMI 530.30.02\
    \              Driver Version: 530.30.02    CUDA Version: 12.1     |\n|-----------------------------------------+----------------------+----------------------+\n\
    | GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.\
    \ ECC |\n| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util\
    \  Compute M. |\n|                                         |                 \
    \     |               MIG M. |\n|=========================================+======================+======================|\n\
    |   0  NVIDIA H100 PCIe                On | 00000000:3F:00.0 Off |           \
    \         0 |\n| N/A   29C    P0               47W / 350W|      0MiB / 81559MiB\
    \ |      0%      Default |\n|                                         |      \
    \                |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n\
    |   1  NVIDIA H100 PCIe                On | 00000000:56:00.0 Off |           \
    \         0 |\n| N/A   30C    P0               51W / 350W|      0MiB / 81559MiB\
    \ |      0%      Default |\n|                                         |      \
    \                |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n\
    |   2  NVIDIA H100 PCIe                On | 00000000:C3:00.0 Off |           \
    \         0 |\n| N/A   31C    P0               49W / 350W|      0MiB / 81559MiB\
    \ |      0%      Default |\n|                                         |      \
    \                |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n\
    |   3  NVIDIA H100 PCIe                On | 00000000:DA:00.0 Off |           \
    \         0 |\n| N/A   32C    P0               51W / 350W|      0MiB / 81559MiB\
    \ |      0%      Default |\n|                                         |      \
    \                |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n\
    \                                                                            \
    \             \n\nI manage to load the model with cblas=1  \n\nllm_load_tensors:\
    \ using CUDA for GPU acceleration\nggml_cuda_set_main_device: using device 0 (NVIDIA\
    \ H100 PCIe) as main device\nllm_load_tensors: mem required  = 5114.10 MB\nllm_load_tensors:\
    \ offloading 1 repeating layers to GPU\nllm_load_tensors: offloaded 1/35 layers\
    \ to GPU\nllm_load_tensors: VRAM used: 158.35 MB\n....................................................................................................\n\
    llama_new_context_with_model: n_ctx      = 3000\nllama_new_context_with_model:\
    \ freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model:\
    \ kv self size  = 1500.00 MB\nllama_build_graph: non-view tensors processed: 740/740\n\
    llama_new_context_with_model: compute buffer total size = 223.99 MB\nllama_new_context_with_model:\
    \ VRAM scratch buffer: 217.36 MB\nllama_new_context_with_model: total VRAM used:\
    \ 375.71 MB (model: 158.35 MB, context: 217.36 MB)\nAVX = 1 | AVX2 = 1 | AVX512\
    \ = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 |\
    \ F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX\
    \ = 0 | \n\nbut when the model is processing a prompt I get a cuda error:\n\n\
    CUDA error 222 at /tmp/pip-install-uq8lpx95/llama-cpp-python_c2bd3bc9a27b49f3805443a95df1ea3d/vendor/llama.cpp/ggml-cuda.cu:7043:\
    \ the provided PTX was compiled with an unsupported toolchain.\ncurrent device:\
    \ 0\n\nany advice to solve this?"
  created_at: 2023-11-06 19:36:39+00:00
  edited: true
  hidden: false
  id: 6549404769da1e356c707716
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3276e1a88f62963e181e5bde9fa9607d.svg
      fullname: MajesticHou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Majestichou99
      type: user
    createdAt: '2023-11-30T02:40:35.000Z'
    data:
      edited: true
      editors:
      - Majestichou99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9324395656585693
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3276e1a88f62963e181e5bde9fa9607d.svg
          fullname: MajesticHou
          isHf: false
          isPro: false
          name: Majestichou99
          type: user
        html: '<p>maybe you can see this blog: <a rel="nofollow" href="https://michaelriedl.com/2023/09/10/llama2-install-gpu.html">https://michaelriedl.com/2023/09/10/llama2-install-gpu.html</a></p>

          '
        raw: 'maybe you can see this blog: https://michaelriedl.com/2023/09/10/llama2-install-gpu.html'
        updatedAt: '2023-11-30T02:41:00.317Z'
      numEdits: 2
      reactions: []
    id: 6567f623fc8724d07242e30c
    type: comment
  author: Majestichou99
  content: 'maybe you can see this blog: https://michaelriedl.com/2023/09/10/llama2-install-gpu.html'
  created_at: 2023-11-30 02:40:35+00:00
  edited: true
  hidden: false
  id: 6567f623fc8724d07242e30c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/498c22a1bf17c72916cc8db6471df0b5.svg
      fullname: JT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jtvinolus
      type: user
    createdAt: '2023-12-05T17:06:18.000Z'
    data:
      edited: true
      editors:
      - jtvinolus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9017176032066345
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/498c22a1bf17c72916cc8db6471df0b5.svg
          fullname: JT
          isHf: false
          isPro: false
          name: jtvinolus
          type: user
        html: '<p>Hey this is still a problem. I followed the steps shown in <a rel="nofollow"
          href="https://michaelriedl.com/2023/09/10/llama2-install-gpu.html">https://michaelriedl.com/2023/09/10/llama2-install-gpu.html</a>
          and it did not solve the problem.</p>

          '
        raw: Hey this is still a problem. I followed the steps shown in https://michaelriedl.com/2023/09/10/llama2-install-gpu.html
          and it did not solve the problem.
        updatedAt: '2023-12-05T17:06:31.701Z'
      numEdits: 1
      reactions: []
    id: 656f588acb4bff8b1058bd88
    type: comment
  author: jtvinolus
  content: Hey this is still a problem. I followed the steps shown in https://michaelriedl.com/2023/09/10/llama2-install-gpu.html
    and it did not solve the problem.
  created_at: 2023-12-05 17:06:18+00:00
  edited: true
  hidden: false
  id: 656f588acb4bff8b1058bd88
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: TheBloke/Llama-2-13B-chat-GGML
repo_type: model
status: open
target_branch: null
title: cuda error when loading llama 7b chat
