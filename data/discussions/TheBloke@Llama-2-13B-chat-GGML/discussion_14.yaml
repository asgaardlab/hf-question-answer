!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zenitica
conflicting_files: null
created_at: 2023-08-23 01:29:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/637ba13901f272413a0c886f8eb700b2.svg
      fullname: Zion Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zenitica
      type: user
    createdAt: '2023-08-23T02:29:18.000Z'
    data:
      edited: false
      editors:
      - zenitica
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46623262763023376
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/637ba13901f272413a0c886f8eb700b2.svg
          fullname: Zion Zhang
          isHf: false
          isPro: false
          name: zenitica
          type: user
        html: '<p>executing this command:</p>

          <blockquote>

          <p>.\build\bin\Release\main.exe -m ./models/llama-2-13b-chat.ggmlv3.q4_0.bin
          -p "Building a website can be done in 10 simple steps:" -n 512</p>

          </blockquote>

          <p>gets the output with the error msg:</p>

          <blockquote>

          <p>main: build = 1018 (8e4364f)<br>main: seed  = 1692754983<br>ggml_init_cublas:
          found 3 CUDA devices:<br>  Device 0: NVIDIA GeForce RTX 3080, compute capability
          8.6<br>  Device 1: NVIDIA GeForce RTX 3080, compute capability 8.6<br>  Device
          2: NVIDIA GeForce RTX 3080, compute capability 8.6<br>gguf_init_from_file:
          invalid magic number 67676a74<br>error loading model: llama_model_loader:
          failed to load model from ./models/llama-2-13b-chat.ggmlv3.q4_0.bin<br>llama_load_model_from_file:
          failed to load model<br>llama_init_from_gpt_params: error: failed to load
          model ''./models/llama-2-13b-chat.ggmlv3.q4_0.bin''<br>main: error: unable
          to load model</p>

          </blockquote>

          '
        raw: "executing this command:\r\n\r\n> .\\build\\bin\\Release\\main.exe -m\
          \ ./models/llama-2-13b-chat.ggmlv3.q4_0.bin -p \"Building a website can\
          \ be done in 10 simple steps:\" -n 512\r\n\r\ngets the output with the error\
          \ msg:\r\n\r\n> main: build = 1018 (8e4364f)\r\nmain: seed  = 1692754983\r\
          \nggml_init_cublas: found 3 CUDA devices:\r\n  Device 0: NVIDIA GeForce\
          \ RTX 3080, compute capability 8.6\r\n  Device 1: NVIDIA GeForce RTX 3080,\
          \ compute capability 8.6\r\n  Device 2: NVIDIA GeForce RTX 3080, compute\
          \ capability 8.6\r\ngguf_init_from_file: invalid magic number 67676a74\r\
          \nerror loading model: llama_model_loader: failed to load model from ./models/llama-2-13b-chat.ggmlv3.q4_0.bin\r\
          \nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params:\
          \ error: failed to load model './models/llama-2-13b-chat.ggmlv3.q4_0.bin'\r\
          \nmain: error: unable to load model"
        updatedAt: '2023-08-23T02:29:18.239Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - dj38fj3ksj0
    id: 64e56efebb843c94d1a2ba6c
    type: comment
  author: zenitica
  content: "executing this command:\r\n\r\n> .\\build\\bin\\Release\\main.exe -m ./models/llama-2-13b-chat.ggmlv3.q4_0.bin\
    \ -p \"Building a website can be done in 10 simple steps:\" -n 512\r\n\r\ngets\
    \ the output with the error msg:\r\n\r\n> main: build = 1018 (8e4364f)\r\nmain:\
    \ seed  = 1692754983\r\nggml_init_cublas: found 3 CUDA devices:\r\n  Device 0:\
    \ NVIDIA GeForce RTX 3080, compute capability 8.6\r\n  Device 1: NVIDIA GeForce\
    \ RTX 3080, compute capability 8.6\r\n  Device 2: NVIDIA GeForce RTX 3080, compute\
    \ capability 8.6\r\ngguf_init_from_file: invalid magic number 67676a74\r\nerror\
    \ loading model: llama_model_loader: failed to load model from ./models/llama-2-13b-chat.ggmlv3.q4_0.bin\r\
    \nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params:\
    \ error: failed to load model './models/llama-2-13b-chat.ggmlv3.q4_0.bin'\r\n\
    main: error: unable to load model"
  created_at: 2023-08-23 01:29:18+00:00
  edited: false
  hidden: false
  id: 64e56efebb843c94d1a2ba6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/637ba13901f272413a0c886f8eb700b2.svg
      fullname: Zion Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zenitica
      type: user
    createdAt: '2023-08-23T03:32:37.000Z'
    data:
      from: latest release cannot import 13B GGML q4.0 model
      to: latest release of llama.cpp cannot import 13B GGML q4.0 model
    id: 64e57dd563a071d9b8b6ac4e
    type: title-change
  author: zenitica
  created_at: 2023-08-23 02:32:37+00:00
  id: 64e57dd563a071d9b8b6ac4e
  new_title: latest release of llama.cpp cannot import 13B GGML q4.0 model
  old_title: latest release cannot import 13B GGML q4.0 model
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/637ba13901f272413a0c886f8eb700b2.svg
      fullname: Zion Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zenitica
      type: user
    createdAt: '2023-08-23T03:33:21.000Z'
    data:
      edited: false
      editors:
      - zenitica
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47045645117759705
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/637ba13901f272413a0c886f8eb700b2.svg
          fullname: Zion Zhang
          isHf: false
          isPro: false
          name: zenitica
          type: user
        html: '<p>rolling back llama.cpp to commit hash a113689 works</p>

          '
        raw: rolling back llama.cpp to commit hash a113689 works
        updatedAt: '2023-08-23T03:33:21.351Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - leohxj
        - Yud07
        - TimYao
    id: 64e57e01aab459f4345a67ad
    type: comment
  author: zenitica
  content: rolling back llama.cpp to commit hash a113689 works
  created_at: 2023-08-23 02:33:21+00:00
  edited: false
  hidden: false
  id: 64e57e01aab459f4345a67ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-23T07:57:53.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9018465280532837
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah, latest llama.cpp is no longer compatible with GGML models.  The
          new model format, GGUF, was merged recently.  As far as llama.cpp is concerned,
          GGML is now dead - though of course many third-party clients/libraries are
          likely to continue to support it for a lot longer.  I need to update my
          GGML READMEs to mention this and will be doing this shortly.</p>

          <p>I will be providing GGUF models for all my repos in the next 2-3 days.
          I''m waiting for another PR to merge, which will add improved k-quant quantisation
          formats.</p>

          <p>For now, if you want to use llama.cpp you will need to downgrade it back
          to commit <code>dadbed99e65252d79f81101a392d0d6497b86caa</code> or earlier.  Or
          use one of the llama.cpp binary releases from before GGUF was merged.  Or
          use a third party client like KoboldCpp, LM Studio, text-generation-webui,
          etc.</p>

          <p>Look out for new <code>-GGUF</code> repos from me in the coming days.
          Or yes, you can convert them yourself using the script <code>ggml_to_gguf.py</code>
          now provided with llama.cpp.</p>

          '
        raw: 'Yeah, latest llama.cpp is no longer compatible with GGML models.  The
          new model format, GGUF, was merged recently.  As far as llama.cpp is concerned,
          GGML is now dead - though of course many third-party clients/libraries are
          likely to continue to support it for a lot longer.  I need to update my
          GGML READMEs to mention this and will be doing this shortly.


          I will be providing GGUF models for all my repos in the next 2-3 days. I''m
          waiting for another PR to merge, which will add improved k-quant quantisation
          formats.


          For now, if you want to use llama.cpp you will need to downgrade it back
          to commit `dadbed99e65252d79f81101a392d0d6497b86caa` or earlier.  Or use
          one of the llama.cpp binary releases from before GGUF was merged.  Or use
          a third party client like KoboldCpp, LM Studio, text-generation-webui, etc.


          Look out for new `-GGUF` repos from me in the coming days. Or yes, you can
          convert them yourself using the script `ggml_to_gguf.py` now provided with
          llama.cpp.

          '
        updatedAt: '2023-08-23T07:58:06.160Z'
      numEdits: 1
      reactions:
      - count: 20
        reaction: "\u2764\uFE0F"
        users:
        - filipealmeida
        - paralin
        - zenitica
        - leohxj
        - SaidTorres3
        - jjones
        - igtm
        - bharatcoder
        - volodvana
        - SegmentationFault
        - JasonXL
        - zuhandenheit
        - ArthursFist
        - li-ping
        - RavindhranSankar
        - vibeihav
        - privatejava
        - auvv
        - siddhardha25
        - shrijayan
      - count: 9
        reaction: "\U0001F44D"
        users:
        - zenitica
        - SaidTorres3
        - GeneralCognition-Hassan
        - dipanshu-c3rl
        - quantisan
        - junepham
        - SegmentationFault
        - greenhorn82
        - musicallyut
      - count: 5
        reaction: "\U0001F92F"
        users:
        - SaidTorres3
        - jiaqi2206
        - GinoGinetti34
        - charlsagente
        - sgowdaks
      - count: 2
        reaction: "\U0001F917"
        users:
        - SaidTorres3
        - Ambiata
      - count: 1
        reaction: "\U0001F91D"
        users:
        - SaidTorres3
    id: 64e5bc015af55fb4d1f9b61d
    type: comment
  author: TheBloke
  content: 'Yeah, latest llama.cpp is no longer compatible with GGML models.  The
    new model format, GGUF, was merged recently.  As far as llama.cpp is concerned,
    GGML is now dead - though of course many third-party clients/libraries are likely
    to continue to support it for a lot longer.  I need to update my GGML READMEs
    to mention this and will be doing this shortly.


    I will be providing GGUF models for all my repos in the next 2-3 days. I''m waiting
    for another PR to merge, which will add improved k-quant quantisation formats.


    For now, if you want to use llama.cpp you will need to downgrade it back to commit
    `dadbed99e65252d79f81101a392d0d6497b86caa` or earlier.  Or use one of the llama.cpp
    binary releases from before GGUF was merged.  Or use a third party client like
    KoboldCpp, LM Studio, text-generation-webui, etc.


    Look out for new `-GGUF` repos from me in the coming days. Or yes, you can convert
    them yourself using the script `ggml_to_gguf.py` now provided with llama.cpp.

    '
  created_at: 2023-08-23 06:57:53+00:00
  edited: true
  hidden: false
  id: 64e5bc015af55fb4d1f9b61d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/637ba13901f272413a0c886f8eb700b2.svg
      fullname: Zion Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zenitica
      type: user
    createdAt: '2023-08-25T08:31:16.000Z'
    data:
      from: latest release of llama.cpp cannot import 13B GGML q4.0 model
      to: 'invalid magic number: latest release of llama.cpp cannot import 13B GGML
        q4.0 model'
    id: 64e866d4e6a52fff352ec83d
    type: title-change
  author: zenitica
  created_at: 2023-08-25 07:31:16+00:00
  id: 64e866d4e6a52fff352ec83d
  new_title: 'invalid magic number: latest release of llama.cpp cannot import 13B
    GGML q4.0 model'
  old_title: latest release of llama.cpp cannot import 13B GGML q4.0 model
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eeff2542d5bc165a78ba25d858b96f06.svg
      fullname: Kevin Webber
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Buck3tHead
      type: user
    createdAt: '2023-08-25T15:44:00.000Z'
    data:
      edited: false
      editors:
      - Buck3tHead
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9875096082687378
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eeff2542d5bc165a78ba25d858b96f06.svg
          fullname: Kevin Webber
          isHf: false
          isPro: false
          name: Buck3tHead
          type: user
        html: '<p>I see, good to know. Was also getting similar. Thank you.</p>

          '
        raw: I see, good to know. Was also getting similar. Thank you.
        updatedAt: '2023-08-25T15:44:00.901Z'
      numEdits: 0
      reactions: []
    id: 64e8cc40fddb4050dd93f8ab
    type: comment
  author: Buck3tHead
  content: I see, good to know. Was also getting similar. Thank you.
  created_at: 2023-08-25 14:44:00+00:00
  edited: false
  hidden: false
  id: 64e8cc40fddb4050dd93f8ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49a27e1155dbdd7db1e8a44eb979cc95.svg
      fullname: Said Torres
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SaidTorres3
      type: user
    createdAt: '2023-08-25T20:07:46.000Z'
    data:
      edited: false
      editors:
      - SaidTorres3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9653967022895813
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49a27e1155dbdd7db1e8a44eb979cc95.svg
          fullname: Said Torres
          isHf: false
          isPro: false
          name: SaidTorres3
          type: user
        html: '<p>Why didn''t they mention that SUPER IMPORTANT INFORMATION in the
          readme.md?!</p>

          '
        raw: Why didn't they mention that SUPER IMPORTANT INFORMATION in the readme.md?!
        updatedAt: '2023-08-25T20:07:46.008Z'
      numEdits: 0
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - Displacer
        - zenitica
        - ArthursFist
        - privatejava
        - vanokio
        - blessmario
        - ashik-entym
      - count: 2
        reaction: "\U0001F614"
        users:
        - t0d4
        - Displacer
    id: 64e90a123b71f1a21015f18b
    type: comment
  author: SaidTorres3
  content: Why didn't they mention that SUPER IMPORTANT INFORMATION in the readme.md?!
  created_at: 2023-08-25 19:07:46+00:00
  edited: false
  hidden: false
  id: 64e90a123b71f1a21015f18b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-25T20:12:07.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9198855757713318
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>They kind of do: </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/VTW_YYOLhY7wv5vuw_Ln9.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/VTW_YYOLhY7wv5vuw_Ln9.png"></a></p>

          <p>But it''s the kind of message that you probably won''t register unless
          you already know what it means..</p>

          <p>(Unless you meant me, in which case I''ve not yet updated all my pre-existing
          GGML repos since the launch of GGUF, but will be starting that process tomorrow,
          as well as providing GGUF versions for most of the existing GGML repos.)</p>

          '
        raw: "They kind of do: \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/VTW_YYOLhY7wv5vuw_Ln9.png)\n\
          \nBut it's the kind of message that you probably won't register unless you\
          \ already know what it means..\n\n(Unless you meant me, in which case I've\
          \ not yet updated all my pre-existing GGML repos since the launch of GGUF,\
          \ but will be starting that process tomorrow, as well as providing GGUF\
          \ versions for most of the existing GGML repos.)"
        updatedAt: '2023-08-25T20:12:07.529Z'
      numEdits: 0
      reactions: []
    id: 64e90b171dce571e85324ee3
    type: comment
  author: TheBloke
  content: "They kind of do: \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/VTW_YYOLhY7wv5vuw_Ln9.png)\n\
    \nBut it's the kind of message that you probably won't register unless you already\
    \ know what it means..\n\n(Unless you meant me, in which case I've not yet updated\
    \ all my pre-existing GGML repos since the launch of GGUF, but will be starting\
    \ that process tomorrow, as well as providing GGUF versions for most of the existing\
    \ GGML repos.)"
  created_at: 2023-08-25 19:12:07+00:00
  edited: false
  hidden: false
  id: 64e90b171dce571e85324ee3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49a27e1155dbdd7db1e8a44eb979cc95.svg
      fullname: Said Torres
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SaidTorres3
      type: user
    createdAt: '2023-08-25T20:37:10.000Z'
    data:
      edited: true
      editors:
      - SaidTorres3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9464583992958069
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49a27e1155dbdd7db1e8a44eb979cc95.svg
          fullname: Said Torres
          isHf: false
          isPro: false
          name: SaidTorres3
          type: user
        html: '<p>In theirs Readme.md tutorial are still using GGML without any warning
          that it doesn''t work anymore. </p>

          '
        raw: 'In theirs Readme.md tutorial are still using GGML without any warning
          that it doesn''t work anymore. '
        updatedAt: '2023-08-25T21:52:59.514Z'
      numEdits: 1
      reactions: []
    id: 64e910f66461925301180bd2
    type: comment
  author: SaidTorres3
  content: 'In theirs Readme.md tutorial are still using GGML without any warning
    that it doesn''t work anymore. '
  created_at: 2023-08-25 19:37:10+00:00
  edited: true
  hidden: false
  id: 64e910f66461925301180bd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ab8336cfbe073b42c4b3bcfc81fc20c.svg
      fullname: Richard Scott
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RichardScottOZ
      type: user
    createdAt: '2023-08-27T04:42:25.000Z'
    data:
      edited: false
      editors:
      - RichardScottOZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8995576500892639
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ab8336cfbe073b42c4b3bcfc81fc20c.svg
          fullname: Richard Scott
          isHf: false
          isPro: false
          name: RichardScottOZ
          type: user
        html: '<p>Thanks for the exact commit tip.</p>

          '
        raw: Thanks for the exact commit tip.
        updatedAt: '2023-08-27T04:42:25.294Z'
      numEdits: 0
      reactions: []
    id: 64ead4312ca4ff1d53bdfa26
    type: comment
  author: RichardScottOZ
  content: Thanks for the exact commit tip.
  created_at: 2023-08-27 03:42:25+00:00
  edited: false
  hidden: false
  id: 64ead4312ca4ff1d53bdfa26
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e96f8f0ee350f48e8b85c3bab6812bfe.svg
      fullname: Snowsayer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: snowsayer
      type: user
    createdAt: '2023-09-10T19:54:23.000Z'
    data:
      edited: false
      editors:
      - snowsayer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5992459654808044
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e96f8f0ee350f48e8b85c3bab6812bfe.svg
          fullname: Snowsayer
          isHf: false
          isPro: false
          name: snowsayer
          type: user
        html: '<p>For those interested and coming from <a rel="nofollow" href="https://replicate.com/blog/run-llama-locally">https://replicate.com/blog/run-llama-locally</a>,
          some notes:</p>

          <ul>

          <li>the command to run is not <code>ggml_to_gguf.py</code> but <code>convert-llama-ggml-to-gguf.py</code></li>

          <li>You will need <code>python3</code> and the <code>numpy</code> libraries.
          You can install <code>numpy</code> using <code>pip3 install numpy</code></li>

          <li>the exact command should be something like this: <code>./convert-llama-ggml-to-gguf.py
          --eps 1e-5 -i ./models/llama-2-13b-chat.ggmlv3.q4_0.bin -o ./models/llama-2-13b-chat.ggmlv3.q4_0.gguf.bin</code></li>

          </ul>

          '
        raw: 'For those interested and coming from https://replicate.com/blog/run-llama-locally,
          some notes:

          * the command to run is not `ggml_to_gguf.py` but `convert-llama-ggml-to-gguf.py`

          * You will need `python3` and the `numpy` libraries. You can install `numpy`
          using `pip3 install numpy`

          * the exact command should be something like this: `./convert-llama-ggml-to-gguf.py
          --eps 1e-5 -i ./models/llama-2-13b-chat.ggmlv3.q4_0.bin -o ./models/llama-2-13b-chat.ggmlv3.q4_0.gguf.bin`'
        updatedAt: '2023-09-10T19:54:23.946Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\u2764\uFE0F"
        users:
        - teblanc
        - dylanhogg
        - webdesignmtl
        - c0rp-aubakirov
        - yepher
        - RavindhranSankar
    id: 64fe1eef8710fc5ebddf43e4
    type: comment
  author: snowsayer
  content: 'For those interested and coming from https://replicate.com/blog/run-llama-locally,
    some notes:

    * the command to run is not `ggml_to_gguf.py` but `convert-llama-ggml-to-gguf.py`

    * You will need `python3` and the `numpy` libraries. You can install `numpy` using
    `pip3 install numpy`

    * the exact command should be something like this: `./convert-llama-ggml-to-gguf.py
    --eps 1e-5 -i ./models/llama-2-13b-chat.ggmlv3.q4_0.bin -o ./models/llama-2-13b-chat.ggmlv3.q4_0.gguf.bin`'
  created_at: 2023-09-10 18:54:23+00:00
  edited: false
  hidden: false
  id: 64fe1eef8710fc5ebddf43e4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: TheBloke/Llama-2-13B-chat-GGML
repo_type: model
status: open
target_branch: null
title: 'invalid magic number: latest release of llama.cpp cannot import 13B GGML q4.0
  model'
