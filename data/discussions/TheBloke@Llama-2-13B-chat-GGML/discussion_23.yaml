!!python/object:huggingface_hub.community.DiscussionWithDetails
author: melindmi
conflicting_files: null
created_at: 2023-10-12 11:06:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4094b1836c2fffb921bb55bdd5b18b8.svg
      fullname: MM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: melindmi
      type: user
    createdAt: '2023-10-12T12:06:07.000Z'
    data:
      edited: false
      editors:
      - melindmi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7619197964668274
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4094b1836c2fffb921bb55bdd5b18b8.svg
          fullname: MM
          isHf: false
          isPro: false
          name: melindmi
          type: user
        html: '<p>Hi, I am trying to use the llama-2-70b-chat.Q5_K_M.gguf model with
          ctransformers on GPU but I get this error:<br><code>CUDA error 222 at /home/runner/work/ctransformers/ctransformers/models/ggml/ggml-cuda.cu:6045:
          the provided PTX was compiled with an unsupported toolchain.</code><br>My
          torch version is ''2.1.0+cu121'' and the GPU Driver Version: 525.125.06  supporting
          CUDA Version: 12.0.</p>

          <p>The code: <code>llm = AutoModelForCausalLM.from_pretrained("../llama",                                            model_file="llama-2-13b-chat.q5_K_M.gguf",                                             model_type="llama",                                             gpu_layers=50,                                            temperature=1,                                            context_length=4096)</code></p>

          <p>Can someone suggest something on this? </p>

          '
        raw: "Hi, I am trying to use the llama-2-70b-chat.Q5_K_M.gguf model with ctransformers\
          \ on GPU but I get this error:\r\n``` CUDA error 222 at /home/runner/work/ctransformers/ctransformers/models/ggml/ggml-cuda.cu:6045:\
          \ the provided PTX was compiled with an unsupported toolchain. ```\r\nMy\
          \ torch version is '2.1.0+cu121' and the GPU Driver Version: 525.125.06\
          \  supporting CUDA Version: 12.0.\r\n\r\nThe code: ```llm = AutoModelForCausalLM.from_pretrained(\"\
          ../llama\",\r\n                                           model_file=\"\
          llama-2-13b-chat.q5_K_M.gguf\", \r\n                                   \
          \        model_type=\"llama\", \r\n                                    \
          \       gpu_layers=50,\r\n                                           temperature=1,\r\
          \n                                           context_length=4096)```\r\n\
          \r\nCan someone suggest something on this? \r\n\r\n"
        updatedAt: '2023-10-12T12:06:07.360Z'
      numEdits: 0
      reactions: []
    id: 6527e12fd38c87e7fb3ef6a6
    type: comment
  author: melindmi
  content: "Hi, I am trying to use the llama-2-70b-chat.Q5_K_M.gguf model with ctransformers\
    \ on GPU but I get this error:\r\n``` CUDA error 222 at /home/runner/work/ctransformers/ctransformers/models/ggml/ggml-cuda.cu:6045:\
    \ the provided PTX was compiled with an unsupported toolchain. ```\r\nMy torch\
    \ version is '2.1.0+cu121' and the GPU Driver Version: 525.125.06  supporting\
    \ CUDA Version: 12.0.\r\n\r\nThe code: ```llm = AutoModelForCausalLM.from_pretrained(\"\
    ../llama\",\r\n                                           model_file=\"llama-2-13b-chat.q5_K_M.gguf\"\
    , \r\n                                           model_type=\"llama\", \r\n  \
    \                                         gpu_layers=50,\r\n                 \
    \                          temperature=1,\r\n                                \
    \           context_length=4096)```\r\n\r\nCan someone suggest something on this?\
    \ \r\n\r\n"
  created_at: 2023-10-12 11:06:07+00:00
  edited: false
  hidden: false
  id: 6527e12fd38c87e7fb3ef6a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4094b1836c2fffb921bb55bdd5b18b8.svg
      fullname: MM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: melindmi
      type: user
    createdAt: '2023-10-13T12:23:24.000Z'
    data:
      edited: false
      editors:
      - melindmi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8677489757537842
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4094b1836c2fffb921bb55bdd5b18b8.svg
          fullname: MM
          isHf: false
          isPro: false
          name: melindmi
          type: user
        html: '<p>In case someone else encounters the same issue, this problem is
          not really related to the model itself but with the ctransformers installation
          and by having a nvcc version not compatible with the GPU driver version.<br>When
          installing the ctransformes with <code>pip install ctransformers[cuda]</code>  precompiled
          libs for CUDA 12.2 are used, but in my cases I needed CUDA version 12.0.<br>If
          I used  <code>CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers</code>
          by default the CUDA compiler path was /usr/bin/ which in my case had an
          older version of nvcc.<br>The solution was to install the right CUDA version
          in a different path and then install ctransformers with:<br><code>CMAKE_ARGS="-DCMAKE_CUDA_COMPILER=/path_to_cuda/bin/nvcc"  CT_CUBLAS=1
          pip install ctransformers --no-binary ctransformers</code></p>

          '
        raw: 'In case someone else encounters the same issue, this problem is not
          really related to the model itself but with the ctransformers installation
          and by having a nvcc version not compatible with the GPU driver version.

          When installing the ctransformes with ```pip install ctransformers[cuda]```  precompiled
          libs for CUDA 12.2 are used, but in my cases I needed CUDA version 12.0.

          If I used  ```CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers```
          by default the CUDA compiler path was /usr/bin/ which in my case had an
          older version of nvcc.

          The solution was to install the right CUDA version in a different path and
          then install ctransformers with:

          ```CMAKE_ARGS="-DCMAKE_CUDA_COMPILER=/path_to_cuda/bin/nvcc"  CT_CUBLAS=1
          pip install ctransformers --no-binary ctransformers```'
        updatedAt: '2023-10-13T12:23:24.480Z'
      numEdits: 0
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - mmkamani7
        - mvonwyl
        - dipshady
        - arelius
        - tkornev
        - whitew1994
        - jorses
      relatedEventId: 652936bca8cd884751739925
    id: 652936bca8cd884751739920
    type: comment
  author: melindmi
  content: 'In case someone else encounters the same issue, this problem is not really
    related to the model itself but with the ctransformers installation and by having
    a nvcc version not compatible with the GPU driver version.

    When installing the ctransformes with ```pip install ctransformers[cuda]```  precompiled
    libs for CUDA 12.2 are used, but in my cases I needed CUDA version 12.0.

    If I used  ```CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers```
    by default the CUDA compiler path was /usr/bin/ which in my case had an older
    version of nvcc.

    The solution was to install the right CUDA version in a different path and then
    install ctransformers with:

    ```CMAKE_ARGS="-DCMAKE_CUDA_COMPILER=/path_to_cuda/bin/nvcc"  CT_CUBLAS=1 pip
    install ctransformers --no-binary ctransformers```'
  created_at: 2023-10-13 11:23:24+00:00
  edited: false
  hidden: false
  id: 652936bca8cd884751739920
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f4094b1836c2fffb921bb55bdd5b18b8.svg
      fullname: MM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: melindmi
      type: user
    createdAt: '2023-10-13T12:23:24.000Z'
    data:
      status: closed
    id: 652936bca8cd884751739925
    type: status-change
  author: melindmi
  created_at: 2023-10-13 11:23:24+00:00
  id: 652936bca8cd884751739925
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6189ae567d9b289cdebafbd5/RwQj3PmM9-gzh-7o0GRVS.jpeg?w=200&h=200&f=face
      fullname: MM Kamani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mmkamani7
      type: user
    createdAt: '2023-10-17T18:13:11.000Z'
    data:
      edited: false
      editors:
      - mmkamani7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8915493488311768
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6189ae567d9b289cdebafbd5/RwQj3PmM9-gzh-7o0GRVS.jpeg?w=200&h=200&f=face
          fullname: MM Kamani
          isHf: false
          isPro: false
          name: mmkamani7
          type: user
        html: '<p>Solved my problem. Thanks</p>

          '
        raw: Solved my problem. Thanks
        updatedAt: '2023-10-17T18:13:11.502Z'
      numEdits: 0
      reactions: []
    id: 652eceb704a34a92829c9189
    type: comment
  author: mmkamani7
  content: Solved my problem. Thanks
  created_at: 2023-10-17 17:13:11+00:00
  edited: false
  hidden: false
  id: 652eceb704a34a92829c9189
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/01a82c8b62985d5580038fa7bebf43e1.svg
      fullname: Dipanshu Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dipshady
      type: user
    createdAt: '2023-11-01T11:48:11.000Z'
    data:
      edited: false
      editors:
      - dipshady
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9934467673301697
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/01a82c8b62985d5580038fa7bebf43e1.svg
          fullname: Dipanshu Gupta
          isHf: false
          isPro: false
          name: dipshady
          type: user
        html: '<p>Also worked for me, thanks! </p>

          '
        raw: 'Also worked for me, thanks! '
        updatedAt: '2023-11-01T11:48:11.356Z'
      numEdits: 0
      reactions: []
    id: 65423afb455609659107d69b
    type: comment
  author: dipshady
  content: 'Also worked for me, thanks! '
  created_at: 2023-11-01 10:48:11+00:00
  edited: false
  hidden: false
  id: 65423afb455609659107d69b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bc2c6d1edf98ab1c74a8999a453ef32d.svg
      fullname: William White
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: whitew1994
      type: user
    createdAt: '2023-11-30T08:50:43.000Z'
    data:
      edited: false
      editors:
      - whitew1994
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6012974977493286
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bc2c6d1edf98ab1c74a8999a453ef32d.svg
          fullname: William White
          isHf: false
          isPro: false
          name: whitew1994
          type: user
        html: '<p>Also sorted - thanks!</p>

          '
        raw: Also sorted - thanks!
        updatedAt: '2023-11-30T08:50:43.716Z'
      numEdits: 0
      reactions: []
    id: 65684ce36b1e4d61d8f63964
    type: comment
  author: whitew1994
  content: Also sorted - thanks!
  created_at: 2023-11-30 08:50:43+00:00
  edited: false
  hidden: false
  id: 65684ce36b1e4d61d8f63964
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d77aa820a25d3c08d77c4b78caabf34.svg
      fullname: Ben Menashe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: benm5678
      type: user
    createdAt: '2023-12-07T16:14:26.000Z'
    data:
      edited: false
      editors:
      - benm5678
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9814505577087402
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d77aa820a25d3c08d77c4b78caabf34.svg
          fullname: Ben Menashe
          isHf: false
          isPro: false
          name: benm5678
          type: user
        html: '<p>How can you know which CUDA version you need for a model? I didn''t
          see it specified in model card.  We''re on CUDA11.7, so need to find something
          that will work for that.</p>

          '
        raw: How can you know which CUDA version you need for a model? I didn't see
          it specified in model card.  We're on CUDA11.7, so need to find something
          that will work for that.
        updatedAt: '2023-12-07T16:14:26.533Z'
      numEdits: 0
      reactions: []
    id: 6571ef62990936e2e9f09710
    type: comment
  author: benm5678
  content: How can you know which CUDA version you need for a model? I didn't see
    it specified in model card.  We're on CUDA11.7, so need to find something that
    will work for that.
  created_at: 2023-12-07 16:14:26+00:00
  edited: false
  hidden: false
  id: 6571ef62990936e2e9f09710
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4094b1836c2fffb921bb55bdd5b18b8.svg
      fullname: MM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: melindmi
      type: user
    createdAt: '2023-12-07T18:04:34.000Z'
    data:
      edited: false
      editors:
      - melindmi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6040212512016296
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4094b1836c2fffb921bb55bdd5b18b8.svg
          fullname: MM
          isHf: false
          isPro: false
          name: melindmi
          type: user
        html: '<p>To check the CUDA version you need you can use: nvidia-smi</p>

          '
        raw: 'To check the CUDA version you need you can use: nvidia-smi


          '
        updatedAt: '2023-12-07T18:04:34.200Z'
      numEdits: 0
      reactions: []
    id: 65720932234b5c8ce8b29a9e
    type: comment
  author: melindmi
  content: 'To check the CUDA version you need you can use: nvidia-smi


    '
  created_at: 2023-12-07 18:04:34+00:00
  edited: false
  hidden: false
  id: 65720932234b5c8ce8b29a9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d77aa820a25d3c08d77c4b78caabf34.svg
      fullname: Ben Menashe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: benm5678
      type: user
    createdAt: '2023-12-07T19:35:57.000Z'
    data:
      edited: false
      editors:
      - benm5678
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.37630078196525574
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d77aa820a25d3c08d77c4b78caabf34.svg
          fullname: Ben Menashe
          isHf: false
          isPro: false
          name: benm5678
          type: user
        html: '<blockquote>

          <p>To check the CUDA version you need you can use: nvidia-smi</p>

          </blockquote>

          <p>nvcc --version showed 11.7.... Nvidia-smi shows below (11.4) -- so given
          below is my version, how can I know which model I can use?  Everything I
          try generates the Cuda 222 error.</p>

          <p>nvidia-smi<br>Thu Dec  7 17:25:41 2023<br>+-----------------------------------------------------------------------------+<br>|
          NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |<br>|-------------------------------+----------------------+----------------------+<br>|
          GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.
          ECC |<br>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute
          M. |<br>|                               |                      |               MIG
          M. |<br>|===============================+======================+======================|<br>|   0  NVIDIA
          A10G         On   | 00000000:00:1E.0 Off |                    0 |<br>|  0%   19C    P8    15W
          / 300W |      0MiB / 22731MiB |      0%      Default |<br>|                               |                      |                  N/A
          |<br>+-------------------------------+----------------------+----------------------+</p>

          <p>+-----------------------------------------------------------------------------+<br>|
          Processes:                                                                  |<br>|  GPU   GI   CI        PID   Type   Process
          name                  GPU Memory |<br>|        ID   ID                                                   Usage      |<br>|=============================================================================|<br>|  No
          running processes found                                                 |<br>+-----------------------------------------------------------------------------+</p>

          '
        raw: "> To check the CUDA version you need you can use: nvidia-smi\n\nnvcc\
          \ --version showed 11.7.... Nvidia-smi shows below (11.4) -- so given below\
          \ is my version, how can I know which model I can use?  Everything I try\
          \ generates the Cuda 222 error.\n\nnvidia-smi\nThu Dec  7 17:25:41 2023\
          \       \n+-----------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4\
          \     |\n|-------------------------------+----------------------+----------------------+\n\
          | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.\
          \ ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util\
          \  Compute M. |\n|                               |                     \
          \ |               MIG M. |\n|===============================+======================+======================|\n\
          |   0  NVIDIA A10G         On   | 00000000:00:1E.0 Off |               \
          \     0 |\n|  0%   19C    P8    15W / 300W |      0MiB / 22731MiB |    \
          \  0%      Default |\n|                               |                \
          \      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\
          \                                                                      \
          \         \n+-----------------------------------------------------------------------------+\n\
          | Processes:                                                           \
          \       |\n|  GPU   GI   CI        PID   Type   Process name           \
          \       GPU Memory |\n|        ID   ID                                 \
          \                  Usage      |\n|=============================================================================|\n\
          |  No running processes found                                          \
          \       |\n+-----------------------------------------------------------------------------+"
        updatedAt: '2023-12-07T19:35:57.191Z'
      numEdits: 0
      reactions: []
    id: 65721e9db268b1c5093dba2e
    type: comment
  author: benm5678
  content: "> To check the CUDA version you need you can use: nvidia-smi\n\nnvcc --version\
    \ showed 11.7.... Nvidia-smi shows below (11.4) -- so given below is my version,\
    \ how can I know which model I can use?  Everything I try generates the Cuda 222\
    \ error.\n\nnvidia-smi\nThu Dec  7 17:25:41 2023       \n+-----------------------------------------------------------------------------+\n\
    | NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4    \
    \ |\n|-------------------------------+----------------------+----------------------+\n\
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC\
    \ |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute\
    \ M. |\n|                               |                      |             \
    \  MIG M. |\n|===============================+======================+======================|\n\
    |   0  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0\
    \ |\n|  0%   19C    P8    15W / 300W |      0MiB / 22731MiB |      0%      Default\
    \ |\n|                               |                      |                \
    \  N/A |\n+-------------------------------+----------------------+----------------------+\n\
    \                                                                            \
    \   \n+-----------------------------------------------------------------------------+\n\
    | Processes:                                                                 \
    \ |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory\
    \ |\n|        ID   ID                                                   Usage\
    \      |\n|=============================================================================|\n\
    |  No running processes found                                                \
    \ |\n+-----------------------------------------------------------------------------+"
  created_at: 2023-12-07 19:35:57+00:00
  edited: false
  hidden: false
  id: 65721e9db268b1c5093dba2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4094b1836c2fffb921bb55bdd5b18b8.svg
      fullname: MM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: melindmi
      type: user
    createdAt: '2023-12-07T20:35:52.000Z'
    data:
      edited: false
      editors:
      - melindmi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6461758017539978
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4094b1836c2fffb921bb55bdd5b18b8.svg
          fullname: MM
          isHf: false
          isPro: false
          name: melindmi
          type: user
        html: '<p>You need to use nvcc version 11.4, this is the version your GPU
          driver supports. So you need to install nvcc version 11.4 then install te
          ctransformers with:<br>CMAKE_ARGS="-DCMAKE_CUDA_COMPILER=/path_to_cuda_nvcc_version_11.4/bin/nvcc"
          CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers</p>

          '
        raw: "You need to use nvcc version 11.4, this is the version your GPU driver\
          \ supports. So you need to install nvcc version 11.4 then install te ctransformers\
          \ with: \nCMAKE_ARGS=\"-DCMAKE_CUDA_COMPILER=/path_to_cuda_nvcc_version_11.4/bin/nvcc\"\
          \ CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers"
        updatedAt: '2023-12-07T20:35:52.288Z'
      numEdits: 0
      reactions: []
    id: 65722ca8d0ae612d41104193
    type: comment
  author: melindmi
  content: "You need to use nvcc version 11.4, this is the version your GPU driver\
    \ supports. So you need to install nvcc version 11.4 then install te ctransformers\
    \ with: \nCMAKE_ARGS=\"-DCMAKE_CUDA_COMPILER=/path_to_cuda_nvcc_version_11.4/bin/nvcc\"\
    \ CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers"
  created_at: 2023-12-07 20:35:52+00:00
  edited: false
  hidden: false
  id: 65722ca8d0ae612d41104193
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d77aa820a25d3c08d77c4b78caabf34.svg
      fullname: Ben Menashe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: benm5678
      type: user
    createdAt: '2023-12-08T01:32:55.000Z'
    data:
      edited: false
      editors:
      - benm5678
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6947076320648193
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d77aa820a25d3c08d77c4b78caabf34.svg
          fullname: Ben Menashe
          isHf: false
          isPro: false
          name: benm5678
          type: user
        html: "<blockquote>\n<p>You need to use nvcc version 11.4, this is the version\
          \ your GPU driver supports. So you need to install nvcc version 11.4 then\
          \ install te ctransformers with:<br>CMAKE_ARGS=\"-DCMAKE_CUDA_COMPILER=/path_to_cuda_nvcc_version_11.4/bin/nvcc\"\
          \ CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers</p>\n\
          </blockquote>\n<p>I was able to follow your steps and get it downgraded.\
          \  So now \"nvcc --version\" shows 11.4 &amp; the pip install was successful\
          \ (pointed at 11.4 path) -- however, it did not help...still same exact\
          \ error:</p>\n<pre><code>1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169):\
          \ stderr create_gpt_params_cuda: loading model /models/llama-2-13b-chat-hf.Q5_0\n\
          1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169): stderr ggml_init_cublas:\
          \ found 1 CUDA devices:\n1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169):\
          \ stderr   Device 0: NVIDIA A10G, compute capability 8.6\n1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169):\
          \ stderr \n1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169): stderr\
          \ CUDA error 222 at /build/sources/go-llama/llama.cpp/ggml-cuda.cu:5548:\
          \ the provided PTX was compiled with an unsupported toolchain.\n</code></pre>\n"
        raw: "> You need to use nvcc version 11.4, this is the version your GPU driver\
          \ supports. So you need to install nvcc version 11.4 then install te ctransformers\
          \ with: \n> CMAKE_ARGS=\"-DCMAKE_CUDA_COMPILER=/path_to_cuda_nvcc_version_11.4/bin/nvcc\"\
          \ CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers\n\nI was\
          \ able to follow your steps and get it downgraded.  So now \"nvcc --version\"\
          \ shows 11.4 & the pip install was successful (pointed at 11.4 path) --\
          \ however, it did not help...still same exact error:\n\n```\n1:25AM DBG\
          \ GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169): stderr create_gpt_params_cuda:\
          \ loading model /models/llama-2-13b-chat-hf.Q5_0\n1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169):\
          \ stderr ggml_init_cublas: found 1 CUDA devices:\n1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169):\
          \ stderr   Device 0: NVIDIA A10G, compute capability 8.6\n1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169):\
          \ stderr \n1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169): stderr\
          \ CUDA error 222 at /build/sources/go-llama/llama.cpp/ggml-cuda.cu:5548:\
          \ the provided PTX was compiled with an unsupported toolchain.\n```"
        updatedAt: '2023-12-08T01:32:55.926Z'
      numEdits: 0
      reactions: []
    id: 657272477749f1696d45baa0
    type: comment
  author: benm5678
  content: "> You need to use nvcc version 11.4, this is the version your GPU driver\
    \ supports. So you need to install nvcc version 11.4 then install te ctransformers\
    \ with: \n> CMAKE_ARGS=\"-DCMAKE_CUDA_COMPILER=/path_to_cuda_nvcc_version_11.4/bin/nvcc\"\
    \ CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers\n\nI was able\
    \ to follow your steps and get it downgraded.  So now \"nvcc --version\" shows\
    \ 11.4 & the pip install was successful (pointed at 11.4 path) -- however, it\
    \ did not help...still same exact error:\n\n```\n1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169):\
    \ stderr create_gpt_params_cuda: loading model /models/llama-2-13b-chat-hf.Q5_0\n\
    1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169): stderr ggml_init_cublas:\
    \ found 1 CUDA devices:\n1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169):\
    \ stderr   Device 0: NVIDIA A10G, compute capability 8.6\n1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169):\
    \ stderr \n1:25AM DBG GRPC(llama-2-13b-chat-hf.Q5_0-127.0.0.1:39169): stderr CUDA\
    \ error 222 at /build/sources/go-llama/llama.cpp/ggml-cuda.cu:5548: the provided\
    \ PTX was compiled with an unsupported toolchain.\n```"
  created_at: 2023-12-08 01:32:55+00:00
  edited: false
  hidden: false
  id: 657272477749f1696d45baa0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4094b1836c2fffb921bb55bdd5b18b8.svg
      fullname: MM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: melindmi
      type: user
    createdAt: '2023-12-08T10:32:05.000Z'
    data:
      edited: false
      editors:
      - melindmi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.950494647026062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4094b1836c2fffb921bb55bdd5b18b8.svg
          fullname: MM
          isHf: false
          isPro: false
          name: melindmi
          type: user
        html: '<p>Not sure what is the problem in your case. The error I had was different
          pointing to ctransformers: "CUDA error 222 at /home/runner/work/ctransformers/ctransformers/models/ggml/ggml-cuda.cu:6045:
          the provided PTX was compiled with an unsupported toolchain."</p>

          '
        raw: 'Not sure what is the problem in your case. The error I had was different
          pointing to ctransformers: "CUDA error 222 at /home/runner/work/ctransformers/ctransformers/models/ggml/ggml-cuda.cu:6045:
          the provided PTX was compiled with an unsupported toolchain."'
        updatedAt: '2023-12-08T10:32:05.744Z'
      numEdits: 0
      reactions: []
    id: 6572f0a5ccb9101b60ae3a06
    type: comment
  author: melindmi
  content: 'Not sure what is the problem in your case. The error I had was different
    pointing to ctransformers: "CUDA error 222 at /home/runner/work/ctransformers/ctransformers/models/ggml/ggml-cuda.cu:6045:
    the provided PTX was compiled with an unsupported toolchain."'
  created_at: 2023-12-08 10:32:05+00:00
  edited: false
  hidden: false
  id: 6572f0a5ccb9101b60ae3a06
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: TheBloke/Llama-2-13B-chat-GGML
repo_type: model
status: closed
target_branch: null
title: CUDA error - the provided PTX was compiled with an unsupported toolchain
