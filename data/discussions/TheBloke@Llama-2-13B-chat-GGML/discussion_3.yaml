!!python/object:huggingface_hub.community.DiscussionWithDetails
author: r3gm
conflicting_files: null
created_at: 2023-07-18 23:00:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e6e242b6a40bf941e520d5/cM8Pu9-vy7U7D9fz38wr6.jpeg?w=200&h=200&f=face
      fullname: Roger Condori
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: r3gm
      type: user
    createdAt: '2023-07-19T00:00:24.000Z'
    data:
      edited: false
      editors:
      - r3gm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3571764826774597
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e6e242b6a40bf941e520d5/cM8Pu9-vy7U7D9fz38wr6.jpeg?w=200&h=200&f=face
          fullname: Roger Condori
          isHf: false
          isPro: false
          name: r3gm
          type: user
        html: '<p>Using the llama-cpp-python library<br><a rel="nofollow" href="https://github.com/R3gm/InsightSolver-Colab/blob/main/LLM_Inference_with_llama_cpp_python__Llama_2_13b_chat.ipynb">https://github.com/R3gm/InsightSolver-Colab/blob/main/LLM_Inference_with_llama_cpp_python__Llama_2_13b_chat.ipynb</a></p>

          '
        raw: "Using the llama-cpp-python library\r\nhttps://github.com/R3gm/InsightSolver-Colab/blob/main/LLM_Inference_with_llama_cpp_python__Llama_2_13b_chat.ipynb"
        updatedAt: '2023-07-19T00:00:24.538Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\u2764\uFE0F"
        users:
        - firejake308
        - Gidinated
        - gaobowen
        - ujjal13
        - dingdongyouarewrong
        - hydro780
    id: 64b72798b727f8771ab47973
    type: comment
  author: r3gm
  content: "Using the llama-cpp-python library\r\nhttps://github.com/R3gm/InsightSolver-Colab/blob/main/LLM_Inference_with_llama_cpp_python__Llama_2_13b_chat.ipynb"
  created_at: 2023-07-18 23:00:24+00:00
  edited: false
  hidden: false
  id: 64b72798b727f8771ab47973
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-07-19T06:15:40.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9146760106086731
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;r3gm&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/r3gm\">@<span class=\"\
          underline\">r3gm</span></a></span>\n\n\t</span></span> , Hii can you show\
          \ an example for CPU basis also for Llama 2 13b models </p>\n"
        raw: '@r3gm , Hii can you show an example for CPU basis also for Llama 2 13b
          models '
        updatedAt: '2023-07-19T06:15:40.213Z'
      numEdits: 0
      reactions: []
    id: 64b77f8c037d6452a3133c18
    type: comment
  author: deepakkaura26
  content: '@r3gm , Hii can you show an example for CPU basis also for Llama 2 13b
    models '
  created_at: 2023-07-19 05:15:40+00:00
  edited: false
  hidden: false
  id: 64b77f8c037d6452a3133c18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5dc2460cb7654b8a4290440adc8a4c46.svg
      fullname: Venu Vasudevan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: venuv62
      type: user
    createdAt: '2023-07-19T15:18:23.000Z'
    data:
      edited: false
      editors:
      - venuv62
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7350828647613525
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5dc2460cb7654b8a4290440adc8a4c46.svg
          fullname: Venu Vasudevan
          isHf: false
          isPro: false
          name: venuv62
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;r3gm&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/r3gm\">@<span class=\"\
          underline\">r3gm</span></a></span>\n\n\t</span></span> , any pointers on\
          \ how to compile for Metal and run locally on an M2.thx</p>\n"
        raw: '@r3gm , any pointers on how to compile for Metal and run locally on
          an M2.thx'
        updatedAt: '2023-07-19T15:18:23.781Z'
      numEdits: 0
      reactions: []
    id: 64b7febfda8017900e8b108d
    type: comment
  author: venuv62
  content: '@r3gm , any pointers on how to compile for Metal and run locally on an
    M2.thx'
  created_at: 2023-07-19 14:18:23+00:00
  edited: false
  hidden: false
  id: 64b7febfda8017900e8b108d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6a5bde4a2ea6ffd1a7552e6821615af.svg
      fullname: Rob
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kroonen
      type: user
    createdAt: '2023-07-22T13:04:26.000Z'
    data:
      edited: false
      editors:
      - kroonen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.653211236000061
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6a5bde4a2ea6ffd1a7552e6821615af.svg
          fullname: Rob
          isHf: false
          isPro: false
          name: kroonen
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;r3gm&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/r3gm\"\
          >@<span class=\"underline\">r3gm</span></a></span>\n\n\t</span></span> ,\
          \ any pointers on how to compile for Metal and run locally on an M2.thx</p>\n\
          </blockquote>\n<p>follow this guide : <a rel=\"nofollow\" href=\"https://llama-cpp-python.readthedocs.io/en/latest/install/macos/\"\
          >https://llama-cpp-python.readthedocs.io/en/latest/install/macos/</a></p>\n"
        raw: '> @r3gm , any pointers on how to compile for Metal and run locally on
          an M2.thx


          follow this guide : https://llama-cpp-python.readthedocs.io/en/latest/install/macos/'
        updatedAt: '2023-07-22T13:04:26.046Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - venuv62
    id: 64bbd3da4561d0aca27445cb
    type: comment
  author: kroonen
  content: '> @r3gm , any pointers on how to compile for Metal and run locally on
    an M2.thx


    follow this guide : https://llama-cpp-python.readthedocs.io/en/latest/install/macos/'
  created_at: 2023-07-22 12:04:26+00:00
  edited: false
  hidden: false
  id: 64bbd3da4561d0aca27445cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5dc2460cb7654b8a4290440adc8a4c46.svg
      fullname: Venu Vasudevan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: venuv62
      type: user
    createdAt: '2023-07-23T15:21:30.000Z'
    data:
      edited: true
      editors:
      - venuv62
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8001096844673157
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5dc2460cb7654b8a4290440adc8a4c46.svg
          fullname: Venu Vasudevan
          isHf: false
          isPro: false
          name: venuv62
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;r3gm&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/r3gm\">@<span class=\"\
          underline\">r3gm</span></a></span>\n\n\t</span></span> or <span data-props=\"\
          {&quot;user&quot;:&quot;kroonen&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/kroonen\">@<span class=\"underline\">kroonen</span></a></span>\n\
          \n\t</span></span> , stayed with ggml3 and 4.0 as recommended but get an\
          \ Illegal Instruction: 4. Any suggestions?</p>\n<p>(llama2-metal) R77NK6JXG7:llama2\
          \ venuvasudevan$ pip list|grep llama<br>llama-cpp-python  0.1.74</p>\n<p><a\
          \ rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/635ff8473605bd411c16d05c/wMaI_uJ5ZLBxsHWvPWwL-.png\"\
          ><img alt=\"Screenshot 2023-07-23 at 10.19.30 AM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/635ff8473605bd411c16d05c/wMaI_uJ5ZLBxsHWvPWwL-.png\"\
          ></a></p>\n"
        raw: '@r3gm or @kroonen , stayed with ggml3 and 4.0 as recommended but get
          an Illegal Instruction: 4. Any suggestions?


          (llama2-metal) R77NK6JXG7:llama2 venuvasudevan$ pip list|grep llama

          llama-cpp-python  0.1.74


          ![Screenshot 2023-07-23 at 10.19.30 AM.png](https://cdn-uploads.huggingface.co/production/uploads/635ff8473605bd411c16d05c/wMaI_uJ5ZLBxsHWvPWwL-.png)

          '
        updatedAt: '2023-07-23T15:23:39.704Z'
      numEdits: 2
      reactions: []
    id: 64bd457af671da974e2d3e66
    type: comment
  author: venuv62
  content: '@r3gm or @kroonen , stayed with ggml3 and 4.0 as recommended but get an
    Illegal Instruction: 4. Any suggestions?


    (llama2-metal) R77NK6JXG7:llama2 venuvasudevan$ pip list|grep llama

    llama-cpp-python  0.1.74


    ![Screenshot 2023-07-23 at 10.19.30 AM.png](https://cdn-uploads.huggingface.co/production/uploads/635ff8473605bd411c16d05c/wMaI_uJ5ZLBxsHWvPWwL-.png)

    '
  created_at: 2023-07-23 14:21:30+00:00
  edited: true
  hidden: false
  id: 64bd457af671da974e2d3e66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e73618acff009aade0bf574103d48026.svg
      fullname: Krishna Pagare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Occupying-Mars
      type: user
    createdAt: '2023-08-08T07:56:45.000Z'
    data:
      edited: false
      editors:
      - Occupying-Mars
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9833537936210632
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e73618acff009aade0bf574103d48026.svg
          fullname: Krishna Pagare
          isHf: false
          isPro: false
          name: Occupying-Mars
          type: user
        html: "<p>is there someway we could increase the context length? currently\
          \ its 512 tokens. <span data-props=\"{&quot;user&quot;:&quot;r3gm&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/r3gm\"\
          >@<span class=\"underline\">r3gm</span></a></span>\n\n\t</span></span> </p>\n"
        raw: 'is there someway we could increase the context length? currently its
          512 tokens. @r3gm '
        updatedAt: '2023-08-08T07:56:45.642Z'
      numEdits: 0
      reactions: []
    id: 64d1f53de9cac0020be4cda6
    type: comment
  author: Occupying-Mars
  content: 'is there someway we could increase the context length? currently its 512
    tokens. @r3gm '
  created_at: 2023-08-08 06:56:45+00:00
  edited: false
  hidden: false
  id: 64d1f53de9cac0020be4cda6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fcd1cc9d04c9e43158f5449712ff2400.svg
      fullname: VITOR M PATALANO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patalanov
      type: user
    createdAt: '2023-08-14T03:36:10.000Z'
    data:
      edited: false
      editors:
      - patalanov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4078596830368042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fcd1cc9d04c9e43158f5449712ff2400.svg
          fullname: VITOR M PATALANO
          isHf: false
          isPro: false
          name: patalanov
          type: user
        html: '<p>yes, you pass the arg n_ctx, like so:</p>

          <p>llm = LlamaLLM(model_path="./models/7B/llama-2-7b-chat.ggmlv3.q4_K_M.bin",
          n_ctx=2048)</p>

          '
        raw: 'yes, you pass the arg n_ctx, like so:


          llm = LlamaLLM(model_path="./models/7B/llama-2-7b-chat.ggmlv3.q4_K_M.bin",
          n_ctx=2048)'
        updatedAt: '2023-08-14T03:36:10.551Z'
      numEdits: 0
      reactions: []
    id: 64d9a12ad38302bf8010afa3
    type: comment
  author: patalanov
  content: 'yes, you pass the arg n_ctx, like so:


    llm = LlamaLLM(model_path="./models/7B/llama-2-7b-chat.ggmlv3.q4_K_M.bin", n_ctx=2048)'
  created_at: 2023-08-14 02:36:10+00:00
  edited: false
  hidden: false
  id: 64d9a12ad38302bf8010afa3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e73618acff009aade0bf574103d48026.svg
      fullname: Krishna Pagare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Occupying-Mars
      type: user
    createdAt: '2023-08-19T14:24:07.000Z'
    data:
      edited: false
      editors:
      - Occupying-Mars
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.904836118221283
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e73618acff009aade0bf574103d48026.svg
          fullname: Krishna Pagare
          isHf: false
          isPro: false
          name: Occupying-Mars
          type: user
        html: '<p>oh thanks</p>

          '
        raw: oh thanks
        updatedAt: '2023-08-19T14:24:07.482Z'
      numEdits: 0
      reactions: []
    id: 64e0d087ee29045622658644
    type: comment
  author: Occupying-Mars
  content: oh thanks
  created_at: 2023-08-19 13:24:07+00:00
  edited: false
  hidden: false
  id: 64e0d087ee29045622658644
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e73618acff009aade0bf574103d48026.svg
      fullname: Krishna Pagare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Occupying-Mars
      type: user
    createdAt: '2023-09-16T08:54:51.000Z'
    data:
      edited: false
      editors:
      - Occupying-Mars
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8806029558181763
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e73618acff009aade0bf574103d48026.svg
          fullname: Krishna Pagare
          isHf: false
          isPro: false
          name: Occupying-Mars
          type: user
        html: '<p>hey i tried running this again today and for some reason it doesn''t
          seem to work when setting lcpp_llm properties and stuff it throws an attribution
          error i have tried running it in colab, kaggle and 3 different machines
          just doesn''t seem to work<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/631489e8d8dc27b2f7f19acb/YwylHHimjHsvIbdxUTHdW.png"><img
          alt="Screenshot 2023-09-16 142418.png" src="https://cdn-uploads.huggingface.co/production/uploads/631489e8d8dc27b2f7f19acb/YwylHHimjHsvIbdxUTHdW.png"></a></p>

          '
        raw: "hey i tried running this again today and for some reason it doesn't\
          \ seem to work when setting lcpp_llm properties and stuff it throws an attribution\
          \ error i have tried running it in colab, kaggle and 3 different machines\
          \ just doesn't seem to work \n![Screenshot 2023-09-16 142418.png](https://cdn-uploads.huggingface.co/production/uploads/631489e8d8dc27b2f7f19acb/YwylHHimjHsvIbdxUTHdW.png)\n"
        updatedAt: '2023-09-16T08:54:51.626Z'
      numEdits: 0
      reactions: []
    id: 65056d5b1ae953ff2f32b1da
    type: comment
  author: Occupying-Mars
  content: "hey i tried running this again today and for some reason it doesn't seem\
    \ to work when setting lcpp_llm properties and stuff it throws an attribution\
    \ error i have tried running it in colab, kaggle and 3 different machines just\
    \ doesn't seem to work \n![Screenshot 2023-09-16 142418.png](https://cdn-uploads.huggingface.co/production/uploads/631489e8d8dc27b2f7f19acb/YwylHHimjHsvIbdxUTHdW.png)\n"
  created_at: 2023-09-16 07:54:51+00:00
  edited: false
  hidden: false
  id: 65056d5b1ae953ff2f32b1da
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Llama-2-13B-chat-GGML
repo_type: model
status: open
target_branch: null
title: Notebook to test Llama 2 in Colab free tier
