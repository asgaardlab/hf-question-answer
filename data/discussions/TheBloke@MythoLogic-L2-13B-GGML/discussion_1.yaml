!!python/object:huggingface_hub.community.DiscussionWithDetails
author: boqsc
conflicting_files: null
created_at: 2023-08-04 18:43:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d563e0483fde1b42b500c09437954e5b.svg
      fullname: boqsc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: boqsc
      type: user
    createdAt: '2023-08-04T19:43:42.000Z'
    data:
      edited: true
      editors:
      - boqsc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9332458972930908
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d563e0483fde1b42b500c09437954e5b.svg
          fullname: boqsc
          isHf: false
          isPro: false
          name: boqsc
          type: user
        html: '<p>LLAMA2 models are horrible at story logic. Nous Hermes1 with LLAMA1
          was best for story generation and Nous GPT4xVicuna was the most satifying
          to interact with.</p>

          <p>In the latest prompt answer of MythoLogic, I even had a story generated
          where it starts with father being dead and a few sentences in suddenly father
          is speaking and interacting.<br>This never happened with LLAMA1 models.</p>

          '
        raw: 'LLAMA2 models are horrible at story logic. Nous Hermes1 with LLAMA1
          was best for story generation and Nous GPT4xVicuna was the most satifying
          to interact with.


          In the latest prompt answer of MythoLogic, I even had a story generated
          where it starts with father being dead and a few sentences in suddenly father
          is speaking and interacting.

          This never happened with LLAMA1 models.'
        updatedAt: '2023-08-04T19:47:21.825Z'
      numEdits: 4
      reactions: []
    id: 64cd54eed2a781d3f0c25ce9
    type: comment
  author: boqsc
  content: 'LLAMA2 models are horrible at story logic. Nous Hermes1 with LLAMA1 was
    best for story generation and Nous GPT4xVicuna was the most satifying to interact
    with.


    In the latest prompt answer of MythoLogic, I even had a story generated where
    it starts with father being dead and a few sentences in suddenly father is speaking
    and interacting.

    This never happened with LLAMA1 models.'
  created_at: 2023-08-04 18:43:42+00:00
  edited: true
  hidden: false
  id: 64cd54eed2a781d3f0c25ce9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/01fce1618b56808c97d98baf2232a584.svg
      fullname: J. Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JWNoctis
      type: user
    createdAt: '2023-08-05T01:15:27.000Z'
    data:
      edited: false
      editors:
      - JWNoctis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9127069115638733
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/01fce1618b56808c97d98baf2232a584.svg
          fullname: J. Wu
          isHf: false
          isPro: false
          name: JWNoctis
          type: user
        html: '<p>One thing to look out for is if you are using llama.cpp to run these
          models, there are certain default hyperparameter values that are incorrect
          for L2 models, specifically <code>-eps</code> which should be 1e-5, which
          will damage generation quality if not overridden in command line. It does
          appear to make a perceptible difference.</p>

          <p>I think their next generation model format, GGUF, should handle it by
          default.</p>

          '
        raw: 'One thing to look out for is if you are using llama.cpp to run these
          models, there are certain default hyperparameter values that are incorrect
          for L2 models, specifically ```-eps``` which should be 1e-5, which will
          damage generation quality if not overridden in command line. It does appear
          to make a perceptible difference.


          I think their next generation model format, GGUF, should handle it by default.'
        updatedAt: '2023-08-05T01:15:27.260Z'
      numEdits: 0
      reactions: []
    id: 64cda2af37a8b7adaf32cb01
    type: comment
  author: JWNoctis
  content: 'One thing to look out for is if you are using llama.cpp to run these models,
    there are certain default hyperparameter values that are incorrect for L2 models,
    specifically ```-eps``` which should be 1e-5, which will damage generation quality
    if not overridden in command line. It does appear to make a perceptible difference.


    I think their next generation model format, GGUF, should handle it by default.'
  created_at: 2023-08-05 00:15:27+00:00
  edited: false
  hidden: false
  id: 64cda2af37a8b7adaf32cb01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-05T09:17:31.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9215782880783081
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>One thing to look out for is if you are using llama.cpp to run these
          models, there are certain default hyperparameter values that are incorrect
          for L2 models, specifically <code>-eps</code> which should be 1e-5, which
          will damage generation quality if not overridden in command line. It does
          appear to make a perceptible difference.</p>

          <p>I think their next generation model format, GGUF, should handle it by
          default.</p>

          </blockquote>

          <p>Oh that''s interesting, I didn''t know that.  So I should add <code>-eps
          1e-5</code> to my llama.cpp example for all L2 models?  Anything else I
          should change at the same time?</p>

          '
        raw: "> One thing to look out for is if you are using llama.cpp to run these\
          \ models, there are certain default hyperparameter values that are incorrect\
          \ for L2 models, specifically ```-eps``` which should be 1e-5, which will\
          \ damage generation quality if not overridden in command line. It does appear\
          \ to make a perceptible difference.\n> \n> I think their next generation\
          \ model format, GGUF, should handle it by default.\n\nOh that's interesting,\
          \ I didn't know that.  So I should add `-eps 1e-5` to my llama.cpp example\
          \ for all L2 models?  Anything else I should change at the same time?"
        updatedAt: '2023-08-05T09:17:53.027Z'
      numEdits: 1
      reactions: []
    id: 64ce13ab2f1f9578a0ec568c
    type: comment
  author: TheBloke
  content: "> One thing to look out for is if you are using llama.cpp to run these\
    \ models, there are certain default hyperparameter values that are incorrect for\
    \ L2 models, specifically ```-eps``` which should be 1e-5, which will damage generation\
    \ quality if not overridden in command line. It does appear to make a perceptible\
    \ difference.\n> \n> I think their next generation model format, GGUF, should\
    \ handle it by default.\n\nOh that's interesting, I didn't know that.  So I should\
    \ add `-eps 1e-5` to my llama.cpp example for all L2 models?  Anything else I\
    \ should change at the same time?"
  created_at: 2023-08-05 08:17:31+00:00
  edited: true
  hidden: false
  id: 64ce13ab2f1f9578a0ec568c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/01fce1618b56808c97d98baf2232a584.svg
      fullname: J. Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JWNoctis
      type: user
    createdAt: '2023-08-05T15:10:54.000Z'
    data:
      edited: true
      editors:
      - JWNoctis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9333395957946777
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/01fce1618b56808c97d98baf2232a584.svg
          fullname: J. Wu
          isHf: false
          isPro: false
          name: JWNoctis
          type: user
        html: '<blockquote>

          <blockquote>

          <p>One thing to look out for is if you are using llama.cpp to run these
          models, there are certain default hyperparameter values that are incorrect
          for L2 models, specifically <code>-eps</code> which should be 1e-5, which
          will damage generation quality if not overridden in command line. It does
          appear to make a perceptible difference.</p>

          <p>I think their next generation model format, GGUF, should handle it by
          default.</p>

          </blockquote>

          <p>Oh that''s interesting, I didn''t know that.  So I should add <code>-eps
          1e-5</code> to my llama.cpp example for all L2 models?  Anything else I
          should change at the same time?</p>

          </blockquote>

          <p>More details could be found in issue #2373 and the corresponding push
          request in llama.cpp repo. I don''t have the technical background to understand
          why this has the effect it had, nor what finetuning would do with this,
          if anything. It might be interesting to test, though.</p>

          <p>EDIT: llama.cpp PR #2384 should have made that unnecessary - Didn''t
          see that earlier. No idea what went wrong with those finetuned llama2 models
          then. FWIW StableBeluga-13B for instance certainly doesn''t seem to me to
          do any worse than Vicuna 1.3 13B or any other finetuned L1 models I''ve
          tried, in terms of storytelling quality and chances of catastrophic error,
          but then I''m not trying anything complicated with them.</p>

          '
        raw: "> > One thing to look out for is if you are using llama.cpp to run these\
          \ models, there are certain default hyperparameter values that are incorrect\
          \ for L2 models, specifically ```-eps``` which should be 1e-5, which will\
          \ damage generation quality if not overridden in command line. It does appear\
          \ to make a perceptible difference.\n> > \n> > I think their next generation\
          \ model format, GGUF, should handle it by default.\n> \n> Oh that's interesting,\
          \ I didn't know that.  So I should add `-eps 1e-5` to my llama.cpp example\
          \ for all L2 models?  Anything else I should change at the same time?\n\n\
          More details could be found in issue #2373 and the corresponding push request\
          \ in llama.cpp repo. I don't have the technical background to understand\
          \ why this has the effect it had, nor what finetuning would do with this,\
          \ if anything. It might be interesting to test, though.\n\nEDIT: llama.cpp\
          \ PR #2384 should have made that unnecessary - Didn't see that earlier.\
          \ No idea what went wrong with those finetuned llama2 models then. FWIW\
          \ StableBeluga-13B for instance certainly doesn't seem to me to do any worse\
          \ than Vicuna 1.3 13B or any other finetuned L1 models I've tried, in terms\
          \ of storytelling quality and chances of catastrophic error, but then I'm\
          \ not trying anything complicated with them."
        updatedAt: '2023-08-05T15:31:46.165Z'
      numEdits: 5
      reactions: []
    id: 64ce667ec9d00e3847bf72ab
    type: comment
  author: JWNoctis
  content: "> > One thing to look out for is if you are using llama.cpp to run these\
    \ models, there are certain default hyperparameter values that are incorrect for\
    \ L2 models, specifically ```-eps``` which should be 1e-5, which will damage generation\
    \ quality if not overridden in command line. It does appear to make a perceptible\
    \ difference.\n> > \n> > I think their next generation model format, GGUF, should\
    \ handle it by default.\n> \n> Oh that's interesting, I didn't know that.  So\
    \ I should add `-eps 1e-5` to my llama.cpp example for all L2 models?  Anything\
    \ else I should change at the same time?\n\nMore details could be found in issue\
    \ #2373 and the corresponding push request in llama.cpp repo. I don't have the\
    \ technical background to understand why this has the effect it had, nor what\
    \ finetuning would do with this, if anything. It might be interesting to test,\
    \ though.\n\nEDIT: llama.cpp PR #2384 should have made that unnecessary - Didn't\
    \ see that earlier. No idea what went wrong with those finetuned llama2 models\
    \ then. FWIW StableBeluga-13B for instance certainly doesn't seem to me to do\
    \ any worse than Vicuna 1.3 13B or any other finetuned L1 models I've tried, in\
    \ terms of storytelling quality and chances of catastrophic error, but then I'm\
    \ not trying anything complicated with them."
  created_at: 2023-08-05 14:10:54+00:00
  edited: true
  hidden: false
  id: 64ce667ec9d00e3847bf72ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-05T20:54:12.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9054611921310425
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK thanks for the details</p>

          '
        raw: OK thanks for the details
        updatedAt: '2023-08-05T20:54:12.390Z'
      numEdits: 0
      reactions: []
    id: 64ceb6f47a7305c5890c7402
    type: comment
  author: TheBloke
  content: OK thanks for the details
  created_at: 2023-08-05 19:54:12+00:00
  edited: false
  hidden: false
  id: 64ceb6f47a7305c5890c7402
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/975f3c34076f0f075d1867849c0a6389.svg
      fullname: BERKUT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: berkut1
      type: user
    createdAt: '2023-08-05T23:18:00.000Z'
    data:
      edited: true
      editors:
      - berkut1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8674031496047974
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/975f3c34076f0f075d1867849c0a6389.svg
          fullname: BERKUT
          isHf: false
          isPro: false
          name: berkut1
          type: user
        html: '<p>Yes, I also noticed this and started to double-check all the parameters
          with the model config. Like this <a href="https://huggingface.co/Gryphe/MythoLogic-L2-13b/blob/main/config.json">https://huggingface.co/Gryphe/MythoLogic-L2-13b/blob/main/config.json</a><br>It
          also very useful for 8k-16k models to check their original config.json,
          because they have settings that you should manually add to llama.cpp (like
          rope_scaling). I hope in GGUF it will be automatically.</p>

          '
        raw: "Yes, I also noticed this and started to double-check all the parameters\
          \ with the model config. Like this https://huggingface.co/Gryphe/MythoLogic-L2-13b/blob/main/config.json\
          \ \nIt also very useful for 8k-16k models to check their original config.json,\
          \ because they have settings that you should manually add to llama.cpp (like\
          \ rope_scaling). I hope in GGUF it will be automatically."
        updatedAt: '2023-08-05T23:19:15.973Z'
      numEdits: 1
      reactions: []
    id: 64ced8a84a204a4d12fc3209
    type: comment
  author: berkut1
  content: "Yes, I also noticed this and started to double-check all the parameters\
    \ with the model config. Like this https://huggingface.co/Gryphe/MythoLogic-L2-13b/blob/main/config.json\
    \ \nIt also very useful for 8k-16k models to check their original config.json,\
    \ because they have settings that you should manually add to llama.cpp (like rope_scaling).\
    \ I hope in GGUF it will be automatically."
  created_at: 2023-08-05 22:18:00+00:00
  edited: true
  hidden: false
  id: 64ced8a84a204a4d12fc3209
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-05T23:27:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9382949471473694
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Can you elaborate on that? What settings are you adding?</p>

          <p>I can look to put those settings in the readme automatically if i know
          what I should be looking for in config json. Thanks </p>

          '
        raw: 'Can you elaborate on that? What settings are you adding?


          I can look to put those settings in the readme automatically if i know what
          I should be looking for in config json. Thanks '
        updatedAt: '2023-08-05T23:27:15.940Z'
      numEdits: 0
      reactions: []
    id: 64cedad37d8cc9f070fe6b94
    type: comment
  author: TheBloke
  content: 'Can you elaborate on that? What settings are you adding?


    I can look to put those settings in the readme automatically if i know what I
    should be looking for in config json. Thanks '
  created_at: 2023-08-05 22:27:15+00:00
  edited: false
  hidden: false
  id: 64cedad37d8cc9f070fe6b94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/975f3c34076f0f075d1867849c0a6389.svg
      fullname: BERKUT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: berkut1
      type: user
    createdAt: '2023-08-05T23:42:11.000Z'
    data:
      edited: true
      editors:
      - berkut1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7345577478408813
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/975f3c34076f0f075d1867849c0a6389.svg
          fullname: BERKUT
          isHf: false
          isPro: false
          name: berkut1
          type: user
        html: "<blockquote>\n<p>Can you elaborate on that? What settings are you adding?</p>\n\
          <p>I can look to put those settings in the readme automatically if i know\
          \ what I should be looking for in config json. Thanks</p>\n</blockquote>\n\
          <p>At this moment on this:</p>\n<pre><code>rms_norm_eps\nrope_scaling\n\
          </code></pre>\n<p><code>rope_scaling</code> if <code>null</code> = that\
          \ means <code>compress_pos_emb =   1</code> aka <code>--rope-freq-scale\
          \ 1.0</code><br>if  <code>\"type\": \"linear\"</code></p>\n<pre><code>\"\
          rope_scaling\": {\n    \"factor\": 4.0,\n    \"type\": \"linear\"\n  },\n\
          </code></pre>\n<p> that means llama.cpp <code>compress_pos_emb = 4</code>\
          \ aka <code>--rope-freq-scale 0.25</code> (1.0\\4  = 0.25)</p>\n<p>I see\
          \ that in all readme you add this :</p>\n<blockquote>\n<p>Change -c 2048\
          \ to the desired sequence length for this model. For example, -c 4096 for\
          \ a Llama 2 model. For models that use RoPE, add --rope-freq-base 10000\
          \ --rope-freq-scale 0.5 for doubled context, or --rope-freq-base 10000 --rope-freq-scale\
          \ 0.25 for 4x context.</p>\n</blockquote>\n<p>That confuse, because llama\
          \ 2 by default has context 4k and you shouldn't touch <code>--rope-freq-scale</code>,\
          \ or if you set <code>--rope-freq-scale 0.5</code> that means you double\
          \ 4k to 8k.</p>\n<p>UPD:  <code>compress_pos_emb</code> that is for text-generation-webui</p>\n"
        raw: "> Can you elaborate on that? What settings are you adding?\n> \n> I\
          \ can look to put those settings in the readme automatically if i know what\
          \ I should be looking for in config json. Thanks\n\nAt this moment on this:\n\
          ```\nrms_norm_eps\nrope_scaling\n```\n`rope_scaling` if `null` = that means\
          \ `compress_pos_emb =   1` aka `--rope-freq-scale 1.0`\nif  `\"type\": \"\
          linear\"`\n```\n\"rope_scaling\": {\n    \"factor\": 4.0,\n    \"type\"\
          : \"linear\"\n  },\n```\n that means llama.cpp `compress_pos_emb = 4` aka\
          \ `--rope-freq-scale 0.25` (1.0\\4  = 0.25)\n\nI see that in all readme\
          \ you add this :\n>Change -c 2048 to the desired sequence length for this\
          \ model. For example, -c 4096 for a Llama 2 model. For models that use RoPE,\
          \ add --rope-freq-base 10000 --rope-freq-scale 0.5 for doubled context,\
          \ or --rope-freq-base 10000 --rope-freq-scale 0.25 for 4x context.\n\nThat\
          \ confuse, because llama 2 by default has context 4k and you shouldn't touch\
          \ `--rope-freq-scale`, or if you set `--rope-freq-scale 0.5` that means\
          \ you double 4k to 8k.\n\nUPD:  `compress_pos_emb` that is for text-generation-webui"
        updatedAt: '2023-08-05T23:46:44.023Z'
      numEdits: 5
      reactions: []
    id: 64cede53132efbe2dcafbacb
    type: comment
  author: berkut1
  content: "> Can you elaborate on that? What settings are you adding?\n> \n> I can\
    \ look to put those settings in the readme automatically if i know what I should\
    \ be looking for in config json. Thanks\n\nAt this moment on this:\n```\nrms_norm_eps\n\
    rope_scaling\n```\n`rope_scaling` if `null` = that means `compress_pos_emb = \
    \  1` aka `--rope-freq-scale 1.0`\nif  `\"type\": \"linear\"`\n```\n\"rope_scaling\"\
    : {\n    \"factor\": 4.0,\n    \"type\": \"linear\"\n  },\n```\n that means llama.cpp\
    \ `compress_pos_emb = 4` aka `--rope-freq-scale 0.25` (1.0\\4  = 0.25)\n\nI see\
    \ that in all readme you add this :\n>Change -c 2048 to the desired sequence length\
    \ for this model. For example, -c 4096 for a Llama 2 model. For models that use\
    \ RoPE, add --rope-freq-base 10000 --rope-freq-scale 0.5 for doubled context,\
    \ or --rope-freq-base 10000 --rope-freq-scale 0.25 for 4x context.\n\nThat confuse,\
    \ because llama 2 by default has context 4k and you shouldn't touch `--rope-freq-scale`,\
    \ or if you set `--rope-freq-scale 0.5` that means you double 4k to 8k.\n\nUPD:\
    \  `compress_pos_emb` that is for text-generation-webui"
  created_at: 2023-08-05 22:42:11+00:00
  edited: true
  hidden: false
  id: 64cede53132efbe2dcafbacb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/MythoLogic-L2-13B-GGML
repo_type: model
status: open
target_branch: null
title: Like any other L2 model, produces illogical gibberish when asked for a story
