!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jonastemplestein
conflicting_files: null
created_at: 2023-11-11 18:01:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/87cde7fc4ab7c35338ab9be20268ab3a.svg
      fullname: Jonas Templestein
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jonastemplestein
      type: user
    createdAt: '2023-11-11T18:01:12.000Z'
    data:
      edited: false
      editors:
      - jonastemplestein
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7033896446228027
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/87cde7fc4ab7c35338ab9be20268ab3a.svg
          fullname: Jonas Templestein
          isHf: false
          isPro: false
          name: jonastemplestein
          type: user
        html: "<p>Hi there, thanks a lot for building all these GGUFs for us!</p>\n\
          <p>Not sure if this is the right place to ask this, but the GGUF version\
          \ of this model doesn't seem to tokenise and detokenise the special strings\
          \ for fill in the middle correctly. Looking at the original model's tokenizer.json,\
          \ I can see this</p>\n<pre><code>{\n      \"id\": 32015,\n      \"content\"\
          : \"&lt;\uFF5Cfim\u2581hole\uFF5C&gt;\",\n      \"single_word\": false,\n\
          \      \"lstrip\": false,\n      \"rstrip\": false,\n      \"normalized\"\
          : true,\n      \"special\": false\n    },\n    {\n      \"id\": 32016,\n\
          \      \"content\": \"&lt;\uFF5Cfim\u2581begin\uFF5C&gt;\",\n      \"single_word\"\
          : false,\n      \"lstrip\": false,\n      \"rstrip\": false,\n      \"normalized\"\
          : true,\n      \"special\": false\n    },\n    {\n      \"id\": 32017,\n\
          \      \"content\": \"&lt;\uFF5Cfim\u2581end\uFF5C&gt;\",\n      \"single_word\"\
          : false,\n      \"lstrip\": false,\n      \"rstrip\": false,\n      \"normalized\"\
          : true,\n      \"special\": false\n    }\n</code></pre>\n<p>But loading\
          \ any of the GGUFs from this repo into llama.cpp's <code>server</code> example\
          \ and hitting the <code>/detokenize</code> endpoint results in \"unordered_map::at:\
          \ key not found\" and hitting <code>/tokenize</code> gives me the wrong\
          \ tokens</p>\n<p>Is there some way to look at the tokenizer configuration\
          \ within the GGUF? Could quantisation somehow lead to this?</p>\n"
        raw: "Hi there, thanks a lot for building all these GGUFs for us!\r\n\r\n\
          Not sure if this is the right place to ask this, but the GGUF version of\
          \ this model doesn't seem to tokenise and detokenise the special strings\
          \ for fill in the middle correctly. Looking at the original model's tokenizer.json,\
          \ I can see this\r\n\r\n```\r\n{\r\n      \"id\": 32015,\r\n      \"content\"\
          : \"<\uFF5Cfim\u2581hole\uFF5C>\",\r\n      \"single_word\": false,\r\n\
          \      \"lstrip\": false,\r\n      \"rstrip\": false,\r\n      \"normalized\"\
          : true,\r\n      \"special\": false\r\n    },\r\n    {\r\n      \"id\":\
          \ 32016,\r\n      \"content\": \"<\uFF5Cfim\u2581begin\uFF5C>\",\r\n   \
          \   \"single_word\": false,\r\n      \"lstrip\": false,\r\n      \"rstrip\"\
          : false,\r\n      \"normalized\": true,\r\n      \"special\": false\r\n\
          \    },\r\n    {\r\n      \"id\": 32017,\r\n      \"content\": \"<\uFF5C\
          fim\u2581end\uFF5C>\",\r\n      \"single_word\": false,\r\n      \"lstrip\"\
          : false,\r\n      \"rstrip\": false,\r\n      \"normalized\": true,\r\n\
          \      \"special\": false\r\n    }\r\n```\r\n\r\nBut loading any of the\
          \ GGUFs from this repo into llama.cpp's `server` example and hitting the\
          \ `/detokenize` endpoint results in \"unordered_map::at: key not found\"\
          \ and hitting `/tokenize` gives me the wrong tokens\r\n\r\nIs there some\
          \ way to look at the tokenizer configuration within the GGUF? Could quantisation\
          \ somehow lead to this?"
        updatedAt: '2023-11-11T18:01:12.969Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - josharian
    id: 654fc168734a0692ec9438db
    type: comment
  author: jonastemplestein
  content: "Hi there, thanks a lot for building all these GGUFs for us!\r\n\r\nNot\
    \ sure if this is the right place to ask this, but the GGUF version of this model\
    \ doesn't seem to tokenise and detokenise the special strings for fill in the\
    \ middle correctly. Looking at the original model's tokenizer.json, I can see\
    \ this\r\n\r\n```\r\n{\r\n      \"id\": 32015,\r\n      \"content\": \"<\uFF5C\
    fim\u2581hole\uFF5C>\",\r\n      \"single_word\": false,\r\n      \"lstrip\":\
    \ false,\r\n      \"rstrip\": false,\r\n      \"normalized\": true,\r\n      \"\
    special\": false\r\n    },\r\n    {\r\n      \"id\": 32016,\r\n      \"content\"\
    : \"<\uFF5Cfim\u2581begin\uFF5C>\",\r\n      \"single_word\": false,\r\n     \
    \ \"lstrip\": false,\r\n      \"rstrip\": false,\r\n      \"normalized\": true,\r\
    \n      \"special\": false\r\n    },\r\n    {\r\n      \"id\": 32017,\r\n    \
    \  \"content\": \"<\uFF5Cfim\u2581end\uFF5C>\",\r\n      \"single_word\": false,\r\
    \n      \"lstrip\": false,\r\n      \"rstrip\": false,\r\n      \"normalized\"\
    : true,\r\n      \"special\": false\r\n    }\r\n```\r\n\r\nBut loading any of\
    \ the GGUFs from this repo into llama.cpp's `server` example and hitting the `/detokenize`\
    \ endpoint results in \"unordered_map::at: key not found\" and hitting `/tokenize`\
    \ gives me the wrong tokens\r\n\r\nIs there some way to look at the tokenizer\
    \ configuration within the GGUF? Could quantisation somehow lead to this?"
  created_at: 2023-11-11 18:01:12+00:00
  edited: false
  hidden: false
  id: 654fc168734a0692ec9438db
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/deepseek-coder-1.3b-base-GGUF
repo_type: model
status: open
target_branch: null
title: Tokenisation for fill in the middle prompts broken
