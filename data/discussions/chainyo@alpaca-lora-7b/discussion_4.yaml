!!python/object:huggingface_hub.community.DiscussionWithDetails
author: syncdoth
conflicting_files: null
created_at: 2023-04-05 17:05:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/02bf16c789d9d77eaf4da37256b0427e.svg
      fullname: Sehyun Choi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: syncdoth
      type: user
    createdAt: '2023-04-05T18:05:40.000Z'
    data:
      edited: false
      editors:
      - syncdoth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/02bf16c789d9d77eaf4da37256b0427e.svg
          fullname: Sehyun Choi
          isHf: false
          isPro: false
          name: syncdoth
          type: user
        html: "<ol>\n<li>I am not sure if the <code>model = LlamaForCausalLM.from_pretrained(\"\
          chainyo/alpaca-lora-7b\")</code> is enough to load to lora weights in <code>adapter_model.bin</code>\
          \ too. I have loaded the model using the example, and looking at the <code>state_dict:</code></li>\n\
          </ol>\n<pre><code>model = LlamaForCausalLM.from_pretrained(\"chainyo/alpaca-lora-7b\"\
          )\nstate_dict = model.state_dict()\nprint([k for k in state_dict.keys()\
          \ if 'lora' in k])\n&gt;&gt;&gt; []\n</code></pre>\n<p>While the state_dict\
          \ in <code>adapter_model.bin</code> are:</p>\n<pre><code>adapter_dict =\
          \ torch.load('adapter_model.bin')\nprint(adapter_dict.keys())\n&gt;&gt;&gt;\
          \ dict_keys(['base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight',\
          \ 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight',  ...\n\
          </code></pre>\n<p>If this is the case, then the loading script should be\
          \ sth like:</p>\n<pre><code>from transformers import LlamaTokenizer, LlamaForCausalLM\n\
          from peft import PeftModel, PeftConfig\n\npeft_model_id = \"chainyo/alpaca-lora-7b\"\
          \nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = LlamaForCausalLM.from_pretrained(config.base_model_name_or_path,\n\
          \                                         load_in_8bit=True,\n         \
          \                                torch_dtype=torch.float16,\n          \
          \                               device_map=\"auto\")\nmodel = PeftModel.from_pretrained(model,\
          \ peft_model_id)\ntokenizer = LlamaTokenizer.from_pretrained(peft_model_id)\n\
          </code></pre>\n<p>Also, the <code>base_model_name_or_path</code> in <code>adapter_config.json</code>\
          \ might need to be corrected so it doesn't download original llama weights\
          \ again.</p>\n<ol start=\"2\">\n<li>On second thought, are lora weights\
          \ required to perform as expected? Maybe the LLaMa weights themselves are\
          \ tuned... (then why the name Lora?)</li>\n</ol>\n<p>Thanks!</p>\n"
        raw: "1. I am not sure if the `model = LlamaForCausalLM.from_pretrained(\"\
          chainyo/alpaca-lora-7b\")` is enough to load to lora weights in `adapter_model.bin`\
          \ too. I have loaded the model using the example, and looking at the `state_dict:`\r\
          \n\r\n```\r\nmodel = LlamaForCausalLM.from_pretrained(\"chainyo/alpaca-lora-7b\"\
          )\r\nstate_dict = model.state_dict()\r\nprint([k for k in state_dict.keys()\
          \ if 'lora' in k])\r\n>>> []\r\n```\r\n\r\nWhile the state_dict in `adapter_model.bin`\
          \ are:\r\n\r\n```\r\nadapter_dict = torch.load('adapter_model.bin')\r\n\
          print(adapter_dict.keys())\r\n>>> dict_keys(['base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight',\
          \ 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight',  ...\r\
          \n```\r\n\r\nIf this is the case, then the loading script should be sth\
          \ like:\r\n\r\n```\r\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\r\
          \nfrom peft import PeftModel, PeftConfig\r\n\r\npeft_model_id = \"chainyo/alpaca-lora-7b\"\
          \r\nconfig = PeftConfig.from_pretrained(peft_model_id)\r\nmodel = LlamaForCausalLM.from_pretrained(config.base_model_name_or_path,\r\
          \n                                         load_in_8bit=True,\r\n      \
          \                                   torch_dtype=torch.float16,\r\n     \
          \                                    device_map=\"auto\")\r\nmodel = PeftModel.from_pretrained(model,\
          \ peft_model_id)\r\ntokenizer = LlamaTokenizer.from_pretrained(peft_model_id)\r\
          \n```\r\n\r\nAlso, the `base_model_name_or_path` in `adapter_config.json`\
          \ might need to be corrected so it doesn't download original llama weights\
          \ again.\r\n\r\n2. On second thought, are lora weights required to perform\
          \ as expected? Maybe the LLaMa weights themselves are tuned... (then why\
          \ the name Lora?)\r\n\r\nThanks!"
        updatedAt: '2023-04-05T18:05:40.199Z'
      numEdits: 0
      reactions: []
    id: 642db8742a10ffb687cb2329
    type: comment
  author: syncdoth
  content: "1. I am not sure if the `model = LlamaForCausalLM.from_pretrained(\"chainyo/alpaca-lora-7b\"\
    )` is enough to load to lora weights in `adapter_model.bin` too. I have loaded\
    \ the model using the example, and looking at the `state_dict:`\r\n\r\n```\r\n\
    model = LlamaForCausalLM.from_pretrained(\"chainyo/alpaca-lora-7b\")\r\nstate_dict\
    \ = model.state_dict()\r\nprint([k for k in state_dict.keys() if 'lora' in k])\r\
    \n>>> []\r\n```\r\n\r\nWhile the state_dict in `adapter_model.bin` are:\r\n\r\n\
    ```\r\nadapter_dict = torch.load('adapter_model.bin')\r\nprint(adapter_dict.keys())\r\
    \n>>> dict_keys(['base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight',\
    \ 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight',  ...\r\n```\r\
    \n\r\nIf this is the case, then the loading script should be sth like:\r\n\r\n\
    ```\r\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\r\nfrom peft\
    \ import PeftModel, PeftConfig\r\n\r\npeft_model_id = \"chainyo/alpaca-lora-7b\"\
    \r\nconfig = PeftConfig.from_pretrained(peft_model_id)\r\nmodel = LlamaForCausalLM.from_pretrained(config.base_model_name_or_path,\r\
    \n                                         load_in_8bit=True,\r\n            \
    \                             torch_dtype=torch.float16,\r\n                 \
    \                        device_map=\"auto\")\r\nmodel = PeftModel.from_pretrained(model,\
    \ peft_model_id)\r\ntokenizer = LlamaTokenizer.from_pretrained(peft_model_id)\r\
    \n```\r\n\r\nAlso, the `base_model_name_or_path` in `adapter_config.json` might\
    \ need to be corrected so it doesn't download original llama weights again.\r\n\
    \r\n2. On second thought, are lora weights required to perform as expected? Maybe\
    \ the LLaMa weights themselves are tuned... (then why the name Lora?)\r\n\r\n\
    Thanks!"
  created_at: 2023-04-05 17:05:40+00:00
  edited: false
  hidden: false
  id: 642db8742a10ffb687cb2329
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/02bf16c789d9d77eaf4da37256b0427e.svg
      fullname: Sehyun Choi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: syncdoth
      type: user
    createdAt: '2023-04-05T18:39:42.000Z'
    data:
      edited: false
      editors:
      - syncdoth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/02bf16c789d9d77eaf4da37256b0427e.svg
          fullname: Sehyun Choi
          isHf: false
          isPro: false
          name: syncdoth
          type: user
        html: '<p>On a deeper investigation, I found that only <code>''self_attn.q_proj.weight'',
          ''self_attn.rotary_emb.inv_freq'', ''self_attn.v_proj.weight''</code> diverge
          from the original llama weight. Since lora is applied to <code>q_proj</code>
          and <code>v_proj</code>, I suppose these weights are re-constructed from
          lora A and B matrices (found PEFT''s <code>merge_and_unload</code> function).</p>

          '
        raw: On a deeper investigation, I found that only `'self_attn.q_proj.weight',
          'self_attn.rotary_emb.inv_freq', 'self_attn.v_proj.weight'` diverge from
          the original llama weight. Since lora is applied to `q_proj` and `v_proj`,
          I suppose these weights are re-constructed from lora A and B matrices (found
          PEFT's `merge_and_unload` function).
        updatedAt: '2023-04-05T18:39:42.017Z'
      numEdits: 0
      reactions: []
      relatedEventId: 642dc06e42b094c9138b6664
    id: 642dc06e42b094c9138b6663
    type: comment
  author: syncdoth
  content: On a deeper investigation, I found that only `'self_attn.q_proj.weight',
    'self_attn.rotary_emb.inv_freq', 'self_attn.v_proj.weight'` diverge from the original
    llama weight. Since lora is applied to `q_proj` and `v_proj`, I suppose these
    weights are re-constructed from lora A and B matrices (found PEFT's `merge_and_unload`
    function).
  created_at: 2023-04-05 17:39:42+00:00
  edited: false
  hidden: false
  id: 642dc06e42b094c9138b6663
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/02bf16c789d9d77eaf4da37256b0427e.svg
      fullname: Sehyun Choi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: syncdoth
      type: user
    createdAt: '2023-04-05T18:39:42.000Z'
    data:
      status: closed
    id: 642dc06e42b094c9138b6664
    type: status-change
  author: syncdoth
  created_at: 2023-04-05 17:39:42+00:00
  id: 642dc06e42b094c9138b6664
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
      fullname: Thomas Chaigneau
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: chainyo
      type: user
    createdAt: '2023-04-05T19:13:15.000Z'
    data:
      edited: true
      editors:
      - chainyo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675273618630-6162dbe0d928851b47350ae2.jpeg?w=200&h=200&f=face
          fullname: Thomas Chaigneau
          isHf: false
          isPro: false
          name: chainyo
          type: user
        html: "<p>I chose to share the adapters and the fully merged Lora weights\
          \ with the base model.</p>\n<p>You can choose to load only the adapters\
          \ or the full model.</p>\n<ul>\n<li>Full model:</li>\n</ul>\n<pre><code\
          \ class=\"language-python\">tokenizer = LlamaTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"chainyo/alpaca-lora-7b\"</span>)\nmodel = LlamaForCausalLM.from_pretrained(\n\
          \    <span class=\"hljs-string\">\"chainyo/alpaca-lora-7b\"</span>,\n  \
          \  load_in_8bit=<span class=\"hljs-literal\">True</span>,\n    torch_dtype=torch.float16,\n\
          \    device_map=<span class=\"hljs-string\">\"auto\"</span>,\n)\n</code></pre>\n\
          <ul>\n<li>Using the adapters:</li>\n</ul>\n<pre><code class=\"language-python\"\
          >model = LlamaForCausalLM.from_pretrained(\n    <span class=\"hljs-string\"\
          >\"decapoda-research/llama-7b-hf\"</span>,\n    load_in_8bit=load_8bit,\n\
          \    torch_dtype=torch.float16,\n    device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>,\n)\nmodel = PeftModel.from_pretrained(\n    model,\n \
          \   <span class=\"hljs-string\">\"chainyo/alpaca-lora-7b\"</span>,\n   \
          \ torch_dtype=torch.float16,\n)\n</code></pre>\n"
        raw: "I chose to share the adapters and the fully merged Lora weights with\
          \ the base model.\n\nYou can choose to load only the adapters or the full\
          \ model.\n\n* Full model:\n\n```python\ntokenizer = LlamaTokenizer.from_pretrained(\"\
          chainyo/alpaca-lora-7b\")\nmodel = LlamaForCausalLM.from_pretrained(\n \
          \   \"chainyo/alpaca-lora-7b\",\n    load_in_8bit=True,\n    torch_dtype=torch.float16,\n\
          \    device_map=\"auto\",\n)\n````\n\n* Using the adapters:\n\n```python\n\
          model = LlamaForCausalLM.from_pretrained(\n    \"decapoda-research/llama-7b-hf\"\
          ,\n    load_in_8bit=load_8bit,\n    torch_dtype=torch.float16,\n    device_map=\"\
          auto\",\n)\nmodel = PeftModel.from_pretrained(\n    model,\n    \"chainyo/alpaca-lora-7b\"\
          ,\n    torch_dtype=torch.float16,\n)\n```"
        updatedAt: '2023-04-05T19:15:04.399Z'
      numEdits: 2
      reactions: []
    id: 642dc84bbaf943d5db45a4cb
    type: comment
  author: chainyo
  content: "I chose to share the adapters and the fully merged Lora weights with the\
    \ base model.\n\nYou can choose to load only the adapters or the full model.\n\
    \n* Full model:\n\n```python\ntokenizer = LlamaTokenizer.from_pretrained(\"chainyo/alpaca-lora-7b\"\
    )\nmodel = LlamaForCausalLM.from_pretrained(\n    \"chainyo/alpaca-lora-7b\",\n\
    \    load_in_8bit=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\
    ,\n)\n````\n\n* Using the adapters:\n\n```python\nmodel = LlamaForCausalLM.from_pretrained(\n\
    \    \"decapoda-research/llama-7b-hf\",\n    load_in_8bit=load_8bit,\n    torch_dtype=torch.float16,\n\
    \    device_map=\"auto\",\n)\nmodel = PeftModel.from_pretrained(\n    model,\n\
    \    \"chainyo/alpaca-lora-7b\",\n    torch_dtype=torch.float16,\n)\n```"
  created_at: 2023-04-05 18:13:15+00:00
  edited: true
  hidden: false
  id: 642dc84bbaf943d5db45a4cb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: chainyo/alpaca-lora-7b
repo_type: model
status: closed
target_branch: null
title: Are Lora weights loaded?
