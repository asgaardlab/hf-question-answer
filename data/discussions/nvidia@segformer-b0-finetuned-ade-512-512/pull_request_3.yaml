!!python/object:huggingface_hub.community.DiscussionWithDetails
author: visheratin
conflicting_files: []
created_at: 2022-10-31 02:27:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b892a3d50b2f8ead0a5f7108564e45d0.svg
      fullname: Alexander Visheratin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: visheratin
      type: user
    createdAt: '2022-10-31T03:27:09.000Z'
    data:
      edited: false
      editors:
      - visheratin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b892a3d50b2f8ead0a5f7108564e45d0.svg
          fullname: Alexander Visheratin
          isHf: false
          isPro: false
          name: visheratin
          type: user
        html: '<p>The model was converted from PyTorch weights using standard ONNX
          methods. The pipeline for conversion and checking can be found <a rel="nofollow"
          href="https://colab.research.google.com/drive/1kvf72PsC4LxSrAqnGvmW_ogFxc3qp_Pg?usp=sharing">here</a>.</p>

          <p><code>model.onnx</code> is an optimized version of the exported model.
          <code>model_quant.onnx</code> is a quantized optimized model.</p>

          <p>I checked the difference between the original predictions and the ones
          from the exported models on a portion of the <code>scene_parse_150</code>
          dataset:</p>

          <ol>

          <li><code>model.onnx</code> - the absolute difference is 127.364 on a 10x150x128x128
          matrix, which looks reasonable. The effective difference (difference in
          per-pixel class predictions) is 0, which is good.</li>

          <li><code>model_quant.onnx</code> - the absolute difference is 7876547,
          which is huge. But the effective difference is only 182, which means that
          the classes are different for only 182 of 10x128x128 pixels (0.1%).</li>

          </ol>

          '
        raw: 'The model was converted from PyTorch weights using standard ONNX methods.
          The pipeline for conversion and checking can be found [here](https://colab.research.google.com/drive/1kvf72PsC4LxSrAqnGvmW_ogFxc3qp_Pg?usp=sharing).


          `model.onnx` is an optimized version of the exported model. `model_quant.onnx`
          is a quantized optimized model.


          I checked the difference between the original predictions and the ones from
          the exported models on a portion of the `scene_parse_150` dataset:


          1. `model.onnx` - the absolute difference is 127.364 on a 10x150x128x128
          matrix, which looks reasonable. The effective difference (difference in
          per-pixel class predictions) is 0, which is good.

          2. `model_quant.onnx` - the absolute difference is 7876547, which is huge.
          But the effective difference is only 182, which means that the classes are
          different for only 182 of 10x128x128 pixels (0.1%).'
        updatedAt: '2022-10-31T03:27:09.077Z'
      numEdits: 0
      reactions: []
    id: 635f408da81c7f7424a6c30e
    type: comment
  author: visheratin
  content: 'The model was converted from PyTorch weights using standard ONNX methods.
    The pipeline for conversion and checking can be found [here](https://colab.research.google.com/drive/1kvf72PsC4LxSrAqnGvmW_ogFxc3qp_Pg?usp=sharing).


    `model.onnx` is an optimized version of the exported model. `model_quant.onnx`
    is a quantized optimized model.


    I checked the difference between the original predictions and the ones from the
    exported models on a portion of the `scene_parse_150` dataset:


    1. `model.onnx` - the absolute difference is 127.364 on a 10x150x128x128 matrix,
    which looks reasonable. The effective difference (difference in per-pixel class
    predictions) is 0, which is good.

    2. `model_quant.onnx` - the absolute difference is 7876547, which is huge. But
    the effective difference is only 182, which means that the classes are different
    for only 182 of 10x128x128 pixels (0.1%).'
  created_at: 2022-10-31 02:27:09+00:00
  edited: false
  hidden: false
  id: 635f408da81c7f7424a6c30e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/b892a3d50b2f8ead0a5f7108564e45d0.svg
      fullname: Alexander Visheratin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: visheratin
      type: user
    createdAt: '2022-10-31T03:27:10.000Z'
    data:
      oid: 6700676d80f989a1e7096fd6d89e88e9b9ca61ce
      parents:
      - 6010ab27127f28679da94c183f62db4322210737
      subject: Add ONNX models
    id: 635f408e0000000000000000
    type: commit
  author: visheratin
  created_at: 2022-10-31 02:27:10+00:00
  id: 635f408e0000000000000000
  oid: 6700676d80f989a1e7096fd6d89e88e9b9ca61ce
  summary: Add ONNX models
  type: commit
is_pull_request: true
merge_commit_oid: null
num: 3
repo_id: nvidia/segformer-b0-finetuned-ade-512-512
repo_type: model
status: open
target_branch: refs/heads/main
title: Add ONNX models
