!!python/object:huggingface_hub.community.DiscussionWithDetails
author: luuksuurmeijer
conflicting_files: null
created_at: 2024-01-24 14:30:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9dc68f868b298b93eb1fb1a36da4a889.svg
      fullname: Luuk Suurmeijer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luuksuurmeijer
      type: user
    createdAt: '2024-01-24T14:30:10.000Z'
    data:
      edited: true
      editors:
      - luuksuurmeijer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6913852095603943
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9dc68f868b298b93eb1fb1a36da4a889.svg
          fullname: Luuk Suurmeijer
          isHf: false
          isPro: false
          name: luuksuurmeijer
          type: user
        html: "<p>I want to run Llama2 for inference on a <code>inf2.xlarge</code>\
          \ instance (I do not have access to larger instances). Since my instance\
          \ runs out of RAM when compiling the model, I want to use the precompiled\
          \ artifacts that come with this repo. My Neuron version is <code>2.16.1</code>,\
          \ I have downloaded the models to the instance and I am running the following\
          \ code:</p>\n<pre><code>import torch\nfrom transformers import AutoTokenizer\n\
          from transformers_neuronx.llama.model import LlamaForCausalLM\n\n\nprint(\"\
          Loading model...\")\nNEURON_MODEL = LlamaForCausalLM.from_pretrained(\n\
          \    \"Llama-2-7b-chat-hf-seqlen-2048-bs-1/checkpoint\", batch_size=1, tp_degree=2,\
          \ amp=\"f16\"\n)\nprint(\"Loading Neuron Artifacts\")\nNEURON_MODEL.load(\n\
          \    \"Llama-2-7b-chat-hf-seqlen-2048-bs-1/compiled\"\n)  # Load the compiled\
          \ Neuron artifacts\nNEURON_MODEL.to_neuron()\n\n# construct a tokenizer\
          \ and encode prompt text\nprint(\"Constructing tokenizer...\")\nTOKENIZER\
          \ = AutoTokenizer.from_pretrained(\"Llama-2-7b-chat-hf-seqlen-2048-bs-1\"\
          )\n</code></pre>\n<p>I can see that the model is being loaded on the device\
          \ with <code>neuron-top</code>, but after a while it crashes with the following\
          \ error:</p>\n<pre><code>    NEURON_MODEL.to_neuron()\n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/base.py\"\
          , line 62,                                                             \
          \                            \nin to_neuron                            \
          \                              \n    self._load_compiled_artifacts(self._compiled_artifacts_directory)\
          \                                                                      \
          \ \n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/base.py\"\
          , line 108,                                                            \
          \                \n in _load_compiled_artifacts               \n    nbs_obj.set_neff_bytes(directory)\
          \                                                           \n  File \"\
          /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/base.py\"\
          , line 387,\n in set_neff_bytes       \n    raise FileNotFoundError(('Could\
          \ not find a matching NEFF for your HLO in this directory. '           \
          \                                                                      \
          \       \nFileNotFoundError: Could not find a matching NEFF for your HLO\
          \ in this directory. Ensure that the model y                           \
          \                                                                     \n\
          ou are trying to load is the same type and has the same parameters as the\
          \ one you saved or call \"save\" on                                    \
          \                                                         \n this model\
          \ to reserialize it.     \n</code></pre>\n<p>I am not sure what is going\
          \ on here, what am I doing wrong?</p>\n"
        raw: "I want to run Llama2 for inference on a `inf2.xlarge` instance (I do\
          \ not have access to larger instances). Since my instance runs out of RAM\
          \ when compiling the model, I want to use the precompiled artifacts that\
          \ come with this repo. My Neuron version is `2.16.1`, I have downloaded\
          \ the models to the instance and I am running the following code:\n\n```\n\
          import torch\nfrom transformers import AutoTokenizer\nfrom transformers_neuronx.llama.model\
          \ import LlamaForCausalLM\n\n\nprint(\"Loading model...\")\nNEURON_MODEL\
          \ = LlamaForCausalLM.from_pretrained(\n    \"Llama-2-7b-chat-hf-seqlen-2048-bs-1/checkpoint\"\
          , batch_size=1, tp_degree=2, amp=\"f16\"\n)\nprint(\"Loading Neuron Artifacts\"\
          )\nNEURON_MODEL.load(\n    \"Llama-2-7b-chat-hf-seqlen-2048-bs-1/compiled\"\
          \n)  # Load the compiled Neuron artifacts\nNEURON_MODEL.to_neuron()\n\n\
          # construct a tokenizer and encode prompt text\nprint(\"Constructing tokenizer...\"\
          )\nTOKENIZER = AutoTokenizer.from_pretrained(\"Llama-2-7b-chat-hf-seqlen-2048-bs-1\"\
          )\n\n```\n\nI can see that the model is being loaded on the device with\
          \ `neuron-top`, but after a while it crashes with the following error:\n\
          \n```\n    NEURON_MODEL.to_neuron()\n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/base.py\"\
          , line 62,                                                             \
          \                            \nin to_neuron                            \
          \                              \n    self._load_compiled_artifacts(self._compiled_artifacts_directory)\
          \                                                                      \
          \ \n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/base.py\"\
          , line 108,                                                            \
          \                \n in _load_compiled_artifacts               \n    nbs_obj.set_neff_bytes(directory)\
          \                                                           \n  File \"\
          /opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/base.py\"\
          , line 387,\n in set_neff_bytes       \n    raise FileNotFoundError(('Could\
          \ not find a matching NEFF for your HLO in this directory. '           \
          \                                                                      \
          \       \nFileNotFoundError: Could not find a matching NEFF for your HLO\
          \ in this directory. Ensure that the model y                           \
          \                                                                     \n\
          ou are trying to load is the same type and has the same parameters as the\
          \ one you saved or call \"save\" on                                    \
          \                                                         \n this model\
          \ to reserialize it.     \n```\n\nI am not sure what is going on here, what\
          \ am I doing wrong?"
        updatedAt: '2024-01-24T14:34:53.094Z'
      numEdits: 1
      reactions: []
    id: 65b11ef2d84a1f119fb159da
    type: comment
  author: luuksuurmeijer
  content: "I want to run Llama2 for inference on a `inf2.xlarge` instance (I do not\
    \ have access to larger instances). Since my instance runs out of RAM when compiling\
    \ the model, I want to use the precompiled artifacts that come with this repo.\
    \ My Neuron version is `2.16.1`, I have downloaded the models to the instance\
    \ and I am running the following code:\n\n```\nimport torch\nfrom transformers\
    \ import AutoTokenizer\nfrom transformers_neuronx.llama.model import LlamaForCausalLM\n\
    \n\nprint(\"Loading model...\")\nNEURON_MODEL = LlamaForCausalLM.from_pretrained(\n\
    \    \"Llama-2-7b-chat-hf-seqlen-2048-bs-1/checkpoint\", batch_size=1, tp_degree=2,\
    \ amp=\"f16\"\n)\nprint(\"Loading Neuron Artifacts\")\nNEURON_MODEL.load(\n  \
    \  \"Llama-2-7b-chat-hf-seqlen-2048-bs-1/compiled\"\n)  # Load the compiled Neuron\
    \ artifacts\nNEURON_MODEL.to_neuron()\n\n# construct a tokenizer and encode prompt\
    \ text\nprint(\"Constructing tokenizer...\")\nTOKENIZER = AutoTokenizer.from_pretrained(\"\
    Llama-2-7b-chat-hf-seqlen-2048-bs-1\")\n\n```\n\nI can see that the model is being\
    \ loaded on the device with `neuron-top`, but after a while it crashes with the\
    \ following error:\n\n```\n    NEURON_MODEL.to_neuron()\n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/base.py\"\
    , line 62,                                                                   \
    \                      \nin to_neuron                                        \
    \                  \n    self._load_compiled_artifacts(self._compiled_artifacts_directory)\
    \                                                                       \n  File\
    \ \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/base.py\"\
    , line 108,                                                                  \
    \          \n in _load_compiled_artifacts               \n    nbs_obj.set_neff_bytes(directory)\
    \                                                           \n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/base.py\"\
    , line 387,\n in set_neff_bytes       \n    raise FileNotFoundError(('Could not\
    \ find a matching NEFF for your HLO in this directory. '                     \
    \                                                                   \nFileNotFoundError:\
    \ Could not find a matching NEFF for your HLO in this directory. Ensure that the\
    \ model y                                                                    \
    \                            \nou are trying to load is the same type and has\
    \ the same parameters as the one you saved or call \"save\" on               \
    \                                                                            \
    \  \n this model to reserialize it.     \n```\n\nI am not sure what is going on\
    \ here, what am I doing wrong?"
  created_at: 2024-01-24 14:30:10+00:00
  edited: true
  hidden: false
  id: 65b11ef2d84a1f119fb159da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
      fullname: David Corvoysier
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dacorvo
      type: user
    createdAt: '2024-01-24T14:34:03.000Z'
    data:
      edited: true
      editors:
      - dacorvo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8369659185409546
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
          fullname: David Corvoysier
          isHf: true
          isPro: false
          name: dacorvo
          type: user
        html: '<p>This is a model serialized for <code>optimum-neuron</code>. It is
          not intended to be used with <code>transformers_neuronx</code> directly,
          although the content of the <code>checkpoint</code> and <code>compiled</code>
          directory are indeed compatible with that package.</p>

          '
        raw: This is a model serialized for `optimum-neuron`. It is not intended to
          be used with `transformers_neuronx` directly, although the content of the
          `checkpoint` and `compiled` directory are indeed compatible with that package.
        updatedAt: '2024-01-24T14:37:53.224Z'
      numEdits: 2
      reactions: []
    id: 65b11fdbd4998725692d8caa
    type: comment
  author: dacorvo
  content: This is a model serialized for `optimum-neuron`. It is not intended to
    be used with `transformers_neuronx` directly, although the content of the `checkpoint`
    and `compiled` directory are indeed compatible with that package.
  created_at: 2024-01-24 14:34:03+00:00
  edited: true
  hidden: false
  id: 65b11fdbd4998725692d8caa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
      fullname: David Corvoysier
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dacorvo
      type: user
    createdAt: '2024-01-24T14:36:08.000Z'
    data:
      edited: false
      editors:
      - dacorvo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9031173586845398
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
          fullname: David Corvoysier
          isHf: true
          isPro: false
          name: dacorvo
          type: user
        html: '<p>Your sequence of calls seem correct though, but unfortunately the
          compiled artifacts are for AWS Neuron SDK 2.15 only.<br><a rel="nofollow"
          href="https://github.com/aws-neuron/transformers-neuronx/issues/78">https://github.com/aws-neuron/transformers-neuronx/issues/78</a></p>

          '
        raw: 'Your sequence of calls seem correct though, but unfortunately the compiled
          artifacts are for AWS Neuron SDK 2.15 only.

          https://github.com/aws-neuron/transformers-neuronx/issues/78'
        updatedAt: '2024-01-24T14:36:08.261Z'
      numEdits: 0
      reactions: []
    id: 65b1205885b6c214483291d2
    type: comment
  author: dacorvo
  content: 'Your sequence of calls seem correct though, but unfortunately the compiled
    artifacts are for AWS Neuron SDK 2.15 only.

    https://github.com/aws-neuron/transformers-neuronx/issues/78'
  created_at: 2024-01-24 14:36:08+00:00
  edited: false
  hidden: false
  id: 65b1205885b6c214483291d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
      fullname: David Corvoysier
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dacorvo
      type: user
    createdAt: '2024-01-24T14:40:27.000Z'
    data:
      edited: false
      editors:
      - dacorvo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7481659054756165
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
          fullname: David Corvoysier
          isHf: true
          isPro: false
          name: dacorvo
          type: user
        html: '<p>You can use <a href="https://huggingface.co/aws-neuron/Llama-2-7b-hf-neuron-latency">https://huggingface.co/aws-neuron/Llama-2-7b-hf-neuron-latency</a>
          instead, which has compiled artifacts compatible with <code>2.16.0</code>.<br>Alternatively,
          using <code>optimum-neuron</code>, you can still export the model without
          needing to recompile it thanks to <a href="https://huggingface.co/aws-neuron/optimum-neuron-cache">https://huggingface.co/aws-neuron/optimum-neuron-cache</a>.</p>

          '
        raw: 'You can use https://huggingface.co/aws-neuron/Llama-2-7b-hf-neuron-latency
          instead, which has compiled artifacts compatible with `2.16.0`.

          Alternatively, using `optimum-neuron`, you can still export the model without
          needing to recompile it thanks to https://huggingface.co/aws-neuron/optimum-neuron-cache.'
        updatedAt: '2024-01-24T14:40:27.132Z'
      numEdits: 0
      reactions: []
    id: 65b1215be22e106747e63712
    type: comment
  author: dacorvo
  content: 'You can use https://huggingface.co/aws-neuron/Llama-2-7b-hf-neuron-latency
    instead, which has compiled artifacts compatible with `2.16.0`.

    Alternatively, using `optimum-neuron`, you can still export the model without
    needing to recompile it thanks to https://huggingface.co/aws-neuron/optimum-neuron-cache.'
  created_at: 2024-01-24 14:40:27+00:00
  edited: false
  hidden: false
  id: 65b1215be22e106747e63712
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9dc68f868b298b93eb1fb1a36da4a889.svg
      fullname: Luuk Suurmeijer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luuksuurmeijer
      type: user
    createdAt: '2024-01-24T15:18:17.000Z'
    data:
      edited: false
      editors:
      - luuksuurmeijer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6906670928001404
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9dc68f868b298b93eb1fb1a36da4a889.svg
          fullname: Luuk Suurmeijer
          isHf: false
          isPro: false
          name: luuksuurmeijer
          type: user
        html: "<p>Thanks for your quick answer! I have tried running the same code\
          \ with the model you mention (<a href=\"https://huggingface.co/aws-neuron/Llama-2-7b-hf-neuron-latency\"\
          >https://huggingface.co/aws-neuron/Llama-2-7b-hf-neuron-latency</a>) and\
          \ a checkpoint from meta-llama/Llama-2-7b-chat-hf (since this model does\
          \ not seem to include checkpoints). But I am running into issues still.</p>\n\
          <pre><code>   NEURON_MODEL.to_neuron()\n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/base.py\"\
          , line 60,                                               \nin to_neuron\n\
          \    self.load_weights()                      \n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/llama/model.py\"\
          , li                                         \nne 84, in load_weights  \
          \                                       \n    layer.materialize()     \n\
          \  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/module.py\"\
          , line 71                                           \n, in materialize \
          \                                           \n    param.copy_(input_param)\
          \                                  \nNotImplementedError: Cannot copy out\
          \ of meta tensor; no data!  \n</code></pre>\n<p>Is the model you mention\
          \ compiled for meta-llama/Llama-2-7b-chat-hf or meta-llama/Llama-2-7b-hf?\
          \ The model card mentions both and I suspect it might be related to that.</p>\n"
        raw: "Thanks for your quick answer! I have tried running the same code with\
          \ the model you mention (https://huggingface.co/aws-neuron/Llama-2-7b-hf-neuron-latency)\
          \ and a checkpoint from meta-llama/Llama-2-7b-chat-hf (since this model\
          \ does not seem to include checkpoints). But I am running into issues still.\n\
          \n```\n   NEURON_MODEL.to_neuron()\n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/base.py\"\
          , line 60,                                               \nin to_neuron\n\
          \    self.load_weights()                      \n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/llama/model.py\"\
          , li                                         \nne 84, in load_weights  \
          \                                       \n    layer.materialize()     \n\
          \  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/module.py\"\
          , line 71                                           \n, in materialize \
          \                                           \n    param.copy_(input_param)\
          \                                  \nNotImplementedError: Cannot copy out\
          \ of meta tensor; no data!  \n```\n\nIs the model you mention compiled for\
          \ meta-llama/Llama-2-7b-chat-hf or meta-llama/Llama-2-7b-hf? The model card\
          \ mentions both and I suspect it might be related to that."
        updatedAt: '2024-01-24T15:18:17.273Z'
      numEdits: 0
      reactions: []
    id: 65b12a391dcf354c00b1da65
    type: comment
  author: luuksuurmeijer
  content: "Thanks for your quick answer! I have tried running the same code with\
    \ the model you mention (https://huggingface.co/aws-neuron/Llama-2-7b-hf-neuron-latency)\
    \ and a checkpoint from meta-llama/Llama-2-7b-chat-hf (since this model does not\
    \ seem to include checkpoints). But I am running into issues still.\n\n```\n \
    \  NEURON_MODEL.to_neuron()\n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/base.py\"\
    , line 60,                                               \nin to_neuron\n    self.load_weights()\
    \                      \n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/llama/model.py\"\
    , li                                         \nne 84, in load_weights        \
    \                                 \n    layer.materialize()     \n  File \"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/module.py\"\
    , line 71                                           \n, in materialize       \
    \                                     \n    param.copy_(input_param)         \
    \                         \nNotImplementedError: Cannot copy out of meta tensor;\
    \ no data!  \n```\n\nIs the model you mention compiled for meta-llama/Llama-2-7b-chat-hf\
    \ or meta-llama/Llama-2-7b-hf? The model card mentions both and I suspect it might\
    \ be related to that."
  created_at: 2024-01-24 15:18:17+00:00
  edited: false
  hidden: false
  id: 65b12a391dcf354c00b1da65
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: aws-neuron/Llama-2-7b-chat-hf-seqlen-2048-bs-1
repo_type: model
status: open
target_branch: null
title: Could not find a matching NEFF for your HLO in this directory.  When trying
  to load precompiled neuron artifacts
