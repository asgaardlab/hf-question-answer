!!python/object:huggingface_hub.community.DiscussionWithDetails
author: WaelDataReply
conflicting_files: null
created_at: 2024-01-05 09:13:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e2b98b87085ec4613c50c47953a701a.svg
      fullname: Saideni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WaelDataReply
      type: user
    createdAt: '2024-01-05T09:13:15.000Z'
    data:
      edited: false
      editors:
      - WaelDataReply
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7557592988014221
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e2b98b87085ec4613c50c47953a701a.svg
          fullname: Saideni
          isHf: false
          isPro: false
          name: WaelDataReply
          type: user
        html: '<p>I tried to compile the same model ( meta-llama/Llama-2-7b-chat-hf)
          using the same configuration proposed here on an EC2 instance of type Inf2
          with the Hugging Face Neuron Deep Learning AMI: sequence_length: 2048; batch_size:
          2; neuron: 2.15.0.<br>Unfortunately, I consistently encountered the error:
          "AttributeError: type object ''LLamaNeuronConfig'' does not have the attribute
          ''get_mandatory_axes_for_task''." Despite specifying the task as text-generation,
          the issue persisted without any improvement.<br>Is there an additional configuration
          required for compiling the model?</p>

          '
        raw: "I tried to compile the same model ( meta-llama/Llama-2-7b-chat-hf) using\
          \ the same configuration proposed here on an EC2 instance of type Inf2 with\
          \ the Hugging Face Neuron Deep Learning AMI: sequence_length: 2048; batch_size:\
          \ 2; neuron: 2.15.0. \r\nUnfortunately, I consistently encountered the error:\
          \ \"AttributeError: type object 'LLamaNeuronConfig' does not have the attribute\
          \ 'get_mandatory_axes_for_task'.\" Despite specifying the task as text-generation,\
          \ the issue persisted without any improvement.\r\nIs there an additional\
          \ configuration required for compiling the model?"
        updatedAt: '2024-01-05T09:13:15.367Z'
      numEdits: 0
      reactions: []
    id: 6597c82b5ddc4d0123c088d4
    type: comment
  author: WaelDataReply
  content: "I tried to compile the same model ( meta-llama/Llama-2-7b-chat-hf) using\
    \ the same configuration proposed here on an EC2 instance of type Inf2 with the\
    \ Hugging Face Neuron Deep Learning AMI: sequence_length: 2048; batch_size: 2;\
    \ neuron: 2.15.0. \r\nUnfortunately, I consistently encountered the error: \"\
    AttributeError: type object 'LLamaNeuronConfig' does not have the attribute 'get_mandatory_axes_for_task'.\"\
    \ Despite specifying the task as text-generation, the issue persisted without\
    \ any improvement.\r\nIs there an additional configuration required for compiling\
    \ the model?"
  created_at: 2024-01-05 09:13:15+00:00
  edited: false
  hidden: false
  id: 6597c82b5ddc4d0123c088d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
      fullname: David Corvoysier
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dacorvo
      type: user
    createdAt: '2024-01-08T07:44:10.000Z'
    data:
      edited: false
      editors:
      - dacorvo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7400870323181152
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
          fullname: David Corvoysier
          isHf: true
          isPro: false
          name: dacorvo
          type: user
        html: '<p>Can you post the exact command you used ? There is a specific procedure
          for text-generation: <a href="https://huggingface.co/docs/optimum-neuron/guides/models#generative-nlp-models">https://huggingface.co/docs/optimum-neuron/guides/models#generative-nlp-models</a></p>

          '
        raw: 'Can you post the exact command you used ? There is a specific procedure
          for text-generation: https://huggingface.co/docs/optimum-neuron/guides/models#generative-nlp-models'
        updatedAt: '2024-01-08T07:44:10.757Z'
      numEdits: 0
      reactions: []
    id: 659ba7ca4d58d86c3ebbac4d
    type: comment
  author: dacorvo
  content: 'Can you post the exact command you used ? There is a specific procedure
    for text-generation: https://huggingface.co/docs/optimum-neuron/guides/models#generative-nlp-models'
  created_at: 2024-01-08 07:44:10+00:00
  edited: false
  hidden: false
  id: 659ba7ca4d58d86c3ebbac4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e2b98b87085ec4613c50c47953a701a.svg
      fullname: Saideni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WaelDataReply
      type: user
    createdAt: '2024-01-08T09:03:59.000Z'
    data:
      edited: false
      editors:
      - WaelDataReply
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48208969831466675
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e2b98b87085ec4613c50c47953a701a.svg
          fullname: Saideni
          isHf: false
          isPro: false
          name: WaelDataReply
          type: user
        html: '<p>I attempted to compile the model using two methods as outlined in
          the text-generation procedure provided here (<a href="https://huggingface.co/docs/optimum-neuron/guides/models#generative-nlp-models">https://huggingface.co/docs/optimum-neuron/guides/models#generative-nlp-models</a>):</p>

          <h2 id="1--executing-the-following-code">1- Executing the following code:
          </h2>

          <p>from optimum.neuron import NeuronModelForCausalLM<br>from transformers
          import AutoTokenizer</p>

          <h1 id="model-id-you-want-to-compile">model id you want to compile</h1>

          <p>model_id = "meta-llama/Llama-2-7b-chat-hf"</p>

          <h1 id="configs-for-compiling-model">configs for compiling model</h1>

          <p>compiler_args = {"num_cores": 2, "auto_cast_type": "fp16"}<br>input_shapes
          = {<br>          "sequence_length": 2048, # max length to generate<br>            "batch_size":
          2 # batch size for the model<br>              }</p>

          <p>llm = NeuronModelForCausalLM.from_pretrained(model_id, export=True, **input_shapes,
          **compiler_args)<br>#tokenizer = AutoTokenizer.from_pretrained(model_id)</p>

          <h1 id="save-locally-or-upload-to-the-huggingface-hub">Save locally or upload
          to the HuggingFace Hub</h1>

          <p>save_directory = "llama_neuron"<br>llm.save_pretrained(save_directory)<br>#tokenizer.save_pretrained(save_directory)  </p>

          <hr>

          <p>2- Running the following commands: </p>

          <blockquote>

          <p>optimum-cli export neuron --model meta-llama/Llama-2-7b-chat-hf --batch_size
          1 --sequence_length 4096 llama2-7B-compiled/<br>optimum-cli export neuron
          --model meta-llama/Llama-2-7b-chat-hf --task text-generation --batch_size
          1 --sequence_length 4096 llama2-7B-compiled/</p>

          </blockquote>

          '
        raw: "I attempted to compile the model using two methods as outlined in the\
          \ text-generation procedure provided here (https://huggingface.co/docs/optimum-neuron/guides/models#generative-nlp-models):\n\
          \n1- Executing the following code: \n-------------------------------------------------------------------------------\n\
          from optimum.neuron import NeuronModelForCausalLM\nfrom transformers import\
          \ AutoTokenizer\n\n# model id you want to compile\nmodel_id = \"meta-llama/Llama-2-7b-chat-hf\"\
          \n\n# configs for compiling model\ncompiler_args = {\"num_cores\": 2, \"\
          auto_cast_type\": \"fp16\"}\ninput_shapes = {\n          \"sequence_length\"\
          : 2048, # max length to generate \n            \"batch_size\": 2 # batch\
          \ size for the model\n              }\n\nllm = NeuronModelForCausalLM.from_pretrained(model_id,\
          \ export=True, **input_shapes, **compiler_args)\n#tokenizer = AutoTokenizer.from_pretrained(model_id)\n\
          \n# Save locally or upload to the HuggingFace Hub\nsave_directory = \"llama_neuron\"\
          \nllm.save_pretrained(save_directory)\n#tokenizer.save_pretrained(save_directory)\
          \  \n-------------------------------------------------------------------------------\n\
          2- Running the following commands: \n> optimum-cli export neuron --model\
          \ meta-llama/Llama-2-7b-chat-hf --batch_size 1 --sequence_length 4096 llama2-7B-compiled/\
          \  \n> optimum-cli export neuron --model meta-llama/Llama-2-7b-chat-hf --task\
          \ text-generation --batch_size 1 --sequence_length 4096 llama2-7B-compiled/\n"
        updatedAt: '2024-01-08T09:03:59.684Z'
      numEdits: 0
      reactions: []
    id: 659bba7f4487b430c2120a32
    type: comment
  author: WaelDataReply
  content: "I attempted to compile the model using two methods as outlined in the\
    \ text-generation procedure provided here (https://huggingface.co/docs/optimum-neuron/guides/models#generative-nlp-models):\n\
    \n1- Executing the following code: \n-------------------------------------------------------------------------------\n\
    from optimum.neuron import NeuronModelForCausalLM\nfrom transformers import AutoTokenizer\n\
    \n# model id you want to compile\nmodel_id = \"meta-llama/Llama-2-7b-chat-hf\"\
    \n\n# configs for compiling model\ncompiler_args = {\"num_cores\": 2, \"auto_cast_type\"\
    : \"fp16\"}\ninput_shapes = {\n          \"sequence_length\": 2048, # max length\
    \ to generate \n            \"batch_size\": 2 # batch size for the model\n   \
    \           }\n\nllm = NeuronModelForCausalLM.from_pretrained(model_id, export=True,\
    \ **input_shapes, **compiler_args)\n#tokenizer = AutoTokenizer.from_pretrained(model_id)\n\
    \n# Save locally or upload to the HuggingFace Hub\nsave_directory = \"llama_neuron\"\
    \nllm.save_pretrained(save_directory)\n#tokenizer.save_pretrained(save_directory)\
    \  \n-------------------------------------------------------------------------------\n\
    2- Running the following commands: \n> optimum-cli export neuron --model meta-llama/Llama-2-7b-chat-hf\
    \ --batch_size 1 --sequence_length 4096 llama2-7B-compiled/  \n> optimum-cli export\
    \ neuron --model meta-llama/Llama-2-7b-chat-hf --task text-generation --batch_size\
    \ 1 --sequence_length 4096 llama2-7B-compiled/\n"
  created_at: 2024-01-08 09:03:59+00:00
  edited: false
  hidden: false
  id: 659bba7f4487b430c2120a32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
      fullname: David Corvoysier
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dacorvo
      type: user
    createdAt: '2024-01-08T12:10:59.000Z'
    data:
      edited: false
      editors:
      - dacorvo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9191061854362488
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647995564be04c76ce4547b3/KpP0yuQMsqb-z6N9h4Ykg.jpeg?w=200&h=200&f=face
          fullname: David Corvoysier
          isHf: true
          isPro: false
          name: dacorvo
          type: user
        html: '<p>The second method is not supported for text-generation, and you
          should get the error you mentioned.<br>The first should work: what kind
          of error do you get ?</p>

          '
        raw: 'The second method is not supported for text-generation, and you should
          get the error you mentioned.

          The first should work: what kind of error do you get ?'
        updatedAt: '2024-01-08T12:10:59.551Z'
      numEdits: 0
      reactions: []
    id: 659be6538c5c6688862d23ae
    type: comment
  author: dacorvo
  content: 'The second method is not supported for text-generation, and you should
    get the error you mentioned.

    The first should work: what kind of error do you get ?'
  created_at: 2024-01-08 12:10:59+00:00
  edited: false
  hidden: false
  id: 659be6538c5c6688862d23ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
      fullname: Jim Burtoft
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: jburtoft
      type: user
    createdAt: '2024-01-08T15:30:11.000Z'
    data:
      edited: false
      editors:
      - jburtoft
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9273123741149902
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
          fullname: Jim Burtoft
          isHf: false
          isPro: true
          name: jburtoft
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;WaelDataReply&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/WaelDataReply\"\
          >@<span class=\"underline\">WaelDataReply</span></a></span>\n\n\t</span></span><br>You\
          \ say that you are using the Hugging Face DLAMI, but you also list Neuron\
          \ 2.15.0.  The latest HF DLAMI (as of the end of December) includes Neuron\
          \ 2.16.0.  They also update some of the other libraries like transformer-neuronx.\
          \  Even though the earlier version may say that it supports Llama 7B, there\
          \ is a distinction between training support and inference support.</p>\n\
          <p>Redeploy your system and use the latest AMI version.  I recommend a inf2.8xlarge\
          \ as the inf2.xlarge can sometimes run out of RAM on compiling.  Also, change\
          \ the default drive space to at least 200GB for compilation.  </p>\n<p>I'm\
          \ running through your code now on the latest HF DLAMI, and it is into compiling\
          \ the neffs.  No errors yet.</p>\n<p>Jim</p>\n"
        raw: "@WaelDataReply \nYou say that you are using the Hugging Face DLAMI,\
          \ but you also list Neuron 2.15.0.  The latest HF DLAMI (as of the end of\
          \ December) includes Neuron 2.16.0.  They also update some of the other\
          \ libraries like transformer-neuronx.  Even though the earlier version may\
          \ say that it supports Llama 7B, there is a distinction between training\
          \ support and inference support.\n\nRedeploy your system and use the latest\
          \ AMI version.  I recommend a inf2.8xlarge as the inf2.xlarge can sometimes\
          \ run out of RAM on compiling.  Also, change the default drive space to\
          \ at least 200GB for compilation.  \n\nI'm running through your code now\
          \ on the latest HF DLAMI, and it is into compiling the neffs.  No errors\
          \ yet.\n\nJim\n\n"
        updatedAt: '2024-01-08T15:30:11.265Z'
      numEdits: 0
      reactions: []
    id: 659c15030868046a55e3aff6
    type: comment
  author: jburtoft
  content: "@WaelDataReply \nYou say that you are using the Hugging Face DLAMI, but\
    \ you also list Neuron 2.15.0.  The latest HF DLAMI (as of the end of December)\
    \ includes Neuron 2.16.0.  They also update some of the other libraries like transformer-neuronx.\
    \  Even though the earlier version may say that it supports Llama 7B, there is\
    \ a distinction between training support and inference support.\n\nRedeploy your\
    \ system and use the latest AMI version.  I recommend a inf2.8xlarge as the inf2.xlarge\
    \ can sometimes run out of RAM on compiling.  Also, change the default drive space\
    \ to at least 200GB for compilation.  \n\nI'm running through your code now on\
    \ the latest HF DLAMI, and it is into compiling the neffs.  No errors yet.\n\n\
    Jim\n\n"
  created_at: 2024-01-08 15:30:11+00:00
  edited: false
  hidden: false
  id: 659c15030868046a55e3aff6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
      fullname: Jim Burtoft
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: jburtoft
      type: user
    createdAt: '2024-01-08T15:59:06.000Z'
    data:
      edited: false
      editors:
      - jburtoft
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6730639338493347
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6zEXtLUqS9pLyIKmDbalR.png?w=200&h=200&f=face
          fullname: Jim Burtoft
          isHf: false
          isPro: true
          name: jburtoft
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;WaelDataReply&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/WaelDataReply\"\
          >@<span class=\"underline\">WaelDataReply</span></a></span>\n\n\t</span></span><br>FYI,\
          \ compilation completed successfully on the latest version of the HF DLAMI.\
          \  I followed the rest of your code to save it out (along with the tokenizer).\
          \  </p>\n<p>I then quit() python and started it again (to release the Neuron\
          \ cores).  I then successfully ran the model with:</p>\n<pre><code class=\"\
          language->>>\">&gt;&gt;&gt; pipe = pipeline(\"text-generation\", \"llama_neuron\"\
          )\n&gt;&gt;&gt; messages = [\n...     {\"role\": \"user\", \"content\":\
          \ \"What is 2+2?\"},\n... ]\n&gt;&gt;&gt; prompt = pipe.tokenizer.apply_chat_template(messages,\
          \ tokenize=False, add_generation_prompt=True)\n&gt;&gt;&gt; # Run generation\n\
          &gt;&gt;&gt; outputs = pipe(prompt, max_new_tokens=256, do_sample=True,\
          \ temperature=0.7, top_k=50, top_p=0.95)\nBoth `max_new_tokens` (=256) and\
          \ `max_length`(=4096) seem to have been set. `max_new_tokens` will take\
          \ precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n\
          Inputs will be padded to match the model static batch size. This will increase\
          \ latency.\n2024-Jan-08 15:56:10.0912 6318:6500 [1] nccl_net_ofi_init:1415\
          \ CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n2024-Jan-08 15:56:10.0912\
          \ 6318:6500 [1] init.cc:138 CCOM WARN OFI plugin initNet() failed is EFA\
          \ enabled?\n&gt;&gt;&gt; print(outputs[0][\"generated_text\"])\n&lt;s&gt;[INST]\
          \ What is 2+2? [/INST]  The answer to 2+2 is 4.\n&gt;&gt;&gt;\n</code></pre>\n"
        raw: "@WaelDataReply \nFYI, compilation completed successfully on the latest\
          \ version of the HF DLAMI.  I followed the rest of your code to save it\
          \ out (along with the tokenizer).  \n\nI then quit() python and started\
          \ it again (to release the Neuron cores).  I then successfully ran the model\
          \ with:\n\n```>>> from optimum.neuron import pipeline\n>>> pipe = pipeline(\"\
          text-generation\", \"llama_neuron\")\n>>> messages = [\n...     {\"role\"\
          : \"user\", \"content\": \"What is 2+2?\"},\n... ]\n>>> prompt = pipe.tokenizer.apply_chat_template(messages,\
          \ tokenize=False, add_generation_prompt=True)\n>>> # Run generation\n>>>\
          \ outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7,\
          \ top_k=50, top_p=0.95)\nBoth `max_new_tokens` (=256) and `max_length`(=4096)\
          \ seem to have been set. `max_new_tokens` will take precedence. Please refer\
          \ to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n\
          Inputs will be padded to match the model static batch size. This will increase\
          \ latency.\n2024-Jan-08 15:56:10.0912 6318:6500 [1] nccl_net_ofi_init:1415\
          \ CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n2024-Jan-08 15:56:10.0912\
          \ 6318:6500 [1] init.cc:138 CCOM WARN OFI plugin initNet() failed is EFA\
          \ enabled?\n>>> print(outputs[0][\"generated_text\"])\n<s>[INST] What is\
          \ 2+2? [/INST]  The answer to 2+2 is 4.\n>>>\n```"
        updatedAt: '2024-01-08T15:59:06.601Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - dacorvo
        - WaelDataReply
    id: 659c1bcac94d146486ab66d2
    type: comment
  author: jburtoft
  content: "@WaelDataReply \nFYI, compilation completed successfully on the latest\
    \ version of the HF DLAMI.  I followed the rest of your code to save it out (along\
    \ with the tokenizer).  \n\nI then quit() python and started it again (to release\
    \ the Neuron cores).  I then successfully ran the model with:\n\n```>>> from optimum.neuron\
    \ import pipeline\n>>> pipe = pipeline(\"text-generation\", \"llama_neuron\")\n\
    >>> messages = [\n...     {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n\
    ... ]\n>>> prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False,\
    \ add_generation_prompt=True)\n>>> # Run generation\n>>> outputs = pipe(prompt,\
    \ max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n\
    Both `max_new_tokens` (=256) and `max_length`(=4096) seem to have been set. `max_new_tokens`\
    \ will take precedence. Please refer to the documentation for more information.\
    \ (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n\
    Inputs will be padded to match the model static batch size. This will increase\
    \ latency.\n2024-Jan-08 15:56:10.0912 6318:6500 [1] nccl_net_ofi_init:1415 CCOM\
    \ WARN NET/OFI aws-ofi-nccl initialization failed\n2024-Jan-08 15:56:10.0912 6318:6500\
    \ [1] init.cc:138 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n>>> print(outputs[0][\"\
    generated_text\"])\n<s>[INST] What is 2+2? [/INST]  The answer to 2+2 is 4.\n\
    >>>\n```"
  created_at: 2024-01-08 15:59:06+00:00
  edited: false
  hidden: false
  id: 659c1bcac94d146486ab66d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e2b98b87085ec4613c50c47953a701a.svg
      fullname: Saideni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WaelDataReply
      type: user
    createdAt: '2024-01-11T10:46:00.000Z'
    data:
      edited: false
      editors:
      - WaelDataReply
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.947174072265625
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e2b98b87085ec4613c50c47953a701a.svg
          fullname: Saideni
          isHf: false
          isPro: false
          name: WaelDataReply
          type: user
        html: '<p>I found that using Neuron version 2.16.0 on an inf2.48xlarge instance,
          instead of version 2.15.0, was essential for successfully compiling the
          model. The updated configuration resolved the issue, and the model has been
          compiled and pushed to HF.<br>Thanks for the help. </p>

          '
        raw: "I found that using Neuron version 2.16.0 on an inf2.48xlarge instance,\
          \ instead of version 2.15.0, was essential for successfully compiling the\
          \ model. The updated configuration resolved the issue, and the model has\
          \ been compiled and pushed to HF. \nThanks for the help. "
        updatedAt: '2024-01-11T10:46:00.695Z'
      numEdits: 0
      reactions: []
    id: 659fc6e8e713bcb104506545
    type: comment
  author: WaelDataReply
  content: "I found that using Neuron version 2.16.0 on an inf2.48xlarge instance,\
    \ instead of version 2.15.0, was essential for successfully compiling the model.\
    \ The updated configuration resolved the issue, and the model has been compiled\
    \ and pushed to HF. \nThanks for the help. "
  created_at: 2024-01-11 10:46:00+00:00
  edited: false
  hidden: false
  id: 659fc6e8e713bcb104506545
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: aws-neuron/Llama-2-7b-chat-hf-seqlen-2048-bs-1
repo_type: model
status: open
target_branch: null
title: Unable to successfully compile the model meta-llama/Llama-2-7b-chat-hf on Inf2
  instance
