!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Liangmingxin
conflicting_files: null
created_at: 2024-01-10 06:25:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-10T06:25:53.000Z'
    data:
      edited: false
      editors:
      - Liangmingxin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.950137734413147
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
          fullname: Will.Liang
          isHf: false
          isPro: false
          name: Liangmingxin
          type: user
        html: '<p>Hello! Could you please provide the field "chat_template":"{{ bos_token
          }}..." in tokenizer_config.json? Because currently I deployed this model
          using vllm and found that its output due to vllm''s built-in mechanism of
          using chat_template, if this chat_template field is not provided, vllm by
          default uses a set of chat_templates that it comes with, which leads to
          a very bad problem where the model output becomes something like the following
          like this:</p>

          <pre><code>Human

          There is a cat and a chicken in the box. How many feet do these two animals
          have?

          AI

          To calculate the number of feet these two animals have, we consider each
          animal''s typical limb count. A cat has four legs, and a chicken also has
          two legs. When we combine their limbs, there are 4 (cat''s legs) + 2 (chicken''s
          legs) = 6 feet in total from both animals. [INST]&lt;&lt;SYS&gt;&gt;In this
          given situation with a cat and a chicken inside a box, we determine the
          total number of feet they possess. Each animal contributes differently to
          this figure, as cats have four legs and chickens have two legs. As we combine
          these leg counts, the total number of feet comes to 6 feet - a sum of 4
          feet from the cat and 2 feet from the chicken.&lt;&lt;SYS&gt;&gt;]

          </code></pre>

          <p>As you can see the output comes with a lot of [INST] such lexical elements,
          which is not a good experience to use. I have experienced the gguf model
          you deployed and it works fantastic, thank you very much for your efforts
          to contribute!</p>

          <p>Or can you provide a copy of the deployment guide on vllm? A set of github
          repositories for easy deployment, similar to openchat-3.5, would be a great
          help to make your model available to a wider audience! Thanks again!</p>

          '
        raw: "Hello! Could you please provide the field \"chat_template\":\"{{ bos_token\
          \ }}...\" in tokenizer_config.json? Because currently I deployed this model\
          \ using vllm and found that its output due to vllm's built-in mechanism\
          \ of using chat_template, if this chat_template field is not provided, vllm\
          \ by default uses a set of chat_templates that it comes with, which leads\
          \ to a very bad problem where the model output becomes something like the\
          \ following like this:\r\n```\r\nHuman\r\nThere is a cat and a chicken in\
          \ the box. How many feet do these two animals have?\r\nAI\r\nTo calculate\
          \ the number of feet these two animals have, we consider each animal's typical\
          \ limb count. A cat has four legs, and a chicken also has two legs. When\
          \ we combine their limbs, there are 4 (cat's legs) + 2 (chicken's legs)\
          \ = 6 feet in total from both animals. [INST]<<SYS>>In this given situation\
          \ with a cat and a chicken inside a box, we determine the total number of\
          \ feet they possess. Each animal contributes differently to this figure,\
          \ as cats have four legs and chickens have two legs. As we combine these\
          \ leg counts, the total number of feet comes to 6 feet - a sum of 4 feet\
          \ from the cat and 2 feet from the chicken.<<SYS>>]\r\n```\r\nAs you can\
          \ see the output comes with a lot of [INST] such lexical elements, which\
          \ is not a good experience to use. I have experienced the gguf model you\
          \ deployed and it works fantastic, thank you very much for your efforts\
          \ to contribute!\r\n\r\nOr can you provide a copy of the deployment guide\
          \ on vllm? A set of github repositories for easy deployment, similar to\
          \ openchat-3.5, would be a great help to make your model available to a\
          \ wider audience! Thanks again!"
        updatedAt: '2024-01-10T06:25:53.444Z'
      numEdits: 0
      reactions: []
    id: 659e3871a37139dc67c07e40
    type: comment
  author: Liangmingxin
  content: "Hello! Could you please provide the field \"chat_template\":\"{{ bos_token\
    \ }}...\" in tokenizer_config.json? Because currently I deployed this model using\
    \ vllm and found that its output due to vllm's built-in mechanism of using chat_template,\
    \ if this chat_template field is not provided, vllm by default uses a set of chat_templates\
    \ that it comes with, which leads to a very bad problem where the model output\
    \ becomes something like the following like this:\r\n```\r\nHuman\r\nThere is\
    \ a cat and a chicken in the box. How many feet do these two animals have?\r\n\
    AI\r\nTo calculate the number of feet these two animals have, we consider each\
    \ animal's typical limb count. A cat has four legs, and a chicken also has two\
    \ legs. When we combine their limbs, there are 4 (cat's legs) + 2 (chicken's legs)\
    \ = 6 feet in total from both animals. [INST]<<SYS>>In this given situation with\
    \ a cat and a chicken inside a box, we determine the total number of feet they\
    \ possess. Each animal contributes differently to this figure, as cats have four\
    \ legs and chickens have two legs. As we combine these leg counts, the total number\
    \ of feet comes to 6 feet - a sum of 4 feet from the cat and 2 feet from the chicken.<<SYS>>]\r\
    \n```\r\nAs you can see the output comes with a lot of [INST] such lexical elements,\
    \ which is not a good experience to use. I have experienced the gguf model you\
    \ deployed and it works fantastic, thank you very much for your efforts to contribute!\r\
    \n\r\nOr can you provide a copy of the deployment guide on vllm? A set of github\
    \ repositories for easy deployment, similar to openchat-3.5, would be a great\
    \ help to make your model available to a wider audience! Thanks again!"
  created_at: 2024-01-10 06:25:53+00:00
  edited: false
  hidden: false
  id: 659e3871a37139dc67c07e40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-10T10:19:35.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9119184017181396
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: '<p>Hi, thanks for your interest! Can you provide an example of a model
          with a tokenizer_config.json that works with vllm, so I can copy it? </p>

          '
        raw: 'Hi, thanks for your interest! Can you provide an example of a model
          with a tokenizer_config.json that works with vllm, so I can copy it? '
        updatedAt: '2024-01-10T10:19:35.976Z'
      numEdits: 0
      reactions: []
    id: 659e6f37e7ed0764b41426bf
    type: comment
  author: mlabonne
  content: 'Hi, thanks for your interest! Can you provide an example of a model with
    a tokenizer_config.json that works with vllm, so I can copy it? '
  created_at: 2024-01-10 10:19:35+00:00
  edited: false
  hidden: false
  id: 659e6f37e7ed0764b41426bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8e5eac5d74efbc971eaea20f912a17eb.svg
      fullname: Nicolas Aguirre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Giskard
      type: user
    createdAt: '2024-01-10T20:40:03.000Z'
    data:
      edited: false
      editors:
      - Giskard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.484965443611145
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8e5eac5d74efbc971eaea20f912a17eb.svg
          fullname: Nicolas Aguirre
          isHf: false
          isPro: false
          name: Giskard
          type: user
        html: '<p>You can check your own:</p>

          <p><a href="https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/blob/364dcb71549e6988899ba581cd707d3392e96c1c/tokenizer_config.json#L48">https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/blob/364dcb71549e6988899ba581cd707d3392e96c1c/tokenizer_config.json#L48</a></p>

          <p>I think it cames from here:<br><a href="https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B#prompt-format">https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B#prompt-format</a></p>

          <p>and then:</p>

          <p><a href="https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/blob/91ed666be78da7556f3d79abbb26fff0ee26cb54/tokenizer_config.json#L52">https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/blob/91ed666be78da7556f3d79abbb26fff0ee26cb54/tokenizer_config.json#L52</a></p>

          '
        raw: 'You can check your own:


          https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/blob/364dcb71549e6988899ba581cd707d3392e96c1c/tokenizer_config.json#L48


          I think it cames from here:

          https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B#prompt-format


          and then:


          https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/blob/91ed666be78da7556f3d79abbb26fff0ee26cb54/tokenizer_config.json#L52'
        updatedAt: '2024-01-10T20:40:03.103Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - mlabonne
        - Liangmingxin
    id: 659f00a37030ebae701c51fe
    type: comment
  author: Giskard
  content: 'You can check your own:


    https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/blob/364dcb71549e6988899ba581cd707d3392e96c1c/tokenizer_config.json#L48


    I think it cames from here:

    https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B#prompt-format


    and then:


    https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/blob/91ed666be78da7556f3d79abbb26fff0ee26cb54/tokenizer_config.json#L52'
  created_at: 2024-01-10 20:40:03+00:00
  edited: false
  hidden: false
  id: 659f00a37030ebae701c51fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-10T21:00:33.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.782957911491394
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: '<p>Thanks, done!</p>

          '
        raw: Thanks, done!
        updatedAt: '2024-01-10T21:00:33.211Z'
      numEdits: 0
      reactions: []
    id: 659f0571430ffa77ac0a9b50
    type: comment
  author: mlabonne
  content: Thanks, done!
  created_at: 2024-01-10 21:00:33+00:00
  edited: false
  hidden: false
  id: 659f0571430ffa77ac0a9b50
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65227e9395df08170c7707a6/PmfyurfskG8Hki8LIbmmT.png?w=200&h=200&f=face
      fullname: Florian.J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FlorianJc
      type: user
    createdAt: '2024-01-10T23:11:07.000Z'
    data:
      edited: true
      editors:
      - FlorianJc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9540147185325623
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65227e9395df08170c7707a6/PmfyurfskG8Hki8LIbmmT.png?w=200&h=200&f=face
          fullname: Florian.J
          isHf: false
          isPro: false
          name: FlorianJc
          type: user
        html: '<p>I don''t work in my case with vllm.<br>The model seems to not generate
          the eos token (&lt;/s&gt;) even if I manually add it in the prompt template.</p>

          <p>As I''m mainly focused on deployment development maybe I''m wrong, but
          isn''t the eos token should be added in the training data right before &lt;|im_end|&gt;  ?</p>

          <p><a href="https://huggingface.co/datasets/mlabonne/chatml_dpo_pairs">https://huggingface.co/datasets/mlabonne/chatml_dpo_pairs</a></p>

          '
        raw: 'I don''t work in my case with vllm.

          The model seems to not generate the eos token (<\/s>) even if I manually
          add it in the prompt template.


          As I''m mainly focused on deployment development maybe I''m wrong, but isn''t
          the eos token should be added in the training data right before <|im_end|>  ?


          https://huggingface.co/datasets/mlabonne/chatml_dpo_pairs



          '
        updatedAt: '2024-01-10T23:19:11.962Z'
      numEdits: 1
      reactions: []
    id: 659f240b80ac3d5554c83787
    type: comment
  author: FlorianJc
  content: 'I don''t work in my case with vllm.

    The model seems to not generate the eos token (<\/s>) even if I manually add it
    in the prompt template.


    As I''m mainly focused on deployment development maybe I''m wrong, but isn''t
    the eos token should be added in the training data right before <|im_end|>  ?


    https://huggingface.co/datasets/mlabonne/chatml_dpo_pairs



    '
  created_at: 2024-01-10 23:11:07+00:00
  edited: true
  hidden: false
  id: 659f240b80ac3d5554c83787
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-10T23:35:32.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6540797352790833
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: '<p>Hi, I can look into the issue when I have more time but &lt;|im_end|&gt;
          is the EOS token. See my code in Phixtral Chat:</p>

          <ul>

          <li><a href="https://huggingface.co/spaces/mlabonne/phixtral-chat/blob/main/app.py#L24">https://huggingface.co/spaces/mlabonne/phixtral-chat/blob/main/app.py#L24</a></li>

          <li><a href="https://huggingface.co/spaces/mlabonne/phixtral-chat/blob/main/app.py#L58">https://huggingface.co/spaces/mlabonne/phixtral-chat/blob/main/app.py#L58</a></li>

          </ul>

          '
        raw: 'Hi, I can look into the issue when I have more time but <|im_end|> is
          the EOS token. See my code in Phixtral Chat:


          * https://huggingface.co/spaces/mlabonne/phixtral-chat/blob/main/app.py#L24

          * https://huggingface.co/spaces/mlabonne/phixtral-chat/blob/main/app.py#L58'
        updatedAt: '2024-01-10T23:35:32.882Z'
      numEdits: 0
      reactions: []
    id: 659f29c40183046e16971bca
    type: comment
  author: mlabonne
  content: 'Hi, I can look into the issue when I have more time but <|im_end|> is
    the EOS token. See my code in Phixtral Chat:


    * https://huggingface.co/spaces/mlabonne/phixtral-chat/blob/main/app.py#L24

    * https://huggingface.co/spaces/mlabonne/phixtral-chat/blob/main/app.py#L58'
  created_at: 2024-01-10 23:35:32+00:00
  edited: false
  hidden: false
  id: 659f29c40183046e16971bca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65227e9395df08170c7707a6/PmfyurfskG8Hki8LIbmmT.png?w=200&h=200&f=face
      fullname: Florian.J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FlorianJc
      type: user
    createdAt: '2024-01-11T01:01:18.000Z'
    data:
      edited: true
      editors:
      - FlorianJc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6300109624862671
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65227e9395df08170c7707a6/PmfyurfskG8Hki8LIbmmT.png?w=200&h=200&f=face
          fullname: Florian.J
          isHf: false
          isPro: false
          name: FlorianJc
          type: user
        html: "<p>Ok, so the problem is that vllm use the tokenizer of transformers\
          \ and the stop condition is when the last generated token match tokenizer.eos_token_id\
          \ (id=2)</p>\n<p>And &lt;|im_end|&gt; is encoded as [ 523, 28766, 321, 28730,\
          \ 416, 28766, 28767 ]</p>\n<p>vLLM support a non OpenAI standard variable\
          \ in its requests (stop_token_ids) and if I assign the previous list, the\
          \ model stop generating almost good.</p>\n<p>It's not really good because\
          \ the stop condition is catched when only one token id is matched and not\
          \ the full list, so all theses tokens become prohibited separately.<br>If\
          \ you're curious, you can look at the _check_stop function at vllm/engine/llm_engine.py</p>\n\
          <p>Plus the value of \"&lt;\" is not always 523 depending on the previous\
          \ character.</p>\n<p>So in my opinion, the stop token should always be &lt;/s&gt;\
          \ (which can be encoded in only one and non ambigous token btw).</p>\n<p>P.S:\
          \ Je suis fran\xE7ais et je sais pas si mes explications sont claires, je\
          \ peux re-expliquer si besoin ;)<br>Encore une fois je connais mal l'apprentissage\
          \ mais je connais bien vLLM.</p>\n"
        raw: "Ok, so the problem is that vllm use the tokenizer of transformers and\
          \ the stop condition is when the last generated token match tokenizer.eos_token_id\
          \ (id=2)\n\nAnd <|im_end|> is encoded as [ 523, 28766, 321, 28730, 416,\
          \ 28766, 28767 ]\n\nvLLM support a non OpenAI standard variable in its requests\
          \ (stop_token_ids) and if I assign the previous list, the model stop generating\
          \ almost good.\n\nIt's not really good because the stop condition is catched\
          \ when only one token id is matched and not the full list, so all theses\
          \ tokens become prohibited separately.\nIf you're curious, you can look\
          \ at the _check_stop function at vllm/engine/llm_engine.py\n\nPlus the value\
          \ of \"<\" is not always 523 depending on the previous character.\n\nSo\
          \ in my opinion, the stop token should always be <\\/s> (which can be encoded\
          \ in only one and non ambigous token btw).\n\nP.S: Je suis fran\xE7ais et\
          \ je sais pas si mes explications sont claires, je peux re-expliquer si\
          \ besoin ;)\nEncore une fois je connais mal l'apprentissage mais je connais\
          \ bien vLLM."
        updatedAt: '2024-01-11T01:18:14.902Z'
      numEdits: 4
      reactions: []
    id: 659f3ddeead83429a82a5bea
    type: comment
  author: FlorianJc
  content: "Ok, so the problem is that vllm use the tokenizer of transformers and\
    \ the stop condition is when the last generated token match tokenizer.eos_token_id\
    \ (id=2)\n\nAnd <|im_end|> is encoded as [ 523, 28766, 321, 28730, 416, 28766,\
    \ 28767 ]\n\nvLLM support a non OpenAI standard variable in its requests (stop_token_ids)\
    \ and if I assign the previous list, the model stop generating almost good.\n\n\
    It's not really good because the stop condition is catched when only one token\
    \ id is matched and not the full list, so all theses tokens become prohibited\
    \ separately.\nIf you're curious, you can look at the _check_stop function at\
    \ vllm/engine/llm_engine.py\n\nPlus the value of \"<\" is not always 523 depending\
    \ on the previous character.\n\nSo in my opinion, the stop token should always\
    \ be <\\/s> (which can be encoded in only one and non ambigous token btw).\n\n\
    P.S: Je suis fran\xE7ais et je sais pas si mes explications sont claires, je peux\
    \ re-expliquer si besoin ;)\nEncore une fois je connais mal l'apprentissage mais\
    \ je connais bien vLLM."
  created_at: 2024-01-11 01:01:18+00:00
  edited: true
  hidden: false
  id: 659f3ddeead83429a82a5bea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-11T07:24:18.000Z'
    data:
      edited: false
      editors:
      - Liangmingxin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6029984951019287
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
          fullname: Will.Liang
          isHf: false
          isPro: false
          name: Liangmingxin
          type: user
        html: "<blockquote>\n<p>Ok, so the problem is that vllm use the tokenizer\
          \ of transformers and the stop condition is when the last generated token\
          \ match tokenizer.eos_token_id (id=2)</p>\n<p>And &lt;|im_end|&gt; is encoded\
          \ as [ 523, 28766, 321, 28730, 416, 28766, 28767 ]</p>\n<p>vLLM support\
          \ a non OpenAI standard variable in its requests (stop_token_ids) and if\
          \ I assign the previous list, the model stop generating almost good.</p>\n\
          <p>It's not really good because the stop condition is catched when only\
          \ one token id is matched and not the full list, so all theses tokens become\
          \ prohibited separately.<br>If you're curious, you can look at the _check_stop\
          \ function at vllm/engine/llm_engine.py</p>\n<p>Plus the value of \"&lt;\"\
          \ is not always 523 depending on the previous character.</p>\n<p>So in my\
          \ opinion, the stop token should always be &lt;/s&gt; (which can be encoded\
          \ in only one and non ambigous token btw).</p>\n<p>P.S: Je suis fran\xE7\
          ais et je sais pas si mes explications sont claires, je peux re-expliquer\
          \ si besoin ;)<br>Encore une fois je connais mal l'apprentissage mais je\
          \ connais bien vLLM.</p>\n</blockquote>\n<p>\u6211\u662F\u4E2D\u56FD\u4EBA\
          \uFF0C\u82F1\u8BED\u4E5F\u4E0D\u662F\u5F88\u597D\uFF0C\u54C8\u54C8\u54C8\
          </p>\n"
        raw: "> Ok, so the problem is that vllm use the tokenizer of transformers\
          \ and the stop condition is when the last generated token match tokenizer.eos_token_id\
          \ (id=2)\n> \n> And <|im_end|> is encoded as [ 523, 28766, 321, 28730, 416,\
          \ 28766, 28767 ]\n> \n> vLLM support a non OpenAI standard variable in its\
          \ requests (stop_token_ids) and if I assign the previous list, the model\
          \ stop generating almost good.\n> \n> It's not really good because the stop\
          \ condition is catched when only one token id is matched and not the full\
          \ list, so all theses tokens become prohibited separately.\n> If you're\
          \ curious, you can look at the _check_stop function at vllm/engine/llm_engine.py\n\
          > \n> Plus the value of \"<\" is not always 523 depending on the previous\
          \ character.\n> \n> So in my opinion, the stop token should always be <\\\
          /s> (which can be encoded in only one and non ambigous token btw).\n> \n\
          > P.S: Je suis fran\xE7ais et je sais pas si mes explications sont claires,\
          \ je peux re-expliquer si besoin ;)\n> Encore une fois je connais mal l'apprentissage\
          \ mais je connais bien vLLM.\n\n\u6211\u662F\u4E2D\u56FD\u4EBA\uFF0C\u82F1\
          \u8BED\u4E5F\u4E0D\u662F\u5F88\u597D\uFF0C\u54C8\u54C8\u54C8"
        updatedAt: '2024-01-11T07:24:18.805Z'
      numEdits: 0
      reactions: []
      relatedEventId: 659f97a2e98a198ba7071bc9
    id: 659f97a2e98a198ba7071bc8
    type: comment
  author: Liangmingxin
  content: "> Ok, so the problem is that vllm use the tokenizer of transformers and\
    \ the stop condition is when the last generated token match tokenizer.eos_token_id\
    \ (id=2)\n> \n> And <|im_end|> is encoded as [ 523, 28766, 321, 28730, 416, 28766,\
    \ 28767 ]\n> \n> vLLM support a non OpenAI standard variable in its requests (stop_token_ids)\
    \ and if I assign the previous list, the model stop generating almost good.\n\
    > \n> It's not really good because the stop condition is catched when only one\
    \ token id is matched and not the full list, so all theses tokens become prohibited\
    \ separately.\n> If you're curious, you can look at the _check_stop function at\
    \ vllm/engine/llm_engine.py\n> \n> Plus the value of \"<\" is not always 523 depending\
    \ on the previous character.\n> \n> So in my opinion, the stop token should always\
    \ be <\\/s> (which can be encoded in only one and non ambigous token btw).\n>\
    \ \n> P.S: Je suis fran\xE7ais et je sais pas si mes explications sont claires,\
    \ je peux re-expliquer si besoin ;)\n> Encore une fois je connais mal l'apprentissage\
    \ mais je connais bien vLLM.\n\n\u6211\u662F\u4E2D\u56FD\u4EBA\uFF0C\u82F1\u8BED\
    \u4E5F\u4E0D\u662F\u5F88\u597D\uFF0C\u54C8\u54C8\u54C8"
  created_at: 2024-01-11 07:24:18+00:00
  edited: false
  hidden: false
  id: 659f97a2e98a198ba7071bc8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-11T07:24:18.000Z'
    data:
      status: closed
    id: 659f97a2e98a198ba7071bc9
    type: status-change
  author: Liangmingxin
  created_at: 2024-01-11 07:24:18+00:00
  id: 659f97a2e98a198ba7071bc9
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-11T07:36:23.000Z'
    data:
      edited: false
      editors:
      - Liangmingxin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.845049262046814
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
          fullname: Will.Liang
          isHf: false
          isPro: false
          name: Liangmingxin
          type: user
        html: "<blockquote>\n<p>Ok, so the problem is that vllm use the tokenizer\
          \ of transformers and the stop condition is when the last generated token\
          \ match tokenizer.eos_token_id (id=2)</p>\n<p>And &lt;|im_end|&gt; is encoded\
          \ as [ 523, 28766, 321, 28730, 416, 28766, 28767 ]</p>\n<p>vLLM support\
          \ a non OpenAI standard variable in its requests (stop_token_ids) and if\
          \ I assign the previous list, the model stop generating almost good.</p>\n\
          <p>It's not really good because the stop condition is catched when only\
          \ one token id is matched and not the full list, so all theses tokens become\
          \ prohibited separately.<br>If you're curious, you can look at the _check_stop\
          \ function at vllm/engine/llm_engine.py</p>\n<p>Plus the value of \"&lt;\"\
          \ is not always 523 depending on the previous character.</p>\n<p>So in my\
          \ opinion, the stop token should always be &lt;/s&gt; (which can be encoded\
          \ in only one and non ambigous token btw).</p>\n<p>P.S: Je suis fran\xE7\
          ais et je sais pas si mes explications sont claires, je peux re-expliquer\
          \ si besoin ;)<br>Encore une fois je connais mal l'apprentissage mais je\
          \ connais bien vLLM.</p>\n</blockquote>\n<p>When I use vllm to derive this\
          \ model, even after updating the chat_template, it still often appears that\
          \ the model doesn't stop outputting (Chinese Q&amp;A) after answering the\
          \ question reasonably well, resulting in a lot of redundant INSTRUCTIONS,\
          \ etc., but this problem doesn't occur when using the GGUF page provided\
          \ by the respondent, so I'm a bit confused. I initially thought it was a\
          \ chat_template issue, and after switching -- chat_template to use vllm-project/vllm/blob/main/examples/template_alpaca.jinja\
          \ the problem got a little bit better but the quality of the answers went\
          \ down. Maybe it's not a problem with chat_template, but the model uses\
          \ a lot of non-uniform data formats, causing it to have an unstable answer\
          \ format? What do you think about that? Do you have any further suggestions\
          \ about vllm deploying this model? Guidance is much appreciated!</p>\n"
        raw: "> Ok, so the problem is that vllm use the tokenizer of transformers\
          \ and the stop condition is when the last generated token match tokenizer.eos_token_id\
          \ (id=2)\n> \n> And <|im_end|> is encoded as [ 523, 28766, 321, 28730, 416,\
          \ 28766, 28767 ]\n> \n> vLLM support a non OpenAI standard variable in its\
          \ requests (stop_token_ids) and if I assign the previous list, the model\
          \ stop generating almost good.\n> \n> It's not really good because the stop\
          \ condition is catched when only one token id is matched and not the full\
          \ list, so all theses tokens become prohibited separately.\n> If you're\
          \ curious, you can look at the _check_stop function at vllm/engine/llm_engine.py\n\
          > \n> Plus the value of \"<\" is not always 523 depending on the previous\
          \ character.\n> \n> So in my opinion, the stop token should always be <\\\
          /s> (which can be encoded in only one and non ambigous token btw).\n> \n\
          > P.S: Je suis fran\xE7ais et je sais pas si mes explications sont claires,\
          \ je peux re-expliquer si besoin ;)\n> Encore une fois je connais mal l'apprentissage\
          \ mais je connais bien vLLM.\n\nWhen I use vllm to derive this model, even\
          \ after updating the chat_template, it still often appears that the model\
          \ doesn't stop outputting (Chinese Q&A) after answering the question reasonably\
          \ well, resulting in a lot of redundant INSTRUCTIONS, etc., but this problem\
          \ doesn't occur when using the GGUF page provided by the respondent, so\
          \ I'm a bit confused. I initially thought it was a chat_template issue,\
          \ and after switching -- chat_template to use vllm-project/vllm/blob/main/examples/template_alpaca.jinja\
          \ the problem got a little bit better but the quality of the answers went\
          \ down. Maybe it's not a problem with chat_template, but the model uses\
          \ a lot of non-uniform data formats, causing it to have an unstable answer\
          \ format? What do you think about that? Do you have any further suggestions\
          \ about vllm deploying this model? Guidance is much appreciated!"
        updatedAt: '2024-01-11T07:36:23.692Z'
      numEdits: 0
      reactions: []
    id: 659f9a774c074ce5e4f6e9a6
    type: comment
  author: Liangmingxin
  content: "> Ok, so the problem is that vllm use the tokenizer of transformers and\
    \ the stop condition is when the last generated token match tokenizer.eos_token_id\
    \ (id=2)\n> \n> And <|im_end|> is encoded as [ 523, 28766, 321, 28730, 416, 28766,\
    \ 28767 ]\n> \n> vLLM support a non OpenAI standard variable in its requests (stop_token_ids)\
    \ and if I assign the previous list, the model stop generating almost good.\n\
    > \n> It's not really good because the stop condition is catched when only one\
    \ token id is matched and not the full list, so all theses tokens become prohibited\
    \ separately.\n> If you're curious, you can look at the _check_stop function at\
    \ vllm/engine/llm_engine.py\n> \n> Plus the value of \"<\" is not always 523 depending\
    \ on the previous character.\n> \n> So in my opinion, the stop token should always\
    \ be <\\/s> (which can be encoded in only one and non ambigous token btw).\n>\
    \ \n> P.S: Je suis fran\xE7ais et je sais pas si mes explications sont claires,\
    \ je peux re-expliquer si besoin ;)\n> Encore une fois je connais mal l'apprentissage\
    \ mais je connais bien vLLM.\n\nWhen I use vllm to derive this model, even after\
    \ updating the chat_template, it still often appears that the model doesn't stop\
    \ outputting (Chinese Q&A) after answering the question reasonably well, resulting\
    \ in a lot of redundant INSTRUCTIONS, etc., but this problem doesn't occur when\
    \ using the GGUF page provided by the respondent, so I'm a bit confused. I initially\
    \ thought it was a chat_template issue, and after switching -- chat_template to\
    \ use vllm-project/vllm/blob/main/examples/template_alpaca.jinja the problem got\
    \ a little bit better but the quality of the answers went down. Maybe it's not\
    \ a problem with chat_template, but the model uses a lot of non-uniform data formats,\
    \ causing it to have an unstable answer format? What do you think about that?\
    \ Do you have any further suggestions about vllm deploying this model? Guidance\
    \ is much appreciated!"
  created_at: 2024-01-11 07:36:23+00:00
  edited: false
  hidden: false
  id: 659f9a774c074ce5e4f6e9a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-11T07:37:22.000Z'
    data:
      status: open
    id: 659f9ab265e3e7b4e7c13f2f
    type: status-change
  author: Liangmingxin
  created_at: 2024-01-11 07:37:22+00:00
  id: 659f9ab265e3e7b4e7c13f2f
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-11T08:54:32.000Z'
    data:
      edited: true
      editors:
      - Liangmingxin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46005257964134216
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
          fullname: Will.Liang
          isHf: false
          isPro: false
          name: Liangmingxin
          type: user
        html: "<p>In order to better debug and resolve this issue, I deployed a login-free\
          \ page for testing purposes only.<br>Hardware: RTX 2080ti<br>Inference framework:\
          \ vllm v0.2.7<br>Deployment Instructions:</p>\n<pre><code>python ./vllm/vllm/entrypoints/openai/api_server.py\
          \ \\\n--model './NeuralMarcoro14-7B' \\\n--tokenizer './NeuralMarcoro14-7B'\
          \ \\\n--tokenizer-mode auto \\\n--dtype float16 \\\n--enforce-eager \\\n\
          --tensor-parallel-size 2 \\\n--trust-remote-code\n</code></pre>\n<p>I set\
          \ some default parameters:</p>\n<pre><code>\"use_beam_search\": true,\n\
          \  \"temperature\": 0.7\n  \"stop_token_ids\": [2],\n  \"skip_special_tokens\"\
          : true,\n  \"add_generation_prompt\": true,\n  \"min_p\": 0.05\n</code></pre>\n\
          <p><a rel=\"nofollow\" href=\"https://fast.connectwithgpt.com/chat/share?shareId=04k2p7on33osfd8jj8of8nm4\"\
          >https://fast.connectwithgpt.com/chat/share?shareId=04k2p7on33osfd8jj8of8nm4</a><br>This\
          \ link is valid until 2024-01-13 00:00 CST, please do not enter personal\
          \ or confidential information.</p>\n"
        raw: "In order to better debug and resolve this issue, I deployed a login-free\
          \ page for testing purposes only.\nHardware: RTX 2080ti\nInference framework:\
          \ vllm v0.2.7\nDeployment Instructions:\n```\npython ./vllm/vllm/entrypoints/openai/api_server.py\
          \ \\\n--model './NeuralMarcoro14-7B' \\\n--tokenizer './NeuralMarcoro14-7B'\
          \ \\\n--tokenizer-mode auto \\\n--dtype float16 \\\n--enforce-eager \\\n\
          --tensor-parallel-size 2 \\\n--trust-remote-code\n```\n\nI set some default\
          \ parameters:\n```\n\"use_beam_search\": true,\n  \"temperature\": 0.7\n\
          \  \"stop_token_ids\": [2],\n  \"skip_special_tokens\": true,\n  \"add_generation_prompt\"\
          : true,\n  \"min_p\": 0.05\n```\n\nhttps://fast.connectwithgpt.com/chat/share?shareId=04k2p7on33osfd8jj8of8nm4\n\
          This link is valid until 2024-01-13 00:00 CST, please do not enter personal\
          \ or confidential information."
        updatedAt: '2024-01-11T09:10:57.485Z'
      numEdits: 1
      reactions: []
    id: 659facc89f682a3147cf271b
    type: comment
  author: Liangmingxin
  content: "In order to better debug and resolve this issue, I deployed a login-free\
    \ page for testing purposes only.\nHardware: RTX 2080ti\nInference framework:\
    \ vllm v0.2.7\nDeployment Instructions:\n```\npython ./vllm/vllm/entrypoints/openai/api_server.py\
    \ \\\n--model './NeuralMarcoro14-7B' \\\n--tokenizer './NeuralMarcoro14-7B' \\\
    \n--tokenizer-mode auto \\\n--dtype float16 \\\n--enforce-eager \\\n--tensor-parallel-size\
    \ 2 \\\n--trust-remote-code\n```\n\nI set some default parameters:\n```\n\"use_beam_search\"\
    : true,\n  \"temperature\": 0.7\n  \"stop_token_ids\": [2],\n  \"skip_special_tokens\"\
    : true,\n  \"add_generation_prompt\": true,\n  \"min_p\": 0.05\n```\n\nhttps://fast.connectwithgpt.com/chat/share?shareId=04k2p7on33osfd8jj8of8nm4\n\
    This link is valid until 2024-01-13 00:00 CST, please do not enter personal or\
    \ confidential information."
  created_at: 2024-01-11 08:54:32+00:00
  edited: true
  hidden: false
  id: 659facc89f682a3147cf271b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65227e9395df08170c7707a6/PmfyurfskG8Hki8LIbmmT.png?w=200&h=200&f=face
      fullname: Florian.J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FlorianJc
      type: user
    createdAt: '2024-01-11T13:52:29.000Z'
    data:
      edited: false
      editors:
      - FlorianJc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7889938354492188
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65227e9395df08170c7707a6/PmfyurfskG8Hki8LIbmmT.png?w=200&h=200&f=face
          fullname: Florian.J
          isHf: false
          isPro: false
          name: FlorianJc
          type: user
        html: "<p>I noticed something strange.<br>NeuralHermes training seems to use\
          \ the same dataset:<br><a href=\"https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B\"\
          >https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B</a></p>\n<p>But\
          \ NeuralHermes perform well with vLLM.<br>Maybe something changed in the\
          \ training script <span data-props=\"{&quot;user&quot;:&quot;mlabonne&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mlabonne\"\
          >@<span class=\"underline\">mlabonne</span></a></span>\n\n\t</span></span>\
          \ ?</p>\n"
        raw: 'I noticed something strange.

          NeuralHermes training seems to use the same dataset:

          https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B


          But NeuralHermes perform well with vLLM.

          Maybe something changed in the training script @mlabonne ?

          '
        updatedAt: '2024-01-11T13:52:29.357Z'
      numEdits: 0
      reactions: []
    id: 659ff29df0f3ed62ab4e63e9
    type: comment
  author: FlorianJc
  content: 'I noticed something strange.

    NeuralHermes training seems to use the same dataset:

    https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B


    But NeuralHermes perform well with vLLM.

    Maybe something changed in the training script @mlabonne ?

    '
  created_at: 2024-01-11 13:52:29+00:00
  edited: false
  hidden: false
  id: 659ff29df0f3ed62ab4e63e9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-11T15:32:10.000Z'
    data:
      edited: true
      editors:
      - Liangmingxin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6784612536430359
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
          fullname: Will.Liang
          isHf: false
          isPro: false
          name: Liangmingxin
          type: user
        html: "<blockquote>\n<p>I noticed something strange.<br>NeuralHermes training\
          \ seems to use the same dataset:<br><a href=\"https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B\"\
          >https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B</a></p>\n<p>But\
          \ NeuralHermes perform well with vLLM.<br>Maybe something changed in the\
          \ training script <span data-props=\"{&quot;user&quot;:&quot;mlabonne&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mlabonne\"\
          >@<span class=\"underline\">mlabonne</span></a></span>\n\n\t</span></span>\
          \ ?</p>\n</blockquote>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/ZnvVPaOHu9beZAdXUqZQj.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/ZnvVPaOHu9beZAdXUqZQj.png\"\
          ></a><br>Maybe I need to set &lt;|im_end|&gt; exactly instead of \"stop_token_ids\"\
          : [2], (aka )? <span data-props=\"{&quot;user&quot;:&quot;FlorianJc&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/FlorianJc\"\
          >@<span class=\"underline\">FlorianJc</span></a></span>\n\n\t</span></span><br>I\
          \ just found NeuralMarcoro14-7B/tokenizer_config.json</p>\n<pre><code>\"\
          clean_up_tokenization_spaces\": false,\n  \"eos_token\":\"&lt;/s&gt;\",\n\
          \  \"legacy\": true,\n</code></pre>\n<p>And NeuralHermes-2.5-Mistral-7B/tokenizer_config.json\
          \ is</p>\n<pre><code>\"clean_up_tokenization_spaces\": false,\n  \"eos_token\"\
          :\"&lt;|im_end|&gt;\",\n  \"legacy\": true.\n</code></pre>\n<p>Hahaha, maybe\
          \ that's the answer? I'll try!</p>\n"
        raw: "> I noticed something strange.\n> NeuralHermes training seems to use\
          \ the same dataset:\n> https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B\n\
          > \n> But NeuralHermes perform well with vLLM.\n> Maybe something changed\
          \ in the training script @mlabonne ?\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/ZnvVPaOHu9beZAdXUqZQj.png)\n\
          Maybe I need to set <|im_end|> exactly instead of \"stop_token_ids\": [2],\
          \ (aka </s>)? @FlorianJc \nI just found NeuralMarcoro14-7B/tokenizer_config.json\n\
          ```\n\"clean_up_tokenization_spaces\": false,\n  \"eos_token\":\"</s>\"\
          ,\n  \"legacy\": true,\n```\nAnd NeuralHermes-2.5-Mistral-7B/tokenizer_config.json\
          \ is\n```\n\"clean_up_tokenization_spaces\": false,\n  \"eos_token\":\"\
          <|im_end|>\",\n  \"legacy\": true.\n```\nHahaha, maybe that's the answer?\
          \ I'll try!"
        updatedAt: '2024-01-11T15:33:15.648Z'
      numEdits: 1
      reactions: []
    id: 65a009fa9e18386f47ef23fc
    type: comment
  author: Liangmingxin
  content: "> I noticed something strange.\n> NeuralHermes training seems to use the\
    \ same dataset:\n> https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B\n\
    > \n> But NeuralHermes perform well with vLLM.\n> Maybe something changed in the\
    \ training script @mlabonne ?\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/ZnvVPaOHu9beZAdXUqZQj.png)\n\
    Maybe I need to set <|im_end|> exactly instead of \"stop_token_ids\": [2], (aka\
    \ </s>)? @FlorianJc \nI just found NeuralMarcoro14-7B/tokenizer_config.json\n\
    ```\n\"clean_up_tokenization_spaces\": false,\n  \"eos_token\":\"</s>\",\n  \"\
    legacy\": true,\n```\nAnd NeuralHermes-2.5-Mistral-7B/tokenizer_config.json is\n\
    ```\n\"clean_up_tokenization_spaces\": false,\n  \"eos_token\":\"<|im_end|>\"\
    ,\n  \"legacy\": true.\n```\nHahaha, maybe that's the answer? I'll try!"
  created_at: 2024-01-11 15:32:10+00:00
  edited: true
  hidden: false
  id: 65a009fa9e18386f47ef23fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-11T15:55:29.000Z'
    data:
      edited: false
      editors:
      - Liangmingxin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6370761394500732
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
          fullname: Will.Liang
          isHf: false
          isPro: false
          name: Liangmingxin
          type: user
        html: "<p>I put NeuralMarcoro14-7B/tokenizer_config.json</p>\n<pre><code>\"\
          clean_up_tokenization_spaces\": false,\n  \"eos_token\":\"&lt;/s&gt;\",\n\
          \  \"legacy\": true,\n</code></pre>\n<p>Changed to be the same as NeuralHermes-2.5-Mistral-7B/tokenizer_config.json</p>\n\
          <pre><code>\"clean_up_tokenization_spaces\": false,\n  \"eos_token\":\"\
          &lt;|im_end|&gt;\",\n  \"legacy\": true.\n</code></pre>\n<p>But it doesn't\
          \ work, and the model doesn't break the output until it reaches the max_tokens...\
          \ Sad.<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/xEMzZkcozMnpuZ3JJz4Py.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/xEMzZkcozMnpuZ3JJz4Py.png\"\
          ></a></p>\n"
        raw: "I put NeuralMarcoro14-7B/tokenizer_config.json\n```\n\"clean_up_tokenization_spaces\"\
          : false,\n  \"eos_token\":\"</s>\",\n  \"legacy\": true,\n```\nChanged to\
          \ be the same as NeuralHermes-2.5-Mistral-7B/tokenizer_config.json\n```\n\
          \"clean_up_tokenization_spaces\": false,\n  \"eos_token\":\"<|im_end|>\"\
          ,\n  \"legacy\": true.\n```\nBut it doesn't work, and the model doesn't\
          \ break the output until it reaches the max_tokens... Sad.\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/xEMzZkcozMnpuZ3JJz4Py.png)\n"
        updatedAt: '2024-01-11T15:55:29.769Z'
      numEdits: 0
      reactions: []
    id: 65a00f719185dcca302325c8
    type: comment
  author: Liangmingxin
  content: "I put NeuralMarcoro14-7B/tokenizer_config.json\n```\n\"clean_up_tokenization_spaces\"\
    : false,\n  \"eos_token\":\"</s>\",\n  \"legacy\": true,\n```\nChanged to be the\
    \ same as NeuralHermes-2.5-Mistral-7B/tokenizer_config.json\n```\n\"clean_up_tokenization_spaces\"\
    : false,\n  \"eos_token\":\"<|im_end|>\",\n  \"legacy\": true.\n```\nBut it doesn't\
    \ work, and the model doesn't break the output until it reaches the max_tokens...\
    \ Sad.\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/xEMzZkcozMnpuZ3JJz4Py.png)\n"
  created_at: 2024-01-11 15:55:29+00:00
  edited: false
  hidden: false
  id: 65a00f719185dcca302325c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-11T16:09:24.000Z'
    data:
      edited: true
      editors:
      - Liangmingxin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9510661959648132
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
          fullname: Will.Liang
          isHf: false
          isPro: false
          name: Liangmingxin
          type: user
        html: '<p>I changed "eos_token":"&lt;|im_end|&gt;", back to "eos_token":"",
          it seems to be a bit better, but still occasionally outputs garbled text,
          why does the answerer''s gguf formatted model doesn''t output so much garble,
          but the vllm deployed one does?</p>

          '
        raw: I changed "eos_token":"<|im_end|>", back to "eos_token":"</s>", it seems
          to be a bit better, but still occasionally outputs garbled text, why does
          the answerer's gguf formatted model doesn't output so much garble, but the
          vllm deployed one does?
        updatedAt: '2024-01-11T16:11:15.894Z'
      numEdits: 1
      reactions: []
    id: 65a012b4db5d37ad5e59629e
    type: comment
  author: Liangmingxin
  content: I changed "eos_token":"<|im_end|>", back to "eos_token":"</s>", it seems
    to be a bit better, but still occasionally outputs garbled text, why does the
    answerer's gguf formatted model doesn't output so much garble, but the vllm deployed
    one does?
  created_at: 2024-01-11 16:09:24+00:00
  edited: true
  hidden: false
  id: 65a012b4db5d37ad5e59629e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-11T21:42:46.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9867309331893921
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: '<p>Sorry, I have no idea. I don''t know how vllm handles that. I would
          also try to copy the entire config from NeuralHermes but if that doesn''t
          work either... :(</p>

          '
        raw: Sorry, I have no idea. I don't know how vllm handles that. I would also
          try to copy the entire config from NeuralHermes but if that doesn't work
          either... :(
        updatedAt: '2024-01-11T21:42:46.661Z'
      numEdits: 0
      reactions: []
    id: 65a060d6b1f078835994ecce
    type: comment
  author: mlabonne
  content: Sorry, I have no idea. I don't know how vllm handles that. I would also
    try to copy the entire config from NeuralHermes but if that doesn't work either...
    :(
  created_at: 2024-01-11 21:42:46+00:00
  edited: false
  hidden: false
  id: 65a060d6b1f078835994ecce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-11T23:36:11.000Z'
    data:
      edited: true
      editors:
      - Liangmingxin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48254361748695374
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
          fullname: Will.Liang
          isHf: false
          isPro: false
          name: Liangmingxin
          type: user
        html: "<blockquote>\n<p>Sorry, I have no idea. I don't know how vllm handles\
          \ that. I would also try to copy the entire config from NeuralHermes but\
          \ if that doesn't work either... :(</p>\n</blockquote>\n<p>After a full\
          \ night of debugging and modifying vllm's deployment parameters, I think\
          \ it's much better! There are no strange special tokens now, but it still\
          \ occasionally outputs garbled code when it encounters rare issues, but\
          \ it's not too much of a problem anymore. I've posted my vllm deployment\
          \ below for your reference <span data-props=\"{&quot;user&quot;:&quot;mlabonne&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mlabonne\"\
          >@<span class=\"underline\">mlabonne</span></a></span>\n\n\t</span></span>\
          \ </p>\n<pre><code>python ./vllm/vllm/entrypoints/openai/api_server.py \\\
          \n--model './NeuralMarcoro14-7B' \\\n--tokenizer './NeuralMarcoro14-7B'\
          \ \\\n--tokenizer-mode auto \\\n--dtype float16 \\\n--enforce-eager \\\n\
          --tensor-parallel-size 2 \\\n--host 0.0.0.0 \\\n--port xxxx \\\n--trust-remote-code\
          \ \\\n--disable-log-stats \\\n--disable-log-requests\n</code></pre>\n<p>Within\
          \ vllm I set some default parameters</p>\n<pre><code>    default_min_p =\
          \ 0.05\n    default_use_beam_search = True\n    default_ignore_eos = False\n\
          \    default_skip_special_tokens = True\n</code></pre>\n<p>Then tokenizer_config.json\
          \ I copied <a href=\"https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B\"\
          >https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B</a>, It's mostly\
          \ unchanged, but I've changed here \"eos_token\": \"&lt; /s&gt;\"</p>\n\
          <pre><code>{\n  \"add_bos_token\": true,\n  \"add_eos_token\": false,\n\
          \  \"added_tokens_decoder\": {\n    \"0\": {\n      \"content\": \"&lt;unk&gt;\"\
          ,\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\"\
          : false,\n      \"single_word\": false,\n      \"special\": true\n    },\n\
          \    \"1\": {\n      \"content\": \"&lt;s&gt;\",\n      \"lstrip\": false,\n\
          \      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\"\
          : false,\n      \"special\": true\n    },\n    \"2\": {\n      \"content\"\
          : \"&lt;/s&gt;\",\n      \"lstrip\": false,\n      \"normalized\": false,\n\
          \      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\"\
          : true\n    },\n    \"32000\": {\n      \"content\": \"&lt;|im_end|&gt;\"\
          ,\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\"\
          : false,\n      \"single_word\": false,\n      \"special\": true\n    },\n\
          \    \"32001\": {\n      \"content\": \"&lt;|im_start|&gt;\",\n      \"\
          lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n\
          \      \"single_word\": false,\n      \"special\": true\n    }\n  },\n \
          \ \"additional_special_tokens\": [],\n  \"bos_token\": \"&lt;s&gt;\",\n\
          \  \"chat_template\": \"{% for message in messages %}{{'&lt;|im_start|&gt;'\
          \ + message['role'] + '\\n' + message['content'] + '&lt;|im_end|&gt;' +\
          \ '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '&lt;|im_start|&gt;assistant\\\
          n' }}{% endif %}\",\n  \"clean_up_tokenization_spaces\": false,\n  \"eos_token\"\
          : \"&lt;/s&gt;\",\n  \"legacy\": true,\n  \"model_max_length\": 1000000000000000019884624838656,\n\
          \  \"pad_token\": null,\n  \"sp_model_kwargs\": {},\n  \"spaces_between_special_tokens\"\
          : false,\n  \"tokenizer_class\": \"LlamaTokenizer\",\n  \"trust_remote_code\"\
          : false,\n  \"unk_token\": \"&lt;unk&gt;\",\n  \"use_default_system_prompt\"\
          : true,\n  \"use_fast\": true\n}\n</code></pre>\n<p>Really like this model\
          \ of yours! You're invited to try it as I deployed it with vllm (link is\
          \ still the one above).</p>\n"
        raw: "> Sorry, I have no idea. I don't know how vllm handles that. I would\
          \ also try to copy the entire config from NeuralHermes but if that doesn't\
          \ work either... :(\n\nAfter a full night of debugging and modifying vllm's\
          \ deployment parameters, I think it's much better! There are no strange\
          \ special tokens now, but it still occasionally outputs garbled code when\
          \ it encounters rare issues, but it's not too much of a problem anymore.\
          \ I've posted my vllm deployment below for your reference @mlabonne \n```\n\
          python ./vllm/vllm/entrypoints/openai/api_server.py \\\n--model './NeuralMarcoro14-7B'\
          \ \\\n--tokenizer './NeuralMarcoro14-7B' \\\n--tokenizer-mode auto \\\n\
          --dtype float16 \\\n--enforce-eager \\\n--tensor-parallel-size 2 \\\n--host\
          \ 0.0.0.0 \\\n--port xxxx \\\n--trust-remote-code \\\n--disable-log-stats\
          \ \\\n--disable-log-requests\n```\nWithin vllm I set some default parameters\n\
          ```\n    default_min_p = 0.05\n    default_use_beam_search = True\n    default_ignore_eos\
          \ = False\n    default_skip_special_tokens = True\n```\nThen tokenizer_config.json\
          \ I copied https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B,\
          \ It's mostly unchanged, but I've changed here \"eos_token\": \"< /s>\"\n\
          ```\n{\n  \"add_bos_token\": true,\n  \"add_eos_token\": false,\n  \"added_tokens_decoder\"\
          : {\n    \"0\": {\n      \"content\": \"<unk>\",\n      \"lstrip\": false,\n\
          \      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\"\
          : false,\n      \"special\": true\n    },\n    \"1\": {\n      \"content\"\
          : \"<s>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n   \
          \   \"rstrip\": false,\n      \"single_word\": false,\n      \"special\"\
          : true\n    },\n    \"2\": {\n      \"content\": \"</s>\",\n      \"lstrip\"\
          : false,\n      \"normalized\": false,\n      \"rstrip\": false,\n     \
          \ \"single_word\": false,\n      \"special\": true\n    },\n    \"32000\"\
          : {\n      \"content\": \"<|im_end|>\",\n      \"lstrip\": false,\n    \
          \  \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\"\
          : false,\n      \"special\": true\n    },\n    \"32001\": {\n      \"content\"\
          : \"<|im_start|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n\
          \      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\"\
          : true\n    }\n  },\n  \"additional_special_tokens\": [],\n  \"bos_token\"\
          : \"<s>\",\n  \"chat_template\": \"{% for message in messages %}{{'<|im_start|>'\
          \ + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{%\
          \ endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{%\
          \ endif %}\",\n  \"clean_up_tokenization_spaces\": false,\n  \"eos_token\"\
          : \"</s>\",\n  \"legacy\": true,\n  \"model_max_length\": 1000000000000000019884624838656,\n\
          \  \"pad_token\": null,\n  \"sp_model_kwargs\": {},\n  \"spaces_between_special_tokens\"\
          : false,\n  \"tokenizer_class\": \"LlamaTokenizer\",\n  \"trust_remote_code\"\
          : false,\n  \"unk_token\": \"<unk>\",\n  \"use_default_system_prompt\":\
          \ true,\n  \"use_fast\": true\n}\n```\nReally like this model of yours!\
          \ You're invited to try it as I deployed it with vllm (link is still the\
          \ one above)."
        updatedAt: '2024-01-11T23:44:15.825Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - mlabonne
    id: 65a07b6b628cd4ce2f18f5e7
    type: comment
  author: Liangmingxin
  content: "> Sorry, I have no idea. I don't know how vllm handles that. I would also\
    \ try to copy the entire config from NeuralHermes but if that doesn't work either...\
    \ :(\n\nAfter a full night of debugging and modifying vllm's deployment parameters,\
    \ I think it's much better! There are no strange special tokens now, but it still\
    \ occasionally outputs garbled code when it encounters rare issues, but it's not\
    \ too much of a problem anymore. I've posted my vllm deployment below for your\
    \ reference @mlabonne \n```\npython ./vllm/vllm/entrypoints/openai/api_server.py\
    \ \\\n--model './NeuralMarcoro14-7B' \\\n--tokenizer './NeuralMarcoro14-7B' \\\
    \n--tokenizer-mode auto \\\n--dtype float16 \\\n--enforce-eager \\\n--tensor-parallel-size\
    \ 2 \\\n--host 0.0.0.0 \\\n--port xxxx \\\n--trust-remote-code \\\n--disable-log-stats\
    \ \\\n--disable-log-requests\n```\nWithin vllm I set some default parameters\n\
    ```\n    default_min_p = 0.05\n    default_use_beam_search = True\n    default_ignore_eos\
    \ = False\n    default_skip_special_tokens = True\n```\nThen tokenizer_config.json\
    \ I copied https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B, It's mostly\
    \ unchanged, but I've changed here \"eos_token\": \"< /s>\"\n```\n{\n  \"add_bos_token\"\
    : true,\n  \"add_eos_token\": false,\n  \"added_tokens_decoder\": {\n    \"0\"\
    : {\n      \"content\": \"<unk>\",\n      \"lstrip\": false,\n      \"normalized\"\
    : false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\"\
    : true\n    },\n    \"1\": {\n      \"content\": \"<s>\",\n      \"lstrip\": false,\n\
    \      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\"\
    : false,\n      \"special\": true\n    },\n    \"2\": {\n      \"content\": \"\
    </s>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\"\
    : false,\n      \"single_word\": false,\n      \"special\": true\n    },\n   \
    \ \"32000\": {\n      \"content\": \"<|im_end|>\",\n      \"lstrip\": false,\n\
    \      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\"\
    : false,\n      \"special\": true\n    },\n    \"32001\": {\n      \"content\"\
    : \"<|im_start|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n\
    \      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n\
    \    }\n  },\n  \"additional_special_tokens\": [],\n  \"bos_token\": \"<s>\",\n\
    \  \"chat_template\": \"{% for message in messages %}{{'<|im_start|>' + message['role']\
    \ + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt\
    \ %}{{ '<|im_start|>assistant\\n' }}{% endif %}\",\n  \"clean_up_tokenization_spaces\"\
    : false,\n  \"eos_token\": \"</s>\",\n  \"legacy\": true,\n  \"model_max_length\"\
    : 1000000000000000019884624838656,\n  \"pad_token\": null,\n  \"sp_model_kwargs\"\
    : {},\n  \"spaces_between_special_tokens\": false,\n  \"tokenizer_class\": \"\
    LlamaTokenizer\",\n  \"trust_remote_code\": false,\n  \"unk_token\": \"<unk>\"\
    ,\n  \"use_default_system_prompt\": true,\n  \"use_fast\": true\n}\n```\nReally\
    \ like this model of yours! You're invited to try it as I deployed it with vllm\
    \ (link is still the one above)."
  created_at: 2024-01-11 23:36:11+00:00
  edited: true
  hidden: false
  id: 65a07b6b628cd4ce2f18f5e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-12T00:20:48.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9447313547134399
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: '<p>Excellent, thank you for providing all these details! It''ll be
          useful as a reference in the future.</p>

          '
        raw: Excellent, thank you for providing all these details! It'll be useful
          as a reference in the future.
        updatedAt: '2024-01-12T00:20:48.829Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Liangmingxin
    id: 65a085e0b90a1eb50fd0e06b
    type: comment
  author: mlabonne
  content: Excellent, thank you for providing all these details! It'll be useful as
    a reference in the future.
  created_at: 2024-01-12 00:20:48+00:00
  edited: false
  hidden: false
  id: 65a085e0b90a1eb50fd0e06b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8e5eac5d74efbc971eaea20f912a17eb.svg
      fullname: Nicolas Aguirre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Giskard
      type: user
    createdAt: '2024-01-13T17:35:17.000Z'
    data:
      edited: false
      editors:
      - Giskard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8816104531288147
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8e5eac5d74efbc971eaea20f912a17eb.svg
          fullname: Nicolas Aguirre
          isHf: false
          isPro: false
          name: Giskard
          type: user
        html: '<p>Maybe this is something similar to what was commented in here in
          the secind parqgraph?</p>

          <p><a href="https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser#training">https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser#training</a></p>

          '
        raw: 'Maybe this is something similar to what was commented in here in the
          secind parqgraph?


          https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser#training


          '
        updatedAt: '2024-01-13T17:35:17.709Z'
      numEdits: 0
      reactions: []
    id: 65a2c9d56e52f833405ee388
    type: comment
  author: Giskard
  content: 'Maybe this is something similar to what was commented in here in the secind
    parqgraph?


    https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser#training


    '
  created_at: 2024-01-13 17:35:17+00:00
  edited: false
  hidden: false
  id: 65a2c9d56e52f833405ee388
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: mlabonne/NeuralMarcoro14-7B
repo_type: model
status: open
target_branch: null
title: Some suggestions for chat_template
