!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CordobaIA
conflicting_files: null
created_at: 2023-10-19 20:08:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fe0fe836b711d74d2244c88972c79264.svg
      fullname: Desarrollos IA
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CordobaIA
      type: user
    createdAt: '2023-10-19T21:08:49.000Z'
    data:
      edited: false
      editors:
      - CordobaIA
      hidden: false
      identifiedLanguage:
        language: es
        probability: 0.5585023164749146
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fe0fe836b711d74d2244c88972c79264.svg
          fullname: Desarrollos IA
          isHf: false
          isPro: false
          name: CordobaIA
          type: user
        html: "<p>Buenas estoy trabajando con el modelo y quiero hacer un fine-tune.\
          \ estoy trabajando en colab con los recursos gratuitos (detallo esto xq\
          \ puede ser relevante para mi consulta). El tema es que al querer hacer\
          \ el fine-tune me da error en una libreria.<br>Pero nose a donde acudir\
          \ por soporte.<br>Este es el codigo que tengo para ahcer el fine-tune</p>\n\
          <pre><code class=\"language-pyton\">from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM, TrainingArguments, Trainer \nfrom datasets import\
          \ load_dataset\n\n# Cargar el modelo base pre-entrenado\nmodel_name = \"\
          flax-community/gpt-2-spanish\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          flax-community/gpt-2-spanish\", bos_token='&lt;|startoftext|&gt;', eos_token='&lt;|endoftext|&gt;',\
          \ pad_token='&lt;|pad|&gt;')\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          flax-community/gpt-2-spanish\")\n\n# Cargar tu conjunto de datos de entrenamiento\
          \ etiquetado\n# Debe estar en un formato adecuado para el fine-tuning\n\
          dataset = load_dataset(\"json\", data_files=\"dataset_finetune_codigolaboral.json\"\
          )\ntrain_dataset = dataset[\"train\"]\n\n# Configurar los argumentos de\
          \ entrenamiento\ntraining_args = TrainingArguments(\n   output_dir=\"./output\"\
          ,\n   overwrite_output_dir=False,  # Cambia a False si quieres continuar\
          \ el entrenamiento\n   num_train_epochs=3,  # N\xFAmero de \xE9pocas de\
          \ entrenamiento\n   per_device_train_batch_size=4,\n   save_steps=1000,\n\
          \   save_total_limit=2,\n   evaluation_strategy=\"steps\",\n   eval_steps=1000,\n\
          \   logging_steps=100,\n)\n\n# Configurar el Trainer para el fine-tuning\n\
          trainer = Trainer(\n   model=model,\n   args=training_args,\n   train_dataset=train_dataset,\
          \  # Utiliza el conjunto de datos de entrenamiento\n)\n\n# Iniciar el fine-tuning\n\
          trainer.train()\n\n# Guardar el modelo fine-tuned\ntrainer.save_model(\"\
          ./fine_tuned_model01_laboral\")\n</code></pre>\n<p>y este es el error \"\
          </p>\n<pre><code>ImportError                               Traceback (most\
          \ recent call last)\n&lt;ipython-input-22-0dd9751fcd2b&gt; in &lt;cell line:\
          \ 17&gt;()\n     15 \n     16 # Configurar los argumentos de entrenamiento\n\
          ---&gt; 17 training_args = TrainingArguments(\n     18     output_dir=\"\
          ./output\",\n     19     overwrite_output_dir=False,  # Cambia a False si\
          \ quieres continuar el entrenamient\n\nImportError: Using the `Trainer`\
          \ with `PyTorch` requires `accelerate&gt;=0.20.1`: Please run `pip install\
          \ transformers[torch]` or `pip install accelerate -U`\n</code></pre>\n<p>Instale\
          \ el acceletate, baje la vercion, luego la mas nueva pero no hay caso.</p>\n\
          <p>Alguien me puede ayudar, con este tema, tiene algun codigo con el que\
          \ pudieron realizar el fine-tune? toda informacion me sirve.</p>\n<p>Lo\
          \ escribo en espa\xF1ol porque... es mi idioma \U0001F60A</p>\n"
        raw: "Buenas estoy trabajando con el modelo y quiero hacer un fine-tune. estoy\
          \ trabajando en colab con los recursos gratuitos (detallo esto xq puede\
          \ ser relevante para mi consulta). El tema es que al querer hacer el fine-tune\
          \ me da error en una libreria.\r\nPero nose a donde acudir por soporte.\r\
          \nEste es el codigo que tengo para ahcer el fine-tune\r\n ``` pyton\r\n\
          from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments,\
          \ Trainer \r\nfrom datasets import load_dataset\r\n\r\n# Cargar el modelo\
          \ base pre-entrenado\r\nmodel_name = \"flax-community/gpt-2-spanish\"\r\n\
          \r\ntokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt-2-spanish\"\
          , bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\r\
          \n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"flax-community/gpt-2-spanish\"\
          )\r\n\r\n# Cargar tu conjunto de datos de entrenamiento etiquetado\r\n#\
          \ Debe estar en un formato adecuado para el fine-tuning\r\ndataset = load_dataset(\"\
          json\", data_files=\"dataset_finetune_codigolaboral.json\")\r\ntrain_dataset\
          \ = dataset[\"train\"]\r\n\r\n# Configurar los argumentos de entrenamiento\r\
          \ntraining_args = TrainingArguments(\r\n    output_dir=\"./output\",\r\n\
          \    overwrite_output_dir=False,  # Cambia a False si quieres continuar\
          \ el entrenamiento\r\n    num_train_epochs=3,  # N\xFAmero de \xE9pocas\
          \ de entrenamiento\r\n    per_device_train_batch_size=4,\r\n    save_steps=1000,\r\
          \n    save_total_limit=2,\r\n    evaluation_strategy=\"steps\",\r\n    eval_steps=1000,\r\
          \n    logging_steps=100,\r\n)\r\n\r\n# Configurar el Trainer para el fine-tuning\r\
          \ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n\
          \    train_dataset=train_dataset,  # Utiliza el conjunto de datos de entrenamiento\r\
          \n)\r\n\r\n# Iniciar el fine-tuning\r\ntrainer.train()\r\n\r\n# Guardar\
          \ el modelo fine-tuned\r\ntrainer.save_model(\"./fine_tuned_model01_laboral\"\
          )\r\n```\r\n\r\ny este es el error \"\r\n```\r\nImportError            \
          \                   Traceback (most recent call last)\r\n<ipython-input-22-0dd9751fcd2b>\
          \ in <cell line: 17>()\r\n     15 \r\n     16 # Configurar los argumentos\
          \ de entrenamiento\r\n---> 17 training_args = TrainingArguments(\r\n   \
          \  18     output_dir=\"./output\",\r\n     19     overwrite_output_dir=False,\
          \  # Cambia a False si quieres continuar el entrenamient\r\n\r\nImportError:\
          \ Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please\
          \ run `pip install transformers[torch]` or `pip install accelerate -U`\r\
          \n```\r\nInstale el acceletate, baje la vercion, luego la mas nueva pero\
          \ no hay caso.\r\n\r\nAlguien me puede ayudar, con este tema, tiene algun\
          \ codigo con el que pudieron realizar el fine-tune? toda informacion me\
          \ sirve.\r\n\r\nLo escribo en espa\xF1ol porque... es mi idioma \U0001F60A"
        updatedAt: '2023-10-19T21:08:49.143Z'
      numEdits: 0
      reactions: []
    id: 65319ae15bc71a02d6665dbe
    type: comment
  author: CordobaIA
  content: "Buenas estoy trabajando con el modelo y quiero hacer un fine-tune. estoy\
    \ trabajando en colab con los recursos gratuitos (detallo esto xq puede ser relevante\
    \ para mi consulta). El tema es que al querer hacer el fine-tune me da error en\
    \ una libreria.\r\nPero nose a donde acudir por soporte.\r\nEste es el codigo\
    \ que tengo para ahcer el fine-tune\r\n ``` pyton\r\nfrom transformers import\
    \ AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer \r\nfrom datasets\
    \ import load_dataset\r\n\r\n# Cargar el modelo base pre-entrenado\r\nmodel_name\
    \ = \"flax-community/gpt-2-spanish\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    flax-community/gpt-2-spanish\", bos_token='<|startoftext|>', eos_token='<|endoftext|>',\
    \ pad_token='<|pad|>')\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    flax-community/gpt-2-spanish\")\r\n\r\n# Cargar tu conjunto de datos de entrenamiento\
    \ etiquetado\r\n# Debe estar en un formato adecuado para el fine-tuning\r\ndataset\
    \ = load_dataset(\"json\", data_files=\"dataset_finetune_codigolaboral.json\"\
    )\r\ntrain_dataset = dataset[\"train\"]\r\n\r\n# Configurar los argumentos de\
    \ entrenamiento\r\ntraining_args = TrainingArguments(\r\n    output_dir=\"./output\"\
    ,\r\n    overwrite_output_dir=False,  # Cambia a False si quieres continuar el\
    \ entrenamiento\r\n    num_train_epochs=3,  # N\xFAmero de \xE9pocas de entrenamiento\r\
    \n    per_device_train_batch_size=4,\r\n    save_steps=1000,\r\n    save_total_limit=2,\r\
    \n    evaluation_strategy=\"steps\",\r\n    eval_steps=1000,\r\n    logging_steps=100,\r\
    \n)\r\n\r\n# Configurar el Trainer para el fine-tuning\r\ntrainer = Trainer(\r\
    \n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\
    \  # Utiliza el conjunto de datos de entrenamiento\r\n)\r\n\r\n# Iniciar el fine-tuning\r\
    \ntrainer.train()\r\n\r\n# Guardar el modelo fine-tuned\r\ntrainer.save_model(\"\
    ./fine_tuned_model01_laboral\")\r\n```\r\n\r\ny este es el error \"\r\n```\r\n\
    ImportError                               Traceback (most recent call last)\r\n\
    <ipython-input-22-0dd9751fcd2b> in <cell line: 17>()\r\n     15 \r\n     16 #\
    \ Configurar los argumentos de entrenamiento\r\n---> 17 training_args = TrainingArguments(\r\
    \n     18     output_dir=\"./output\",\r\n     19     overwrite_output_dir=False,\
    \  # Cambia a False si quieres continuar el entrenamient\r\n\r\nImportError: Using\
    \ the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip\
    \ install transformers[torch]` or `pip install accelerate -U`\r\n```\r\nInstale\
    \ el acceletate, baje la vercion, luego la mas nueva pero no hay caso.\r\n\r\n\
    Alguien me puede ayudar, con este tema, tiene algun codigo con el que pudieron\
    \ realizar el fine-tune? toda informacion me sirve.\r\n\r\nLo escribo en espa\xF1\
    ol porque... es mi idioma \U0001F60A"
  created_at: 2023-10-19 20:08:49+00:00
  edited: false
  hidden: false
  id: 65319ae15bc71a02d6665dbe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: flax-community/gpt-2-spanish
repo_type: model
status: open
target_branch: null
title: Fine-Tune
