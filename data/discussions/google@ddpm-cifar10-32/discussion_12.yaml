!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nakajimayoshi
conflicting_files: null
created_at: 2023-05-26 03:30:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/adb565ba5692ee14d6b6ce8eb3d9394d.svg
      fullname: Yoshi Nakajima
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nakajimayoshi
      type: user
    createdAt: '2023-05-26T04:30:21.000Z'
    data:
      edited: true
      editors:
      - nakajimayoshi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/adb565ba5692ee14d6b6ce8eb3d9394d.svg
          fullname: Yoshi Nakajima
          isHf: false
          isPro: false
          name: nakajimayoshi
          type: user
        html: "<p>Hello, I am working on training a model based on the official training\
          \ example which can be located here: <a href=\"https://huggingface.co/nakajimayoshi/ddpm-iris-256/tree/main/\"\
          >https://huggingface.co/nakajimayoshi/ddpm-iris-256/tree/main/</a> </p>\n\
          <p>I was able to successfully train the model, and the training logs/samples\
          \ were successfully uploaded, but the model was neither saved in the runtime\
          \ as a .bin or .pth or pushed to my repository. I have made no modifications\
          \ to the training loop, only the training config and dataset loading pipeline.\
          \ You can see the modification of the training config below:</p>\n<pre><code\
          \ class=\"language-py\"><span class=\"hljs-keyword\">from</span> dataclasses\
          \ <span class=\"hljs-keyword\">import</span> dataclass\n\n<span class=\"\
          hljs-meta\">@dataclass</span>\n<span class=\"hljs-keyword\">class</span>\
          \ <span class=\"hljs-title class_\">TrainingConfig</span>:\n    image_size\
          \ = <span class=\"hljs-number\">256</span>  <span class=\"hljs-comment\"\
          ># the generated image resolution</span>\n    train_batch_size = <span class=\"\
          hljs-number\">16</span>\n    eval_batch_size = <span class=\"hljs-number\"\
          >16</span>  <span class=\"hljs-comment\"># how many images to sample during\
          \ evaluation</span>\n    num_epochs = <span class=\"hljs-number\">50</span>\n\
          \    gradient_accumulation_steps = <span class=\"hljs-number\">1</span>\n\
          \    learning_rate = <span class=\"hljs-number\">1e-4</span>\n    lr_warmup_steps\
          \ = <span class=\"hljs-number\">500</span>\n    save_image_epochs = <span\
          \ class=\"hljs-number\">10</span>\n    dataset_name= <span class=\"hljs-string\"\
          >'imagefolder'</span>\n    save_model_epochs = <span class=\"hljs-number\"\
          >30</span>\n    mixed_precision = <span class=\"hljs-string\">'fp16'</span>\
          \  <span class=\"hljs-comment\"># `no` for float32, `fp16` for automatic\
          \ mixed precision</span>\n    output_dir = <span class=\"hljs-string\">'ddpm-iris-256'</span>\
          \  <span class=\"hljs-comment\"># the model namy locally and on the HF Hub</span>\n\
          \n    push_to_hub = <span class=\"hljs-literal\">True</span>  <span class=\"\
          hljs-comment\"># whether to upload the saved model to the HF Hub</span>\n\
          \    hub_private_repo = <span class=\"hljs-literal\">False</span>\n    overwrite_output_dir\
          \ = <span class=\"hljs-literal\">True</span>  <span class=\"hljs-comment\"\
          ># overwrite the old model when re-running the notebook</span>\n    seed\
          \ = <span class=\"hljs-number\">0</span>\n\nconfig = TrainingConfig()\n\
          </code></pre>\n<p>On my repository, you can see the logs and samples were\
          \ uploaded, but none of the model checkpoints were uploaded nor can I find\
          \ them in my google colab notebook. Any help is appreciated. Thanks</p>\n"
        raw: "Hello, I am working on training a model based on the official training\
          \ example which can be located here: https://huggingface.co/nakajimayoshi/ddpm-iris-256/tree/main/\
          \ \n\nI was able to successfully train the model, and the training logs/samples\
          \ were successfully uploaded, but the model was neither saved in the runtime\
          \ as a .bin or .pth or pushed to my repository. I have made no modifications\
          \ to the training loop, only the training config and dataset loading pipeline.\
          \ You can see the modification of the training config below:\n\n```py\n\
          from dataclasses import dataclass\n\n@dataclass\nclass TrainingConfig:\n\
          \    image_size = 256  # the generated image resolution\n    train_batch_size\
          \ = 16\n    eval_batch_size = 16  # how many images to sample during evaluation\n\
          \    num_epochs = 50\n    gradient_accumulation_steps = 1\n    learning_rate\
          \ = 1e-4\n    lr_warmup_steps = 500\n    save_image_epochs = 10\n    dataset_name=\
          \ 'imagefolder'\n    save_model_epochs = 30\n    mixed_precision = 'fp16'\
          \  # `no` for float32, `fp16` for automatic mixed precision\n    output_dir\
          \ = 'ddpm-iris-256'  # the model namy locally and on the HF Hub\n\n    push_to_hub\
          \ = True  # whether to upload the saved model to the HF Hub\n    hub_private_repo\
          \ = False\n    overwrite_output_dir = True  # overwrite the old model when\
          \ re-running the notebook\n    seed = 0\n\nconfig = TrainingConfig()\n```\n\
          \nOn my repository, you can see the logs and samples were uploaded, but\
          \ none of the model checkpoints were uploaded nor can I find them in my\
          \ google colab notebook. Any help is appreciated. Thanks"
        updatedAt: '2023-05-28T04:19:17.608Z'
      numEdits: 3
      reactions: []
    id: 647035ddd742e9ef6522e089
    type: comment
  author: nakajimayoshi
  content: "Hello, I am working on training a model based on the official training\
    \ example which can be located here: https://huggingface.co/nakajimayoshi/ddpm-iris-256/tree/main/\
    \ \n\nI was able to successfully train the model, and the training logs/samples\
    \ were successfully uploaded, but the model was neither saved in the runtime as\
    \ a .bin or .pth or pushed to my repository. I have made no modifications to the\
    \ training loop, only the training config and dataset loading pipeline. You can\
    \ see the modification of the training config below:\n\n```py\nfrom dataclasses\
    \ import dataclass\n\n@dataclass\nclass TrainingConfig:\n    image_size = 256\
    \  # the generated image resolution\n    train_batch_size = 16\n    eval_batch_size\
    \ = 16  # how many images to sample during evaluation\n    num_epochs = 50\n \
    \   gradient_accumulation_steps = 1\n    learning_rate = 1e-4\n    lr_warmup_steps\
    \ = 500\n    save_image_epochs = 10\n    dataset_name= 'imagefolder'\n    save_model_epochs\
    \ = 30\n    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic\
    \ mixed precision\n    output_dir = 'ddpm-iris-256'  # the model namy locally\
    \ and on the HF Hub\n\n    push_to_hub = True  # whether to upload the saved model\
    \ to the HF Hub\n    hub_private_repo = False\n    overwrite_output_dir = True\
    \  # overwrite the old model when re-running the notebook\n    seed = 0\n\nconfig\
    \ = TrainingConfig()\n```\n\nOn my repository, you can see the logs and samples\
    \ were uploaded, but none of the model checkpoints were uploaded nor can I find\
    \ them in my google colab notebook. Any help is appreciated. Thanks"
  created_at: 2023-05-26 03:30:21+00:00
  edited: true
  hidden: false
  id: 647035ddd742e9ef6522e089
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/adb565ba5692ee14d6b6ce8eb3d9394d.svg
      fullname: Yoshi Nakajima
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nakajimayoshi
      type: user
    createdAt: '2023-05-26T05:43:32.000Z'
    data:
      edited: true
      editors:
      - nakajimayoshi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/adb565ba5692ee14d6b6ce8eb3d9394d.svg
          fullname: Yoshi Nakajima
          isHf: false
          isPro: false
          name: nakajimayoshi
          type: user
        html: "<p>I have found a work around for this issue:<br>The issue is in the\
          \ training loop:</p>\n<pre><code class=\"language-py\"> <span class=\"hljs-keyword\"\
          >if</span> accelerator.is_main_process:\n            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model),\
          \ scheduler=noise_scheduler)\n\n            <span class=\"hljs-keyword\"\
          >if</span> (epoch + <span class=\"hljs-number\">1</span>) % config.save_image_epochs\
          \ == <span class=\"hljs-number\">0</span> <span class=\"hljs-keyword\">or</span>\
          \ epoch == config.num_epochs - <span class=\"hljs-number\">1</span>:\n \
          \               evaluate(config, epoch, pipeline)\n\n            <span class=\"\
          hljs-keyword\">if</span> (epoch + <span class=\"hljs-number\">1</span>)\
          \ % config.save_model_epochs == <span class=\"hljs-number\">0</span> <span\
          \ class=\"hljs-keyword\">or</span> epoch == config.num_epochs - <span class=\"\
          hljs-number\">1</span>:\n                <span class=\"hljs-keyword\">if</span>\
          \ config.push_to_hub:\n                    repo.push_to_hub(commit_message=<span\
          \ class=\"hljs-string\">f\"Epoch <span class=\"hljs-subst\">{epoch}</span>\"\
          </span>, blocking=<span class=\"hljs-literal\">True</span>)\n          \
          \      <span class=\"hljs-keyword\">else</span>:\n                    pipeline.save_pretrained(config.output_dir)\
          \ <span class=\"hljs-comment\"># this never gets called</span>\n</code></pre>\n\
          <p>For one reason or another, the 'else' condition is not being reached,\
          \ therefore pipline.save_pretrained(config.output_dir) never gets called.\
          \ I solved this by simply moving the method call out of the else statement\
          \ and saving it on every epoch:</p>\n<pre><code class=\"language-py\"> <span\
          \ class=\"hljs-keyword\">if</span> accelerator.is_main_process:\n      \
          \      pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n\
          \            pipeline.save_pretrained(config.output_dir) <span class=\"\
          hljs-comment\"># move to here</span>\n            <span class=\"hljs-keyword\"\
          >if</span> (epoch + <span class=\"hljs-number\">1</span>) % config.save_image_epochs\
          \ == <span class=\"hljs-number\">0</span> <span class=\"hljs-keyword\">or</span>\
          \ epoch == config.num_epochs - <span class=\"hljs-number\">1</span>:\n \
          \               evaluate(config, epoch, pipeline)\n\n            <span class=\"\
          hljs-keyword\">if</span> (epoch + <span class=\"hljs-number\">1</span>)\
          \ % config.save_model_epochs == <span class=\"hljs-number\">0</span> <span\
          \ class=\"hljs-keyword\">or</span> epoch == config.num_epochs - <span class=\"\
          hljs-number\">1</span>:\n                <span class=\"hljs-keyword\">if</span>\
          \ config.push_to_hub:\n                    repo.push_to_hub(commit_message=<span\
          \ class=\"hljs-string\">f\"Epoch <span class=\"hljs-subst\">{epoch}</span>\"\
          </span>, blocking=<span class=\"hljs-literal\">True</span>)\n          \
          \      <span class=\"hljs-keyword\">else</span>:\n                    <span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">'saving..'</span>)\
          \ <span class=\"hljs-comment\"># replaced with print to see if it gets called</span>\n\
          </code></pre>\n<p>note I could have easily just removed the entire nested\
          \ if statement and have it push to hub, but to prevent any unexpected behaviors\
          \ I left it as is, and only moved the method call.<br>This slows down the\
          \ training speed but at the very least the model doesn't get lost.</p>\n"
        raw: "I have found a work around for this issue:\nThe issue is in the training\
          \ loop:\n```py\n if accelerator.is_main_process:\n            pipeline =\
          \ DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n\
          \n            if (epoch + 1) % config.save_image_epochs == 0 or epoch ==\
          \ config.num_epochs - 1:\n                evaluate(config, epoch, pipeline)\n\
          \n            if (epoch + 1) % config.save_model_epochs == 0 or epoch ==\
          \ config.num_epochs - 1:\n                if config.push_to_hub:\n     \
          \               repo.push_to_hub(commit_message=f\"Epoch {epoch}\", blocking=True)\n\
          \                else:\n                    pipeline.save_pretrained(config.output_dir)\
          \ # this never gets called\n```\n\nFor one reason or another, the 'else'\
          \ condition is not being reached, therefore pipline.save_pretrained(config.output_dir)\
          \ never gets called. I solved this by simply moving the method call out\
          \ of the else statement and saving it on every epoch:\n\n\n```py\n if accelerator.is_main_process:\n\
          \            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model),\
          \ scheduler=noise_scheduler)\n            pipeline.save_pretrained(config.output_dir)\
          \ # move to here\n            if (epoch + 1) % config.save_image_epochs\
          \ == 0 or epoch == config.num_epochs - 1:\n                evaluate(config,\
          \ epoch, pipeline)\n\n            if (epoch + 1) % config.save_model_epochs\
          \ == 0 or epoch == config.num_epochs - 1:\n                if config.push_to_hub:\n\
          \                    repo.push_to_hub(commit_message=f\"Epoch {epoch}\"\
          , blocking=True)\n                else:\n                    print('saving..')\
          \ # replaced with print to see if it gets called\n```\n\nnote I could have\
          \ easily just removed the entire nested if statement and have it push to\
          \ hub, but to prevent any unexpected behaviors I left it as is, and only\
          \ moved the method call.\nThis slows down the training speed but at the\
          \ very least the model doesn't get lost."
        updatedAt: '2023-05-26T05:46:10.473Z'
      numEdits: 2
      reactions: []
    id: 64704704150f4cab863903be
    type: comment
  author: nakajimayoshi
  content: "I have found a work around for this issue:\nThe issue is in the training\
    \ loop:\n```py\n if accelerator.is_main_process:\n            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model),\
    \ scheduler=noise_scheduler)\n\n            if (epoch + 1) % config.save_image_epochs\
    \ == 0 or epoch == config.num_epochs - 1:\n                evaluate(config, epoch,\
    \ pipeline)\n\n            if (epoch + 1) % config.save_model_epochs == 0 or epoch\
    \ == config.num_epochs - 1:\n                if config.push_to_hub:\n        \
    \            repo.push_to_hub(commit_message=f\"Epoch {epoch}\", blocking=True)\n\
    \                else:\n                    pipeline.save_pretrained(config.output_dir)\
    \ # this never gets called\n```\n\nFor one reason or another, the 'else' condition\
    \ is not being reached, therefore pipline.save_pretrained(config.output_dir) never\
    \ gets called. I solved this by simply moving the method call out of the else\
    \ statement and saving it on every epoch:\n\n\n```py\n if accelerator.is_main_process:\n\
    \            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n\
    \            pipeline.save_pretrained(config.output_dir) # move to here\n    \
    \        if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs\
    \ - 1:\n                evaluate(config, epoch, pipeline)\n\n            if (epoch\
    \ + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n  \
    \              if config.push_to_hub:\n                    repo.push_to_hub(commit_message=f\"\
    Epoch {epoch}\", blocking=True)\n                else:\n                    print('saving..')\
    \ # replaced with print to see if it gets called\n```\n\nnote I could have easily\
    \ just removed the entire nested if statement and have it push to hub, but to\
    \ prevent any unexpected behaviors I left it as is, and only moved the method\
    \ call.\nThis slows down the training speed but at the very least the model doesn't\
    \ get lost."
  created_at: 2023-05-26 04:43:32+00:00
  edited: true
  hidden: false
  id: 64704704150f4cab863903be
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: google/ddpm-cifar10-32
repo_type: model
status: open
target_branch: null
title: Using official training example, model was neither saved nor pushed to repo
