!!python/object:huggingface_hub.community.DiscussionWithDetails
author: PierreColombo
conflicting_files: null
created_at: 2024-01-03 19:55:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2d5dac3d92757ed48e37e126a3464a3.svg
      fullname: Colombo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PierreColombo
      type: user
    createdAt: '2024-01-03T19:55:58.000Z'
    data:
      edited: true
      editors:
      - PierreColombo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.774379551410675
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2d5dac3d92757ed48e37e126a3464a3.svg
          fullname: Colombo
          isHf: false
          isPro: false
          name: PierreColombo
          type: user
        html: "<p>Hellooo :)<br>Thanks for the model :)<br>Is this broken?  </p>\n\
          <pre><code> size mismatch for model.layers.31.block_sparse_moe.experts.5.w3.weight:\
          \ copying a param with shape torch.Size([29360128, 1]) from checkpoint,\
          \ the shape in current model is torch.Size([14336, 4096]).\n    size mismatch\
          \ for model.layers.31.block_sparse_moe.experts.6.w1.weight: copying a param\
          \ with shape torch.Size([29360128, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([14336, 4096]).\n    size mismatch for model.layers.31.block_sparse_moe.experts.6.w2.weight:\
          \ copying a param with shape torch.Size([29360128, 1]) from checkpoint,\
          \ the shape in current model is torch.Size([4096, 14336]).\n    size mismatch\
          \ for model.layers.31.block_sparse_moe.experts.6.w3.weight: copying a param\
          \ with shape torch.Size([29360128, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([14336, 4096]).\n    size mismatch for model.layers.31.block_sparse_moe.experts.7.w1.weight:\
          \ copying a param with shape torch.Size([29360128, 1]) from checkpoint,\
          \ the shape in current model is torch.Size([14336, 4096]).\n    size mismatch\
          \ for model.layers.31.block_sparse_moe.experts.7.w2.weight: copying a param\
          \ with shape torch.Size([29360128, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([4096, 14336]).\n    size mismatch for model.layers.31.block_sparse_moe.experts.7.w3.weight:\
          \ copying a param with shape torch.Size([29360128, 1]) from checkpoint,\
          \ the shape in current model is torch.Size([14336, 4096]).\n    You may\
          \ consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained`\
          \ method.\n</code></pre>\n"
        raw: "Hellooo :)\nThanks for the model :)\nIs this broken?  \n\n     size\
          \ mismatch for model.layers.31.block_sparse_moe.experts.5.w3.weight: copying\
          \ a param with shape torch.Size([29360128, 1]) from checkpoint, the shape\
          \ in current model is torch.Size([14336, 4096]).\n        size mismatch\
          \ for model.layers.31.block_sparse_moe.experts.6.w1.weight: copying a param\
          \ with shape torch.Size([29360128, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([14336, 4096]).\n        size mismatch for model.layers.31.block_sparse_moe.experts.6.w2.weight:\
          \ copying a param with shape torch.Size([29360128, 1]) from checkpoint,\
          \ the shape in current model is torch.Size([4096, 14336]).\n        size\
          \ mismatch for model.layers.31.block_sparse_moe.experts.6.w3.weight: copying\
          \ a param with shape torch.Size([29360128, 1]) from checkpoint, the shape\
          \ in current model is torch.Size([14336, 4096]).\n        size mismatch\
          \ for model.layers.31.block_sparse_moe.experts.7.w1.weight: copying a param\
          \ with shape torch.Size([29360128, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([14336, 4096]).\n        size mismatch for model.layers.31.block_sparse_moe.experts.7.w2.weight:\
          \ copying a param with shape torch.Size([29360128, 1]) from checkpoint,\
          \ the shape in current model is torch.Size([4096, 14336]).\n        size\
          \ mismatch for model.layers.31.block_sparse_moe.experts.7.w3.weight: copying\
          \ a param with shape torch.Size([29360128, 1]) from checkpoint, the shape\
          \ in current model is torch.Size([14336, 4096]).\n        You may consider\
          \ adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
        updatedAt: '2024-01-04T09:15:50.184Z'
      numEdits: 1
      reactions: []
    id: 6595bbcec41c73bcfe0c232c
    type: comment
  author: PierreColombo
  content: "Hellooo :)\nThanks for the model :)\nIs this broken?  \n\n     size mismatch\
    \ for model.layers.31.block_sparse_moe.experts.5.w3.weight: copying a param with\
    \ shape torch.Size([29360128, 1]) from checkpoint, the shape in current model\
    \ is torch.Size([14336, 4096]).\n        size mismatch for model.layers.31.block_sparse_moe.experts.6.w1.weight:\
    \ copying a param with shape torch.Size([29360128, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([14336, 4096]).\n        size mismatch for model.layers.31.block_sparse_moe.experts.6.w2.weight:\
    \ copying a param with shape torch.Size([29360128, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 14336]).\n        size mismatch for model.layers.31.block_sparse_moe.experts.6.w3.weight:\
    \ copying a param with shape torch.Size([29360128, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([14336, 4096]).\n        size mismatch for model.layers.31.block_sparse_moe.experts.7.w1.weight:\
    \ copying a param with shape torch.Size([29360128, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([14336, 4096]).\n        size mismatch for model.layers.31.block_sparse_moe.experts.7.w2.weight:\
    \ copying a param with shape torch.Size([29360128, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([4096, 14336]).\n        size mismatch for model.layers.31.block_sparse_moe.experts.7.w3.weight:\
    \ copying a param with shape torch.Size([29360128, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([14336, 4096]).\n        You may consider adding\
    \ `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
  created_at: 2024-01-03 19:55:58+00:00
  edited: true
  hidden: false
  id: 6595bbcec41c73bcfe0c232c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64eb3ab2345cf9d8c4c45e1b/tHiMZO80fYai_o3dAANDh.jpeg?w=200&h=200&f=face
      fullname: Mohamed Achraf Miftah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: miftahmoha
      type: user
    createdAt: '2024-01-05T20:49:22.000Z'
    data:
      edited: true
      editors:
      - miftahmoha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7193989753723145
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64eb3ab2345cf9d8c4c45e1b/tHiMZO80fYai_o3dAANDh.jpeg?w=200&h=200&f=face
          fullname: Mohamed Achraf Miftah
          isHf: false
          isPro: false
          name: miftahmoha
          type: user
        html: '<p>Hi Pierre,</p>

          <p>You should make sure to have the latest version of bitsandbytes and transformers
          installed from source:</p>

          <p>pip install -U bitsandbytes<br>pip install -U git+<a rel="nofollow" href="https://github.com/huggingface/transformers.git">https://github.com/huggingface/transformers.git</a></p>

          '
        raw: 'Hi Pierre,


          You should make sure to have the latest version of bitsandbytes and transformers
          installed from source:


          pip install -U bitsandbytes

          pip install -U git+https://github.com/huggingface/transformers.git


          '
        updatedAt: '2024-01-05T20:53:22.583Z'
      numEdits: 2
      reactions: []
    id: 65986b5211b48706ba411545
    type: comment
  author: miftahmoha
  content: 'Hi Pierre,


    You should make sure to have the latest version of bitsandbytes and transformers
    installed from source:


    pip install -U bitsandbytes

    pip install -U git+https://github.com/huggingface/transformers.git


    '
  created_at: 2024-01-05 20:49:22+00:00
  edited: true
  hidden: false
  id: 65986b5211b48706ba411545
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2d5dac3d92757ed48e37e126a3464a3.svg
      fullname: Colombo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PierreColombo
      type: user
    createdAt: '2024-01-12T12:23:27.000Z'
    data:
      edited: false
      editors:
      - PierreColombo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.14711041748523712
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2d5dac3d92757ed48e37e126a3464a3.svg
          fullname: Colombo
          isHf: false
          isPro: false
          name: PierreColombo
          type: user
        html: '<p>:) Thankss :)) brilliant !</p>

          '
        raw: ':) Thankss :)) brilliant !

          '
        updatedAt: '2024-01-12T12:23:27.900Z'
      numEdits: 0
      reactions: []
    id: 65a12f3f1754e2f2119dda8f
    type: comment
  author: PierreColombo
  content: ':) Thankss :)) brilliant !

    '
  created_at: 2024-01-12 12:23:27+00:00
  edited: false
  hidden: false
  id: 65a12f3f1754e2f2119dda8f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ybelkada/Mixtral-8x7B-Instruct-v0.1-bnb-4bit
repo_type: model
status: open
target_branch: null
title: This is broken ?
