!!python/object:huggingface_hub.community.DiscussionWithDetails
author: amartinr
conflicting_files: null
created_at: 2023-04-09 20:00:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6428965aeb320ead3d2cadbb/pzEFzfwrX2TzyddnUGQti.png?w=200&h=200&f=face
      fullname: "Abel Mart\xEDn"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: amartinr
      type: user
    createdAt: '2023-04-09T21:00:38.000Z'
    data:
      edited: false
      editors:
      - amartinr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6428965aeb320ead3d2cadbb/pzEFzfwrX2TzyddnUGQti.png?w=200&h=200&f=face
          fullname: "Abel Mart\xEDn"
          isHf: false
          isPro: false
          name: amartinr
          type: user
        html: '<p>I think both models come from the same model from chavinlo, but
          wanted to know what are their main differences.<br><a href="https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/tree/main/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g">anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g</a></p>

          <p>Great work, BTW!</p>

          '
        raw: "I think both models come from the same model from chavinlo, but wanted\
          \ to know what are their main differences.\r\n[anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g](https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/tree/main/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g)\r\
          \n\r\nGreat work, BTW!"
        updatedAt: '2023-04-09T21:00:38.364Z'
      numEdits: 0
      reactions: []
    id: 6433277607bad11484ad1553
    type: comment
  author: amartinr
  content: "I think both models come from the same model from chavinlo, but wanted\
    \ to know what are their main differences.\r\n[anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g](https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/tree/main/gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g)\r\
    \n\r\nGreat work, BTW!"
  created_at: 2023-04-09 20:00:38+00:00
  edited: false
  hidden: false
  id: 6433277607bad11484ad1553
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2023-04-10T16:35:18.000Z'
    data:
      edited: false
      editors:
      - Henk717
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
          fullname: Henky!!
          isHf: false
          isPro: false
          name: Henk717
          type: user
        html: '<p>This is for CPU based solutions such as <a rel="nofollow" href="https://koboldai.org/cpp">https://koboldai.org/cpp</a>
          and the other one is for GPTQ.<br>Performance is similar.</p>

          '
        raw: 'This is for CPU based solutions such as https://koboldai.org/cpp and
          the other one is for GPTQ.

          Performance is similar.'
        updatedAt: '2023-04-10T16:35:18.827Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - amartinr
    id: 64343ac65408e9c12afc4852
    type: comment
  author: Henk717
  content: 'This is for CPU based solutions such as https://koboldai.org/cpp and the
    other one is for GPTQ.

    Performance is similar.'
  created_at: 2023-04-10 15:35:18+00:00
  edited: false
  hidden: false
  id: 64343ac65408e9c12afc4852
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
      fullname: Pi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Pi3141
      type: user
    createdAt: '2023-04-23T21:24:02.000Z'
    data:
      edited: false
      editors:
      - Pi3141
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
          fullname: Pi
          isHf: false
          isPro: false
          name: Pi3141
          type: user
        html: '<p>This one was converted to GGML from the original GPT4 x Alpaca 13B
          model, then quantized. The one by anon was quantized with GPTQ, then converted
          to GGML after. Mine should most likely give slightly better results due
          to less data loss. But I''m not quite sure, especially given that mine is
          a bit smaller.</p>

          '
        raw: This one was converted to GGML from the original GPT4 x Alpaca 13B model,
          then quantized. The one by anon was quantized with GPTQ, then converted
          to GGML after. Mine should most likely give slightly better results due
          to less data loss. But I'm not quite sure, especially given that mine is
          a bit smaller.
        updatedAt: '2023-04-23T21:24:02.514Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - amartinr
    id: 6445a1f253ecc52f50f5aa0d
    type: comment
  author: Pi3141
  content: This one was converted to GGML from the original GPT4 x Alpaca 13B model,
    then quantized. The one by anon was quantized with GPTQ, then converted to GGML
    after. Mine should most likely give slightly better results due to less data loss.
    But I'm not quite sure, especially given that mine is a bit smaller.
  created_at: 2023-04-23 20:24:02+00:00
  edited: false
  hidden: false
  id: 6445a1f253ecc52f50f5aa0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/36ad491930893f97b1c19c9cd9d00a87.svg
      fullname: A ANDRE
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yugihu
      type: user
    createdAt: '2023-04-24T21:22:39.000Z'
    data:
      edited: false
      editors:
      - yugihu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/36ad491930893f97b1c19c9cd9d00a87.svg
          fullname: A ANDRE
          isHf: false
          isPro: false
          name: yugihu
          type: user
        html: '<p>i like the one here better (my opinion)</p>

          '
        raw: i like the one here better (my opinion)
        updatedAt: '2023-04-24T21:22:39.639Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Pi3141
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Pi3141
    id: 6446f31ff9dc06bea2a9f2f8
    type: comment
  author: yugihu
  content: i like the one here better (my opinion)
  created_at: 2023-04-24 20:22:39+00:00
  edited: false
  hidden: false
  id: 6446f31ff9dc06bea2a9f2f8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Pi3141/gpt4-x-alpaca-native-13B-ggml
repo_type: model
status: open
target_branch: null
title: How does this model compare to anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g?
