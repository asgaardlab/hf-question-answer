!!python/object:huggingface_hub.community.DiscussionWithDetails
author: radfaraf
conflicting_files: null
created_at: 2023-01-02 21:54:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
      fullname: Robert W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: radfaraf
      type: user
    createdAt: '2023-01-02T21:54:18.000Z'
    data:
      edited: false
      editors:
      - radfaraf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
          fullname: Robert W
          isHf: false
          isPro: false
          name: radfaraf
          type: user
        html: '<p>I tried converting the .ckpt files from here:</p>

          <p><a rel="nofollow" href="https://civitai.com/models/1254/elldreths-dream-mix">https://civitai.com/models/1254/elldreths-dream-mix</a>
          (I used the All settings)<br>AND<br><a rel="nofollow" href="https://civitai.com/models/2968/wlop">https://civitai.com/models/2968/wlop</a>
          (I used the CPU and NE setting only just to try something different)</p>

          <p>Waited about 20 minutes of processing and eventually see a folder created
          for them to output the files, but then it errors and no files are created:<br>Screenshot
          showing error:<br><a rel="nofollow" href="https://prnt.sc/GAgfHFaZgPL4">https://prnt.sc/GAgfHFaZgPL4</a></p>

          '
        raw: "I tried converting the .ckpt files from here:\r\n\r\nhttps://civitai.com/models/1254/elldreths-dream-mix\
          \ (I used the All settings)\r\nAND\r\nhttps://civitai.com/models/2968/wlop\
          \ (I used the CPU and NE setting only just to try something different)\r\
          \n\r\nWaited about 20 minutes of processing and eventually see a folder\
          \ created for them to output the files, but then it errors and no files\
          \ are created:\r\nScreenshot showing error:\r\nhttps://prnt.sc/GAgfHFaZgPL4"
        updatedAt: '2023-01-02T21:54:18.065Z'
      numEdits: 0
      reactions: []
    id: 63b3528a18e5cf2cdd3b6813
    type: comment
  author: radfaraf
  content: "I tried converting the .ckpt files from here:\r\n\r\nhttps://civitai.com/models/1254/elldreths-dream-mix\
    \ (I used the All settings)\r\nAND\r\nhttps://civitai.com/models/2968/wlop (I\
    \ used the CPU and NE setting only just to try something different)\r\n\r\nWaited\
    \ about 20 minutes of processing and eventually see a folder created for them\
    \ to output the files, but then it errors and no files are created:\r\nScreenshot\
    \ showing error:\r\nhttps://prnt.sc/GAgfHFaZgPL4"
  created_at: 2023-01-02 21:54:18+00:00
  edited: false
  hidden: false
  id: 63b3528a18e5cf2cdd3b6813
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672782907557-6313af200b24eab4746e7d34.jpeg?w=200&h=200&f=face
      fullname: Guillermo Cique
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: GuiyeC
      type: user
    createdAt: '2023-01-02T21:58:48.000Z'
    data:
      edited: false
      editors:
      - GuiyeC
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672782907557-6313af200b24eab4746e7d34.jpeg?w=200&h=200&f=face
          fullname: Guillermo Cique
          isHf: false
          isPro: false
          name: GuiyeC
          type: user
        html: '<p>Thank you for the info, I''ll take a look and I should probably
          add the error message on that alert.</p>

          '
        raw: Thank you for the info, I'll take a look and I should probably add the
          error message on that alert.
        updatedAt: '2023-01-02T21:58:48.930Z'
      numEdits: 0
      reactions: []
    id: 63b353980dddc8f717f901e3
    type: comment
  author: GuiyeC
  content: Thank you for the info, I'll take a look and I should probably add the
    error message on that alert.
  created_at: 2023-01-02 21:58:48+00:00
  edited: false
  hidden: false
  id: 63b353980dddc8f717f901e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
      fullname: Robert W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: radfaraf
      type: user
    createdAt: '2023-01-02T22:01:23.000Z'
    data:
      edited: false
      editors:
      - radfaraf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
          fullname: Robert W
          isHf: false
          isPro: false
          name: radfaraf
          type: user
        html: '<p>Also strange I noticed after I start the conversion the program
          opens a second copy of itself after a short while, is that on purpose?</p>

          '
        raw: Also strange I noticed after I start the conversion the program opens
          a second copy of itself after a short while, is that on purpose?
        updatedAt: '2023-01-02T22:01:23.774Z'
      numEdits: 0
      reactions: []
    id: 63b354336c3e6df5c38dfb1d
    type: comment
  author: radfaraf
  content: Also strange I noticed after I start the conversion the program opens a
    second copy of itself after a short while, is that on purpose?
  created_at: 2023-01-02 22:01:23+00:00
  edited: false
  hidden: false
  id: 63b354336c3e6df5c38dfb1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672782907557-6313af200b24eab4746e7d34.jpeg?w=200&h=200&f=face
      fullname: Guillermo Cique
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: GuiyeC
      type: user
    createdAt: '2023-01-02T22:05:55.000Z'
    data:
      edited: false
      editors:
      - GuiyeC
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672782907557-6313af200b24eab4746e7d34.jpeg?w=200&h=200&f=face
          fullname: Guillermo Cique
          isHf: false
          isPro: false
          name: GuiyeC
          type: user
        html: '<p>Not on purpose, it should only launch one instance but I noticed
          the same happened to me, this is my first PyInstaller app so I''ll have
          to work on it a bit more, I''m trying to convert the models you shared,
          might take a while but if you want to test with better output you can right
          click the app "Show Package Contents" then go to Contents -&gt; MacOS and
          double click the "Guernika Model Converter" there, it should launch with
          a terminal window showing all the output of the python scripts, could be
          helpful.</p>

          '
        raw: Not on purpose, it should only launch one instance but I noticed the
          same happened to me, this is my first PyInstaller app so I'll have to work
          on it a bit more, I'm trying to convert the models you shared, might take
          a while but if you want to test with better output you can right click the
          app "Show Package Contents" then go to Contents -> MacOS and double click
          the "Guernika Model Converter" there, it should launch with a terminal window
          showing all the output of the python scripts, could be helpful.
        updatedAt: '2023-01-02T22:05:55.640Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Billyballo
    id: 63b355430dddc8f717f91809
    type: comment
  author: GuiyeC
  content: Not on purpose, it should only launch one instance but I noticed the same
    happened to me, this is my first PyInstaller app so I'll have to work on it a
    bit more, I'm trying to convert the models you shared, might take a while but
    if you want to test with better output you can right click the app "Show Package
    Contents" then go to Contents -> MacOS and double click the "Guernika Model Converter"
    there, it should launch with a terminal window showing all the output of the python
    scripts, could be helpful.
  created_at: 2023-01-02 22:05:55+00:00
  edited: false
  hidden: false
  id: 63b355430dddc8f717f91809
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
      fullname: Robert W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: radfaraf
      type: user
    createdAt: '2023-01-02T23:27:02.000Z'
    data:
      edited: false
      editors:
      - radfaraf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
          fullname: Robert W
          isHf: false
          isPro: false
          name: radfaraf
          type: user
        html: "<p>Here's a log:</p>\n<p>Torch version 1.13.0 has not been tested with\
          \ coremltools. You may run into unexpected errors. Torch 1.12.1 is the most\
          \ recent version that has been tested.<br>2023-01-02 18:05:04.438 Guernika\
          \ Model Converter[1645:21862] +[CATransaction synchronize] called within\
          \ transaction<br>2023-01-02 18:05:04.702 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction<br>2023-01-02 18:05:34.758\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction<br>2023-01-02 18:05:39.160 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction<br>2023-01-02 18:05:47.889\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction<br>2023-01-02 18:05:53.117 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction<br>2023-01-02 18:06:03.700\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction<br>2023-01-02 18:06:36.963 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction<br>2023-01-02 18:06:46.755\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction<br>2023-01-02 18:07:14.505 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction<br>2023-01-02 18:07:15.689\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction<br>2023-01-02 18:07:15.796 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction<br>2023-01-02 18:07:15.899\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction<br>2023-01-02 18:07:16.005 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction<br>2023-01-02 18:07:16.040\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction<br>2023-01-02 18:07:16.690 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction<br>2023-01-02 18:07:39.022\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction<br>2023-01-02 18:07:43.987 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction<br>2023-01-02 18:07:47.804\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction<br>2023-01-02 18:07:52.819 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction<br>2023-01-02 18:08:22.899\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction<br>2023-01-02 18:08:37.131 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction<br>INFO:python_coreml_stable_diffusion.torch2coreml:Initializing\
          \ StableDiffusionPipeline from /Volumes/2TB SSD/Stable-diffusion/elldrethSDreamMix_v10.ckpt..<br>\
          \  % Total    % Received % Xferd  Average Speed   Time    Time     Time\
          \  Current<br>                                 Dload  Upload   Total   Spent\
          \    Left  Speed<br>100  1873  100  1873    0     0   5501      0 --:--:--\
          \ --:--:-- --:--:--  5557<br>Some weights of the model checkpoint at openai/clip-vit-large-patch14\
          \ were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.5.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight',\
          \ 'visual_projection.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias',\
          \ 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight',\
          \ 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight',\
          \ 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.20.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.embeddings.position_embedding.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight',\
          \ 'text_projection.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight',\
          \ 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight',\
          \ 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.23.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.20.layer_norm1.bias', 'logit_scale', 'vision_model.encoder.layers.5.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.post_layernorm.weight',\
          \ 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight',\
          \ 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.2.mlp.fc1.weight']</p>\n<ul>\n<li>This IS\
          \ expected if you are initializing CLIPTextModel from the checkpoint of\
          \ a model trained on another task or with another architecture (e.g. initializing\
          \ a BertForSequenceClassification model from a BertForPreTraining model).</li>\n\
          <li>This IS NOT expected if you are initializing CLIPTextModel from the\
          \ checkpoint of a model that you expect to be exactly identical (initializing\
          \ a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).<br>diffusers/utils/deprecation_utils.py:35: FutureWarning: The\
          \ configuration file of this scheduler: PNDMScheduler {<br>\"_class_name\"\
          : \"PNDMScheduler\",<br>\"_diffusers_version\": \"0.9.0\",<br>\"beta_end\"\
          : 0.012,<br>\"beta_schedule\": \"scaled_linear\",<br>\"beta_start\": 0.00085,<br>\"\
          num_train_timesteps\": 1000,<br>\"set_alpha_to_one\": false,<br>\"skip_prk_steps\"\
          : true,<br>\"steps_offset\": 0,<br>\"trained_betas\": null<br>}<br> is outdated.\
          \ <code>steps_offset</code> should be set to 1 instead of 0. Please make\
          \ sure to update the config accordingly as leaving <code>steps_offset</code>\
          \ might led to incorrect results in future versions. If you have downloaded\
          \ this checkpoint from the Hugging Face Hub, it would be very nice if you\
          \ could open a Pull request for the <code>scheduler/scheduler_config.json</code>\
          \ file<br>INFO:python_coreml_stable_diffusion.torch2coreml:Done.<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ vae_decoder<br>diffusers/models/resnet.py:109: TracerWarning: Converting\
          \ a tensor to a Python boolean might cause the trace to be incorrect. We\
          \ can't record the data flow of Python values, so this value will be treated\
          \ as a constant in the future. This means that the trace might not generalize\
          \ to other inputs!<br>diffusers/models/resnet.py:122: TracerWarning: Converting\
          \ a tensor to a Python boolean might cause the trace to be incorrect. We\
          \ can't record the data flow of Python values, so this value will be treated\
          \ as a constant in the future. This means that the trace might not generalize\
          \ to other inputs!<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ vae_decoder to CoreML..<br>Converting PyTorch Frontend ==&gt; MIL Ops:\
          \   0%|                                                                \
          \                                                                      \
          \       | 0/353 [00:00&lt;?, ? ops/s]WARNING:python_coreml_stable_diffusion.torch2coreml:Casted\
          \ the <code>beta</code>(value=0.0) argument of <code>baddbmm</code> op from\
          \ int32 to float32 dtype for conversion!<br>Converting PyTorch Frontend\
          \ ==&gt; MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258B\
          | 352/353 [00:00&lt;00:00, 2370.68 ops/s]<br>Running MIL Common passes:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 39/39 [00:00&lt;00:00, 79.24 passes/s]<br>Running MIL FP16ComputePrecision\
          \ pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.87 passes/s]<br>Running\
          \ MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 11/11 [00:02&lt;00:00,  5.23 passes/s]<br>Torch version 1.13.0\
          \ has not been tested with coremltools. You may run into unexpected errors.\
          \ Torch 1.12.1 is the most recent version that has been tested.<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ vae_decoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_vae_decoder.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ vae_decoder into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_vae_decoder.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converted\
          \ vae_decoder<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ vae_encoder<br>diffusers/models/resnet.py:182: TracerWarning: Converting\
          \ a tensor to a Python boolean might cause the trace to be incorrect. We\
          \ can't record the data flow of Python values, so this value will be treated\
          \ as a constant in the future. This means that the trace might not generalize\
          \ to other inputs!<br>diffusers/models/resnet.py:187: TracerWarning: Converting\
          \ a tensor to a Python boolean might cause the trace to be incorrect. We\
          \ can't record the data flow of Python values, so this value will be treated\
          \ as a constant in the future. This means that the trace might not generalize\
          \ to other inputs!<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ vae_encoder to CoreML..<br>Converting PyTorch Frontend ==&gt; MIL Ops:\
          \   0%|                                                                \
          \                                                                      \
          \       | 0/281 [00:00&lt;?, ? ops/s]WARNING:python_coreml_stable_diffusion.torch2coreml:Casted\
          \ the <code>beta</code>(value=0.0) argument of <code>baddbmm</code> op from\
          \ int32 to float32 dtype for conversion!<br>Converting PyTorch Frontend\
          \ ==&gt; MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258C\
          | 280/281 [00:00&lt;00:00, 3289.24 ops/s]<br>Running MIL Common passes:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 39/39 [00:00&lt;00:00, 132.70 passes/s]<br>Running MIL FP16ComputePrecision\
          \ pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  3.05 passes/s]<br>Running\
          \ MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 11/11 [00:01&lt;00:00,  9.79 passes/s]<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ vae_encoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_vae_encoder.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ vae_encoder into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_vae_encoder.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converted\
          \ vae_encoder<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ unet<br>INFO:python_coreml_stable_diffusion.torch2coreml:Attention implementation\
          \ in effect: AttentionImplementations.SPLIT_EINSUM<br>INFO:python_coreml_stable_diffusion.torch2coreml:Sample\
          \ inputs spec: {'sample': (torch.Size([2, 4, 64, 64]), torch.float32), 'timestep':\
          \ (torch.Size([2]), torch.float32), 'encoder_hidden_states': (torch.Size([2,\
          \ 768, 1, 77]), torch.float32)}<br>INFO:python_coreml_stable_diffusion.torch2coreml:JIT\
          \ tracing..<br>python_coreml_stable_diffusion/layer_norm.py:61: TracerWarning:\
          \ Converting a tensor to a Python boolean might cause the trace to be incorrect.\
          \ We can't record the data flow of Python values, so this value will be\
          \ treated as a constant in the future. This means that the trace might not\
          \ generalize to other inputs!<br>INFO:python_coreml_stable_diffusion.torch2coreml:Done.<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ unet to CoreML..<br>WARNING:coremltools:Tuple detected at graph output.\
          \ This will be flattened in the converted model.<br>Converting PyTorch Frontend\
          \ ==&gt; MIL Ops:   0%|                                                \
          \                                                                      \
          \                      | 0/7876 [00:00&lt;?, ? ops/s]WARNING:coremltools:Saving\
          \ value type of int64 into a builtin type of int32, might lose precision!<br>Converting\
          \ PyTorch Frontend ==&gt; MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2589| 7874/7876 [00:01&lt;00:00, 5789.89 ops/s]<br>Running MIL Common\
          \ passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 39/39 [00:07&lt;00:00,  4.93 passes/s]<br>Running MIL FP16ComputePrecision\
          \ pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 1/1 [00:22&lt;00:00, 22.21s/ passes]<br>Running\
          \ MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 11/11 [02:38&lt;00:00, 14.43s/ passes]<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ unet model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_unet.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ unet into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_unet.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converted\
          \ unet<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting text_encoder<br>INFO:python_coreml_stable_diffusion.torch2coreml:Sample\
          \ inputs spec: {'input_ids': (torch.Size([1, 77]), torch.float32)}<br>INFO:python_coreml_stable_diffusion.torch2coreml:JIT\
          \ tracing text_encoder..<br>transformers/models/clip/modeling_clip.py:280:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!<br>if attn_weights.size()\
          \ != (bsz * self.num_heads, tgt_len, src_len):<br>transformers/models/clip/modeling_clip.py:288:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!<br>if causal_attention_mask.size()\
          \ != (bsz, 1, tgt_len, src_len):<br>transformers/models/clip/modeling_clip.py:320:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!<br>if attn_output.size()\
          \ != (bsz * self.num_heads, tgt_len, self.head_dim):<br>INFO:python_coreml_stable_diffusion.torch2coreml:Done.<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ text_encoder to CoreML..<br>WARNING:coremltools:Tuple detected at graph\
          \ output. This will be flattened in the converted model.<br>Converting PyTorch\
          \ Frontend ==&gt; MIL Ops:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A   | 794/814\
          \ [00:00&lt;00:00, 4135.97 ops/s]WARNING:coremltools:Saving value type of\
          \ int64 into a builtin type of int32, might lose precision!<br>Converting\
          \ PyTorch Frontend ==&gt; MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u258B| 812/814 [00:00&lt;00:00, 3978.24 ops/s]<br>Running MIL\
          \ Common passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 39/39 [00:00&lt;00:00, 97.36 passes/s]<br>Running MIL\
          \ FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.18 passes/s]<br>Running\
          \ MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 11/11 [00:02&lt;00:00,  4.11 passes/s]<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ text_encoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_text_encoder.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ text_encoder into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_text_encoder.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converted\
          \ text_encoder<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ safety_checker<br>INFO:python_coreml_stable_diffusion.torch2coreml:Sample\
          \ inputs spec: {'clip_input': (torch.Size([1, 3, 224, 224]), torch.float32),\
          \ 'images': (torch.Size([1, 512, 512, 3]), torch.float32), 'adjustment':\
          \ (torch.Size([1]), torch.float32)}<br>INFO:python_coreml_stable_diffusion.torch2coreml:JIT\
          \ tracing..<br>INFO:python_coreml_stable_diffusion.torch2coreml:Done.<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ safety_checker to CoreML..<br>WARNING:coremltools:Tuple detected at graph\
          \ output. This will be flattened in the converted model.<br>Converting PyTorch\
          \ Frontend ==&gt; MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A\
          | 1476/1478 [00:00&lt;00:00, 4831.01 ops/s]<br>Running MIL Common passes:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 39/39 [00:01&lt;00:00, 35.42 passes/s]<br>Running MIL FP16ComputePrecision\
          \ pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 1/1 [00:02&lt;00:00,  2.24s/ passes]<br>Running\
          \ MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 11/11 [00:07&lt;00:00,  1.43 passes/s]<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ safety_checker model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_safety_checker.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converted\
          \ safety_checker<br>INFO:python_coreml_stable_diffusion.torch2coreml:Bundling\
          \ resources for Guernika<br>INFO:python_coreml_stable_diffusion.torch2coreml:Created\
          \ /Volumes/2TB SSD/@OUTPUT/elldrethSDreamMix_v10 for Guernika assets<br>INFO:python_coreml_stable_diffusion.torch2coreml:Compiling\
          \ /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_text_encoder.mlpackage<br>xcrun:\
          \ error: unable to find utility \"coremlcompiler\", not a developer tool\
          \ or in PATH</li>\n</ul>\n"
        raw: "Here's a log:\n\nTorch version 1.13.0 has not been tested with coremltools.\
          \ You may run into unexpected errors. Torch 1.12.1 is the most recent version\
          \ that has been tested.\n2023-01-02 18:05:04.438 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:05:04.702\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction\n2023-01-02 18:05:34.758 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:05:39.160\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction\n2023-01-02 18:05:47.889 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:05:53.117\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction\n2023-01-02 18:06:03.700 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:06:36.963\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction\n2023-01-02 18:06:46.755 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:07:14.505\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction\n2023-01-02 18:07:15.689 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:07:15.796\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction\n2023-01-02 18:07:15.899 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:07:16.005\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction\n2023-01-02 18:07:16.040 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:07:16.690\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction\n2023-01-02 18:07:39.022 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:07:43.987\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction\n2023-01-02 18:07:47.804 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:07:52.819\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction\n2023-01-02 18:08:22.899 Guernika Model Converter[1645:21862]\
          \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:08:37.131\
          \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called\
          \ within transaction\nINFO:python_coreml_stable_diffusion.torch2coreml:Initializing\
          \ StableDiffusionPipeline from /Volumes/2TB SSD/Stable-diffusion/elldrethSDreamMix_v10.ckpt..\n\
          \  % Total    % Received % Xferd  Average Speed   Time    Time     Time\
          \  Current\n                                 Dload  Upload   Total   Spent\
          \    Left  Speed\n100  1873  100  1873    0     0   5501      0 --:--:--\
          \ --:--:-- --:--:--  5557\nSome weights of the model checkpoint at openai/clip-vit-large-patch14\
          \ were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.5.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight',\
          \ 'visual_projection.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias',\
          \ 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight',\
          \ 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight',\
          \ 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.20.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.embeddings.position_embedding.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight',\
          \ 'text_projection.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight',\
          \ 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight',\
          \ 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.23.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.20.layer_norm1.bias', 'logit_scale', 'vision_model.encoder.layers.5.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.post_layernorm.weight',\
          \ 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight',\
          \ 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.2.mlp.fc1.weight']\n- This IS expected if\
          \ you are initializing CLIPTextModel from the checkpoint of a model trained\
          \ on another task or with another architecture (e.g. initializing a BertForSequenceClassification\
          \ model from a BertForPreTraining model).\n- This IS NOT expected if you\
          \ are initializing CLIPTextModel from the checkpoint of a model that you\
          \ expect to be exactly identical (initializing a BertForSequenceClassification\
          \ model from a BertForSequenceClassification model).\ndiffusers/utils/deprecation_utils.py:35:\
          \ FutureWarning: The configuration file of this scheduler: PNDMScheduler\
          \ {\n  \"_class_name\": \"PNDMScheduler\",\n  \"_diffusers_version\": \"\
          0.9.0\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\": \"scaled_linear\"\
          ,\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n  \"set_alpha_to_one\"\
          : false,\n  \"skip_prk_steps\": true,\n  \"steps_offset\": 0,\n  \"trained_betas\"\
          : null\n}\n is outdated. `steps_offset` should be set to 1 instead of 0.\
          \ Please make sure to update the config accordingly as leaving `steps_offset`\
          \ might led to incorrect results in future versions. If you have downloaded\
          \ this checkpoint from the Hugging Face Hub, it would be very nice if you\
          \ could open a Pull request for the `scheduler/scheduler_config.json` file\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Done.\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ vae_decoder\ndiffusers/models/resnet.py:109: TracerWarning: Converting\
          \ a tensor to a Python boolean might cause the trace to be incorrect. We\
          \ can't record the data flow of Python values, so this value will be treated\
          \ as a constant in the future. This means that the trace might not generalize\
          \ to other inputs!\ndiffusers/models/resnet.py:122: TracerWarning: Converting\
          \ a tensor to a Python boolean might cause the trace to be incorrect. We\
          \ can't record the data flow of Python values, so this value will be treated\
          \ as a constant in the future. This means that the trace might not generalize\
          \ to other inputs!\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ vae_decoder to CoreML..\nConverting PyTorch Frontend ==> MIL Ops:   0%|\
          \                                                                      \
          \                                                                      \
          \ | 0/353 [00:00<?, ? ops/s]WARNING:python_coreml_stable_diffusion.torch2coreml:Casted\
          \ the `beta`(value=0.0) argument of `baddbmm` op from int32 to float32 dtype\
          \ for conversion!\nConverting PyTorch Frontend ==> MIL Ops: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u258B| 352/353 [00:00<00:00, 2370.68\
          \ ops/s]\nRunning MIL Common passes: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:00<00:00, 79.24 passes/s]\n\
          Running MIL FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,\
          \  1.87 passes/s]\nRunning MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:02<00:00,  5.23 passes/s]\n\
          Torch version 1.13.0 has not been tested with coremltools. You may run into\
          \ unexpected errors. Torch 1.12.1 is the most recent version that has been\
          \ tested.\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved vae_decoder\
          \ model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_vae_decoder.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Saved vae_decoder into\
          \ /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_vae_decoder.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converted vae_decoder\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting vae_encoder\n\
          diffusers/models/resnet.py:182: TracerWarning: Converting a tensor to a\
          \ Python boolean might cause the trace to be incorrect. We can't record\
          \ the data flow of Python values, so this value will be treated as a constant\
          \ in the future. This means that the trace might not generalize to other\
          \ inputs!\ndiffusers/models/resnet.py:187: TracerWarning: Converting a tensor\
          \ to a Python boolean might cause the trace to be incorrect. We can't record\
          \ the data flow of Python values, so this value will be treated as a constant\
          \ in the future. This means that the trace might not generalize to other\
          \ inputs!\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting vae_encoder\
          \ to CoreML..\nConverting PyTorch Frontend ==> MIL Ops:   0%|          \
          \                                                                      \
          \                                                             | 0/281 [00:00<?,\
          \ ? ops/s]WARNING:python_coreml_stable_diffusion.torch2coreml:Casted the\
          \ `beta`(value=0.0) argument of `baddbmm` op from int32 to float32 dtype\
          \ for conversion!\nConverting PyTorch Frontend ==> MIL Ops: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u258C| 280/281 [00:00<00:00, 3289.24\
          \ ops/s]\nRunning MIL Common passes: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:00<00:00, 132.70 passes/s]\n\
          Running MIL FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,\
          \  3.05 passes/s]\nRunning MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:01<00:00,  9.79 passes/s]\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Saved vae_encoder model\
          \ to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_vae_encoder.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Saved vae_encoder into\
          \ /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_vae_encoder.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converted vae_encoder\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting unet\nINFO:python_coreml_stable_diffusion.torch2coreml:Attention\
          \ implementation in effect: AttentionImplementations.SPLIT_EINSUM\nINFO:python_coreml_stable_diffusion.torch2coreml:Sample\
          \ inputs spec: {'sample': (torch.Size([2, 4, 64, 64]), torch.float32), 'timestep':\
          \ (torch.Size([2]), torch.float32), 'encoder_hidden_states': (torch.Size([2,\
          \ 768, 1, 77]), torch.float32)}\nINFO:python_coreml_stable_diffusion.torch2coreml:JIT\
          \ tracing..\npython_coreml_stable_diffusion/layer_norm.py:61: TracerWarning:\
          \ Converting a tensor to a Python boolean might cause the trace to be incorrect.\
          \ We can't record the data flow of Python values, so this value will be\
          \ treated as a constant in the future. This means that the trace might not\
          \ generalize to other inputs!\nINFO:python_coreml_stable_diffusion.torch2coreml:Done.\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting unet to CoreML..\n\
          WARNING:coremltools:Tuple detected at graph output. This will be flattened\
          \ in the converted model.\nConverting PyTorch Frontend ==> MIL Ops:   0%|\
          \                                                                      \
          \                                                                      |\
          \ 0/7876 [00:00<?, ? ops/s]WARNING:coremltools:Saving value type of int64\
          \ into a builtin type of int32, might lose precision!\nConverting PyTorch\
          \ Frontend ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 7874/7876\
          \ [00:01<00:00, 5789.89 ops/s]\nRunning MIL Common passes: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:07<00:00,\
          \  4.93 passes/s]\nRunning MIL FP16ComputePrecision pass: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 1/1 [00:22<00:00, 22.21s/ passes]\nRunning MIL Clean up passes: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [02:38<00:00,\
          \ 14.43s/ passes]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ unet model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_unet.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Saved unet into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_unet.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converted unet\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ text_encoder\nINFO:python_coreml_stable_diffusion.torch2coreml:Sample\
          \ inputs spec: {'input_ids': (torch.Size([1, 77]), torch.float32)}\nINFO:python_coreml_stable_diffusion.torch2coreml:JIT\
          \ tracing text_encoder..\ntransformers/models/clip/modeling_clip.py:280:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!\n  if attn_weights.size()\
          \ != (bsz * self.num_heads, tgt_len, src_len):\ntransformers/models/clip/modeling_clip.py:288:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!\n  if causal_attention_mask.size()\
          \ != (bsz, 1, tgt_len, src_len):\ntransformers/models/clip/modeling_clip.py:320:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!\n  if attn_output.size()\
          \ != (bsz * self.num_heads, tgt_len, self.head_dim):\nINFO:python_coreml_stable_diffusion.torch2coreml:Done.\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting text_encoder\
          \ to CoreML..\nWARNING:coremltools:Tuple detected at graph output. This\
          \ will be flattened in the converted model.\nConverting PyTorch Frontend\
          \ ==> MIL Ops:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A   | 794/814 [00:00<00:00,\
          \ 4135.97 ops/s]WARNING:coremltools:Saving value type of int64 into a builtin\
          \ type of int32, might lose precision!\nConverting PyTorch Frontend ==>\
          \ MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258B| 812/814\
          \ [00:00<00:00, 3978.24 ops/s]\nRunning MIL Common passes: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:00<00:00,\
          \ 97.36 passes/s]\nRunning MIL FP16ComputePrecision pass: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 1/1 [00:00<00:00,  1.18 passes/s]\nRunning MIL Clean up passes: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:02<00:00,\
          \  4.11 passes/s]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ text_encoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_text_encoder.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Saved text_encoder into\
          \ /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_text_encoder.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converted text_encoder\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting safety_checker\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Sample inputs spec: {'clip_input':\
          \ (torch.Size([1, 3, 224, 224]), torch.float32), 'images': (torch.Size([1,\
          \ 512, 512, 3]), torch.float32), 'adjustment': (torch.Size([1]), torch.float32)}\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:JIT tracing..\nINFO:python_coreml_stable_diffusion.torch2coreml:Done.\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting safety_checker\
          \ to CoreML..\nWARNING:coremltools:Tuple detected at graph output. This\
          \ will be flattened in the converted model.\nConverting PyTorch Frontend\
          \ ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A| 1476/1478\
          \ [00:00<00:00, 4831.01 ops/s]\nRunning MIL Common passes: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:01<00:00,\
          \ 35.42 passes/s]\nRunning MIL FP16ComputePrecision pass: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 1/1 [00:02<00:00,  2.24s/ passes]\nRunning MIL Clean up passes: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:07<00:00,\
          \  1.43 passes/s]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ safety_checker model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_safety_checker.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converted safety_checker\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Bundling resources for\
          \ Guernika\nINFO:python_coreml_stable_diffusion.torch2coreml:Created /Volumes/2TB\
          \ SSD/@OUTPUT/elldrethSDreamMix_v10 for Guernika assets\nINFO:python_coreml_stable_diffusion.torch2coreml:Compiling\
          \ /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_text_encoder.mlpackage\n\
          xcrun: error: unable to find utility \"coremlcompiler\", not a developer\
          \ tool or in PATH"
        updatedAt: '2023-01-02T23:27:02.492Z'
      numEdits: 0
      reactions: []
    id: 63b368460dddc8f717fa1612
    type: comment
  author: radfaraf
  content: "Here's a log:\n\nTorch version 1.13.0 has not been tested with coremltools.\
    \ You may run into unexpected errors. Torch 1.12.1 is the most recent version\
    \ that has been tested.\n2023-01-02 18:05:04.438 Guernika Model Converter[1645:21862]\
    \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:05:04.702\
    \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called within\
    \ transaction\n2023-01-02 18:05:34.758 Guernika Model Converter[1645:21862] +[CATransaction\
    \ synchronize] called within transaction\n2023-01-02 18:05:39.160 Guernika Model\
    \ Converter[1645:21862] +[CATransaction synchronize] called within transaction\n\
    2023-01-02 18:05:47.889 Guernika Model Converter[1645:21862] +[CATransaction synchronize]\
    \ called within transaction\n2023-01-02 18:05:53.117 Guernika Model Converter[1645:21862]\
    \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:06:03.700\
    \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called within\
    \ transaction\n2023-01-02 18:06:36.963 Guernika Model Converter[1645:21862] +[CATransaction\
    \ synchronize] called within transaction\n2023-01-02 18:06:46.755 Guernika Model\
    \ Converter[1645:21862] +[CATransaction synchronize] called within transaction\n\
    2023-01-02 18:07:14.505 Guernika Model Converter[1645:21862] +[CATransaction synchronize]\
    \ called within transaction\n2023-01-02 18:07:15.689 Guernika Model Converter[1645:21862]\
    \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:07:15.796\
    \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called within\
    \ transaction\n2023-01-02 18:07:15.899 Guernika Model Converter[1645:21862] +[CATransaction\
    \ synchronize] called within transaction\n2023-01-02 18:07:16.005 Guernika Model\
    \ Converter[1645:21862] +[CATransaction synchronize] called within transaction\n\
    2023-01-02 18:07:16.040 Guernika Model Converter[1645:21862] +[CATransaction synchronize]\
    \ called within transaction\n2023-01-02 18:07:16.690 Guernika Model Converter[1645:21862]\
    \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:07:39.022\
    \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called within\
    \ transaction\n2023-01-02 18:07:43.987 Guernika Model Converter[1645:21862] +[CATransaction\
    \ synchronize] called within transaction\n2023-01-02 18:07:47.804 Guernika Model\
    \ Converter[1645:21862] +[CATransaction synchronize] called within transaction\n\
    2023-01-02 18:07:52.819 Guernika Model Converter[1645:21862] +[CATransaction synchronize]\
    \ called within transaction\n2023-01-02 18:08:22.899 Guernika Model Converter[1645:21862]\
    \ +[CATransaction synchronize] called within transaction\n2023-01-02 18:08:37.131\
    \ Guernika Model Converter[1645:21862] +[CATransaction synchronize] called within\
    \ transaction\nINFO:python_coreml_stable_diffusion.torch2coreml:Initializing StableDiffusionPipeline\
    \ from /Volumes/2TB SSD/Stable-diffusion/elldrethSDreamMix_v10.ckpt..\n  % Total\
    \    % Received % Xferd  Average Speed   Time    Time     Time  Current\n    \
    \                             Dload  Upload   Total   Spent    Left  Speed\n100\
    \  1873  100  1873    0     0   5501      0 --:--:-- --:--:-- --:--:--  5557\n\
    Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not\
    \ used when initializing CLIPTextModel: ['vision_model.encoder.layers.5.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight',\
    \ 'visual_projection.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias',\
    \ 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight',\
    \ 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight',\
    \ 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.20.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.embeddings.position_embedding.weight',\
    \ 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight',\
    \ 'text_projection.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight',\
    \ 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight',\
    \ 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.23.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.20.layer_norm1.bias', 'logit_scale', 'vision_model.encoder.layers.5.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.post_layernorm.weight',\
    \ 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight',\
    \ 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.2.mlp.fc1.weight']\n- This IS expected if you are\
    \ initializing CLIPTextModel from the checkpoint of a model trained on another\
    \ task or with another architecture (e.g. initializing a BertForSequenceClassification\
    \ model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing\
    \ CLIPTextModel from the checkpoint of a model that you expect to be exactly identical\
    \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
    \ model).\ndiffusers/utils/deprecation_utils.py:35: FutureWarning: The configuration\
    \ file of this scheduler: PNDMScheduler {\n  \"_class_name\": \"PNDMScheduler\"\
    ,\n  \"_diffusers_version\": \"0.9.0\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\"\
    : \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n\
    \  \"set_alpha_to_one\": false,\n  \"skip_prk_steps\": true,\n  \"steps_offset\"\
    : 0,\n  \"trained_betas\": null\n}\n is outdated. `steps_offset` should be set\
    \ to 1 instead of 0. Please make sure to update the config accordingly as leaving\
    \ `steps_offset` might led to incorrect results in future versions. If you have\
    \ downloaded this checkpoint from the Hugging Face Hub, it would be very nice\
    \ if you could open a Pull request for the `scheduler/scheduler_config.json` file\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Done.\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ vae_decoder\ndiffusers/models/resnet.py:109: TracerWarning: Converting a tensor\
    \ to a Python boolean might cause the trace to be incorrect. We can't record the\
    \ data flow of Python values, so this value will be treated as a constant in the\
    \ future. This means that the trace might not generalize to other inputs!\ndiffusers/models/resnet.py:122:\
    \ TracerWarning: Converting a tensor to a Python boolean might cause the trace\
    \ to be incorrect. We can't record the data flow of Python values, so this value\
    \ will be treated as a constant in the future. This means that the trace might\
    \ not generalize to other inputs!\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ vae_decoder to CoreML..\nConverting PyTorch Frontend ==> MIL Ops:   0%|    \
    \                                                                            \
    \                                                             | 0/353 [00:00<?,\
    \ ? ops/s]WARNING:python_coreml_stable_diffusion.torch2coreml:Casted the `beta`(value=0.0)\
    \ argument of `baddbmm` op from int32 to float32 dtype for conversion!\nConverting\
    \ PyTorch Frontend ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u258B| 352/353 [00:00<00:00, 2370.68 ops/s]\nRunning\
    \ MIL Common passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 39/39 [00:00<00:00, 79.24 passes/s]\nRunning MIL\
    \ FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.87\
    \ passes/s]\nRunning MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:02<00:00,  5.23 passes/s]\nTorch\
    \ version 1.13.0 has not been tested with coremltools. You may run into unexpected\
    \ errors. Torch 1.12.1 is the most recent version that has been tested.\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
    \ vae_decoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_vae_decoder.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Saved vae_decoder into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_vae_decoder.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Converted vae_decoder\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ vae_encoder\ndiffusers/models/resnet.py:182: TracerWarning: Converting a tensor\
    \ to a Python boolean might cause the trace to be incorrect. We can't record the\
    \ data flow of Python values, so this value will be treated as a constant in the\
    \ future. This means that the trace might not generalize to other inputs!\ndiffusers/models/resnet.py:187:\
    \ TracerWarning: Converting a tensor to a Python boolean might cause the trace\
    \ to be incorrect. We can't record the data flow of Python values, so this value\
    \ will be treated as a constant in the future. This means that the trace might\
    \ not generalize to other inputs!\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ vae_encoder to CoreML..\nConverting PyTorch Frontend ==> MIL Ops:   0%|    \
    \                                                                            \
    \                                                             | 0/281 [00:00<?,\
    \ ? ops/s]WARNING:python_coreml_stable_diffusion.torch2coreml:Casted the `beta`(value=0.0)\
    \ argument of `baddbmm` op from int32 to float32 dtype for conversion!\nConverting\
    \ PyTorch Frontend ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u258C| 280/281 [00:00<00:00, 3289.24 ops/s]\nRunning\
    \ MIL Common passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 39/39 [00:00<00:00, 132.70 passes/s]\nRunning MIL FP16ComputePrecision\
    \ pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.05 passes/s]\nRunning\
    \ MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 11/11 [00:01<00:00,  9.79 passes/s]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
    \ vae_encoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_vae_encoder.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Saved vae_encoder into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_vae_encoder.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Converted vae_encoder\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ unet\nINFO:python_coreml_stable_diffusion.torch2coreml:Attention implementation\
    \ in effect: AttentionImplementations.SPLIT_EINSUM\nINFO:python_coreml_stable_diffusion.torch2coreml:Sample\
    \ inputs spec: {'sample': (torch.Size([2, 4, 64, 64]), torch.float32), 'timestep':\
    \ (torch.Size([2]), torch.float32), 'encoder_hidden_states': (torch.Size([2, 768,\
    \ 1, 77]), torch.float32)}\nINFO:python_coreml_stable_diffusion.torch2coreml:JIT\
    \ tracing..\npython_coreml_stable_diffusion/layer_norm.py:61: TracerWarning: Converting\
    \ a tensor to a Python boolean might cause the trace to be incorrect. We can't\
    \ record the data flow of Python values, so this value will be treated as a constant\
    \ in the future. This means that the trace might not generalize to other inputs!\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Done.\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ unet to CoreML..\nWARNING:coremltools:Tuple detected at graph output. This will\
    \ be flattened in the converted model.\nConverting PyTorch Frontend ==> MIL Ops:\
    \   0%|                                                                      \
    \                                                                      | 0/7876\
    \ [00:00<?, ? ops/s]WARNING:coremltools:Saving value type of int64 into a builtin\
    \ type of int32, might lose precision!\nConverting PyTorch Frontend ==> MIL Ops:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 7874/7876\
    \ [00:01<00:00, 5789.89 ops/s]\nRunning MIL Common passes: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:07<00:00,\
    \  4.93 passes/s]\nRunning MIL FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 1/1 [00:22<00:00, 22.21s/ passes]\nRunning MIL Clean up passes: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [02:38<00:00,\
    \ 14.43s/ passes]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved unet\
    \ model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_unet.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Saved unet into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_unet.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Converted unet\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ text_encoder\nINFO:python_coreml_stable_diffusion.torch2coreml:Sample inputs\
    \ spec: {'input_ids': (torch.Size([1, 77]), torch.float32)}\nINFO:python_coreml_stable_diffusion.torch2coreml:JIT\
    \ tracing text_encoder..\ntransformers/models/clip/modeling_clip.py:280: TracerWarning:\
    \ Converting a tensor to a Python boolean might cause the trace to be incorrect.\
    \ We can't record the data flow of Python values, so this value will be treated\
    \ as a constant in the future. This means that the trace might not generalize\
    \ to other inputs!\n  if attn_weights.size() != (bsz * self.num_heads, tgt_len,\
    \ src_len):\ntransformers/models/clip/modeling_clip.py:288: TracerWarning: Converting\
    \ a tensor to a Python boolean might cause the trace to be incorrect. We can't\
    \ record the data flow of Python values, so this value will be treated as a constant\
    \ in the future. This means that the trace might not generalize to other inputs!\n\
    \  if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\ntransformers/models/clip/modeling_clip.py:320:\
    \ TracerWarning: Converting a tensor to a Python boolean might cause the trace\
    \ to be incorrect. We can't record the data flow of Python values, so this value\
    \ will be treated as a constant in the future. This means that the trace might\
    \ not generalize to other inputs!\n  if attn_output.size() != (bsz * self.num_heads,\
    \ tgt_len, self.head_dim):\nINFO:python_coreml_stable_diffusion.torch2coreml:Done.\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Converting text_encoder to CoreML..\n\
    WARNING:coremltools:Tuple detected at graph output. This will be flattened in\
    \ the converted model.\nConverting PyTorch Frontend ==> MIL Ops:  98%|\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u258A   | 794/814 [00:00<00:00, 4135.97 ops/s]WARNING:coremltools:Saving\
    \ value type of int64 into a builtin type of int32, might lose precision!\nConverting\
    \ PyTorch Frontend ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u258B| 812/814 [00:00<00:00, 3978.24 ops/s]\nRunning\
    \ MIL Common passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 39/39 [00:00<00:00, 97.36 passes/s]\nRunning MIL\
    \ FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.18\
    \ passes/s]\nRunning MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:02<00:00,  4.11 passes/s]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
    \ text_encoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_text_encoder.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Saved text_encoder into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_text_encoder.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Converted text_encoder\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ safety_checker\nINFO:python_coreml_stable_diffusion.torch2coreml:Sample inputs\
    \ spec: {'clip_input': (torch.Size([1, 3, 224, 224]), torch.float32), 'images':\
    \ (torch.Size([1, 512, 512, 3]), torch.float32), 'adjustment': (torch.Size([1]),\
    \ torch.float32)}\nINFO:python_coreml_stable_diffusion.torch2coreml:JIT tracing..\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Done.\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ safety_checker to CoreML..\nWARNING:coremltools:Tuple detected at graph output.\
    \ This will be flattened in the converted model.\nConverting PyTorch Frontend\
    \ ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A\
    | 1476/1478 [00:00<00:00, 4831.01 ops/s]\nRunning MIL Common passes: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    | 39/39 [00:01<00:00, 35.42 passes/s]\nRunning MIL FP16ComputePrecision pass:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.24s/ passes]\nRunning MIL\
    \ Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 11/11 [00:07<00:00,  1.43 passes/s]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
    \ safety_checker model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_safety_checker.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Converted safety_checker\nINFO:python_coreml_stable_diffusion.torch2coreml:Bundling\
    \ resources for Guernika\nINFO:python_coreml_stable_diffusion.torch2coreml:Created\
    \ /Volumes/2TB SSD/@OUTPUT/elldrethSDreamMix_v10 for Guernika assets\nINFO:python_coreml_stable_diffusion.torch2coreml:Compiling\
    \ /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_elldrethSDreamMix_v10_text_encoder.mlpackage\n\
    xcrun: error: unable to find utility \"coremlcompiler\", not a developer tool\
    \ or in PATH"
  created_at: 2023-01-02 23:27:02+00:00
  edited: false
  hidden: false
  id: 63b368460dddc8f717fa1612
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672782907557-6313af200b24eab4746e7d34.jpeg?w=200&h=200&f=face
      fullname: Guillermo Cique
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: GuiyeC
      type: user
    createdAt: '2023-01-02T23:35:37.000Z'
    data:
      edited: false
      editors:
      - GuiyeC
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672782907557-6313af200b24eab4746e7d34.jpeg?w=200&h=200&f=face
          fullname: Guillermo Cique
          isHf: false
          isPro: false
          name: GuiyeC
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;radfaraf&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/radfaraf\">@<span class=\"\
          underline\">radfaraf</span></a></span>\n\n\t</span></span> I found <a rel=\"\
          nofollow\" href=\"https://github.com/apple/ml-stable-diffusion/issues/17\"\
          >this</a> which seems to be exactly your problem, it looks like you may\
          \ need to install Xcode and run:</p>\n<pre><code class=\"language-shell\"\
          >sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer/\n\
          </code></pre>\n<p>I'll check to see if there is someway to avoid this, but\
          \ this does seem to be your problem.</p>\n"
        raw: '@radfaraf I found [this](https://github.com/apple/ml-stable-diffusion/issues/17)
          which seems to be exactly your problem, it looks like you may need to install
          Xcode and run:

          ```shell

          sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer/

          ```


          I''ll check to see if there is someway to avoid this, but this does seem
          to be your problem.'
        updatedAt: '2023-01-02T23:35:37.545Z'
      numEdits: 0
      reactions: []
    id: 63b36a496c3e6df5c38f1d30
    type: comment
  author: GuiyeC
  content: '@radfaraf I found [this](https://github.com/apple/ml-stable-diffusion/issues/17)
    which seems to be exactly your problem, it looks like you may need to install
    Xcode and run:

    ```shell

    sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer/

    ```


    I''ll check to see if there is someway to avoid this, but this does seem to be
    your problem.'
  created_at: 2023-01-02 23:35:37+00:00
  edited: false
  hidden: false
  id: 63b36a496c3e6df5c38f1d30
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
      fullname: Robert W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: radfaraf
      type: user
    createdAt: '2023-01-02T23:58:45.000Z'
    data:
      edited: false
      editors:
      - radfaraf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
          fullname: Robert W
          isHf: false
          isPro: false
          name: radfaraf
          type: user
        html: '<p>Installed it will see if it works soon.</p>

          '
        raw: Installed it will see if it works soon.
        updatedAt: '2023-01-02T23:58:45.586Z'
      numEdits: 0
      reactions: []
    id: 63b36fb56c3e6df5c38f5b8f
    type: comment
  author: radfaraf
  content: Installed it will see if it works soon.
  created_at: 2023-01-02 23:58:45+00:00
  edited: false
  hidden: false
  id: 63b36fb56c3e6df5c38f5b8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
      fullname: Robert W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: radfaraf
      type: user
    createdAt: '2023-01-03T00:28:02.000Z'
    data:
      edited: false
      editors:
      - radfaraf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
          fullname: Robert W
          isHf: false
          isPro: false
          name: radfaraf
          type: user
        html: "<p>Strange some sort of permission error now: </p>\n<p>Torch version\
          \ 1.13.0 has not been tested with coremltools. You may run into unexpected\
          \ errors. Torch 1.12.1 is the most recent version that has been tested.<br>2023-01-02\
          \ 19:09:00.874 Guernika Model Converter[7095:92309] +[CATransaction synchronize]\
          \ called within transaction<br>2023-01-02 19:09:01.139 Guernika Model Converter[7095:92309]\
          \ +[CATransaction synchronize] called within transaction<br>2023-01-02 19:09:17.739\
          \ Guernika Model Converter[7095:92309] +[CATransaction synchronize] called\
          \ within transaction<br>INFO:python_coreml_stable_diffusion.torch2coreml:Initializing\
          \ StableDiffusionPipeline from /Volumes/2TB SSD/Stable-diffusion/wlop_1.ckpt..<br>\
          \  % Total    % Received % Xferd  Average Speed   Time    Time     Time\
          \  Current<br>                                 Dload  Upload   Total   Spent\
          \    Left  Speed<br>100  1873  100  1873    0     0  29141      0 --:--:--\
          \ --:--:-- --:--:-- 30704<br>Some weights of the model checkpoint at openai/clip-vit-large-patch14\
          \ were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.11.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.embeddings.patch_embedding.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight',\
          \ 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias',\
          \ 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'text_projection.weight',\
          \ 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.weight',\
          \ 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias',\
          \ 'visual_projection.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids',\
          \ 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'logit_scale',\
          \ 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.embeddings.position_embedding.weight',\
          \ 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight',\
          \ 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.21.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias']</p>\n\
          <ul>\n<li>This IS expected if you are initializing CLIPTextModel from the\
          \ checkpoint of a model trained on another task or with another architecture\
          \ (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining\
          \ model).</li>\n<li>This IS NOT expected if you are initializing CLIPTextModel\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).<br>diffusers/utils/deprecation_utils.py:35: FutureWarning: The\
          \ configuration file of this scheduler: PNDMScheduler {<br>\"_class_name\"\
          : \"PNDMScheduler\",<br>\"_diffusers_version\": \"0.9.0\",<br>\"beta_end\"\
          : 0.012,<br>\"beta_schedule\": \"scaled_linear\",<br>\"beta_start\": 0.00085,<br>\"\
          num_train_timesteps\": 1000,<br>\"set_alpha_to_one\": false,<br>\"skip_prk_steps\"\
          : true,<br>\"steps_offset\": 0,<br>\"trained_betas\": null<br>}<br> is outdated.\
          \ <code>steps_offset</code> should be set to 1 instead of 0. Please make\
          \ sure to update the config accordingly as leaving <code>steps_offset</code>\
          \ might led to incorrect results in future versions. If you have downloaded\
          \ this checkpoint from the Hugging Face Hub, it would be very nice if you\
          \ could open a Pull request for the <code>scheduler/scheduler_config.json</code>\
          \ file<br>INFO:python_coreml_stable_diffusion.torch2coreml:Done.<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ vae_decoder<br>diffusers/models/resnet.py:109: TracerWarning: Converting\
          \ a tensor to a Python boolean might cause the trace to be incorrect. We\
          \ can't record the data flow of Python values, so this value will be treated\
          \ as a constant in the future. This means that the trace might not generalize\
          \ to other inputs!<br>diffusers/models/resnet.py:122: TracerWarning: Converting\
          \ a tensor to a Python boolean might cause the trace to be incorrect. We\
          \ can't record the data flow of Python values, so this value will be treated\
          \ as a constant in the future. This means that the trace might not generalize\
          \ to other inputs!<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ vae_decoder to CoreML..<br>Converting PyTorch Frontend ==&gt; MIL Ops:\
          \   0%|                                                                \
          \                                                                      \
          \                     | 0/353 [00:00&lt;?, ? ops/s]WARNING:python_coreml_stable_diffusion.torch2coreml:Casted\
          \ the <code>beta</code>(value=0.0) argument of <code>baddbmm</code> op from\
          \ int32 to float32 dtype for conversion!<br>Converting PyTorch Frontend\
          \ ==&gt; MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u258C| 352/353 [00:00&lt;00:00, 2087.75 ops/s]<br>Running MIL Common\
          \ passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 39/39 [00:00&lt;00:00, 60.37 passes/s]<br>Running\
          \ MIL FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,\
          \  1.39 passes/s]<br>Running MIL Clean up passes: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:02&lt;00:00,\
          \  4.18 passes/s]<br>Torch version 1.13.0 has not been tested with coremltools.\
          \ You may run into unexpected errors. Torch 1.12.1 is the most recent version\
          \ that has been tested.<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ vae_decoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_vae_decoder.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ vae_decoder into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_vae_decoder.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converted\
          \ vae_decoder<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ vae_encoder<br>diffusers/models/resnet.py:182: TracerWarning: Converting\
          \ a tensor to a Python boolean might cause the trace to be incorrect. We\
          \ can't record the data flow of Python values, so this value will be treated\
          \ as a constant in the future. This means that the trace might not generalize\
          \ to other inputs!<br>diffusers/models/resnet.py:187: TracerWarning: Converting\
          \ a tensor to a Python boolean might cause the trace to be incorrect. We\
          \ can't record the data flow of Python values, so this value will be treated\
          \ as a constant in the future. This means that the trace might not generalize\
          \ to other inputs!<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ vae_encoder to CoreML..<br>Converting PyTorch Frontend ==&gt; MIL Ops:\
          \   0%|                                                                \
          \                                                                      \
          \                     | 0/281 [00:00&lt;?, ? ops/s]WARNING:python_coreml_stable_diffusion.torch2coreml:Casted\
          \ the <code>beta</code>(value=0.0) argument of <code>baddbmm</code> op from\
          \ int32 to float32 dtype for conversion!<br>Converting PyTorch Frontend\
          \ ==&gt; MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u258D| 280/281 [00:00&lt;00:00, 3187.36 ops/s]<br>Running MIL Common\
          \ passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 39/39 [00:00&lt;00:00, 131.09 passes/s]<br>Running MIL\
          \ FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,\
          \  3.09 passes/s]<br>Running MIL Clean up passes: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:01&lt;00:00,\
          \  9.61 passes/s]<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ vae_encoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_vae_encoder.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ vae_encoder into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_vae_encoder.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converted\
          \ vae_encoder<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ unet<br>INFO:python_coreml_stable_diffusion.torch2coreml:Attention implementation\
          \ in effect: AttentionImplementations.ORIGINAL<br>INFO:python_coreml_stable_diffusion.torch2coreml:Sample\
          \ inputs spec: {'sample': (torch.Size([2, 4, 64, 64]), torch.float32), 'timestep':\
          \ (torch.Size([2]), torch.float32), 'encoder_hidden_states': (torch.Size([2,\
          \ 768, 1, 77]), torch.float32)}<br>INFO:python_coreml_stable_diffusion.torch2coreml:JIT\
          \ tracing..<br>python_coreml_stable_diffusion/layer_norm.py:61: TracerWarning:\
          \ Converting a tensor to a Python boolean might cause the trace to be incorrect.\
          \ We can't record the data flow of Python values, so this value will be\
          \ treated as a constant in the future. This means that the trace might not\
          \ generalize to other inputs!<br>INFO:python_coreml_stable_diffusion.torch2coreml:Done.<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ unet to CoreML..<br>WARNING:coremltools:Tuple detected at graph output.\
          \ This will be flattened in the converted model.<br>Converting PyTorch Frontend\
          \ ==&gt; MIL Ops:   0%|                                                \
          \                                                                      \
          \                                    | 0/3400 [00:00&lt;?, ? ops/s]WARNING:coremltools:Saving\
          \ value type of int64 into a builtin type of int32, might lose precision!<br>Converting\
          \ PyTorch Frontend ==&gt; MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2589| 3398/3400 [00:01&lt;00:00, 3238.73 ops/s]<br>Running\
          \ MIL Common passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:10&lt;00:00,  3.61 passes/s]<br>Running\
          \ MIL FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:27&lt;00:00,\
          \ 27.12s/ passes]<br>Running MIL Clean up passes: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [01:52&lt;00:00,\
          \ 10.24s/ passes]<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ unet model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_unet.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ unet into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_unet.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converted\
          \ unet<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting text_encoder<br>INFO:python_coreml_stable_diffusion.torch2coreml:Sample\
          \ inputs spec: {'input_ids': (torch.Size([1, 77]), torch.float32)}<br>INFO:python_coreml_stable_diffusion.torch2coreml:JIT\
          \ tracing text_encoder..<br>transformers/models/clip/modeling_clip.py:280:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!<br>if attn_weights.size()\
          \ != (bsz * self.num_heads, tgt_len, src_len):<br>transformers/models/clip/modeling_clip.py:288:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!<br>if causal_attention_mask.size()\
          \ != (bsz, 1, tgt_len, src_len):<br>transformers/models/clip/modeling_clip.py:320:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!<br>if attn_output.size()\
          \ != (bsz * self.num_heads, tgt_len, self.head_dim):<br>INFO:python_coreml_stable_diffusion.torch2coreml:Done.<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ text_encoder to CoreML..<br>WARNING:coremltools:Tuple detected at graph\
          \ output. This will be flattened in the converted model.<br>Converting PyTorch\
          \ Frontend ==&gt; MIL Ops:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u258D             | 737/814 [00:00&lt;00:00, 3566.20 ops/s]WARNING:coremltools:Saving\
          \ value type of int64 into a builtin type of int32, might lose precision!<br>Converting\
          \ PyTorch Frontend ==&gt; MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u258B| 812/814 [00:00&lt;00:00, 3551.48 ops/s]<br>Running\
          \ MIL Common passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:00&lt;00:00, 71.28 passes/s]<br>Running\
          \ MIL FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,\
          \  1.18s/ passes]<br>Running MIL Clean up passes: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:03&lt;00:00,\
          \  3.40 passes/s]<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ text_encoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_text_encoder.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ text_encoder into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_text_encoder.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converted\
          \ text_encoder<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ safety_checker<br>INFO:python_coreml_stable_diffusion.torch2coreml:Sample\
          \ inputs spec: {'clip_input': (torch.Size([1, 3, 224, 224]), torch.float32),\
          \ 'images': (torch.Size([1, 512, 512, 3]), torch.float32), 'adjustment':\
          \ (torch.Size([1]), torch.float32)}<br>INFO:python_coreml_stable_diffusion.torch2coreml:JIT\
          \ tracing..<br>INFO:python_coreml_stable_diffusion.torch2coreml:Done.<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ safety_checker to CoreML..<br>WARNING:coremltools:Tuple detected at graph\
          \ output. This will be flattened in the converted model.<br>Converting PyTorch\
          \ Frontend ==&gt; MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u258A| 1476/1478 [00:00&lt;00:00, 4415.21 ops/s]<br>Running MIL Common\
          \ passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 39/39 [00:01&lt;00:00, 32.64 passes/s]<br>Running\
          \ MIL FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02&lt;00:00,\
          \  2.61s/ passes]<br>Running MIL Clean up passes: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:07&lt;00:00,\
          \  1.40 passes/s]<br>INFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ safety_checker model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_safety_checker.mlpackage<br>INFO:python_coreml_stable_diffusion.torch2coreml:Converted\
          \ safety_checker<br>INFO:python_coreml_stable_diffusion.torch2coreml:Bundling\
          \ resources for Guernika<br>INFO:python_coreml_stable_diffusion.torch2coreml:Compiling\
          \ /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_text_encoder.mlpackage<br>2023-01-02\
          \ 19:25:36.085 coremlcompiler[8736:112819] error creating directory: Error\
          \ Domain=NSCocoaErrorDomain Code=513 \"You don\u2019t have permission to\
          \ save the file \u201CStable_Diffusion_version_wlop_1_text_encoder.mlmodelc\u201D\
          \ in the folder \u201C2TB\u201D.\" UserInfo={NSFilePath=/Volumes/2TB/Stable_Diffusion_version_wlop_1_text_encoder.mlmodelc,\
          \ NSUnderlyingError=0x6000038b4000 {Error Domain=NSPOSIXErrorDomain Code=13\
          \ \"Permission denied\"}}<br>coremlcompiler: error: You don\u201A\xC4\xF4\
          t have permission to save the file \u201A\xC4\xFAStable_Diffusion_version_wlop_1_text_encoder.mlmodelc\u201A\
          \xC4\xF9 in the folder \u201A\xC4\xFA2TB\u201A\xC4\xF9.: unspecified iostream_category\
          \ error</li>\n</ul>\n"
        raw: "Strange some sort of permission error now: \n\nTorch version 1.13.0\
          \ has not been tested with coremltools. You may run into unexpected errors.\
          \ Torch 1.12.1 is the most recent version that has been tested.\n2023-01-02\
          \ 19:09:00.874 Guernika Model Converter[7095:92309] +[CATransaction synchronize]\
          \ called within transaction\n2023-01-02 19:09:01.139 Guernika Model Converter[7095:92309]\
          \ +[CATransaction synchronize] called within transaction\n2023-01-02 19:09:17.739\
          \ Guernika Model Converter[7095:92309] +[CATransaction synchronize] called\
          \ within transaction\nINFO:python_coreml_stable_diffusion.torch2coreml:Initializing\
          \ StableDiffusionPipeline from /Volumes/2TB SSD/Stable-diffusion/wlop_1.ckpt..\n\
          \  % Total    % Received % Xferd  Average Speed   Time    Time     Time\
          \  Current\n                                 Dload  Upload   Total   Spent\
          \    Left  Speed\n100  1873  100  1873    0     0  29141      0 --:--:--\
          \ --:--:-- --:--:-- 30704\nSome weights of the model checkpoint at openai/clip-vit-large-patch14\
          \ were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.11.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.embeddings.patch_embedding.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight',\
          \ 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias',\
          \ 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'text_projection.weight',\
          \ 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.weight',\
          \ 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias',\
          \ 'visual_projection.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids',\
          \ 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'logit_scale',\
          \ 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias',\
          \ 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight',\
          \ 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias',\
          \ 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias',\
          \ 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight',\
          \ 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight',\
          \ 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.embeddings.position_embedding.weight',\
          \ 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias',\
          \ 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight',\
          \ 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias',\
          \ 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias',\
          \ 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight',\
          \ 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight',\
          \ 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight',\
          \ 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.21.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight',\
          \ 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias',\
          \ 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight',\
          \ 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias']\n\
          - This IS expected if you are initializing CLIPTextModel from the checkpoint\
          \ of a model trained on another task or with another architecture (e.g.\
          \ initializing a BertForSequenceClassification model from a BertForPreTraining\
          \ model).\n- This IS NOT expected if you are initializing CLIPTextModel\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).\ndiffusers/utils/deprecation_utils.py:35: FutureWarning: The configuration\
          \ file of this scheduler: PNDMScheduler {\n  \"_class_name\": \"PNDMScheduler\"\
          ,\n  \"_diffusers_version\": \"0.9.0\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\"\
          : \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\"\
          : 1000,\n  \"set_alpha_to_one\": false,\n  \"skip_prk_steps\": true,\n \
          \ \"steps_offset\": 0,\n  \"trained_betas\": null\n}\n is outdated. `steps_offset`\
          \ should be set to 1 instead of 0. Please make sure to update the config\
          \ accordingly as leaving `steps_offset` might led to incorrect results in\
          \ future versions. If you have downloaded this checkpoint from the Hugging\
          \ Face Hub, it would be very nice if you could open a Pull request for the\
          \ `scheduler/scheduler_config.json` file\nINFO:python_coreml_stable_diffusion.torch2coreml:Done.\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting vae_decoder\n\
          diffusers/models/resnet.py:109: TracerWarning: Converting a tensor to a\
          \ Python boolean might cause the trace to be incorrect. We can't record\
          \ the data flow of Python values, so this value will be treated as a constant\
          \ in the future. This means that the trace might not generalize to other\
          \ inputs!\ndiffusers/models/resnet.py:122: TracerWarning: Converting a tensor\
          \ to a Python boolean might cause the trace to be incorrect. We can't record\
          \ the data flow of Python values, so this value will be treated as a constant\
          \ in the future. This means that the trace might not generalize to other\
          \ inputs!\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting vae_decoder\
          \ to CoreML..\nConverting PyTorch Frontend ==> MIL Ops:   0%|          \
          \                                                                      \
          \                                                                      \
          \     | 0/353 [00:00<?, ? ops/s]WARNING:python_coreml_stable_diffusion.torch2coreml:Casted\
          \ the `beta`(value=0.0) argument of `baddbmm` op from int32 to float32 dtype\
          \ for conversion!\nConverting PyTorch Frontend ==> MIL Ops: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258C| 352/353 [00:00<00:00,\
          \ 2087.75 ops/s]\nRunning MIL Common passes: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:00<00:00,\
          \ 60.37 passes/s]\nRunning MIL FP16ComputePrecision pass: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 1/1 [00:00<00:00,  1.39 passes/s]\nRunning MIL Clean up passes:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 11/11 [00:02<00:00,  4.18 passes/s]\nTorch version 1.13.0 has not\
          \ been tested with coremltools. You may run into unexpected errors. Torch\
          \ 1.12.1 is the most recent version that has been tested.\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ vae_decoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_vae_decoder.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Saved vae_decoder into\
          \ /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_vae_decoder.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converted vae_decoder\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting vae_encoder\n\
          diffusers/models/resnet.py:182: TracerWarning: Converting a tensor to a\
          \ Python boolean might cause the trace to be incorrect. We can't record\
          \ the data flow of Python values, so this value will be treated as a constant\
          \ in the future. This means that the trace might not generalize to other\
          \ inputs!\ndiffusers/models/resnet.py:187: TracerWarning: Converting a tensor\
          \ to a Python boolean might cause the trace to be incorrect. We can't record\
          \ the data flow of Python values, so this value will be treated as a constant\
          \ in the future. This means that the trace might not generalize to other\
          \ inputs!\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting vae_encoder\
          \ to CoreML..\nConverting PyTorch Frontend ==> MIL Ops:   0%|          \
          \                                                                      \
          \                                                                      \
          \     | 0/281 [00:00<?, ? ops/s]WARNING:python_coreml_stable_diffusion.torch2coreml:Casted\
          \ the `beta`(value=0.0) argument of `baddbmm` op from int32 to float32 dtype\
          \ for conversion!\nConverting PyTorch Frontend ==> MIL Ops: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258D| 280/281 [00:00<00:00,\
          \ 3187.36 ops/s]\nRunning MIL Common passes: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:00<00:00,\
          \ 131.09 passes/s]\nRunning MIL FP16ComputePrecision pass: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 1/1 [00:00<00:00,  3.09 passes/s]\nRunning MIL Clean up passes:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 11/11 [00:01<00:00,  9.61 passes/s]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ vae_encoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_vae_encoder.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Saved vae_encoder into\
          \ /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_vae_encoder.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converted vae_encoder\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting unet\nINFO:python_coreml_stable_diffusion.torch2coreml:Attention\
          \ implementation in effect: AttentionImplementations.ORIGINAL\nINFO:python_coreml_stable_diffusion.torch2coreml:Sample\
          \ inputs spec: {'sample': (torch.Size([2, 4, 64, 64]), torch.float32), 'timestep':\
          \ (torch.Size([2]), torch.float32), 'encoder_hidden_states': (torch.Size([2,\
          \ 768, 1, 77]), torch.float32)}\nINFO:python_coreml_stable_diffusion.torch2coreml:JIT\
          \ tracing..\npython_coreml_stable_diffusion/layer_norm.py:61: TracerWarning:\
          \ Converting a tensor to a Python boolean might cause the trace to be incorrect.\
          \ We can't record the data flow of Python values, so this value will be\
          \ treated as a constant in the future. This means that the trace might not\
          \ generalize to other inputs!\nINFO:python_coreml_stable_diffusion.torch2coreml:Done.\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting unet to CoreML..\n\
          WARNING:coremltools:Tuple detected at graph output. This will be flattened\
          \ in the converted model.\nConverting PyTorch Frontend ==> MIL Ops:   0%|\
          \                                                                      \
          \                                                                      \
          \              | 0/3400 [00:00<?, ? ops/s]WARNING:coremltools:Saving value\
          \ type of int64 into a builtin type of int32, might lose precision!\nConverting\
          \ PyTorch Frontend ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2589| 3398/3400 [00:01<00:00, 3238.73 ops/s]\nRunning MIL\
          \ Common passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 39/39 [00:10<00:00,  3.61 passes/s]\nRunning\
          \ MIL FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:27<00:00,\
          \ 27.12s/ passes]\nRunning MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [01:52<00:00, 10.24s/\
          \ passes]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved unet model\
          \ to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_unet.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Saved unet into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_unet.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converted unet\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
          \ text_encoder\nINFO:python_coreml_stable_diffusion.torch2coreml:Sample\
          \ inputs spec: {'input_ids': (torch.Size([1, 77]), torch.float32)}\nINFO:python_coreml_stable_diffusion.torch2coreml:JIT\
          \ tracing text_encoder..\ntransformers/models/clip/modeling_clip.py:280:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!\n  if attn_weights.size()\
          \ != (bsz * self.num_heads, tgt_len, src_len):\ntransformers/models/clip/modeling_clip.py:288:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!\n  if causal_attention_mask.size()\
          \ != (bsz, 1, tgt_len, src_len):\ntransformers/models/clip/modeling_clip.py:320:\
          \ TracerWarning: Converting a tensor to a Python boolean might cause the\
          \ trace to be incorrect. We can't record the data flow of Python values,\
          \ so this value will be treated as a constant in the future. This means\
          \ that the trace might not generalize to other inputs!\n  if attn_output.size()\
          \ != (bsz * self.num_heads, tgt_len, self.head_dim):\nINFO:python_coreml_stable_diffusion.torch2coreml:Done.\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting text_encoder\
          \ to CoreML..\nWARNING:coremltools:Tuple detected at graph output. This\
          \ will be flattened in the converted model.\nConverting PyTorch Frontend\
          \ ==> MIL Ops:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u258D             | 737/814 [00:00<00:00, 3566.20 ops/s]WARNING:coremltools:Saving\
          \ value type of int64 into a builtin type of int32, might lose precision!\n\
          Converting PyTorch Frontend ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u258B| 812/814 [00:00<00:00, 3551.48 ops/s]\n\
          Running MIL Common passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:00<00:00, 71.28 passes/s]\n\
          Running MIL FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1\
          \ [00:01<00:00,  1.18s/ passes]\nRunning MIL Clean up passes: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11\
          \ [00:03<00:00,  3.40 passes/s]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ text_encoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_text_encoder.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Saved text_encoder into\
          \ /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_text_encoder.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converted text_encoder\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting safety_checker\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Sample inputs spec: {'clip_input':\
          \ (torch.Size([1, 3, 224, 224]), torch.float32), 'images': (torch.Size([1,\
          \ 512, 512, 3]), torch.float32), 'adjustment': (torch.Size([1]), torch.float32)}\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:JIT tracing..\nINFO:python_coreml_stable_diffusion.torch2coreml:Done.\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converting safety_checker\
          \ to CoreML..\nWARNING:coremltools:Tuple detected at graph output. This\
          \ will be flattened in the converted model.\nConverting PyTorch Frontend\
          \ ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A\
          | 1476/1478 [00:00<00:00, 4415.21 ops/s]\nRunning MIL Common passes: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 39/39 [00:01<00:00, 32.64 passes/s]\nRunning MIL FP16ComputePrecision\
          \ pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.61s/ passes]\n\
          Running MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 11/11 [00:07<00:00,  1.40 passes/s]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
          \ safety_checker model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_safety_checker.mlpackage\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Converted safety_checker\n\
          INFO:python_coreml_stable_diffusion.torch2coreml:Bundling resources for\
          \ Guernika\nINFO:python_coreml_stable_diffusion.torch2coreml:Compiling /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_text_encoder.mlpackage\n\
          2023-01-02 19:25:36.085 coremlcompiler[8736:112819] error creating directory:\
          \ Error Domain=NSCocoaErrorDomain Code=513 \"You don\u2019t have permission\
          \ to save the file \u201CStable_Diffusion_version_wlop_1_text_encoder.mlmodelc\u201D\
          \ in the folder \u201C2TB\u201D.\" UserInfo={NSFilePath=/Volumes/2TB/Stable_Diffusion_version_wlop_1_text_encoder.mlmodelc,\
          \ NSUnderlyingError=0x6000038b4000 {Error Domain=NSPOSIXErrorDomain Code=13\
          \ \"Permission denied\"}}\ncoremlcompiler: error: You don\u201A\xC4\xF4\
          t have permission to save the file \u201A\xC4\xFAStable_Diffusion_version_wlop_1_text_encoder.mlmodelc\u201A\
          \xC4\xF9 in the folder \u201A\xC4\xFA2TB\u201A\xC4\xF9.: unspecified iostream_category\
          \ error"
        updatedAt: '2023-01-03T00:28:02.276Z'
      numEdits: 0
      reactions: []
    id: 63b376920dddc8f717fac6f6
    type: comment
  author: radfaraf
  content: "Strange some sort of permission error now: \n\nTorch version 1.13.0 has\
    \ not been tested with coremltools. You may run into unexpected errors. Torch\
    \ 1.12.1 is the most recent version that has been tested.\n2023-01-02 19:09:00.874\
    \ Guernika Model Converter[7095:92309] +[CATransaction synchronize] called within\
    \ transaction\n2023-01-02 19:09:01.139 Guernika Model Converter[7095:92309] +[CATransaction\
    \ synchronize] called within transaction\n2023-01-02 19:09:17.739 Guernika Model\
    \ Converter[7095:92309] +[CATransaction synchronize] called within transaction\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Initializing StableDiffusionPipeline\
    \ from /Volumes/2TB SSD/Stable-diffusion/wlop_1.ckpt..\n  % Total    % Received\
    \ % Xferd  Average Speed   Time    Time     Time  Current\n                  \
    \               Dload  Upload   Total   Spent    Left  Speed\n100  1873  100 \
    \ 1873    0     0  29141      0 --:--:-- --:--:-- --:--:-- 30704\nSome weights\
    \ of the model checkpoint at openai/clip-vit-large-patch14 were not used when\
    \ initializing CLIPTextModel: ['vision_model.encoder.layers.11.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.embeddings.patch_embedding.weight',\
    \ 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight',\
    \ 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias',\
    \ 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'text_projection.weight',\
    \ 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.weight',\
    \ 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias',\
    \ 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias',\
    \ 'visual_projection.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids',\
    \ 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'logit_scale', 'vision_model.encoder.layers.7.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight',\
    \ 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias',\
    \ 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight',\
    \ 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight',\
    \ 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight',\
    \ 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias',\
    \ 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias',\
    \ 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight',\
    \ 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias',\
    \ 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias',\
    \ 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight',\
    \ 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight',\
    \ 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight',\
    \ 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight',\
    \ 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.embeddings.class_embedding',\
    \ 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias',\
    \ 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias',\
    \ 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias',\
    \ 'vision_model.encoder.layers.1.self_attn.v_proj.bias']\n- This IS expected if\
    \ you are initializing CLIPTextModel from the checkpoint of a model trained on\
    \ another task or with another architecture (e.g. initializing a BertForSequenceClassification\
    \ model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing\
    \ CLIPTextModel from the checkpoint of a model that you expect to be exactly identical\
    \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
    \ model).\ndiffusers/utils/deprecation_utils.py:35: FutureWarning: The configuration\
    \ file of this scheduler: PNDMScheduler {\n  \"_class_name\": \"PNDMScheduler\"\
    ,\n  \"_diffusers_version\": \"0.9.0\",\n  \"beta_end\": 0.012,\n  \"beta_schedule\"\
    : \"scaled_linear\",\n  \"beta_start\": 0.00085,\n  \"num_train_timesteps\": 1000,\n\
    \  \"set_alpha_to_one\": false,\n  \"skip_prk_steps\": true,\n  \"steps_offset\"\
    : 0,\n  \"trained_betas\": null\n}\n is outdated. `steps_offset` should be set\
    \ to 1 instead of 0. Please make sure to update the config accordingly as leaving\
    \ `steps_offset` might led to incorrect results in future versions. If you have\
    \ downloaded this checkpoint from the Hugging Face Hub, it would be very nice\
    \ if you could open a Pull request for the `scheduler/scheduler_config.json` file\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Done.\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ vae_decoder\ndiffusers/models/resnet.py:109: TracerWarning: Converting a tensor\
    \ to a Python boolean might cause the trace to be incorrect. We can't record the\
    \ data flow of Python values, so this value will be treated as a constant in the\
    \ future. This means that the trace might not generalize to other inputs!\ndiffusers/models/resnet.py:122:\
    \ TracerWarning: Converting a tensor to a Python boolean might cause the trace\
    \ to be incorrect. We can't record the data flow of Python values, so this value\
    \ will be treated as a constant in the future. This means that the trace might\
    \ not generalize to other inputs!\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ vae_decoder to CoreML..\nConverting PyTorch Frontend ==> MIL Ops:   0%|    \
    \                                                                            \
    \                                                                           |\
    \ 0/353 [00:00<?, ? ops/s]WARNING:python_coreml_stable_diffusion.torch2coreml:Casted\
    \ the `beta`(value=0.0) argument of `baddbmm` op from int32 to float32 dtype for\
    \ conversion!\nConverting PyTorch Frontend ==> MIL Ops: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258C| 352/353 [00:00<00:00,\
    \ 2087.75 ops/s]\nRunning MIL Common passes: 100%|\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:00<00:00,\
    \ 60.37 passes/s]\nRunning MIL FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 1/1 [00:00<00:00,  1.39 passes/s]\nRunning MIL Clean up passes:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 11/11 [00:02<00:00,  4.18 passes/s]\nTorch version 1.13.0 has not been\
    \ tested with coremltools. You may run into unexpected errors. Torch 1.12.1 is\
    \ the most recent version that has been tested.\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
    \ vae_decoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_vae_decoder.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Saved vae_decoder into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_vae_decoder.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Converted vae_decoder\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ vae_encoder\ndiffusers/models/resnet.py:182: TracerWarning: Converting a tensor\
    \ to a Python boolean might cause the trace to be incorrect. We can't record the\
    \ data flow of Python values, so this value will be treated as a constant in the\
    \ future. This means that the trace might not generalize to other inputs!\ndiffusers/models/resnet.py:187:\
    \ TracerWarning: Converting a tensor to a Python boolean might cause the trace\
    \ to be incorrect. We can't record the data flow of Python values, so this value\
    \ will be treated as a constant in the future. This means that the trace might\
    \ not generalize to other inputs!\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ vae_encoder to CoreML..\nConverting PyTorch Frontend ==> MIL Ops:   0%|    \
    \                                                                            \
    \                                                                           |\
    \ 0/281 [00:00<?, ? ops/s]WARNING:python_coreml_stable_diffusion.torch2coreml:Casted\
    \ the `beta`(value=0.0) argument of `baddbmm` op from int32 to float32 dtype for\
    \ conversion!\nConverting PyTorch Frontend ==> MIL Ops: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258D| 280/281 [00:00<00:00,\
    \ 3187.36 ops/s]\nRunning MIL Common passes: 100%|\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39/39 [00:00<00:00, 131.09\
    \ passes/s]\nRunning MIL FP16ComputePrecision pass: 100%|\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 1/1 [00:00<00:00,  3.09 passes/s]\nRunning MIL Clean up passes: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11\
    \ [00:01<00:00,  9.61 passes/s]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
    \ vae_encoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_vae_encoder.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Saved vae_encoder into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_vae_encoder.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Converted vae_encoder\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ unet\nINFO:python_coreml_stable_diffusion.torch2coreml:Attention implementation\
    \ in effect: AttentionImplementations.ORIGINAL\nINFO:python_coreml_stable_diffusion.torch2coreml:Sample\
    \ inputs spec: {'sample': (torch.Size([2, 4, 64, 64]), torch.float32), 'timestep':\
    \ (torch.Size([2]), torch.float32), 'encoder_hidden_states': (torch.Size([2, 768,\
    \ 1, 77]), torch.float32)}\nINFO:python_coreml_stable_diffusion.torch2coreml:JIT\
    \ tracing..\npython_coreml_stable_diffusion/layer_norm.py:61: TracerWarning: Converting\
    \ a tensor to a Python boolean might cause the trace to be incorrect. We can't\
    \ record the data flow of Python values, so this value will be treated as a constant\
    \ in the future. This means that the trace might not generalize to other inputs!\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Done.\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ unet to CoreML..\nWARNING:coremltools:Tuple detected at graph output. This will\
    \ be flattened in the converted model.\nConverting PyTorch Frontend ==> MIL Ops:\
    \   0%|                                                                      \
    \                                                                            \
    \        | 0/3400 [00:00<?, ? ops/s]WARNING:coremltools:Saving value type of int64\
    \ into a builtin type of int32, might lose precision!\nConverting PyTorch Frontend\
    \ ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2589| 3398/3400 [00:01<00:00, 3238.73 ops/s]\nRunning MIL Common passes: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 39/39 [00:10<00:00,  3.61 passes/s]\nRunning MIL FP16ComputePrecision\
    \ pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:27<00:00, 27.12s/ passes]\n\
    Running MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 11/11 [01:52<00:00, 10.24s/ passes]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
    \ unet model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_unet.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Saved unet into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_unet.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Converted unet\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ text_encoder\nINFO:python_coreml_stable_diffusion.torch2coreml:Sample inputs\
    \ spec: {'input_ids': (torch.Size([1, 77]), torch.float32)}\nINFO:python_coreml_stable_diffusion.torch2coreml:JIT\
    \ tracing text_encoder..\ntransformers/models/clip/modeling_clip.py:280: TracerWarning:\
    \ Converting a tensor to a Python boolean might cause the trace to be incorrect.\
    \ We can't record the data flow of Python values, so this value will be treated\
    \ as a constant in the future. This means that the trace might not generalize\
    \ to other inputs!\n  if attn_weights.size() != (bsz * self.num_heads, tgt_len,\
    \ src_len):\ntransformers/models/clip/modeling_clip.py:288: TracerWarning: Converting\
    \ a tensor to a Python boolean might cause the trace to be incorrect. We can't\
    \ record the data flow of Python values, so this value will be treated as a constant\
    \ in the future. This means that the trace might not generalize to other inputs!\n\
    \  if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\ntransformers/models/clip/modeling_clip.py:320:\
    \ TracerWarning: Converting a tensor to a Python boolean might cause the trace\
    \ to be incorrect. We can't record the data flow of Python values, so this value\
    \ will be treated as a constant in the future. This means that the trace might\
    \ not generalize to other inputs!\n  if attn_output.size() != (bsz * self.num_heads,\
    \ tgt_len, self.head_dim):\nINFO:python_coreml_stable_diffusion.torch2coreml:Done.\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Converting text_encoder to CoreML..\n\
    WARNING:coremltools:Tuple detected at graph output. This will be flattened in\
    \ the converted model.\nConverting PyTorch Frontend ==> MIL Ops:  91%|\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258D           \
    \  | 737/814 [00:00<00:00, 3566.20 ops/s]WARNING:coremltools:Saving value type\
    \ of int64 into a builtin type of int32, might lose precision!\nConverting PyTorch\
    \ Frontend ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u258B| 812/814 [00:00<00:00, 3551.48 ops/s]\nRunning MIL Common\
    \ passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 39/39 [00:00<00:00, 71.28 passes/s]\nRunning MIL FP16ComputePrecision\
    \ pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.18s/ passes]\n\
    Running MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 11/11 [00:03<00:00,  3.40 passes/s]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
    \ text_encoder model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_text_encoder.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Saved text_encoder into /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_text_encoder.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Converted text_encoder\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ safety_checker\nINFO:python_coreml_stable_diffusion.torch2coreml:Sample inputs\
    \ spec: {'clip_input': (torch.Size([1, 3, 224, 224]), torch.float32), 'images':\
    \ (torch.Size([1, 512, 512, 3]), torch.float32), 'adjustment': (torch.Size([1]),\
    \ torch.float32)}\nINFO:python_coreml_stable_diffusion.torch2coreml:JIT tracing..\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Done.\nINFO:python_coreml_stable_diffusion.torch2coreml:Converting\
    \ safety_checker to CoreML..\nWARNING:coremltools:Tuple detected at graph output.\
    \ This will be flattened in the converted model.\nConverting PyTorch Frontend\
    \ ==> MIL Ops: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u258A| 1476/1478 [00:00<00:00, 4415.21 ops/s]\nRunning MIL Common passes: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 39/39 [00:01<00:00, 32.64 passes/s]\nRunning MIL FP16ComputePrecision\
    \ pass: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.61s/ passes]\n\
    Running MIL Clean up passes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 11/11 [00:07<00:00,  1.40 passes/s]\nINFO:python_coreml_stable_diffusion.torch2coreml:Saved\
    \ safety_checker model to /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_safety_checker.mlpackage\n\
    INFO:python_coreml_stable_diffusion.torch2coreml:Converted safety_checker\nINFO:python_coreml_stable_diffusion.torch2coreml:Bundling\
    \ resources for Guernika\nINFO:python_coreml_stable_diffusion.torch2coreml:Compiling\
    \ /var/folders/jc/w_542bpx5m545pgd5y6_8yzc0000gn/T/Stable_Diffusion_version_wlop_1_text_encoder.mlpackage\n\
    2023-01-02 19:25:36.085 coremlcompiler[8736:112819] error creating directory:\
    \ Error Domain=NSCocoaErrorDomain Code=513 \"You don\u2019t have permission to\
    \ save the file \u201CStable_Diffusion_version_wlop_1_text_encoder.mlmodelc\u201D\
    \ in the folder \u201C2TB\u201D.\" UserInfo={NSFilePath=/Volumes/2TB/Stable_Diffusion_version_wlop_1_text_encoder.mlmodelc,\
    \ NSUnderlyingError=0x6000038b4000 {Error Domain=NSPOSIXErrorDomain Code=13 \"\
    Permission denied\"}}\ncoremlcompiler: error: You don\u201A\xC4\xF4t have permission\
    \ to save the file \u201A\xC4\xFAStable_Diffusion_version_wlop_1_text_encoder.mlmodelc\u201A\
    \xC4\xF9 in the folder \u201A\xC4\xFA2TB\u201A\xC4\xF9.: unspecified iostream_category\
    \ error"
  created_at: 2023-01-03 00:28:02+00:00
  edited: false
  hidden: false
  id: 63b376920dddc8f717fac6f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
      fullname: Robert W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: radfaraf
      type: user
    createdAt: '2023-01-03T00:38:22.000Z'
    data:
      edited: false
      editors:
      - radfaraf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
          fullname: Robert W
          isHf: false
          isPro: false
          name: radfaraf
          type: user
        html: '<p>Tried one more time and same error, then I changed the output to
          my internal drive instead of my thunberbolt SSD and it worked to write the
          output!</p>

          '
        raw: Tried one more time and same error, then I changed the output to my internal
          drive instead of my thunberbolt SSD and it worked to write the output!
        updatedAt: '2023-01-03T00:38:22.521Z'
      numEdits: 0
      reactions: []
    id: 63b378fe54211fcd06317adc
    type: comment
  author: radfaraf
  content: Tried one more time and same error, then I changed the output to my internal
    drive instead of my thunberbolt SSD and it worked to write the output!
  created_at: 2023-01-03 00:38:22+00:00
  edited: false
  hidden: false
  id: 63b378fe54211fcd06317adc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672782907557-6313af200b24eab4746e7d34.jpeg?w=200&h=200&f=face
      fullname: Guillermo Cique
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: GuiyeC
      type: user
    createdAt: '2023-01-03T01:03:56.000Z'
    data:
      edited: false
      editors:
      - GuiyeC
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672782907557-6313af200b24eab4746e7d34.jpeg?w=200&h=200&f=face
          fullname: Guillermo Cique
          isHf: false
          isPro: false
          name: GuiyeC
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;radfaraf&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/radfaraf\">@<span class=\"\
          underline\">radfaraf</span></a></span>\n\n\t</span></span> thank you for\
          \ all the information! I'm adding a check to make sure Xcode is installed,\
          \ maybe I can do something about the writing permissions too \U0001F914\
          <br>Did you ever run <code>sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer/</code>\
          \ or you did not have to do it?</p>\n"
        raw: "@radfaraf thank you for all the information! I'm adding a check to make\
          \ sure Xcode is installed, maybe I can do something about the writing permissions\
          \ too \U0001F914\nDid you ever run `sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer/`\
          \ or you did not have to do it?"
        updatedAt: '2023-01-03T01:03:56.445Z'
      numEdits: 0
      reactions: []
    id: 63b37efc54211fcd0631ba00
    type: comment
  author: GuiyeC
  content: "@radfaraf thank you for all the information! I'm adding a check to make\
    \ sure Xcode is installed, maybe I can do something about the writing permissions\
    \ too \U0001F914\nDid you ever run `sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer/`\
    \ or you did not have to do it?"
  created_at: 2023-01-03 01:03:56+00:00
  edited: false
  hidden: false
  id: 63b37efc54211fcd0631ba00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
      fullname: Robert W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: radfaraf
      type: user
    createdAt: '2023-01-03T01:10:23.000Z'
    data:
      edited: false
      editors:
      - radfaraf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
          fullname: Robert W
          isHf: false
          isPro: false
          name: radfaraf
          type: user
        html: '<p>I didn''t have xcode installed at all, so first I installed it.
          Then I did sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer/
          just to be safe from the instruction, so I don''t know if it''s really needed
          or not. Then I ran into an issue where it would run and soon after halt
          because it needed me to accept the license for Xcode. It would show up only
          in the console and it''s possible I only saw it because I used your instructions
          to launch it where it shows everything in the terminal console. Once I accepted
          that I had the write error, and once I switched to internal drive it worked.
          Already converted both those files and tested that they work.</p>

          '
        raw: I didn't have xcode installed at all, so first I installed it. Then I
          did sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer/
          just to be safe from the instruction, so I don't know if it's really needed
          or not. Then I ran into an issue where it would run and soon after halt
          because it needed me to accept the license for Xcode. It would show up only
          in the console and it's possible I only saw it because I used your instructions
          to launch it where it shows everything in the terminal console. Once I accepted
          that I had the write error, and once I switched to internal drive it worked.
          Already converted both those files and tested that they work.
        updatedAt: '2023-01-03T01:10:23.040Z'
      numEdits: 0
      reactions: []
    id: 63b3807f54211fcd0631ca8f
    type: comment
  author: radfaraf
  content: I didn't have xcode installed at all, so first I installed it. Then I did
    sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer/ just to
    be safe from the instruction, so I don't know if it's really needed or not. Then
    I ran into an issue where it would run and soon after halt because it needed me
    to accept the license for Xcode. It would show up only in the console and it's
    possible I only saw it because I used your instructions to launch it where it
    shows everything in the terminal console. Once I accepted that I had the write
    error, and once I switched to internal drive it worked. Already converted both
    those files and tested that they work.
  created_at: 2023-01-03 01:10:23+00:00
  edited: false
  hidden: false
  id: 63b3807f54211fcd0631ca8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672782907557-6313af200b24eab4746e7d34.jpeg?w=200&h=200&f=face
      fullname: Guillermo Cique
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: GuiyeC
      type: user
    createdAt: '2023-01-03T01:13:58.000Z'
    data:
      edited: false
      editors:
      - GuiyeC
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672782907557-6313af200b24eab4746e7d34.jpeg?w=200&h=200&f=face
          fullname: Guillermo Cique
          isHf: false
          isPro: false
          name: GuiyeC
          type: user
        html: '<p>Well, I''ll add a message suggesting to run that just in case then,
          I''m glad this was helpful. Enjoy!</p>

          '
        raw: Well, I'll add a message suggesting to run that just in case then, I'm
          glad this was helpful. Enjoy!
        updatedAt: '2023-01-03T01:13:58.832Z'
      numEdits: 0
      reactions: []
    id: 63b3815654211fcd0631d3fc
    type: comment
  author: GuiyeC
  content: Well, I'll add a message suggesting to run that just in case then, I'm
    glad this was helpful. Enjoy!
  created_at: 2023-01-03 01:13:58+00:00
  edited: false
  hidden: false
  id: 63b3815654211fcd0631d3fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
      fullname: Robert W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: radfaraf
      type: user
    createdAt: '2023-01-03T02:19:03.000Z'
    data:
      edited: false
      editors:
      - radfaraf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
          fullname: Robert W
          isHf: false
          isPro: false
          name: radfaraf
          type: user
        html: '<p>add this step to the instruction for those who do a new xcode install
          as it will help accept the license and won''t work without it:</p>

          <p>sudo xcodebuild -license accept </p>

          '
        raw: 'add this step to the instruction for those who do a new xcode install
          as it will help accept the license and won''t work without it:


          sudo xcodebuild -license accept '
        updatedAt: '2023-01-03T02:19:03.205Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63b390976c3e6df5c390e15f
    id: 63b390976c3e6df5c390e15e
    type: comment
  author: radfaraf
  content: 'add this step to the instruction for those who do a new xcode install
    as it will help accept the license and won''t work without it:


    sudo xcodebuild -license accept '
  created_at: 2023-01-03 02:19:03+00:00
  edited: false
  hidden: false
  id: 63b390976c3e6df5c390e15e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/4aeaf0cd7a2b84c26a1f90abc3183567.svg
      fullname: Robert W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: radfaraf
      type: user
    createdAt: '2023-01-03T02:19:03.000Z'
    data:
      status: closed
    id: 63b390976c3e6df5c390e15f
    type: status-change
  author: radfaraf
  created_at: 2023-01-03 02:19:03+00:00
  id: 63b390976c3e6df5c390e15f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Guernika/CoreMLStableDiffusion
repo_type: model
status: closed
target_branch: null
title: Getting errors using Guernika Model Converter on .ckpt files
