!!python/object:huggingface_hub.community.DiscussionWithDetails
author: loranger
conflicting_files: null
created_at: 2023-06-12 20:36:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670669324653-635d2c785f7e2662f71b8c6a.png?w=200&h=200&f=face
      fullname: Laurent
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: loranger
      type: user
    createdAt: '2023-06-12T21:36:21.000Z'
    data:
      edited: false
      editors:
      - loranger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7945933938026428
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670669324653-635d2c785f7e2662f71b8c6a.png?w=200&h=200&f=face
          fullname: Laurent
          isHf: false
          isPro: false
          name: loranger
          type: user
        html: "<p>Hi,</p>\n<p>I try to convert a dreambooth model based on SD 2.1.768\
          \ using Guernika Model Converter but it fails</p>\n<details>\n<summary>Logs</summary>\n\
          \n<pre><code>Starting python converter\nInitializing StableDiffusionPipeline\
          \ from /Users/loranger/StableDiffusion/Models/custom-2-1-768.ckpt..\n% Total\
          \    % Received % Xferd  Ave\nrage Speed   Time    Time     Time  Current\n\
          \                                 Dload  Upload   Total   Spent    Left\
          \  Speed\n\n  0\n0    0     0    0     0      0      0 --:--:-- --:--:--\
          \ --\n:--:--     0\n100  1873  100  1873    0     0   5872      0 --:--:--\
          \ --\n:--:-- --:--:--  5927\nTraceback (most recent call last):\n  File\
          \ \"python_coreml_stable_diffusion/torch2coreml.py\", line 1513, in &lt;module&gt;\n\
          File \"python_coreml_stable_diffusion/torch2coreml.py\", line 1281, in main\n\
          \  File \"diffusers/pipelines/stable_diffusion/convert_from_ckpt.py\", line\
          \ 1164, in download_from_original_stable_diffusion_ckpt\n  File \"torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\nRuntimeError: Error(s) in loading state_dict\
          \ for UNet2DConditionModel:\n    size mismatch for down_blocks.0.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for down_blocks.0.attentions.0.proj_out.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for down_blocks.0.attentions.1.proj_in.weight: copying a param with shape\
          \ torch.Size([320, 320]) from checkpoint, the shape in current model is\
          \ torch.Size([320, 320, 1, 1]).\n    size mismatch for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for down_blocks.0.attentions.1.proj_out.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for down_blocks.1.attentions.0.proj_in.weight: copying a param with shape\
          \ torch.Size([640, 640]) from checkpoint, the shape in current model is\
          \ torch.Size([640, 640, 1, 1]).\n    size mismatch for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for down_blocks.1.attentions.0.proj_out.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for down_blocks.1.attentions.1.proj_in.weight: copying a param with shape\
          \ torch.Size([640, 640]) from checkpoint, the shape in current model is\
          \ torch.Size([640, 640, 1, 1]).\n    size mismatch for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for down_blocks.1.attentions.1.proj_out.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for down_blocks.2.attentions.0.proj_in.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for down_blocks.2.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for down_blocks.2.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for down_blocks.2.attentions.1.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for up_blocks.1.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\n    size mismatch for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for up_blocks.1.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for up_blocks.1.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\n    size mismatch for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for up_blocks.1.attentions.1.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for up_blocks.1.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\n    size mismatch for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for up_blocks.1.attentions.2.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for up_blocks.2.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ up_blocks.2.attentions.0.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\n    size mismatch for up_blocks.2.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ up_blocks.2.attentions.1.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\n    size mismatch for up_blocks.2.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ up_blocks.2.attentions.2.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\n    size mismatch for up_blocks.3.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ up_blocks.3.attentions.0.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\n    size mismatch for up_blocks.3.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ up_blocks.3.attentions.1.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\n    size mismatch for up_blocks.3.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ up_blocks.3.attentions.2.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\n    size mismatch for mid_block.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\n    size mismatch for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for mid_block.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n[13030] Failed to execute script 'torch2coreml'\
          \ due to unhandled exception: Error(s) in loading state_dict for UNet2DConditionModel:\n\
          \    size mismatch for down_blocks.0.attentions.0.proj_in.weight: copying\
          \ a param with shape torch.Size([320, 320]) from checkpoint, the shape in\
          \ current model is torch.Size([320, 320, 1, 1]).\n    size mismatch for\
          \ down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ down_blocks.0.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([320, 320]) from checkpoint, the shape in current model is\
          \ torch.Size([320, 320, 1, 1]).\n    size mismatch for down_blocks.0.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for down_blocks.0.attentions.1.proj_out.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for down_blocks.1.attentions.0.proj_in.weight: copying a param with shape\
          \ torch.Size([640, 640]) from checkpoint, the shape in current model is\
          \ torch.Size([640, 640, 1, 1]).\n    size mismatch for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for down_blocks.1.attentions.0.proj_out.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for down_blocks.1.attentions.1.proj_in.weight: copying a param with shape\
          \ torch.Size([640, 640]) from checkpoint, the shape in current model is\
          \ torch.Size([640, 640, 1, 1]).\n    size mismatch for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for down_blocks.1.attentions.1.proj_out.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for down_bl\nocks.2.attentions.0.proj_in.weight: copying a param with\
          \ shape torch.Size([1280, 1280]) from checkpoint, the shape in current model\
          \ is torch.Size([1280, 1280, 1, 1]).\n    size mismatch for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for down_blocks.2.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for down_blocks.2.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for down_blocks.2.attentions.1.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for up_blocks.1.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\n    size mismatch for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for up_blocks.1.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for up_blocks.1.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\n    size mismatch for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for up_blocks.1.attentions.1.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for up_blocks.1.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\n    size mismatch for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for up_blocks.1.attentions.2.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for up_blocks.2.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ up_blocks.2.attentions.0.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\n    size mismatch for up_blocks.2.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ up_blocks.2.attentions.1.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\n    size mismatch for up_blocks.2.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ up_blocks.2.attentions.2.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\n    size mismatch for up_blocks.3.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ up_blocks.3.attentions.0.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\n    size mismatch for up_blocks.3.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ up_blocks.3.attentions.1.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\n    size mismatch for up_blocks.3.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ up_blocks.3.attentions.2.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\n    size mismatch for mid_block.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\n    size mismatch for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for mid_block.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n[13030] Traceback:\nTraceback (most recent\
          \ call last):\n  File \"python_coreml_stable_diffusion/torch2coreml.py\"\
          , line 1513, in &lt;module&gt;\n  File \"python_coreml_stable_diffusion/torch2coreml.py\"\
          , line 1281, in main\n  File \"diffusers/pipelines/stable_diffusion/convert_from_ckpt.py\"\
          , line 1164, in download_from_original_stable_diffusion_ckpt\n  File \"\
          torch/nn/modules/module.py\", line 2041, in load_state_dict\n    raise RuntimeError('Error(s)\
          \ in loading state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s)\
          \ in loading state_dict for UNet2DConditionModel:\n    size mismatch for\
          \ down_blocks.0.attentions.0.proj_in.weight: copying a param with shape\
          \ torch.Size([320, 320]) from checkpoint, the shape in current model is\
          \ torch.Size([320, 320, 1, 1]).\n    size mismatch for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for down_blocks.0.attentions.0.proj_out.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for down_blocks.0.attentions.1.proj_in.weight: copying a param with shape\
          \ torch.Size([320, 320]) from checkpoint, the shape in current model is\
          \ torch.Size([320, 320, 1, 1]).\n    size mismatch for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for down_blocks.0.attentions.1.proj_out.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for down_blocks.1.attentions.0.proj_in.weight: copying a param with shape\
          \ torch.Size([640, 640]) from checkpoint, the shape in current model is\
          \ torch.Size([640, 640, 1, 1]).\n    size mismatch for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for down_blocks.1.attentions.0.proj_out.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for down_blocks.1.attentions.1.proj_in.weight: copying a param with shape\
          \ torch.Size([640, 640]) from checkpoint, the shape in current model is\
          \ torch.Size([640, 640, 1, 1]).\n    size mismatch for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for down_blocks.1.attentions.1.proj_out.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for down_blocks.2.attentions.0.proj_in.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for down_blocks.2.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for down_blocks.2.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for down_blocks.2.attentions.1.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for up_blocks.1.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\n    size mismatch for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for up_blocks.1.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for up_blocks.1.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\n    size mismatch for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for up_blocks.1.attentions.1.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the\nshape in current model\
          \ is torch.Size([1280, 1280, 1, 1]).\n    size mismatch for up_blocks.1.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\n    size mismatch for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for up_blocks.1.attentions.2.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n    size mismatch for up_blocks.2.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ up_blocks.2.attentions.0.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\n    size mismatch for up_blocks.2.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ up_blocks.2.attentions.1.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\n    size mismatch for up_blocks.2.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\n    size mismatch\
          \ for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\n    size mismatch for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\n    size mismatch for\
          \ up_blocks.2.attentions.2.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\n    size mismatch for up_blocks.3.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ up_blocks.3.attentions.0.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\n    size mismatch for up_blocks.3.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ up_blocks.3.attentions.1.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\n    size mismatch for up_blocks.3.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\n    size mismatch\
          \ for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\n    size mismatch for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\n    size mismatch for\
          \ up_blocks.3.attentions.2.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\n    size mismatch for mid_block.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\n    size mismatch\
          \ for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\n    size mismatch for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\n    size mismatch\
          \ for mid_block.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\n\nglobal_step key not found in model\n\
          </code></pre>\n</details>\n\n<p>Did I miss something?</p>\n"
        raw: "Hi,\r\n\r\nI try to convert a dreambooth model based on SD 2.1.768 using\
          \ Guernika Model Converter but it fails\r\n\r\n<details>\r\n<summary>Logs</summary>\r\
          \n\r\n```\r\nStarting python converter\r\nInitializing StableDiffusionPipeline\
          \ from /Users/loranger/StableDiffusion/Models/custom-2-1-768.ckpt..\r\n\
          % Total    % Received % Xferd  Ave\r\nrage Speed   Time    Time     Time\
          \  Current\r\n                                 Dload  Upload   Total   Spent\
          \    Left  Speed\r\n\r\n  0\r\n0    0     0    0     0      0      0 --:--:--\
          \ --:--:-- --\r\n:--:--     0\r\n100  1873  100  1873    0     0   5872\
          \      0 --:--:-- --\r\n:--:-- --:--:--  5927\r\nTraceback (most recent\
          \ call last):\r\n  File \"python_coreml_stable_diffusion/torch2coreml.py\"\
          , line 1513, in <module>\r\nFile \"python_coreml_stable_diffusion/torch2coreml.py\"\
          , line 1281, in main\r\n  File \"diffusers/pipelines/stable_diffusion/convert_from_ckpt.py\"\
          , line 1164, in download_from_original_stable_diffusion_ckpt\r\n  File \"\
          torch/nn/modules/module.py\", line 2041, in load_state_dict\r\nRuntimeError:\
          \ Error(s) in loading state_dict for UNet2DConditionModel:\r\n\tsize mismatch\
          \ for down_blocks.0.attentions.0.proj_in.weight: copying a param with shape\
          \ torch.Size([320, 320]) from checkpoint, the shape in current model is\
          \ torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.0.proj_out.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.0.attentions.1.proj_in.weight: copying a param with shape\
          \ torch.Size([320, 320]) from checkpoint, the shape in current model is\
          \ torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.1.proj_out.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.1.attentions.0.proj_in.weight: copying a param with shape\
          \ torch.Size([640, 640]) from checkpoint, the shape in current model is\
          \ torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.0.proj_out.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.1.attentions.1.proj_in.weight: copying a param with shape\
          \ torch.Size([640, 640]) from checkpoint, the shape in current model is\
          \ torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.1.proj_out.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.0.proj_in.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.1.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.1.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.2.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ up_blocks.2.attentions.0.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ up_blocks.2.attentions.1.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ up_blocks.2.attentions.2.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ up_blocks.3.attentions.0.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ up_blocks.3.attentions.1.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ up_blocks.3.attentions.2.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\r\n\tsize mismatch for mid_block.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for mid_block.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n[13030] Failed to execute script 'torch2coreml'\
          \ due to unhandled exception: Error(s) in loading state_dict for UNet2DConditionModel:\r\
          \n\tsize mismatch for down_blocks.0.attentions.0.proj_in.weight: copying\
          \ a param with shape torch.Size([320, 320]) from checkpoint, the shape in\
          \ current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for\
          \ down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ down_blocks.0.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([320, 320]) from checkpoint, the shape in current model is\
          \ torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.1.proj_out.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.1.attentions.0.proj_in.weight: copying a param with shape\
          \ torch.Size([640, 640]) from checkpoint, the shape in current model is\
          \ torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.0.proj_out.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.1.attentions.1.proj_in.weight: copying a param with shape\
          \ torch.Size([640, 640]) from checkpoint, the shape in current model is\
          \ torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.1.proj_out.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for down_bl\r\nocks.2.attentions.0.proj_in.weight: copying a param with\
          \ shape torch.Size([1280, 1280]) from checkpoint, the shape in current model\
          \ is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.1.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.1.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.2.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ up_blocks.2.attentions.0.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ up_blocks.2.attentions.1.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ up_blocks.2.attentions.2.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ up_blocks.3.attentions.0.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ up_blocks.3.attentions.1.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ up_blocks.3.attentions.2.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\r\n\tsize mismatch for mid_block.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for mid_block.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n[13030] Traceback:\r\nTraceback (most\
          \ recent call last):\r\n  File \"python_coreml_stable_diffusion/torch2coreml.py\"\
          , line 1513, in <module>\r\n  File \"python_coreml_stable_diffusion/torch2coreml.py\"\
          , line 1281, in main\r\n  File \"diffusers/pipelines/stable_diffusion/convert_from_ckpt.py\"\
          , line 1164, in download_from_original_stable_diffusion_ckpt\r\n  File \"\
          torch/nn/modules/module.py\", line 2041, in load_state_dict\r\n    raise\
          \ RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\
          \nRuntimeError: Error(s) in loading state_dict for UNet2DConditionModel:\r\
          \n\tsize mismatch for down_blocks.0.attentions.0.proj_in.weight: copying\
          \ a param with shape torch.Size([320, 320]) from checkpoint, the shape in\
          \ current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for\
          \ down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ down_blocks.0.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([320, 320]) from checkpoint, the shape in current model is\
          \ torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.1.proj_out.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.1.attentions.0.proj_in.weight: copying a param with shape\
          \ torch.Size([640, 640]) from checkpoint, the shape in current model is\
          \ torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.0.proj_out.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.1.attentions.1.proj_in.weight: copying a param with shape\
          \ torch.Size([640, 640]) from checkpoint, the shape in current model is\
          \ torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.1.proj_out.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.0.proj_in.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for down_blocks.2.attentions.1.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.1.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the\r\nshape in current model\
          \ is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for up_blocks.1.attentions.2.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ up_blocks.2.attentions.0.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ up_blocks.2.attentions.1.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([640, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([640, 768]).\r\n\tsize mismatch for\
          \ up_blocks.2.attentions.2.proj_out.weight: copying a param with shape torch.Size([640,\
          \ 640]) from checkpoint, the shape in current model is torch.Size([640,\
          \ 640, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ up_blocks.3.attentions.0.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.1.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ up_blocks.3.attentions.1.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.2.proj_in.weight:\
          \ copying a param with shape torch.Size([320, 320]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch\
          \ for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([320, 768]).\r\n\tsize mismatch for\
          \ up_blocks.3.attentions.2.proj_out.weight: copying a param with shape torch.Size([320,\
          \ 320]) from checkpoint, the shape in current model is torch.Size([320,\
          \ 320, 1, 1]).\r\n\tsize mismatch for mid_block.attentions.0.proj_in.weight:\
          \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch\
          \ for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying\
          \ a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
          \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
          \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([1280, 768]).\r\n\tsize mismatch\
          \ for mid_block.attentions.0.proj_out.weight: copying a param with shape\
          \ torch.Size([1280, 1280]) from checkpoint, the shape in current model is\
          \ torch.Size([1280, 1280, 1, 1]).\r\n\r\nglobal_step key not found in model\r\
          \n```\r\n\r\n</details>\r\n\r\nDid I miss something?"
        updatedAt: '2023-06-12T21:36:21.952Z'
      numEdits: 0
      reactions: []
    id: 64878fd561be6ad0fabede1f
    type: comment
  author: loranger
  content: "Hi,\r\n\r\nI try to convert a dreambooth model based on SD 2.1.768 using\
    \ Guernika Model Converter but it fails\r\n\r\n<details>\r\n<summary>Logs</summary>\r\
    \n\r\n```\r\nStarting python converter\r\nInitializing StableDiffusionPipeline\
    \ from /Users/loranger/StableDiffusion/Models/custom-2-1-768.ckpt..\r\n% Total\
    \    % Received % Xferd  Ave\r\nrage Speed   Time    Time     Time  Current\r\n\
    \                                 Dload  Upload   Total   Spent    Left  Speed\r\
    \n\r\n  0\r\n0    0     0    0     0      0      0 --:--:-- --:--:-- --\r\n:--:--\
    \     0\r\n100  1873  100  1873    0     0   5872      0 --:--:-- --\r\n:--:--\
    \ --:--:--  5927\r\nTraceback (most recent call last):\r\n  File \"python_coreml_stable_diffusion/torch2coreml.py\"\
    , line 1513, in <module>\r\nFile \"python_coreml_stable_diffusion/torch2coreml.py\"\
    , line 1281, in main\r\n  File \"diffusers/pipelines/stable_diffusion/convert_from_ckpt.py\"\
    , line 1164, in download_from_original_stable_diffusion_ckpt\r\n  File \"torch/nn/modules/module.py\"\
    , line 2041, in load_state_dict\r\nRuntimeError: Error(s) in loading state_dict\
    \ for UNet2DConditionModel:\r\n\tsize mismatch for down_blocks.0.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for down_blocks.2.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for down_blocks.2.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.2.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.2.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.2.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.2.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.2.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.2.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for mid_block.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for mid_block.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n[13030] Failed to execute\
    \ script 'torch2coreml' due to unhandled exception: Error(s) in loading state_dict\
    \ for UNet2DConditionModel:\r\n\tsize mismatch for down_blocks.0.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_bl\r\
    \nocks.2.attentions.0.proj_in.weight: copying a param with shape torch.Size([1280,\
    \ 1280]) from checkpoint, the shape in current model is torch.Size([1280, 1280,\
    \ 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for down_blocks.2.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for down_blocks.2.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.2.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.2.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.2.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.2.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.2.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.2.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for mid_block.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for mid_block.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n[13030] Traceback:\r\n\
    Traceback (most recent call last):\r\n  File \"python_coreml_stable_diffusion/torch2coreml.py\"\
    , line 1513, in <module>\r\n  File \"python_coreml_stable_diffusion/torch2coreml.py\"\
    , line 1281, in main\r\n  File \"diffusers/pipelines/stable_diffusion/convert_from_ckpt.py\"\
    , line 1164, in download_from_original_stable_diffusion_ckpt\r\n  File \"torch/nn/modules/module.py\"\
    , line 2041, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading\
    \ state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict\
    \ for UNet2DConditionModel:\r\n\tsize mismatch for down_blocks.0.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for down_blocks.0.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for down_blocks.1.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for down_blocks.2.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for down_blocks.2.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the\r\n\
    shape in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for\
    \ up_blocks.1.attentions.2.proj_in.weight: copying a param with shape torch.Size([1280,\
    \ 1280]) from checkpoint, the shape in current model is torch.Size([1280, 1280,\
    \ 1, 1]).\r\n\tsize mismatch for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for up_blocks.1.attentions.2.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.2.proj_in.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 768]).\r\n\tsize mismatch for up_blocks.2.attentions.2.proj_out.weight:\
    \ copying a param with shape torch.Size([640, 640]) from checkpoint, the shape\
    \ in current model is torch.Size([640, 640, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.1.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.1.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.2.proj_in.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 768]).\r\n\tsize mismatch for up_blocks.3.attentions.2.proj_out.weight:\
    \ copying a param with shape torch.Size([320, 320]) from checkpoint, the shape\
    \ in current model is torch.Size([320, 320, 1, 1]).\r\n\tsize mismatch for mid_block.attentions.0.proj_in.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\tsize mismatch for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight:\
    \ copying a param with shape torch.Size([1280, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 768]).\r\n\tsize mismatch for mid_block.attentions.0.proj_out.weight:\
    \ copying a param with shape torch.Size([1280, 1280]) from checkpoint, the shape\
    \ in current model is torch.Size([1280, 1280, 1, 1]).\r\n\r\nglobal_step key not\
    \ found in model\r\n```\r\n\r\n</details>\r\n\r\nDid I miss something?"
  created_at: 2023-06-12 20:36:21+00:00
  edited: false
  hidden: false
  id: 64878fd561be6ad0fabede1f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: Guernika/CoreMLStableDiffusion
repo_type: model
status: open
target_branch: null
title: Can't convert SD 2.1 checkpoint
