!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yumeshiro
conflicting_files: null
created_at: 2023-11-07 04:05:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/tz5GwJAcU9lt696CA4cFU.png?w=200&h=200&f=face
      fullname: noname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yumeshiro
      type: user
    createdAt: '2023-11-07T04:05:16.000Z'
    data:
      edited: true
      editors:
      - yumeshiro
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.38706114888191223
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/tz5GwJAcU9lt696CA4cFU.png?w=200&h=200&f=face
          fullname: noname
          isHf: false
          isPro: false
          name: yumeshiro
          type: user
        html: '<p>I and others that I''ve seen have the following error when trying
          to load this model with Oobabooga. I get the error regardless of what type
          quantization for the GGUF I try and load.</p>

          <p>ERROR:Failed to load the model.<br>Traceback (most recent call last):<br>  File
          "D:\0\Oobabooga2\modules\ui_model_menu.py", line 206, in load_model_wrapper<br>    shared.model,
          shared.tokenizer = load_model(shared.model_name, loader)<br>                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "D:\0\Oobabooga2\modules\models.py", line 84, in load_model<br>    output
          = load_func_map<a rel="nofollow" href="model_name">loader</a><br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "D:\0\Oobabooga2\modules\models.py", line 235, in llamacpp_loader<br>    model,
          tokenizer = LlamaCppModel.from_pretrained(model_file)<br>                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "D:\0\Oobabooga2\modules\llamacpp_model.py", line 91, in from_pretrained<br>    result.model
          = Llama(**params)<br>                   ^^^^^^^^^^^^^^^<br>  File "D:\0\Oobabooga2\installer_files\env\Lib\site-packages\llama_cpp_cuda\llama.py",
          line 357, in <strong>init</strong><br>    self.model = llama_cpp.llama_load_model_from_file(<br>                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "D:\0\Oobabooga2\installer_files\env\Lib\site-packages\llama_cpp_cuda\llama_cpp.py",
          line 498, in llama_load_model_from_file<br>    return _lib.llama_load_model_from_file(path_model,
          params)<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>OSError:
          exception: access violation reading 0x0000000000000000</p>

          <p>Exception ignored in: &lt;function LlamaCppModel.__del__ at 0x000002DD709C6FC0&gt;<br>Traceback
          (most recent call last):<br>  File "D:\0\Oobabooga2\modules\llamacpp_model.py",
          line 49, in <strong>del</strong><br>    self.model.<strong>del</strong>()<br>    ^^^^^^^^^^<br>AttributeError:
          ''LlamaCppModel'' object has no attribute ''model''</p>

          <p>This happens on a new and up-to-date Oobabooga. I myself am running a
          3080 10gb, am on Windows 10, and all of my other models work fine.</p>

          '
        raw: "I and others that I've seen have the following error when trying to\
          \ load this model with Oobabooga. I get the error regardless of what type\
          \ quantization for the GGUF I try and load.\n\nERROR:Failed to load the\
          \ model.\nTraceback (most recent call last):\n  File \"D:\\0\\Oobabooga2\\\
          modules\\ui_model_menu.py\", line 206, in load_model_wrapper\n    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name, loader)\n           \
          \                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \"D:\\0\\Oobabooga2\\modules\\models.py\", line 84, in load_model\n  \
          \  output = load_func_map[loader](model_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"D:\\0\\Oobabooga2\\modules\\models.py\", line 235, in llamacpp_loader\n\
          \    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n    \
          \                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
          D:\\0\\Oobabooga2\\modules\\llamacpp_model.py\", line 91, in from_pretrained\n\
          \    result.model = Llama(**params)\n                   ^^^^^^^^^^^^^^^\n\
          \  File \"D:\\0\\Oobabooga2\\installer_files\\env\\Lib\\site-packages\\\
          llama_cpp_cuda\\llama.py\", line 357, in __init__\n    self.model = llama_cpp.llama_load_model_from_file(\n\
          \                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\0\\\
          Oobabooga2\\installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\llama_cpp.py\"\
          , line 498, in llama_load_model_from_file\n    return _lib.llama_load_model_from_file(path_model,\
          \ params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          OSError: exception: access violation reading 0x0000000000000000\n\nException\
          \ ignored in: <function LlamaCppModel.__del__ at 0x000002DD709C6FC0>\nTraceback\
          \ (most recent call last):\n  File \"D:\\0\\Oobabooga2\\modules\\llamacpp_model.py\"\
          , line 49, in __del__\n    self.model.__del__()\n    ^^^^^^^^^^\nAttributeError:\
          \ 'LlamaCppModel' object has no attribute 'model'\n\nThis happens on a new\
          \ and up-to-date Oobabooga. I myself am running a 3080 10gb, am on Windows\
          \ 10, and all of my other models work fine."
        updatedAt: '2023-11-07T04:05:31.358Z'
      numEdits: 1
      reactions: []
    id: 6549b77c6299d816639cecac
    type: comment
  author: yumeshiro
  content: "I and others that I've seen have the following error when trying to load\
    \ this model with Oobabooga. I get the error regardless of what type quantization\
    \ for the GGUF I try and load.\n\nERROR:Failed to load the model.\nTraceback (most\
    \ recent call last):\n  File \"D:\\0\\Oobabooga2\\modules\\ui_model_menu.py\"\
    , line 206, in load_model_wrapper\n    shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"D:\\0\\Oobabooga2\\modules\\models.py\", line 84, in load_model\n  \
    \  output = load_func_map[loader](model_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"D:\\0\\Oobabooga2\\modules\\models.py\", line 235, in llamacpp_loader\n\
    \    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\n          \
    \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\0\\Oobabooga2\\\
    modules\\llamacpp_model.py\", line 91, in from_pretrained\n    result.model =\
    \ Llama(**params)\n                   ^^^^^^^^^^^^^^^\n  File \"D:\\0\\Oobabooga2\\\
    installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 357,\
    \ in __init__\n    self.model = llama_cpp.llama_load_model_from_file(\n      \
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\0\\Oobabooga2\\\
    installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\llama_cpp.py\", line\
    \ 498, in llama_load_model_from_file\n    return _lib.llama_load_model_from_file(path_model,\
    \ params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError:\
    \ exception: access violation reading 0x0000000000000000\n\nException ignored\
    \ in: <function LlamaCppModel.__del__ at 0x000002DD709C6FC0>\nTraceback (most\
    \ recent call last):\n  File \"D:\\0\\Oobabooga2\\modules\\llamacpp_model.py\"\
    , line 49, in __del__\n    self.model.__del__()\n    ^^^^^^^^^^\nAttributeError:\
    \ 'LlamaCppModel' object has no attribute 'model'\n\nThis happens on a new and\
    \ up-to-date Oobabooga. I myself am running a 3080 10gb, am on Windows 10, and\
    \ all of my other models work fine."
  created_at: 2023-11-07 04:05:16+00:00
  edited: true
  hidden: false
  id: 6549b77c6299d816639cecac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/tz5GwJAcU9lt696CA4cFU.png?w=200&h=200&f=face
      fullname: noname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yumeshiro
      type: user
    createdAt: '2023-11-07T04:18:38.000Z'
    data:
      from: 'AttributeError: ''LlamaCppModel'' object has no attribute ''model'''
      to: 'Oobabooga: "AttributeError: ''LlamaCppModel'' object has no attribute ''model''"'
    id: 6549ba9ea4635977148b41c6
    type: title-change
  author: yumeshiro
  created_at: 2023-11-07 04:18:38+00:00
  id: 6549ba9ea4635977148b41c6
  new_title: 'Oobabooga: "AttributeError: ''LlamaCppModel'' object has no attribute
    ''model''"'
  old_title: 'AttributeError: ''LlamaCppModel'' object has no attribute ''model'''
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1c1d1bd0d729c6355145a55e4e0d46f.svg
      fullname: tastypear
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tastypear
      type: user
    createdAt: '2023-11-25T06:44:14.000Z'
    data:
      edited: false
      editors:
      - tastypear
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8863988518714905
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1c1d1bd0d729c6355145a55e4e0d46f.svg
          fullname: tastypear
          isHf: false
          isPro: false
          name: tastypear
          type: user
        html: '<p>It''s the issue of Oobabooga. The latest llama.cpp-python can load
          it with no error. Maybe Oobabooga is using the old one.<br>btw, the new
          version called CausalLM-14B-DPO-alpha has relesed.</p>

          '
        raw: 'It''s the issue of Oobabooga. The latest llama.cpp-python can load it
          with no error. Maybe Oobabooga is using the old one.

          btw, the new version called CausalLM-14B-DPO-alpha has relesed.'
        updatedAt: '2023-11-25T06:44:14.380Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - f1yingbanana
    id: 656197be4e8918182d76b7f1
    type: comment
  author: tastypear
  content: 'It''s the issue of Oobabooga. The latest llama.cpp-python can load it
    with no error. Maybe Oobabooga is using the old one.

    btw, the new version called CausalLM-14B-DPO-alpha has relesed.'
  created_at: 2023-11-25 06:44:14+00:00
  edited: false
  hidden: false
  id: 656197be4e8918182d76b7f1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/CausalLM-14B-GGUF
repo_type: model
status: open
target_branch: null
title: 'Oobabooga: "AttributeError: ''LlamaCppModel'' object has no attribute ''model''"'
