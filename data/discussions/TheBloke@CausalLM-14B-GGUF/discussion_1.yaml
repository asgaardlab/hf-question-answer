!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheYuriLover
conflicting_files: null
created_at: 2023-10-22 20:51:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-10-22T21:51:47.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7824255228042603
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>Title</p>

          '
        raw: Title
        updatedAt: '2023-10-22T21:51:47.165Z'
      numEdits: 0
      reactions: []
    id: 65359973e983fb23fa5d8b32
    type: comment
  author: TheYuriLover
  content: Title
  created_at: 2023-10-22 20:51:47+00:00
  edited: false
  hidden: false
  id: 65359973e983fb23fa5d8b32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
      fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KerfuffleV2
      type: user
    createdAt: '2023-10-23T01:52:46.000Z'
    data:
      edited: true
      editors:
      - KerfuffleV2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.96645587682724
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/4j4M_alYew0CbD7wn2zo5.jpeg?w=200&h=200&f=face
          fullname: Kerfuffle V. II, Esq, Ltd, all rights reserved
          isHf: false
          isPro: false
          name: KerfuffleV2
          type: user
        html: '<p>The tensors have to be a multiple of the k-quants block size to
          use k-quants. LLaMA models usually fit that requirements, but the 14B here
          doesn''t. (Technically there''s a way to use k-quants anyway but it requires
          compiling with a special flag to quantize and load the models and you lose
          some of the advantage of k-quants that way also.)<br><del>There is also
          another issue with the conversion where the BPE merges didn''t get added
          to the GGUF files (both 7b and 14b as far as I know) so you can''t load
          the models. This is not TB''s fault. But I suggest waiting for a fixed version
          before trying to download them. Associated GitHub issue: <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/issues/3732">https://github.com/ggerganov/llama.cpp/issues/3732</a></del>
          <em>edit: Should be fixed now.</em></p>

          '
        raw: 'The tensors have to be a multiple of the k-quants block size to use
          k-quants. LLaMA models usually fit that requirements, but the 14B here doesn''t.
          (Technically there''s a way to use k-quants anyway but it requires compiling
          with a special flag to quantize and load the models and you lose some of
          the advantage of k-quants that way also.)

          ~~There is also another issue with the conversion where the BPE merges didn''t
          get added to the GGUF files (both 7b and 14b as far as I know) so you can''t
          load the models. This is not TB''s fault. But I suggest waiting for a fixed
          version before trying to download them. Associated GitHub issue: https://github.com/ggerganov/llama.cpp/issues/3732~~
          *edit: Should be fixed now.*'
        updatedAt: '2023-10-23T14:19:51.542Z'
      numEdits: 2
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - mufeed
        - James3691
        - PrimeD
    id: 6535d1ee3da0ff3c70bce121
    type: comment
  author: KerfuffleV2
  content: 'The tensors have to be a multiple of the k-quants block size to use k-quants.
    LLaMA models usually fit that requirements, but the 14B here doesn''t. (Technically
    there''s a way to use k-quants anyway but it requires compiling with a special
    flag to quantize and load the models and you lose some of the advantage of k-quants
    that way also.)

    ~~There is also another issue with the conversion where the BPE merges didn''t
    get added to the GGUF files (both 7b and 14b as far as I know) so you can''t load
    the models. This is not TB''s fault. But I suggest waiting for a fixed version
    before trying to download them. Associated GitHub issue: https://github.com/ggerganov/llama.cpp/issues/3732~~
    *edit: Should be fixed now.*'
  created_at: 2023-10-23 00:52:46+00:00
  edited: true
  hidden: false
  id: 6535d1ee3da0ff3c70bce121
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d6860a551de0d4912e08e64589921dc.svg
      fullname: John Steward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HDiffusion
      type: user
    createdAt: '2023-10-23T15:38:52.000Z'
    data:
      edited: false
      editors:
      - HDiffusion
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.863067090511322
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d6860a551de0d4912e08e64589921dc.svg
          fullname: John Steward
          isHf: false
          isPro: false
          name: HDiffusion
          type: user
        html: '<p>There''s a pr to change this behavior <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/3747">https://github.com/ggerganov/llama.cpp/pull/3747</a></p>

          '
        raw: 'There''s a pr to change this behavior https://github.com/ggerganov/llama.cpp/pull/3747

          '
        updatedAt: '2023-10-23T15:38:52.313Z'
      numEdits: 0
      reactions: []
    id: 6536938c62dd8126cf75bd2f
    type: comment
  author: HDiffusion
  content: 'There''s a pr to change this behavior https://github.com/ggerganov/llama.cpp/pull/3747

    '
  created_at: 2023-10-23 14:38:52+00:00
  edited: false
  hidden: false
  id: 6536938c62dd8126cf75bd2f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/CausalLM-14B-GGUF
repo_type: model
status: open
target_branch: null
title: No Q_K quants?
