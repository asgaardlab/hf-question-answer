!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 0mij
conflicting_files: null
created_at: 2023-07-25 02:36:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4629cf01028c58665bf44937b6f3cf2d.svg
      fullname: omij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0mij
      type: user
    createdAt: '2023-07-25T03:36:02.000Z'
    data:
      edited: false
      editors:
      - 0mij
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7140969634056091
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4629cf01028c58665bf44937b6f3cf2d.svg
          fullname: omij
          isHf: false
          isPro: false
          name: 0mij
          type: user
        html: '<p>have been trying to fine-tune the llama using alpaca-lora repo,
          i am able to train it without any error.<br>The issue is after training
          the adapter_model.bin is just 443 bytes, here are the commands that i used
          for training</p>

          <pre><code class="language-bash">python finetune.py --base_model <span class="hljs-string">''decapoda-research/llama-7b-hf''</span>
          --data_path <span class="hljs-string">''./alpaca_data_2k.json''</span> --output_dir
          <span class="hljs-string">''./lora-alpaca-2k-6h''</span> --val_set_size
          256 --num_epochs=10 --cutoff_len=512 --group_by_length --lora_target_modules=<span
          class="hljs-string">''[q_proj,k_proj,v_proj,o_proj]''</span> --lora_r=16
          --micro_batch_size=8

          </code></pre>

          <p>explanation:</p>

          <ul>

          <li>so this command is to train it in slurm based cluster (my uni. cluster).
          and i have generic script that accepts the pyenv and pyscript to execute
          after activating that env.</li>

          <li>also the data i have sampled 2k examples from alpaca.json</li>

          <li>i tried to scroll through finetune.py but couldn''t get what''s missing.</li>

          </ul>

          <p>would really appriciate if anyone can point out the missing argument
          or command. </p>

          <p>link to my hf model: <code>0mij/llama-v1-alpaca-2k</code><br>the only
          difference i saw is peft version.</p>

          '
        raw: "have been trying to fine-tune the llama using alpaca-lora repo, i am\
          \ able to train it without any error. \r\nThe issue is after training the\
          \ adapter_model.bin is just 443 bytes, here are the commands that i used\
          \ for training\r\n```bash\r\npython finetune.py --base_model 'decapoda-research/llama-7b-hf'\
          \ --data_path './alpaca_data_2k.json' --output_dir './lora-alpaca-2k-6h'\
          \ --val_set_size 256 --num_epochs=10 --cutoff_len=512 --group_by_length\
          \ --lora_target_modules='[q_proj,k_proj,v_proj,o_proj]' --lora_r=16 --micro_batch_size=8\r\
          \n```\r\n\r\nexplanation:\r\n- so this command is to train it in slurm based\
          \ cluster (my uni. cluster). and i have generic script that accepts the\
          \ pyenv and pyscript to execute after activating that env.\r\n- also the\
          \ data i have sampled 2k examples from alpaca.json\r\n- i tried to scroll\
          \ through finetune.py but couldn't get what's missing.\r\n\r\nwould really\
          \ appriciate if anyone can point out the missing argument or command. \r\
          \n\r\nlink to my hf model: `0mij/llama-v1-alpaca-2k`\r\nthe only difference\
          \ i saw is peft version."
        updatedAt: '2023-07-25T03:36:02.148Z'
      numEdits: 0
      reactions: []
    id: 64bf4322f671da974e6cdd6e
    type: comment
  author: 0mij
  content: "have been trying to fine-tune the llama using alpaca-lora repo, i am able\
    \ to train it without any error. \r\nThe issue is after training the adapter_model.bin\
    \ is just 443 bytes, here are the commands that i used for training\r\n```bash\r\
    \npython finetune.py --base_model 'decapoda-research/llama-7b-hf' --data_path\
    \ './alpaca_data_2k.json' --output_dir './lora-alpaca-2k-6h' --val_set_size 256\
    \ --num_epochs=10 --cutoff_len=512 --group_by_length --lora_target_modules='[q_proj,k_proj,v_proj,o_proj]'\
    \ --lora_r=16 --micro_batch_size=8\r\n```\r\n\r\nexplanation:\r\n- so this command\
    \ is to train it in slurm based cluster (my uni. cluster). and i have generic\
    \ script that accepts the pyenv and pyscript to execute after activating that\
    \ env.\r\n- also the data i have sampled 2k examples from alpaca.json\r\n- i tried\
    \ to scroll through finetune.py but couldn't get what's missing.\r\n\r\nwould\
    \ really appriciate if anyone can point out the missing argument or command. \r\
    \n\r\nlink to my hf model: `0mij/llama-v1-alpaca-2k`\r\nthe only difference i\
    \ saw is peft version."
  created_at: 2023-07-25 02:36:02+00:00
  edited: false
  hidden: false
  id: 64bf4322f671da974e6cdd6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d259158e6a82b38e4ec3d3fb71295129.svg
      fullname: Shenchen Zhu
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Creling
      type: user
    createdAt: '2023-07-25T03:59:53.000Z'
    data:
      edited: false
      editors:
      - Creling
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6735475659370422
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d259158e6a82b38e4ec3d3fb71295129.svg
          fullname: Shenchen Zhu
          isHf: false
          isPro: false
          name: Creling
          type: user
        html: '<p>Well, I just wonder how you could find this repo? I mean, I just
          create it minutes ago...</p>

          <p>For your problem: firstly, the parameters shown in the command line do
          not match the parameters in the adapter_config.json, so does the adapter_model.bin
          in <code>0mij/llama-v1-alpaca-2k</code> is actually created by the command
          you mentioned above?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644f3305722da4dfd874cd81/j-AMGH0ny9ORmSxrv_Gmk.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644f3305722da4dfd874cd81/j-AMGH0ny9ORmSxrv_Gmk.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644f3305722da4dfd874cd81/_KgHjv43NLGP6Jsa7vOjy.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644f3305722da4dfd874cd81/_KgHjv43NLGP6Jsa7vOjy.png"></a></p>

          '
        raw: 'Well, I just wonder how you could find this repo? I mean, I just create
          it minutes ago...


          For your problem: firstly, the parameters shown in the command line do not
          match the parameters in the adapter_config.json, so does the adapter_model.bin
          in `0mij/llama-v1-alpaca-2k` is actually created by the command you mentioned
          above?



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644f3305722da4dfd874cd81/j-AMGH0ny9ORmSxrv_Gmk.png)


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644f3305722da4dfd874cd81/_KgHjv43NLGP6Jsa7vOjy.png)


          '
        updatedAt: '2023-07-25T03:59:53.809Z'
      numEdits: 0
      reactions: []
    id: 64bf48b96999b520ed9e6101
    type: comment
  author: Creling
  content: 'Well, I just wonder how you could find this repo? I mean, I just create
    it minutes ago...


    For your problem: firstly, the parameters shown in the command line do not match
    the parameters in the adapter_config.json, so does the adapter_model.bin in `0mij/llama-v1-alpaca-2k`
    is actually created by the command you mentioned above?



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644f3305722da4dfd874cd81/j-AMGH0ny9ORmSxrv_Gmk.png)


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644f3305722da4dfd874cd81/_KgHjv43NLGP6Jsa7vOjy.png)


    '
  created_at: 2023-07-25 02:59:53+00:00
  edited: false
  hidden: false
  id: 64bf48b96999b520ed9e6101
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4629cf01028c58665bf44937b6f3cf2d.svg
      fullname: omij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0mij
      type: user
    createdAt: '2023-07-25T04:05:31.000Z'
    data:
      edited: false
      editors:
      - 0mij
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9854045510292053
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4629cf01028c58665bf44937b6f3cf2d.svg
          fullname: omij
          isHf: false
          isPro: false
          name: 0mij
          type: user
        html: '<p>thanks for the reply, yes i forgot to mention that the model i pushed
          to hf, was trained using default command, which does have these 2 modules
          as target. the command i shared was new one, because like you i also thought
          that this might be problem. i have downgraded peft version so it''s in training,
          will get back here if it resolves.<br>thanks!</p>

          '
        raw: "thanks for the reply, yes i forgot to mention that the model i pushed\
          \ to hf, was trained using default command, which does have these 2 modules\
          \ as target. the command i shared was new one, because like you i also thought\
          \ that this might be problem. i have downgraded peft version so it's in\
          \ training, will get back here if it resolves. \nthanks!"
        updatedAt: '2023-07-25T04:05:31.429Z'
      numEdits: 0
      reactions: []
    id: 64bf4a0b8496ee0fb6371df4
    type: comment
  author: 0mij
  content: "thanks for the reply, yes i forgot to mention that the model i pushed\
    \ to hf, was trained using default command, which does have these 2 modules as\
    \ target. the command i shared was new one, because like you i also thought that\
    \ this might be problem. i have downgraded peft version so it's in training, will\
    \ get back here if it resolves. \nthanks!"
  created_at: 2023-07-25 03:05:31+00:00
  edited: false
  hidden: false
  id: 64bf4a0b8496ee0fb6371df4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4629cf01028c58665bf44937b6f3cf2d.svg
      fullname: omij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0mij
      type: user
    createdAt: '2023-07-25T04:10:27.000Z'
    data:
      edited: false
      editors:
      - 0mij
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.980981707572937
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4629cf01028c58665bf44937b6f3cf2d.svg
          fullname: omij
          isHf: false
          isPro: false
          name: 0mij
          type: user
        html: '<p>regarding your curiosity about finding this repo. i looked into
          all the lora weights for llama model on hf, but they all were old, so i
          searched using filter and you just pushed the weights, i compared with your
          adaperconfig.json and just created discussion, i also saw someone else with
          peft0.5dev was having only 443bytes .bin file similar to me. </p>

          '
        raw: 'regarding your curiosity about finding this repo. i looked into all
          the lora weights for llama model on hf, but they all were old, so i searched
          using filter and you just pushed the weights, i compared with your adaperconfig.json
          and just created discussion, i also saw someone else with peft0.5dev was
          having only 443bytes .bin file similar to me. '
        updatedAt: '2023-07-25T04:10:27.766Z'
      numEdits: 0
      reactions: []
    id: 64bf4b336999b520ed9eb802
    type: comment
  author: 0mij
  content: 'regarding your curiosity about finding this repo. i looked into all the
    lora weights for llama model on hf, but they all were old, so i searched using
    filter and you just pushed the weights, i compared with your adaperconfig.json
    and just created discussion, i also saw someone else with peft0.5dev was having
    only 443bytes .bin file similar to me. '
  created_at: 2023-07-25 03:10:27+00:00
  edited: false
  hidden: false
  id: 64bf4b336999b520ed9eb802
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d259158e6a82b38e4ec3d3fb71295129.svg
      fullname: Shenchen Zhu
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Creling
      type: user
    createdAt: '2023-07-25T05:52:17.000Z'
    data:
      edited: false
      editors:
      - Creling
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7411192655563354
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d259158e6a82b38e4ec3d3fb71295129.svg
          fullname: Shenchen Zhu
          isHf: false
          isPro: false
          name: Creling
          type: user
        html: "<p>After some digging, I find it a known issue on <a rel=\"nofollow\"\
          \ href=\"https://github.com/tloen/alpaca-lora\">https://github.com/tloen/alpaca-lora</a>.\
          \ To be specifical, tloen save the state_dict of the peft model in a tricky\
          \ way which may not work in the latest PEFT:</p>\n<pre><code class=\"language-python\"\
          >ref: https://github.com/tloen/alpaca-lora/blob/630d1146c8b5a968f5bf4f02f50f153a0c9d449d/finetune.py<span\
          \ class=\"hljs-comment\">#L256-L261</span>\n\nold_state_dict = model.state_dict\n\
          model.state_dict = (\n    <span class=\"hljs-keyword\">lambda</span> self,\
          \ *_, **__: get_peft_model_state_dict(\n        self, old_state_dict()\n\
          \    )\n).__get__(model, <span class=\"hljs-built_in\">type</span>(model))\n\
          </code></pre>\n<p>Please ref <a rel=\"nofollow\" href=\"https://github.com/huggingface/peft/issues/286#issuecomment-1501617281\"\
          >https://github.com/huggingface/peft/issues/286#issuecomment-1501617281</a>\
          \ for more details.</p>\n<p>By the way, I implement the Lora code on my\
          \ own instead of using tloen's  and I am 100% certain that the code above\
          \ can be replaced by </p>\n<pre><code>model.save_pretrained(\"./llama_lora_alpaca\"\
          )\n</code></pre>\n<p>Of course, downgrading PEFT can solve the problem as\
          \ well.</p>\n"
        raw: "After some digging, I find it a known issue on https://github.com/tloen/alpaca-lora.\
          \ To be specifical, tloen save the state_dict of the peft model in a tricky\
          \ way which may not work in the latest PEFT:\n\n```python\nref: https://github.com/tloen/alpaca-lora/blob/630d1146c8b5a968f5bf4f02f50f153a0c9d449d/finetune.py#L256-L261\n\
          \nold_state_dict = model.state_dict\nmodel.state_dict = (\n    lambda self,\
          \ *_, **__: get_peft_model_state_dict(\n        self, old_state_dict()\n\
          \    )\n).__get__(model, type(model))\n```\nPlease ref https://github.com/huggingface/peft/issues/286#issuecomment-1501617281\
          \ for more details.\n\nBy the way, I implement the Lora code on my own instead\
          \ of using tloen's  and I am 100% certain that the code above can be replaced\
          \ by \n```\nmodel.save_pretrained(\"./llama_lora_alpaca\")\n```\nOf course,\
          \ downgrading PEFT can solve the problem as well.\n"
        updatedAt: '2023-07-25T05:52:17.384Z'
      numEdits: 0
      reactions: []
    id: 64bf6311b567ae97c37335a1
    type: comment
  author: Creling
  content: "After some digging, I find it a known issue on https://github.com/tloen/alpaca-lora.\
    \ To be specifical, tloen save the state_dict of the peft model in a tricky way\
    \ which may not work in the latest PEFT:\n\n```python\nref: https://github.com/tloen/alpaca-lora/blob/630d1146c8b5a968f5bf4f02f50f153a0c9d449d/finetune.py#L256-L261\n\
    \nold_state_dict = model.state_dict\nmodel.state_dict = (\n    lambda self, *_,\
    \ **__: get_peft_model_state_dict(\n        self, old_state_dict()\n    )\n).__get__(model,\
    \ type(model))\n```\nPlease ref https://github.com/huggingface/peft/issues/286#issuecomment-1501617281\
    \ for more details.\n\nBy the way, I implement the Lora code on my own instead\
    \ of using tloen's  and I am 100% certain that the code above can be replaced\
    \ by \n```\nmodel.save_pretrained(\"./llama_lora_alpaca\")\n```\nOf course, downgrading\
    \ PEFT can solve the problem as well.\n"
  created_at: 2023-07-25 04:52:17+00:00
  edited: false
  hidden: false
  id: 64bf6311b567ae97c37335a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4629cf01028c58665bf44937b6f3cf2d.svg
      fullname: omij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0mij
      type: user
    createdAt: '2023-07-25T15:14:47.000Z'
    data:
      edited: false
      editors:
      - 0mij
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9825003743171692
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4629cf01028c58665bf44937b6f3cf2d.svg
          fullname: omij
          isHf: false
          isPro: false
          name: 0mij
          type: user
        html: '<p>Thanks a lot for your comment. I read the comments from the issue
          above, and its not the problem with peft, so downgrading didn''t worked.<br>Now,
          i will test by commenting this code block, and will see.</p>

          '
        raw: "Thanks a lot for your comment. I read the comments from the issue above,\
          \ and its not the problem with peft, so downgrading didn't worked. \nNow,\
          \ i will test by commenting this code block, and will see."
        updatedAt: '2023-07-25T15:14:47.563Z'
      numEdits: 0
      reactions: []
    id: 64bfe6e7a5dbf6d5b6b8a151
    type: comment
  author: 0mij
  content: "Thanks a lot for your comment. I read the comments from the issue above,\
    \ and its not the problem with peft, so downgrading didn't worked. \nNow, i will\
    \ test by commenting this code block, and will see."
  created_at: 2023-07-25 14:14:47+00:00
  edited: false
  hidden: false
  id: 64bfe6e7a5dbf6d5b6b8a151
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4629cf01028c58665bf44937b6f3cf2d.svg
      fullname: omij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0mij
      type: user
    createdAt: '2023-07-25T16:47:51.000Z'
    data:
      edited: false
      editors:
      - 0mij
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9781854152679443
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4629cf01028c58665bf44937b6f3cf2d.svg
          fullname: omij
          isHf: false
          isPro: false
          name: 0mij
          type: user
        html: "<p>it worked! thanks \U0001F60A</p>\n"
        raw: "it worked! thanks \U0001F60A"
        updatedAt: '2023-07-25T16:47:51.088Z'
      numEdits: 0
      reactions: []
    id: 64bffcb767cdd7f4d58065af
    type: comment
  author: 0mij
  content: "it worked! thanks \U0001F60A"
  created_at: 2023-07-25 15:47:51+00:00
  edited: false
  hidden: false
  id: 64bffcb767cdd7f4d58065af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/4629cf01028c58665bf44937b6f3cf2d.svg
      fullname: omij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0mij
      type: user
    createdAt: '2023-07-25T16:47:54.000Z'
    data:
      status: closed
    id: 64bffcba294fd4d20d130886
    type: status-change
  author: 0mij
  created_at: 2023-07-25 15:47:54+00:00
  id: 64bffcba294fd4d20d130886
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Creling/llama7b-lora-alpaca
repo_type: model
status: closed
target_branch: null
title: PEFT version
