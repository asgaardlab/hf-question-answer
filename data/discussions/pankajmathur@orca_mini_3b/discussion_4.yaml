!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ibraweeb
conflicting_files: null
created_at: 2023-07-04 07:59:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85598e42cd28168a1dda59149e9c5c79.svg
      fullname: Ibrahim Junaid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ibraweeb
      type: user
    createdAt: '2023-07-04T08:59:07.000Z'
    data:
      edited: false
      editors:
      - ibraweeb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8438971638679504
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85598e42cd28168a1dda59149e9c5c79.svg
          fullname: Ibrahim Junaid
          isHf: false
          isPro: false
          name: ibraweeb
          type: user
        html: '<p>I''m currently trying to use the orca mini 3b and getting the Cuda
          out of memory error. The error line says the following:<br>OutOfMemoryError:
          CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 8.00 GiB total
          capacity; 6.65 GiB already allocated; 0 bytes free; 7.05 GiB reserved in
          total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>This is strange in my case as I was easily able to use the model a while
          ago.</p>

          <p>This is the cell where I''m importing the model:<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/648ea494c20f218a8cc55f76/tUSVYrZG6pWJa2B0hFv4F.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/648ea494c20f218a8cc55f76/tUSVYrZG6pWJa2B0hFv4F.png"></a></p>

          <p>This is the standard generate text function I''m using with the default
          prompt to write a letter:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/648ea494c20f218a8cc55f76/V4S0YPZsP5ioCSRMpvKxi.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/648ea494c20f218a8cc55f76/V4S0YPZsP5ioCSRMpvKxi.png"></a></p>

          <p>Instead of an output I get the error:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/648ea494c20f218a8cc55f76/hDyK3VAGa9qXVZPypGjrl.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/648ea494c20f218a8cc55f76/hDyK3VAGa9qXVZPypGjrl.png"></a></p>

          <p>Usually when people get errors like this a common solution is to reduce
          the batch size. I''m not exactly sure how to reduce the batch size here
          as I don''t see any parameters regarding it. I am also new to using huggingface/transformers
          so if anyone has any suggestions, it would be much appreciated.</p>

          '
        raw: "I'm currently trying to use the orca mini 3b and getting the Cuda out\
          \ of memory error. The error line says the following:\r\nOutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 8.00 GiB total\
          \ capacity; 6.65 GiB already allocated; 0 bytes free; 7.05 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\nThis is strange in my case\
          \ as I was easily able to use the model a while ago.\r\n\r\nThis is the\
          \ cell where I'm importing the model:\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/648ea494c20f218a8cc55f76/tUSVYrZG6pWJa2B0hFv4F.png)\r\
          \n\r\nThis is the standard generate text function I'm using with the default\
          \ prompt to write a letter:\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/648ea494c20f218a8cc55f76/V4S0YPZsP5ioCSRMpvKxi.png)\r\
          \n\r\nInstead of an output I get the error:\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/648ea494c20f218a8cc55f76/hDyK3VAGa9qXVZPypGjrl.png)\r\
          \n\r\nUsually when people get errors like this a common solution is to reduce\
          \ the batch size. I'm not exactly sure how to reduce the batch size here\
          \ as I don't see any parameters regarding it. I am also new to using huggingface/transformers\
          \ so if anyone has any suggestions, it would be much appreciated.\r\n\r\n"
        updatedAt: '2023-07-04T08:59:07.732Z'
      numEdits: 0
      reactions: []
    id: 64a3df5b38f48d7a6d775a08
    type: comment
  author: ibraweeb
  content: "I'm currently trying to use the orca mini 3b and getting the Cuda out\
    \ of memory error. The error line says the following:\r\nOutOfMemoryError: CUDA\
    \ out of memory. Tried to allocate 392.00 MiB (GPU 0; 8.00 GiB total capacity;\
    \ 6.65 GiB already allocated; 0 bytes free; 7.05 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
    \n\r\nThis is strange in my case as I was easily able to use the model a while\
    \ ago.\r\n\r\nThis is the cell where I'm importing the model:\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/648ea494c20f218a8cc55f76/tUSVYrZG6pWJa2B0hFv4F.png)\r\
    \n\r\nThis is the standard generate text function I'm using with the default prompt\
    \ to write a letter:\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/648ea494c20f218a8cc55f76/V4S0YPZsP5ioCSRMpvKxi.png)\r\
    \n\r\nInstead of an output I get the error:\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/648ea494c20f218a8cc55f76/hDyK3VAGa9qXVZPypGjrl.png)\r\
    \n\r\nUsually when people get errors like this a common solution is to reduce\
    \ the batch size. I'm not exactly sure how to reduce the batch size here as I\
    \ don't see any parameters regarding it. I am also new to using huggingface/transformers\
    \ so if anyone has any suggestions, it would be much appreciated.\r\n\r\n"
  created_at: 2023-07-04 07:59:07+00:00
  edited: false
  hidden: false
  id: 64a3df5b38f48d7a6d775a08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f0b9bb3610282e0abc7937d443dc45a3.svg
      fullname: Pankaj Mathur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: pankajmathur
      type: user
    createdAt: '2023-07-04T18:17:06.000Z'
    data:
      edited: true
      editors:
      - pankajmathur
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8970062136650085
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f0b9bb3610282e0abc7937d443dc45a3.svg
          fullname: Pankaj Mathur
          isHf: false
          isPro: true
          name: pankajmathur
          type: user
        html: "<p>Thanks for detail post and screenshot, it\u2019s look like your\
          \ current machine which is shown is screenshot has only 8GB VRAM, so I am\
          \ not sure you can directly use this  model and code from the repo data\
          \ card. You should try quantized version provided by \u201CTheBloke\u201D\
          \ on his HF Repo and follow this detail post on how to setup local webui\
          \ to play around with quantized orca-minis or any other quantized HF models.<br><a\
          \ rel=\"nofollow\" href=\"https://www.reddit.com/r/LocalLLaMA/wiki/guide?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=ioscss&amp;utm_content=1&amp;utm_term=1\"\
          >https://www.reddit.com/r/LocalLLaMA/wiki/guide?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=ioscss&amp;utm_content=1&amp;utm_term=1</a></p>\n"
        raw: "Thanks for detail post and screenshot, it\u2019s look like your current\
          \ machine which is shown is screenshot has only 8GB VRAM, so I am not sure\
          \ you can directly use this  model and code from the repo data card. You\
          \ should try quantized version provided by \u201CTheBloke\u201D on his HF\
          \ Repo and follow this detail post on how to setup local webui to play around\
          \ with quantized orca-minis or any other quantized HF models. \nhttps://www.reddit.com/r/LocalLLaMA/wiki/guide?utm_source=share&utm_medium=ios_app&utm_name=ioscss&utm_content=1&utm_term=1"
        updatedAt: '2023-07-04T18:17:42.463Z'
      numEdits: 1
      reactions: []
      relatedEventId: 64a46222250bfdecd95cfc32
    id: 64a46222250bfdecd95cfc2f
    type: comment
  author: pankajmathur
  content: "Thanks for detail post and screenshot, it\u2019s look like your current\
    \ machine which is shown is screenshot has only 8GB VRAM, so I am not sure you\
    \ can directly use this  model and code from the repo data card. You should try\
    \ quantized version provided by \u201CTheBloke\u201D on his HF Repo and follow\
    \ this detail post on how to setup local webui to play around with quantized orca-minis\
    \ or any other quantized HF models. \nhttps://www.reddit.com/r/LocalLLaMA/wiki/guide?utm_source=share&utm_medium=ios_app&utm_name=ioscss&utm_content=1&utm_term=1"
  created_at: 2023-07-04 17:17:06+00:00
  edited: true
  hidden: false
  id: 64a46222250bfdecd95cfc2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f0b9bb3610282e0abc7937d443dc45a3.svg
      fullname: Pankaj Mathur
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: pankajmathur
      type: user
    createdAt: '2023-07-04T18:17:06.000Z'
    data:
      status: closed
    id: 64a46222250bfdecd95cfc32
    type: status-change
  author: pankajmathur
  created_at: 2023-07-04 17:17:06+00:00
  id: 64a46222250bfdecd95cfc32
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: pankajmathur/orca_mini_3b
repo_type: model
status: closed
target_branch: null
title: Cuda Out of Memory Error
