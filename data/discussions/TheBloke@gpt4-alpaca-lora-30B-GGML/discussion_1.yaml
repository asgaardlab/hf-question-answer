!!python/object:huggingface_hub.community.DiscussionWithDetails
author: venn12
conflicting_files: null
created_at: 2023-04-18 09:57:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b498694d13b6c2596efc7b0ca13ac2ae.svg
      fullname: venn venner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: venn12
      type: user
    createdAt: '2023-04-18T10:57:04.000Z'
    data:
      edited: false
      editors:
      - venn12
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b498694d13b6c2596efc7b0ca13ac2ae.svg
          fullname: venn venner
          isHf: false
          isPro: false
          name: venn12
          type: user
        html: '<p>I tried to run this through llama.cpp and it gives me the failure.
          invalid model file (bad F16 value 5). Other models seem to run fine. </p>

          '
        raw: 'I tried to run this through llama.cpp and it gives me the failure. invalid
          model file (bad F16 value 5). Other models seem to run fine. '
        updatedAt: '2023-04-18T10:57:04.951Z'
      numEdits: 0
      reactions: []
    id: 643e7780e08e86aec10f1a99
    type: comment
  author: venn12
  content: 'I tried to run this through llama.cpp and it gives me the failure. invalid
    model file (bad F16 value 5). Other models seem to run fine. '
  created_at: 2023-04-18 09:57:04+00:00
  edited: false
  hidden: false
  id: 643e7780e08e86aec10f1a99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-18T11:00:25.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Sounds like you're running a too-old version of llama.cpp? Try updating\
          \ it and re-compiling.  It works fine here:</p>\n<pre><code>tomj@Eddie ~/src/llama.cpp\
          \ (master\u25CF\u25CF)$ ./main -t 18 -m ~/src/huggingface/gpt4-alpaca-lora-30B-4bit-GGML/gpt4-alpaca-lora-30B.GGML.q4_1.bin\
          \ --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n 500 -p \"Below is an\
          \ instruction that describes a task. Write a response that appropriately\
          \ completes the request.\n### Instruction:\nWrite a story about llamas\n\
          ### Response:\"\nmain: seed = 1681815529\nllama.cpp: loading model from\
          \ /Users/tomj/src/huggingface/gpt4-alpaca-lora-30B-4bit-GGML/gpt4-alpaca-lora-30B.GGML.q4_1.bin\n\
          llama_model_load_internal: format     = ggjt v1 (latest)\nllama_model_load_internal:\
          \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
          \ n_embd     = 6656\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
          \ n_head     = 52\nllama_model_load_internal: n_layer    = 60\nllama_model_load_internal:\
          \ n_rot      = 128\nllama_model_load_internal: ftype      = 5 (unknown,\
          \ may not work)\nllama_model_load_internal: n_ff       = 17920\nllama_model_load_internal:\
          \ n_parts    = 1\nllama_model_load_internal: model size = 30B\nllama_model_load_internal:\
          \ ggml ctx size = 110.30 KB\nllama_model_load_internal: mem required  =\
          \ 25573.12 MB (+ 3124.00 MB per state)\nllama_init_from_file: kv self size\
          \  = 3120.00 MB\n\nsystem_info: n_threads = 18 / 36 | AVX = 1 | AVX2 = 1\
          \ | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0\
          \ | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3\
          \ = 1 | VSX = 0 |\nsampling: temp = 0.700000, top_k = 40, top_p = 0.950000,\
          \ repeat_last_n = 64, repeat_penalty = 1.100000\ngenerate: n_ctx = 2048,\
          \ n_batch = 8, n_predict = 500, n_keep = 0\n\n\n Below is an instruction\
          \ that describes a task. Write a response that appropriately completes the\
          \ request.\n### Instruction:\nWrite a story about llamas\n### Response:\n\
          Once upon a time, in the land of Llama, there was a small town named Camelot.\
          \ The people of Camelot were very kind and generous, always looking out\
          \ for each other and helping one another in times of need.\nOne day, while\
          \ the villagers of Camelot were going about their daily tasks, a terrible\
          \ storm rolled in. Thunder roared, lightning flashed, and rain poured down\
          \ from the sky. Soon, the town was flooded, and the people had to evacuate\
          \ their homes.\n</code></pre>\n"
        raw: "Sounds like you're running a too-old version of llama.cpp? Try updating\
          \ it and re-compiling.  It works fine here:\n\n```\ntomj@Eddie ~/src/llama.cpp\
          \ (master\u25CF\u25CF)$ ./main -t 18 -m ~/src/huggingface/gpt4-alpaca-lora-30B-4bit-GGML/gpt4-alpaca-lora-30B.GGML.q4_1.bin\
          \ --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n 500 -p \"Below is an\
          \ instruction that describes a task. Write a response that appropriately\
          \ completes the request.\n### Instruction:\nWrite a story about llamas\n\
          ### Response:\"\nmain: seed = 1681815529\nllama.cpp: loading model from\
          \ /Users/tomj/src/huggingface/gpt4-alpaca-lora-30B-4bit-GGML/gpt4-alpaca-lora-30B.GGML.q4_1.bin\n\
          llama_model_load_internal: format     = ggjt v1 (latest)\nllama_model_load_internal:\
          \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
          \ n_embd     = 6656\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
          \ n_head     = 52\nllama_model_load_internal: n_layer    = 60\nllama_model_load_internal:\
          \ n_rot      = 128\nllama_model_load_internal: ftype      = 5 (unknown,\
          \ may not work)\nllama_model_load_internal: n_ff       = 17920\nllama_model_load_internal:\
          \ n_parts    = 1\nllama_model_load_internal: model size = 30B\nllama_model_load_internal:\
          \ ggml ctx size = 110.30 KB\nllama_model_load_internal: mem required  =\
          \ 25573.12 MB (+ 3124.00 MB per state)\nllama_init_from_file: kv self size\
          \  = 3120.00 MB\n\nsystem_info: n_threads = 18 / 36 | AVX = 1 | AVX2 = 1\
          \ | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0\
          \ | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3\
          \ = 1 | VSX = 0 |\nsampling: temp = 0.700000, top_k = 40, top_p = 0.950000,\
          \ repeat_last_n = 64, repeat_penalty = 1.100000\ngenerate: n_ctx = 2048,\
          \ n_batch = 8, n_predict = 500, n_keep = 0\n\n\n Below is an instruction\
          \ that describes a task. Write a response that appropriately completes the\
          \ request.\n### Instruction:\nWrite a story about llamas\n### Response:\n\
          Once upon a time, in the land of Llama, there was a small town named Camelot.\
          \ The people of Camelot were very kind and generous, always looking out\
          \ for each other and helping one another in times of need.\nOne day, while\
          \ the villagers of Camelot were going about their daily tasks, a terrible\
          \ storm rolled in. Thunder roared, lightning flashed, and rain poured down\
          \ from the sky. Soon, the town was flooded, and the people had to evacuate\
          \ their homes.\n```"
        updatedAt: '2023-04-18T11:59:23.621Z'
      numEdits: 1
      reactions: []
    id: 643e7849ab725487d8e7c216
    type: comment
  author: TheBloke
  content: "Sounds like you're running a too-old version of llama.cpp? Try updating\
    \ it and re-compiling.  It works fine here:\n\n```\ntomj@Eddie ~/src/llama.cpp\
    \ (master\u25CF\u25CF)$ ./main -t 18 -m ~/src/huggingface/gpt4-alpaca-lora-30B-4bit-GGML/gpt4-alpaca-lora-30B.GGML.q4_1.bin\
    \ --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n 500 -p \"Below is an instruction\
    \ that describes a task. Write a response that appropriately completes the request.\n\
    ### Instruction:\nWrite a story about llamas\n### Response:\"\nmain: seed = 1681815529\n\
    llama.cpp: loading model from /Users/tomj/src/huggingface/gpt4-alpaca-lora-30B-4bit-GGML/gpt4-alpaca-lora-30B.GGML.q4_1.bin\n\
    llama_model_load_internal: format     = ggjt v1 (latest)\nllama_model_load_internal:\
    \ n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
    \ n_embd     = 6656\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
    \ n_head     = 52\nllama_model_load_internal: n_layer    = 60\nllama_model_load_internal:\
    \ n_rot      = 128\nllama_model_load_internal: ftype      = 5 (unknown, may not\
    \ work)\nllama_model_load_internal: n_ff       = 17920\nllama_model_load_internal:\
    \ n_parts    = 1\nllama_model_load_internal: model size = 30B\nllama_model_load_internal:\
    \ ggml ctx size = 110.30 KB\nllama_model_load_internal: mem required  = 25573.12\
    \ MB (+ 3124.00 MB per state)\nllama_init_from_file: kv self size  = 3120.00 MB\n\
    \nsystem_info: n_threads = 18 / 36 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI\
    \ = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA\
    \ = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\nsampling: temp = 0.700000,\
    \ top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.100000\n\
    generate: n_ctx = 2048, n_batch = 8, n_predict = 500, n_keep = 0\n\n\n Below is\
    \ an instruction that describes a task. Write a response that appropriately completes\
    \ the request.\n### Instruction:\nWrite a story about llamas\n### Response:\n\
    Once upon a time, in the land of Llama, there was a small town named Camelot.\
    \ The people of Camelot were very kind and generous, always looking out for each\
    \ other and helping one another in times of need.\nOne day, while the villagers\
    \ of Camelot were going about their daily tasks, a terrible storm rolled in. Thunder\
    \ roared, lightning flashed, and rain poured down from the sky. Soon, the town\
    \ was flooded, and the people had to evacuate their homes.\n```"
  created_at: 2023-04-18 10:00:25+00:00
  edited: true
  hidden: false
  id: 643e7849ab725487d8e7c216
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b498694d13b6c2596efc7b0ca13ac2ae.svg
      fullname: venn venner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: venn12
      type: user
    createdAt: '2023-04-19T22:21:48.000Z'
    data:
      edited: false
      editors:
      - venn12
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b498694d13b6c2596efc7b0ca13ac2ae.svg
          fullname: venn venner
          isHf: false
          isPro: false
          name: venn12
          type: user
        html: '<p>Yeah llama.cpp is changing so rapidly it  seems it was out of date
          despite getting it only a few days ago. Redownloading worked, Thanks. </p>

          '
        raw: 'Yeah llama.cpp is changing so rapidly it  seems it was out of date despite
          getting it only a few days ago. Redownloading worked, Thanks. '
        updatedAt: '2023-04-19T22:21:48.121Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6440697c194b02fd30967924
    id: 6440697c194b02fd30967923
    type: comment
  author: venn12
  content: 'Yeah llama.cpp is changing so rapidly it  seems it was out of date despite
    getting it only a few days ago. Redownloading worked, Thanks. '
  created_at: 2023-04-19 21:21:48+00:00
  edited: false
  hidden: false
  id: 6440697c194b02fd30967923
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b498694d13b6c2596efc7b0ca13ac2ae.svg
      fullname: venn venner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: venn12
      type: user
    createdAt: '2023-04-19T22:21:48.000Z'
    data:
      status: closed
    id: 6440697c194b02fd30967924
    type: status-change
  author: venn12
  created_at: 2023-04-19 21:21:48+00:00
  id: 6440697c194b02fd30967924
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/gpt4-alpaca-lora-30B-GGML
repo_type: model
status: closed
target_branch: null
title: Fails to run (bad F16 value 5)
