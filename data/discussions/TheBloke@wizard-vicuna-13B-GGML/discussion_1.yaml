!!python/object:huggingface_hub.community.DiscussionWithDetails
author: thefaheem
conflicting_files: null
created_at: 2023-05-05 11:57:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-05-05T12:57:01.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<p>I Wonder If There any Perfomance Drop in ggml version of the model
          compared to HF version?</p>

          '
        raw: I Wonder If There any Perfomance Drop in ggml version of the model compared
          to HF version?
        updatedAt: '2023-05-05T12:57:01.431Z'
      numEdits: 0
      reactions: []
    id: 6454fd1d1f9406d488010158
    type: comment
  author: thefaheem
  content: I Wonder If There any Perfomance Drop in ggml version of the model compared
    to HF version?
  created_at: 2023-05-05 11:57:01+00:00
  edited: false
  hidden: false
  id: 6454fd1d1f9406d488010158
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T13:02:30.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You mean performance in terms of accuracy of answers?  Or in terms
          of how fast it responds?</p>

          <p>In terms of speed of response: this depends. HF models load on GPU, and
          GPU is much faster for inference than CPU.  But you also need a lot of VRAM,
          and the model is very large.<br>GGML runs pretty well on CPU, and if you
          don''t have a powerful GPU it is likely to be quicker.  But if you do have
          a powerful GPU, then HF may be quicker - especially in 8bit or 4bit.   4bit
          quantised GPU inference (GPTQ) is likely to be fastest of all.</p>

          <p>If you meant in terms of accuracy of answers, then in theory there may
          be  a small drop in theoretical accuracy, as GGML is quantised. But if you
          use q5_0 or q5_1 it is likely to be impossible to notice. </p>

          <p>If you look at the README for llama.cpp there is a table showing benchmark
          results for 7B float16 (like HF), vs the various 4bit and 5bit GGML quantisation
          methods. </p>

          <p>If you use the q5_1 file the difference to float16 is tiny, like less
          than 0.1%.  So it is very unlikely you will notice any drop in accuracy
          in practical usage.</p>

          '
        raw: "You mean performance in terms of accuracy of answers?  Or in terms of\
          \ how fast it responds?\n\nIn terms of speed of response: this depends.\
          \ HF models load on GPU, and GPU is much faster for inference than CPU.\
          \  But you also need a lot of VRAM, and the model is very large.\nGGML runs\
          \ pretty well on CPU, and if you don't have a powerful GPU it is likely\
          \ to be quicker.  But if you do have a powerful GPU, then HF may be quicker\
          \ - especially in 8bit or 4bit.   4bit quantised GPU inference (GPTQ) is\
          \ likely to be fastest of all.\n\nIf you meant in terms of accuracy of answers,\
          \ then in theory there may be  a small drop in theoretical accuracy, as\
          \ GGML is quantised. But if you use q5_0 or q5_1 it is likely to be impossible\
          \ to notice. \n\nIf you look at the README for llama.cpp there is a table\
          \ showing benchmark results for 7B float16 (like HF), vs the various 4bit\
          \ and 5bit GGML quantisation methods. \n\nIf you use the q5_1 file the difference\
          \ to float16 is tiny, like less than 0.1%.  So it is very unlikely you will\
          \ notice any drop in accuracy in practical usage."
        updatedAt: '2023-05-05T13:02:30.606Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Bharatvid
        - CeeGee
        - thiner
    id: 6454fe66fe2f48cb4b64a59c
    type: comment
  author: TheBloke
  content: "You mean performance in terms of accuracy of answers?  Or in terms of\
    \ how fast it responds?\n\nIn terms of speed of response: this depends. HF models\
    \ load on GPU, and GPU is much faster for inference than CPU.  But you also need\
    \ a lot of VRAM, and the model is very large.\nGGML runs pretty well on CPU, and\
    \ if you don't have a powerful GPU it is likely to be quicker.  But if you do\
    \ have a powerful GPU, then HF may be quicker - especially in 8bit or 4bit.  \
    \ 4bit quantised GPU inference (GPTQ) is likely to be fastest of all.\n\nIf you\
    \ meant in terms of accuracy of answers, then in theory there may be  a small\
    \ drop in theoretical accuracy, as GGML is quantised. But if you use q5_0 or q5_1\
    \ it is likely to be impossible to notice. \n\nIf you look at the README for llama.cpp\
    \ there is a table showing benchmark results for 7B float16 (like HF), vs the\
    \ various 4bit and 5bit GGML quantisation methods. \n\nIf you use the q5_1 file\
    \ the difference to float16 is tiny, like less than 0.1%.  So it is very unlikely\
    \ you will notice any drop in accuracy in practical usage."
  created_at: 2023-05-05 12:02:30+00:00
  edited: false
  hidden: false
  id: 6454fe66fe2f48cb4b64a59c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-05-05T13:07:47.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<p>Ahh.. Thanks For The Detailed Response!</p>

          '
        raw: Ahh.. Thanks For The Detailed Response!
        updatedAt: '2023-05-05T13:07:47.492Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
      relatedEventId: 6454ffa3a473375be56e5bcb
    id: 6454ffa3a473375be56e5bca
    type: comment
  author: thefaheem
  content: Ahh.. Thanks For The Detailed Response!
  created_at: 2023-05-05 12:07:47+00:00
  edited: false
  hidden: false
  id: 6454ffa3a473375be56e5bca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-05-05T13:07:47.000Z'
    data:
      status: closed
    id: 6454ffa3a473375be56e5bcb
    type: status-change
  author: thefaheem
  created_at: 2023-05-05 12:07:47+00:00
  id: 6454ffa3a473375be56e5bcb
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/wizard-vicuna-13B-GGML
repo_type: model
status: closed
target_branch: null
title: Perfomance of the model
