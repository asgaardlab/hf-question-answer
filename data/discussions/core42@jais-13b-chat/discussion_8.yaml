!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rsalshalan
conflicting_files: null
created_at: 2023-09-11 10:11:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/003b612a62700f1719c23bf4cff81b20.svg
      fullname: Raghad s
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rsalshalan
      type: user
    createdAt: '2023-09-11T11:11:13.000Z'
    data:
      edited: false
      editors:
      - rsalshalan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6891787648200989
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/003b612a62700f1719c23bf4cff81b20.svg
          fullname: Raghad s
          isHf: false
          isPro: false
          name: rsalshalan
          type: user
        html: "<p>Hi all,<br>Thanks for the great efforts! </p>\n<p>I would like to\
          \ ask why cant i use the model with text-generation-inference</p>\n<p>I\
          \ tried to launch the server as follows <code>text-generation-launcher --model-id\
          \ data/jais-13b-chat </code> (I downloaded the repo locally).   </p>\n<p>Here\
          \ are the results: </p>\n<pre><code>023-09-11T11:02:01.949055Z ERROR shard-manager:\
          \ text_generation_launcher: Shard complete standard error output:\n\nTraceback\
          \ (most recent call last):\n\n  File \"/opt/conda/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n    sys.exit(app())\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 81, in serve\n    server.serve(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 184, in serve\n    asyncio.run(\n\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n\n  File \"\
          /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n\
          \    return future.result()\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 136, in serve_inner\n    model = get_model(\n\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 298, in get_model\n    raise ValueError(f\"Unsupported model type\
          \ {model_type}\")\n\nValueError: Unsupported model type jais\n</code></pre>\n\
          <p>Am I missing something? </p>\n<p>Would appreciate your support and if\
          \ you need any more details about this please let me know</p>\n"
        raw: "Hi all,\r\nThanks for the great efforts! \r\n\r\nI would like to ask\
          \ why cant i use the model with text-generation-inference\r\n\r\nI tried\
          \ to launch the server as follows `text-generation-launcher --model-id data/jais-13b-chat\
          \ ` (I downloaded the repo locally).   \r\n\r\nHere are the results: \r\n\
          ```\r\n023-09-11T11:02:01.949055Z ERROR shard-manager: text_generation_launcher:\
          \ Shard complete standard error output:\r\n\r\nTraceback (most recent call\
          \ last):\r\n\r\n  File \"/opt/conda/bin/text-generation-server\", line 8,\
          \ in <module>\r\n    sys.exit(app())\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 81, in serve\r\n    server.serve(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 184, in serve\r\n    asyncio.run(\r\n\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n\r\n  File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
          \n    return future.result()\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 136, in serve_inner\r\n    model = get_model(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 298, in get_model\r\n    raise ValueError(f\"Unsupported model type\
          \ {model_type}\")\r\n\r\nValueError: Unsupported model type jais\r\n```\r\
          \n\r\n\r\nAm I missing something? \r\n\r\nWould appreciate your support\
          \ and if you need any more details about this please let me know\r\n"
        updatedAt: '2023-09-11T11:11:13.746Z'
      numEdits: 0
      reactions: []
    id: 64fef5d1cccea7ce7b254c46
    type: comment
  author: rsalshalan
  content: "Hi all,\r\nThanks for the great efforts! \r\n\r\nI would like to ask why\
    \ cant i use the model with text-generation-inference\r\n\r\nI tried to launch\
    \ the server as follows `text-generation-launcher --model-id data/jais-13b-chat\
    \ ` (I downloaded the repo locally).   \r\n\r\nHere are the results: \r\n```\r\
    \n023-09-11T11:02:01.949055Z ERROR shard-manager: text_generation_launcher: Shard\
    \ complete standard error output:\r\n\r\nTraceback (most recent call last):\r\n\
    \r\n  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\n\
    \    sys.exit(app())\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 81, in serve\r\n    server.serve(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 184, in serve\r\n    asyncio.run(\r\n\r\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n\r\n  File \"\
    /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
    \n    return future.result()\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 136, in serve_inner\r\n    model = get_model(\r\n\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 298, in get_model\r\n    raise ValueError(f\"Unsupported model type {model_type}\"\
    )\r\n\r\nValueError: Unsupported model type jais\r\n```\r\n\r\n\r\nAm I missing\
    \ something? \r\n\r\nWould appreciate your support and if you need any more details\
    \ about this please let me know\r\n"
  created_at: 2023-09-11 10:11:13+00:00
  edited: false
  hidden: false
  id: 64fef5d1cccea7ce7b254c46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42e099d46a14d1c1de10aa8d60675745.svg
      fullname: Ali Alkhawaher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alihkhawaher
      type: user
    createdAt: '2023-09-12T12:29:38.000Z'
    data:
      edited: false
      editors:
      - alihkhawaher
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9201836585998535
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/42e099d46a14d1c1de10aa8d60675745.svg
          fullname: Ali Alkhawaher
          isHf: false
          isPro: false
          name: alihkhawaher
          type: user
        html: '<p>It works with me but you need to use transformer loader. I do not
          know how to do this using command line</p>

          '
        raw: It works with me but you need to use transformer loader. I do not know
          how to do this using command line
        updatedAt: '2023-09-12T12:29:38.539Z'
      numEdits: 0
      reactions: []
    id: 650059b277745ae9ee6c2500
    type: comment
  author: alihkhawaher
  content: It works with me but you need to use transformer loader. I do not know
    how to do this using command line
  created_at: 2023-09-12 11:29:38+00:00
  edited: false
  hidden: false
  id: 650059b277745ae9ee6c2500
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1a76191a97259ce9c6fc9ad44e6c4338.svg
      fullname: Tony Alapatt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tonyAlapatt
      type: user
    createdAt: '2023-09-15T11:07:43.000Z'
    data:
      edited: true
      editors:
      - tonyAlapatt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7411897778511047
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1a76191a97259ce9c6fc9ad44e6c4338.svg
          fullname: Tony Alapatt
          isHf: false
          isPro: false
          name: tonyAlapatt
          type: user
        html: '<p>I''m getting the same error. How do I run it on a server for inference
          ? Using TGI or anthing else ? Do help us with the necessary parameters.
          </p>

          '
        raw: 'I''m getting the same error. How do I run it on a server for inference
          ? Using TGI or anthing else ? Do help us with the necessary parameters. '
        updatedAt: '2023-09-15T11:08:04.261Z'
      numEdits: 1
      reactions: []
    id: 65043affa450492f84419b86
    type: comment
  author: tonyAlapatt
  content: 'I''m getting the same error. How do I run it on a server for inference
    ? Using TGI or anthing else ? Do help us with the necessary parameters. '
  created_at: 2023-09-15 10:07:43+00:00
  edited: true
  hidden: false
  id: 65043affa450492f84419b86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42e099d46a14d1c1de10aa8d60675745.svg
      fullname: Ali Alkhawaher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alihkhawaher
      type: user
    createdAt: '2023-09-15T15:19:16.000Z'
    data:
      edited: true
      editors:
      - alihkhawaher
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.417707622051239
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/42e099d46a14d1c1de10aa8d60675745.svg
          fullname: Ali Alkhawaher
          isHf: false
          isPro: false
          name: alihkhawaher
          type: user
        html: "<p>I dont know what you mean but if you wants to load it with 4bit\
          \ to work with low VRAM here what I am using</p>\n<p>my windows,<br><a rel=\"\
          nofollow\" href=\"https://developer.nvidia.com/cuda-downloads?target_os=Windows&amp;target_arch=x86_64\"\
          >https://developer.nvidia.com/cuda-downloads?target_os=Windows&amp;target_arch=x86_64</a><br>python\
          \ 3.11</p>\n<p>then python environment (i.e python -m vevn venv)</p>\n<p>pip\
          \ install  transformers, accelerate<br>pip install <a rel=\"nofollow\" href=\"\
          https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl\"\
          >https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl</a><br>pip\
          \ install torch==2.0.1+cu117 --index-url <a rel=\"nofollow\" href=\"https://download.pytorch.org/whl/cu117\"\
          >https://download.pytorch.org/whl/cu117</a></p>\n<p> a working python, I\
          \ have two GPUs that is why I am specifying cuda:0 because it has 24G vram</p>\n\
          <pre><code># -*- coding: utf-8 -*-\n\nimport torch\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\nmodel_path = \"C:\\AI\\ML Models\\\
          inception-mbzuai_jais-13b\"\n\ndevice = \"cuda:0\" if torch.cuda.is_available()\
          \ else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\
          model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device,load_in_4bit=True,\
          \ trust_remote_code=True,\n                                            \
          \ bnb_4bit_compute_dtype=torch.float16)\n\ndef get_response(text,tokenizer=tokenizer,model=model):\n\
          \    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n    inputs\
          \ = input_ids.to(device)\n    input_len = inputs.shape[-1]\n    generate_ids\
          \ = model.generate(\n        inputs,\n        top_p=0.9,\n        temperature=0.3,\n\
          \        max_length=200-input_len,\n        min_length=input_len + 4,\n\
          \        repetition_penalty=1.2,\n        do_sample=True,\n    )\n    response\
          \ = tokenizer.batch_decode(\n        generate_ids, skip_special_tokens=True,\
          \ clean_up_tokenization_spaces=True\n    )[0]\n    return response\n\n\n\
          text= \"\u0639\u0627\u0635\u0645\u0629 \u062F\u0648\u0644\u0629 \u0627\u0644\
          \u0625\u0645\u0627\u0631\u0627\u062A \u0627\u0644\u0639\u0631\u0628\u064A\
          \u0629 \u0627\u0644\u0645\u062A\u062D\u062F\u0629 \u0647\"\nprint(get_response(text))\n\
          \ntext = \"The capital of UAE is\"\nprint(get_response(text))\n</code></pre>\n"
        raw: "I dont know what you mean but if you wants to load it with 4bit to work\
          \ with low VRAM here what I am using\n\nmy windows,\nhttps://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64\n\
          python 3.11\n\nthen python environment (i.e python -m vevn venv)\n\npip\
          \ install  transformers, accelerate\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl\n\
          pip install torch==2.0.1+cu117 --index-url https://download.pytorch.org/whl/cu117\n\
          \n\n a working python, I have two GPUs that is why I am specifying cuda:0\
          \ because it has 24G vram\n\n```\n# -*- coding: utf-8 -*-\n\nimport torch\n\
          from transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_path\
          \ = \"C:\\AI\\ML Models\\inception-mbzuai_jais-13b\"\n\ndevice = \"cuda:0\"\
          \ if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\
          model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device,load_in_4bit=True,\
          \ trust_remote_code=True,\n                                            \
          \ bnb_4bit_compute_dtype=torch.float16)\n\ndef get_response(text,tokenizer=tokenizer,model=model):\n\
          \    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n    inputs\
          \ = input_ids.to(device)\n    input_len = inputs.shape[-1]\n    generate_ids\
          \ = model.generate(\n        inputs,\n        top_p=0.9,\n        temperature=0.3,\n\
          \        max_length=200-input_len,\n        min_length=input_len + 4,\n\
          \        repetition_penalty=1.2,\n        do_sample=True,\n    )\n    response\
          \ = tokenizer.batch_decode(\n        generate_ids, skip_special_tokens=True,\
          \ clean_up_tokenization_spaces=True\n    )[0]\n    return response\n\n\n\
          text= \"\u0639\u0627\u0635\u0645\u0629 \u062F\u0648\u0644\u0629 \u0627\u0644\
          \u0625\u0645\u0627\u0631\u0627\u062A \u0627\u0644\u0639\u0631\u0628\u064A\
          \u0629 \u0627\u0644\u0645\u062A\u062D\u062F\u0629 \u0647\"\nprint(get_response(text))\n\
          \ntext = \"The capital of UAE is\"\nprint(get_response(text))\n```"
        updatedAt: '2023-09-15T15:21:58.714Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - quickbenx
    id: 650475f4d4a3b0f508b7ff33
    type: comment
  author: alihkhawaher
  content: "I dont know what you mean but if you wants to load it with 4bit to work\
    \ with low VRAM here what I am using\n\nmy windows,\nhttps://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64\n\
    python 3.11\n\nthen python environment (i.e python -m vevn venv)\n\npip install\
    \  transformers, accelerate\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl\n\
    pip install torch==2.0.1+cu117 --index-url https://download.pytorch.org/whl/cu117\n\
    \n\n a working python, I have two GPUs that is why I am specifying cuda:0 because\
    \ it has 24G vram\n\n```\n# -*- coding: utf-8 -*-\n\nimport torch\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM\nmodel_path = \"C:\\AI\\ML Models\\\
    inception-mbzuai_jais-13b\"\n\ndevice = \"cuda:0\" if torch.cuda.is_available()\
    \ else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(model_path, device_map=device,load_in_4bit=True,\
    \ trust_remote_code=True,\n                                             bnb_4bit_compute_dtype=torch.float16)\n\
    \ndef get_response(text,tokenizer=tokenizer,model=model):\n    input_ids = tokenizer(text,\
    \ return_tensors=\"pt\").input_ids\n    inputs = input_ids.to(device)\n    input_len\
    \ = inputs.shape[-1]\n    generate_ids = model.generate(\n        inputs,\n  \
    \      top_p=0.9,\n        temperature=0.3,\n        max_length=200-input_len,\n\
    \        min_length=input_len + 4,\n        repetition_penalty=1.2,\n        do_sample=True,\n\
    \    )\n    response = tokenizer.batch_decode(\n        generate_ids, skip_special_tokens=True,\
    \ clean_up_tokenization_spaces=True\n    )[0]\n    return response\n\n\ntext=\
    \ \"\u0639\u0627\u0635\u0645\u0629 \u062F\u0648\u0644\u0629 \u0627\u0644\u0625\
    \u0645\u0627\u0631\u0627\u062A \u0627\u0644\u0639\u0631\u0628\u064A\u0629 \u0627\
    \u0644\u0645\u062A\u062D\u062F\u0629 \u0647\"\nprint(get_response(text))\n\ntext\
    \ = \"The capital of UAE is\"\nprint(get_response(text))\n```"
  created_at: 2023-09-15 14:19:16+00:00
  edited: true
  hidden: false
  id: 650475f4d4a3b0f508b7ff33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42e099d46a14d1c1de10aa8d60675745.svg
      fullname: Ali Alkhawaher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alihkhawaher
      type: user
    createdAt: '2023-09-15T15:25:26.000Z'
    data:
      edited: false
      editors:
      - alihkhawaher
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6853000521659851
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/42e099d46a14d1c1de10aa8d60675745.svg
          fullname: Ali Alkhawaher
          isHf: false
          isPro: false
          name: alihkhawaher
          type: user
        html: "<p>Also you may need to use peft, I am not sure what it does but it\
          \ solves some an error I was getting</p>\n<p>pip install peft</p>\n<pre><code>#\
          \ quantization_config\nnf4_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n\
          \   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True,\n \
          \  bnb_4bit_compute_dtype=torch.float16\n)\n\nfrom peft import prepare_model_for_kbit_training\n\
          model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"cuda:0\"\
          , quantization_config=nf4_config,trust_remote_code=True)\nmodel = prepare_model_for_kbit_training(model)\n\
          </code></pre>\n"
        raw: "Also you may need to use peft, I am not sure what it does but it solves\
          \ some an error I was getting\n\npip install peft\n\n```\n# quantization_config\n\
          nf4_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"\
          nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.float16\n\
          )\n\nfrom peft import prepare_model_for_kbit_training\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
          \ device_map=\"cuda:0\", quantization_config=nf4_config,trust_remote_code=True)\n\
          model = prepare_model_for_kbit_training(model)\n```"
        updatedAt: '2023-09-15T15:25:26.175Z'
      numEdits: 0
      reactions: []
    id: 65047766d55010f0e1eddab6
    type: comment
  author: alihkhawaher
  content: "Also you may need to use peft, I am not sure what it does but it solves\
    \ some an error I was getting\n\npip install peft\n\n```\n# quantization_config\n\
    nf4_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"\
    nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.float16\n\
    )\n\nfrom peft import prepare_model_for_kbit_training\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
    \ device_map=\"cuda:0\", quantization_config=nf4_config,trust_remote_code=True)\n\
    model = prepare_model_for_kbit_training(model)\n```"
  created_at: 2023-09-15 14:25:26+00:00
  edited: false
  hidden: false
  id: 65047766d55010f0e1eddab6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: core42/jais-13b-chat
repo_type: model
status: open
target_branch: null
title: Supporting the model on text-generation-inference server
