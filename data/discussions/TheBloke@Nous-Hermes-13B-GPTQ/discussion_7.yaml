!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ernobyl
conflicting_files: null
created_at: 2023-06-19 19:07:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0d1fb10358be1ad3085099c754354eea.svg
      fullname: 'Hendrik  Jacobs '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ernobyl
      type: user
    createdAt: '2023-06-19T20:07:28.000Z'
    data:
      edited: false
      editors:
      - Ernobyl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.19813430309295654
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0d1fb10358be1ad3085099c754354eea.svg
          fullname: 'Hendrik  Jacobs '
          isHf: false
          isPro: false
          name: Ernobyl
          type: user
        html: "<p>Hi, I tried to run your example code in google colab, but I get\
          \ an error when creating the tokenizer. </p>\n<p>\u256D\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u256E<br>\u2502 in &lt;cell line: 2&gt;:2\
          \                                                                      \
          \        \u2502<br>\u2502 in init_hermes:7                             \
          \                                                    \u2502<br>\u2502  \
          \                                                                      \
          \                          \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:691\
          \ in     \u2502<br>\u2502 from_pretrained                              \
          \                                                    \u2502<br>\u2502  \
          \                                                                      \
          \                          \u2502<br>\u2502   688 \u2502   \u2502   \u2502\
          \   \u2502   raise ValueError(                                         \
          \                 \u2502<br>\u2502   689 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   f\"Tokenizer class {tokenizer_class_candidate} does not exist\
          \ or is n   \u2502<br>\u2502   690 \u2502   \u2502   \u2502   \u2502   )\
          \                                                                      \
          \    \u2502<br>\u2502 \u2771 691 \u2502   \u2502   \u2502   return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *input   \u2502<br>\u2502   692 \u2502   \u2502                      \
          \                                                                \u2502\
          <br>\u2502   693 \u2502   \u2502   # Otherwise we have to be creative. \
          \                                               \u2502<br>\u2502   694 \u2502\
          \   \u2502   # if model is an encoder decoder, the encoder tokenizer class\
          \ is used by default   \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1825\
          \ in          \u2502<br>\u2502 from_pretrained                         \
          \                                                         \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502   1822 \u2502   \u2502   \u2502\
          \   else:                                                              \
          \           \u2502<br>\u2502   1823 \u2502   \u2502   \u2502   \u2502  \
          \ logger.info(f\"loading file {file_path} from cache at {resolved_vocab_fil\
          \  \u2502<br>\u2502   1824 \u2502   \u2502                             \
          \                                                        \u2502<br>\u2502\
          \ \u2771 1825 \u2502   \u2502   return cls._from_pretrained(           \
          \                                           \u2502<br>\u2502   1826 \u2502\
          \   \u2502   \u2502   resolved_vocab_files,                            \
          \                             \u2502<br>\u2502   1827 \u2502   \u2502  \
          \ \u2502   pretrained_model_name_or_path,                              \
          \                  \u2502<br>\u2502   1828 \u2502   \u2502   \u2502   init_configuration,\
          \                                                           \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1988\
          \ in          \u2502<br>\u2502 _from_pretrained                        \
          \                                                         \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502   1985 \u2502   \u2502   \
          \                                                                      \
          \            \u2502<br>\u2502   1986 \u2502   \u2502   # Instantiate tokenizer.\
          \                                                          \u2502<br>\u2502\
          \   1987 \u2502   \u2502   try:                                        \
          \                                      \u2502<br>\u2502 \u2771 1988 \u2502\
          \   \u2502   \u2502   tokenizer = cls(*init_inputs, **init_kwargs)     \
          \                             \u2502<br>\u2502   1989 \u2502   \u2502  \
          \ except OSError:                                                      \
          \             \u2502<br>\u2502   1990 \u2502   \u2502   \u2502   raise OSError(\
          \                                                                \u2502\
          <br>\u2502   1991 \u2502   \u2502   \u2502   \u2502   \"Unable to load vocabulary\
          \ from file. \"                                   \u2502<br>\u2502     \
          \                                                                      \
          \                       \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama_fast.py:93\
          \  \u2502<br>\u2502 in <strong>init</strong>                           \
          \                                                           \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502    90 \u2502   \u2502   add_eos_token=False,\
          \                                                               \u2502<br>\u2502\
          \    91 \u2502   \u2502   **kwargs,                                    \
          \                                      \u2502<br>\u2502    92 \u2502   ):\
          \                                                                      \
          \               \u2502<br>\u2502 \u2771  93 \u2502   \u2502   super().<strong>init</strong>(\
          \                                                                  \u2502\
          <br>\u2502    94 \u2502   \u2502   \u2502   vocab_file=vocab_file,     \
          \                                                    \u2502<br>\u2502  \
          \  95 \u2502   \u2502   \u2502   tokenizer_file=tokenizer_file,        \
          \                                         \u2502<br>\u2502    96 \u2502\
          \   \u2502   \u2502   clean_up_tokenization_spaces=clean_up_tokenization_spaces,\
          \                     \u2502<br>\u2502                                 \
          \                                                                 \u2502\
          <br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py:120\
          \ in <strong>init</strong>  \u2502<br>\u2502                           \
          \                                                                      \
          \ \u2502<br>\u2502   117 \u2502   \u2502   \u2502   slow_tokenizer = self.slow_tokenizer_class(*args,\
          \ **kwargs)                    \u2502<br>\u2502   118 \u2502   \u2502  \
          \ \u2502   fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)     \
          \                   \u2502<br>\u2502   119 \u2502   \u2502   else:     \
          \                                                                      \
          \   \u2502<br>\u2502 \u2771 120 \u2502   \u2502   \u2502   raise ValueError(\
          \                                                              \u2502<br>\u2502\
          \   121 \u2502   \u2502   \u2502   \u2502   \"Couldn't instantiate the backend\
          \ tokenizer from one of: \\n\"               \u2502<br>\u2502   122 \u2502\
          \   \u2502   \u2502   \u2502   \"(1) a <code>tokenizers</code> library serialization\
          \ file, \\n\"                        \u2502<br>\u2502   123 \u2502   \u2502\
          \   \u2502   \u2502   \"(2) a slow tokenizer instance to convert or \\n\"\
          \                           \u2502<br>\u2570\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F<br>ValueError:\
          \ Couldn't instantiate the backend tokenizer from one of:<br>(1) a <code>tokenizers</code>\
          \ library serialization file,<br>(2) a slow tokenizer instance to convert\
          \ or<br>(3) an equivalent slow tokenizer class to instantiate and convert.<br>You\
          \ need to have sentencepiece installed to convert a slow tokenizer to a\
          \ fast one.</p>\n"
        raw: "Hi, I tried to run your example code in google colab, but I get an error\
          \ when creating the tokenizer. \r\n\r\n\u256D\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u256E\r\n\u2502 in <cell line: 2>:2                 \
          \                                                             \u2502\r\n\
          \u2502 in init_hermes:7                                                \
          \                                 \u2502\r\n\u2502                     \
          \                                                                      \
          \       \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:691\
          \ in     \u2502\r\n\u2502 from_pretrained                              \
          \                                                    \u2502\r\n\u2502  \
          \                                                                      \
          \                          \u2502\r\n\u2502   688 \u2502   \u2502   \u2502\
          \   \u2502   raise ValueError(                                         \
          \                 \u2502\r\n\u2502   689 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   f\"Tokenizer class {tokenizer_class_candidate} does not exist\
          \ or is n   \u2502\r\n\u2502   690 \u2502   \u2502   \u2502   \u2502   )\
          \                                                                      \
          \    \u2502\r\n\u2502 \u2771 691 \u2502   \u2502   \u2502   return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *input   \u2502\r\n\u2502   692 \u2502   \u2502                      \
          \                                                                \u2502\r\
          \n\u2502   693 \u2502   \u2502   # Otherwise we have to be creative.   \
          \                                             \u2502\r\n\u2502   694 \u2502\
          \   \u2502   # if model is an encoder decoder, the encoder tokenizer class\
          \ is used by default   \u2502\r\n\u2502                                \
          \                                                                  \u2502\
          \r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1825\
          \ in          \u2502\r\n\u2502 from_pretrained                         \
          \                                                         \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502   1822 \u2502   \u2502   \u2502\
          \   else:                                                              \
          \           \u2502\r\n\u2502   1823 \u2502   \u2502   \u2502   \u2502  \
          \ logger.info(f\"loading file {file_path} from cache at {resolved_vocab_fil\
          \  \u2502\r\n\u2502   1824 \u2502   \u2502                             \
          \                                                        \u2502\r\n\u2502\
          \ \u2771 1825 \u2502   \u2502   return cls._from_pretrained(           \
          \                                           \u2502\r\n\u2502   1826 \u2502\
          \   \u2502   \u2502   resolved_vocab_files,                            \
          \                             \u2502\r\n\u2502   1827 \u2502   \u2502  \
          \ \u2502   pretrained_model_name_or_path,                              \
          \                  \u2502\r\n\u2502   1828 \u2502   \u2502   \u2502   init_configuration,\
          \                                                           \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1988\
          \ in          \u2502\r\n\u2502 _from_pretrained                        \
          \                                                         \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502   1985 \u2502   \u2502   \
          \                                                                      \
          \            \u2502\r\n\u2502   1986 \u2502   \u2502   # Instantiate tokenizer.\
          \                                                          \u2502\r\n\u2502\
          \   1987 \u2502   \u2502   try:                                        \
          \                                      \u2502\r\n\u2502 \u2771 1988 \u2502\
          \   \u2502   \u2502   tokenizer = cls(*init_inputs, **init_kwargs)     \
          \                             \u2502\r\n\u2502   1989 \u2502   \u2502  \
          \ except OSError:                                                      \
          \             \u2502\r\n\u2502   1990 \u2502   \u2502   \u2502   raise OSError(\
          \                                                                \u2502\r\
          \n\u2502   1991 \u2502   \u2502   \u2502   \u2502   \"Unable to load vocabulary\
          \ from file. \"                                   \u2502\r\n\u2502     \
          \                                                                      \
          \                       \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama_fast.py:93\
          \  \u2502\r\n\u2502 in __init__                                        \
          \                                              \u2502\r\n\u2502        \
          \                                                                      \
          \                    \u2502\r\n\u2502    90 \u2502   \u2502   add_eos_token=False,\
          \                                                               \u2502\r\
          \n\u2502    91 \u2502   \u2502   **kwargs,                             \
          \                                             \u2502\r\n\u2502    92 \u2502\
          \   ):                                                                 \
          \                    \u2502\r\n\u2502 \u2771  93 \u2502   \u2502   super().__init__(\
          \                                                                  \u2502\
          \r\n\u2502    94 \u2502   \u2502   \u2502   vocab_file=vocab_file,     \
          \                                                    \u2502\r\n\u2502  \
          \  95 \u2502   \u2502   \u2502   tokenizer_file=tokenizer_file,        \
          \                                         \u2502\r\n\u2502    96 \u2502\
          \   \u2502   \u2502   clean_up_tokenization_spaces=clean_up_tokenization_spaces,\
          \                     \u2502\r\n\u2502                                 \
          \                                                                 \u2502\
          \r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py:120\
          \ in __init__  \u2502\r\n\u2502                                        \
          \                                                          \u2502\r\n\u2502\
          \   117 \u2502   \u2502   \u2502   slow_tokenizer = self.slow_tokenizer_class(*args,\
          \ **kwargs)                    \u2502\r\n\u2502   118 \u2502   \u2502  \
          \ \u2502   fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)     \
          \                   \u2502\r\n\u2502   119 \u2502   \u2502   else:     \
          \                                                                      \
          \   \u2502\r\n\u2502 \u2771 120 \u2502   \u2502   \u2502   raise ValueError(\
          \                                                              \u2502\r\n\
          \u2502   121 \u2502   \u2502   \u2502   \u2502   \"Couldn't instantiate\
          \ the backend tokenizer from one of: \\n\"               \u2502\r\n\u2502\
          \   122 \u2502   \u2502   \u2502   \u2502   \"(1) a `tokenizers` library\
          \ serialization file, \\n\"                        \u2502\r\n\u2502   123\
          \ \u2502   \u2502   \u2502   \u2502   \"(2) a slow tokenizer instance to\
          \ convert or \\n\"                           \u2502\r\n\u2570\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256F\r\nValueError: Couldn't instantiate the backend tokenizer from one\
          \ of: \r\n(1) a `tokenizers` library serialization file, \r\n(2) a slow\
          \ tokenizer instance to convert or \r\n(3) an equivalent slow tokenizer\
          \ class to instantiate and convert. \r\nYou need to have sentencepiece installed\
          \ to convert a slow tokenizer to a fast one."
        updatedAt: '2023-06-19T20:07:28.498Z'
      numEdits: 0
      reactions: []
    id: 6490b580f5a6d47892e015fb
    type: comment
  author: Ernobyl
  content: "Hi, I tried to run your example code in google colab, but I get an error\
    \ when creating the tokenizer. \r\n\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback\
    \ (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502 in\
    \ <cell line: 2>:2                                                           \
    \                   \u2502\r\n\u2502 in init_hermes:7                        \
    \                                                         \u2502\r\n\u2502   \
    \                                                                            \
    \                   \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:691\
    \ in     \u2502\r\n\u2502 from_pretrained                                    \
    \                                              \u2502\r\n\u2502              \
    \                                                                            \
    \        \u2502\r\n\u2502   688 \u2502   \u2502   \u2502   \u2502   raise ValueError(\
    \                                                          \u2502\r\n\u2502  \
    \ 689 \u2502   \u2502   \u2502   \u2502   \u2502   f\"Tokenizer class {tokenizer_class_candidate}\
    \ does not exist or is n   \u2502\r\n\u2502   690 \u2502   \u2502   \u2502   \u2502\
    \   )                                                                        \
    \  \u2502\r\n\u2502 \u2771 691 \u2502   \u2502   \u2502   return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *input   \u2502\r\n\u2502   692 \u2502   \u2502                            \
    \                                                          \u2502\r\n\u2502  \
    \ 693 \u2502   \u2502   # Otherwise we have to be creative.                  \
    \                              \u2502\r\n\u2502   694 \u2502   \u2502   # if model\
    \ is an encoder decoder, the encoder tokenizer class is used by default   \u2502\
    \r\n\u2502                                                                   \
    \                               \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1825\
    \ in          \u2502\r\n\u2502 from_pretrained                               \
    \                                                   \u2502\r\n\u2502         \
    \                                                                            \
    \             \u2502\r\n\u2502   1822 \u2502   \u2502   \u2502   else:       \
    \                                                                  \u2502\r\n\u2502\
    \   1823 \u2502   \u2502   \u2502   \u2502   logger.info(f\"loading file {file_path}\
    \ from cache at {resolved_vocab_fil  \u2502\r\n\u2502   1824 \u2502   \u2502 \
    \                                                                            \
    \        \u2502\r\n\u2502 \u2771 1825 \u2502   \u2502   return cls._from_pretrained(\
    \                                                      \u2502\r\n\u2502   1826\
    \ \u2502   \u2502   \u2502   resolved_vocab_files,                           \
    \                              \u2502\r\n\u2502   1827 \u2502   \u2502   \u2502\
    \   pretrained_model_name_or_path,                                           \
    \     \u2502\r\n\u2502   1828 \u2502   \u2502   \u2502   init_configuration, \
    \                                                          \u2502\r\n\u2502  \
    \                                                                            \
    \                    \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1988\
    \ in          \u2502\r\n\u2502 _from_pretrained                              \
    \                                                   \u2502\r\n\u2502         \
    \                                                                            \
    \             \u2502\r\n\u2502   1985 \u2502   \u2502                        \
    \                                                             \u2502\r\n\u2502\
    \   1986 \u2502   \u2502   # Instantiate tokenizer.                          \
    \                                \u2502\r\n\u2502   1987 \u2502   \u2502   try:\
    \                                                                            \
    \  \u2502\r\n\u2502 \u2771 1988 \u2502   \u2502   \u2502   tokenizer = cls(*init_inputs,\
    \ **init_kwargs)                                  \u2502\r\n\u2502   1989 \u2502\
    \   \u2502   except OSError:                                                 \
    \                  \u2502\r\n\u2502   1990 \u2502   \u2502   \u2502   raise OSError(\
    \                                                                \u2502\r\n\u2502\
    \   1991 \u2502   \u2502   \u2502   \u2502   \"Unable to load vocabulary from\
    \ file. \"                                   \u2502\r\n\u2502                \
    \                                                                            \
    \      \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama_fast.py:93\
    \  \u2502\r\n\u2502 in __init__                                              \
    \                                        \u2502\r\n\u2502                    \
    \                                                                            \
    \  \u2502\r\n\u2502    90 \u2502   \u2502   add_eos_token=False,             \
    \                                                  \u2502\r\n\u2502    91 \u2502\
    \   \u2502   **kwargs,                                                       \
    \                   \u2502\r\n\u2502    92 \u2502   ):                       \
    \                                                              \u2502\r\n\u2502\
    \ \u2771  93 \u2502   \u2502   super().__init__(                             \
    \                                     \u2502\r\n\u2502    94 \u2502   \u2502 \
    \  \u2502   vocab_file=vocab_file,                                           \
    \              \u2502\r\n\u2502    95 \u2502   \u2502   \u2502   tokenizer_file=tokenizer_file,\
    \                                                 \u2502\r\n\u2502    96 \u2502\
    \   \u2502   \u2502   clean_up_tokenization_spaces=clean_up_tokenization_spaces,\
    \                     \u2502\r\n\u2502                                       \
    \                                                           \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py:120\
    \ in __init__  \u2502\r\n\u2502                                              \
    \                                                    \u2502\r\n\u2502   117 \u2502\
    \   \u2502   \u2502   slow_tokenizer = self.slow_tokenizer_class(*args, **kwargs)\
    \                    \u2502\r\n\u2502   118 \u2502   \u2502   \u2502   fast_tokenizer\
    \ = convert_slow_tokenizer(slow_tokenizer)                        \u2502\r\n\u2502\
    \   119 \u2502   \u2502   else:                                              \
    \                                \u2502\r\n\u2502 \u2771 120 \u2502   \u2502 \
    \  \u2502   raise ValueError(                                                \
    \              \u2502\r\n\u2502   121 \u2502   \u2502   \u2502   \u2502   \"Couldn't\
    \ instantiate the backend tokenizer from one of: \\n\"               \u2502\r\n\
    \u2502   122 \u2502   \u2502   \u2502   \u2502   \"(1) a `tokenizers` library\
    \ serialization file, \\n\"                        \u2502\r\n\u2502   123 \u2502\
    \   \u2502   \u2502   \u2502   \"(2) a slow tokenizer instance to convert or \\\
    n\"                           \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u256F\r\nValueError: Couldn't instantiate the backend tokenizer from one\
    \ of: \r\n(1) a `tokenizers` library serialization file, \r\n(2) a slow tokenizer\
    \ instance to convert or \r\n(3) an equivalent slow tokenizer class to instantiate\
    \ and convert. \r\nYou need to have sentencepiece installed to convert a slow\
    \ tokenizer to a fast one."
  created_at: 2023-06-19 19:07:28+00:00
  edited: false
  hidden: false
  id: 6490b580f5a6d47892e015fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-19T20:42:55.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9792338609695435
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The error is caused by the fact that the code is requesting a fast
          tokenizer (<code>use_fast=True</code>) and there wasn''t uploaded in this
          repo. </p>

          <p>transformers will try to auto convert, but that requires the <code>sentencepiece</code>
          and <code>protobuf</code> libraries installed.</p>

          <p>But you don''t need to worry about that as I''ve just uploaded a fast
          tokenizer.  Download the newly uploaded <code>tokenizer.json</code> and
          try again.</p>

          '
        raw: "The error is caused by the fact that the code is requesting a fast tokenizer\
          \ (`use_fast=True`) and there wasn't uploaded in this repo. \n\ntransformers\
          \ will try to auto convert, but that requires the `sentencepiece` and `protobuf`\
          \ libraries installed.\n\nBut you don't need to worry about that as I've\
          \ just uploaded a fast tokenizer.  Download the newly uploaded `tokenizer.json`\
          \ and try again."
        updatedAt: '2023-06-19T20:42:55.439Z'
      numEdits: 0
      reactions: []
    id: 6490bdcfb95c3f0a1e58401f
    type: comment
  author: TheBloke
  content: "The error is caused by the fact that the code is requesting a fast tokenizer\
    \ (`use_fast=True`) and there wasn't uploaded in this repo. \n\ntransformers will\
    \ try to auto convert, but that requires the `sentencepiece` and `protobuf` libraries\
    \ installed.\n\nBut you don't need to worry about that as I've just uploaded a\
    \ fast tokenizer.  Download the newly uploaded `tokenizer.json` and try again."
  created_at: 2023-06-19 19:42:55+00:00
  edited: false
  hidden: false
  id: 6490bdcfb95c3f0a1e58401f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0d1fb10358be1ad3085099c754354eea.svg
      fullname: 'Hendrik  Jacobs '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ernobyl
      type: user
    createdAt: '2023-06-20T20:05:24.000Z'
    data:
      edited: false
      editors:
      - Ernobyl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9945768117904663
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0d1fb10358be1ad3085099c754354eea.svg
          fullname: 'Hendrik  Jacobs '
          isHf: false
          isPro: false
          name: Ernobyl
          type: user
        html: '<p>Thank you, it''s working now!</p>

          '
        raw: Thank you, it's working now!
        updatedAt: '2023-06-20T20:05:24.785Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64920684aa27df62883e344c
    id: 64920684aa27df62883e344b
    type: comment
  author: Ernobyl
  content: Thank you, it's working now!
  created_at: 2023-06-20 19:05:24+00:00
  edited: false
  hidden: false
  id: 64920684aa27df62883e344b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0d1fb10358be1ad3085099c754354eea.svg
      fullname: 'Hendrik  Jacobs '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ernobyl
      type: user
    createdAt: '2023-06-20T20:05:24.000Z'
    data:
      status: closed
    id: 64920684aa27df62883e344c
    type: status-change
  author: Ernobyl
  created_at: 2023-06-20 19:05:24+00:00
  id: 64920684aa27df62883e344c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/Nous-Hermes-13B-GPTQ
repo_type: model
status: closed
target_branch: null
title: Tokenizer error
