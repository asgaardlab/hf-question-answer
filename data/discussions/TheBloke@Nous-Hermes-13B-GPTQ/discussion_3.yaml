!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kristada673
conflicting_files: null
created_at: 2023-06-10 06:14:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e84d6c171a7aaf95bf9a0836e2c80869.svg
      fullname: Prasanta Saikia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kristada673
      type: user
    createdAt: '2023-06-10T07:14:30.000Z'
    data:
      edited: false
      editors:
      - kristada673
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.18616683781147003
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e84d6c171a7aaf95bf9a0836e2c80869.svg
          fullname: Prasanta Saikia
          isHf: false
          isPro: false
          name: kristada673
          type: user
        html: "<p>I copy-pasted your Python script and ran it, gives the following\
          \ error:</p>\n<pre><code>C:\\Users\\Username\\Documents&gt;python script.py\n\
          \u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502\
          \ C:\\Users\\Username\\Documents\\script.py:12 in &lt;module&gt;       \
          \           \u2502\n\u2502                                             \
          \                                                     \u2502\n\u2502   \
          \ 9                                                                    \
          \                         \u2502\n\u2502   10 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)                \u2502\n\u2502   11                    \
          \                                                                      \
          \   \u2502\n\u2502 \u2771 12 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\
          \                              \u2502\n\u2502   13 \u2502   \u2502   model_basename=model_basename,\
          \                                                      \u2502\n\u2502  \
          \ 14 \u2502   \u2502   use_safetensors=True,                           \
          \                                    \u2502\n\u2502   15 \u2502   \u2502\
          \   trust_remote_code=True,                                            \
          \                 \u2502\n\u2502                                       \
          \                                                           \u2502\n\u2502\
          \ C:\\Users\\Username\\anaconda3\\lib\\site-packages\\auto_gptq\\modeling\\\
          auto.py:63 in from_quantized \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502   60 \u2502   \u2502   trust_remote_code: bool = False        \
          \                                             \u2502\n\u2502   61 \u2502\
          \   ) -&gt; BaseGPTQForCausalLM:                                       \
          \                        \u2502\n\u2502   62 \u2502   \u2502   model_type\
          \ = check_and_get_model_type(save_dir)                                 \
          \    \u2502\n\u2502 \u2771 63 \u2502   \u2502   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\
          \                         \u2502\n\u2502   64 \u2502   \u2502   \u2502 \
          \  save_dir=save_dir,                                                  \
          \            \u2502\n\u2502   65 \u2502   \u2502   \u2502   device=device,\
          \                                                                  \u2502\
          \n\u2502   66 \u2502   \u2502   \u2502   use_safetensors=use_safetensors,\
          \                                                \u2502\n\u2502        \
          \                                                                      \
          \                    \u2502\n\u2502 C:\\Users\\Username\\anaconda3\\lib\\\
          site-packages\\auto_gptq\\modeling\\_base.py:501 in              \u2502\n\
          \u2502 from_quantized                                                  \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502   498 \u2502   \u2502   \u2502   raise TypeError(f\"\
          {config.model_type} isn't supported yet.\")                   \u2502\n\u2502\
          \   499 \u2502   \u2502                                                \
          \                                      \u2502\n\u2502   500 \u2502   \u2502\
          \   if quantize_config is None:                                        \
          \                \u2502\n\u2502 \u2771 501 \u2502   \u2502   \u2502   quantize_config\
          \ = BaseQuantizeConfig.from_pretrained(save_dir)                 \u2502\n\
          \u2502   502 \u2502   \u2502                                           \
          \                                           \u2502\n\u2502   503 \u2502\
          \   \u2502   if model_basename is None:                                \
          \                         \u2502\n\u2502   504 \u2502   \u2502   \u2502\
          \   model_basename = f\"gptq_model-{quantize_config.bits}bit-{quantize_config.gro\
          \   \u2502\n\u2502                                                     \
          \                                             \u2502\n\u2502 C:\\Users\\\
          Username\\anaconda3\\lib\\site-packages\\auto_gptq\\modeling\\_base.py:51\
          \ in               \u2502\n\u2502 from_pretrained                      \
          \                                                            \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502    48 \u2502               \
          \                                                                      \
          \     \u2502\n\u2502    49 \u2502   @classmethod                       \
          \                                                    \u2502\n\u2502    50\
          \ \u2502   def from_pretrained(cls, save_dir: str):                    \
          \                           \u2502\n\u2502 \u2771  51 \u2502   \u2502  \
          \ with open(join(save_dir, \"quantize_config.json\"), \"r\", encoding=\"\
          utf-8\") as f:     \u2502\n\u2502    52 \u2502   \u2502   \u2502   return\
          \ cls(**json.load(f))                                                  \
          \   \u2502\n\u2502    53 \u2502                                        \
          \                                                  \u2502\n\u2502    54\
          \ \u2502   def to_dict(self):                                          \
          \                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: 'TheBloke/Nous-Hermes-13B-GPTQ\\\\\
          quantize_config.json'\n</code></pre>\n<p>I suspect it might be due to the\
          \ directory naming structure in Windows vs Linux. But I don't want to edit\
          \ the library files before ensuring that that's indeed the issue.</p>\n"
        raw: "I copy-pasted your Python script and ran it, gives the following error:\r\
          \n\r\n```\r\nC:\\Users\\Username\\Documents>python script.py\r\n\u256D\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502 C:\\Users\\Username\\\
          Documents\\script.py:12 in <module>                  \u2502\r\n\u2502  \
          \                                                                      \
          \                          \u2502\r\n\u2502    9                       \
          \                                                                      \u2502\
          \r\n\u2502   10 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)                \u2502\r\n\u2502   11                  \
          \                                                                      \
          \     \u2502\r\n\u2502 \u2771 12 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\
          \                              \u2502\r\n\u2502   13 \u2502   \u2502   model_basename=model_basename,\
          \                                                      \u2502\r\n\u2502\
          \   14 \u2502   \u2502   use_safetensors=True,                         \
          \                                      \u2502\r\n\u2502   15 \u2502   \u2502\
          \   trust_remote_code=True,                                            \
          \                 \u2502\r\n\u2502                                     \
          \                                                             \u2502\r\n\
          \u2502 C:\\Users\\Username\\anaconda3\\lib\\site-packages\\auto_gptq\\modeling\\\
          auto.py:63 in from_quantized \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502   60 \u2502   \u2502   trust_remote_code: bool = False\
          \                                                     \u2502\r\n\u2502 \
          \  61 \u2502   ) -> BaseGPTQForCausalLM:                               \
          \                                \u2502\r\n\u2502   62 \u2502   \u2502 \
          \  model_type = check_and_get_model_type(save_dir)                     \
          \                \u2502\r\n\u2502 \u2771 63 \u2502   \u2502   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\
          \                         \u2502\r\n\u2502   64 \u2502   \u2502   \u2502\
          \   save_dir=save_dir,                                                 \
          \             \u2502\r\n\u2502   65 \u2502   \u2502   \u2502   device=device,\
          \                                                                  \u2502\
          \r\n\u2502   66 \u2502   \u2502   \u2502   use_safetensors=use_safetensors,\
          \                                                \u2502\r\n\u2502      \
          \                                                                      \
          \                      \u2502\r\n\u2502 C:\\Users\\Username\\anaconda3\\\
          lib\\site-packages\\auto_gptq\\modeling\\_base.py:501 in              \u2502\
          \r\n\u2502 from_quantized                                              \
          \                                     \u2502\r\n\u2502                 \
          \                                                                      \
          \           \u2502\r\n\u2502   498 \u2502   \u2502   \u2502   raise TypeError(f\"\
          {config.model_type} isn't supported yet.\")                   \u2502\r\n\
          \u2502   499 \u2502   \u2502                                           \
          \                                           \u2502\r\n\u2502   500 \u2502\
          \   \u2502   if quantize_config is None:                               \
          \                         \u2502\r\n\u2502 \u2771 501 \u2502   \u2502  \
          \ \u2502   quantize_config = BaseQuantizeConfig.from_pretrained(save_dir)\
          \                 \u2502\r\n\u2502   502 \u2502   \u2502               \
          \                                                                      \
          \ \u2502\r\n\u2502   503 \u2502   \u2502   if model_basename is None:  \
          \                                                       \u2502\r\n\u2502\
          \   504 \u2502   \u2502   \u2502   model_basename = f\"gptq_model-{quantize_config.bits}bit-{quantize_config.gro\
          \   \u2502\r\n\u2502                                                   \
          \                                               \u2502\r\n\u2502 C:\\Users\\\
          Username\\anaconda3\\lib\\site-packages\\auto_gptq\\modeling\\_base.py:51\
          \ in               \u2502\r\n\u2502 from_pretrained                    \
          \                                                              \u2502\r\n\
          \u2502                                                                 \
          \                                 \u2502\r\n\u2502    48 \u2502        \
          \                                                                      \
          \            \u2502\r\n\u2502    49 \u2502   @classmethod              \
          \                                                             \u2502\r\n\
          \u2502    50 \u2502   def from_pretrained(cls, save_dir: str):         \
          \                                      \u2502\r\n\u2502 \u2771  51 \u2502\
          \   \u2502   with open(join(save_dir, \"quantize_config.json\"), \"r\",\
          \ encoding=\"utf-8\") as f:     \u2502\r\n\u2502    52 \u2502   \u2502 \
          \  \u2502   return cls(**json.load(f))                                 \
          \                    \u2502\r\n\u2502    53 \u2502                     \
          \                                                                     \u2502\
          \r\n\u2502    54 \u2502   def to_dict(self):                           \
          \                                          \u2502\r\n\u2570\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\
          \r\nFileNotFoundError: [Errno 2] No such file or directory: 'TheBloke/Nous-Hermes-13B-GPTQ\\\
          \\quantize_config.json'\r\n```\r\n\r\nI suspect it might be due to the directory\
          \ naming structure in Windows vs Linux. But I don't want to edit the library\
          \ files before ensuring that that's indeed the issue."
        updatedAt: '2023-06-10T07:14:30.969Z'
      numEdits: 0
      reactions: []
    id: 648422d6ab2e70016beaf40a
    type: comment
  author: kristada673
  content: "I copy-pasted your Python script and ran it, gives the following error:\r\
    \n\r\n```\r\nC:\\Users\\Username\\Documents>python script.py\r\n\u256D\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u256E\r\n\u2502 C:\\Users\\Username\\Documents\\script.py:12 in <module>\
    \                  \u2502\r\n\u2502                                          \
    \                                                        \u2502\r\n\u2502    9\
    \                                                                            \
    \                 \u2502\r\n\u2502   10 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)                \u2502\r\n\u2502   11                        \
    \                                                                     \u2502\r\
    \n\u2502 \u2771 12 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\
    \                              \u2502\r\n\u2502   13 \u2502   \u2502   model_basename=model_basename,\
    \                                                      \u2502\r\n\u2502   14 \u2502\
    \   \u2502   use_safetensors=True,                                           \
    \                    \u2502\r\n\u2502   15 \u2502   \u2502   trust_remote_code=True,\
    \                                                             \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502 C:\\Users\\Username\\anaconda3\\lib\\\
    site-packages\\auto_gptq\\modeling\\auto.py:63 in from_quantized \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502   60 \u2502   \u2502   trust_remote_code:\
    \ bool = False                                                     \u2502\r\n\u2502\
    \   61 \u2502   ) -> BaseGPTQForCausalLM:                                    \
    \                           \u2502\r\n\u2502   62 \u2502   \u2502   model_type\
    \ = check_and_get_model_type(save_dir)                                     \u2502\
    \r\n\u2502 \u2771 63 \u2502   \u2502   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\
    \                         \u2502\r\n\u2502   64 \u2502   \u2502   \u2502   save_dir=save_dir,\
    \                                                              \u2502\r\n\u2502\
    \   65 \u2502   \u2502   \u2502   device=device,                             \
    \                                     \u2502\r\n\u2502   66 \u2502   \u2502  \
    \ \u2502   use_safetensors=use_safetensors,                                  \
    \              \u2502\r\n\u2502                                              \
    \                                                    \u2502\r\n\u2502 C:\\Users\\\
    Username\\anaconda3\\lib\\site-packages\\auto_gptq\\modeling\\_base.py:501 in\
    \              \u2502\r\n\u2502 from_quantized                               \
    \                                                    \u2502\r\n\u2502        \
    \                                                                            \
    \              \u2502\r\n\u2502   498 \u2502   \u2502   \u2502   raise TypeError(f\"\
    {config.model_type} isn't supported yet.\")                   \u2502\r\n\u2502\
    \   499 \u2502   \u2502                                                      \
    \                                \u2502\r\n\u2502   500 \u2502   \u2502   if quantize_config\
    \ is None:                                                        \u2502\r\n\u2502\
    \ \u2771 501 \u2502   \u2502   \u2502   quantize_config = BaseQuantizeConfig.from_pretrained(save_dir)\
    \                 \u2502\r\n\u2502   502 \u2502   \u2502                     \
    \                                                                 \u2502\r\n\u2502\
    \   503 \u2502   \u2502   if model_basename is None:                         \
    \                                \u2502\r\n\u2502   504 \u2502   \u2502   \u2502\
    \   model_basename = f\"gptq_model-{quantize_config.bits}bit-{quantize_config.gro\
    \   \u2502\r\n\u2502                                                         \
    \                                         \u2502\r\n\u2502 C:\\Users\\Username\\\
    anaconda3\\lib\\site-packages\\auto_gptq\\modeling\\_base.py:51 in           \
    \    \u2502\r\n\u2502 from_pretrained                                        \
    \                                          \u2502\r\n\u2502                  \
    \                                                                            \
    \    \u2502\r\n\u2502    48 \u2502                                           \
    \                                               \u2502\r\n\u2502    49 \u2502\
    \   @classmethod                                                             \
    \              \u2502\r\n\u2502    50 \u2502   def from_pretrained(cls, save_dir:\
    \ str):                                               \u2502\r\n\u2502 \u2771\
    \  51 \u2502   \u2502   with open(join(save_dir, \"quantize_config.json\"), \"\
    r\", encoding=\"utf-8\") as f:     \u2502\r\n\u2502    52 \u2502   \u2502   \u2502\
    \   return cls(**json.load(f))                                               \
    \      \u2502\r\n\u2502    53 \u2502                                         \
    \                                                 \u2502\r\n\u2502    54 \u2502\
    \   def to_dict(self):                                                       \
    \              \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\
    \r\nFileNotFoundError: [Errno 2] No such file or directory: 'TheBloke/Nous-Hermes-13B-GPTQ\\\
    \\quantize_config.json'\r\n```\r\n\r\nI suspect it might be due to the directory\
    \ naming structure in Windows vs Linux. But I don't want to edit the library files\
    \ before ensuring that that's indeed the issue."
  created_at: 2023-06-10 06:14:30+00:00
  edited: false
  hidden: false
  id: 648422d6ab2e70016beaf40a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Nous-Hermes-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: How to use the model in Windows?
