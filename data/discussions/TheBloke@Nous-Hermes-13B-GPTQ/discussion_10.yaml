!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Law00
conflicting_files: null
created_at: 2023-06-30 01:56:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a3aa876d835ba9e668f503f94f9e25b.svg
      fullname: E
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Law00
      type: user
    createdAt: '2023-06-30T02:56:30.000Z'
    data:
      edited: true
      editors:
      - Law00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.608002245426178
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a3aa876d835ba9e668f503f94f9e25b.svg
          fullname: E
          isHf: false
          isPro: false
          name: Law00
          type: user
        html: '<p>Bonjour everyone, </p>

          <p>Just installed text generator UI via single click installer.<br>I had
          trouble because since I have an AMD GPU, seems things didnt install right.<br>So
          I ran the requirements txt again. And am running it in --cpu mode. But now
          it says this whenever I try to text/enter anything and generate/send it.
          Cant type a single thing. </p>

          <p>Would love some assistance. Thanks for making this!<br>Also this thread:<br><a
          href="https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/discussions/12#646650c33b99ed9970fc64cc">https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/discussions/12#646650c33b99ed9970fc64cc</a></p>

          <p>Doesnt help, it isnt specific to this issue.<br>Its not replying at all.<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/649e405f2ccae3ea1f2ebdf0/sp53D-6D4DpQUgrz7GbFg.png"><img
          alt="Screenshot 2023-06-30 055558.png" src="https://cdn-uploads.huggingface.co/production/uploads/649e405f2ccae3ea1f2ebdf0/sp53D-6D4DpQUgrz7GbFg.png"></a><br>Tried:
          --cpu mode<br>Alpaca parameter<br>upgrading and updating<br>It does say
          this after starting</p>

          <p>2023-06-30 05:32:48 WARNING:CUDA extension not installed.<br>2023-06-30
          05:32:50 WARNING:The model weights are not tied. Please use the <code>tie_weights</code>
          method before using the <code>infer_auto_device</code> function.<br>2023-06-30
          05:32:50 WARNING:The safetensors archive passed at models\TheBloke_Nous-Hermes-13B-GPTQ\nous-hermes-13b-GPTQ-4bit-128g.no-act.order.safetensors
          does not contain metadata. Make sure to save your model with the <code>save_pretrained</code>
          method. Defaulting to ''pt'' metadata.<br>2023-06-30 05:33:09 WARNING:skip
          module injection for FusedLlamaMLPForQuantizedModel not support integrate
          without triton yet.</p>

          <p>Traceback (most recent call last):<br>  File "C:\Users\Nicholas\Documents\oobabooga_windows\text-generation-webui\modules\callbacks.py",
          line 55, in gentask<br>    ret = self.mfunc(callback=_callback, *args, **self.kwargs)<br>  File
          "C:\Users\Nicholas\Documents\oobabooga_windows\text-generation-webui\modules\text_generation.py",
          line 289, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "C:\Users\Nicholas\Documents\oobabooga_windows\installer_files\env\lib\site-packages\auto_gptq\modeling_base.py",
          line 422, in generate<br>    with torch.inference_mode(), torch.amp.autocast(device_type=self.device.type):<br>  File
          "C:\Users\Nicholas\Documents\oobabooga_windows\installer_files\env\lib\site-packages\auto_gptq\modeling_base.py",
          line 411, in device<br>    device = [d for d in self.hf_device_map.values()
          if d not in {''cpu'', ''disk''}][0]<br>IndexError: list index out of range<br>Output
          generated in 0.39 seconds (0.00 tokens/s, 0 tokens, context 44, seed 1283501170)</p>

          <p>32GB DDR5<br>RX 6800 16GB VRAM<br>13700K </p>

          '
        raw: "Bonjour everyone, \n\nJust installed text generator UI via single click\
          \ installer. \nI had trouble because since I have an AMD GPU, seems things\
          \ didnt install right. \nSo I ran the requirements txt again. And am running\
          \ it in --cpu mode. But now it says this whenever I try to text/enter anything\
          \ and generate/send it. Cant type a single thing. \n\nWould love some assistance.\
          \ Thanks for making this! \nAlso this thread: \nhttps://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/discussions/12#646650c33b99ed9970fc64cc\n\
          \nDoesnt help, it isnt specific to this issue.\nIts not replying at all.\n\
          ![Screenshot 2023-06-30 055558.png](https://cdn-uploads.huggingface.co/production/uploads/649e405f2ccae3ea1f2ebdf0/sp53D-6D4DpQUgrz7GbFg.png)\n\
          Tried: --cpu mode \nAlpaca parameter \nupgrading and updating\nIt does say\
          \ this after starting\n\n2023-06-30 05:32:48 WARNING:CUDA extension not\
          \ installed.\n2023-06-30 05:32:50 WARNING:The model weights are not tied.\
          \ Please use the `tie_weights` method before using the `infer_auto_device`\
          \ function.\n2023-06-30 05:32:50 WARNING:The safetensors archive passed\
          \ at models\\TheBloke_Nous-Hermes-13B-GPTQ\\nous-hermes-13b-GPTQ-4bit-128g.no-act.order.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\n2023-06-30 05:33:09 WARNING:skip\
          \ module injection for FusedLlamaMLPForQuantizedModel not support integrate\
          \ without triton yet.\n\nTraceback (most recent call last):\n  File \"C:\\\
          Users\\Nicholas\\Documents\\oobabooga_windows\\text-generation-webui\\modules\\\
          callbacks.py\", line 55, in gentask\n    ret = self.mfunc(callback=_callback,\
          \ *args, **self.kwargs)\n  File \"C:\\Users\\Nicholas\\Documents\\oobabooga_windows\\\
          text-generation-webui\\modules\\text_generation.py\", line 289, in generate_with_callback\n\
          \    shared.model.generate(**kwargs)\n  File \"C:\\Users\\Nicholas\\Documents\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
          modeling\\_base.py\", line 422, in generate\n    with torch.inference_mode(),\
          \ torch.amp.autocast(device_type=self.device.type):\n  File \"C:\\Users\\\
          Nicholas\\Documents\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          auto_gptq\\modeling\\_base.py\", line 411, in device\n    device = [d for\
          \ d in self.hf_device_map.values() if d not in {'cpu', 'disk'}][0]\nIndexError:\
          \ list index out of range\nOutput generated in 0.39 seconds (0.00 tokens/s,\
          \ 0 tokens, context 44, seed 1283501170)\n\n32GB DDR5 \nRX 6800 16GB VRAM\n\
          13700K \n"
        updatedAt: '2023-06-30T03:12:50.431Z'
      numEdits: 2
      reactions: []
    id: 649e445e1b8a453e8997a24a
    type: comment
  author: Law00
  content: "Bonjour everyone, \n\nJust installed text generator UI via single click\
    \ installer. \nI had trouble because since I have an AMD GPU, seems things didnt\
    \ install right. \nSo I ran the requirements txt again. And am running it in --cpu\
    \ mode. But now it says this whenever I try to text/enter anything and generate/send\
    \ it. Cant type a single thing. \n\nWould love some assistance. Thanks for making\
    \ this! \nAlso this thread: \nhttps://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/discussions/12#646650c33b99ed9970fc64cc\n\
    \nDoesnt help, it isnt specific to this issue.\nIts not replying at all.\n![Screenshot\
    \ 2023-06-30 055558.png](https://cdn-uploads.huggingface.co/production/uploads/649e405f2ccae3ea1f2ebdf0/sp53D-6D4DpQUgrz7GbFg.png)\n\
    Tried: --cpu mode \nAlpaca parameter \nupgrading and updating\nIt does say this\
    \ after starting\n\n2023-06-30 05:32:48 WARNING:CUDA extension not installed.\n\
    2023-06-30 05:32:50 WARNING:The model weights are not tied. Please use the `tie_weights`\
    \ method before using the `infer_auto_device` function.\n2023-06-30 05:32:50 WARNING:The\
    \ safetensors archive passed at models\\TheBloke_Nous-Hermes-13B-GPTQ\\nous-hermes-13b-GPTQ-4bit-128g.no-act.order.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\n2023-06-30 05:33:09 WARNING:skip module\
    \ injection for FusedLlamaMLPForQuantizedModel not support integrate without triton\
    \ yet.\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Nicholas\\Documents\\\
    oobabooga_windows\\text-generation-webui\\modules\\callbacks.py\", line 55, in\
    \ gentask\n    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n  File\
    \ \"C:\\Users\\Nicholas\\Documents\\oobabooga_windows\\text-generation-webui\\\
    modules\\text_generation.py\", line 289, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
    \  File \"C:\\Users\\Nicholas\\Documents\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\auto_gptq\\modeling\\_base.py\", line 422, in generate\n\
    \    with torch.inference_mode(), torch.amp.autocast(device_type=self.device.type):\n\
    \  File \"C:\\Users\\Nicholas\\Documents\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\auto_gptq\\modeling\\_base.py\", line 411, in device\n\
    \    device = [d for d in self.hf_device_map.values() if d not in {'cpu', 'disk'}][0]\n\
    IndexError: list index out of range\nOutput generated in 0.39 seconds (0.00 tokens/s,\
    \ 0 tokens, context 44, seed 1283501170)\n\n32GB DDR5 \nRX 6800 16GB VRAM\n13700K\
    \ \n"
  created_at: 2023-06-30 01:56:30+00:00
  edited: true
  hidden: false
  id: 649e445e1b8a453e8997a24a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-08T09:56:58.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9732417464256287
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>AutoGPTQ doesn''t do GPU acceleration on AMD GPUs unfortunately.  But
          you can use ExLlama instead, which does. And is much faster anyway.</p>

          '
        raw: AutoGPTQ doesn't do GPU acceleration on AMD GPUs unfortunately.  But
          you can use ExLlama instead, which does. And is much faster anyway.
        updatedAt: '2023-07-08T09:56:58.342Z'
      numEdits: 0
      reactions: []
    id: 64a932ea5fc663fee22a711a
    type: comment
  author: TheBloke
  content: AutoGPTQ doesn't do GPU acceleration on AMD GPUs unfortunately.  But you
    can use ExLlama instead, which does. And is much faster anyway.
  created_at: 2023-07-08 08:56:58+00:00
  edited: false
  hidden: false
  id: 64a932ea5fc663fee22a711a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: TheBloke/Nous-Hermes-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'IndexError: list index out of range'
