!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nerner94
conflicting_files: null
created_at: 2023-07-25 17:32:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d2816ecbe4a9cc143805a9d0d5a3e86.svg
      fullname: N Ozen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nerner94
      type: user
    createdAt: '2023-07-25T18:32:17.000Z'
    data:
      edited: false
      editors:
      - nerner94
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8040274977684021
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d2816ecbe4a9cc143805a9d0d5a3e86.svg
          fullname: N Ozen
          isHf: false
          isPro: false
          name: nerner94
          type: user
        html: '<p>Hello, </p>

          <p>I wish to generate text in batches given a prompt. The length of prompt
          changes as I provide different input each time. Currently I use padding
          to accommodate for the difference in token length but it padding distorts
          text generation heavily. Is there anyway I can avoid padding and still introduce
          different length prompts in a batch? Thanks, groetjes :) </p>

          '
        raw: "Hello, \r\n\r\nI wish to generate text in batches given a prompt. The\
          \ length of prompt changes as I provide different input each time. Currently\
          \ I use padding to accommodate for the difference in token length but it\
          \ padding distorts text generation heavily. Is there anyway I can avoid\
          \ padding and still introduce different length prompts in a batch? Thanks,\
          \ groetjes :) "
        updatedAt: '2023-07-25T18:32:17.268Z'
      numEdits: 0
      reactions: []
    id: 64c01531f7f5bd062fa0175f
    type: comment
  author: nerner94
  content: "Hello, \r\n\r\nI wish to generate text in batches given a prompt. The\
    \ length of prompt changes as I provide different input each time. Currently I\
    \ use padding to accommodate for the difference in token length but it padding\
    \ distorts text generation heavily. Is there anyway I can avoid padding and still\
    \ introduce different length prompts in a batch? Thanks, groetjes :) "
  created_at: 2023-07-25 17:32:17+00:00
  edited: false
  hidden: false
  id: 64c01531f7f5bd062fa0175f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: TheBloke/Nous-Hermes-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: text generation in batches
