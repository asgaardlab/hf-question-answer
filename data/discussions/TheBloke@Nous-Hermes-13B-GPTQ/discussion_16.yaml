!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pseudotensor
conflicting_files: null
created_at: 2023-08-21 21:28:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
      fullname: Jonathan McKinney
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pseudotensor
      type: user
    createdAt: '2023-08-21T22:28:53.000Z'
    data:
      edited: false
      editors:
      - pseudotensor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3156319856643677
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
          fullname: Jonathan McKinney
          isHf: false
          isPro: false
          name: pseudotensor
          type: user
        html: "<p><a href=\"https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ/commit/05c24345fc9a7b94b9e5ed7deebb534cd928a578\"\
          >https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ/commit/05c24345fc9a7b94b9e5ed7deebb534cd928a578</a></p>\n\
          <p>Since this, the example fails:</p>\n<pre><code>from transformers import\
          \ AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path = \"TheBloke/Nous-Hermes-13B-GPTQ\"\
          \nmodel_basename = \"nous-hermes-13b-GPTQ-4bit-128g.no-act.order\"\n\nuse_triton\
          \ = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n</code></pre>\n<p>says:</p>\n<pre><code>(h2ogpt)\
          \ jon@pseudotensor:~/h2ogpt$ python\nPython 3.10.12 | packaged by conda-forge\
          \ | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0] on linux\nType \"help\",\
          \ \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\
          \ from transformers import AutoTokenizer, pipeline, logging\n\n&gt;&gt;&gt;\
          \ from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n&gt;&gt;&gt;\
          \ import argparse\n&gt;&gt;&gt; \n&gt;&gt;&gt; model_name_or_path = \"TheBloke/Nous-Hermes-13B-GPTQ\"\
          \n&gt;&gt;&gt; model_basename = \"nous-hermes-13b-GPTQ-4bit-128g.no-act.order\"\
          \n&gt;&gt;&gt; use_triton = False\n&gt;&gt;&gt; \n&gt;&gt;&gt; tokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n&gt;&gt;&gt;\
          \ \n&gt;&gt;&gt; model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          ...         model_basename=model_basename,\n...         use_safetensors=True,\n\
          ...         trust_remote_code=True,\n...         device=\"cuda:0\",\n...\
          \         use_triton=use_triton,\n...         quantize_config=None)\n\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last)\
          \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502 &lt;stdin&gt;:1\
          \ in &lt;module&gt;                                                    \
          \                        \u2502\n\u2502                                \
          \                                                                  \u2502\
          \n\u2502 /home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:94\
          \ in   \u2502\n\u2502 from_quantized                                   \
          \                                                \u2502\n\u2502        \
          \                                                                      \
          \                    \u2502\n\u2502    91 \u2502   \u2502   \u2502   for\
          \ key in signature(quant_func).parameters                              \
          \      \u2502\n\u2502    92 \u2502   \u2502   \u2502   if key in kwargs\
          \                                                               \u2502\n\
          \u2502    93 \u2502   \u2502   }                                       \
          \                                           \u2502\n\u2502 \u2771  94 \u2502\
          \   \u2502   return quant_func(                                        \
          \                         \u2502\n\u2502    95 \u2502   \u2502   \u2502\
          \   model_name_or_path=model_name_or_path,                             \
          \            \u2502\n\u2502    96 \u2502   \u2502   \u2502   save_dir=save_dir,\
          \                                                             \u2502\n\u2502\
          \    97 \u2502   \u2502   \u2502   device_map=device_map,              \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502 /home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:714\
          \ in \u2502\n\u2502 from_quantized                                     \
          \                                              \u2502\n\u2502          \
          \                                                                      \
          \                  \u2502\n\u2502   711 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   break                                                     \
          \             \u2502\n\u2502   712 \u2502   \u2502                     \
          \                                                                 \u2502\
          \n\u2502   713 \u2502   \u2502   if resolved_archive_file is None: # Could\
          \ not find a model file to use             \u2502\n\u2502 \u2771 714 \u2502\
          \   \u2502   \u2502   raise FileNotFoundError(f\"Could not find model in\
          \ {model_name_or_path}\")       \u2502\n\u2502   715 \u2502   \u2502   \
          \                                                                      \
          \             \u2502\n\u2502   716 \u2502   \u2502   model_save_name = resolved_archive_file\
          \                                            \u2502\n\u2502   717      \
          \                                                                      \
          \                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nFileNotFoundError: Could\
          \ not find model in TheBloke/Nous-Hermes-13B-GPTQ\n&gt;&gt;&gt; \n</code></pre>\n"
        raw: "https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ/commit/05c24345fc9a7b94b9e5ed7deebb534cd928a578\r\
          \n\r\nSince this, the example fails:\r\n```\r\nfrom transformers import\
          \ AutoTokenizer, pipeline, logging\r\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\r\nimport argparse\r\n\r\nmodel_name_or_path = \"TheBloke/Nous-Hermes-13B-GPTQ\"\
          \r\nmodel_basename = \"nous-hermes-13b-GPTQ-4bit-128g.no-act.order\"\r\n\
          \r\nuse_triton = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n        model_basename=model_basename,\r\n        use_safetensors=True,\r\
          \n        trust_remote_code=True,\r\n        device=\"cuda:0\",\r\n    \
          \    use_triton=use_triton,\r\n        quantize_config=None)\r\n```\r\n\r\
          \nsays:\r\n\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ python\r\nPython\
          \ 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC\
          \ 12.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\"\
          \ for more information.\r\n>>> from transformers import AutoTokenizer, pipeline,\
          \ logging\r\n\r\n>>> from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\
          \n>>> import argparse\r\n>>> \r\n>>> model_name_or_path = \"TheBloke/Nous-Hermes-13B-GPTQ\"\
          \r\n>>> model_basename = \"nous-hermes-13b-GPTQ-4bit-128g.no-act.order\"\
          \r\n>>> use_triton = False\r\n>>> \r\n>>> tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\r\n>>> \r\n>>> model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n...         model_basename=model_basename,\r\n...         use_safetensors=True,\r\
          \n...         trust_remote_code=True,\r\n...         device=\"cuda:0\",\r\
          \n...         use_triton=use_triton,\r\n...         quantize_config=None)\r\
          \n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502\
          \ <stdin>:1 in <module>                                                \
          \                            \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502 /home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:94\
          \ in   \u2502\r\n\u2502 from_quantized                                 \
          \                                                  \u2502\r\n\u2502    \
          \                                                                      \
          \                        \u2502\r\n\u2502    91 \u2502   \u2502   \u2502\
          \   for key in signature(quant_func).parameters                        \
          \            \u2502\r\n\u2502    92 \u2502   \u2502   \u2502   if key in\
          \ kwargs                                                               \u2502\
          \r\n\u2502    93 \u2502   \u2502   }                                   \
          \                                               \u2502\r\n\u2502 \u2771\
          \  94 \u2502   \u2502   return quant_func(                             \
          \                                    \u2502\r\n\u2502    95 \u2502   \u2502\
          \   \u2502   model_name_or_path=model_name_or_path,                    \
          \                     \u2502\r\n\u2502    96 \u2502   \u2502   \u2502  \
          \ save_dir=save_dir,                                                   \
          \          \u2502\r\n\u2502    97 \u2502   \u2502   \u2502   device_map=device_map,\
          \                                                         \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:714\
          \ in \u2502\r\n\u2502 from_quantized                                   \
          \                                                \u2502\r\n\u2502      \
          \                                                                      \
          \                      \u2502\r\n\u2502   711 \u2502   \u2502   \u2502 \
          \  \u2502   \u2502   break                                             \
          \                     \u2502\r\n\u2502   712 \u2502   \u2502           \
          \                                                                      \
          \     \u2502\r\n\u2502   713 \u2502   \u2502   if resolved_archive_file\
          \ is None: # Could not find a model file to use             \u2502\r\n\u2502\
          \ \u2771 714 \u2502   \u2502   \u2502   raise FileNotFoundError(f\"Could\
          \ not find model in {model_name_or_path}\")       \u2502\r\n\u2502   715\
          \ \u2502   \u2502                                                      \
          \                                \u2502\r\n\u2502   716 \u2502   \u2502\
          \   model_save_name = resolved_archive_file                            \
          \                \u2502\r\n\u2502   717                                \
          \                                                            \u2502\r\n\u2570\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256F\r\nFileNotFoundError: Could not find model in TheBloke/Nous-Hermes-13B-GPTQ\r\
          \n>>> \r\n\r\n```"
        updatedAt: '2023-08-21T22:28:53.128Z'
      numEdits: 0
      reactions: []
    id: 64e3e525825f4133e7569f88
    type: comment
  author: pseudotensor
  content: "https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ/commit/05c24345fc9a7b94b9e5ed7deebb534cd928a578\r\
    \n\r\nSince this, the example fails:\r\n```\r\nfrom transformers import AutoTokenizer,\
    \ pipeline, logging\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\
    \nimport argparse\r\n\r\nmodel_name_or_path = \"TheBloke/Nous-Hermes-13B-GPTQ\"\
    \r\nmodel_basename = \"nous-hermes-13b-GPTQ-4bit-128g.no-act.order\"\r\n\r\nuse_triton\
    \ = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n        model_basename=model_basename,\r\n        use_safetensors=True,\r\n\
    \        trust_remote_code=True,\r\n        device=\"cuda:0\",\r\n        use_triton=use_triton,\r\
    \n        quantize_config=None)\r\n```\r\n\r\nsays:\r\n\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$\
    \ python\r\nPython 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32)\
    \ [GCC 12.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\"\
    \ for more information.\r\n>>> from transformers import AutoTokenizer, pipeline,\
    \ logging\r\n\r\n>>> from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\
    \n>>> import argparse\r\n>>> \r\n>>> model_name_or_path = \"TheBloke/Nous-Hermes-13B-GPTQ\"\
    \r\n>>> model_basename = \"nous-hermes-13b-GPTQ-4bit-128g.no-act.order\"\r\n>>>\
    \ use_triton = False\r\n>>> \r\n>>> tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n>>> \r\n>>> model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n...         model_basename=model_basename,\r\n...         use_safetensors=True,\r\
    \n...         trust_remote_code=True,\r\n...         device=\"cuda:0\",\r\n...\
    \         use_triton=use_triton,\r\n...         quantize_config=None)\r\n\u256D\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u256E\r\n\u2502 <stdin>:1 in <module>                     \
    \                                                       \u2502\r\n\u2502     \
    \                                                                            \
    \                 \u2502\r\n\u2502 /home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:94\
    \ in   \u2502\r\n\u2502 from_quantized                                       \
    \                                            \u2502\r\n\u2502                \
    \                                                                            \
    \      \u2502\r\n\u2502    91 \u2502   \u2502   \u2502   for key in signature(quant_func).parameters\
    \                                    \u2502\r\n\u2502    92 \u2502   \u2502  \
    \ \u2502   if key in kwargs                                                  \
    \             \u2502\r\n\u2502    93 \u2502   \u2502   }                     \
    \                                                             \u2502\r\n\u2502\
    \ \u2771  94 \u2502   \u2502   return quant_func(                            \
    \                                     \u2502\r\n\u2502    95 \u2502   \u2502 \
    \  \u2502   model_name_or_path=model_name_or_path,                           \
    \              \u2502\r\n\u2502    96 \u2502   \u2502   \u2502   save_dir=save_dir,\
    \                                                             \u2502\r\n\u2502\
    \    97 \u2502   \u2502   \u2502   device_map=device_map,                    \
    \                                     \u2502\r\n\u2502                       \
    \                                                                           \u2502\
    \r\n\u2502 /home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:714\
    \ in \u2502\r\n\u2502 from_quantized                                         \
    \                                          \u2502\r\n\u2502                  \
    \                                                                            \
    \    \u2502\r\n\u2502   711 \u2502   \u2502   \u2502   \u2502   \u2502   break\
    \                                                                  \u2502\r\n\u2502\
    \   712 \u2502   \u2502                                                      \
    \                                \u2502\r\n\u2502   713 \u2502   \u2502   if resolved_archive_file\
    \ is None: # Could not find a model file to use             \u2502\r\n\u2502 \u2771\
    \ 714 \u2502   \u2502   \u2502   raise FileNotFoundError(f\"Could not find model\
    \ in {model_name_or_path}\")       \u2502\r\n\u2502   715 \u2502   \u2502    \
    \                                                                            \
    \      \u2502\r\n\u2502   716 \u2502   \u2502   model_save_name = resolved_archive_file\
    \                                            \u2502\r\n\u2502   717          \
    \                                                                            \
    \      \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nFileNotFoundError:\
    \ Could not find model in TheBloke/Nous-Hermes-13B-GPTQ\r\n>>> \r\n\r\n```"
  created_at: 2023-08-21 21:28:53+00:00
  edited: false
  hidden: false
  id: 64e3e525825f4133e7569f88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
      fullname: Jonathan McKinney
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pseudotensor
      type: user
    createdAt: '2023-08-21T22:29:53.000Z'
    data:
      edited: false
      editors:
      - pseudotensor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48935744166374207
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
          fullname: Jonathan McKinney
          isHf: false
          isPro: false
          name: pseudotensor
          type: user
        html: "<p>However this now works:</p>\n<pre><code>from transformers import\
          \ AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path = \"TheBloke/Nous-Hermes-13B-GPTQ\"\
          \nmodel_basename = \"model\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n</code></pre>\n"
        raw: "However this now works:\n\n```\nfrom transformers import AutoTokenizer,\
          \ pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          import argparse\n\nmodel_name_or_path = \"TheBloke/Nous-Hermes-13B-GPTQ\"\
          \nmodel_basename = \"model\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n```"
        updatedAt: '2023-08-21T22:29:53.494Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64e3e56154e18f390ef2065d
    id: 64e3e56154e18f390ef20658
    type: comment
  author: pseudotensor
  content: "However this now works:\n\n```\nfrom transformers import AutoTokenizer,\
    \ pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
    import argparse\n\nmodel_name_or_path = \"TheBloke/Nous-Hermes-13B-GPTQ\"\nmodel_basename\
    \ = \"model\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        model_basename=model_basename,\n        use_safetensors=True,\n     \
    \   trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
    \        quantize_config=None)\n```"
  created_at: 2023-08-21 21:29:53+00:00
  edited: false
  hidden: false
  id: 64e3e56154e18f390ef20658
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
      fullname: Jonathan McKinney
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pseudotensor
      type: user
    createdAt: '2023-08-21T22:29:53.000Z'
    data:
      status: closed
    id: 64e3e56154e18f390ef2065d
    type: status-change
  author: pseudotensor
  created_at: 2023-08-21 21:29:53+00:00
  id: 64e3e56154e18f390ef2065d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-21T22:32:34.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8941192626953125
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Correct, I have renamed all models to <code>model.safetensors</code>
          to prepare for native Transformers GPTQ support which is coming in the next
          couple of days.</p>

          <p>I''ve updated all my code examples to show that <code>model_basename
          = "model"</code> should be used. But I''ve not yet put out more detailed
          documentation.  That will be coming to all GPTQ repos as soon as the new
          Transformers version goes live, hopefully tomorrow.</p>

          <p>In fact you can now leave out <code>model_basename</code> entirely -
          I also updated <code>quantize_config.json</code> to indicate that <code>model_basename=model</code>
          so there''s no need to manually specify <code>model_basename</code> in <code>.from_quantized()</code>
          any more.  When I update the docs properly I will remove that.  Actually
          I''ll remove all AutoGPTQ code, and show loading it directly from Transformers.</p>

          '
        raw: 'Correct, I have renamed all models to `model.safetensors` to prepare
          for native Transformers GPTQ support which is coming in the next couple
          of days.


          I''ve updated all my code examples to show that `model_basename = "model"`
          should be used. But I''ve not yet put out more detailed documentation.  That
          will be coming to all GPTQ repos as soon as the new Transformers version
          goes live, hopefully tomorrow.


          In fact you can now leave out `model_basename` entirely - I also updated
          `quantize_config.json` to indicate that `model_basename=model` so there''s
          no need to manually specify `model_basename` in `.from_quantized()` any
          more.  When I update the docs properly I will remove that.  Actually I''ll
          remove all AutoGPTQ code, and show loading it directly from Transformers.'
        updatedAt: '2023-08-21T22:32:34.911Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pseudotensor
    id: 64e3e602c21dd0c666023ce5
    type: comment
  author: TheBloke
  content: 'Correct, I have renamed all models to `model.safetensors` to prepare for
    native Transformers GPTQ support which is coming in the next couple of days.


    I''ve updated all my code examples to show that `model_basename = "model"` should
    be used. But I''ve not yet put out more detailed documentation.  That will be
    coming to all GPTQ repos as soon as the new Transformers version goes live, hopefully
    tomorrow.


    In fact you can now leave out `model_basename` entirely - I also updated `quantize_config.json`
    to indicate that `model_basename=model` so there''s no need to manually specify
    `model_basename` in `.from_quantized()` any more.  When I update the docs properly
    I will remove that.  Actually I''ll remove all AutoGPTQ code, and show loading
    it directly from Transformers.'
  created_at: 2023-08-21 21:32:34+00:00
  edited: false
  hidden: false
  id: 64e3e602c21dd0c666023ce5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
      fullname: Jonathan McKinney
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pseudotensor
      type: user
    createdAt: '2023-08-22T05:24:14.000Z'
    data:
      edited: false
      editors:
      - pseudotensor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9773927927017212
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
          fullname: Jonathan McKinney
          isHf: false
          isPro: false
          name: pseudotensor
          type: user
        html: '<p>So will you be moving away from AutoGPTQ as main inspiration for
          GPTQ?  I know you tracked his project and was pushing him some to get working
          more on it :)  Once in transformers, no need for AutoGPTQ or GPTQ-for-LLaMa?</p>

          '
        raw: So will you be moving away from AutoGPTQ as main inspiration for GPTQ?  I
          know you tracked his project and was pushing him some to get working more
          on it :)  Once in transformers, no need for AutoGPTQ or GPTQ-for-LLaMa?
        updatedAt: '2023-08-22T05:24:14.398Z'
      numEdits: 0
      reactions: []
    id: 64e4467e2fbf570ac70bb926
    type: comment
  author: pseudotensor
  content: So will you be moving away from AutoGPTQ as main inspiration for GPTQ?  I
    know you tracked his project and was pushing him some to get working more on it
    :)  Once in transformers, no need for AutoGPTQ or GPTQ-for-LLaMa?
  created_at: 2023-08-22 04:24:14+00:00
  edited: false
  hidden: false
  id: 64e4467e2fbf570ac70bb926
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-22T07:48:41.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8893911838531494
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The Transformers implementation uses AutoGPTQ as its backend, so
          AutoGPTQ will still be required.  To use GPTQ in Transformers, the user
          will need three packages:<br><code>transformers optimum auto-gptq</code></p>

          <p>So AutoGPTQ will still be vital.</p>

          <p>But yeah GPTQ-for-LLaMa is dead as far as I''m concerned!</p>

          '
        raw: 'The Transformers implementation uses AutoGPTQ as its backend, so AutoGPTQ
          will still be required.  To use GPTQ in Transformers, the user will need
          three packages:

          `transformers optimum auto-gptq`


          So AutoGPTQ will still be vital.


          But yeah GPTQ-for-LLaMa is dead as far as I''m concerned!'
        updatedAt: '2023-08-22T07:48:41.657Z'
      numEdits: 0
      reactions: []
    id: 64e46859fe53a047e5c0baa8
    type: comment
  author: TheBloke
  content: 'The Transformers implementation uses AutoGPTQ as its backend, so AutoGPTQ
    will still be required.  To use GPTQ in Transformers, the user will need three
    packages:

    `transformers optimum auto-gptq`


    So AutoGPTQ will still be vital.


    But yeah GPTQ-for-LLaMa is dead as far as I''m concerned!'
  created_at: 2023-08-22 06:48:41+00:00
  edited: false
  hidden: false
  id: 64e46859fe53a047e5c0baa8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
      fullname: Jonathan McKinney
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pseudotensor
      type: user
    createdAt: '2023-08-22T08:36:53.000Z'
    data:
      edited: false
      editors:
      - pseudotensor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8969810605049133
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6308791ac038bf42d568153f/z9TovAddXU3OQR9N_2KFP.jpeg?w=200&h=200&f=face
          fullname: Jonathan McKinney
          isHf: false
          isPro: false
          name: pseudotensor
          type: user
        html: '<p>Ok cool.</p>

          '
        raw: Ok cool.
        updatedAt: '2023-08-22T08:36:53.352Z'
      numEdits: 0
      reactions: []
    id: 64e473a5d7424239dfc8f21b
    type: comment
  author: pseudotensor
  content: Ok cool.
  created_at: 2023-08-22 07:36:53+00:00
  edited: false
  hidden: false
  id: 64e473a5d7424239dfc8f21b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: TheBloke/Nous-Hermes-13B-GPTQ
repo_type: model
status: closed
target_branch: null
title: latest update broke use
