!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xuqiantong
conflicting_files: null
created_at: 2022-07-19 01:08:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/249986aa608cc119ce07ff2cc669009e.svg
      fullname: Qiantong Xu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xuqiantong
      type: user
    createdAt: '2022-07-19T02:08:49.000Z'
    data:
      edited: false
      editors:
      - xuqiantong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/249986aa608cc119ce07ff2cc669009e.svg
          fullname: Qiantong Xu
          isHf: false
          isPro: false
          name: xuqiantong
          type: user
        html: '<p>Hi Anton,</p>

          <p>Thanks for sharing this model.</p>

          <p>I have a question about using this model in inference. Suppose I have
          a single-channel audio that has 2 speakers talking without overlapping.
          What kind of output should I expect from this model? How can I tell which
          part of the audio is spoken by which speaker?</p>

          <p>Looking forward to your reply.</p>

          <p>Thanks,<br>Qiantong</p>

          '
        raw: "Hi Anton,\r\n\r\nThanks for sharing this model.\r\n\r\nI have a question\
          \ about using this model in inference. Suppose I have a single-channel audio\
          \ that has 2 speakers talking without overlapping. What kind of output should\
          \ I expect from this model? How can I tell which part of the audio is spoken\
          \ by which speaker?\r\n\r\nLooking forward to your reply.\r\n\r\nThanks,\r\
          \nQiantong"
        updatedAt: '2022-07-19T02:08:49.805Z'
      numEdits: 0
      reactions: []
    id: 62d61231df299556f9efcd90
    type: comment
  author: xuqiantong
  content: "Hi Anton,\r\n\r\nThanks for sharing this model.\r\n\r\nI have a question\
    \ about using this model in inference. Suppose I have a single-channel audio that\
    \ has 2 speakers talking without overlapping. What kind of output should I expect\
    \ from this model? How can I tell which part of the audio is spoken by which speaker?\r\
    \n\r\nLooking forward to your reply.\r\n\r\nThanks,\r\nQiantong"
  created_at: 2022-07-19 01:08:49+00:00
  edited: false
  hidden: false
  id: 62d61231df299556f9efcd90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/249986aa608cc119ce07ff2cc669009e.svg
      fullname: Qiantong Xu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xuqiantong
      type: user
    createdAt: '2022-07-19T02:20:24.000Z'
    data:
      edited: false
      editors:
      - xuqiantong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/249986aa608cc119ce07ff2cc669009e.svg
          fullname: Qiantong Xu
          isHf: false
          isPro: false
          name: xuqiantong
          type: user
        html: '<p>The output will be a 2d tensor, with [shape sequence_length, 2].
          My understading is that I could get the frame-level prediction by applying
          <code>torch.sigmoid(output) &gt; 0.5</code>? However, the output looks a
          bit messy on my test sample.</p>

          '
        raw: The output will be a 2d tensor, with [shape sequence_length, 2]. My understading
          is that I could get the frame-level prediction by applying `torch.sigmoid(output)
          > 0.5`? However, the output looks a bit messy on my test sample.
        updatedAt: '2022-07-19T02:20:24.722Z'
      numEdits: 0
      reactions: []
    id: 62d614e8c46278c4a999d953
    type: comment
  author: xuqiantong
  content: The output will be a 2d tensor, with [shape sequence_length, 2]. My understading
    is that I could get the frame-level prediction by applying `torch.sigmoid(output)
    > 0.5`? However, the output looks a bit messy on my test sample.
  created_at: 2022-07-19 01:20:24+00:00
  edited: false
  hidden: false
  id: 62d614e8c46278c4a999d953
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3cd11c89e4a3089462a6e3c055a1d64c.svg
      fullname: Chaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ChS
      type: user
    createdAt: '2022-07-20T06:34:12.000Z'
    data:
      edited: true
      editors:
      - ChS
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3cd11c89e4a3089462a6e3c055a1d64c.svg
          fullname: Chaka
          isHf: false
          isPro: false
          name: ChS
          type: user
        html: '<p>Hi Anton,</p>

          <p>Thanks a lot for sharing this model. I am posting my question here since
          it is related to what Qiantong has asked.<br>I can see that the output is
          of shape (num_frames, num_speakers). Could you please guide us on how to
          map each frame to its corresponding time-stamp? Or better said how to chunk
          the audio to pieces based on speakers.</p>

          <p>Best<br>Chakka</p>

          '
        raw: 'Hi Anton,


          Thanks a lot for sharing this model. I am posting my question here since
          it is related to what Qiantong has asked.

          I can see that the output is of shape (num_frames, num_speakers). Could
          you please guide us on how to map each frame to its corresponding time-stamp?
          Or better said how to chunk the audio to pieces based on speakers.


          Best

          Chakka'
        updatedAt: '2022-07-20T06:40:55.999Z'
      numEdits: 1
      reactions: []
    id: 62d7a1e48edc52b363387ce0
    type: comment
  author: ChS
  content: 'Hi Anton,


    Thanks a lot for sharing this model. I am posting my question here since it is
    related to what Qiantong has asked.

    I can see that the output is of shape (num_frames, num_speakers). Could you please
    guide us on how to map each frame to its corresponding time-stamp? Or better said
    how to chunk the audio to pieces based on speakers.


    Best

    Chakka'
  created_at: 2022-07-20 05:34:12+00:00
  edited: true
  hidden: false
  id: 62d7a1e48edc52b363387ce0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665065246134-633d8250ae5e55314709e7af.jpeg?w=200&h=200&f=face
      fullname: Shripad Anant Bhat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shripadbhat
      type: user
    createdAt: '2022-10-05T13:15:38.000Z'
    data:
      edited: false
      editors:
      - shripadbhat
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665065246134-633d8250ae5e55314709e7af.jpeg?w=200&h=200&f=face
          fullname: Shripad Anant Bhat
          isHf: false
          isPro: false
          name: shripadbhat
          type: user
        html: '<p>Hi </p>

          <p>Did anyone find a solution to map model outputs to timestamps ?</p>

          '
        raw: "Hi \n\nDid anyone find a solution to map model outputs to timestamps\
          \ ?"
        updatedAt: '2022-10-05T13:15:38.352Z'
      numEdits: 0
      reactions: []
    id: 633d837a2d24f72e563cd2db
    type: comment
  author: shripadbhat
  content: "Hi \n\nDid anyone find a solution to map model outputs to timestamps ?"
  created_at: 2022-10-05 12:15:38+00:00
  edited: false
  hidden: false
  id: 633d837a2d24f72e563cd2db
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: anton-l/wav2vec2-base-superb-sd
repo_type: model
status: open
target_branch: null
title: How can I use this model in inference?
