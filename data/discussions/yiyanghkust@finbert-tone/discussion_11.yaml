!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hirindu
conflicting_files: null
created_at: 2023-12-15 11:16:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642ee80870daaa6e7205c220/Pap7xTjVRuskdZQzQW19D.jpeg?w=200&h=200&f=face
      fullname: Hirindu Kawshala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hirindu
      type: user
    createdAt: '2023-12-15T11:16:19.000Z'
    data:
      edited: false
      editors:
      - Hirindu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5532975792884827
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642ee80870daaa6e7205c220/Pap7xTjVRuskdZQzQW19D.jpeg?w=200&h=200&f=face
          fullname: Hirindu Kawshala
          isHf: false
          isPro: false
          name: Hirindu
          type: user
        html: "<p>How I can derive one score for each transcript using Pycharm:<br>Can\
          \ you please correct my codes:<br>from transformers import BertTokenizer,\
          \ BertForSequenceClassification<br>from transformers import pipeline</p>\n\
          <p>finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',\
          \ num_labels=3)<br>tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')</p>\n\
          <p>nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)</p>\n\
          <h1 id=\"set-the-base-path-for-the-transcript-files\">Set the base path\
          \ for the transcript files</h1>\n<p>basepath = 'D:/ANALYSIS/DATABASE/All'</p>\n\
          <p>output_directory = 'C:/Users/Desktop/'<br>os.makedirs(output_directory,\
          \ exist_ok=True)</p>\n<h1 id=\"open-the-csv-file-for-writing-and-write-the-headers\"\
          >Open the CSV file for writing and write the headers</h1>\n<p>with open(os.path.join(output_directory,\
          \ 'FinBert_Sentiments.csv'), 'w', encoding='utf-8', newline='') as content:<br>\
          \    writer = csv.writer(content)<br>    writer.writerow((\"Firm Name\"\
          , \"Label\", \"Score\"))</p>\n<pre><code># Loop through all files in the\
          \ base path\nfor root, dirs, files in os.walk(basepath):\n    for file in\
          \ files:\n        if file.endswith('.txt'):\n            # Extract firm\
          \ name from transcript text\n            with open(os.path.join(root, file),\
          \ 'r', encoding='utf-8') as f:\n                transcript = f.read().lower()\n\
          \                match = re.search(r'q\\d\\s\\d{4}\\s(.+)', transcript)\n\
          \                firm_name = match.group(1) if match else ''\n\n       \
          \         # Split the transcript into chunks of 512 tokens\n           \
          \     max_chunk_size = 512\n                chunks = [transcript[i:i + max_chunk_size]\
          \ for i in range(0, len(transcript), max_chunk_size)]\n\n              \
          \  for chunk in chunks:\n                    # Tokenize and truncate/split\
          \ input text to fit the model's maximum sequence length\n              \
          \      tokens = tokenizer.encode_plus(chunk, max_length=1300000000, truncation=True,\
          \ return_tensors='pt')\n                    input_ids = tokens['input_ids']\n\
          \n                    # Perform sentiment analysis on the transcript\n \
          \                   results = nlp(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n\
          \                    label = results[0]['label']\n                    score\
          \ = results[0]['score']\n\n                    # Write the results to the\
          \ CSV file\n                    writer.writerow([firm_name, label, score])\n\
          </code></pre>\n"
        raw: "How I can derive one score for each transcript using Pycharm: \r\nCan\
          \ you please correct my codes: \r\nfrom transformers import BertTokenizer,\
          \ BertForSequenceClassification\r\nfrom transformers import pipeline\r\n\
          \r\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',\
          \ num_labels=3)\r\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\r\
          \n\r\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\r\
          \n\r\n# Set the base path for the transcript files\r\nbasepath = 'D:/ANALYSIS/DATABASE/All'\r\
          \n\r\noutput_directory = 'C:/Users/Desktop/'\r\nos.makedirs(output_directory,\
          \ exist_ok=True)\r\n\r\n# Open the CSV file for writing and write the headers\r\
          \nwith open(os.path.join(output_directory, 'FinBert_Sentiments.csv'), 'w',\
          \ encoding='utf-8', newline='') as content:\r\n    writer = csv.writer(content)\r\
          \n    writer.writerow((\"Firm Name\", \"Label\", \"Score\"))\r\n\r\n   \
          \ # Loop through all files in the base path\r\n    for root, dirs, files\
          \ in os.walk(basepath):\r\n        for file in files:\r\n            if\
          \ file.endswith('.txt'):\r\n                # Extract firm name from transcript\
          \ text\r\n                with open(os.path.join(root, file), 'r', encoding='utf-8')\
          \ as f:\r\n                    transcript = f.read().lower()\r\n       \
          \             match = re.search(r'q\\d\\s\\d{4}\\s(.+)', transcript)\r\n\
          \                    firm_name = match.group(1) if match else ''\r\n\r\n\
          \                    # Split the transcript into chunks of 512 tokens\r\n\
          \                    max_chunk_size = 512\r\n                    chunks\
          \ = [transcript[i:i + max_chunk_size] for i in range(0, len(transcript),\
          \ max_chunk_size)]\r\n\r\n                    for chunk in chunks:\r\n \
          \                       # Tokenize and truncate/split input text to fit\
          \ the model's maximum sequence length\r\n                        tokens\
          \ = tokenizer.encode_plus(chunk, max_length=1300000000, truncation=True,\
          \ return_tensors='pt')\r\n                        input_ids = tokens['input_ids']\r\
          \n\r\n                        # Perform sentiment analysis on the transcript\r\
          \n                        results = nlp(tokenizer.decode(input_ids[0], skip_special_tokens=True))\r\
          \n                        label = results[0]['label']\r\n              \
          \          score = results[0]['score']\r\n\r\n                        #\
          \ Write the results to the CSV file\r\n                        writer.writerow([firm_name,\
          \ label, score])\r\n"
        updatedAt: '2023-12-15T11:16:19.962Z'
      numEdits: 0
      reactions: []
    id: 657c35837f2503bfc67c5ff2
    type: comment
  author: Hirindu
  content: "How I can derive one score for each transcript using Pycharm: \r\nCan\
    \ you please correct my codes: \r\nfrom transformers import BertTokenizer, BertForSequenceClassification\r\
    \nfrom transformers import pipeline\r\n\r\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',\
    \ num_labels=3)\r\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\r\
    \n\r\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\r\
    \n\r\n# Set the base path for the transcript files\r\nbasepath = 'D:/ANALYSIS/DATABASE/All'\r\
    \n\r\noutput_directory = 'C:/Users/Desktop/'\r\nos.makedirs(output_directory,\
    \ exist_ok=True)\r\n\r\n# Open the CSV file for writing and write the headers\r\
    \nwith open(os.path.join(output_directory, 'FinBert_Sentiments.csv'), 'w', encoding='utf-8',\
    \ newline='') as content:\r\n    writer = csv.writer(content)\r\n    writer.writerow((\"\
    Firm Name\", \"Label\", \"Score\"))\r\n\r\n    # Loop through all files in the\
    \ base path\r\n    for root, dirs, files in os.walk(basepath):\r\n        for\
    \ file in files:\r\n            if file.endswith('.txt'):\r\n                #\
    \ Extract firm name from transcript text\r\n                with open(os.path.join(root,\
    \ file), 'r', encoding='utf-8') as f:\r\n                    transcript = f.read().lower()\r\
    \n                    match = re.search(r'q\\d\\s\\d{4}\\s(.+)', transcript)\r\
    \n                    firm_name = match.group(1) if match else ''\r\n\r\n    \
    \                # Split the transcript into chunks of 512 tokens\r\n        \
    \            max_chunk_size = 512\r\n                    chunks = [transcript[i:i\
    \ + max_chunk_size] for i in range(0, len(transcript), max_chunk_size)]\r\n\r\n\
    \                    for chunk in chunks:\r\n                        # Tokenize\
    \ and truncate/split input text to fit the model's maximum sequence length\r\n\
    \                        tokens = tokenizer.encode_plus(chunk, max_length=1300000000,\
    \ truncation=True, return_tensors='pt')\r\n                        input_ids =\
    \ tokens['input_ids']\r\n\r\n                        # Perform sentiment analysis\
    \ on the transcript\r\n                        results = nlp(tokenizer.decode(input_ids[0],\
    \ skip_special_tokens=True))\r\n                        label = results[0]['label']\r\
    \n                        score = results[0]['score']\r\n\r\n                \
    \        # Write the results to the CSV file\r\n                        writer.writerow([firm_name,\
    \ label, score])\r\n"
  created_at: 2023-12-15 11:16:19+00:00
  edited: false
  hidden: false
  id: 657c35837f2503bfc67c5ff2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: yiyanghkust/finbert-tone
repo_type: model
status: open
target_branch: null
title: FinBert Model Token Size
