!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil337
conflicting_files: null
created_at: 2023-11-09 16:57:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-11-09T16:57:40.000Z'
    data:
      edited: true
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9797681570053101
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>I understand that it''s an hallucination when an LLM confidently
          says something is true when it isn''t. However, it''s also an hallucination
          when it says something isn''t true when it is, so why doesn''t the Open
          Orca data set train LLMs to say things like ''I have no knowledge of that''
          rather than absolute phrases like it ''never'' happened?</p>

          <p>For example, I have a large collection of questions that I test LLMs
          with, all of which are backed with conclusive evidence. For example, a video
          of a notable celebrity interview, such as the Britney Spears interview with
          Diane Sawyer.</p>

          <p>And while it''s OK for LLMs not to know everything, why does Open Orca
          train LLMs to ALWAYS say something isn''t true or doesn''t exist just because
          it has no knowledge of it?  Humans instinctively say things like ''I have
          no knowledge of that'' unless the fact can be verified (e.g. Venus is not
          the 3rd planet from the sun, but rather Earth is).</p>

          <p>Without fail every time Open Orca is asked something like, ''did Britney
          Spears say.... during her interview with Diane Sawyer'' it never hedges
          its bets. It simply says she never said that (even though it was word for
          word what she said). All it has to do is say ''I don''t know'' or ''I don''t
          think so''.</p>

          <p>And even when I respond by saying that I''m currently watching the interview
          and that''s exactly what she said, Open Orca will apologize before once
          again confidently stating that she never said that. And no matter how I
          word the point ''just because you don''t know something doesn''t mean it
          doesn''t exist'' it will politely say I''m wrong and stubbornly stick to
          the declarative statement that she never said it.</p>

          '
        raw: 'I understand that it''s an hallucination when an LLM confidently says
          something is true when it isn''t. However, it''s also an hallucination when
          it says something isn''t true when it is, so why doesn''t the Open Orca
          data set train LLMs to say things like ''I have no knowledge of that'' rather
          than absolute phrases like it ''never'' happened?


          For example, I have a large collection of questions that I test LLMs with,
          all of which are backed with conclusive evidence. For example, a video of
          a notable celebrity interview, such as the Britney Spears interview with
          Diane Sawyer.


          And while it''s OK for LLMs not to know everything, why does Open Orca train
          LLMs to ALWAYS say something isn''t true or doesn''t exist just because
          it has no knowledge of it?  Humans instinctively say things like ''I have
          no knowledge of that'' unless the fact can be verified (e.g. Venus is not
          the 3rd planet from the sun, but rather Earth is).


          Without fail every time Open Orca is asked something like, ''did Britney
          Spears say.... during her interview with Diane Sawyer'' it never hedges
          its bets. It simply says she never said that (even though it was word for
          word what she said). All it has to do is say ''I don''t know'' or ''I don''t
          think so''.


          And even when I respond by saying that I''m currently watching the interview
          and that''s exactly what she said, Open Orca will apologize before once
          again confidently stating that she never said that. And no matter how I
          word the point ''just because you don''t know something doesn''t mean it
          doesn''t exist'' it will politely say I''m wrong and stubbornly stick to
          the declarative statement that she never said it.'
        updatedAt: '2023-11-09T16:58:23.532Z'
      numEdits: 1
      reactions: []
    id: 654d0f841e6216cb2d81098d
    type: comment
  author: Phil337
  content: 'I understand that it''s an hallucination when an LLM confidently says
    something is true when it isn''t. However, it''s also an hallucination when it
    says something isn''t true when it is, so why doesn''t the Open Orca data set
    train LLMs to say things like ''I have no knowledge of that'' rather than absolute
    phrases like it ''never'' happened?


    For example, I have a large collection of questions that I test LLMs with, all
    of which are backed with conclusive evidence. For example, a video of a notable
    celebrity interview, such as the Britney Spears interview with Diane Sawyer.


    And while it''s OK for LLMs not to know everything, why does Open Orca train LLMs
    to ALWAYS say something isn''t true or doesn''t exist just because it has no knowledge
    of it?  Humans instinctively say things like ''I have no knowledge of that'' unless
    the fact can be verified (e.g. Venus is not the 3rd planet from the sun, but rather
    Earth is).


    Without fail every time Open Orca is asked something like, ''did Britney Spears
    say.... during her interview with Diane Sawyer'' it never hedges its bets. It
    simply says she never said that (even though it was word for word what she said).
    All it has to do is say ''I don''t know'' or ''I don''t think so''.


    And even when I respond by saying that I''m currently watching the interview and
    that''s exactly what she said, Open Orca will apologize before once again confidently
    stating that she never said that. And no matter how I word the point ''just because
    you don''t know something doesn''t mean it doesn''t exist'' it will politely say
    I''m wrong and stubbornly stick to the declarative statement that she never said
    it.'
  created_at: 2023-11-09 16:57:40+00:00
  edited: true
  hidden: false
  id: 654d0f841e6216cb2d81098d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-11-09T17:07:54.000Z'
    data:
      edited: false
      editors:
      - deleted
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.964228630065918
      isReport: false
      latest:
        html: '<p>You see, it is not a rlhf/rlft model and the OpenOrca dataset doesn''t
          have any samples to guide the model to do so. And adding such instances
          might cause a lot of degradation in performance, i.e. Rejection of known
          content too. But their are methods to prevent hallucination, like apologising
          for not having the knowledge about something, can be done by training a
          model which just looking at the main model''s states would tell whether
          hallucination is occurring or not, thus provide rejection. Hope it helps
          </p>

          '
        raw: 'You see, it is not a rlhf/rlft model and the OpenOrca dataset doesn''t
          have any samples to guide the model to do so. And adding such instances
          might cause a lot of degradation in performance, i.e. Rejection of known
          content too. But their are methods to prevent hallucination, like apologising
          for not having the knowledge about something, can be done by training a
          model which just looking at the main model''s states would tell whether
          hallucination is occurring or not, thus provide rejection. Hope it helps '
        updatedAt: '2023-11-09T17:07:54.966Z'
      numEdits: 0
      reactions: []
    id: 654d11eaf0f195070242a83a
    type: comment
  author: deleted
  content: 'You see, it is not a rlhf/rlft model and the OpenOrca dataset doesn''t
    have any samples to guide the model to do so. And adding such instances might
    cause a lot of degradation in performance, i.e. Rejection of known content too.
    But their are methods to prevent hallucination, like apologising for not having
    the knowledge about something, can be done by training a model which just looking
    at the main model''s states would tell whether hallucination is occurring or not,
    thus provide rejection. Hope it helps '
  created_at: 2023-11-09 17:07:54+00:00
  edited: false
  hidden: false
  id: 654d11eaf0f195070242a83a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-11-09T17:51:08.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9615170359611511
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>Thanks. I don''t know a lot about the inner working of LLMs, but
          that makes sense. And the issue of universally and confidently stating something
          isn''t true, even though it is, plagues most other LLMs, including Zephyr
          and Open Hermes.</p>

          <p>It''s just from a human perspective this seems like a needless mistake,
          perhaps because we have an instinctual self-RAG-like mechanism. Confidence
          levels are INVALUABLE when conveying information. Any human that always
          confidently says things are either true or false would be avoided like the
          plague.  ''I don''t know'', ''I don''t think so'', ''most likely'' are crucial
          modifiers. For example, saying ''Jenny is pregnant'' vs ''I think Jenny
          is pregnant'' helps prevent incredibly awkward situations.</p>

          <p>Perhaps in time someone will find a way to train LLMs in less black and
          white terms without significantly degrading performance.</p>

          '
        raw: 'Thanks. I don''t know a lot about the inner working of LLMs, but that
          makes sense. And the issue of universally and confidently stating something
          isn''t true, even though it is, plagues most other LLMs, including Zephyr
          and Open Hermes.


          It''s just from a human perspective this seems like a needless mistake,
          perhaps because we have an instinctual self-RAG-like mechanism. Confidence
          levels are INVALUABLE when conveying information. Any human that always
          confidently says things are either true or false would be avoided like the
          plague.  ''I don''t know'', ''I don''t think so'', ''most likely'' are crucial
          modifiers. For example, saying ''Jenny is pregnant'' vs ''I think Jenny
          is pregnant'' helps prevent incredibly awkward situations.


          Perhaps in time someone will find a way to train LLMs in less black and
          white terms without significantly degrading performance.'
        updatedAt: '2023-11-09T17:51:08.252Z'
      numEdits: 0
      reactions: []
    id: 654d1c0c93023cd3d82f5f01
    type: comment
  author: Phil337
  content: 'Thanks. I don''t know a lot about the inner working of LLMs, but that
    makes sense. And the issue of universally and confidently stating something isn''t
    true, even though it is, plagues most other LLMs, including Zephyr and Open Hermes.


    It''s just from a human perspective this seems like a needless mistake, perhaps
    because we have an instinctual self-RAG-like mechanism. Confidence levels are
    INVALUABLE when conveying information. Any human that always confidently says
    things are either true or false would be avoided like the plague.  ''I don''t
    know'', ''I don''t think so'', ''most likely'' are crucial modifiers. For example,
    saying ''Jenny is pregnant'' vs ''I think Jenny is pregnant'' helps prevent incredibly
    awkward situations.


    Perhaps in time someone will find a way to train LLMs in less black and white
    terms without significantly degrading performance.'
  created_at: 2023-11-09 17:51:08+00:00
  edited: false
  hidden: false
  id: 654d1c0c93023cd3d82f5f01
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: Open-Orca/Mistral-7B-OpenOrca
repo_type: model
status: open
target_branch: null
title: Why is Open Orca trained to say a fact isn't true just because it can't find
  said fact?
