!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tonic
conflicting_files: null
created_at: 2023-10-03 02:36:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a3bb1cd0d8c2c2169f0b88/eT2TS0IlQbZtz-F_zHLz9.jpeg?w=200&h=200&f=face
      fullname: Joseph Pollack
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tonic
      type: user
    createdAt: '2023-10-03T03:36:03.000Z'
    data:
      edited: false
      editors:
      - Tonic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9605355858802795
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a3bb1cd0d8c2c2169f0b88/eT2TS0IlQbZtz-F_zHLz9.jpeg?w=200&h=200&f=face
          fullname: Joseph Pollack
          isHf: false
          isPro: false
          name: Tonic
          type: user
        html: '<p>I''m having trouble with the provided tokenizer , unclear what''s
          happening in that error. (sorry for not being more helpful!)</p>

          '
        raw: I'm having trouble with the provided tokenizer , unclear what's happening
          in that error. (sorry for not being more helpful!)
        updatedAt: '2023-10-03T03:36:03.919Z'
      numEdits: 0
      reactions: []
    id: 651b8c2326b2dee1ed1cd4be
    type: comment
  author: Tonic
  content: I'm having trouble with the provided tokenizer , unclear what's happening
    in that error. (sorry for not being more helpful!)
  created_at: 2023-10-03 02:36:03+00:00
  edited: false
  hidden: false
  id: 651b8c2326b2dee1ed1cd4be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f69d15e270df4127cf4e393a6c1dbe0e.svg
      fullname: Aaron Mihalik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: writerflether
      type: user
    createdAt: '2023-10-03T07:20:58.000Z'
    data:
      edited: false
      editors:
      - writerflether
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5135740041732788
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f69d15e270df4127cf4e393a6c1dbe0e.svg
          fullname: Aaron Mihalik
          isHf: false
          isPro: false
          name: writerflether
          type: user
        html: "<p>I concur.  I'm trying to load this model in text-generation-inference.\
          \  Here's the stack:</p>\n<pre><code>2023-10-03T07:17:50.796666Z  INFO download:\
          \ text_generation_launcher: Successfully downloaded weights.\n2023-10-03T07:17:50.796964Z\
          \  INFO shard-manager: text_generation_launcher: Starting shard rank=0\n\
          2023-10-03T07:18:00.806182Z  INFO shard-manager: text_generation_launcher:\
          \ Waiting for shard to be ready... rank=0\n2023-10-03T07:18:05.539569Z ERROR\
          \ text_generation_launcher: Error when initializing model\nTraceback (most\
          \ recent call last):\n  File \"/opt/conda/bin/text-generation-server\",\
          \ line 8, in &lt;module&gt;\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          &gt; File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 252, in get_model\n    return FlashMistral(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 297, in __init__\n    tokenizer = LlamaTokenizerFast.from_pretrained(\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1854, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1886, in _from_pretrained\n    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2073, in _from_pretrained\n    raise ValueError(\nValueError: Non-consecutive\
          \ added token '&lt;unk&gt;' found. Should have index 32000 but has index\
          \ 0 in saved vocabulary.\n</code></pre>\n"
        raw: "I concur.  I'm trying to load this model in text-generation-inference.\
          \  Here's the stack:\n\n```\n2023-10-03T07:17:50.796666Z  INFO download:\
          \ text_generation_launcher: Successfully downloaded weights.\n2023-10-03T07:17:50.796964Z\
          \  INFO shard-manager: text_generation_launcher: Starting shard rank=0\n\
          2023-10-03T07:18:00.806182Z  INFO shard-manager: text_generation_launcher:\
          \ Waiting for shard to be ready... rank=0\n2023-10-03T07:18:05.539569Z ERROR\
          \ text_generation_launcher: Error when initializing model\nTraceback (most\
          \ recent call last):\n  File \"/opt/conda/bin/text-generation-server\",\
          \ line 8, in <module>\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157,\
          \ in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
          , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434,\
          \ in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 783, in\
          \ invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          > File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 252, in get_model\n    return FlashMistral(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 297, in __init__\n    tokenizer = LlamaTokenizerFast.from_pretrained(\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1854, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1886, in _from_pretrained\n    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
          , line 2073, in _from_pretrained\n    raise ValueError(\nValueError: Non-consecutive\
          \ added token '<unk>' found. Should have index 32000 but has index 0 in\
          \ saved vocabulary.\n```"
        updatedAt: '2023-10-03T07:20:58.392Z'
      numEdits: 0
      reactions: []
    id: 651bc0da851fe726ad99e960
    type: comment
  author: writerflether
  content: "I concur.  I'm trying to load this model in text-generation-inference.\
    \  Here's the stack:\n\n```\n2023-10-03T07:17:50.796666Z  INFO download: text_generation_launcher:\
    \ Successfully downloaded weights.\n2023-10-03T07:17:50.796964Z  INFO shard-manager:\
    \ text_generation_launcher: Starting shard rank=0\n2023-10-03T07:18:00.806182Z\
    \  INFO shard-manager: text_generation_launcher: Waiting for shard to be ready...\
    \ rank=0\n2023-10-03T07:18:05.539569Z ERROR text_generation_launcher: Error when\
    \ initializing model\nTraceback (most recent call last):\n  File \"/opt/conda/bin/text-generation-server\"\
    , line 8, in <module>\n    sys.exit(app())\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1157, in __call__\n\
    \    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.9/site-packages/typer/core.py\"\
    , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\", line 1434, in\
    \ invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/opt/conda/lib/python3.9/site-packages/click/core.py\"\
    , line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.9/site-packages/typer/main.py\"\
    , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\", line\
    \ 83, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 207, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 634, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 601, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
    , line 1905, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.9/asyncio/events.py\"\
    , line 80, in _run\n    self._context.run(self._callback, *self._args)\n> File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 159, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 252, in get_model\n    return FlashMistral(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py\"\
    , line 297, in __init__\n    tokenizer = LlamaTokenizerFast.from_pretrained(\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1854, in from_pretrained\n    return cls._from_pretrained(\n  File \"/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1886, in _from_pretrained\n    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(\n\
    \  File \"/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\"\
    , line 2073, in _from_pretrained\n    raise ValueError(\nValueError: Non-consecutive\
    \ added token '<unk>' found. Should have index 32000 but has index 0 in saved\
    \ vocabulary.\n```"
  created_at: 2023-10-03 06:20:58+00:00
  edited: false
  hidden: false
  id: 651bc0da851fe726ad99e960
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9441d895e3308a15a50c5dab942454ff.svg
      fullname: Bleys
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: bleysg
      type: user
    createdAt: '2023-10-03T07:26:42.000Z'
    data:
      edited: false
      editors:
      - bleysg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9366235136985779
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9441d895e3308a15a50c5dab942454ff.svg
          fullname: Bleys
          isHf: false
          isPro: true
          name: bleysg
          type: user
        html: '<p>You''ll need to get into whatever environment you have setup for
          ooba (e.g. conda) and do:</p>

          <pre><code>pip install git+https://github.com/huggingface/transformers

          </code></pre>

          <p>This is because support for Mistral in Transformers is not merged to
          PyPI yet, so you need to install from the development snapshot.</p>

          '
        raw: 'You''ll need to get into whatever environment you have setup for ooba
          (e.g. conda) and do:

          ```

          pip install git+https://github.com/huggingface/transformers

          ```


          This is because support for Mistral in Transformers is not merged to PyPI
          yet, so you need to install from the development snapshot.'
        updatedAt: '2023-10-03T07:26:42.176Z'
      numEdits: 0
      reactions: []
    id: 651bc2324560189b7a513d4d
    type: comment
  author: bleysg
  content: 'You''ll need to get into whatever environment you have setup for ooba
    (e.g. conda) and do:

    ```

    pip install git+https://github.com/huggingface/transformers

    ```


    This is because support for Mistral in Transformers is not merged to PyPI yet,
    so you need to install from the development snapshot.'
  created_at: 2023-10-03 06:26:42+00:00
  edited: false
  hidden: false
  id: 651bc2324560189b7a513d4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f69d15e270df4127cf4e393a6c1dbe0e.svg
      fullname: Aaron Mihalik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: writerflether
      type: user
    createdAt: '2023-10-03T08:43:54.000Z'
    data:
      edited: false
      editors:
      - writerflether
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8053708076477051
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f69d15e270df4127cf4e393a6c1dbe0e.svg
          fullname: Aaron Mihalik
          isHf: false
          isPro: false
          name: writerflether
          type: user
        html: "<p>Thanks, that worked for me.  </p>\n<p>I assumed that since text-generation-inference:1.1.0\
          \ has support for Mistral, that it would work out of the box.  Instead I\
          \ had to create a new image.  eg:</p>\n<pre><code>FROM ghcr.io/huggingface/text-generation-inference:1.1.0\n\
          \nRUN apt-get update -y &amp;&amp; \\\n DEBIAN_FRONTEND=noninteractive apt-get\
          \ install -y git &amp;&amp; \\\n apt-get clean &amp;&amp; \\\n rm -rf /var/lib/apt/lists/*\n\
          \nRUN pip3 install --no-cache-dir \\\n    \"git+https://github.com/huggingface/transformers\"\
          \n</code></pre>\n"
        raw: "Thanks, that worked for me.  \n\nI assumed that since text-generation-inference:1.1.0\
          \ has support for Mistral, that it would work out of the box.  Instead I\
          \ had to create a new image.  eg:\n\n```\nFROM ghcr.io/huggingface/text-generation-inference:1.1.0\n\
          \nRUN apt-get update -y && \\\n DEBIAN_FRONTEND=noninteractive apt-get install\
          \ -y git && \\\n apt-get clean && \\\n rm -rf /var/lib/apt/lists/*\n\nRUN\
          \ pip3 install --no-cache-dir \\\n    \"git+https://github.com/huggingface/transformers\"\
          \n```"
        updatedAt: '2023-10-03T08:43:54.648Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - bleysg
        - Tonic
        - delphi-mirvine
    id: 651bd44a6042caa8e02a0d92
    type: comment
  author: writerflether
  content: "Thanks, that worked for me.  \n\nI assumed that since text-generation-inference:1.1.0\
    \ has support for Mistral, that it would work out of the box.  Instead I had to\
    \ create a new image.  eg:\n\n```\nFROM ghcr.io/huggingface/text-generation-inference:1.1.0\n\
    \nRUN apt-get update -y && \\\n DEBIAN_FRONTEND=noninteractive apt-get install\
    \ -y git && \\\n apt-get clean && \\\n rm -rf /var/lib/apt/lists/*\n\nRUN pip3\
    \ install --no-cache-dir \\\n    \"git+https://github.com/huggingface/transformers\"\
    \n```"
  created_at: 2023-10-03 07:43:54+00:00
  edited: false
  hidden: false
  id: 651bd44a6042caa8e02a0d92
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a3bb1cd0d8c2c2169f0b88/eT2TS0IlQbZtz-F_zHLz9.jpeg?w=200&h=200&f=face
      fullname: Joseph Pollack
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tonic
      type: user
    createdAt: '2023-10-03T09:26:27.000Z'
    data:
      edited: false
      editors:
      - Tonic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9533979892730713
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a3bb1cd0d8c2c2169f0b88/eT2TS0IlQbZtz-F_zHLz9.jpeg?w=200&h=200&f=face
          fullname: Joseph Pollack
          isHf: false
          isPro: false
          name: Tonic
          type: user
        html: '<p>is there a way to do this programmatically - yet ? (i''m trying
          to host it here, on hugging face)</p>

          '
        raw: is there a way to do this programmatically - yet ? (i'm trying to host
          it here, on hugging face)
        updatedAt: '2023-10-03T09:26:27.293Z'
      numEdits: 0
      reactions: []
    id: 651bde438010d2458e1ed7f4
    type: comment
  author: Tonic
  content: is there a way to do this programmatically - yet ? (i'm trying to host
    it here, on hugging face)
  created_at: 2023-10-03 08:26:27+00:00
  edited: false
  hidden: false
  id: 651bde438010d2458e1ed7f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/qAgetu7v3KWc4jOjz85tP.jpeg?w=200&h=200&f=face
      fullname: Wenxuan Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gilnore
      type: user
    createdAt: '2023-10-05T07:37:26.000Z'
    data:
      edited: true
      editors:
      - gilnore
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8888737559318542
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/qAgetu7v3KWc4jOjz85tP.jpeg?w=200&h=200&f=face
          fullname: Wenxuan Wang
          isHf: false
          isPro: false
          name: gilnore
          type: user
        html: '<p>I seem to be still getting:<br>"""raise TypeError(f"{config.model_type}
          isn''t supported yet.")</p>

          <p>TypeError: mistral isn''t supported yet."""<br>even after updating with
          the given command.</p>

          <p>I''m just loading it through AutoTokenizer.from_pretrained</p>

          '
        raw: 'I seem to be still getting:

          """raise TypeError(f"{config.model_type} isn''t supported yet.")


          TypeError: mistral isn''t supported yet."""

          even after updating with the given command.


          I''m just loading it through AutoTokenizer.from_pretrained'
        updatedAt: '2023-10-05T07:38:24.852Z'
      numEdits: 1
      reactions: []
    id: 651e67b660b54f44a127ede8
    type: comment
  author: gilnore
  content: 'I seem to be still getting:

    """raise TypeError(f"{config.model_type} isn''t supported yet.")


    TypeError: mistral isn''t supported yet."""

    even after updating with the given command.


    I''m just loading it through AutoTokenizer.from_pretrained'
  created_at: 2023-10-05 06:37:26+00:00
  edited: true
  hidden: false
  id: 651e67b660b54f44a127ede8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a3bb1cd0d8c2c2169f0b88/eT2TS0IlQbZtz-F_zHLz9.jpeg?w=200&h=200&f=face
      fullname: Joseph Pollack
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tonic
      type: user
    createdAt: '2023-10-06T21:25:44.000Z'
    data:
      edited: false
      editors:
      - Tonic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9640482664108276
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a3bb1cd0d8c2c2169f0b88/eT2TS0IlQbZtz-F_zHLz9.jpeg?w=200&h=200&f=face
          fullname: Joseph Pollack
          isHf: false
          isPro: false
          name: Tonic
          type: user
        html: '<p>normally it''s been fixed, you have to set the max token lengths
          when you deploy</p>

          '
        raw: normally it's been fixed, you have to set the max token lengths when
          you deploy
        updatedAt: '2023-10-06T21:25:44.171Z'
      numEdits: 0
      reactions: []
    id: 65207b58fe5881ad35a87723
    type: comment
  author: Tonic
  content: normally it's been fixed, you have to set the max token lengths when you
    deploy
  created_at: 2023-10-06 20:25:44+00:00
  edited: false
  hidden: false
  id: 65207b58fe5881ad35a87723
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Open-Orca/Mistral-7B-OpenOrca
repo_type: model
status: open
target_branch: null
title: 'I''m getting error : <unc> set to 0 in the tokenizer config'
