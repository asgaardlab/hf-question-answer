!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Anuraag-pal
conflicting_files: null
created_at: 2023-11-22 21:07:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/404ea345ab14024124f39d025374bca7.svg
      fullname: Anurag Pal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anuraag-pal
      type: user
    createdAt: '2023-11-22T21:07:05.000Z'
    data:
      edited: false
      editors:
      - Anuraag-pal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6709402799606323
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/404ea345ab14024124f39d025374bca7.svg
          fullname: Anurag Pal
          isHf: false
          isPro: false
          name: Anuraag-pal
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64dea5d72802bd66a8a675e2/pGchx03U6sy1BfSSnbkEB.png"><img
          alt="Screenshot 2023-11-22 at 9.51.44 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/64dea5d72802bd66a8a675e2/pGchx03U6sy1BfSSnbkEB.png"></a></p>

          <p>Llama 2 7b is running fine with llama_index but when querying with mistral7b
          varient, it''s showing this error!</p>

          '
        raw: "\r\n![Screenshot 2023-11-22 at 9.51.44 PM.png](https://cdn-uploads.huggingface.co/production/uploads/64dea5d72802bd66a8a675e2/pGchx03U6sy1BfSSnbkEB.png)\r\
          \n\r\nLlama 2 7b is running fine with llama_index but when querying with\
          \ mistral7b varient, it's showing this error!"
        updatedAt: '2023-11-22T21:07:05.277Z'
      numEdits: 0
      reactions: []
    id: 655e6d79d5c0d3db5362b16c
    type: comment
  author: Anuraag-pal
  content: "\r\n![Screenshot 2023-11-22 at 9.51.44 PM.png](https://cdn-uploads.huggingface.co/production/uploads/64dea5d72802bd66a8a675e2/pGchx03U6sy1BfSSnbkEB.png)\r\
    \n\r\nLlama 2 7b is running fine with llama_index but when querying with mistral7b\
    \ varient, it's showing this error!"
  created_at: 2023-11-22 21:07:05+00:00
  edited: false
  hidden: false
  id: 655e6d79d5c0d3db5362b16c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/457bc8db4a0d94789773d29e186cf748.svg
      fullname: Raza Abbas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smrazaabbas
      type: user
    createdAt: '2023-11-29T09:48:38.000Z'
    data:
      edited: false
      editors:
      - smrazaabbas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8899356722831726
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/457bc8db4a0d94789773d29e186cf748.svg
          fullname: Raza Abbas
          isHf: false
          isPro: false
          name: smrazaabbas
          type: user
        html: '<p>Facing the same error, wondering if you''ve found out a solution
          yet?</p>

          '
        raw: Facing the same error, wondering if you've found out a solution yet?
        updatedAt: '2023-11-29T09:48:38.114Z'
      numEdits: 0
      reactions: []
    id: 656708f666d5f87c62ce7f3c
    type: comment
  author: smrazaabbas
  content: Facing the same error, wondering if you've found out a solution yet?
  created_at: 2023-11-29 09:48:38+00:00
  edited: false
  hidden: false
  id: 656708f666d5f87c62ce7f3c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: Open-Orca/Mistral-7B-OpenOrca
repo_type: model
status: open
target_branch: null
title: '"OutOfMemoryError: CUDA out of memory"'
