!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jiaxiangc
conflicting_files: null
created_at: 2023-10-30 08:58:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641c38e045487810d13265bd/Xjr0nRXfeijFojYKWfxLQ.jpeg?w=200&h=200&f=face
      fullname: Jiaxiang Cheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jiaxiangc
      type: user
    createdAt: '2023-10-30T09:58:59.000Z'
    data:
      edited: false
      editors:
      - jiaxiangc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9090387225151062
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641c38e045487810d13265bd/Xjr0nRXfeijFojYKWfxLQ.jpeg?w=200&h=200&f=face
          fullname: Jiaxiang Cheng
          isHf: false
          isPro: false
          name: jiaxiangc
          type: user
        html: '<p>Thanks for your contribution.<br>After I fine tuning LLaMA-13B on
          OpenOrca or SlimOrca, I want to ask two questions.</p>

          <ol>

          <li>What is your training configuration? Such as, GPU numers, learning rate,
          fine tune strategy and epoch numbers.</li>

          <li>Does your fine-tuning process overfit? When i start the second epoch,
          the training loss dropped significantly. Is this normal? Do you have any
          suggestions to avoid this problem?</li>

          </ol>

          '
        raw: "Thanks for your contribution.\r\nAfter I fine tuning LLaMA-13B on OpenOrca\
          \ or SlimOrca, I want to ask two questions.\r\n1. What is your training\
          \ configuration? Such as, GPU numers, learning rate, fine tune strategy\
          \ and epoch numbers.\r\n2. Does your fine-tuning process overfit? When i\
          \ start the second epoch, the training loss dropped significantly. Is this\
          \ normal? Do you have any suggestions to avoid this problem?"
        updatedAt: '2023-10-30T09:58:59.565Z'
      numEdits: 0
      reactions: []
    id: 653f7e638a67c542ee275440
    type: comment
  author: jiaxiangc
  content: "Thanks for your contribution.\r\nAfter I fine tuning LLaMA-13B on OpenOrca\
    \ or SlimOrca, I want to ask two questions.\r\n1. What is your training configuration?\
    \ Such as, GPU numers, learning rate, fine tune strategy and epoch numbers.\r\n\
    2. Does your fine-tuning process overfit? When i start the second epoch, the training\
    \ loss dropped significantly. Is this normal? Do you have any suggestions to avoid\
    \ this problem?"
  created_at: 2023-10-30 08:58:59+00:00
  edited: false
  hidden: false
  id: 653f7e638a67c542ee275440
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643c81209f5d314db2de9743/EgFTMX41xoaPdp5jffj1G.png?w=200&h=200&f=face
      fullname: Nathan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: unaidedelf87777
      type: user
    createdAt: '2023-10-30T16:20:07.000Z'
    data:
      edited: false
      editors:
      - unaidedelf87777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9200713038444519
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643c81209f5d314db2de9743/EgFTMX41xoaPdp5jffj1G.png?w=200&h=200&f=face
          fullname: Nathan
          isHf: false
          isPro: false
          name: unaidedelf87777
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jiaxiangc&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jiaxiangc\">@<span class=\"\
          underline\">jiaxiangc</span></a></span>\n\n\t</span></span>  </p>\n<p>For\
          \ the compute config, it is 8x a6000 gpus, rented from runpod.io. To prevent\
          \ overfitting we use packing, which also will speed up training a considerable\
          \ amount. As far as that trainer we use, it is called axolotl, and you can\
          \ find it here <a rel=\"nofollow\" href=\"https://github.com/OpenAccess-AI-Collective/axolotl\"\
          >https://github.com/OpenAccess-AI-Collective/axolotl</a>.  for learning\
          \ rate and all other config options, in the configs folder on each model\
          \ there is a yaml file which details all the options which axolotl uses.\
          \ </p>\n<p>Hope that helps!</p>\n"
        raw: "@jiaxiangc  \n\nFor the compute config, it is 8x a6000 gpus, rented\
          \ from runpod.io. To prevent overfitting we use packing, which also will\
          \ speed up training a considerable amount. As far as that trainer we use,\
          \ it is called axolotl, and you can find it here https://github.com/OpenAccess-AI-Collective/axolotl.\
          \  for learning rate and all other config options, in the configs folder\
          \ on each model there is a yaml file which details all the options which\
          \ axolotl uses. \n\nHope that helps!"
        updatedAt: '2023-10-30T16:20:07.651Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - jlzhou
        - layoric
    id: 653fd7b7945f53b87e0f128f
    type: comment
  author: unaidedelf87777
  content: "@jiaxiangc  \n\nFor the compute config, it is 8x a6000 gpus, rented from\
    \ runpod.io. To prevent overfitting we use packing, which also will speed up training\
    \ a considerable amount. As far as that trainer we use, it is called axolotl,\
    \ and you can find it here https://github.com/OpenAccess-AI-Collective/axolotl.\
    \  for learning rate and all other config options, in the configs folder on each\
    \ model there is a yaml file which details all the options which axolotl uses.\
    \ \n\nHope that helps!"
  created_at: 2023-10-30 15:20:07+00:00
  edited: false
  hidden: false
  id: 653fd7b7945f53b87e0f128f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c5d0031c796a3c11bcb0d01b959168dc.svg
      fullname: John Kirchenbauer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jwkirchenbauer
      type: user
    createdAt: '2023-11-30T21:55:33.000Z'
    data:
      edited: true
      editors:
      - jwkirchenbauer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9247604012489319
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c5d0031c796a3c11bcb0d01b959168dc.svg
          fullname: John Kirchenbauer
          isHf: false
          isPro: false
          name: jwkirchenbauer
          type: user
        html: '<p>Thanks for storing the axolotl config! I suggest you add this to
          the model card so that people know where to find it :] just my 2c</p>

          '
        raw: Thanks for storing the axolotl config! I suggest you add this to the
          model card so that people know where to find it :] just my 2c
        updatedAt: '2023-11-30T21:55:54.990Z'
      numEdits: 1
      reactions: []
    id: 656904d51d7c2ca7b79be0e4
    type: comment
  author: jwkirchenbauer
  content: Thanks for storing the axolotl config! I suggest you add this to the model
    card so that people know where to find it :] just my 2c
  created_at: 2023-11-30 21:55:33+00:00
  edited: true
  hidden: false
  id: 656904d51d7c2ca7b79be0e4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: Open-Orca/Mistral-7B-OpenOrca
repo_type: model
status: open
target_branch: null
title: Does your fine-tuning process overfit?
