!!python/object:huggingface_hub.community.DiscussionWithDetails
author: timlim123
conflicting_files: null
created_at: 2023-12-02 10:22:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12ec5e367e2eac1650b2961097f74fa2.svg
      fullname: Lim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: timlim123
      type: user
    createdAt: '2023-12-02T10:22:33.000Z'
    data:
      edited: false
      editors:
      - timlim123
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8881706595420837
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12ec5e367e2eac1650b2961097f74fa2.svg
          fullname: Lim
          isHf: false
          isPro: false
          name: timlim123
          type: user
        html: "<p>In your example, </p>\n<pre><code>chat = [\n  {\"role\": \"system\"\
          , \"content\": \"You are MistralOrca, a large language model trained by\
          \ Alignment Lab AI. Write out your reasoning step-by-step to be sure you\
          \ get the right answers!\"}\n  {\"role\": \"user\", \"content\": \"How are\
          \ you?\"},\n  {\"role\": \"assistant\", \"content\": \"I am doing well!\"\
          },\n  {\"role\": \"user\", \"content\": \"Please tell me about how mistral\
          \ winds have attracted super-orcas.\"},\n]\n</code></pre>\n<p>However, the\
          \ finetune data is OpenOrca which is a single-round instruction data.  \
          \ How can the model learn to handle multi-round conversation? </p>\n"
        raw: "In your example, \r\n\r\n```\r\nchat = [\r\n  {\"role\": \"system\"\
          , \"content\": \"You are MistralOrca, a large language model trained by\
          \ Alignment Lab AI. Write out your reasoning step-by-step to be sure you\
          \ get the right answers!\"}\r\n  {\"role\": \"user\", \"content\": \"How\
          \ are you?\"},\r\n  {\"role\": \"assistant\", \"content\": \"I am doing\
          \ well!\"},\r\n  {\"role\": \"user\", \"content\": \"Please tell me about\
          \ how mistral winds have attracted super-orcas.\"},\r\n]\r\n```\r\n\r\n\
          However, the finetune data is OpenOrca which is a single-round instruction\
          \ data.   How can the model learn to handle multi-round conversation? "
        updatedAt: '2023-12-02T10:22:33.295Z'
      numEdits: 0
      reactions: []
    id: 656b05698a37acfa3fbe39ef
    type: comment
  author: timlim123
  content: "In your example, \r\n\r\n```\r\nchat = [\r\n  {\"role\": \"system\", \"\
    content\": \"You are MistralOrca, a large language model trained by Alignment\
    \ Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\"\
    }\r\n  {\"role\": \"user\", \"content\": \"How are you?\"},\r\n  {\"role\": \"\
    assistant\", \"content\": \"I am doing well!\"},\r\n  {\"role\": \"user\", \"\
    content\": \"Please tell me about how mistral winds have attracted super-orcas.\"\
    },\r\n]\r\n```\r\n\r\nHowever, the finetune data is OpenOrca which is a single-round\
    \ instruction data.   How can the model learn to handle multi-round conversation? "
  created_at: 2023-12-02 10:22:33+00:00
  edited: false
  hidden: false
  id: 656b05698a37acfa3fbe39ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4015b046f1951942058a0b613681d375.svg
      fullname: zek nife
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zekozeko
      type: user
    createdAt: '2023-12-06T16:04:24.000Z'
    data:
      edited: true
      editors:
      - zekozeko
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9655176997184753
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4015b046f1951942058a0b613681d375.svg
          fullname: zek nife
          isHf: false
          isPro: false
          name: zekozeko
          type: user
        html: '<p>I don''t have a precise knowledge on how this model was tuned, but
          multi-round conversation is a general property of large language models,
          and this property is maintained when fine-tuning on single-round instructions</p>

          '
        raw: I don't have a precise knowledge on how this model was tuned, but multi-round
          conversation is a general property of large language models, and this property
          is maintained when fine-tuning on single-round instructions
        updatedAt: '2023-12-06T16:04:39.423Z'
      numEdits: 1
      reactions: []
    id: 65709b8887a92b76922db3eb
    type: comment
  author: zekozeko
  content: I don't have a precise knowledge on how this model was tuned, but multi-round
    conversation is a general property of large language models, and this property
    is maintained when fine-tuning on single-round instructions
  created_at: 2023-12-06 16:04:24+00:00
  edited: true
  hidden: false
  id: 65709b8887a92b76922db3eb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: Open-Orca/Mistral-7B-OpenOrca
repo_type: model
status: open
target_branch: null
title: How is this model multi-round?
