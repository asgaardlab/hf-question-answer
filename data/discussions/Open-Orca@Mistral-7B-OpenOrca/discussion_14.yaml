!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mattma1970
conflicting_files: null
created_at: 2023-10-16 23:57:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8addf8cb4b243ac1e2ac14ef1e707f81.svg
      fullname: Matt Ma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: mattma1970
      type: user
    createdAt: '2023-10-17T00:57:29.000Z'
    data:
      edited: false
      editors:
      - mattma1970
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5324925184249878
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8addf8cb4b243ac1e2ac14ef1e707f81.svg
          fullname: Matt Ma
          isHf: false
          isPro: true
          name: mattma1970
          type: user
        html: '<p>I''ve been trying to download the OpenOrca dataset to use it to
          finetune some other models. It seems there is something wrong with the recently
          committed parquet conversion functions. Downloading with load_dataset("Open-Orca/OpenOrca")
          results in an error when it build the training dataset. This also happens
          when I clone the repo manually. </p>

          <p>ArrowInvalid                              Traceback (most recent call
          last)<br>File ~/Documents/Repos/llama2-4int/.venv/lib/python3.10/site-packages/datasets/builder.py:1879,
          in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format,
          max_shard_size, job_id)<br>   1878 _time = time.time()<br>-&gt; 1879 for
          _, table in generator:<br>   1880     if max_shard_size is not None and
          writer._num_bytes &gt; max_shard_size:</p>

          <p>File ~/Documents/Repos/llama2-4int/.venv/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py:73,
          in Parquet._generate_tables(self, files)<br>     72 with open(file, "rb")
          as f:<br>---&gt; 73     parquet_file = pq.ParquetFile(f)<br>     74     try:</p>

          <p>File ~/Documents/Repos/llama2-4int/.venv/lib/python3.10/site-packages/pyarrow/parquet/core.py:341,
          in ParquetFile.<strong>init</strong>(self, source, metadata, common_metadata,
          read_dictionary, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit,
          decryption_properties, thrift_string_size_limit, thrift_container_size_limit,
          filesystem)<br>    340 self.reader = ParquetReader()<br>--&gt; 341 self.reader.open(<br>    342     source,
          use_memory_map=memory_map,<br>    343     buffer_size=buffer_size, pre_buffer=pre_buffer,<br>    344     read_dictionary=read_dictionary,
          metadata=metadata,<br>    345     coerce_int96_timestamp_unit=coerce_int96_timestamp_unit,<br>    346     decryption_properties=decryption_properties,<br>    347     thrift_string_size_limit=thrift_string_size_limit,<br>    348     thrift_container_size_limit=thrift_container_size_limit,<br>    349
          )<br>    350 self.common_metadata = common_metadata<br>...<br>   1911         e
          = e.<strong>context</strong><br>-&gt; 1912     raise DatasetGenerationError("An
          error occurred while generating the dataset") from e<br>   1914 yield job_id,
          True, (total_num_examples, total_num_bytes, writer._features, num_shards,
          shard_lengths)</p>

          <p>DatasetGenerationError: An error occurred while generating the dataset</p>

          <p>Anyone else having the same issue? Any solution? </p>

          '
        raw: "I've been trying to download the OpenOrca dataset to use it to finetune\
          \ some other models. It seems there is something wrong with the recently\
          \ committed parquet conversion functions. Downloading with load_dataset(\"\
          Open-Orca/OpenOrca\") results in an error when it build the training dataset.\
          \ This also happens when I clone the repo manually. \r\n\r\nArrowInvalid\
          \                              Traceback (most recent call last)\r\nFile\
          \ ~/Documents/Repos/llama2-4int/.venv/lib/python3.10/site-packages/datasets/builder.py:1879,\
          \ in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format,\
          \ max_shard_size, job_id)\r\n   1878 _time = time.time()\r\n-> 1879 for\
          \ _, table in generator:\r\n   1880     if max_shard_size is not None and\
          \ writer._num_bytes > max_shard_size:\r\n\r\nFile ~/Documents/Repos/llama2-4int/.venv/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py:73,\
          \ in Parquet._generate_tables(self, files)\r\n     72 with open(file, \"\
          rb\") as f:\r\n---> 73     parquet_file = pq.ParquetFile(f)\r\n     74 \
          \    try:\r\n\r\nFile ~/Documents/Repos/llama2-4int/.venv/lib/python3.10/site-packages/pyarrow/parquet/core.py:341,\
          \ in ParquetFile.__init__(self, source, metadata, common_metadata, read_dictionary,\
          \ memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties,\
          \ thrift_string_size_limit, thrift_container_size_limit, filesystem)\r\n\
          \    340 self.reader = ParquetReader()\r\n--> 341 self.reader.open(\r\n\
          \    342     source, use_memory_map=memory_map,\r\n    343     buffer_size=buffer_size,\
          \ pre_buffer=pre_buffer,\r\n    344     read_dictionary=read_dictionary,\
          \ metadata=metadata,\r\n    345     coerce_int96_timestamp_unit=coerce_int96_timestamp_unit,\r\
          \n    346     decryption_properties=decryption_properties,\r\n    347  \
          \   thrift_string_size_limit=thrift_string_size_limit,\r\n    348     thrift_container_size_limit=thrift_container_size_limit,\r\
          \n    349 )\r\n    350 self.common_metadata = common_metadata\r\n...\r\n\
          \   1911         e = e.__context__\r\n-> 1912     raise DatasetGenerationError(\"\
          An error occurred while generating the dataset\") from e\r\n   1914 yield\
          \ job_id, True, (total_num_examples, total_num_bytes, writer._features,\
          \ num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred\
          \ while generating the dataset\r\n\r\nAnyone else having the same issue?\
          \ Any solution? "
        updatedAt: '2023-10-17T00:57:29.088Z'
      numEdits: 0
      reactions: []
    id: 652ddbf9a3da41257d6107ff
    type: comment
  author: mattma1970
  content: "I've been trying to download the OpenOrca dataset to use it to finetune\
    \ some other models. It seems there is something wrong with the recently committed\
    \ parquet conversion functions. Downloading with load_dataset(\"Open-Orca/OpenOrca\"\
    ) results in an error when it build the training dataset. This also happens when\
    \ I clone the repo manually. \r\n\r\nArrowInvalid                            \
    \  Traceback (most recent call last)\r\nFile ~/Documents/Repos/llama2-4int/.venv/lib/python3.10/site-packages/datasets/builder.py:1879,\
    \ in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format,\
    \ max_shard_size, job_id)\r\n   1878 _time = time.time()\r\n-> 1879 for _, table\
    \ in generator:\r\n   1880     if max_shard_size is not None and writer._num_bytes\
    \ > max_shard_size:\r\n\r\nFile ~/Documents/Repos/llama2-4int/.venv/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py:73,\
    \ in Parquet._generate_tables(self, files)\r\n     72 with open(file, \"rb\")\
    \ as f:\r\n---> 73     parquet_file = pq.ParquetFile(f)\r\n     74     try:\r\n\
    \r\nFile ~/Documents/Repos/llama2-4int/.venv/lib/python3.10/site-packages/pyarrow/parquet/core.py:341,\
    \ in ParquetFile.__init__(self, source, metadata, common_metadata, read_dictionary,\
    \ memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties,\
    \ thrift_string_size_limit, thrift_container_size_limit, filesystem)\r\n    340\
    \ self.reader = ParquetReader()\r\n--> 341 self.reader.open(\r\n    342     source,\
    \ use_memory_map=memory_map,\r\n    343     buffer_size=buffer_size, pre_buffer=pre_buffer,\r\
    \n    344     read_dictionary=read_dictionary, metadata=metadata,\r\n    345 \
    \    coerce_int96_timestamp_unit=coerce_int96_timestamp_unit,\r\n    346     decryption_properties=decryption_properties,\r\
    \n    347     thrift_string_size_limit=thrift_string_size_limit,\r\n    348  \
    \   thrift_container_size_limit=thrift_container_size_limit,\r\n    349 )\r\n\
    \    350 self.common_metadata = common_metadata\r\n...\r\n   1911         e =\
    \ e.__context__\r\n-> 1912     raise DatasetGenerationError(\"An error occurred\
    \ while generating the dataset\") from e\r\n   1914 yield job_id, True, (total_num_examples,\
    \ total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError:\
    \ An error occurred while generating the dataset\r\n\r\nAnyone else having the\
    \ same issue? Any solution? "
  created_at: 2023-10-16 23:57:29+00:00
  edited: false
  hidden: false
  id: 652ddbf9a3da41257d6107ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8addf8cb4b243ac1e2ac14ef1e707f81.svg
      fullname: Matt Ma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: mattma1970
      type: user
    createdAt: '2023-10-17T02:59:55.000Z'
    data:
      edited: false
      editors:
      - mattma1970
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7613916397094727
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8addf8cb4b243ac1e2ac14ef1e707f81.svg
          fullname: Matt Ma
          isHf: false
          isPro: true
          name: mattma1970
          type: user
        html: '<p>More details after debugging in IDE</p>

          <p>Exception has occurred: DatasetGenerationError<br>An error occurred while
          generating the dataset<br>pyarrow.lib.ArrowInvalid: Parquet magic bytes
          not found in footer. Either the file is corrupted or this is not a parquet
          file.</p>

          <p>The above exception was the direct cause of the following exception:</p>

          <p>  File "/home/mtman/Documents/Repos/llama2-4int/data.py", line 6, in
          <br>    dataset = load_dataset("./data/OpenOrca/")<br>datasets.builder.DatasetGenerationError:
          An error occurred while generating the dataset</p>

          '
        raw: "More details after debugging in IDE\n\nException has occurred: DatasetGenerationError\n\
          An error occurred while generating the dataset\npyarrow.lib.ArrowInvalid:\
          \ Parquet magic bytes not found in footer. Either the file is corrupted\
          \ or this is not a parquet file.\n\nThe above exception was the direct cause\
          \ of the following exception:\n\n  File \"/home/mtman/Documents/Repos/llama2-4int/data.py\"\
          , line 6, in <module>\n    dataset = load_dataset(\"./data/OpenOrca/\")\n\
          datasets.builder.DatasetGenerationError: An error occurred while generating\
          \ the dataset"
        updatedAt: '2023-10-17T02:59:55.368Z'
      numEdits: 0
      reactions: []
    id: 652df8ab39b7a1b6c85207f0
    type: comment
  author: mattma1970
  content: "More details after debugging in IDE\n\nException has occurred: DatasetGenerationError\n\
    An error occurred while generating the dataset\npyarrow.lib.ArrowInvalid: Parquet\
    \ magic bytes not found in footer. Either the file is corrupted or this is not\
    \ a parquet file.\n\nThe above exception was the direct cause of the following\
    \ exception:\n\n  File \"/home/mtman/Documents/Repos/llama2-4int/data.py\", line\
    \ 6, in <module>\n    dataset = load_dataset(\"./data/OpenOrca/\")\ndatasets.builder.DatasetGenerationError:\
    \ An error occurred while generating the dataset"
  created_at: 2023-10-17 01:59:55+00:00
  edited: false
  hidden: false
  id: 652df8ab39b7a1b6c85207f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8addf8cb4b243ac1e2ac14ef1e707f81.svg
      fullname: Matt Ma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: mattma1970
      type: user
    createdAt: '2023-10-17T03:00:26.000Z'
    data:
      edited: false
      editors:
      - mattma1970
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9720422029495239
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8addf8cb4b243ac1e2ac14ef1e707f81.svg
          fullname: Matt Ma
          isHf: false
          isPro: true
          name: mattma1970
          type: user
        html: '<p>UPDATE: I upgraded datasets to v 2.14.5 and it solved the problem.
          I was using 2.13. </p>

          '
        raw: "UPDATE: I upgraded datasets to v 2.14.5 and it solved the problem. I\
          \ was using 2.13. \n"
        updatedAt: '2023-10-17T03:00:26.410Z'
      numEdits: 0
      reactions: []
      relatedEventId: 652df8cab86e108d0ff165a3
    id: 652df8cab86e108d0ff165a1
    type: comment
  author: mattma1970
  content: "UPDATE: I upgraded datasets to v 2.14.5 and it solved the problem. I was\
    \ using 2.13. \n"
  created_at: 2023-10-17 02:00:26+00:00
  edited: false
  hidden: false
  id: 652df8cab86e108d0ff165a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/8addf8cb4b243ac1e2ac14ef1e707f81.svg
      fullname: Matt Ma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: mattma1970
      type: user
    createdAt: '2023-10-17T03:00:26.000Z'
    data:
      status: closed
    id: 652df8cab86e108d0ff165a3
    type: status-change
  author: mattma1970
  created_at: 2023-10-17 02:00:26+00:00
  id: 652df8cab86e108d0ff165a3
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643c81209f5d314db2de9743/EgFTMX41xoaPdp5jffj1G.png?w=200&h=200&f=face
      fullname: Nathan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: unaidedelf87777
      type: user
    createdAt: '2023-10-17T03:35:59.000Z'
    data:
      edited: false
      editors:
      - unaidedelf87777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9239158034324646
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643c81209f5d314db2de9743/EgFTMX41xoaPdp5jffj1G.png?w=200&h=200&f=face
          fullname: Nathan
          isHf: false
          isPro: false
          name: unaidedelf87777
          type: user
        html: "<blockquote>\n<p>UPDATE: I upgraded datasets to v 2.14.5 and it solved\
          \ the problem. I was using 2.13.</p>\n</blockquote>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;mattma1970&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/mattma1970\">@<span class=\"underline\"\
          >mattma1970</span></a></span>\n\n\t</span></span>  FYI, I recommend using\
          \ the SlimOrca subset of our data. It is verified answers, and is smaller.\
          \ it will cost hundreds to train on all 4.5m entries, and to be frank some\
          \ are little to no learning value. <a href=\"https://huggingface.co/Open-Orca/SlimOrca/\"\
          >https://huggingface.co/Open-Orca/SlimOrca/</a></p>\n"
        raw: '> UPDATE: I upgraded datasets to v 2.14.5 and it solved the problem.
          I was using 2.13.


          @mattma1970  FYI, I recommend using the SlimOrca subset of our data. It
          is verified answers, and is smaller. it will cost hundreds to train on all
          4.5m entries, and to be frank some are little to no learning value. https://huggingface.co/Open-Orca/SlimOrca/'
        updatedAt: '2023-10-17T03:35:59.576Z'
      numEdits: 0
      reactions: []
    id: 652e011f2c5f2e2fcf4d9cb4
    type: comment
  author: unaidedelf87777
  content: '> UPDATE: I upgraded datasets to v 2.14.5 and it solved the problem. I
    was using 2.13.


    @mattma1970  FYI, I recommend using the SlimOrca subset of our data. It is verified
    answers, and is smaller. it will cost hundreds to train on all 4.5m entries, and
    to be frank some are little to no learning value. https://huggingface.co/Open-Orca/SlimOrca/'
  created_at: 2023-10-17 02:35:59+00:00
  edited: false
  hidden: false
  id: 652e011f2c5f2e2fcf4d9cb4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8addf8cb4b243ac1e2ac14ef1e707f81.svg
      fullname: Matt Ma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: mattma1970
      type: user
    createdAt: '2023-10-17T04:59:25.000Z'
    data:
      edited: false
      editors:
      - mattma1970
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9509015679359436
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8addf8cb4b243ac1e2ac14ef1e707f81.svg
          fullname: Matt Ma
          isHf: false
          isPro: true
          name: mattma1970
          type: user
        html: '<p>Thanks for the feedback. I''ll check it out.<br>Cheers<br>Matt</p>

          '
        raw: "Thanks for the feedback. I'll check it out. \nCheers\nMatt\n"
        updatedAt: '2023-10-17T04:59:25.277Z'
      numEdits: 0
      reactions: []
    id: 652e14add786913fc7eaf658
    type: comment
  author: mattma1970
  content: "Thanks for the feedback. I'll check it out. \nCheers\nMatt\n"
  created_at: 2023-10-17 03:59:25+00:00
  edited: false
  hidden: false
  id: 652e14add786913fc7eaf658
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: Open-Orca/Mistral-7B-OpenOrca
repo_type: model
status: closed
target_branch: null
title: OpenOrca Dataset 'Fail to generate dataset'
