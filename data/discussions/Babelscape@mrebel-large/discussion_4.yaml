!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aschimmenti
conflicting_files: null
created_at: 2023-11-08 15:34:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7cc91c9085dd5c6973441708c56572b2.svg
      fullname: Andrea Schimmenti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aschimmenti
      type: user
    createdAt: '2023-11-08T15:34:03.000Z'
    data:
      edited: false
      editors:
      - aschimmenti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7242841720581055
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7cc91c9085dd5c6973441708c56572b2.svg
          fullname: Andrea Schimmenti
          isHf: false
          isPro: false
          name: aschimmenti
          type: user
        html: '<p>Hello,<br>I was testing the model with sentences like:<br>"According
          to Martin Kemp, Leonardo was the sole author of Salvator Mundi, while according
          to Jacques Franck the Salvator Mundi was the result of a collaboration between
          Leonardo da Vinci and Salai"<br>And got the following output:<br>[{''head'':
          ''Salvator Mundi'', ''head_type'': ''media'', ''type'': ''author'', ''tail'':
          ''Leonardo da Vinci'', ''tail_type'': ''per''}, {''head'': ''Salvator Mundi'',
          ''head_type'': ''media'', ''type'': ''author'', ''tail'': ''Leonardo da
          Vinci'', ''tail_type'': ''per''}, {''head'': ''Leonardo da Vinci'', ''head_type'':
          ''per'', ''type'': ''notable work'', ''tail'': ''Salvator Mundi'', ''tail_type'':
          ''media''}, {''head'': ''Leonardo da Vinci'', ''head_type'': ''per'', ''type'':
          ''notable work'', ''tail'': ''Salvator Mundi'', ''tail_type'': ''media''}]<br>Technically
          it''s right, but it completely skipped any of the provenance mentions.<br>I
          would like to add this option through fine-tuning (which would require also
          substantial changes in the output format), but I don''t think I can achieve
          it with only that. Do you have any suggestions?  </p>

          '
        raw: "Hello, \r\nI was testing the model with sentences like: \r\n\"According\
          \ to Martin Kemp, Leonardo was the sole author of Salvator Mundi, while\
          \ according to Jacques Franck the Salvator Mundi was the result of a collaboration\
          \ between Leonardo da Vinci and Salai\"\r\nAnd got the following output:\
          \ \r\n[{'head': 'Salvator Mundi', 'head_type': 'media', 'type': 'author',\
          \ 'tail': 'Leonardo da Vinci', 'tail_type': 'per'}, {'head': 'Salvator Mundi',\
          \ 'head_type': 'media', 'type': 'author', 'tail': 'Leonardo da Vinci', 'tail_type':\
          \ 'per'}, {'head': 'Leonardo da Vinci', 'head_type': 'per', 'type': 'notable\
          \ work', 'tail': 'Salvator Mundi', 'tail_type': 'media'}, {'head': 'Leonardo\
          \ da Vinci', 'head_type': 'per', 'type': 'notable work', 'tail': 'Salvator\
          \ Mundi', 'tail_type': 'media'}]\r\nTechnically it's right, but it completely\
          \ skipped any of the provenance mentions. \r\nI would like to add this option\
          \ through fine-tuning (which would require also substantial changes in the\
          \ output format), but I don't think I can achieve it with only that. Do\
          \ you have any suggestions?  "
        updatedAt: '2023-11-08T15:34:03.422Z'
      numEdits: 0
      reactions: []
    id: 654baa6bd6153dccbe2ad8b6
    type: comment
  author: aschimmenti
  content: "Hello, \r\nI was testing the model with sentences like: \r\n\"According\
    \ to Martin Kemp, Leonardo was the sole author of Salvator Mundi, while according\
    \ to Jacques Franck the Salvator Mundi was the result of a collaboration between\
    \ Leonardo da Vinci and Salai\"\r\nAnd got the following output: \r\n[{'head':\
    \ 'Salvator Mundi', 'head_type': 'media', 'type': 'author', 'tail': 'Leonardo\
    \ da Vinci', 'tail_type': 'per'}, {'head': 'Salvator Mundi', 'head_type': 'media',\
    \ 'type': 'author', 'tail': 'Leonardo da Vinci', 'tail_type': 'per'}, {'head':\
    \ 'Leonardo da Vinci', 'head_type': 'per', 'type': 'notable work', 'tail': 'Salvator\
    \ Mundi', 'tail_type': 'media'}, {'head': 'Leonardo da Vinci', 'head_type': 'per',\
    \ 'type': 'notable work', 'tail': 'Salvator Mundi', 'tail_type': 'media'}]\r\n\
    Technically it's right, but it completely skipped any of the provenance mentions.\
    \ \r\nI would like to add this option through fine-tuning (which would require\
    \ also substantial changes in the output format), but I don't think I can achieve\
    \ it with only that. Do you have any suggestions?  "
  created_at: 2023-11-08 15:34:03+00:00
  edited: false
  hidden: false
  id: 654baa6bd6153dccbe2ad8b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7cc91c9085dd5c6973441708c56572b2.svg
      fullname: Andrea Schimmenti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aschimmenti
      type: user
    createdAt: '2023-11-08T15:42:27.000Z'
    data:
      edited: false
      editors:
      - aschimmenti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7718884348869324
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7cc91c9085dd5c6973441708c56572b2.svg
          fullname: Andrea Schimmenti
          isHf: false
          isPro: false
          name: aschimmenti
          type: user
        html: '<p>There''s already the "performer" relation when a person makes a
          statement, which overrides however the stated relationship (Sabrina - is
          - beautifulGirl); or completely irrational outputs. A couple examples:<br>"Marco
          says Sabrina is a beautiful girl" =&gt; [{''head'': ''Sabrina'', ''head_type'':
          ''media'', ''type'': ''performer'', ''tail'': ''Marco'', ''tail_type'':
          ''per''}]<br>"Al Jazeera reports about the IDF occupying Gaza" =&gt; [{''head'':
          ''Al Jazeera'', ''head_type'': ''concept'', ''type'': ''part of'', ''tail'':
          ''IDF'', ''tail_type'': ''concept''}]</p>

          <p>I have access to a big dataset (few million triples) of statements about
          statements (conflicting provenance on conflicting art attributions mostly)
          that could be leveraged for this scope.  </p>

          '
        raw: "There's already the \"performer\" relation when a person makes a statement,\
          \ which overrides however the stated relationship (Sabrina - is - beautifulGirl);\
          \ or completely irrational outputs. A couple examples: \n\"Marco says Sabrina\
          \ is a beautiful girl\" => [{'head': 'Sabrina', 'head_type': 'media', 'type':\
          \ 'performer', 'tail': 'Marco', 'tail_type': 'per'}]\n\"Al Jazeera reports\
          \ about the IDF occupying Gaza\" => [{'head': 'Al Jazeera', 'head_type':\
          \ 'concept', 'type': 'part of', 'tail': 'IDF', 'tail_type': 'concept'}]\n\
          \nI have access to a big dataset (few million triples) of statements about\
          \ statements (conflicting provenance on conflicting art attributions mostly)\
          \ that could be leveraged for this scope.  "
        updatedAt: '2023-11-08T15:42:27.853Z'
      numEdits: 0
      reactions: []
    id: 654bac637824e2bb58ced783
    type: comment
  author: aschimmenti
  content: "There's already the \"performer\" relation when a person makes a statement,\
    \ which overrides however the stated relationship (Sabrina - is - beautifulGirl);\
    \ or completely irrational outputs. A couple examples: \n\"Marco says Sabrina\
    \ is a beautiful girl\" => [{'head': 'Sabrina', 'head_type': 'media', 'type':\
    \ 'performer', 'tail': 'Marco', 'tail_type': 'per'}]\n\"Al Jazeera reports about\
    \ the IDF occupying Gaza\" => [{'head': 'Al Jazeera', 'head_type': 'concept',\
    \ 'type': 'part of', 'tail': 'IDF', 'tail_type': 'concept'}]\n\nI have access\
    \ to a big dataset (few million triples) of statements about statements (conflicting\
    \ provenance on conflicting art attributions mostly) that could be leveraged for\
    \ this scope.  "
  created_at: 2023-11-08 15:42:27+00:00
  edited: false
  hidden: false
  id: 654bac637824e2bb58ced783
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f04e7545d08220171a0ad3e/M0dWlakxnLakQlZl5iOq6.jpeg?w=200&h=200&f=face
      fullname: Pere-Lluis Huguet Cabot
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: PereLluis13
      type: user
    createdAt: '2023-11-09T09:36:49.000Z'
    data:
      edited: false
      editors:
      - PereLluis13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9597580432891846
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f04e7545d08220171a0ad3e/M0dWlakxnLakQlZl5iOq6.jpeg?w=200&h=200&f=face
          fullname: Pere-Lluis Huguet Cabot
          isHf: false
          isPro: false
          name: PereLluis13
          type: user
        html: '<p>If you want to leverage the pretraining, it is a good idea to preserve
          the format, but be aware that if only new data is used with new type of
          relations is used, you will run into catastrophic forgetting issues, and
          the model will probably only predict the new relations in the new training
          data. You can always combine your data with that of sredfm to avoid that,
          as I am sure that if you have triplets you could adapt them to follow a
          similar format.</p>

          <p>About your specific use case/example, "performer" is not about what you
          point, but rather <a rel="nofollow" href="https://www.wikidata.org/wiki/Property:P175">https://www.wikidata.org/wiki/Property:P175</a>,
          i.e. actor, musician, band or other performer associated with this role
          or musical work, as all training data comes from Wikidata properties. The
          two examples you give are indeed errors/irrational, due to the fact that
          training data was quite different in nature, and one of the shortcomings
          of rebel/mrebel was that it is not trained to give "no predictions" when
          there are none to predict from within the relation types it is trained on.
          Instead, due to its autoregressive nature, it is "biased" (in the sense
          that at training time it was always given a sequence to generate) to give
          a prediction no matter what, leading to weird outputs. Perplexity can be
          a good tool to assess the "quality" of the generated sequence, as I expect
          it to be high for those two examples.</p>

          <p>Hope this helps,<br>Pere-Lluis.</p>

          '
        raw: 'If you want to leverage the pretraining, it is a good idea to preserve
          the format, but be aware that if only new data is used with new type of
          relations is used, you will run into catastrophic forgetting issues, and
          the model will probably only predict the new relations in the new training
          data. You can always combine your data with that of sredfm to avoid that,
          as I am sure that if you have triplets you could adapt them to follow a
          similar format.


          About your specific use case/example, "performer" is not about what you
          point, but rather https://www.wikidata.org/wiki/Property:P175, i.e. actor,
          musician, band or other performer associated with this role or musical work,
          as all training data comes from Wikidata properties. The two examples you
          give are indeed errors/irrational, due to the fact that training data was
          quite different in nature, and one of the shortcomings of rebel/mrebel was
          that it is not trained to give "no predictions" when there are none to predict
          from within the relation types it is trained on. Instead, due to its autoregressive
          nature, it is "biased" (in the sense that at training time it was always
          given a sequence to generate) to give a prediction no matter what, leading
          to weird outputs. Perplexity can be a good tool to assess the "quality"
          of the generated sequence, as I expect it to be high for those two examples.


          Hope this helps,

          Pere-Lluis.'
        updatedAt: '2023-11-09T09:36:49.117Z'
      numEdits: 0
      reactions: []
    id: 654ca831d5c6faa9e380b76a
    type: comment
  author: PereLluis13
  content: 'If you want to leverage the pretraining, it is a good idea to preserve
    the format, but be aware that if only new data is used with new type of relations
    is used, you will run into catastrophic forgetting issues, and the model will
    probably only predict the new relations in the new training data. You can always
    combine your data with that of sredfm to avoid that, as I am sure that if you
    have triplets you could adapt them to follow a similar format.


    About your specific use case/example, "performer" is not about what you point,
    but rather https://www.wikidata.org/wiki/Property:P175, i.e. actor, musician,
    band or other performer associated with this role or musical work, as all training
    data comes from Wikidata properties. The two examples you give are indeed errors/irrational,
    due to the fact that training data was quite different in nature, and one of the
    shortcomings of rebel/mrebel was that it is not trained to give "no predictions"
    when there are none to predict from within the relation types it is trained on.
    Instead, due to its autoregressive nature, it is "biased" (in the sense that at
    training time it was always given a sequence to generate) to give a prediction
    no matter what, leading to weird outputs. Perplexity can be a good tool to assess
    the "quality" of the generated sequence, as I expect it to be high for those two
    examples.


    Hope this helps,

    Pere-Lluis.'
  created_at: 2023-11-09 09:36:49+00:00
  edited: false
  hidden: false
  id: 654ca831d5c6faa9e380b76a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Babelscape/mrebel-large
repo_type: model
status: open
target_branch: null
title: Fine-tuning mrebel-large for named graphs/quads
