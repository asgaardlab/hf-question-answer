!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vijay-jaisankar
conflicting_files: null
created_at: 2023-12-05 18:59:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e169f40c0684269ae38a31abad1475d1.svg
      fullname: Jaisankar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vijay-jaisankar
      type: user
    createdAt: '2023-12-05T18:59:33.000Z'
    data:
      edited: false
      editors:
      - vijay-jaisankar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8429709672927856
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e169f40c0684269ae38a31abad1475d1.svg
          fullname: Jaisankar
          isHf: false
          isPro: false
          name: vijay-jaisankar
          type: user
        html: '<p>Hello, thanks for releasing your models.</p>

          <p>Is there any way to extract <em>just</em> the image features for a given
          image? If so, kindly point me to resources of the same.</p>

          <p>Thank you! </p>

          '
        raw: "Hello, thanks for releasing your models.\r\n\r\nIs there any way to\
          \ extract _just_ the image features for a given image? If so, kindly point\
          \ me to resources of the same.\r\n\r\nThank you! "
        updatedAt: '2023-12-05T18:59:33.549Z'
      numEdits: 0
      reactions: []
    id: 656f7315ae61260b610ef28b
    type: comment
  author: vijay-jaisankar
  content: "Hello, thanks for releasing your models.\r\n\r\nIs there any way to extract\
    \ _just_ the image features for a given image? If so, kindly point me to resources\
    \ of the same.\r\n\r\nThank you! "
  created_at: 2023-12-05 18:59:33+00:00
  edited: false
  hidden: false
  id: 656f7315ae61260b610ef28b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2023-12-05T21:20:14.000Z'
    data:
      edited: false
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6933066844940186
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;vijay-jaisankar&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/vijay-jaisankar\"\
          >@<span class=\"underline\">vijay-jaisankar</span></a></span>\n\n\t</span></span>\
          \ it's in the code snippet on front-page of OpenCLIP <a rel=\"nofollow\"\
          \ href=\"https://github.com/mlfoundations/open_clip\">https://github.com/mlfoundations/open_clip</a>,\
          \ <code>image_features = model.encode_image(image)</code></p>\n<pre><code>import\
          \ torch\nfrom PIL import Image\nimport open_clip\n\nmodel, _, preprocess\
          \ = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n\
          tokenizer = open_clip.get_tokenizer('ViT-B-32')\n\nimage = preprocess(Image.open(\"\
          CLIP.png\")).unsqueeze(0)\ntext = tokenizer([\"a diagram\", \"a dog\", \"\
          a cat\"])\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features\
          \ = model.encode_image(image)\n    text_features = model.encode_text(text)\n\
          \    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features\
          \ /= text_features.norm(dim=-1, keepdim=True)\n\n    text_probs = (100.0\
          \ * image_features @ text_features.T).softmax(dim=-1)\n\nprint(\"Label probs:\"\
          , text_probs)  # prints: [[1., 0., 0.]]\n</code></pre>\n<p>See also, <a\
          \ rel=\"nofollow\" href=\"https://github.com/mlfoundations/open_clip/discussions/721\"\
          >https://github.com/mlfoundations/open_clip/discussions/721</a></p>\n<p>This\
          \ model can also be loaded in transformers and there's ways of getting the\
          \ image features there.</p>\n<p>Additionally can be loaded in timm and used\
          \ to get features  <code>timm.create_model('vit_huge_patch14_clip_224.laion2b',\
          \ pretrained=True)</code></p>\n"
        raw: "@vijay-jaisankar it's in the code snippet on front-page of OpenCLIP\
          \ https://github.com/mlfoundations/open_clip, `image_features = model.encode_image(image)`\n\
          \n```\nimport torch\nfrom PIL import Image\nimport open_clip\n\nmodel, _,\
          \ preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n\
          tokenizer = open_clip.get_tokenizer('ViT-B-32')\n\nimage = preprocess(Image.open(\"\
          CLIP.png\")).unsqueeze(0)\ntext = tokenizer([\"a diagram\", \"a dog\", \"\
          a cat\"])\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features\
          \ = model.encode_image(image)\n    text_features = model.encode_text(text)\n\
          \    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features\
          \ /= text_features.norm(dim=-1, keepdim=True)\n\n    text_probs = (100.0\
          \ * image_features @ text_features.T).softmax(dim=-1)\n\nprint(\"Label probs:\"\
          , text_probs)  # prints: [[1., 0., 0.]]\n```\n\nSee also, https://github.com/mlfoundations/open_clip/discussions/721\n\
          \nThis model can also be loaded in transformers and there's ways of getting\
          \ the image features there.\n\nAdditionally can be loaded in timm and used\
          \ to get features  `timm.create_model('vit_huge_patch14_clip_224.laion2b',\
          \ pretrained=True)`"
        updatedAt: '2023-12-05T21:20:14.045Z'
      numEdits: 0
      reactions: []
    id: 656f940eb7b6010db391b839
    type: comment
  author: rwightman
  content: "@vijay-jaisankar it's in the code snippet on front-page of OpenCLIP https://github.com/mlfoundations/open_clip,\
    \ `image_features = model.encode_image(image)`\n\n```\nimport torch\nfrom PIL\
    \ import Image\nimport open_clip\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32',\
    \ pretrained='laion2b_s34b_b79k')\ntokenizer = open_clip.get_tokenizer('ViT-B-32')\n\
    \nimage = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0)\ntext = tokenizer([\"\
    a diagram\", \"a dog\", \"a cat\"])\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n\
    \    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n\
    \    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features\
    \ /= text_features.norm(dim=-1, keepdim=True)\n\n    text_probs = (100.0 * image_features\
    \ @ text_features.T).softmax(dim=-1)\n\nprint(\"Label probs:\", text_probs)  #\
    \ prints: [[1., 0., 0.]]\n```\n\nSee also, https://github.com/mlfoundations/open_clip/discussions/721\n\
    \nThis model can also be loaded in transformers and there's ways of getting the\
    \ image features there.\n\nAdditionally can be loaded in timm and used to get\
    \ features  `timm.create_model('vit_huge_patch14_clip_224.laion2b', pretrained=True)`"
  created_at: 2023-12-05 21:20:14+00:00
  edited: false
  hidden: false
  id: 656f940eb7b6010db391b839
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e169f40c0684269ae38a31abad1475d1.svg
      fullname: Jaisankar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vijay-jaisankar
      type: user
    createdAt: '2024-01-12T08:37:25.000Z'
    data:
      edited: false
      editors:
      - vijay-jaisankar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9884105324745178
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e169f40c0684269ae38a31abad1475d1.svg
          fullname: Jaisankar
          isHf: false
          isPro: false
          name: vijay-jaisankar
          type: user
        html: "<p>Thanks very much <span data-props=\"{&quot;user&quot;:&quot;rwightman&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rwightman\"\
          >@<span class=\"underline\">rwightman</span></a></span>\n\n\t</span></span>!\
          \ This method worked and I am able to get the image features :)</p>\n"
        raw: Thanks very much @rwightman! This method worked and I am able to get
          the image features :)
        updatedAt: '2024-01-12T08:37:25.262Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65a0fa45628cd4ce2f516b4f
    id: 65a0fa45628cd4ce2f516b4a
    type: comment
  author: vijay-jaisankar
  content: Thanks very much @rwightman! This method worked and I am able to get the
    image features :)
  created_at: 2024-01-12 08:37:25+00:00
  edited: false
  hidden: false
  id: 65a0fa45628cd4ce2f516b4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e169f40c0684269ae38a31abad1475d1.svg
      fullname: Jaisankar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vijay-jaisankar
      type: user
    createdAt: '2024-01-12T08:37:25.000Z'
    data:
      status: closed
    id: 65a0fa45628cd4ce2f516b4f
    type: status-change
  author: vijay-jaisankar
  created_at: 2024-01-12 08:37:25+00:00
  id: 65a0fa45628cd4ce2f516b4f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: laion/CLIP-ViT-H-14-laion2B-s32B-b79K
repo_type: model
status: closed
target_branch: null
title: How to extract just the image features?
