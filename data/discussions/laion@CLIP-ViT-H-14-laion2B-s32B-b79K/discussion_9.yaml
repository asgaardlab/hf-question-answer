!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Chanuhf
conflicting_files: null
created_at: 2023-09-19 00:06:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f1d4e375abd87a512324282f3b28e022.svg
      fullname: Chanakya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Chanuhf
      type: user
    createdAt: '2023-09-19T01:06:44.000Z'
    data:
      edited: false
      editors:
      - Chanuhf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5491673946380615
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f1d4e375abd87a512324282f3b28e022.svg
          fullname: Chanakya
          isHf: false
          isPro: false
          name: Chanuhf
          type: user
        html: '<p>I''ve loaded the pre-trained CLIP model variant <a href="https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K"><code>ViT-H-14</code></a>
          using <code>open_clip_torch</code>. While I can get the tokenizer with <code>open_clip.get_tokenizer(''ViT-H-14'')</code>,
          I''m unsure how to extract the <code>text_encoder</code>.</p>

          <p>Can anyone guide me on obtaining the <code>text_encoder</code> from this
          model?</p>

          <p>For example:</p>

          <pre><code>!pip install transformers

          from transformers import CLIPTextModel, CLIPTokenizer


          tokenizer = CLIPTokenizer.from_pretrained(''openai/clip-vit-large-patch14'')

          text_encoder = CLIPTextModel.from_pretrained(''openai/clip-vit-large-patch14'')

          </code></pre>

          <p>Expecting:</p>

          <pre><code>!pip install open_clip_torch


          model, train_transform, eval_transform = open_clip.create_model_and_transforms(''ViT-H-14'',pretrained=''laion2b_s32b_b79k'')

          tokenizer = open_clip.get_tokenizer(''ViT-H-14'')

          text_encoder = ________________________________

          </code></pre>

          '
        raw: "I've loaded the pre-trained CLIP model variant [`ViT-H-14`](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K)\
          \ using `open_clip_torch`. While I can get the tokenizer with `open_clip.get_tokenizer('ViT-H-14')`,\
          \ I'm unsure how to extract the `text_encoder`.\r\n\r\nCan anyone guide\
          \ me on obtaining the `text_encoder` from this model?\r\n\r\nFor example:\r\
          \n\r\n    !pip install transformers\r\n    from transformers import CLIPTextModel,\
          \ CLIPTokenizer\r\n    \r\n    tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\r\
          \n    text_encoder = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14')\r\
          \n\r\nExpecting:\r\n\r\n    !pip install open_clip_torch\r\n\r\n    model,\
          \ train_transform, eval_transform = open_clip.create_model_and_transforms('ViT-H-14',pretrained='laion2b_s32b_b79k')\r\
          \n    tokenizer = open_clip.get_tokenizer('ViT-H-14')\r\n    text_encoder\
          \ = ________________________________"
        updatedAt: '2023-09-19T01:06:44.234Z'
      numEdits: 0
      reactions: []
    id: 6508f4247e0d56c27152feda
    type: comment
  author: Chanuhf
  content: "I've loaded the pre-trained CLIP model variant [`ViT-H-14`](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K)\
    \ using `open_clip_torch`. While I can get the tokenizer with `open_clip.get_tokenizer('ViT-H-14')`,\
    \ I'm unsure how to extract the `text_encoder`.\r\n\r\nCan anyone guide me on\
    \ obtaining the `text_encoder` from this model?\r\n\r\nFor example:\r\n\r\n  \
    \  !pip install transformers\r\n    from transformers import CLIPTextModel, CLIPTokenizer\r\
    \n    \r\n    tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\r\
    \n    text_encoder = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14')\r\
    \n\r\nExpecting:\r\n\r\n    !pip install open_clip_torch\r\n\r\n    model, train_transform,\
    \ eval_transform = open_clip.create_model_and_transforms('ViT-H-14',pretrained='laion2b_s32b_b79k')\r\
    \n    tokenizer = open_clip.get_tokenizer('ViT-H-14')\r\n    text_encoder = ________________________________"
  created_at: 2023-09-19 00:06:44+00:00
  edited: false
  hidden: false
  id: 6508f4247e0d56c27152feda
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2023-09-20T17:35:22.000Z'
    data:
      edited: false
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7114703059196472
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Chanuhf&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Chanuhf\">@<span class=\"\
          underline\">Chanuhf</span></a></span>\n\n\t</span></span> there is no method\
          \ that creates the text or image encoder by themselves, but it's easy enough\
          \ to encode just text (or images), or extract either tower, to extract text\
          \ tower you want to set the custom text flag so all of the text bits are\
          \ pushed into their own sub-module</p>\n<pre><code>model, train_transform,\
          \ eval_transform = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k',\
          \ force_custom_text=True)\ntokenizer = open_clip.get_tokenizer('ViT-H-14')\n\
          text_encoder = model.text\ndel model\nx = text(input)\nx = F.normalize(x,\
          \ dim=-1)  # if normalized output desired\n</code></pre>\n<pre><code>model,\
          \ train_transform, eval_transform = open_clip.create_model_and_transforms('ViT-H-14',\
          \ pretrained='laion2b_s32b_b79k')\ntokenizer = open_clip.get_tokenizer('ViT-H-14')\n\
          model.encode_text(text_inputs)\n</code></pre>\n"
        raw: '@Chanuhf there is no method that creates the text or image encoder by
          themselves, but it''s easy enough to encode just text (or images), or extract
          either tower, to extract text tower you want to set the custom text flag
          so all of the text bits are pushed into their own sub-module


          ```

          model, train_transform, eval_transform = open_clip.create_model_and_transforms(''ViT-H-14'',
          pretrained=''laion2b_s32b_b79k'', force_custom_text=True)

          tokenizer = open_clip.get_tokenizer(''ViT-H-14'')

          text_encoder = model.text

          del model

          x = text(input)

          x = F.normalize(x, dim=-1)  # if normalized output desired

          ```


          ```

          model, train_transform, eval_transform = open_clip.create_model_and_transforms(''ViT-H-14'',
          pretrained=''laion2b_s32b_b79k'')

          tokenizer = open_clip.get_tokenizer(''ViT-H-14'')

          model.encode_text(text_inputs)

          ```


          '
        updatedAt: '2023-09-20T17:35:22.064Z'
      numEdits: 0
      reactions: []
    id: 650b2d5aa27ba14323dc8b8d
    type: comment
  author: rwightman
  content: '@Chanuhf there is no method that creates the text or image encoder by
    themselves, but it''s easy enough to encode just text (or images), or extract
    either tower, to extract text tower you want to set the custom text flag so all
    of the text bits are pushed into their own sub-module


    ```

    model, train_transform, eval_transform = open_clip.create_model_and_transforms(''ViT-H-14'',
    pretrained=''laion2b_s32b_b79k'', force_custom_text=True)

    tokenizer = open_clip.get_tokenizer(''ViT-H-14'')

    text_encoder = model.text

    del model

    x = text(input)

    x = F.normalize(x, dim=-1)  # if normalized output desired

    ```


    ```

    model, train_transform, eval_transform = open_clip.create_model_and_transforms(''ViT-H-14'',
    pretrained=''laion2b_s32b_b79k'')

    tokenizer = open_clip.get_tokenizer(''ViT-H-14'')

    model.encode_text(text_inputs)

    ```


    '
  created_at: 2023-09-20 16:35:22+00:00
  edited: false
  hidden: false
  id: 650b2d5aa27ba14323dc8b8d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: laion/CLIP-ViT-H-14-laion2B-s32B-b79K
repo_type: model
status: open
target_branch: null
title: Extracting `text_encoder` from `ViT-H-14` using `open_clip_torch`?
