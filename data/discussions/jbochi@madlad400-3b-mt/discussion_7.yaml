!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Runo888
conflicting_files: null
created_at: 2023-11-18 17:51:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24d46fe83296bce27859bdd182da885c.svg
      fullname: R.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Runo888
      type: user
    createdAt: '2023-11-18T17:51:32.000Z'
    data:
      edited: false
      editors:
      - Runo888
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9831184148788452
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24d46fe83296bce27859bdd182da885c.svg
          fullname: R.
          isHf: false
          isPro: false
          name: Runo888
          type: user
        html: '<p>Will you create a ggml version of the larger 7B and 10B versions
          like you did with the 3B version here? I''d try converting it myself but
          I can''t figure out how to do it.</p>

          '
        raw: Will you create a ggml version of the larger 7B and 10B versions like
          you did with the 3B version here? I'd try converting it myself but I can't
          figure out how to do it.
        updatedAt: '2023-11-18T17:51:32.566Z'
      numEdits: 0
      reactions: []
    id: 6558f9a4029b03128bd7663f
    type: comment
  author: Runo888
  content: Will you create a ggml version of the larger 7B and 10B versions like you
    did with the 3B version here? I'd try converting it myself but I can't figure
    out how to do it.
  created_at: 2023-11-18 17:51:32+00:00
  edited: false
  hidden: false
  id: 6558f9a4029b03128bd7663f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-11-19T14:12:16.000Z'
    data:
      edited: false
      editors:
      - jbochi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5471354126930237
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
          fullname: J Bochi
          isHf: false
          isPro: false
          name: jbochi
          type: user
        html: "<p>I used this candle tool to generate the quantized files: <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/candle/blob/main/candle-examples/examples/quantized-t5/README.md#generating-quantized-weight-files\"\
          >https://github.com/huggingface/candle/blob/main/candle-examples/examples/quantized-t5/README.md#generating-quantized-weight-files</a></p>\n\
          <p>Here are all the commands to run it in a Google Colab (in a high-ram\
          \ instance):</p>\n<pre><code>! wget https://static.rust-lang.org/rustup/dist/x86_64-unknown-linux-gnu/rustup-init\n\
          ! chmod a+x rustup-init\n! ./rustup-init -y\n\nimport os\nos.environ['PATH']\
          \ += ':/root/.cargo/bin'\n\n! rustup toolchain install nightly --component\
          \ rust-src\n!git clone https://github.com/huggingface/candle\n\nmodel_name\
          \ = \"madlad400-7b-mt\"\n\n! git lfs install\n! git clone https://huggingface.co/jbochi/{model_name}\n\
          \nfiles = \" \".join([\n    f\"/content/{model_name}/{f}\"\n    for f in\
          \ os.listdir(model_name) if f.endswith(\".safetensors\")])\n\nquantization_format\
          \ = \"q4k\"\n\n!cd candle; cargo run --example tensor-tools --release --\
          \ \\\n  quantize --quantization {quantization_format} \\\n  {files} \\\n\
          \  --out-file ../{model_name}/model-{quantization_format}.gguf\n\n!ls {model_name}/*.gguf\n\
          </code></pre>\n<p>I'll try to upload some later.</p>\n"
        raw: "I used this candle tool to generate the quantized files: https://github.com/huggingface/candle/blob/main/candle-examples/examples/quantized-t5/README.md#generating-quantized-weight-files\n\
          \nHere are all the commands to run it in a Google Colab (in a high-ram instance):\n\
          \n```\n! wget https://static.rust-lang.org/rustup/dist/x86_64-unknown-linux-gnu/rustup-init\n\
          ! chmod a+x rustup-init\n! ./rustup-init -y\n\nimport os\nos.environ['PATH']\
          \ += ':/root/.cargo/bin'\n\n! rustup toolchain install nightly --component\
          \ rust-src\n!git clone https://github.com/huggingface/candle\n\nmodel_name\
          \ = \"madlad400-7b-mt\"\n\n! git lfs install\n! git clone https://huggingface.co/jbochi/{model_name}\n\
          \nfiles = \" \".join([\n    f\"/content/{model_name}/{f}\"\n    for f in\
          \ os.listdir(model_name) if f.endswith(\".safetensors\")])\n\nquantization_format\
          \ = \"q4k\"\n\n!cd candle; cargo run --example tensor-tools --release --\
          \ \\\n  quantize --quantization {quantization_format} \\\n  {files} \\\n\
          \  --out-file ../{model_name}/model-{quantization_format}.gguf\n\n!ls {model_name}/*.gguf\n\
          ```\n\nI'll try to upload some later."
        updatedAt: '2023-11-19T14:12:16.297Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Runo888
    id: 655a17c004a63a0dfb0807ef
    type: comment
  author: jbochi
  content: "I used this candle tool to generate the quantized files: https://github.com/huggingface/candle/blob/main/candle-examples/examples/quantized-t5/README.md#generating-quantized-weight-files\n\
    \nHere are all the commands to run it in a Google Colab (in a high-ram instance):\n\
    \n```\n! wget https://static.rust-lang.org/rustup/dist/x86_64-unknown-linux-gnu/rustup-init\n\
    ! chmod a+x rustup-init\n! ./rustup-init -y\n\nimport os\nos.environ['PATH'] +=\
    \ ':/root/.cargo/bin'\n\n! rustup toolchain install nightly --component rust-src\n\
    !git clone https://github.com/huggingface/candle\n\nmodel_name = \"madlad400-7b-mt\"\
    \n\n! git lfs install\n! git clone https://huggingface.co/jbochi/{model_name}\n\
    \nfiles = \" \".join([\n    f\"/content/{model_name}/{f}\"\n    for f in os.listdir(model_name)\
    \ if f.endswith(\".safetensors\")])\n\nquantization_format = \"q4k\"\n\n!cd candle;\
    \ cargo run --example tensor-tools --release -- \\\n  quantize --quantization\
    \ {quantization_format} \\\n  {files} \\\n  --out-file ../{model_name}/model-{quantization_format}.gguf\n\
    \n!ls {model_name}/*.gguf\n```\n\nI'll try to upload some later."
  created_at: 2023-11-19 14:12:16+00:00
  edited: false
  hidden: false
  id: 655a17c004a63a0dfb0807ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-11-19T15:47:28.000Z'
    data:
      edited: false
      editors:
      - jbochi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9698687791824341
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
          fullname: J Bochi
          isHf: false
          isPro: false
          name: jbochi
          type: user
        html: '<p>I uploaded q4k and q6k weights for the 7b and 10b models. Let me
          know if you are interested in a specific quantization type and model size
          that''s missing.</p>

          '
        raw: I uploaded q4k and q6k weights for the 7b and 10b models. Let me know
          if you are interested in a specific quantization type and model size that's
          missing.
        updatedAt: '2023-11-19T15:47:28.443Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Runo888
      relatedEventId: 655a2e1027902877025b40a0
    id: 655a2e1027902877025b409c
    type: comment
  author: jbochi
  content: I uploaded q4k and q6k weights for the 7b and 10b models. Let me know if
    you are interested in a specific quantization type and model size that's missing.
  created_at: 2023-11-19 15:47:28+00:00
  edited: false
  hidden: false
  id: 655a2e1027902877025b409c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-11-19T15:47:28.000Z'
    data:
      status: closed
    id: 655a2e1027902877025b40a0
    type: status-change
  author: jbochi
  created_at: 2023-11-19 15:47:28+00:00
  id: 655a2e1027902877025b40a0
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24d46fe83296bce27859bdd182da885c.svg
      fullname: R.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Runo888
      type: user
    createdAt: '2023-11-19T17:04:33.000Z'
    data:
      edited: false
      editors:
      - Runo888
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9245143532752991
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24d46fe83296bce27859bdd182da885c.svg
          fullname: R.
          isHf: false
          isPro: false
          name: Runo888
          type: user
        html: '<p>Thanks man, you''re a hero!</p>

          '
        raw: Thanks man, you're a hero!
        updatedAt: '2023-11-19T17:04:33.024Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - jbochi
    id: 655a40213fc7998474941cea
    type: comment
  author: Runo888
  content: Thanks man, you're a hero!
  created_at: 2023-11-19 17:04:33+00:00
  edited: false
  hidden: false
  id: 655a40213fc7998474941cea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ca2d750fb67cc848dc07a9161bfa9dd.svg
      fullname: Martin Viewegger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Viewegger
      type: user
    createdAt: '2023-12-03T22:13:46.000Z'
    data:
      edited: false
      editors:
      - Viewegger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5297985076904297
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ca2d750fb67cc848dc07a9161bfa9dd.svg
          fullname: Martin Viewegger
          isHf: false
          isPro: false
          name: Viewegger
          type: user
        html: '<p>Hello, that you for quatization! Could you maybe show example/modified
          version of the original code bellow, to use the 4bit quatized version?</p>

          <p>from transformers import T5ForConditionalGeneration, T5Tokenizer</p>

          <p>model_name = ''jbochi/madlad400-10b-mt''<br>model = T5ForConditionalGeneration.from_pretrained(model_name,
          device_map="auto")<br>tokenizer = T5Tokenizer.from_pretrained(model_name)</p>

          <p>text = "&lt;2pt&gt; I love pizza!"<br>input_ids = tokenizer(text, return_tensors="pt").input_ids.to(model.device)<br>outputs
          = model.generate(input_ids=input_ids)</p>

          <p>tokenizer.decode(outputs[0], skip_special_tokens=True)</p>

          <h1 id="eu-adoro-pizza">Eu adoro pizza!</h1>

          '
        raw: 'Hello, that you for quatization! Could you maybe show example/modified
          version of the original code bellow, to use the 4bit quatized version?


          from transformers import T5ForConditionalGeneration, T5Tokenizer


          model_name = ''jbochi/madlad400-10b-mt''

          model = T5ForConditionalGeneration.from_pretrained(model_name, device_map="auto")

          tokenizer = T5Tokenizer.from_pretrained(model_name)


          text = "<2pt> I love pizza!"

          input_ids = tokenizer(text, return_tensors="pt").input_ids.to(model.device)

          outputs = model.generate(input_ids=input_ids)


          tokenizer.decode(outputs[0], skip_special_tokens=True)

          # Eu adoro pizza!

          '
        updatedAt: '2023-12-03T22:13:46.385Z'
      numEdits: 0
      reactions: []
    id: 656cfd9a9496f21be8961706
    type: comment
  author: Viewegger
  content: 'Hello, that you for quatization! Could you maybe show example/modified
    version of the original code bellow, to use the 4bit quatized version?


    from transformers import T5ForConditionalGeneration, T5Tokenizer


    model_name = ''jbochi/madlad400-10b-mt''

    model = T5ForConditionalGeneration.from_pretrained(model_name, device_map="auto")

    tokenizer = T5Tokenizer.from_pretrained(model_name)


    text = "<2pt> I love pizza!"

    input_ids = tokenizer(text, return_tensors="pt").input_ids.to(model.device)

    outputs = model.generate(input_ids=input_ids)


    tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Eu adoro pizza!

    '
  created_at: 2023-12-03 22:13:46+00:00
  edited: false
  hidden: false
  id: 656cfd9a9496f21be8961706
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-12-04T13:21:34.000Z'
    data:
      edited: false
      editors:
      - jbochi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.604404628276825
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
          fullname: J Bochi
          isHf: false
          isPro: false
          name: jbochi
          type: user
        html: "<p>Hello,</p>\n<p>I'm unsure if you can load the weights directly from\
          \ the GGUF files with HF transformers, sorry.  I have only tested them with\
          \ candle:</p>\n<pre><code>cargo run --example quantized-t5 --release  --\
          \ \\\n  --model-id \"jbochi/madlad400-3b-mt\" --weight-file \"model-q4k.gguf\"\
          \ \\\n  --prompt \"&lt;2de&gt; How are you, my friend?\" \\\n  --temperature\
          \ 0\n</code></pre>\n"
        raw: "Hello,\n\nI'm unsure if you can load the weights directly from the GGUF\
          \ files with HF transformers, sorry.  I have only tested them with candle:\n\
          \n```\ncargo run --example quantized-t5 --release  -- \\\n  --model-id \"\
          jbochi/madlad400-3b-mt\" --weight-file \"model-q4k.gguf\" \\\n  --prompt\
          \ \"<2de> How are you, my friend?\" \\\n  --temperature 0\n```\n"
        updatedAt: '2023-12-04T13:21:34.099Z'
      numEdits: 0
      reactions: []
    id: 656dd25eef1e27e9e34ee17e
    type: comment
  author: jbochi
  content: "Hello,\n\nI'm unsure if you can load the weights directly from the GGUF\
    \ files with HF transformers, sorry.  I have only tested them with candle:\n\n\
    ```\ncargo run --example quantized-t5 --release  -- \\\n  --model-id \"jbochi/madlad400-3b-mt\"\
    \ --weight-file \"model-q4k.gguf\" \\\n  --prompt \"<2de> How are you, my friend?\"\
    \ \\\n  --temperature 0\n```\n"
  created_at: 2023-12-04 13:21:34+00:00
  edited: false
  hidden: false
  id: 656dd25eef1e27e9e34ee17e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ca2d750fb67cc848dc07a9161bfa9dd.svg
      fullname: Martin Viewegger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Viewegger
      type: user
    createdAt: '2023-12-04T16:49:03.000Z'
    data:
      edited: false
      editors:
      - Viewegger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8158996105194092
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ca2d750fb67cc848dc07a9161bfa9dd.svg
          fullname: Martin Viewegger
          isHf: false
          isPro: false
          name: Viewegger
          type: user
        html: '<p>Thank you, sorry for bothering you, but if I am not wrong this would
          load and reload the model again and again with each request,  I have no
          experience with cargo, but is it possible to make interface that can run
          iteratively, like running the following code repeatedly in the jupyter cell
          for example. </p>

          <p>text = "&lt;2pt&gt; I love pizza!"<br>input_ids = tokenizer(text, return_tensors="pt").input_ids.to(model.device)<br>outputs
          = model.generate(input_ids=input_ids)</p>

          <p>tokenizer.decode(outputs[0], skip_special_tokens=True) </p>

          <p>Again thank you for your time, I should probably just google it...</p>

          '
        raw: "Thank you, sorry for bothering you, but if I am not wrong this would\
          \ load and reload the model again and again with each request,  I have no\
          \ experience with cargo, but is it possible to make interface that can run\
          \ iteratively, like running the following code repeatedly in the jupyter\
          \ cell for example. \n\ntext = \"<2pt> I love pizza!\"\ninput_ids = tokenizer(text,\
          \ return_tensors=\"pt\").input_ids.to(model.device)\noutputs = model.generate(input_ids=input_ids)\n\
          \ntokenizer.decode(outputs[0], skip_special_tokens=True) \n\nAgain thank\
          \ you for your time, I should probably just google it..."
        updatedAt: '2023-12-04T16:49:03.147Z'
      numEdits: 0
      reactions: []
    id: 656e02ff72c19de723bd9b9e
    type: comment
  author: Viewegger
  content: "Thank you, sorry for bothering you, but if I am not wrong this would load\
    \ and reload the model again and again with each request,  I have no experience\
    \ with cargo, but is it possible to make interface that can run iteratively, like\
    \ running the following code repeatedly in the jupyter cell for example. \n\n\
    text = \"<2pt> I love pizza!\"\ninput_ids = tokenizer(text, return_tensors=\"\
    pt\").input_ids.to(model.device)\noutputs = model.generate(input_ids=input_ids)\n\
    \ntokenizer.decode(outputs[0], skip_special_tokens=True) \n\nAgain thank you for\
    \ your time, I should probably just google it..."
  created_at: 2023-12-04 16:49:03+00:00
  edited: false
  hidden: false
  id: 656e02ff72c19de723bd9b9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-12-05T16:17:31.000Z'
    data:
      edited: false
      editors:
      - jbochi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8688651323318481
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
          fullname: J Bochi
          isHf: false
          isPro: false
          name: jbochi
          type: user
        html: '<p>You can definitely do that with candle. You just need to modify
          the t5 example and reuse the <a rel="nofollow" href="https://github.com/huggingface/candle/blob/2648e797c22ea9a25dd43cf6ecf0222dfd221170/candle-examples/examples/quantized-t5/main.rs#L175">model
          once it''s initialized</a>.</p>

          <p>Loading the model is surprisingly relatively cheap when the sentences
          are long, so it''s possible the speed up is not going to be that big.</p>

          '
        raw: 'You can definitely do that with candle. You just need to modify the
          t5 example and reuse the [model once it''s initialized](https://github.com/huggingface/candle/blob/2648e797c22ea9a25dd43cf6ecf0222dfd221170/candle-examples/examples/quantized-t5/main.rs#L175).


          Loading the model is surprisingly relatively cheap when the sentences are
          long, so it''s possible the speed up is not going to be that big.'
        updatedAt: '2023-12-05T16:17:31.657Z'
      numEdits: 0
      reactions: []
    id: 656f4d1b1351900344e7853c
    type: comment
  author: jbochi
  content: 'You can definitely do that with candle. You just need to modify the t5
    example and reuse the [model once it''s initialized](https://github.com/huggingface/candle/blob/2648e797c22ea9a25dd43cf6ecf0222dfd221170/candle-examples/examples/quantized-t5/main.rs#L175).


    Loading the model is surprisingly relatively cheap when the sentences are long,
    so it''s possible the speed up is not going to be that big.'
  created_at: 2023-12-05 16:17:31+00:00
  edited: false
  hidden: false
  id: 656f4d1b1351900344e7853c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: jbochi/madlad400-3b-mt
repo_type: model
status: closed
target_branch: null
title: Quantized model of the larger versions?
