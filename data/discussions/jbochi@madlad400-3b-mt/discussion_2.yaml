!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jisx
conflicting_files: null
created_at: 2023-11-06 13:46:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png?w=200&h=200&f=face
      fullname: Shaoxiong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jisx
      type: user
    createdAt: '2023-11-06T13:46:44.000Z'
    data:
      edited: false
      editors:
      - jisx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6311744451522827
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png?w=200&h=200&f=face
          fullname: Shaoxiong
          isHf: false
          isPro: false
          name: jisx
          type: user
        html: '<p>Hi, do you plan to convert LM to HF format? </p>

          '
        raw: 'Hi, do you plan to convert LM to HF format? '
        updatedAt: '2023-11-06T13:46:44.682Z'
      numEdits: 0
      reactions: []
    id: 6548ee449a9e263e2b511337
    type: comment
  author: jisx
  content: 'Hi, do you plan to convert LM to HF format? '
  created_at: 2023-11-06 13:46:44+00:00
  edited: false
  hidden: false
  id: 6548ee449a9e263e2b511337
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-11-06T13:51:22.000Z'
    data:
      edited: false
      editors:
      - jbochi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9424344897270203
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
          fullname: J Bochi
          isHf: false
          isPro: false
          name: jbochi
          type: user
        html: '<p>Hey,</p>

          <p>I was planning to, but I just realized that apparently there''s no decoder
          only T5 in HF''s transformers:</p>

          <ul>

          <li><a rel="nofollow" href="https://discuss.huggingface.co/t/how-to-separately-use-t5-decoder/13536">https://discuss.huggingface.co/t/how-to-separately-use-t5-decoder/13536</a></li>

          <li><a rel="nofollow" href="https://github.com/huggingface/transformers/issues/26647">https://github.com/huggingface/transformers/issues/26647</a></li>

          </ul>

          <p>Is that true? We might need to implement it as well.</p>

          '
        raw: 'Hey,


          I was planning to, but I just realized that apparently there''s no decoder
          only T5 in HF''s transformers:

          - https://discuss.huggingface.co/t/how-to-separately-use-t5-decoder/13536

          - https://github.com/huggingface/transformers/issues/26647


          Is that true? We might need to implement it as well.

          '
        updatedAt: '2023-11-06T13:51:22.553Z'
      numEdits: 0
      reactions: []
    id: 6548ef5ad20091fdc4db86aa
    type: comment
  author: jbochi
  content: 'Hey,


    I was planning to, but I just realized that apparently there''s no decoder only
    T5 in HF''s transformers:

    - https://discuss.huggingface.co/t/how-to-separately-use-t5-decoder/13536

    - https://github.com/huggingface/transformers/issues/26647


    Is that true? We might need to implement it as well.

    '
  created_at: 2023-11-06 13:51:22+00:00
  edited: false
  hidden: false
  id: 6548ef5ad20091fdc4db86aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png?w=200&h=200&f=face
      fullname: Shaoxiong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jisx
      type: user
    createdAt: '2023-11-06T14:07:56.000Z'
    data:
      edited: false
      editors:
      - jisx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9857629537582397
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png?w=200&h=200&f=face
          fullname: Shaoxiong
          isHf: false
          isPro: false
          name: jisx
          type: user
        html: '<p>I was also trying to convert. But I didn''t find decoder-only T5
          (or decoder-only UL2) in HF. MADLAD paper said they use the same configuration
          as previous work [27, 52]. But I cannot find configs of these two works.
          </p>

          '
        raw: 'I was also trying to convert. But I didn''t find decoder-only T5 (or
          decoder-only UL2) in HF. MADLAD paper said they use the same configuration
          as previous work [27, 52]. But I cannot find configs of these two works. '
        updatedAt: '2023-11-06T14:07:56.470Z'
      numEdits: 0
      reactions: []
    id: 6548f33cc6cd77f0a7c9798a
    type: comment
  author: jisx
  content: 'I was also trying to convert. But I didn''t find decoder-only T5 (or decoder-only
    UL2) in HF. MADLAD paper said they use the same configuration as previous work
    [27, 52]. But I cannot find configs of these two works. '
  created_at: 2023-11-06 14:07:56+00:00
  edited: false
  hidden: false
  id: 6548f33cc6cd77f0a7c9798a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-11-06T14:14:21.000Z'
    data:
      edited: false
      editors:
      - jbochi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9992486834526062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
          fullname: J Bochi
          isHf: false
          isPro: false
          name: jbochi
          type: user
        html: '<p>I had a look at the code and I think I know what needs to be done.
          I will try it later today.</p>

          '
        raw: I had a look at the code and I think I know what needs to be done. I
          will try it later today.
        updatedAt: '2023-11-06T14:14:21.227Z'
      numEdits: 0
      reactions: []
    id: 6548f4bddd0fabf88298d468
    type: comment
  author: jbochi
  content: I had a look at the code and I think I know what needs to be done. I will
    try it later today.
  created_at: 2023-11-06 14:14:21+00:00
  edited: false
  hidden: false
  id: 6548f4bddd0fabf88298d468
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-11-06T23:17:48.000Z'
    data:
      edited: false
      editors:
      - jbochi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9195969700813293
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
          fullname: J Bochi
          isHf: false
          isPro: false
          name: jbochi
          type: user
        html: '<p><code>convert_t5x_checkpoint_to_pytorch.py</code> is crashing for
          me even on a machine with 128GB of RAM. It worked for the 10B param model,
          but somehow this smaller model requires more RAM.</p>

          <p>I can change the code to avoid putting it all in memory when I have the
          time, but it will take a bit longer.</p>

          '
        raw: '`convert_t5x_checkpoint_to_pytorch.py` is crashing for me even on a
          machine with 128GB of RAM. It worked for the 10B param model, but somehow
          this smaller model requires more RAM.


          I can change the code to avoid putting it all in memory when I have the
          time, but it will take a bit longer.


          '
        updatedAt: '2023-11-06T23:17:48.003Z'
      numEdits: 0
      reactions: []
    id: 6549741c137b501e315132aa
    type: comment
  author: jbochi
  content: '`convert_t5x_checkpoint_to_pytorch.py` is crashing for me even on a machine
    with 128GB of RAM. It worked for the 10B param model, but somehow this smaller
    model requires more RAM.


    I can change the code to avoid putting it all in memory when I have the time,
    but it will take a bit longer.


    '
  created_at: 2023-11-06 23:17:48+00:00
  edited: false
  hidden: false
  id: 6549741c137b501e315132aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-11-08T11:12:40.000Z'
    data:
      edited: true
      editors:
      - jbochi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.768473744392395
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
          fullname: J Bochi
          isHf: false
          isPro: false
          name: jbochi
          type: user
        html: '<p>Ok, so looking at the <a rel="nofollow" href="https://storage.googleapis.com/madlad-400-checkpoints/checkpoints/8b-lm/8b-lm.gin">gin
          file</a>, they used the <a rel="nofollow" href="https://github.com/google/flaxformer/blob/ea17eb012a1d340ddff017b7a534c2162aaec34c/flaxformer/architectures/t5/t5_architecture.py#L1484">DecoderOnly
          model from flaxformer</a>.</p>

          <p>I''ve converted the weights with this <a rel="nofollow" href="https://colab.research.google.com/drive/1OCbFZEdc-iQgVmQSPXELHxi4dC5QyuA_?usp=sharing">colab</a>,
          but we need the transformers code to use them.</p>

          <p>The weights are in <a href="https://huggingface.co/jbochi/madlad400-8b-lm">https://huggingface.co/jbochi/madlad400-8b-lm</a></p>

          <p>Some interesting things:</p>

          <ul>

          <li>There''s just one layer norm per block. so <a rel="nofollow" href="https://github.com/google/flaxformer/blob/ea17eb012a1d340ddff017b7a534c2162aaec34c/flaxformer/architectures/t5/t5_architecture.py#L534">parallel
          is true here</a> (<code>PARALLEL_LAYERS = True</code> in the gin file)</li>

          <li>There''s no relative position embeddings (disabled in the gin file:
          <code>t5_architecture.Decoder.shared_relative_position_bias_factory = None</code>)</li>

          <li>lm_head and token_embeddings have tied weights</li>

          </ul>

          <p>Just out of curiosity, how do you plan to use the model?</p>

          '
        raw: 'Ok, so looking at the [gin file](https://storage.googleapis.com/madlad-400-checkpoints/checkpoints/8b-lm/8b-lm.gin),
          they used the [DecoderOnly model from flaxformer](https://github.com/google/flaxformer/blob/ea17eb012a1d340ddff017b7a534c2162aaec34c/flaxformer/architectures/t5/t5_architecture.py#L1484).


          I''ve converted the weights with this [colab](https://colab.research.google.com/drive/1OCbFZEdc-iQgVmQSPXELHxi4dC5QyuA_?usp=sharing),
          but we need the transformers code to use them.


          The weights are in https://huggingface.co/jbochi/madlad400-8b-lm


          Some interesting things:

          - There''s just one layer norm per block. so [parallel is true here](https://github.com/google/flaxformer/blob/ea17eb012a1d340ddff017b7a534c2162aaec34c/flaxformer/architectures/t5/t5_architecture.py#L534)
          (`PARALLEL_LAYERS = True` in the gin file)

          - There''s no relative position embeddings (disabled in the gin file: `t5_architecture.Decoder.shared_relative_position_bias_factory
          = None`)

          - lm_head and token_embeddings have tied weights


          Just out of curiosity, how do you plan to use the model?

          '
        updatedAt: '2023-11-08T14:05:20.989Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dinhanhx
    id: 654b6d28eeb563c6c86a3536
    type: comment
  author: jbochi
  content: 'Ok, so looking at the [gin file](https://storage.googleapis.com/madlad-400-checkpoints/checkpoints/8b-lm/8b-lm.gin),
    they used the [DecoderOnly model from flaxformer](https://github.com/google/flaxformer/blob/ea17eb012a1d340ddff017b7a534c2162aaec34c/flaxformer/architectures/t5/t5_architecture.py#L1484).


    I''ve converted the weights with this [colab](https://colab.research.google.com/drive/1OCbFZEdc-iQgVmQSPXELHxi4dC5QyuA_?usp=sharing),
    but we need the transformers code to use them.


    The weights are in https://huggingface.co/jbochi/madlad400-8b-lm


    Some interesting things:

    - There''s just one layer norm per block. so [parallel is true here](https://github.com/google/flaxformer/blob/ea17eb012a1d340ddff017b7a534c2162aaec34c/flaxformer/architectures/t5/t5_architecture.py#L534)
    (`PARALLEL_LAYERS = True` in the gin file)

    - There''s no relative position embeddings (disabled in the gin file: `t5_architecture.Decoder.shared_relative_position_bias_factory
    = None`)

    - lm_head and token_embeddings have tied weights


    Just out of curiosity, how do you plan to use the model?

    '
  created_at: 2023-11-08 11:12:40+00:00
  edited: true
  hidden: false
  id: 654b6d28eeb563c6c86a3536
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png?w=200&h=200&f=face
      fullname: Shaoxiong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jisx
      type: user
    createdAt: '2023-11-08T18:09:16.000Z'
    data:
      edited: false
      editors:
      - jisx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9530837535858154
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png?w=200&h=200&f=face
          fullname: Shaoxiong
          isHf: false
          isPro: false
          name: jisx
          type: user
        html: '<p>Interesting! thanks for converting!<br>I''m interested in its downstream
          performance on low-resource langauges. </p>

          '
        raw: "Interesting! thanks for converting! \nI'm interested in its downstream\
          \ performance on low-resource langauges. "
        updatedAt: '2023-11-08T18:09:16.635Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jbochi
    id: 654bcecc15149d4fd69fbad7
    type: comment
  author: jisx
  content: "Interesting! thanks for converting! \nI'm interested in its downstream\
    \ performance on low-resource langauges. "
  created_at: 2023-11-08 18:09:16+00:00
  edited: false
  hidden: false
  id: 654bcecc15149d4fd69fbad7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png?w=200&h=200&f=face
      fullname: Shaoxiong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jisx
      type: user
    createdAt: '2023-11-08T18:14:46.000Z'
    data:
      status: closed
    id: 654bd016be11400417ba2414
    type: status-change
  author: jisx
  created_at: 2023-11-08 18:14:46+00:00
  id: 654bd016be11400417ba2414
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: jbochi/madlad400-3b-mt
repo_type: model
status: closed
target_branch: null
title: converting LM
