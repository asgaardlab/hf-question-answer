!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cmp-nct
conflicting_files: null
created_at: 2023-12-04 00:00:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-12-04T00:00:55.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9441786408424377
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>Hi,<br>I noticed the gguf you provided, I''m not sure how you converted
          them but they are not valid gguf binaries and llama.cpp sadly doesn''t support
          the architecture</p>

          '
        raw: "Hi,\r\nI noticed the gguf you provided, I'm not sure how you converted\
          \ them but they are not valid gguf binaries and llama.cpp sadly doesn't\
          \ support the architecture"
        updatedAt: '2023-12-04T00:00:55.259Z'
      numEdits: 0
      reactions: []
    id: 656d16b73dbac3a83d330f55
    type: comment
  author: cmp-nct
  content: "Hi,\r\nI noticed the gguf you provided, I'm not sure how you converted\
    \ them but they are not valid gguf binaries and llama.cpp sadly doesn't support\
    \ the architecture"
  created_at: 2023-12-04 00:00:55+00:00
  edited: false
  hidden: false
  id: 656d16b73dbac3a83d330f55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-12-04T13:19:33.000Z'
    data:
      edited: false
      editors:
      - jbochi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6434589624404907
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
          fullname: J Bochi
          isHf: false
          isPro: false
          name: jbochi
          type: user
        html: "<p>Hello,</p>\n<p>I believe you are right that llama.cpp does not support\
          \ T5 models, but the binaries are not invalid. You can use them with candle:</p>\n\
          <pre><code>cargo run --example quantized-t5 --release  -- \\\n  --model-id\
          \ \"jbochi/madlad400-3b-mt\" --weight-file \"model-q4k.gguf\" \\\n  --prompt\
          \ \"&lt;2de&gt; How are you, my friend?\" \\\n  --temperature 0\n</code></pre>\n\
          <p>I generated the files using the util described in the <a rel=\"nofollow\"\
          \ href=\"https://github.com/huggingface/candle/tree/main/candle-examples/examples/quantized-t5\"\
          >quantized-t5 example</a></p>\n"
        raw: "Hello,\n\nI believe you are right that llama.cpp does not support T5\
          \ models, but the binaries are not invalid. You can use them with candle:\n\
          \n```\ncargo run --example quantized-t5 --release  -- \\\n  --model-id \"\
          jbochi/madlad400-3b-mt\" --weight-file \"model-q4k.gguf\" \\\n  --prompt\
          \ \"<2de> How are you, my friend?\" \\\n  --temperature 0\n```\n\nI generated\
          \ the files using the util described in the [quantized-t5 example](https://github.com/huggingface/candle/tree/main/candle-examples/examples/quantized-t5)"
        updatedAt: '2023-12-04T13:19:33.987Z'
      numEdits: 0
      reactions: []
      relatedEventId: 656dd1e672c19de723b2430a
    id: 656dd1e572c19de723b24303
    type: comment
  author: jbochi
  content: "Hello,\n\nI believe you are right that llama.cpp does not support T5 models,\
    \ but the binaries are not invalid. You can use them with candle:\n\n```\ncargo\
    \ run --example quantized-t5 --release  -- \\\n  --model-id \"jbochi/madlad400-3b-mt\"\
    \ --weight-file \"model-q4k.gguf\" \\\n  --prompt \"<2de> How are you, my friend?\"\
    \ \\\n  --temperature 0\n```\n\nI generated the files using the util described\
    \ in the [quantized-t5 example](https://github.com/huggingface/candle/tree/main/candle-examples/examples/quantized-t5)"
  created_at: 2023-12-04 13:19:33+00:00
  edited: false
  hidden: false
  id: 656dd1e572c19de723b24303
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jbochi
      type: user
    createdAt: '2023-12-04T13:19:34.000Z'
    data:
      status: closed
    id: 656dd1e672c19de723b2430a
    type: status-change
  author: jbochi
  created_at: 2023-12-04 13:19:34+00:00
  id: 656dd1e672c19de723b2430a
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-12-04T15:02:45.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9587894082069397
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>Oh I see, that''s really painful that they used the same format
          and filename in an incompatible fashion.<br>Thanks for clarification</p>

          '
        raw: 'Oh I see, that''s really painful that they used the same format and
          filename in an incompatible fashion.

          Thanks for clarification'
        updatedAt: '2023-12-04T15:02:45.115Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - jbochi
        - XelotX
    id: 656dea15ef1e27e9e353dffd
    type: comment
  author: cmp-nct
  content: 'Oh I see, that''s really painful that they used the same format and filename
    in an incompatible fashion.

    Thanks for clarification'
  created_at: 2023-12-04 15:02:45+00:00
  edited: false
  hidden: false
  id: 656dea15ef1e27e9e353dffd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: jbochi/madlad400-3b-mt
repo_type: model
status: closed
target_branch: null
title: gguf invalid and no support in llama.cpp
