!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tensiondriven
conflicting_files: null
created_at: 2023-05-25 21:42:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
      fullname: Jonathan Yankovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tensiondriven
      type: user
    createdAt: '2023-05-25T22:42:30.000Z'
    data:
      edited: false
      editors:
      - tensiondriven
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b21515ab063324ad5b374b0866aef2d0.svg
          fullname: Jonathan Yankovich
          isHf: false
          isPro: false
          name: tensiondriven
          type: user
        html: '<p>Nice work - From the readme,</p>

          <blockquote>

          <p>Training took ~30hrs on 5x3090s and used almost 23gb of vram on each.
          DDP was used for pytorch parallelism.</p>

          </blockquote>

          <p>I think I know the answer to this, but given that you used DDP, does
          that mean that this was trained across multiple CPU''s/systems?s (I am on
          the hunt for a motherboard/system that can support several GPU''s in one
          system and was initially excited that you may have used such a system)</p>

          <p>If this was on a single system, do you happen to know what motherboard/system
          specs were that support 5x3090''s?  And if not, then the search continues...</p>

          '
        raw: "Nice work - From the readme,\r\n\r\n> Training took ~30hrs on 5x3090s\
          \ and used almost 23gb of vram on each. DDP was used for pytorch parallelism.\r\
          \n\r\nI think I know the answer to this, but given that you used DDP, does\
          \ that mean that this was trained across multiple CPU's/systems?s (I am\
          \ on the hunt for a motherboard/system that can support several GPU's in\
          \ one system and was initially excited that you may have used such a system)\r\
          \n\r\nIf this was on a single system, do you happen to know what motherboard/system\
          \ specs were that support 5x3090's?  And if not, then the search continues..."
        updatedAt: '2023-05-25T22:42:30.661Z'
      numEdits: 0
      reactions: []
    id: 646fe4567a64358741c543ef
    type: comment
  author: tensiondriven
  content: "Nice work - From the readme,\r\n\r\n> Training took ~30hrs on 5x3090s\
    \ and used almost 23gb of vram on each. DDP was used for pytorch parallelism.\r\
    \n\r\nI think I know the answer to this, but given that you used DDP, does that\
    \ mean that this was trained across multiple CPU's/systems?s (I am on the hunt\
    \ for a motherboard/system that can support several GPU's in one system and was\
    \ initially excited that you may have used such a system)\r\n\r\nIf this was on\
    \ a single system, do you happen to know what motherboard/system specs were that\
    \ support 5x3090's?  And if not, then the search continues..."
  created_at: 2023-05-25 21:42:30+00:00
  edited: false
  hidden: false
  id: 646fe4567a64358741c543ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6deb50bede94aa8f26db9468bda65f4d.svg
      fullname: Lawrence Stewart
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: winddude
      type: user
    createdAt: '2023-05-25T22:55:01.000Z'
    data:
      edited: true
      editors:
      - winddude
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6deb50bede94aa8f26db9468bda65f4d.svg
          fullname: Lawrence Stewart
          isHf: false
          isPro: false
          name: winddude
          type: user
        html: '<p>1 CPU, 1 system, multiple GPUs,  it''s server components, tyan S8030GM2NE,
          Epyc 7532 32core</p>

          <p>DDP is pytorch''s parallel implementation for multiple gpus or multiple
          systems, <a rel="nofollow" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a></p>

          '
        raw: '1 CPU, 1 system, multiple GPUs,  it''s server components, tyan S8030GM2NE,
          Epyc 7532 32core


          DDP is pytorch''s parallel implementation for multiple gpus or multiple
          systems, https://pytorch.org/tutorials/intermediate/ddp_tutorial.html'
        updatedAt: '2023-05-25T22:56:33.860Z'
      numEdits: 2
      reactions: []
    id: 646fe745bc42f4b002360934
    type: comment
  author: winddude
  content: '1 CPU, 1 system, multiple GPUs,  it''s server components, tyan S8030GM2NE,
    Epyc 7532 32core


    DDP is pytorch''s parallel implementation for multiple gpus or multiple systems,
    https://pytorch.org/tutorials/intermediate/ddp_tutorial.html'
  created_at: 2023-05-25 21:55:01+00:00
  edited: true
  hidden: false
  id: 646fe745bc42f4b002360934
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b229669d21227b914badbb/umr0ngvZ5L_Nv2nVlC-Zo.png?w=200&h=200&f=face
      fullname: Hypersniper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hypersniper
      type: user
    createdAt: '2023-05-25T23:48:25.000Z'
    data:
      edited: false
      editors:
      - Hypersniper
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b229669d21227b914badbb/umr0ngvZ5L_Nv2nVlC-Zo.png?w=200&h=200&f=face
          fullname: Hypersniper
          isHf: false
          isPro: false
          name: Hypersniper
          type: user
        html: '<p>Just a newbie question, why did it take so much resources to train
          8bit 7B Lora? Usually in text generation webui I can do the same with 8GB
          vram batch size 1 . Is it do to the data size?</p>

          '
        raw: Just a newbie question, why did it take so much resources to train 8bit
          7B Lora? Usually in text generation webui I can do the same with 8GB vram
          batch size 1 . Is it do to the data size?
        updatedAt: '2023-05-25T23:48:25.494Z'
      numEdits: 0
      reactions: []
    id: 646ff3c96098ee820fc25c28
    type: comment
  author: Hypersniper
  content: Just a newbie question, why did it take so much resources to train 8bit
    7B Lora? Usually in text generation webui I can do the same with 8GB vram batch
    size 1 . Is it do to the data size?
  created_at: 2023-05-25 22:48:25+00:00
  edited: false
  hidden: false
  id: 646ff3c96098ee820fc25c28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6deb50bede94aa8f26db9468bda65f4d.svg
      fullname: Lawrence Stewart
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: winddude
      type: user
    createdAt: '2023-05-26T02:46:48.000Z'
    data:
      edited: true
      editors:
      - winddude
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6deb50bede94aa8f26db9468bda65f4d.svg
          fullname: Lawrence Stewart
          isHf: false
          isPro: false
          name: winddude
          type: user
        html: '<p>wow, only 8gb vram, that''s impressive. There are a number of factors,
          one is the batch size, as well as gradient accumulation steps. I also used
          the adamw_bnb_8bit optimizer, and optimizer could effect vram usage... My
          sequence length was also 1000, and padded, so they would all be 1000 tokens...
          that contributes for sure. I''m not sure on the total datasize, I don''t
          know if the entire dataset is loaded in vram.  The higher lora_r and lora_alpha
          would also contribute to higher vram usage. </p>

          <p>So to be full honest, I''m not 100% certain on everything, you''re vram
          usage seems surprisingly low, and mine seemed surprisingly high.</p>

          <p>Edit: update, dataset size seems to have zero effect on vram usage, which
          makes sense, because it''s loaded in in batch_size</p>

          '
        raw: "wow, only 8gb vram, that's impressive. There are a number of factors,\
          \ one is the batch size, as well as gradient accumulation steps. I also\
          \ used the adamw_bnb_8bit optimizer, and optimizer could effect vram usage...\
          \ My sequence length was also 1000, and padded, so they would all be 1000\
          \ tokens... that contributes for sure. I'm not sure on the total datasize,\
          \ I don't know if the entire dataset is loaded in vram.  The higher lora_r\
          \ and lora_alpha would also contribute to higher vram usage. \n\nSo to be\
          \ full honest, I'm not 100% certain on everything, you're vram usage seems\
          \ surprisingly low, and mine seemed surprisingly high.\n\nEdit: update,\
          \ dataset size seems to have zero effect on vram usage, which makes sense,\
          \ because it's loaded in in batch_size"
        updatedAt: '2023-05-26T16:25:28.813Z'
      numEdits: 3
      reactions: []
    id: 64701d98150f4cab8636eb4c
    type: comment
  author: winddude
  content: "wow, only 8gb vram, that's impressive. There are a number of factors,\
    \ one is the batch size, as well as gradient accumulation steps. I also used the\
    \ adamw_bnb_8bit optimizer, and optimizer could effect vram usage... My sequence\
    \ length was also 1000, and padded, so they would all be 1000 tokens... that contributes\
    \ for sure. I'm not sure on the total datasize, I don't know if the entire dataset\
    \ is loaded in vram.  The higher lora_r and lora_alpha would also contribute to\
    \ higher vram usage. \n\nSo to be full honest, I'm not 100% certain on everything,\
    \ you're vram usage seems surprisingly low, and mine seemed surprisingly high.\n\
    \nEdit: update, dataset size seems to have zero effect on vram usage, which makes\
    \ sense, because it's loaded in in batch_size"
  created_at: 2023-05-26 01:46:48+00:00
  edited: true
  hidden: false
  id: 64701d98150f4cab8636eb4c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: winddude/pb_lora_7b_v0.1
repo_type: model
status: open
target_branch: null
title: Hardware Question - Single system or multiple?
