!!python/object:huggingface_hub.community.DiscussionWithDetails
author: David841130
conflicting_files: null
created_at: 2023-03-31 16:58:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/21f1b898da149011e5c08a2386faa323.svg
      fullname: HuangYongZhen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: David841130
      type: user
    createdAt: '2023-03-31T17:58:33.000Z'
    data:
      edited: false
      editors:
      - David841130
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/21f1b898da149011e5c08a2386faa323.svg
          fullname: HuangYongZhen
          isHf: false
          isPro: false
          name: David841130
          type: user
        html: '<p>I try to use but get error</p>

          <p>from transformers import AutoTokenizer, AutoModelForSequenceClassification</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("yangheng/deberta-v3-base-absa-v1.1")</p>

          <p>model = AutoModelForSequenceClassification.from_pretrained("yangheng/deberta-v3-base-absa-v1.1")</p>

          <p>ValueError: Couldn''t instantiate the backend tokenizer from one of:<br>(1)
          a <code>tokenizers</code> library serialization file,<br>(2) a slow tokenizer
          instance to convert or<br>(3) an equivalent slow tokenizer class to instantiate
          and convert.<br>You need to have sentencepiece installed to convert a slow
          tokenizer to a fast one.</p>

          '
        raw: "I try to use but get error\r\n\r\nfrom transformers import AutoTokenizer,\
          \ AutoModelForSequenceClassification\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          yangheng/deberta-v3-base-absa-v1.1\")\r\n\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"\
          yangheng/deberta-v3-base-absa-v1.1\")\r\n\r\n\r\nValueError: Couldn't instantiate\
          \ the backend tokenizer from one of: \r\n(1) a `tokenizers` library serialization\
          \ file, \r\n(2) a slow tokenizer instance to convert or \r\n(3) an equivalent\
          \ slow tokenizer class to instantiate and convert. \r\nYou need to have\
          \ sentencepiece installed to convert a slow tokenizer to a fast one."
        updatedAt: '2023-03-31T17:58:33.104Z'
      numEdits: 0
      reactions: []
    id: 64271f495bca6f17a317be62
    type: comment
  author: David841130
  content: "I try to use but get error\r\n\r\nfrom transformers import AutoTokenizer,\
    \ AutoModelForSequenceClassification\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    yangheng/deberta-v3-base-absa-v1.1\")\r\n\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"\
    yangheng/deberta-v3-base-absa-v1.1\")\r\n\r\n\r\nValueError: Couldn't instantiate\
    \ the backend tokenizer from one of: \r\n(1) a `tokenizers` library serialization\
    \ file, \r\n(2) a slow tokenizer instance to convert or \r\n(3) an equivalent\
    \ slow tokenizer class to instantiate and convert. \r\nYou need to have sentencepiece\
    \ installed to convert a slow tokenizer to a fast one."
  created_at: 2023-03-31 16:58:33+00:00
  edited: false
  hidden: false
  id: 64271f495bca6f17a317be62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fa685f82ff9789c6fb68ac5642f955b7.svg
      fullname: Naif Saad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: naif576
      type: user
    createdAt: '2023-04-02T01:48:56.000Z'
    data:
      edited: false
      editors:
      - naif576
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fa685f82ff9789c6fb68ac5642f955b7.svg
          fullname: Naif Saad
          isHf: false
          isPro: false
          name: naif576
          type: user
        html: '<p>I have the same issue<br>I also tried use_fast=False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("yangheng/deberta-v3-base-absa-v1.1",use_fast
          = False)<br>TypeError: ''NoneType'' object is not callable</p>

          '
        raw: "I have the same issue \nI also tried use_fast=False\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          yangheng/deberta-v3-base-absa-v1.1\",use_fast = False)\nTypeError: 'NoneType'\
          \ object is not callable"
        updatedAt: '2023-04-02T01:48:56.933Z'
      numEdits: 0
      reactions: []
    id: 6428df08b443581755b04451
    type: comment
  author: naif576
  content: "I have the same issue \nI also tried use_fast=False\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    yangheng/deberta-v3-base-absa-v1.1\",use_fast = False)\nTypeError: 'NoneType'\
    \ object is not callable"
  created_at: 2023-04-02 00:48:56+00:00
  edited: false
  hidden: false
  id: 6428df08b443581755b04451
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94bdd764735a096bd94c6878a60eab27.svg
      fullname: Alon Cohen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AlonCohen
      type: user
    createdAt: '2023-04-08T17:06:38.000Z'
    data:
      edited: false
      editors:
      - AlonCohen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94bdd764735a096bd94c6878a60eab27.svg
          fullname: Alon Cohen
          isHf: false
          isPro: false
          name: AlonCohen
          type: user
        html: '<p>What worked for me is:<br>first, install sentencepiece and transformers</p>

          <p>!pip install sentencepiece<br>!pip install transformers</p>

          <p>Then load model and tokenizer:</p>

          <p>from transformers import AutoTokenizer, AutoModelForSequenceClassification<br>tokenizer
          = AutoTokenizer.from_pretrained("yangheng/deberta-v3-base-absa-v1.1")<br>model
          = AutoModelForSequenceClassification.from_pretrained("yangheng/deberta-v3-base-absa-v1.1")</p>

          '
        raw: "What worked for me is: \nfirst, install sentencepiece and transformers\n\
          \n!pip install sentencepiece\n!pip install transformers\n\nThen load model\
          \ and tokenizer:\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\
          tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\"\
          )\nmodel = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\"\
          )"
        updatedAt: '2023-04-08T17:06:38.537Z'
      numEdits: 0
      reactions: []
    id: 64319f1e46d9d4f9d899542d
    type: comment
  author: AlonCohen
  content: "What worked for me is: \nfirst, install sentencepiece and transformers\n\
    \n!pip install sentencepiece\n!pip install transformers\n\nThen load model and\
    \ tokenizer:\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\
    tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\"\
    )\nmodel = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\"\
    )"
  created_at: 2023-04-08 16:06:38+00:00
  edited: false
  hidden: false
  id: 64319f1e46d9d4f9d899542d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: yangheng/deberta-v3-base-absa-v1.1
repo_type: model
status: open
target_branch: null
title: 'ValueError: Couldn''t instantiate the backend tokenizer from one of'
