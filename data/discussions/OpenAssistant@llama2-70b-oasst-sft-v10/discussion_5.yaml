!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheBloke
conflicting_files: null
created_at: 2023-09-19 16:29:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-19T17:29:03.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5317485928535461
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Hey</p>\n<p>I've been making AWQs of all models I've done recently.\
          \ This one failed, due to this error:</p>\n<pre><code>ValueError: Trying\
          \ to set a tensor of shape torch.Size([32128, 8192]) in \"weight\" (which\
          \ has shape torch.Size([32007, 8192])), this look incorrect.\n</code></pre>\n\
          <p>I did some more digging, and realised I can't even load the model in\
          \ plain Transformers:</p>\n<pre><code class=\"language-python\">In [<span\
          \ class=\"hljs-number\">1</span>]: <span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM\n\
          \nIn [<span class=\"hljs-number\">2</span>]: model = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\".\"</span>, low_cpu_mem_usage=<span class=\"hljs-literal\"\
          >True</span>)\nLoading checkpoint shards:   <span class=\"hljs-number\"\
          >0</span>%|                                                            \
          \                                                                      \
          \                                                     | <span class=\"hljs-number\"\
          >0</span>/<span class=\"hljs-number\">15</span> [<span class=\"hljs-number\"\
          >00</span>:02&lt;?, ?it/s]\n---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[<span class=\"hljs-number\">2</span>], line <span class=\"hljs-number\"\
          >1</span>\n----&gt; <span class=\"hljs-number\">1</span> model = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\".\"</span>, low_cpu_mem_usage=<span class=\"hljs-literal\"\
          >True</span>)\n\nFile /workspace/venv/pytorch2/lib/python3<span class=\"\
          hljs-number\">.10</span>/site-packages/transformers/models/auto/auto_factory.py:<span\
          \ class=\"hljs-number\">563</span>, <span class=\"hljs-keyword\">in</span>\
          \ _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    <span class=\"hljs-number\">561</span> <span\
          \ class=\"hljs-keyword\">elif</span> <span class=\"hljs-built_in\">type</span>(config)\
          \ <span class=\"hljs-keyword\">in</span> cls._model_mapping.keys():\n  \
          \  <span class=\"hljs-number\">562</span>     model_class = _get_model_class(config,\
          \ cls._model_mapping)\n--&gt; <span class=\"hljs-number\">563</span>   \
          \  <span class=\"hljs-keyword\">return</span> model_class.from_pretrained(\n\
          \    <span class=\"hljs-number\">564</span>         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    <span class=\"\
          hljs-number\">565</span>     )\n    <span class=\"hljs-number\">566</span>\
          \ <span class=\"hljs-keyword\">raise</span> ValueError(\n    <span class=\"\
          hljs-number\">567</span>     <span class=\"hljs-string\">f\"Unrecognized\
          \ configuration class <span class=\"hljs-subst\">{config.__class__}</span>\
          \ for this kind of AutoModel: <span class=\"hljs-subst\">{cls.__name__}</span>.\\\
          n\"</span>\n    <span class=\"hljs-number\">568</span>     <span class=\"\
          hljs-string\">f\"Model type should be one of <span class=\"hljs-subst\"\
          >{<span class=\"hljs-string\">', '</span>.join(c.__name__ <span class=\"\
          hljs-keyword\">for</span> c <span class=\"hljs-keyword\">in</span> cls._model_mapping.keys())}</span>.\"\
          </span>\n    <span class=\"hljs-number\">569</span> )\n\nFile /workspace/venv/pytorch2/lib/python3<span\
          \ class=\"hljs-number\">.10</span>/site-packages/transformers/modeling_utils.py:<span\
          \ class=\"hljs-number\">3187</span>, <span class=\"hljs-keyword\">in</span>\
          \ PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config,\
          \ cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\n   <span class=\"\
          hljs-number\">3177</span>     <span class=\"hljs-keyword\">if</span> dtype_orig\
          \ <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span>\
          \ <span class=\"hljs-literal\">None</span>:\n   <span class=\"hljs-number\"\
          >3178</span>         torch.set_default_dtype(dtype_orig)\n   <span class=\"\
          hljs-number\">3180</span>     (\n   <span class=\"hljs-number\">3181</span>\
          \         model,\n   <span class=\"hljs-number\">3182</span>         missing_keys,\n\
          \   <span class=\"hljs-number\">3183</span>         unexpected_keys,\n \
          \  <span class=\"hljs-number\">3184</span>         mismatched_keys,\n  \
          \ <span class=\"hljs-number\">3185</span>         offload_index,\n   <span\
          \ class=\"hljs-number\">3186</span>         error_msgs,\n-&gt; <span class=\"\
          hljs-number\">3187</span>     ) = cls._load_pretrained_model(\n   <span\
          \ class=\"hljs-number\">3188</span>         model,\n   <span class=\"hljs-number\"\
          >3189</span>         state_dict,\n   <span class=\"hljs-number\">3190</span>\
          \         loaded_state_dict_keys,  <span class=\"hljs-comment\"># <span\
          \ class=\"hljs-doctag\">XXX:</span> rename?</span>\n   <span class=\"hljs-number\"\
          >3191</span>         resolved_archive_file,\n   <span class=\"hljs-number\"\
          >3192</span>         pretrained_model_name_or_path,\n   <span class=\"hljs-number\"\
          >3193</span>         ignore_mismatched_sizes=ignore_mismatched_sizes,\n\
          \   <span class=\"hljs-number\">3194</span>         sharded_metadata=sharded_metadata,\n\
          \   <span class=\"hljs-number\">3195</span>         _fast_init=_fast_init,\n\
          \   <span class=\"hljs-number\">3196</span>         low_cpu_mem_usage=low_cpu_mem_usage,\n\
          \   <span class=\"hljs-number\">3197</span>         device_map=device_map,\n\
          \   <span class=\"hljs-number\">3198</span>         offload_folder=offload_folder,\n\
          \   <span class=\"hljs-number\">3199</span>         offload_state_dict=offload_state_dict,\n\
          \   <span class=\"hljs-number\">3200</span>         dtype=torch_dtype,\n\
          \   <span class=\"hljs-number\">3201</span>         is_quantized=(<span\
          \ class=\"hljs-built_in\">getattr</span>(model, <span class=\"hljs-string\"\
          >\"quantization_method\"</span>, <span class=\"hljs-literal\">None</span>)\
          \ == QuantizationMethod.BITS_AND_BYTES),\n   <span class=\"hljs-number\"\
          >3202</span>         keep_in_fp32_modules=keep_in_fp32_modules,\n   <span\
          \ class=\"hljs-number\">3203</span>     )\n   <span class=\"hljs-number\"\
          >3205</span> model.is_loaded_in_4bit = load_in_4bit\n   <span class=\"hljs-number\"\
          >3206</span> model.is_loaded_in_8bit = load_in_8bit\n\nFile /workspace/venv/pytorch2/lib/python3<span\
          \ class=\"hljs-number\">.10</span>/site-packages/transformers/modeling_utils.py:<span\
          \ class=\"hljs-number\">3575</span>, <span class=\"hljs-keyword\">in</span>\
          \ PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys,\
          \ resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes,\
          \ sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder,\
          \ offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\n   <span\
          \ class=\"hljs-number\">3573</span> <span class=\"hljs-keyword\">if</span>\
          \ low_cpu_mem_usage:\n   <span class=\"hljs-number\">3574</span>     <span\
          \ class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span>\
          \ is_fsdp_enabled() <span class=\"hljs-keyword\">or</span> is_fsdp_enabled_and_dist_rank_0():\n\
          -&gt; <span class=\"hljs-number\">3575</span>         new_error_msgs, offload_index,\
          \ state_dict_index = _load_state_dict_into_meta_model(\n   <span class=\"\
          hljs-number\">3576</span>             model_to_load,\n   <span class=\"\
          hljs-number\">3577</span>             state_dict,\n   <span class=\"hljs-number\"\
          >3578</span>             loaded_keys,\n   <span class=\"hljs-number\">3579</span>\
          \             start_prefix,\n   <span class=\"hljs-number\">3580</span>\
          \             expected_keys,\n   <span class=\"hljs-number\">3581</span>\
          \             device_map=device_map,\n   <span class=\"hljs-number\">3582</span>\
          \             offload_folder=offload_folder,\n   <span class=\"hljs-number\"\
          >3583</span>             offload_index=offload_index,\n   <span class=\"\
          hljs-number\">3584</span>             state_dict_folder=state_dict_folder,\n\
          \   <span class=\"hljs-number\">3585</span>             state_dict_index=state_dict_index,\n\
          \   <span class=\"hljs-number\">3586</span>             dtype=dtype,\n \
          \  <span class=\"hljs-number\">3587</span>             is_quantized=is_quantized,\n\
          \   <span class=\"hljs-number\">3588</span>             is_safetensors=is_safetensors,\n\
          \   <span class=\"hljs-number\">3589</span>             keep_in_fp32_modules=keep_in_fp32_modules,\n\
          \   <span class=\"hljs-number\">3590</span>         )\n   <span class=\"\
          hljs-number\">3591</span>         error_msgs += new_error_msgs\n   <span\
          \ class=\"hljs-number\">3592</span>     <span class=\"hljs-keyword\">else</span>:\n\
          \nFile /workspace/venv/pytorch2/lib/python3<span class=\"hljs-number\">.10</span>/site-packages/transformers/modeling_utils.py:<span\
          \ class=\"hljs-number\">745</span>, <span class=\"hljs-keyword\">in</span>\
          \ _load_state_dict_into_meta_model(model, state_dict, loaded_state_dict_keys,\
          \ start_prefix, expected_keys, device_map, offload_folder, offload_index,\
          \ state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors,\
          \ keep_in_fp32_modules)\n    <span class=\"hljs-number\">742</span>    \
          \ state_dict_index = offload_weight(param, param_name, state_dict_folder,\
          \ state_dict_index)\n    <span class=\"hljs-number\">743</span> <span class=\"\
          hljs-keyword\">elif</span> <span class=\"hljs-keyword\">not</span> is_quantized:\n\
          \    <span class=\"hljs-number\">744</span>     <span class=\"hljs-comment\"\
          ># For backward compatibility with older versions of `accelerate`</span>\n\
          --&gt; <span class=\"hljs-number\">745</span>     set_module_tensor_to_device(model,\
          \ param_name, param_device, **set_module_kwargs)\n    <span class=\"hljs-number\"\
          >746</span> <span class=\"hljs-keyword\">else</span>:\n    <span class=\"\
          hljs-number\">747</span>     <span class=\"hljs-keyword\">if</span> param.dtype\
          \ == torch.int8 <span class=\"hljs-keyword\">and</span> param_name.replace(<span\
          \ class=\"hljs-string\">\"weight\"</span>, <span class=\"hljs-string\">\"\
          SCB\"</span>) <span class=\"hljs-keyword\">in</span> state_dict.keys():\n\
          \nFile /workspace/venv/pytorch2/lib/python3<span class=\"hljs-number\">.10</span>/site-packages/accelerate/utils/modeling.py:<span\
          \ class=\"hljs-number\">285</span>, <span class=\"hljs-keyword\">in</span>\
          \ set_module_tensor_to_device(module, tensor_name, device, value, dtype,\
          \ fp16_statistics)\n    <span class=\"hljs-number\">283</span> <span class=\"\
          hljs-keyword\">if</span> value <span class=\"hljs-keyword\">is</span> <span\
          \ class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n\
          \    <span class=\"hljs-number\">284</span>     <span class=\"hljs-keyword\"\
          >if</span> old_value.shape != value.shape:\n--&gt; <span class=\"hljs-number\"\
          >285</span>         <span class=\"hljs-keyword\">raise</span> ValueError(\n\
          \    <span class=\"hljs-number\">286</span>             <span class=\"hljs-string\"\
          >f'Trying to set a tensor of shape <span class=\"hljs-subst\">{value.shape}</span>\
          \ in \"<span class=\"hljs-subst\">{tensor_name}</span>\" (which has shape\
          \ <span class=\"hljs-subst\">{old_value.shape}</span>), this look incorrect.'</span>\n\
          \    <span class=\"hljs-number\">287</span>         )\n    <span class=\"\
          hljs-number\">289</span>     <span class=\"hljs-keyword\">if</span> dtype\
          \ <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n\
          \    <span class=\"hljs-number\">290</span>         <span class=\"hljs-comment\"\
          ># For compatibility with PyTorch load_state_dict which converts state dict\
          \ dtype to existing dtype in model</span>\n    <span class=\"hljs-number\"\
          >291</span>         value = value.to(old_value.dtype)\n\nValueError: Trying\
          \ to <span class=\"hljs-built_in\">set</span> a tensor of shape torch.Size([<span\
          \ class=\"hljs-number\">32128</span>, <span class=\"hljs-number\">8192</span>])\
          \ <span class=\"hljs-keyword\">in</span> <span class=\"hljs-string\">\"\
          weight\"</span> (which has shape torch.Size([<span class=\"hljs-number\"\
          >32007</span>, <span class=\"hljs-number\">8192</span>])), this look incorrect.\n\
          \nIn [<span class=\"hljs-number\">3</span>]:\n</code></pre>\n<p>Is there\
          \ any workaround you know of?  I'm curious how this is working for people\
          \ still - or maybe it isn't any more.</p>\n<p>It worked for me when I made\
          \ GPTQs three weeks ago, so I'm wondering if a recent update to Transformers\
          \ or Accelerate (the error comes from Accelerate) is what's triggering the\
          \ problem.</p>\n<p>I'll see if I can go back to the earlier revision, before\
          \ the 128 padding, to make the AWQ.</p>\n"
        raw: "Hey\r\n\r\nI've been making AWQs of all models I've done recently. This\
          \ one failed, due to this error:\r\n```\r\nValueError: Trying to set a tensor\
          \ of shape torch.Size([32128, 8192]) in \"weight\" (which has shape torch.Size([32007,\
          \ 8192])), this look incorrect.\r\n```\r\n\r\nI did some more digging, and\
          \ realised I can't even load the model in plain Transformers:\r\n```python\r\
          \nIn [1]: from transformers import AutoModelForCausalLM\r\n\r\nIn [2]: model\
          \ = AutoModelForCausalLM.from_pretrained(\".\", low_cpu_mem_usage=True)\r\
          \nLoading checkpoint shards:   0%|                                     \
          \                                                                      \
          \                                                                      \
          \      | 0/15 [00:02<?, ?it/s]\r\n---------------------------------------------------------------------------\r\
          \nValueError                                Traceback (most recent call\
          \ last)\r\nCell In[2], line 1\r\n----> 1 model = AutoModelForCausalLM.from_pretrained(\"\
          .\", low_cpu_mem_usage=True)\r\n\r\nFile /workspace/venv/pytorch2/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\r\n    561 elif type(config) in cls._model_mapping.keys():\r\
          \n    562     model_class = _get_model_class(config, cls._model_mapping)\r\
          \n--> 563     return model_class.from_pretrained(\r\n    564         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\r\n    565     )\r\n\
          \    566 raise ValueError(\r\n    567     f\"Unrecognized configuration\
          \ class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\\
          n\"\r\n    568     f\"Model type should be one of {', '.join(c.__name__\
          \ for c in cls._model_mapping.keys())}.\"\r\n    569 )\r\n\r\nFile /workspace/venv/pytorch2/lib/python3.10/site-packages/transformers/modeling_utils.py:3187,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\r\n   3177  \
          \   if dtype_orig is not None:\r\n   3178         torch.set_default_dtype(dtype_orig)\r\
          \n   3180     (\r\n   3181         model,\r\n   3182         missing_keys,\r\
          \n   3183         unexpected_keys,\r\n   3184         mismatched_keys,\r\
          \n   3185         offload_index,\r\n   3186         error_msgs,\r\n-> 3187\
          \     ) = cls._load_pretrained_model(\r\n   3188         model,\r\n   3189\
          \         state_dict,\r\n   3190         loaded_state_dict_keys,  # XXX:\
          \ rename?\r\n   3191         resolved_archive_file,\r\n   3192         pretrained_model_name_or_path,\r\
          \n   3193         ignore_mismatched_sizes=ignore_mismatched_sizes,\r\n \
          \  3194         sharded_metadata=sharded_metadata,\r\n   3195         _fast_init=_fast_init,\r\
          \n   3196         low_cpu_mem_usage=low_cpu_mem_usage,\r\n   3197      \
          \   device_map=device_map,\r\n   3198         offload_folder=offload_folder,\r\
          \n   3199         offload_state_dict=offload_state_dict,\r\n   3200    \
          \     dtype=torch_dtype,\r\n   3201         is_quantized=(getattr(model,\
          \ \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES),\r\
          \n   3202         keep_in_fp32_modules=keep_in_fp32_modules,\r\n   3203\
          \     )\r\n   3205 model.is_loaded_in_4bit = load_in_4bit\r\n   3206 model.is_loaded_in_8bit\
          \ = load_in_8bit\r\n\r\nFile /workspace/venv/pytorch2/lib/python3.10/site-packages/transformers/modeling_utils.py:3575,\
          \ in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys,\
          \ resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes,\
          \ sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder,\
          \ offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\r\n   3573\
          \ if low_cpu_mem_usage:\r\n   3574     if not is_fsdp_enabled() or is_fsdp_enabled_and_dist_rank_0():\r\
          \n-> 3575         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\r\
          \n   3576             model_to_load,\r\n   3577             state_dict,\r\
          \n   3578             loaded_keys,\r\n   3579             start_prefix,\r\
          \n   3580             expected_keys,\r\n   3581             device_map=device_map,\r\
          \n   3582             offload_folder=offload_folder,\r\n   3583        \
          \     offload_index=offload_index,\r\n   3584             state_dict_folder=state_dict_folder,\r\
          \n   3585             state_dict_index=state_dict_index,\r\n   3586    \
          \         dtype=dtype,\r\n   3587             is_quantized=is_quantized,\r\
          \n   3588             is_safetensors=is_safetensors,\r\n   3589        \
          \     keep_in_fp32_modules=keep_in_fp32_modules,\r\n   3590         )\r\n\
          \   3591         error_msgs += new_error_msgs\r\n   3592     else:\r\n\r\
          \nFile /workspace/venv/pytorch2/lib/python3.10/site-packages/transformers/modeling_utils.py:745,\
          \ in _load_state_dict_into_meta_model(model, state_dict, loaded_state_dict_keys,\
          \ start_prefix, expected_keys, device_map, offload_folder, offload_index,\
          \ state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors,\
          \ keep_in_fp32_modules)\r\n    742     state_dict_index = offload_weight(param,\
          \ param_name, state_dict_folder, state_dict_index)\r\n    743 elif not is_quantized:\r\
          \n    744     # For backward compatibility with older versions of `accelerate`\r\
          \n--> 745     set_module_tensor_to_device(model, param_name, param_device,\
          \ **set_module_kwargs)\r\n    746 else:\r\n    747     if param.dtype ==\
          \ torch.int8 and param_name.replace(\"weight\", \"SCB\") in state_dict.keys():\r\
          \n\r\nFile /workspace/venv/pytorch2/lib/python3.10/site-packages/accelerate/utils/modeling.py:285,\
          \ in set_module_tensor_to_device(module, tensor_name, device, value, dtype,\
          \ fp16_statistics)\r\n    283 if value is not None:\r\n    284     if old_value.shape\
          \ != value.shape:\r\n--> 285         raise ValueError(\r\n    286      \
          \       f'Trying to set a tensor of shape {value.shape} in \"{tensor_name}\"\
          \ (which has shape {old_value.shape}), this look incorrect.'\r\n    287\
          \         )\r\n    289     if dtype is None:\r\n    290         # For compatibility\
          \ with PyTorch load_state_dict which converts state dict dtype to existing\
          \ dtype in model\r\n    291         value = value.to(old_value.dtype)\r\n\
          \r\nValueError: Trying to set a tensor of shape torch.Size([32128, 8192])\
          \ in \"weight\" (which has shape torch.Size([32007, 8192])), this look incorrect.\r\
          \n\r\nIn [3]:\r\n```\r\n\r\nIs there any workaround you know of?  I'm curious\
          \ how this is working for people still - or maybe it isn't any more.\r\n\
          \r\nIt worked for me when I made GPTQs three weeks ago, so I'm wondering\
          \ if a recent update to Transformers or Accelerate (the error comes from\
          \ Accelerate) is what's triggering the problem.\r\n\r\nI'll see if I can\
          \ go back to the earlier revision, before the 128 padding, to make the AWQ."
        updatedAt: '2023-09-19T17:29:03.627Z'
      numEdits: 0
      reactions: []
    id: 6509da5f2c804aed59e1a817
    type: comment
  author: TheBloke
  content: "Hey\r\n\r\nI've been making AWQs of all models I've done recently. This\
    \ one failed, due to this error:\r\n```\r\nValueError: Trying to set a tensor\
    \ of shape torch.Size([32128, 8192]) in \"weight\" (which has shape torch.Size([32007,\
    \ 8192])), this look incorrect.\r\n```\r\n\r\nI did some more digging, and realised\
    \ I can't even load the model in plain Transformers:\r\n```python\r\nIn [1]: from\
    \ transformers import AutoModelForCausalLM\r\n\r\nIn [2]: model = AutoModelForCausalLM.from_pretrained(\"\
    .\", low_cpu_mem_usage=True)\r\nLoading checkpoint shards:   0%|             \
    \                                                                            \
    \                                                                            \
    \                  | 0/15 [00:02<?, ?it/s]\r\n---------------------------------------------------------------------------\r\
    \nValueError                                Traceback (most recent call last)\r\
    \nCell In[2], line 1\r\n----> 1 model = AutoModelForCausalLM.from_pretrained(\"\
    .\", low_cpu_mem_usage=True)\r\n\r\nFile /workspace/venv/pytorch2/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\r\n    561 elif type(config) in cls._model_mapping.keys():\r\n   \
    \ 562     model_class = _get_model_class(config, cls._model_mapping)\r\n--> 563\
    \     return model_class.from_pretrained(\r\n    564         pretrained_model_name_or_path,\
    \ *model_args, config=config, **hub_kwargs, **kwargs\r\n    565     )\r\n    566\
    \ raise ValueError(\r\n    567     f\"Unrecognized configuration class {config.__class__}\
    \ for this kind of AutoModel: {cls.__name__}.\\n\"\r\n    568     f\"Model type\
    \ should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\
    \r\n    569 )\r\n\r\nFile /workspace/venv/pytorch2/lib/python3.10/site-packages/transformers/modeling_utils.py:3187,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config,\
    \ cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token,\
    \ revision, use_safetensors, *model_args, **kwargs)\r\n   3177     if dtype_orig\
    \ is not None:\r\n   3178         torch.set_default_dtype(dtype_orig)\r\n   3180\
    \     (\r\n   3181         model,\r\n   3182         missing_keys,\r\n   3183\
    \         unexpected_keys,\r\n   3184         mismatched_keys,\r\n   3185    \
    \     offload_index,\r\n   3186         error_msgs,\r\n-> 3187     ) = cls._load_pretrained_model(\r\
    \n   3188         model,\r\n   3189         state_dict,\r\n   3190         loaded_state_dict_keys,\
    \  # XXX: rename?\r\n   3191         resolved_archive_file,\r\n   3192       \
    \  pretrained_model_name_or_path,\r\n   3193         ignore_mismatched_sizes=ignore_mismatched_sizes,\r\
    \n   3194         sharded_metadata=sharded_metadata,\r\n   3195         _fast_init=_fast_init,\r\
    \n   3196         low_cpu_mem_usage=low_cpu_mem_usage,\r\n   3197         device_map=device_map,\r\
    \n   3198         offload_folder=offload_folder,\r\n   3199         offload_state_dict=offload_state_dict,\r\
    \n   3200         dtype=torch_dtype,\r\n   3201         is_quantized=(getattr(model,\
    \ \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES),\r\n  \
    \ 3202         keep_in_fp32_modules=keep_in_fp32_modules,\r\n   3203     )\r\n\
    \   3205 model.is_loaded_in_4bit = load_in_4bit\r\n   3206 model.is_loaded_in_8bit\
    \ = load_in_8bit\r\n\r\nFile /workspace/venv/pytorch2/lib/python3.10/site-packages/transformers/modeling_utils.py:3575,\
    \ in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys,\
    \ resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes,\
    \ sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder,\
    \ offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\r\n   3573 if\
    \ low_cpu_mem_usage:\r\n   3574     if not is_fsdp_enabled() or is_fsdp_enabled_and_dist_rank_0():\r\
    \n-> 3575         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\r\
    \n   3576             model_to_load,\r\n   3577             state_dict,\r\n  \
    \ 3578             loaded_keys,\r\n   3579             start_prefix,\r\n   3580\
    \             expected_keys,\r\n   3581             device_map=device_map,\r\n\
    \   3582             offload_folder=offload_folder,\r\n   3583             offload_index=offload_index,\r\
    \n   3584             state_dict_folder=state_dict_folder,\r\n   3585        \
    \     state_dict_index=state_dict_index,\r\n   3586             dtype=dtype,\r\
    \n   3587             is_quantized=is_quantized,\r\n   3588             is_safetensors=is_safetensors,\r\
    \n   3589             keep_in_fp32_modules=keep_in_fp32_modules,\r\n   3590  \
    \       )\r\n   3591         error_msgs += new_error_msgs\r\n   3592     else:\r\
    \n\r\nFile /workspace/venv/pytorch2/lib/python3.10/site-packages/transformers/modeling_utils.py:745,\
    \ in _load_state_dict_into_meta_model(model, state_dict, loaded_state_dict_keys,\
    \ start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder,\
    \ state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\r\
    \n    742     state_dict_index = offload_weight(param, param_name, state_dict_folder,\
    \ state_dict_index)\r\n    743 elif not is_quantized:\r\n    744     # For backward\
    \ compatibility with older versions of `accelerate`\r\n--> 745     set_module_tensor_to_device(model,\
    \ param_name, param_device, **set_module_kwargs)\r\n    746 else:\r\n    747 \
    \    if param.dtype == torch.int8 and param_name.replace(\"weight\", \"SCB\")\
    \ in state_dict.keys():\r\n\r\nFile /workspace/venv/pytorch2/lib/python3.10/site-packages/accelerate/utils/modeling.py:285,\
    \ in set_module_tensor_to_device(module, tensor_name, device, value, dtype, fp16_statistics)\r\
    \n    283 if value is not None:\r\n    284     if old_value.shape != value.shape:\r\
    \n--> 285         raise ValueError(\r\n    286             f'Trying to set a tensor\
    \ of shape {value.shape} in \"{tensor_name}\" (which has shape {old_value.shape}),\
    \ this look incorrect.'\r\n    287         )\r\n    289     if dtype is None:\r\
    \n    290         # For compatibility with PyTorch load_state_dict which converts\
    \ state dict dtype to existing dtype in model\r\n    291         value = value.to(old_value.dtype)\r\
    \n\r\nValueError: Trying to set a tensor of shape torch.Size([32128, 8192]) in\
    \ \"weight\" (which has shape torch.Size([32007, 8192])), this look incorrect.\r\
    \n\r\nIn [3]:\r\n```\r\n\r\nIs there any workaround you know of?  I'm curious\
    \ how this is working for people still - or maybe it isn't any more.\r\n\r\nIt\
    \ worked for me when I made GPTQs three weeks ago, so I'm wondering if a recent\
    \ update to Transformers or Accelerate (the error comes from Accelerate) is what's\
    \ triggering the problem.\r\n\r\nI'll see if I can go back to the earlier revision,\
    \ before the 128 padding, to make the AWQ."
  created_at: 2023-09-19 16:29:03+00:00
  edited: false
  hidden: false
  id: 6509da5f2c804aed59e1a817
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-19T17:41:57.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9633303880691528
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I went back to the earlier commit (<code><a href="/OpenAssistant/llama2-70b-oasst-sft-v10/commit/d9f292769e461eec1f7bfe416ccd4e8043a46179">d9f292769e461eec1f7bfe416ccd4e8043a46179</a></code>)
          and now I can load the model and the AWQ is being created now with no errors.</p>

          <p>I guess this AWQ probably won''t be shardable due to the uneven vocab_size.  But
          better than not being able to make it at all!</p>

          <p>Let me know if you''ve got any thoughts as to why I can''t load the pad-to-128
          version.</p>

          '
        raw: 'I went back to the earlier commit (`d9f292769e461eec1f7bfe416ccd4e8043a46179`)
          and now I can load the model and the AWQ is being created now with no errors.


          I guess this AWQ probably won''t be shardable due to the uneven vocab_size.  But
          better than not being able to make it at all!


          Let me know if you''ve got any thoughts as to why I can''t load the pad-to-128
          version.'
        updatedAt: '2023-09-19T17:42:22.093Z'
      numEdits: 1
      reactions: []
    id: 6509dd655b34509e16f6cfb6
    type: comment
  author: TheBloke
  content: 'I went back to the earlier commit (`d9f292769e461eec1f7bfe416ccd4e8043a46179`)
    and now I can load the model and the AWQ is being created now with no errors.


    I guess this AWQ probably won''t be shardable due to the uneven vocab_size.  But
    better than not being able to make it at all!


    Let me know if you''ve got any thoughts as to why I can''t load the pad-to-128
    version.'
  created_at: 2023-09-19 16:41:57+00:00
  edited: true
  hidden: false
  id: 6509dd655b34509e16f6cfb6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2ef02f2e45870e438dfa61cefcd6969.svg
      fullname: Buddy W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Buddx
      type: user
    createdAt: '2023-10-02T10:00:13.000Z'
    data:
      edited: false
      editors:
      - Buddx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9680067896842957
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2ef02f2e45870e438dfa61cefcd6969.svg
          fullname: Buddy W
          isHf: false
          isPro: false
          name: Buddx
          type: user
        html: '<p>Switching to accelerate==0.21.0 worked for me.</p>

          '
        raw: Switching to accelerate==0.21.0 worked for me.
        updatedAt: '2023-10-02T10:00:13.261Z'
      numEdits: 0
      reactions: []
    id: 651a94ad186bc3b699abe503
    type: comment
  author: Buddx
  content: Switching to accelerate==0.21.0 worked for me.
  created_at: 2023-10-02 09:00:13+00:00
  edited: false
  hidden: false
  id: 651a94ad186bc3b699abe503
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1645757180654-noauth.jpeg?w=200&h=200&f=face
      fullname: Long Phan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: justinphan3110
      type: user
    createdAt: '2023-10-19T21:29:53.000Z'
    data:
      edited: false
      editors:
      - justinphan3110
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8039015531539917
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1645757180654-noauth.jpeg?w=200&h=200&f=face
          fullname: Long Phan
          isHf: false
          isPro: false
          name: justinphan3110
          type: user
        html: '<p>any update on the official fix for this to for up-to-date transformer+accelerate
          version?</p>

          '
        raw: any update on the official fix for this to for up-to-date transformer+accelerate
          version?
        updatedAt: '2023-10-19T21:29:53.044Z'
      numEdits: 0
      reactions: []
    id: 65319fd14c02c1e17cd76aba
    type: comment
  author: justinphan3110
  content: any update on the official fix for this to for up-to-date transformer+accelerate
    version?
  created_at: 2023-10-19 20:29:53+00:00
  edited: false
  hidden: false
  id: 65319fd14c02c1e17cd76aba
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: OpenAssistant/llama2-70b-oasst-sft-v10
repo_type: model
status: open
target_branch: null
title: Model appears to be unusable now, due to the 128 padding (perhaps due to recent
  changes in Transformers?)
