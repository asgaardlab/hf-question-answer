!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yotti0618
conflicting_files: null
created_at: 2023-06-05 09:18:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/05f7605620a04257c5cde19263bbbdd9.svg
      fullname: "\u5DDD\u897F\u6176\u5C1A"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yotti0618
      type: user
    createdAt: '2023-06-05T10:18:33.000Z'
    data:
      edited: true
      editors:
      - yotti0618
      hidden: false
      identifiedLanguage:
        language: ja
        probability: 0.7626757621765137
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/05f7605620a04257c5cde19263bbbdd9.svg
          fullname: "\u5DDD\u897F\u6176\u5C1A"
          isHf: false
          isPro: false
          name: yotti0618
          type: user
        html: "<p>\u3053\u3093\u306B\u3061\u306F\u3001Hugging Face\u306E\u30B3\u30DF\
          \u30E5\u30CB\u30C6\u30A3\u306E\u7686\u3055\u3093\u3002</p>\n<p>\u79C1\u306F\
          \u300Cstudio-ousia/luke-japanese-large\u300D\u306ELUKE Japanese Large\u30E2\
          \u30C7\u30EB\u3092\u4F7F\u7528\u3057\u3066\u3044\u3066\u3001\u30C8\u30FC\
          \u30AF\u30F3\u3092\u30DE\u30B9\u30AF\u3057\u3066\u4E88\u6E2C\u3059\u308B\
          \u969B\u306B\u554F\u984C\u306B\u76F4\u9762\u3057\u307E\u3057\u305F\u3002\
          </p>\n<p>\u4EE5\u4E0B\u306B\u4F7F\u7528\u3057\u305F\u30B3\u30FC\u30C9\u3092\
          \u793A\u3057\u307E\u3059\uFF1A</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, AutoModelForMaskedLM\n<span class=\"hljs-keyword\"\
          >import</span> torch\n\n<span class=\"hljs-comment\"># Define the model</span>\n\
          MODEL_NAME = <span class=\"hljs-string\">\"studio-ousia/luke-japanese-large\"\
          </span>\n\n<span class=\"hljs-comment\"># Load the pre-trained model's tokenizer\
          \ (vocabulary)</span>\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\
          \n<span class=\"hljs-comment\"># Tokenize the input</span>\ntext = <span\
          \ class=\"hljs-string\">\"\u4EBA\u9593\u3068\u30ED\u30DC\u30C3\u30C8\u306F\
          \u9577\u3089\u304F\u4EBA\u751F\u3092\u4E00\u7DD2\u306B\u904E\u3054\u3057\
          \u3066\u304D\u305F\u306E\u3067\u3042\u308B\u3002\"</span>\ntokenized_text\
          \ = tokenizer.tokenize(text)\n\n<span class=\"hljs-comment\"># Mask a token\
          \ for prediction using BertForMaskedLM</span>\nmasked_index = <span class=\"\
          hljs-number\">2</span>\ntokenized_text[masked_index] = <span class=\"hljs-string\"\
          >'[MASK]'</span>\n\n<span class=\"hljs-comment\"># Convert the tokens into\
          \ vocabulary indices</span>\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n\
          \n<span class=\"hljs-comment\"># Convert the input into PyTorch tensor</span>\n\
          tokens_tensor = torch.tensor([indexed_tokens]).to(<span class=\"hljs-string\"\
          >\"cuda\"</span>)\n\n<span class=\"hljs-comment\"># Load the pre-trained\
          \ model (weights)</span>\nmodel = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n\
          model.<span class=\"hljs-built_in\">eval</span>()\nmodel.to(<span class=\"\
          hljs-string\">\"cuda\"</span>)\n\n<span class=\"hljs-comment\"># Predict\
          \ all tokens</span>\n<span class=\"hljs-keyword\">with</span> torch.no_grad():\n\
          \    outputs = model(tokens_tensor)\n    predictions = outputs[<span class=\"\
          hljs-number\">0</span>]\n\n<span class=\"hljs-comment\"># Confirm that 'S'\
          \ is predicted</span>\npredicted_index = torch.argmax(predictions[<span\
          \ class=\"hljs-number\">0</span>, masked_index]).item()\npredicted_token\
          \ = tokenizer.convert_ids_to_tokens([predicted_index])[<span class=\"hljs-number\"\
          >0</span>]\n\n<span class=\"hljs-built_in\">print</span>(predicted_token)\n\
          <span class=\"hljs-built_in\">print</span>(predicted_index)\n</code></pre>\n\
          <p>\u5F97\u3089\u308C\u305F\u51FA\u529B\u306F'''&lt; \"unk \"&gt;\u3068\
          \ 3 '''\u3067\u3001\u30DE\u30B9\u30AF\u3057\u305F\u4F4D\u7F6E\u306B\u5BFE\
          \u3057\u3066\u306F\u3042\u307E\u308A\u610F\u5473\u304C\u306A\u3044\u3088\
          \u3046\u306B\u601D\u308F\u308C\u307E\u3057\u305F\u3002</p>\n<p>\u79C1\u304C\
          \u4F55\u304B\u898B\u843D\u3068\u3057\u3066\u3044\u308B\u70B9\u3084\u3001\
          \u3053\u306E\u7D50\u679C\u3092\u6539\u5584\u3059\u308B\u305F\u3081\u306E\
          \u63D0\u6848\u306F\u3042\u308A\u307E\u3059\u304B\uFF1F \u3053\u306E\u65B9\
          \u6CD5\u3092\u4F7F\u7528\u3057\u3066\u30DE\u30B9\u30AF\u3057\u305F\u30C8\
          \u30FC\u30AF\u30F3\u3092\u9069\u5207\u306B\u4E88\u6E2C\u3067\u304D\u3066\
          \u3044\u306A\u3044\u3088\u3046\u306B\u601D\u3048\u307E\u3059\u3002</p>\n\
          <p>\u3042\u3089\u304B\u3058\u3081\u3054\u5354\u529B\u3044\u305F\u3060\u304D\
          \u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\u3059\u3002</p>\n\
          <p>\u3088\u308D\u3057\u304F\u304A\u9858\u3044\u3044\u305F\u3057\u307E\u3059\
          \u3001</p>\n<p>yotti0618</p>\n"
        raw: "\u3053\u3093\u306B\u3061\u306F\u3001Hugging Face\u306E\u30B3\u30DF\u30E5\
          \u30CB\u30C6\u30A3\u306E\u7686\u3055\u3093\u3002\n\n\u79C1\u306F\u300Cstudio-ousia/luke-japanese-large\u300D\
          \u306ELUKE Japanese Large\u30E2\u30C7\u30EB\u3092\u4F7F\u7528\u3057\u3066\
          \u3044\u3066\u3001\u30C8\u30FC\u30AF\u30F3\u3092\u30DE\u30B9\u30AF\u3057\
          \u3066\u4E88\u6E2C\u3059\u308B\u969B\u306B\u554F\u984C\u306B\u76F4\u9762\
          \u3057\u307E\u3057\u305F\u3002\n\n\u4EE5\u4E0B\u306B\u4F7F\u7528\u3057\u305F\
          \u30B3\u30FC\u30C9\u3092\u793A\u3057\u307E\u3059\uFF1A\n\n```python\nfrom\
          \ transformers import AutoTokenizer, AutoModelForMaskedLM\nimport torch\n\
          \n# Define the model\nMODEL_NAME = \"studio-ousia/luke-japanese-large\"\n\
          \n# Load the pre-trained model's tokenizer (vocabulary)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\
          \n# Tokenize the input\ntext = \"\u4EBA\u9593\u3068\u30ED\u30DC\u30C3\u30C8\
          \u306F\u9577\u3089\u304F\u4EBA\u751F\u3092\u4E00\u7DD2\u306B\u904E\u3054\
          \u3057\u3066\u304D\u305F\u306E\u3067\u3042\u308B\u3002\"\ntokenized_text\
          \ = tokenizer.tokenize(text)\n\n# Mask a token for prediction using BertForMaskedLM\n\
          masked_index = 2\ntokenized_text[masked_index] = '[MASK]'\n\n# Convert the\
          \ tokens into vocabulary indices\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n\
          \n# Convert the input into PyTorch tensor\ntokens_tensor = torch.tensor([indexed_tokens]).to(\"\
          cuda\")\n\n# Load the pre-trained model (weights)\nmodel = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n\
          model.eval()\nmodel.to(\"cuda\")\n\n# Predict all tokens\nwith torch.no_grad():\n\
          \    outputs = model(tokens_tensor)\n    predictions = outputs[0]\n\n# Confirm\
          \ that 'S' is predicted\npredicted_index = torch.argmax(predictions[0, masked_index]).item()\n\
          predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n\
          \nprint(predicted_token)\nprint(predicted_index)\n```\n\n\u5F97\u3089\u308C\
          \u305F\u51FA\u529B\u306F'''< \"unk \">\u3068 3 '''\u3067\u3001\u30DE\u30B9\
          \u30AF\u3057\u305F\u4F4D\u7F6E\u306B\u5BFE\u3057\u3066\u306F\u3042\u307E\
          \u308A\u610F\u5473\u304C\u306A\u3044\u3088\u3046\u306B\u601D\u308F\u308C\
          \u307E\u3057\u305F\u3002\n\n\u79C1\u304C\u4F55\u304B\u898B\u843D\u3068\u3057\
          \u3066\u3044\u308B\u70B9\u3084\u3001\u3053\u306E\u7D50\u679C\u3092\u6539\
          \u5584\u3059\u308B\u305F\u3081\u306E\u63D0\u6848\u306F\u3042\u308A\u307E\
          \u3059\u304B\uFF1F \u3053\u306E\u65B9\u6CD5\u3092\u4F7F\u7528\u3057\u3066\
          \u30DE\u30B9\u30AF\u3057\u305F\u30C8\u30FC\u30AF\u30F3\u3092\u9069\u5207\
          \u306B\u4E88\u6E2C\u3067\u304D\u3066\u3044\u306A\u3044\u3088\u3046\u306B\
          \u601D\u3048\u307E\u3059\u3002\n\n\u3042\u3089\u304B\u3058\u3081\u3054\u5354\
          \u529B\u3044\u305F\u3060\u304D\u3042\u308A\u304C\u3068\u3046\u3054\u3056\
          \u3044\u307E\u3059\u3002\n\n\u3088\u308D\u3057\u304F\u304A\u9858\u3044\u3044\
          \u305F\u3057\u307E\u3059\u3001\n\nyotti0618"
        updatedAt: '2023-06-05T10:20:43.650Z'
      numEdits: 1
      reactions: []
    id: 647db6790ed7d0c876094a8d
    type: comment
  author: yotti0618
  content: "\u3053\u3093\u306B\u3061\u306F\u3001Hugging Face\u306E\u30B3\u30DF\u30E5\
    \u30CB\u30C6\u30A3\u306E\u7686\u3055\u3093\u3002\n\n\u79C1\u306F\u300Cstudio-ousia/luke-japanese-large\u300D\
    \u306ELUKE Japanese Large\u30E2\u30C7\u30EB\u3092\u4F7F\u7528\u3057\u3066\u3044\
    \u3066\u3001\u30C8\u30FC\u30AF\u30F3\u3092\u30DE\u30B9\u30AF\u3057\u3066\u4E88\
    \u6E2C\u3059\u308B\u969B\u306B\u554F\u984C\u306B\u76F4\u9762\u3057\u307E\u3057\
    \u305F\u3002\n\n\u4EE5\u4E0B\u306B\u4F7F\u7528\u3057\u305F\u30B3\u30FC\u30C9\u3092\
    \u793A\u3057\u307E\u3059\uFF1A\n\n```python\nfrom transformers import AutoTokenizer,\
    \ AutoModelForMaskedLM\nimport torch\n\n# Define the model\nMODEL_NAME = \"studio-ousia/luke-japanese-large\"\
    \n\n# Load the pre-trained model's tokenizer (vocabulary)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\
    \n# Tokenize the input\ntext = \"\u4EBA\u9593\u3068\u30ED\u30DC\u30C3\u30C8\u306F\
    \u9577\u3089\u304F\u4EBA\u751F\u3092\u4E00\u7DD2\u306B\u904E\u3054\u3057\u3066\
    \u304D\u305F\u306E\u3067\u3042\u308B\u3002\"\ntokenized_text = tokenizer.tokenize(text)\n\
    \n# Mask a token for prediction using BertForMaskedLM\nmasked_index = 2\ntokenized_text[masked_index]\
    \ = '[MASK]'\n\n# Convert the tokens into vocabulary indices\nindexed_tokens =\
    \ tokenizer.convert_tokens_to_ids(tokenized_text)\n\n# Convert the input into\
    \ PyTorch tensor\ntokens_tensor = torch.tensor([indexed_tokens]).to(\"cuda\")\n\
    \n# Load the pre-trained model (weights)\nmodel = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n\
    model.eval()\nmodel.to(\"cuda\")\n\n# Predict all tokens\nwith torch.no_grad():\n\
    \    outputs = model(tokens_tensor)\n    predictions = outputs[0]\n\n# Confirm\
    \ that 'S' is predicted\npredicted_index = torch.argmax(predictions[0, masked_index]).item()\n\
    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n\nprint(predicted_token)\n\
    print(predicted_index)\n```\n\n\u5F97\u3089\u308C\u305F\u51FA\u529B\u306F'''<\
    \ \"unk \">\u3068 3 '''\u3067\u3001\u30DE\u30B9\u30AF\u3057\u305F\u4F4D\u7F6E\u306B\
    \u5BFE\u3057\u3066\u306F\u3042\u307E\u308A\u610F\u5473\u304C\u306A\u3044\u3088\
    \u3046\u306B\u601D\u308F\u308C\u307E\u3057\u305F\u3002\n\n\u79C1\u304C\u4F55\u304B\
    \u898B\u843D\u3068\u3057\u3066\u3044\u308B\u70B9\u3084\u3001\u3053\u306E\u7D50\
    \u679C\u3092\u6539\u5584\u3059\u308B\u305F\u3081\u306E\u63D0\u6848\u306F\u3042\
    \u308A\u307E\u3059\u304B\uFF1F \u3053\u306E\u65B9\u6CD5\u3092\u4F7F\u7528\u3057\
    \u3066\u30DE\u30B9\u30AF\u3057\u305F\u30C8\u30FC\u30AF\u30F3\u3092\u9069\u5207\
    \u306B\u4E88\u6E2C\u3067\u304D\u3066\u3044\u306A\u3044\u3088\u3046\u306B\u601D\
    \u3048\u307E\u3059\u3002\n\n\u3042\u3089\u304B\u3058\u3081\u3054\u5354\u529B\u3044\
    \u305F\u3060\u304D\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\u3059\
    \u3002\n\n\u3088\u308D\u3057\u304F\u304A\u9858\u3044\u3044\u305F\u3057\u307E\u3059\
    \u3001\n\nyotti0618"
  created_at: 2023-06-05 09:18:33+00:00
  edited: true
  hidden: false
  id: 647db6790ed7d0c876094a8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
      fullname: Ryokan Ri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ryo0634
      type: user
    createdAt: '2023-06-11T14:12:49.000Z'
    data:
      edited: false
      editors:
      - ryo0634
      hidden: false
      identifiedLanguage:
        language: ja
        probability: 0.8723044991493225
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
          fullname: Ryokan Ri
          isHf: false
          isPro: false
          name: ryo0634
          type: user
        html: "<p>\u3053\u3093\u306B\u3061\u306F\u3002<br>\u4E0A\u8A18\u306E\u30E2\
          \u30C7\u30EB\u306E\u6319\u52D5\u306F\u3001\uFF12\u70B9\u4FEE\u6B63\u3059\
          \u308B\u3053\u3068\u3067\u6539\u5584\u3055\u308C\u307E\u3059\u3002</p>\n\
          <ol>\n<li><p>\u30DE\u30B9\u30AF\u306E\u30C8\u30FC\u30AF\u30F3\u3092 <code>[MASK]</code>\
          \ \u3067\u306F\u306A\u304F\u3001<code>&lt;mask&gt;</code> \u306B\u7F6E\u304D\
          \u63DB\u3048\u308B<br>\u30E2\u30C7\u30EB\u306B\u3088\u3063\u3066\u3001\u30DE\
          \u30B9\u30AF\u306E\u30C8\u30FC\u30AF\u30F3\u304C\u7570\u306A\u308B\u5834\
          \u5408\u304C\u3042\u308B\u306E\u3067\u3001\u3054\u6CE8\u610F\u304F\u3060\
          \u3055\u3044\u3002<br>\u30DE\u30B9\u30AF\u306E\u30C8\u30FC\u30AF\u30F3\u306F\
          \ <code>tokenizer.mask_token</code> \u3067\u53D6\u5F97\u3059\u308B\u3053\
          \u3068\u3082\u3067\u304D\u307E\u3059\u3002</p>\n</li>\n<li><p>\u5165\u529B\
          \u306E\u5148\u982D\u3068\u672B\u5C3E\u306B\u7279\u6B8A\u30C8\u30FC\u30AF\
          \u30F3\u3092\u52A0\u3048\u308B\u3002<br>\u30E2\u30C7\u30EB\u304C\u5165\u529B\
          \u306B\u7279\u6B8A\u30C8\u30FC\u30AF\u30F3\u304C\u4ED8\u52A0\u3055\u308C\
          \u3066\u3044\u308B\u72B6\u6CC1\u3067\u5B66\u7FD2\u3055\u308C\u3066\u3044\
          \u308B\u306E\u3067\u3001\u4F7F\u7528\u3059\u308B\u5834\u5408\u3082\u7279\
          \u6B8A\u30C8\u30FC\u30AF\u30F3\u304C\u5FC5\u8981\u306B\u306A\u308A\u307E\
          \u3059\u3002<br>\u5148\u982D\u306E\u7279\u6B8A\u30C8\u30FC\u30AF\u30F3\u306F\
          \ <code>tokenizer.cls_token</code>\u3001\u672B\u5C3E\u306E\u7279\u6B8A\u30C8\
          \u30FC\u30AF\u30F3\u306F <code>tokenizer.sep_token</code> \u3067\u78BA\u8A8D\
          \u3067\u304D\u307E\u3059\u3002</p>\n</li>\n</ol>\n<p>\u4EE5\u4E0B\u306E\u30B3\
          \u30FC\u30C9\u306F\u3001<code>tokenizer</code> \u306B\u30C6\u30AD\u30B9\u30C8\
          \u3092\u6E21\u3059\u3053\u3068\u3067\u3001\u81EA\u52D5\u3067\u7279\u6B8A\
          \u30C8\u30FC\u30AF\u30F3\u3092\u5148\u982D\u3068\u672B\u5C3E\u306B\u4ED8\
          \u52A0\u3057\u3066\u3044\u307E\u3059\u3002<br>\u4E88\u6E2C\u7D50\u679C\u3082\
          \u610F\u5473\u304C\u3042\u308B\u3082\u306E\u306B\u306A\u308A\u307E\u3057\
          \u305F\u306E\u3067\u3001\u3054\u53C2\u8003\u306B\u306A\u308C\u3070\u5E78\
          \u3044\u3067\u3059\u3002</p>\n<pre><code>from transformers import AutoTokenizer,\
          \ AutoModelForMaskedLM\nimport torch\n\n# Define the model\nMODEL_NAME =\
          \ \"studio-ousia/luke-japanese-large\"\n\n# Load the pre-trained model's\
          \ tokenizer (vocabulary)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\
          \n# Tokenize the input\ntext = \"\u4EBA\u9593\u3068\u30ED\u30DC\u30C3\u30C8\
          \u306F\u9577\u3089\u304F\u4EBA\u751F\u3092\u4E00\u7DD2\u306B\u904E\u3054\
          \u3057\u3066\u304D\u305F\u306E\u3067\u3042\u308B\u3002\"\n\n# Convert the\
          \ input into PyTorch tensor\nencoded_text = tokenizer(text, return_tensors=\"\
          pt\")\nmasked_index = 3\nencoded_text[\"input_ids\"][0][masked_index] =\
          \ tokenizer.mask_token_id\n\nprint(tokenizer.convert_ids_to_tokens(encoded_text[\"\
          input_ids\"][0]))\n\nencoded_text = encoded_text.to(\"cuda\")\n\n# Load\
          \ the pre-trained model (weights)\nmodel = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n\
          model.eval()\nmodel.to(\"cuda\")\n\n\n# Predict all tokens\nwith torch.no_grad():\n\
          \    outputs = model(**encoded_text)\n    predictions = outputs[0]\n\n#\
          \ Confirm that 'S' is predicted\npredicted_index = torch.argmax(predictions[0,\
          \ masked_index]).item()\npredicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n\
          \nprint(predicted_token)\nprint(predicted_index)\n</code></pre>\n<p>\u51FA\
          \u529B</p>\n<pre><code>['&lt;s&gt;', '\u2581\u4EBA\u9593', '\u3068', '&lt;mask&gt;',\
          \ '\u306F', '\u9577', '\u3089\u304F', '\u4EBA\u751F\u3092', '\u4E00\u7DD2\
          \u306B', '\u904E\u3054', '\u3057\u3066\u304D\u305F', '\u306E\u3067\u3042\
          \u308B', '\u3002', '&lt;/s&gt;']\n\u52D5\u7269\n2661\n</code></pre>\n"
        raw: "\u3053\u3093\u306B\u3061\u306F\u3002\n\u4E0A\u8A18\u306E\u30E2\u30C7\
          \u30EB\u306E\u6319\u52D5\u306F\u3001\uFF12\u70B9\u4FEE\u6B63\u3059\u308B\
          \u3053\u3068\u3067\u6539\u5584\u3055\u308C\u307E\u3059\u3002\n\n1. \u30DE\
          \u30B9\u30AF\u306E\u30C8\u30FC\u30AF\u30F3\u3092 `[MASK]` \u3067\u306F\u306A\
          \u304F\u3001`<mask>` \u306B\u7F6E\u304D\u63DB\u3048\u308B\n\u30E2\u30C7\u30EB\
          \u306B\u3088\u3063\u3066\u3001\u30DE\u30B9\u30AF\u306E\u30C8\u30FC\u30AF\
          \u30F3\u304C\u7570\u306A\u308B\u5834\u5408\u304C\u3042\u308B\u306E\u3067\
          \u3001\u3054\u6CE8\u610F\u304F\u3060\u3055\u3044\u3002\n\u30DE\u30B9\u30AF\
          \u306E\u30C8\u30FC\u30AF\u30F3\u306F `tokenizer.mask_token` \u3067\u53D6\
          \u5F97\u3059\u308B\u3053\u3068\u3082\u3067\u304D\u307E\u3059\u3002\n\n2.\
          \ \u5165\u529B\u306E\u5148\u982D\u3068\u672B\u5C3E\u306B\u7279\u6B8A\u30C8\
          \u30FC\u30AF\u30F3\u3092\u52A0\u3048\u308B\u3002\n\u30E2\u30C7\u30EB\u304C\
          \u5165\u529B\u306B\u7279\u6B8A\u30C8\u30FC\u30AF\u30F3\u304C\u4ED8\u52A0\
          \u3055\u308C\u3066\u3044\u308B\u72B6\u6CC1\u3067\u5B66\u7FD2\u3055\u308C\
          \u3066\u3044\u308B\u306E\u3067\u3001\u4F7F\u7528\u3059\u308B\u5834\u5408\
          \u3082\u7279\u6B8A\u30C8\u30FC\u30AF\u30F3\u304C\u5FC5\u8981\u306B\u306A\
          \u308A\u307E\u3059\u3002\n\u5148\u982D\u306E\u7279\u6B8A\u30C8\u30FC\u30AF\
          \u30F3\u306F `tokenizer.cls_token`\u3001\u672B\u5C3E\u306E\u7279\u6B8A\u30C8\
          \u30FC\u30AF\u30F3\u306F `tokenizer.sep_token` \u3067\u78BA\u8A8D\u3067\u304D\
          \u307E\u3059\u3002\n\n\u4EE5\u4E0B\u306E\u30B3\u30FC\u30C9\u306F\u3001`tokenizer`\
          \ \u306B\u30C6\u30AD\u30B9\u30C8\u3092\u6E21\u3059\u3053\u3068\u3067\u3001\
          \u81EA\u52D5\u3067\u7279\u6B8A\u30C8\u30FC\u30AF\u30F3\u3092\u5148\u982D\
          \u3068\u672B\u5C3E\u306B\u4ED8\u52A0\u3057\u3066\u3044\u307E\u3059\u3002\
          \n\u4E88\u6E2C\u7D50\u679C\u3082\u610F\u5473\u304C\u3042\u308B\u3082\u306E\
          \u306B\u306A\u308A\u307E\u3057\u305F\u306E\u3067\u3001\u3054\u53C2\u8003\
          \u306B\u306A\u308C\u3070\u5E78\u3044\u3067\u3059\u3002\n\n```\nfrom transformers\
          \ import AutoTokenizer, AutoModelForMaskedLM\nimport torch\n\n# Define the\
          \ model\nMODEL_NAME = \"studio-ousia/luke-japanese-large\"\n\n# Load the\
          \ pre-trained model's tokenizer (vocabulary)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\
          \n# Tokenize the input\ntext = \"\u4EBA\u9593\u3068\u30ED\u30DC\u30C3\u30C8\
          \u306F\u9577\u3089\u304F\u4EBA\u751F\u3092\u4E00\u7DD2\u306B\u904E\u3054\
          \u3057\u3066\u304D\u305F\u306E\u3067\u3042\u308B\u3002\"\n\n# Convert the\
          \ input into PyTorch tensor\nencoded_text = tokenizer(text, return_tensors=\"\
          pt\")\nmasked_index = 3\nencoded_text[\"input_ids\"][0][masked_index] =\
          \ tokenizer.mask_token_id\n\nprint(tokenizer.convert_ids_to_tokens(encoded_text[\"\
          input_ids\"][0]))\n\nencoded_text = encoded_text.to(\"cuda\")\n\n# Load\
          \ the pre-trained model (weights)\nmodel = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n\
          model.eval()\nmodel.to(\"cuda\")\n\n\n# Predict all tokens\nwith torch.no_grad():\n\
          \    outputs = model(**encoded_text)\n    predictions = outputs[0]\n\n#\
          \ Confirm that 'S' is predicted\npredicted_index = torch.argmax(predictions[0,\
          \ masked_index]).item()\npredicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n\
          \nprint(predicted_token)\nprint(predicted_index)\n```\n\n\u51FA\u529B\n\
          ```\n['<s>', '\u2581\u4EBA\u9593', '\u3068', '<mask>', '\u306F', '\u9577\
          ', '\u3089\u304F', '\u4EBA\u751F\u3092', '\u4E00\u7DD2\u306B', '\u904E\u3054\
          ', '\u3057\u3066\u304D\u305F', '\u306E\u3067\u3042\u308B', '\u3002', '</s>']\n\
          \u52D5\u7269\n2661\n```"
        updatedAt: '2023-06-11T14:12:49.405Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - yotti0618
    id: 6485d661c95cd0f92383db8b
    type: comment
  author: ryo0634
  content: "\u3053\u3093\u306B\u3061\u306F\u3002\n\u4E0A\u8A18\u306E\u30E2\u30C7\u30EB\
    \u306E\u6319\u52D5\u306F\u3001\uFF12\u70B9\u4FEE\u6B63\u3059\u308B\u3053\u3068\
    \u3067\u6539\u5584\u3055\u308C\u307E\u3059\u3002\n\n1. \u30DE\u30B9\u30AF\u306E\
    \u30C8\u30FC\u30AF\u30F3\u3092 `[MASK]` \u3067\u306F\u306A\u304F\u3001`<mask>`\
    \ \u306B\u7F6E\u304D\u63DB\u3048\u308B\n\u30E2\u30C7\u30EB\u306B\u3088\u3063\u3066\
    \u3001\u30DE\u30B9\u30AF\u306E\u30C8\u30FC\u30AF\u30F3\u304C\u7570\u306A\u308B\
    \u5834\u5408\u304C\u3042\u308B\u306E\u3067\u3001\u3054\u6CE8\u610F\u304F\u3060\
    \u3055\u3044\u3002\n\u30DE\u30B9\u30AF\u306E\u30C8\u30FC\u30AF\u30F3\u306F `tokenizer.mask_token`\
    \ \u3067\u53D6\u5F97\u3059\u308B\u3053\u3068\u3082\u3067\u304D\u307E\u3059\u3002\
    \n\n2. \u5165\u529B\u306E\u5148\u982D\u3068\u672B\u5C3E\u306B\u7279\u6B8A\u30C8\
    \u30FC\u30AF\u30F3\u3092\u52A0\u3048\u308B\u3002\n\u30E2\u30C7\u30EB\u304C\u5165\
    \u529B\u306B\u7279\u6B8A\u30C8\u30FC\u30AF\u30F3\u304C\u4ED8\u52A0\u3055\u308C\
    \u3066\u3044\u308B\u72B6\u6CC1\u3067\u5B66\u7FD2\u3055\u308C\u3066\u3044\u308B\
    \u306E\u3067\u3001\u4F7F\u7528\u3059\u308B\u5834\u5408\u3082\u7279\u6B8A\u30C8\
    \u30FC\u30AF\u30F3\u304C\u5FC5\u8981\u306B\u306A\u308A\u307E\u3059\u3002\n\u5148\
    \u982D\u306E\u7279\u6B8A\u30C8\u30FC\u30AF\u30F3\u306F `tokenizer.cls_token`\u3001\
    \u672B\u5C3E\u306E\u7279\u6B8A\u30C8\u30FC\u30AF\u30F3\u306F `tokenizer.sep_token`\
    \ \u3067\u78BA\u8A8D\u3067\u304D\u307E\u3059\u3002\n\n\u4EE5\u4E0B\u306E\u30B3\
    \u30FC\u30C9\u306F\u3001`tokenizer` \u306B\u30C6\u30AD\u30B9\u30C8\u3092\u6E21\
    \u3059\u3053\u3068\u3067\u3001\u81EA\u52D5\u3067\u7279\u6B8A\u30C8\u30FC\u30AF\
    \u30F3\u3092\u5148\u982D\u3068\u672B\u5C3E\u306B\u4ED8\u52A0\u3057\u3066\u3044\
    \u307E\u3059\u3002\n\u4E88\u6E2C\u7D50\u679C\u3082\u610F\u5473\u304C\u3042\u308B\
    \u3082\u306E\u306B\u306A\u308A\u307E\u3057\u305F\u306E\u3067\u3001\u3054\u53C2\
    \u8003\u306B\u306A\u308C\u3070\u5E78\u3044\u3067\u3059\u3002\n\n```\nfrom transformers\
    \ import AutoTokenizer, AutoModelForMaskedLM\nimport torch\n\n# Define the model\n\
    MODEL_NAME = \"studio-ousia/luke-japanese-large\"\n\n# Load the pre-trained model's\
    \ tokenizer (vocabulary)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\
    \n# Tokenize the input\ntext = \"\u4EBA\u9593\u3068\u30ED\u30DC\u30C3\u30C8\u306F\
    \u9577\u3089\u304F\u4EBA\u751F\u3092\u4E00\u7DD2\u306B\u904E\u3054\u3057\u3066\
    \u304D\u305F\u306E\u3067\u3042\u308B\u3002\"\n\n# Convert the input into PyTorch\
    \ tensor\nencoded_text = tokenizer(text, return_tensors=\"pt\")\nmasked_index\
    \ = 3\nencoded_text[\"input_ids\"][0][masked_index] = tokenizer.mask_token_id\n\
    \nprint(tokenizer.convert_ids_to_tokens(encoded_text[\"input_ids\"][0]))\n\nencoded_text\
    \ = encoded_text.to(\"cuda\")\n\n# Load the pre-trained model (weights)\nmodel\
    \ = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\nmodel.eval()\nmodel.to(\"\
    cuda\")\n\n\n# Predict all tokens\nwith torch.no_grad():\n    outputs = model(**encoded_text)\n\
    \    predictions = outputs[0]\n\n# Confirm that 'S' is predicted\npredicted_index\
    \ = torch.argmax(predictions[0, masked_index]).item()\npredicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n\
    \nprint(predicted_token)\nprint(predicted_index)\n```\n\n\u51FA\u529B\n```\n['<s>',\
    \ '\u2581\u4EBA\u9593', '\u3068', '<mask>', '\u306F', '\u9577', '\u3089\u304F\
    ', '\u4EBA\u751F\u3092', '\u4E00\u7DD2\u306B', '\u904E\u3054', '\u3057\u3066\u304D\
    \u305F', '\u306E\u3067\u3042\u308B', '\u3002', '</s>']\n\u52D5\u7269\n2661\n```"
  created_at: 2023-06-11 13:12:49+00:00
  edited: false
  hidden: false
  id: 6485d661c95cd0f92383db8b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/05f7605620a04257c5cde19263bbbdd9.svg
      fullname: "\u5DDD\u897F\u6176\u5C1A"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yotti0618
      type: user
    createdAt: '2023-06-22T15:55:45.000Z'
    data:
      edited: false
      editors:
      - yotti0618
      hidden: false
      identifiedLanguage:
        language: ja
        probability: 0.9998341798782349
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/05f7605620a04257c5cde19263bbbdd9.svg
          fullname: "\u5DDD\u897F\u6176\u5C1A"
          isHf: false
          isPro: false
          name: yotti0618
          type: user
        html: "<p>\u3053\u3093\u3070\u3093\u306F\u3002\u78BA\u8A8D\u3067\u304D\u305F\
          \u307E\u3057\u305F\u3002\u30E2\u30C7\u30EB\u306E\u5B66\u7FD2\u65B9\u6CD5\
          \u306B\u5408\u308F\u305B\u308B\u3053\u3068\u304C\u91CD\u8981\u3060\u3063\
          \u305F\u3093\u3067\u3059\u306D\u3002<br>\u81EA\u52D5\u3067\u7279\u6B8A\u30C8\
          \u30FC\u30AF\u30F3\u3092\u4ED8\u52A0\u3059\u308B\u65B9\u6CD5\u307E\u3067\
          \u6559\u3048\u3066\u3044\u305F\u3060\u304D\u3001\u3042\u308A\u304C\u3068\
          \u3046\u3054\u3056\u3044\u307E\u3059\u3002</p>\n<p>\u5148\u982D\u306E\u7279\
          \u6B8A\u30C8\u30FC\u30AF\u30F3\u306F tokenizer.cls_token\u3001\u672B\u5C3E\
          \u306E\u7279\u6B8A\u30C8\u30FC\u30AF\u30F3\u306F tokenizer.sep_token \u3001\
          \u30DE\u30B9\u30AF\u306E\u30C8\u30FC\u30AF\u30F3\u306F tokenizer.mask_token\u3067\
          \u78BA\u8A8D\u3067\u304D\u308B\u3053\u3068\u304C\u3067\u304D\u308B\u3053\
          \u3068\u3082\u77E5\u308C\u3066\u3068\u3066\u3082\u826F\u304B\u3063\u305F\
          \u3067\u3059\u3002</p>\n<p>Luke\u3092\u4F7F\u3063\u3066\u3001\u7814\u7A76\
          \u3092\u9032\u3081\u3066\u3044\u304D\u305F\u3044\u3068\u601D\u3044\u307E\
          \u3059\u3002</p>\n<p>\u3054\u5354\u529B\u3044\u305F\u3060\u304D\u3001\u3042\
          \u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\u3059\u3002</p>\n"
        raw: "\u3053\u3093\u3070\u3093\u306F\u3002\u78BA\u8A8D\u3067\u304D\u305F\u307E\
          \u3057\u305F\u3002\u30E2\u30C7\u30EB\u306E\u5B66\u7FD2\u65B9\u6CD5\u306B\
          \u5408\u308F\u305B\u308B\u3053\u3068\u304C\u91CD\u8981\u3060\u3063\u305F\
          \u3093\u3067\u3059\u306D\u3002\n\u81EA\u52D5\u3067\u7279\u6B8A\u30C8\u30FC\
          \u30AF\u30F3\u3092\u4ED8\u52A0\u3059\u308B\u65B9\u6CD5\u307E\u3067\u6559\
          \u3048\u3066\u3044\u305F\u3060\u304D\u3001\u3042\u308A\u304C\u3068\u3046\
          \u3054\u3056\u3044\u307E\u3059\u3002\n\n\u5148\u982D\u306E\u7279\u6B8A\u30C8\
          \u30FC\u30AF\u30F3\u306F tokenizer.cls_token\u3001\u672B\u5C3E\u306E\u7279\
          \u6B8A\u30C8\u30FC\u30AF\u30F3\u306F tokenizer.sep_token \u3001\u30DE\u30B9\
          \u30AF\u306E\u30C8\u30FC\u30AF\u30F3\u306F tokenizer.mask_token\u3067\u78BA\
          \u8A8D\u3067\u304D\u308B\u3053\u3068\u304C\u3067\u304D\u308B\u3053\u3068\
          \u3082\u77E5\u308C\u3066\u3068\u3066\u3082\u826F\u304B\u3063\u305F\u3067\
          \u3059\u3002\n\nLuke\u3092\u4F7F\u3063\u3066\u3001\u7814\u7A76\u3092\u9032\
          \u3081\u3066\u3044\u304D\u305F\u3044\u3068\u601D\u3044\u307E\u3059\u3002\
          \n\n\u3054\u5354\u529B\u3044\u305F\u3060\u304D\u3001\u3042\u308A\u304C\u3068\
          \u3046\u3054\u3056\u3044\u307E\u3059\u3002"
        updatedAt: '2023-06-22T15:55:45.219Z'
      numEdits: 0
      reactions: []
    id: 64946f018ca8542df07392c7
    type: comment
  author: yotti0618
  content: "\u3053\u3093\u3070\u3093\u306F\u3002\u78BA\u8A8D\u3067\u304D\u305F\u307E\
    \u3057\u305F\u3002\u30E2\u30C7\u30EB\u306E\u5B66\u7FD2\u65B9\u6CD5\u306B\u5408\
    \u308F\u305B\u308B\u3053\u3068\u304C\u91CD\u8981\u3060\u3063\u305F\u3093\u3067\
    \u3059\u306D\u3002\n\u81EA\u52D5\u3067\u7279\u6B8A\u30C8\u30FC\u30AF\u30F3\u3092\
    \u4ED8\u52A0\u3059\u308B\u65B9\u6CD5\u307E\u3067\u6559\u3048\u3066\u3044\u305F\
    \u3060\u304D\u3001\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\u3059\
    \u3002\n\n\u5148\u982D\u306E\u7279\u6B8A\u30C8\u30FC\u30AF\u30F3\u306F tokenizer.cls_token\u3001\
    \u672B\u5C3E\u306E\u7279\u6B8A\u30C8\u30FC\u30AF\u30F3\u306F tokenizer.sep_token\
    \ \u3001\u30DE\u30B9\u30AF\u306E\u30C8\u30FC\u30AF\u30F3\u306F tokenizer.mask_token\u3067\
    \u78BA\u8A8D\u3067\u304D\u308B\u3053\u3068\u304C\u3067\u304D\u308B\u3053\u3068\
    \u3082\u77E5\u308C\u3066\u3068\u3066\u3082\u826F\u304B\u3063\u305F\u3067\u3059\
    \u3002\n\nLuke\u3092\u4F7F\u3063\u3066\u3001\u7814\u7A76\u3092\u9032\u3081\u3066\
    \u3044\u304D\u305F\u3044\u3068\u601D\u3044\u307E\u3059\u3002\n\n\u3054\u5354\u529B\
    \u3044\u305F\u3060\u304D\u3001\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\
    \u307E\u3059\u3002"
  created_at: 2023-06-22 14:55:45+00:00
  edited: false
  hidden: false
  id: 64946f018ca8542df07392c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ff1cdf6b1cf5890b8daba082b15466a7.svg
      fullname: Ryokan Ri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ryo0634
      type: user
    createdAt: '2023-07-28T14:03:08.000Z'
    data:
      status: closed
    id: 64c3ca9c17bd806045fa3a37
    type: status-change
  author: ryo0634
  created_at: 2023-07-28 13:03:08+00:00
  id: 64c3ca9c17bd806045fa3a37
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: studio-ousia/luke-japanese-large
repo_type: model
status: closed
target_branch: null
title: "LUKE Japanese Large\u30E2\u30C7\u30EB\uFF1A\u30C8\u30FC\u30AF\u30F3\u306E\u30DE\
  \u30B9\u30AD\u30F3\u30B0\u3068\u4E88\u6E2C\u6642\u306B\u4E88\u60F3\u5916\u306E\u51FA\
  \u529B"
