!!python/object:huggingface_hub.community.DiscussionWithDetails
author: odellus
conflicting_files: null
created_at: 2023-12-09 14:58:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
      fullname: Thomas Wood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: odellus
      type: user
    createdAt: '2023-12-09T14:58:51.000Z'
    data:
      edited: true
      editors:
      - odellus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.37053921818733215
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
          fullname: Thomas Wood
          isHf: false
          isPro: true
          name: odellus
          type: user
        html: '<p>Not able to get <code>tokenizer.apply_chat_template</code> to append
          the generation prompt for stablelm-zephyr-3b</p>

          <pre><code class="language-python"><span class="hljs-built_in">print</span>(tokenizer.chat_template)

          <span class="hljs-string">"{% for message in messages %}\n{% if message[''role'']
          == ''user'' %}\n{{ ''&lt;|user|&gt;\n'' + message[''content''] + eos_token
          }}\n{% elif message[''role''] == ''system'' %}\n{{ ''&lt;|system|&gt;\n''
          + message[''content''] + eos_token }}\n{% elif message[''role''] == ''assistant''
          %}\n{{ ''&lt;|assistant|&gt;\n''  + message[''content''] + eos_token }}\n{%
          endif %}\n{% if loop.last and add_generation_prompt %}\n{{ ''&lt;|assistant|&gt;''
          }}\n{% endif %}\n{% endfor %}"</span>


          chat = [{<span class="hljs-string">''role''</span>: <span class="hljs-string">''system''</span>,
          <span class="hljs-string">''content''</span>: <span class="hljs-string">''You
          are an excellent C++ programmer''</span>}, {<span class="hljs-string">''role''</span>:
          <span class="hljs-string">''user''</span>, <span class="hljs-string">''content''</span>:
          <span class="hljs-string">''Write a program to compute pairwise distances
          between atoms in a PDB file''</span>}]


          tokenizer.apply_chat_template(chat, tokenize=<span class="hljs-literal">False</span>,
          add_generation_prompt=<span class="hljs-literal">True</span>)

          <span class="hljs-string">''&lt;|system|&gt;\nYou are an excellent C++ programmer&lt;|endoftext|&gt;\n&lt;|user|&gt;\nWrite
          a program to compute pairwise distances between atoms in a PDB file&lt;|endoftext|&gt;\n''</span>


          tokenizer.apply_chat_template(chat, tokenize=<span class="hljs-literal">False</span>,
          add_generation_prompt=<span class="hljs-literal">False</span>)

          <span class="hljs-string">''&lt;|system|&gt;\nYou are an excellent C++ programmer&lt;|endoftext|&gt;\n&lt;|user|&gt;\nWrite
          a program to compute pairwise distances between atoms in a PDB file&lt;|endoftext|&gt;\n''</span>

          </code></pre>

          <p>Could this be an issue with tokenizer module? The chat template looks
          right.</p>

          '
        raw: 'Not able to get `tokenizer.apply_chat_template` to append the generation
          prompt for stablelm-zephyr-3b


          ```python

          print(tokenizer.chat_template)

          "{% for message in messages %}\n{% if message[''role''] == ''user'' %}\n{{
          ''<|user|>\n'' + message[''content''] + eos_token }}\n{% elif message[''role'']
          == ''system'' %}\n{{ ''<|system|>\n'' + message[''content''] + eos_token
          }}\n{% elif message[''role''] == ''assistant'' %}\n{{ ''<|assistant|>\n''  +
          message[''content''] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt
          %}\n{{ ''<|assistant|>'' }}\n{% endif %}\n{% endfor %}"


          chat = [{''role'': ''system'', ''content'': ''You are an excellent C++ programmer''},
          {''role'': ''user'', ''content'': ''Write a program to compute pairwise
          distances between atoms in a PDB file''}]


          tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

          ''<|system|>\nYou are an excellent C++ programmer<|endoftext|>\n<|user|>\nWrite
          a program to compute pairwise distances between atoms in a PDB file<|endoftext|>\n''


          tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)

          ''<|system|>\nYou are an excellent C++ programmer<|endoftext|>\n<|user|>\nWrite
          a program to compute pairwise distances between atoms in a PDB file<|endoftext|>\n''

          ```


          Could this be an issue with tokenizer module? The chat template looks right.'
        updatedAt: '2023-12-09T15:05:42.057Z'
      numEdits: 1
      reactions: []
    id: 657480ab39831c6862803f97
    type: comment
  author: odellus
  content: 'Not able to get `tokenizer.apply_chat_template` to append the generation
    prompt for stablelm-zephyr-3b


    ```python

    print(tokenizer.chat_template)

    "{% for message in messages %}\n{% if message[''role''] == ''user'' %}\n{{ ''<|user|>\n''
    + message[''content''] + eos_token }}\n{% elif message[''role''] == ''system''
    %}\n{{ ''<|system|>\n'' + message[''content''] + eos_token }}\n{% elif message[''role'']
    == ''assistant'' %}\n{{ ''<|assistant|>\n''  + message[''content''] + eos_token
    }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ ''<|assistant|>''
    }}\n{% endif %}\n{% endfor %}"


    chat = [{''role'': ''system'', ''content'': ''You are an excellent C++ programmer''},
    {''role'': ''user'', ''content'': ''Write a program to compute pairwise distances
    between atoms in a PDB file''}]


    tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

    ''<|system|>\nYou are an excellent C++ programmer<|endoftext|>\n<|user|>\nWrite
    a program to compute pairwise distances between atoms in a PDB file<|endoftext|>\n''


    tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)

    ''<|system|>\nYou are an excellent C++ programmer<|endoftext|>\n<|user|>\nWrite
    a program to compute pairwise distances between atoms in a PDB file<|endoftext|>\n''

    ```


    Could this be an issue with tokenizer module? The chat template looks right.'
  created_at: 2023-12-09 14:58:51+00:00
  edited: true
  hidden: false
  id: 657480ab39831c6862803f97
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
      fullname: Thomas Wood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: odellus
      type: user
    createdAt: '2023-12-10T22:37:40.000Z'
    data:
      edited: false
      editors:
      - odellus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9544185996055603
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
          fullname: Thomas Wood
          isHf: false
          isPro: true
          name: odellus
          type: user
        html: '<p>Upgrading transformers fixed this issue for me. Closing.</p>

          '
        raw: Upgrading transformers fixed this issue for me. Closing.
        updatedAt: '2023-12-10T22:37:40.401Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65763db4177b3b46638240e6
    id: 65763db4177b3b46638240e5
    type: comment
  author: odellus
  content: Upgrading transformers fixed this issue for me. Closing.
  created_at: 2023-12-10 22:37:40+00:00
  edited: false
  hidden: false
  id: 65763db4177b3b46638240e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
      fullname: Thomas Wood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: odellus
      type: user
    createdAt: '2023-12-10T22:37:40.000Z'
    data:
      status: closed
    id: 65763db4177b3b46638240e6
    type: status-change
  author: odellus
  created_at: 2023-12-10 22:37:40+00:00
  id: 65763db4177b3b46638240e6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: stabilityai/stablelm-zephyr-3b
repo_type: model
status: closed
target_branch: null
title: apply_chat_template(add_generation_prompt=True) not working
