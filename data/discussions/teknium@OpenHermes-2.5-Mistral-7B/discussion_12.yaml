!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dyoung
conflicting_files: null
created_at: 2023-12-06 12:12:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2023-12-06T12:12:50.000Z'
    data:
      edited: true
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8147255182266235
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: "<p>Hello,</p>\n<p>I'm not getting a proper prompt string from huggingfaces\
          \ transformers AutoTokenizer apply_chat_template for this model when using\
          \ <code>add_generation_prompt=True</code> as a parameter to apply_chat_template.</p>\n\
          <p>Here is the code that causes what I'm seeing:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\nMODEL =  <span\
          \ class=\"hljs-string\">\"teknium/OpenHermes-2.5-Mistral-7B\"</span>\n\n\
          tokenizer = AutoTokenizer.from_pretrained(MODEL ,  skip_special_tokens=<span\
          \ class=\"hljs-literal\">True</span>, use_fast=<span class=\"hljs-literal\"\
          >True</span>)\n\ntest_msg = [\n        {<span class=\"hljs-string\">\"role\"\
          </span>: <span class=\"hljs-string\">\"system\"</span>, <span class=\"hljs-string\"\
          >\"content\"</span>: <span class=\"hljs-string\">\"You are Alex, a friendly\
          \ AI assistant.\"</span>},\n        {<span class=\"hljs-string\">\"role\"\
          </span>: <span class=\"hljs-string\">\"user\"</span>, <span class=\"hljs-string\"\
          >\"content\"</span>: <span class=\"hljs-string\">\"Hello, how are you?\"\
          </span>},\n        {<span class=\"hljs-string\">\"role\"</span>: <span class=\"\
          hljs-string\">\"assistant\"</span>, <span class=\"hljs-string\">\"content\"\
          </span>: <span class=\"hljs-string\">\"I'm doing great. How can I help you\
          \ today?\"</span>},\n        {<span class=\"hljs-string\">\"role\"</span>:\
          \ <span class=\"hljs-string\">\"user\"</span>, <span class=\"hljs-string\"\
          >\"content\"</span>: <span class=\"hljs-string\">\"Tell me a good joke!\
          \ Not a short one either.\"</span>},\n    ]\n\nprompt_str = tokenizer.apply_chat_template(test_msg,\
          \ tokenize=<span class=\"hljs-literal\">False</span>, add_generation_prompt=<span\
          \ class=\"hljs-literal\">True</span>)\n\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f'\"\"\"<span class=\"hljs-subst\">{prompt_str}</span>\"\
          \"\"'</span>)\n</code></pre>\n<p>Which outputs:</p>\n<pre><code class=\"\
          language-shell\">\"\"\"&lt;|im_start|&gt;system\nYou are Alex, a friendly\
          \ AI assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHello, how are\
          \ you?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nI'm doing great. How\
          \ can I help you today?&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nTell me\
          \ a good joke! Not a short one either.&lt;|im_end|&gt;\n\"\"\"\n</code></pre>\n\
          <p>Notice that its missing <code>\"&lt;|im_start|&gt;assistant\\n\"</code>\
          \ at the end. Which should be included as noted in the HF documentation:\
          \  <a href=\"https://huggingface.co/docs/transformers/main/chat_templating#what-are-generation-prompts\"\
          >https://huggingface.co/docs/transformers/main/chat_templating#what-are-generation-prompts</a></p>\n\
          <p>I can do a dirty fix by just appending what I need to the end of the\
          \ prompt string after calling apply_chat_template. But I feel that it would\
          \ be best that 'add_generation_prompt' worked as intended for this model.</p>\n\
          <p>I'm not sure if this is a tokenizer config template that is malformed\
          \ and accompanying this models repo files, or if it's something with HF's\
          \ transformer tokenizer library.</p>\n<p>I feel that I'm doing things properly\
          \ and the issue is likely not on my side. If not, I'm happy to hear some\
          \ feed back.</p>\n<p>If I'm doing things properly, I hope that me pointing\
          \ out this unexpected behavior helps in some way.</p>\n<p>Thanks for taking\
          \ time to read this.</p>\n"
        raw: "Hello,\n\nI'm not getting a proper prompt string from huggingfaces transformers\
          \ AutoTokenizer apply_chat_template for this model when using `add_generation_prompt=True`\
          \ as a parameter to apply_chat_template.\n\nHere is the code that causes\
          \ what I'm seeing:\n\n```python\nfrom transformers import AutoTokenizer\n\
          \nMODEL =  \"teknium/OpenHermes-2.5-Mistral-7B\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL\
          \ ,  skip_special_tokens=True, use_fast=True)\n\ntest_msg = [\n        {\"\
          role\": \"system\", \"content\": \"You are Alex, a friendly AI assistant.\"\
          },\n        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n\
          \        {\"role\": \"assistant\", \"content\": \"I'm doing great. How can\
          \ I help you today?\"},\n        {\"role\": \"user\", \"content\": \"Tell\
          \ me a good joke! Not a short one either.\"},\n    ]\n\nprompt_str = tokenizer.apply_chat_template(test_msg,\
          \ tokenize=False, add_generation_prompt=True)\n\nprint(f'\"\"\"{prompt_str}\"\
          \"\"')\n```\n\nWhich outputs:\n```shell\n\"\"\"<|im_start|>system\nYou are\
          \ Alex, a friendly AI assistant.<|im_end|>\n<|im_start|>user\nHello, how\
          \ are you?<|im_end|>\n<|im_start|>assistant\nI'm doing great. How can I\
          \ help you today?<|im_end|>\n<|im_start|>user\nTell me a good joke! Not\
          \ a short one either.<|im_end|>\n\"\"\"\n```\n\nNotice that its missing\
          \ `\"<|im_start|>assistant\\n\"` at the end. Which should be included as\
          \ noted in the HF documentation:  https://huggingface.co/docs/transformers/main/chat_templating#what-are-generation-prompts\n\
          \nI can do a dirty fix by just appending what I need to the end of the prompt\
          \ string after calling apply_chat_template. But I feel that it would be\
          \ best that 'add_generation_prompt' worked as intended for this model.\n\
          \nI'm not sure if this is a tokenizer config template that is malformed\
          \ and accompanying this models repo files, or if it's something with HF's\
          \ transformer tokenizer library.\n\nI feel that I'm doing things properly\
          \ and the issue is likely not on my side. If not, I'm happy to hear some\
          \ feed back.\n\nIf I'm doing things properly, I hope that me pointing out\
          \ this unexpected behavior helps in some way.\n\nThanks for taking time\
          \ to read this."
        updatedAt: '2023-12-06T12:40:42.077Z'
      numEdits: 1
      reactions: []
    id: 657065428489a9ee97dc64ac
    type: comment
  author: dyoung
  content: "Hello,\n\nI'm not getting a proper prompt string from huggingfaces transformers\
    \ AutoTokenizer apply_chat_template for this model when using `add_generation_prompt=True`\
    \ as a parameter to apply_chat_template.\n\nHere is the code that causes what\
    \ I'm seeing:\n\n```python\nfrom transformers import AutoTokenizer\n\nMODEL =\
    \  \"teknium/OpenHermes-2.5-Mistral-7B\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL\
    \ ,  skip_special_tokens=True, use_fast=True)\n\ntest_msg = [\n        {\"role\"\
    : \"system\", \"content\": \"You are Alex, a friendly AI assistant.\"},\n    \
    \    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n        {\"\
    role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"\
    },\n        {\"role\": \"user\", \"content\": \"Tell me a good joke! Not a short\
    \ one either.\"},\n    ]\n\nprompt_str = tokenizer.apply_chat_template(test_msg,\
    \ tokenize=False, add_generation_prompt=True)\n\nprint(f'\"\"\"{prompt_str}\"\"\
    \"')\n```\n\nWhich outputs:\n```shell\n\"\"\"<|im_start|>system\nYou are Alex,\
    \ a friendly AI assistant.<|im_end|>\n<|im_start|>user\nHello, how are you?<|im_end|>\n\
    <|im_start|>assistant\nI'm doing great. How can I help you today?<|im_end|>\n\
    <|im_start|>user\nTell me a good joke! Not a short one either.<|im_end|>\n\"\"\
    \"\n```\n\nNotice that its missing `\"<|im_start|>assistant\\n\"` at the end.\
    \ Which should be included as noted in the HF documentation:  https://huggingface.co/docs/transformers/main/chat_templating#what-are-generation-prompts\n\
    \nI can do a dirty fix by just appending what I need to the end of the prompt\
    \ string after calling apply_chat_template. But I feel that it would be best that\
    \ 'add_generation_prompt' worked as intended for this model.\n\nI'm not sure if\
    \ this is a tokenizer config template that is malformed and accompanying this\
    \ models repo files, or if it's something with HF's transformer tokenizer library.\n\
    \nI feel that I'm doing things properly and the issue is likely not on my side.\
    \ If not, I'm happy to hear some feed back.\n\nIf I'm doing things properly, I\
    \ hope that me pointing out this unexpected behavior helps in some way.\n\nThanks\
    \ for taking time to read this."
  created_at: 2023-12-06 12:12:50+00:00
  edited: true
  hidden: false
  id: 657065428489a9ee97dc64ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2023-12-06T12:35:55.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9772175550460815
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: '<p>I think I have an idea of what''s up. I''m running transformers
          4.43.0. 4.45.x is the current stable I think. Likely updating my transformers
          library will fix this. I''ll keep you all posted on how that goes.</p>

          '
        raw: I think I have an idea of what's up. I'm running transformers 4.43.0.
          4.45.x is the current stable I think. Likely updating my transformers library
          will fix this. I'll keep you all posted on how that goes.
        updatedAt: '2023-12-06T12:35:55.533Z'
      numEdits: 0
      reactions: []
    id: 65706aabb6ce26dd713c22f3
    type: comment
  author: dyoung
  content: I think I have an idea of what's up. I'm running transformers 4.43.0. 4.45.x
    is the current stable I think. Likely updating my transformers library will fix
    this. I'll keep you all posted on how that goes.
  created_at: 2023-12-06 12:35:55+00:00
  edited: false
  hidden: false
  id: 65706aabb6ce26dd713c22f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2023-12-06T12:47:12.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9553802013397217
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: '<p>Yep, that was the solution to the situation. So I''m good now.<br>Found
          this which helped: <a rel="nofollow" href="https://github.com/huggingface/transformers/issues/27370">https://github.com/huggingface/transformers/issues/27370</a><br>Would
          of been nice if I got a warning or raised exception from HF''s AutoTokenizer.apply_chat_template.
          I might say something to them about it.</p>

          '
        raw: 'Yep, that was the solution to the situation. So I''m good now.

          Found this which helped: https://github.com/huggingface/transformers/issues/27370

          Would of been nice if I got a warning or raised exception from HF''s AutoTokenizer.apply_chat_template.
          I might say something to them about it.'
        updatedAt: '2023-12-06T12:47:12.437Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65706d503100d8692141e918
    id: 65706d503100d8692141e916
    type: comment
  author: dyoung
  content: 'Yep, that was the solution to the situation. So I''m good now.

    Found this which helped: https://github.com/huggingface/transformers/issues/27370

    Would of been nice if I got a warning or raised exception from HF''s AutoTokenizer.apply_chat_template.
    I might say something to them about it.'
  created_at: 2023-12-06 12:47:12+00:00
  edited: false
  hidden: false
  id: 65706d503100d8692141e916
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2023-12-06T12:47:12.000Z'
    data:
      status: closed
    id: 65706d503100d8692141e918
    type: status-change
  author: dyoung
  created_at: 2023-12-06 12:47:12+00:00
  id: 65706d503100d8692141e918
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: teknium/OpenHermes-2.5-Mistral-7B
repo_type: model
status: closed
target_branch: null
title: Issue with HF transformer tokenizer apply chat template.
