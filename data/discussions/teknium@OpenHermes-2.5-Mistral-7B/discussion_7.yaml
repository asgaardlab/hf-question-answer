!!python/object:huggingface_hub.community.DiscussionWithDetails
author: banghua
conflicting_files: null
created_at: 2023-11-22 04:46:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/128ceae78490110ae41202851e84d58e.svg
      fullname: Banghua Zhu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: banghua
      type: user
    createdAt: '2023-11-22T04:46:18.000Z'
    data:
      edited: false
      editors:
      - banghua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9580195546150208
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/128ceae78490110ae41202851e84d58e.svg
          fullname: Banghua Zhu
          isHf: false
          isPro: false
          name: banghua
          type: user
        html: '<p>Hi, </p>

          <p>Thank you for the great work! I''m curious why the reported evaluation
          is very different than what is reported for Openchat 3.5 (<a href="https://huggingface.co/openchat/openchat_3.5">https://huggingface.co/openchat/openchat_3.5</a>)?
          It''s kind of interesting that in Openchat 3.5, they also compared with
          OpenHermes 2.5 and claimed that they are better. And I noticed that the
          scores reported for HumanEval, TruthfulQA etc. do not match on both sides..
          </p>

          '
        raw: "Hi, \r\n\r\nThank you for the great work! I'm curious why the reported\
          \ evaluation is very different than what is reported for Openchat 3.5 (https://huggingface.co/openchat/openchat_3.5)?\
          \ It's kind of interesting that in Openchat 3.5, they also compared with\
          \ OpenHermes 2.5 and claimed that they are better. And I noticed that the\
          \ scores reported for HumanEval, TruthfulQA etc. do not match on both sides.. "
        updatedAt: '2023-11-22T04:46:18.993Z'
      numEdits: 0
      reactions: []
    id: 655d879ac166f4c0a7a7f73f
    type: comment
  author: banghua
  content: "Hi, \r\n\r\nThank you for the great work! I'm curious why the reported\
    \ evaluation is very different than what is reported for Openchat 3.5 (https://huggingface.co/openchat/openchat_3.5)?\
    \ It's kind of interesting that in Openchat 3.5, they also compared with OpenHermes\
    \ 2.5 and claimed that they are better. And I noticed that the scores reported\
    \ for HumanEval, TruthfulQA etc. do not match on both sides.. "
  created_at: 2023-11-22 04:46:18+00:00
  edited: false
  hidden: false
  id: 655d879ac166f4c0a7a7f73f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-11-24T07:40:32.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9551982283592224
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<blockquote>

          <p>Hi, </p>

          <p>Thank you for the great work! I''m curious why the reported evaluation
          is very different than what is reported for Openchat 3.5 (<a href="https://huggingface.co/openchat/openchat_3.5">https://huggingface.co/openchat/openchat_3.5</a>)?
          It''s kind of interesting that in Openchat 3.5, they also compared with
          OpenHermes 2.5 and claimed that they are better. And I noticed that the
          scores reported for HumanEval, TruthfulQA etc. do not match on both sides..</p>

          </blockquote>

          <p>Openchat uses a proprietary unknown method to generate evaluation results,
          and does not use LM Eval Harness, so it''s impossible for me to say how
          the benchmark scores were derived</p>

          '
        raw: "> Hi, \n> \n> Thank you for the great work! I'm curious why the reported\
          \ evaluation is very different than what is reported for Openchat 3.5 (https://huggingface.co/openchat/openchat_3.5)?\
          \ It's kind of interesting that in Openchat 3.5, they also compared with\
          \ OpenHermes 2.5 and claimed that they are better. And I noticed that the\
          \ scores reported for HumanEval, TruthfulQA etc. do not match on both sides..\n\
          \nOpenchat uses a proprietary unknown method to generate evaluation results,\
          \ and does not use LM Eval Harness, so it's impossible for me to say how\
          \ the benchmark scores were derived"
        updatedAt: '2023-11-24T07:40:32.025Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - banghua
    id: 656053704309e401443bc0b0
    type: comment
  author: teknium
  content: "> Hi, \n> \n> Thank you for the great work! I'm curious why the reported\
    \ evaluation is very different than what is reported for Openchat 3.5 (https://huggingface.co/openchat/openchat_3.5)?\
    \ It's kind of interesting that in Openchat 3.5, they also compared with OpenHermes\
    \ 2.5 and claimed that they are better. And I noticed that the scores reported\
    \ for HumanEval, TruthfulQA etc. do not match on both sides..\n\nOpenchat uses\
    \ a proprietary unknown method to generate evaluation results, and does not use\
    \ LM Eval Harness, so it's impossible for me to say how the benchmark scores were\
    \ derived"
  created_at: 2023-11-24 07:40:32+00:00
  edited: false
  hidden: false
  id: 656053704309e401443bc0b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b6cbbdbfb266841ec0f24a/PHUVNOOMEw_R2CF3u-sMS.png?w=200&h=200&f=face
      fullname: One
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: imone
      type: user
    createdAt: '2023-11-24T13:55:34.000Z'
    data:
      edited: false
      editors:
      - imone
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5314629673957825
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b6cbbdbfb266841ec0f24a/PHUVNOOMEw_R2CF3u-sMS.png?w=200&h=200&f=face
          fullname: One
          isHf: false
          isPro: false
          name: imone
          type: user
        html: '<p>OpenChat evaluation results are run using zero-shot and few-shot
          CoT. For reproduction instructions, see <a rel="nofollow" href="https://github.com/imoneoi/openchat#%EF%B8%8F-benchmarks">here</a></p>

          <pre><code>python -m ochat.evaluation.run_eval --model-type chatml_mistral
          --model teknium/OpenHermes-2.5-Mistral-7B

          </code></pre>

          '
        raw: 'OpenChat evaluation results are run using zero-shot and few-shot CoT.
          For reproduction instructions, see [here](https://github.com/imoneoi/openchat#%EF%B8%8F-benchmarks)


          ```

          python -m ochat.evaluation.run_eval --model-type chatml_mistral --model
          teknium/OpenHermes-2.5-Mistral-7B

          ```'
        updatedAt: '2023-11-24T13:55:34.413Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - banghua
        - Wanfq
        - teknium
        - alpayariyak
    id: 6560ab562cd8dac56832f3bd
    type: comment
  author: imone
  content: 'OpenChat evaluation results are run using zero-shot and few-shot CoT.
    For reproduction instructions, see [here](https://github.com/imoneoi/openchat#%EF%B8%8F-benchmarks)


    ```

    python -m ochat.evaluation.run_eval --model-type chatml_mistral --model teknium/OpenHermes-2.5-Mistral-7B

    ```'
  created_at: 2023-11-24 13:55:34+00:00
  edited: false
  hidden: false
  id: 6560ab562cd8dac56832f3bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-11-25T06:03:13.000Z'
    data:
      status: closed
    id: 65618e215b395bcbf170ce7c
    type: status-change
  author: teknium
  created_at: 2023-11-25 06:03:13+00:00
  id: 65618e215b395bcbf170ce7c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: teknium/OpenHermes-2.5-Mistral-7B
repo_type: model
status: closed
target_branch: null
title: Inconsistent Eval Results with Openchat 3.5?
