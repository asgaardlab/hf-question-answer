!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ghaandy
conflicting_files: null
created_at: 2023-12-08 08:32:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ac3144638de6846773adddd1b9859a8.svg
      fullname: Tod Salomonsson Segerby
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ghaandy
      type: user
    createdAt: '2023-12-08T08:32:35.000Z'
    data:
      edited: true
      editors:
      - Ghaandy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7985281348228455
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ac3144638de6846773adddd1b9859a8.svg
          fullname: Tod Salomonsson Segerby
          isHf: false
          isPro: false
          name: Ghaandy
          type: user
        html: '<p>Hello!</p>

          <p>I came across this model and I must say, I''m very impressed with the
          results.<br>While examining the tokenizer_config.json file, I noticed that
          it specifies &lt; s &gt; as the BOS-token and add_bos_token:true.</p>

          <p>Although the model card mentions using the ChatML format, this raised
          some questions that I hope to receive assistance with.<br>How should &lt;
          s &gt;  be specified in the prompt/prompts when chatting with the model?</p>

          <p>Example 1 - Not at all.<br>"&lt;|im_start|&gt;User<br>Question 1?&lt;|im_end|&gt;<br>&lt;|im_start|&gt;Assistant<br>Answer
          1.&lt;|im_end|&gt;<br>&lt;|im_start|&gt;User<br>Question 2 based on question
          1?&lt;|im_end|&gt;<br>&lt;|im_start|&gt;Assistant"</p>

          <p>Example 2 - Before each message/prompt.<br>"&lt; s &gt; &lt;|im_start|&gt;User<br>I''m
          human&lt;|im_end|&gt;<br>&lt; s &gt; &lt;|im_start|&gt;Assistant<br>I''m
          Assistant&lt;|im_end|&gt;<br>&lt; s &gt; |im_start|&gt;User<br>I''m human&lt;|im_end|&gt;<br>&lt;
          s &gt;|im_start|&gt;Assistant<br>"<br>Example 3 - At the start of the sequence.<br>"&lt;
          s &gt; &lt;|im_start|&gt;User<br>Question 1?&lt;|im_end|&gt;<br>&lt;|im_start|&gt;Assistant<br>Answer
          1&lt;|im_end|&gt;<br>&lt;|im_start|&gt;User<br>Question 2 based on question
          1?&lt;|im_end|<br>&lt;|im_start|&gt;Assistant"</p>

          <p>Thanks.</p>

          <p>edit. I can''t write the proper BOS-token for some reason without formatting
          the whole text on here. I''ve added spaces to make it appear in the text.</p>

          '
        raw: "Hello!\n\nI came across this model and I must say, I'm very impressed\
          \ with the results. \nWhile examining the tokenizer_config.json file, I\
          \ noticed that it specifies < s > as the BOS-token and add_bos_token:true.\n\
          \nAlthough the model card mentions using the ChatML format, this raised\
          \ some questions that I hope to receive assistance with. \nHow should <\
          \ s >  be specified in the prompt/prompts when chatting with the model?\n\
          \nExample 1 - Not at all.\n\"<|im_start|>User\nQuestion 1?<|im_end|>\n<|im_start|>Assistant\n\
          Answer 1.<|im_end|>\n<|im_start|>User\nQuestion 2 based on question 1?<|im_end|>\n\
          <|im_start|>Assistant\"\n\nExample 2 - Before each message/prompt.\n\"<\
          \ s > <|im_start|>User\nI'm human<|im_end|>\n< s > <|im_start|>Assistant\n\
          I'm Assistant<|im_end|>\n< s > |im_start|>User\nI'm human<|im_end|>\n< s\
          \ >|im_start|>Assistant\n\"\nExample 3 - At the start of the sequence.\n\
          \"< s > <|im_start|>User\nQuestion 1?<|im_end|>\n<|im_start|>Assistant\n\
          Answer 1<|im_end|>\n<|im_start|>User\nQuestion 2 based on question 1?<|im_end|\n\
          <|im_start|>Assistant\"\n\nThanks.\n\nedit. I can't write the proper BOS-token\
          \ for some reason without formatting the whole text on here. I've added\
          \ spaces to make it appear in the text."
        updatedAt: '2023-12-12T20:08:30.877Z'
      numEdits: 1
      reactions: []
    id: 6572d4a3b2597ee59766200d
    type: comment
  author: Ghaandy
  content: "Hello!\n\nI came across this model and I must say, I'm very impressed\
    \ with the results. \nWhile examining the tokenizer_config.json file, I noticed\
    \ that it specifies < s > as the BOS-token and add_bos_token:true.\n\nAlthough\
    \ the model card mentions using the ChatML format, this raised some questions\
    \ that I hope to receive assistance with. \nHow should < s >  be specified in\
    \ the prompt/prompts when chatting with the model?\n\nExample 1 - Not at all.\n\
    \"<|im_start|>User\nQuestion 1?<|im_end|>\n<|im_start|>Assistant\nAnswer 1.<|im_end|>\n\
    <|im_start|>User\nQuestion 2 based on question 1?<|im_end|>\n<|im_start|>Assistant\"\
    \n\nExample 2 - Before each message/prompt.\n\"< s > <|im_start|>User\nI'm human<|im_end|>\n\
    < s > <|im_start|>Assistant\nI'm Assistant<|im_end|>\n< s > |im_start|>User\n\
    I'm human<|im_end|>\n< s >|im_start|>Assistant\n\"\nExample 3 - At the start of\
    \ the sequence.\n\"< s > <|im_start|>User\nQuestion 1?<|im_end|>\n<|im_start|>Assistant\n\
    Answer 1<|im_end|>\n<|im_start|>User\nQuestion 2 based on question 1?<|im_end|\n\
    <|im_start|>Assistant\"\n\nThanks.\n\nedit. I can't write the proper BOS-token\
    \ for some reason without formatting the whole text on here. I've added spaces\
    \ to make it appear in the text."
  created_at: 2023-12-08 08:32:35+00:00
  edited: true
  hidden: false
  id: 6572d4a3b2597ee59766200d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: teknium/OpenHermes-2.5-Mistral-7B
repo_type: model
status: open
target_branch: null
title: Question regarding BOS-token when chatting with the model.
