!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Pazuzzu
conflicting_files: null
created_at: 2023-11-19 03:15:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669970038440-noauth.jpeg?w=200&h=200&f=face
      fullname: El Mehdi CHOUHAM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pazuzzu
      type: user
    createdAt: '2023-11-19T03:15:12.000Z'
    data:
      edited: true
      editors:
      - Pazuzzu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4895874559879303
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669970038440-noauth.jpeg?w=200&h=200&f=face
          fullname: El Mehdi CHOUHAM
          isHf: false
          isPro: false
          name: Pazuzzu
          type: user
        html: "<p>Hello there, first of all thank you for this great work.<br>I have\
          \ an issue when I try to run the model</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">'teknium/OpenHermes-2.5-Mistral-7B'</span>, load_in_8bit=<span\
          \ class=\"hljs-literal\">True</span>,  device_map=<span class=\"hljs-string\"\
          >'auto'</span>)\ntokenizer = transformers.AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">'teknium/OpenHermes-2.5-Mistral-7B'</span>, add_eos_token=<span\
          \ class=\"hljs-literal\">True</span>)\n\nmessages = [\n    {<span class=\"\
          hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"system\"</span>,\
          \ <span class=\"hljs-string\">\"content\"</span>: <span class=\"hljs-string\"\
          >\"You are Hermes 2.\"</span>},\n    {<span class=\"hljs-string\">\"role\"\
          </span>: <span class=\"hljs-string\">\"user\"</span>, <span class=\"hljs-string\"\
          >\"content\"</span>: <span class=\"hljs-string\">\"Hello, what are your\
          \ limitations ?\"</span>}\n]\ngen_input = tokenizer.apply_chat_template(messages,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\ngen_output\
          \ = model.generate(**gen_input)\ntokenizer.decode(gen_output[<span class=\"\
          hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>)\n</code></pre>\n<p>I get the error :</p>\n<pre><code class=\"\
          language-python\">---------------------------------------------------------------------------\n\
          TypeError                                 Traceback (most recent call last)\n\
          Cell In [<span class=\"hljs-number\">100</span>], line <span class=\"hljs-number\"\
          >7</span>\n      <span class=\"hljs-number\">2</span> messages = [\n   \
          \   <span class=\"hljs-number\">3</span>     {<span class=\"hljs-string\"\
          >\"role\"</span>: <span class=\"hljs-string\">\"system\"</span>, <span class=\"\
          hljs-string\">\"content\"</span>: <span class=\"hljs-string\">\"You are\
          \ Hermes 2.\"</span>},\n      <span class=\"hljs-number\">4</span>     {<span\
          \ class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"\
          user\"</span>, <span class=\"hljs-string\">\"content\"</span>: <span class=\"\
          hljs-string\">\"Hello, what are your limitations ?\"</span>}\n      <span\
          \ class=\"hljs-number\">5</span> ]\n      <span class=\"hljs-number\">6</span>\
          \ gen_input = tokenizer.apply_chat_template(messages, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>)\n----&gt; <span class=\"hljs-number\"\
          >7</span> gen_output = model.generate(**gen_input)\n      <span class=\"\
          hljs-number\">8</span> tokenizer.decode(gen_output[<span class=\"hljs-number\"\
          >0</span>], skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n\
          \nTypeError: transformers.generation.utils.GenerationMixin.generate() argument\
          \ after ** must be a mapping, <span class=\"hljs-keyword\">not</span> Tensor\n\
          </code></pre>\n<p>Please, is there something I'm doing wrong ? I am using\
          \ (transformers==4.36.0.dev0)</p>\n"
        raw: "Hello there, first of all thank you for this great work.\nI have an\
          \ issue when I try to run the model\n\n```python\nimport transformers\n\
          model = transformers.AutoModelForCausalLM.from_pretrained('teknium/OpenHermes-2.5-Mistral-7B',\
          \ load_in_8bit=True,  device_map='auto')\ntokenizer = transformers.AutoTokenizer.from_pretrained('teknium/OpenHermes-2.5-Mistral-7B',\
          \ add_eos_token=True)\n\nmessages = [\n    {\"role\": \"system\", \"content\"\
          : \"You are Hermes 2.\"},\n    {\"role\": \"user\", \"content\": \"Hello,\
          \ what are your limitations ?\"}\n]\ngen_input = tokenizer.apply_chat_template(messages,\
          \ return_tensors=\"pt\")\ngen_output = model.generate(**gen_input)\ntokenizer.decode(gen_output[0],\
          \ skip_special_tokens=True)\n```\n\nI get the error :\n```python\n---------------------------------------------------------------------------\n\
          TypeError                                 Traceback (most recent call last)\n\
          Cell In [100], line 7\n      2 messages = [\n      3     {\"role\": \"system\"\
          , \"content\": \"You are Hermes 2.\"},\n      4     {\"role\": \"user\"\
          , \"content\": \"Hello, what are your limitations ?\"}\n      5 ]\n    \
          \  6 gen_input = tokenizer.apply_chat_template(messages, return_tensors=\"\
          pt\")\n----> 7 gen_output = model.generate(**gen_input)\n      8 tokenizer.decode(gen_output[0],\
          \ skip_special_tokens=True)\n\nTypeError: transformers.generation.utils.GenerationMixin.generate()\
          \ argument after ** must be a mapping, not Tensor\n```\n\nPlease, is there\
          \ something I'm doing wrong ? I am using (transformers==4.36.0.dev0)\n"
        updatedAt: '2023-11-19T13:59:46.623Z'
      numEdits: 2
      reactions: []
    id: 65597dc0ed8df8312822b43b
    type: comment
  author: Pazuzzu
  content: "Hello there, first of all thank you for this great work.\nI have an issue\
    \ when I try to run the model\n\n```python\nimport transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained('teknium/OpenHermes-2.5-Mistral-7B',\
    \ load_in_8bit=True,  device_map='auto')\ntokenizer = transformers.AutoTokenizer.from_pretrained('teknium/OpenHermes-2.5-Mistral-7B',\
    \ add_eos_token=True)\n\nmessages = [\n    {\"role\": \"system\", \"content\"\
    : \"You are Hermes 2.\"},\n    {\"role\": \"user\", \"content\": \"Hello, what\
    \ are your limitations ?\"}\n]\ngen_input = tokenizer.apply_chat_template(messages,\
    \ return_tensors=\"pt\")\ngen_output = model.generate(**gen_input)\ntokenizer.decode(gen_output[0],\
    \ skip_special_tokens=True)\n```\n\nI get the error :\n```python\n---------------------------------------------------------------------------\n\
    TypeError                                 Traceback (most recent call last)\n\
    Cell In [100], line 7\n      2 messages = [\n      3     {\"role\": \"system\"\
    , \"content\": \"You are Hermes 2.\"},\n      4     {\"role\": \"user\", \"content\"\
    : \"Hello, what are your limitations ?\"}\n      5 ]\n      6 gen_input = tokenizer.apply_chat_template(messages,\
    \ return_tensors=\"pt\")\n----> 7 gen_output = model.generate(**gen_input)\n \
    \     8 tokenizer.decode(gen_output[0], skip_special_tokens=True)\n\nTypeError:\
    \ transformers.generation.utils.GenerationMixin.generate() argument after ** must\
    \ be a mapping, not Tensor\n```\n\nPlease, is there something I'm doing wrong\
    \ ? I am using (transformers==4.36.0.dev0)\n"
  created_at: 2023-11-19 03:15:12+00:00
  edited: true
  hidden: false
  id: 65597dc0ed8df8312822b43b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-11-24T07:38:45.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8306073546409607
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>I couldnt get the chat template thing to work, try just using the
          string of what it should be</p>

          '
        raw: I couldnt get the chat template thing to work, try just using the string
          of what it should be
        updatedAt: '2023-11-24T07:38:45.505Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Pazuzzu
    id: 6560530557a180c7225896e5
    type: comment
  author: teknium
  content: I couldnt get the chat template thing to work, try just using the string
    of what it should be
  created_at: 2023-11-24 07:38:45+00:00
  edited: false
  hidden: false
  id: 6560530557a180c7225896e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669970038440-noauth.jpeg?w=200&h=200&f=face
      fullname: El Mehdi CHOUHAM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pazuzzu
      type: user
    createdAt: '2023-12-05T11:36:03.000Z'
    data:
      edited: true
      editors:
      - Pazuzzu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9171000719070435
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669970038440-noauth.jpeg?w=200&h=200&f=face
          fullname: El Mehdi CHOUHAM
          isHf: false
          isPro: false
          name: Pazuzzu
          type: user
        html: '<p>Apologies,<br>I have no issue with tokenizer.apply_chat_template
          but my model loading statement was missing the "return_dict=True", that
          is what caused the error.</p>

          '
        raw: 'Apologies,

          I have no issue with tokenizer.apply_chat_template but my model loading
          statement was missing the "return_dict=True", that is what caused the error.'
        updatedAt: '2023-12-05T11:36:40.141Z'
      numEdits: 1
      reactions: []
    id: 656f0b231677b468ff84da76
    type: comment
  author: Pazuzzu
  content: 'Apologies,

    I have no issue with tokenizer.apply_chat_template but my model loading statement
    was missing the "return_dict=True", that is what caused the error.'
  created_at: 2023-12-05 11:36:03+00:00
  edited: true
  hidden: false
  id: 656f0b231677b468ff84da76
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: teknium/OpenHermes-2.5-Mistral-7B
repo_type: model
status: open
target_branch: null
title: 'TypeError: transformers.generation.utils.GenerationMixin.generate() argument
  after ** must be a mapping, not Tensor'
