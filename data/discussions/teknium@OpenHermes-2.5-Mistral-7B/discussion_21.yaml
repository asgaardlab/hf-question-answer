!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ziizu
conflicting_files: null
created_at: 2024-01-09 23:01:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/733303db25fffbba6dbeb2448dab4dd5.svg
      fullname: Nason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ziizu
      type: user
    createdAt: '2024-01-09T23:01:45.000Z'
    data:
      edited: false
      editors:
      - Ziizu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8583530783653259
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/733303db25fffbba6dbeb2448dab4dd5.svg
          fullname: Nason
          isHf: false
          isPro: false
          name: Ziizu
          type: user
        html: "<p>I have an Nvidia A10 (24GB of VRAM) but I'm getting out of memory\
          \ errors.</p>\n<pre><code>model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\
          \n\n\ndef load_model(model_name: str):\n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\
          \n    with torch.device(\"cuda:0\"):\n        model = transformers.AutoModelForCausalLM.from_pretrained(model_name).eval()\n\
          \    \n    return tokenizer, model\n\ntokenizer, model = load_model(model_name)\n\
          </code></pre>\n<p><code>OutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 64.00 MiB. GPU 0 has a total capacty of 21.99 GiB of which 13.00 MiB is\
          \ free. Including non-PyTorch memory, this process has 21.96 GiB memory\
          \ in use. Of the allocated memory 21.58 GiB is allocated by PyTorch, and\
          \ 99.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
          \ memory is large try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF...</code></p>\n\
          <p>I assumed 24GB would be enough for  a 7B model, how much VRAM do I need\
          \ to run this model?</p>\n"
        raw: "I have an Nvidia A10 (24GB of VRAM) but I'm getting out of memory errors.\r\
          \n\r\n\r\n```\r\nmodel_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\r\n\r\
          \n\r\ndef load_model(model_name: str):\r\n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\r\
          \n\r\n    with torch.device(\"cuda:0\"):\r\n        model = transformers.AutoModelForCausalLM.from_pretrained(model_name).eval()\r\
          \n    \r\n    return tokenizer, model\r\n\r\ntokenizer, model = load_model(model_name)\r\
          \n```\r\n\r\n```OutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 64.00 MiB. GPU 0 has a total capacty of 21.99 GiB of which 13.00 MiB is\
          \ free. Including non-PyTorch memory, this process has 21.96 GiB memory\
          \ in use. Of the allocated memory 21.58 GiB is allocated by PyTorch, and\
          \ 99.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
          \ memory is large try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF...```\r\
          \n\r\n\r\nI assumed 24GB would be enough for  a 7B model, how much VRAM\
          \ do I need to run this model?"
        updatedAt: '2024-01-09T23:01:45.102Z'
      numEdits: 0
      reactions: []
    id: 659dd059c474a955d4d99ce7
    type: comment
  author: Ziizu
  content: "I have an Nvidia A10 (24GB of VRAM) but I'm getting out of memory errors.\r\
    \n\r\n\r\n```\r\nmodel_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\r\n\r\n\r\n\
    def load_model(model_name: str):\r\n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\r\
    \n\r\n    with torch.device(\"cuda:0\"):\r\n        model = transformers.AutoModelForCausalLM.from_pretrained(model_name).eval()\r\
    \n    \r\n    return tokenizer, model\r\n\r\ntokenizer, model = load_model(model_name)\r\
    \n```\r\n\r\n```OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00\
    \ MiB. GPU 0 has a total capacty of 21.99 GiB of which 13.00 MiB is free. Including\
    \ non-PyTorch memory, this process has 21.96 GiB memory in use. Of the allocated\
    \ memory 21.58 GiB is allocated by PyTorch, and 99.18 MiB is reserved by PyTorch\
    \ but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb\
    \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF...```\r\
    \n\r\n\r\nI assumed 24GB would be enough for  a 7B model, how much VRAM do I need\
    \ to run this model?"
  created_at: 2024-01-09 23:01:45+00:00
  edited: false
  hidden: false
  id: 659dd059c474a955d4d99ce7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2024-01-10T10:21:28.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.857883632183075
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: "<blockquote>\n<p>I have an Nvidia A10 (24GB of VRAM) but I'm getting\
          \ out of memory errors.</p>\n<pre><code>model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\
          \n\n\ndef load_model(model_name: str):\n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\
          \n    with torch.device(\"cuda:0\"):\n        model = transformers.AutoModelForCausalLM.from_pretrained(model_name).eval()\n\
          \    \n    return tokenizer, model\n\ntokenizer, model = load_model(model_name)\n\
          </code></pre>\n<p><code>OutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 64.00 MiB. GPU 0 has a total capacty of 21.99 GiB of which 13.00 MiB is\
          \ free. Including non-PyTorch memory, this process has 21.96 GiB memory\
          \ in use. Of the allocated memory 21.58 GiB is allocated by PyTorch, and\
          \ 99.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
          \ memory is large try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF...</code></p>\n\
          <p>I assumed 24GB would be enough for  a 7B model, how much VRAM do I need\
          \ to run this model?</p>\n</blockquote>\n<p>You're loading it likely in\
          \ fp32. in fp32, it needs 28GB. In fp/bf16 it needs 14GB, in 8bit, 7GB,\
          \ and in 4bit, ~4GB - add 1GB to all for CUDA Kernel</p>\n"
        raw: "> I have an Nvidia A10 (24GB of VRAM) but I'm getting out of memory\
          \ errors.\n> \n> \n> ```\n> model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\
          \n> \n> \n> def load_model(model_name: str):\n>     tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\
          > \n>     with torch.device(\"cuda:0\"):\n>         model = transformers.AutoModelForCausalLM.from_pretrained(model_name).eval()\n\
          >     \n>     return tokenizer, model\n> \n> tokenizer, model = load_model(model_name)\n\
          > ```\n> \n> ```OutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 64.00 MiB. GPU 0 has a total capacty of 21.99 GiB of which 13.00 MiB is\
          \ free. Including non-PyTorch memory, this process has 21.96 GiB memory\
          \ in use. Of the allocated memory 21.58 GiB is allocated by PyTorch, and\
          \ 99.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
          \ memory is large try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF...```\n\
          > \n> \n> I assumed 24GB would be enough for  a 7B model, how much VRAM\
          \ do I need to run this model?\n\nYou're loading it likely in fp32. in fp32,\
          \ it needs 28GB. In fp/bf16 it needs 14GB, in 8bit, 7GB, and in 4bit, ~4GB\
          \ - add 1GB to all for CUDA Kernel"
        updatedAt: '2024-01-10T10:21:28.994Z'
      numEdits: 0
      reactions: []
    id: 659e6fa818dc7360293f86e3
    type: comment
  author: teknium
  content: "> I have an Nvidia A10 (24GB of VRAM) but I'm getting out of memory errors.\n\
    > \n> \n> ```\n> model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\n> \n> \n\
    > def load_model(model_name: str):\n>     tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\
    > \n>     with torch.device(\"cuda:0\"):\n>         model = transformers.AutoModelForCausalLM.from_pretrained(model_name).eval()\n\
    >     \n>     return tokenizer, model\n> \n> tokenizer, model = load_model(model_name)\n\
    > ```\n> \n> ```OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00\
    \ MiB. GPU 0 has a total capacty of 21.99 GiB of which 13.00 MiB is free. Including\
    \ non-PyTorch memory, this process has 21.96 GiB memory in use. Of the allocated\
    \ memory 21.58 GiB is allocated by PyTorch, and 99.18 MiB is reserved by PyTorch\
    \ but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb\
    \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF...```\n\
    > \n> \n> I assumed 24GB would be enough for  a 7B model, how much VRAM do I need\
    \ to run this model?\n\nYou're loading it likely in fp32. in fp32, it needs 28GB.\
    \ In fp/bf16 it needs 14GB, in 8bit, 7GB, and in 4bit, ~4GB - add 1GB to all for\
    \ CUDA Kernel"
  created_at: 2024-01-10 10:21:28+00:00
  edited: false
  hidden: false
  id: 659e6fa818dc7360293f86e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/733303db25fffbba6dbeb2448dab4dd5.svg
      fullname: Nason
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ziizu
      type: user
    createdAt: '2024-01-10T15:34:09.000Z'
    data:
      edited: true
      editors:
      - Ziizu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5128788352012634
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/733303db25fffbba6dbeb2448dab4dd5.svg
          fullname: Nason
          isHf: false
          isPro: false
          name: Ziizu
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;teknium&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/teknium\">@<span class=\"\
          underline\">teknium</span></a></span>\n\n\t</span></span> Thanks for the\
          \ response, this may be a naive question but how do I load in 16/8 bit?</p>\n\
          <p>I've tried loading in bf16: </p>\n<pre><code>model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\
          \n\ndef load_model(model_name: str):\n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\
          \n    with torch.device(\"cuda:0\"):\n        model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\
          \ torch_dtype=torch.bfloat16).eval()\n    \n    return tokenizer, model\n\
          \ntokenizer, model = load_model(model_name)\n</code></pre>\n<p>which gave:</p>\n\
          <pre><code>Loading checkpoint shards:   0%|          | 0/2 [00:49&lt;?,\
          \ ?it/s]\n---------------------------------------------------------------------------\n\
          OutOfMemoryError                          Traceback (most recent call last)\n\
          Cell In[3], line 19\n     15         model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\
          \ torch_dtype=torch.bfloat16).eval()\n     17     return model\n---&gt;\
          \ 19 model = load_model(model_name)\n\nCell In[3], line 15\n     11 def\
          \ load_model(model_name: str):\n     12     #tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\
          \     14     with torch.device(\"cuda:0\"):\n---&gt; 15         model =\
          \ transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).eval()\n\
          \     17     return model\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    564 elif type(config) in cls._model_mapping.keys():\n\
          \    565     model_class = _get_model_class(config, cls._model_mapping)\n\
          --&gt; 566     return model_class.from_pretrained(\n    567         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    568     )\n  \
          \  569 raise ValueError(\n    570     f\"Unrecognized configuration class\
          \ {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n\
          \    571     f\"Model type should be one of {', '.join(c.__name__ for c\
          \ in cls._model_mapping.keys())}.\"\n    572 )\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py:3706,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\n   3697    \
          \ if dtype_orig is not None:\n   3698         torch.set_default_dtype(dtype_orig)\n\
          \   3699     (\n   3700         model,\n   3701         missing_keys,\n\
          \   3702         unexpected_keys,\n   3703         mismatched_keys,\n  \
          \ 3704         offload_index,\n   3705         error_msgs,\n-&gt; 3706 \
          \    ) = cls._load_pretrained_model(\n   3707         model,\n   3708  \
          \       state_dict,\n   3709         loaded_state_dict_keys,  # XXX: rename?\n\
          \   3710         resolved_archive_file,\n   3711         pretrained_model_name_or_path,\n\
          \   3712         ignore_mismatched_sizes=ignore_mismatched_sizes,\n   3713\
          \         sharded_metadata=sharded_metadata,\n   3714         _fast_init=_fast_init,\n\
          \   3715         low_cpu_mem_usage=low_cpu_mem_usage,\n   3716         device_map=device_map,\n\
          \   3717         offload_folder=offload_folder,\n   3718         offload_state_dict=offload_state_dict,\n\
          \   3719         dtype=torch_dtype,\n   3720         is_quantized=(getattr(model,\
          \ \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES),\n\
          \   3721         keep_in_fp32_modules=keep_in_fp32_modules,\n   3722   \
          \  )\n   3724 model.is_loaded_in_4bit = load_in_4bit\n   3725 model.is_loaded_in_8bit\
          \ = load_in_8bit\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py:4091,\
          \ in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys,\
          \ resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes,\
          \ sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder,\
          \ offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\n   4089\
          \ if shard_file in disk_only_shard_files:\n   4090     continue\n-&gt; 4091\
          \ state_dict = load_state_dict(shard_file)\n   4093 # Mistmatched keys contains\
          \ tuples key/shape1/shape2 of weights in the checkpoint that have a shape\
          \ not\n   4094 # matching the weights in the model.\n   4095 mismatched_keys\
          \ += _find_mismatched_keys(\n   4096     state_dict,\n   4097     model_state_dict,\n\
          \   (...)\n   4101     ignore_mismatched_sizes,\n   4102 )\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py:510,\
          \ in load_state_dict(checkpoint_file)\n    505     if metadata.get(\"format\"\
          ) not in [\"pt\", \"tf\", \"flax\"]:\n    506         raise OSError(\n \
          \   507             f\"The safetensors archive passed at {checkpoint_file}\
          \ does not contain the valid metadata. Make sure \"\n    508           \
          \  \"you save your model with the `save_pretrained` method.\"\n    509 \
          \        )\n--&gt; 510     return safe_load_file(checkpoint_file)\n    511\
          \ try:\n    512     if (\n    513         is_deepspeed_zero3_enabled() and\
          \ torch.distributed.is_initialized() and torch.distributed.get_rank() &gt;\
          \ 0\n    514     ) or (is_fsdp_enabled() and not is_local_dist_rank_0()):\n\
          \nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/safetensors/torch.py:310,\
          \ in load_file(filename, device)\n    308 with safe_open(filename, framework=\"\
          pt\", device=device) as f:\n    309     for k in f.keys():\n--&gt; 310 \
          \        result[k] = f.get_tensor(k)\n    311 return result\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/torch/utils/_device.py:77,\
          \ in DeviceContext.__torch_function__(self, func, types, args, kwargs)\n\
          \     75 if func in _device_constructors() and kwargs.get('device') is None:\n\
          \     76     kwargs['device'] = self.device\n---&gt; 77 return func(*args,\
          \ **kwargs)\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate 112.00\
          \ MiB. GPU 0 has a total capacty of 21.99 GiB of which 59.00 MiB is free.\
          \ Including non-PyTorch memory, this process has 21.92 GiB memory in use.\
          \ Of the allocated memory 21.44 GiB is allocated by PyTorch, and 203.16\
          \ MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
          \ memory is large try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          </code></pre>\n<p>For reference/context, I can load the quantised GGUF version\
          \ of this model without issue.</p>\n"
        raw: "@teknium Thanks for the response, this may be a naive question but how\
          \ do I load in 16/8 bit?\n\nI've tried loading in bf16: \n```\nmodel_name\
          \ = \"teknium/OpenHermes-2.5-Mistral-7B\"\n\ndef load_model(model_name:\
          \ str):\n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\
          \n    with torch.device(\"cuda:0\"):\n        model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\
          \ torch_dtype=torch.bfloat16).eval()\n    \n    return tokenizer, model\n\
          \ntokenizer, model = load_model(model_name)\n```\n\nwhich gave:\n\n```\n\
          Loading checkpoint shards:   0%|          | 0/2 [00:49<?, ?it/s]\n---------------------------------------------------------------------------\n\
          OutOfMemoryError                          Traceback (most recent call last)\n\
          Cell In[3], line 19\n     15         model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\
          \ torch_dtype=torch.bfloat16).eval()\n     17     return model\n---> 19\
          \ model = load_model(model_name)\n\nCell In[3], line 15\n     11 def load_model(model_name:\
          \ str):\n     12     #tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\
          \     14     with torch.device(\"cuda:0\"):\n---> 15         model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\
          \ torch_dtype=torch.bfloat16).eval()\n     17     return model\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    564 elif type(config) in cls._model_mapping.keys():\n\
          \    565     model_class = _get_model_class(config, cls._model_mapping)\n\
          --> 566     return model_class.from_pretrained(\n    567         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    568     )\n  \
          \  569 raise ValueError(\n    570     f\"Unrecognized configuration class\
          \ {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n\
          \    571     f\"Model type should be one of {', '.join(c.__name__ for c\
          \ in cls._model_mapping.keys())}.\"\n    572 )\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py:3706,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\n   3697    \
          \ if dtype_orig is not None:\n   3698         torch.set_default_dtype(dtype_orig)\n\
          \   3699     (\n   3700         model,\n   3701         missing_keys,\n\
          \   3702         unexpected_keys,\n   3703         mismatched_keys,\n  \
          \ 3704         offload_index,\n   3705         error_msgs,\n-> 3706    \
          \ ) = cls._load_pretrained_model(\n   3707         model,\n   3708     \
          \    state_dict,\n   3709         loaded_state_dict_keys,  # XXX: rename?\n\
          \   3710         resolved_archive_file,\n   3711         pretrained_model_name_or_path,\n\
          \   3712         ignore_mismatched_sizes=ignore_mismatched_sizes,\n   3713\
          \         sharded_metadata=sharded_metadata,\n   3714         _fast_init=_fast_init,\n\
          \   3715         low_cpu_mem_usage=low_cpu_mem_usage,\n   3716         device_map=device_map,\n\
          \   3717         offload_folder=offload_folder,\n   3718         offload_state_dict=offload_state_dict,\n\
          \   3719         dtype=torch_dtype,\n   3720         is_quantized=(getattr(model,\
          \ \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES),\n\
          \   3721         keep_in_fp32_modules=keep_in_fp32_modules,\n   3722   \
          \  )\n   3724 model.is_loaded_in_4bit = load_in_4bit\n   3725 model.is_loaded_in_8bit\
          \ = load_in_8bit\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py:4091,\
          \ in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys,\
          \ resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes,\
          \ sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder,\
          \ offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\n   4089\
          \ if shard_file in disk_only_shard_files:\n   4090     continue\n-> 4091\
          \ state_dict = load_state_dict(shard_file)\n   4093 # Mistmatched keys contains\
          \ tuples key/shape1/shape2 of weights in the checkpoint that have a shape\
          \ not\n   4094 # matching the weights in the model.\n   4095 mismatched_keys\
          \ += _find_mismatched_keys(\n   4096     state_dict,\n   4097     model_state_dict,\n\
          \   (...)\n   4101     ignore_mismatched_sizes,\n   4102 )\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py:510,\
          \ in load_state_dict(checkpoint_file)\n    505     if metadata.get(\"format\"\
          ) not in [\"pt\", \"tf\", \"flax\"]:\n    506         raise OSError(\n \
          \   507             f\"The safetensors archive passed at {checkpoint_file}\
          \ does not contain the valid metadata. Make sure \"\n    508           \
          \  \"you save your model with the `save_pretrained` method.\"\n    509 \
          \        )\n--> 510     return safe_load_file(checkpoint_file)\n    511\
          \ try:\n    512     if (\n    513         is_deepspeed_zero3_enabled() and\
          \ torch.distributed.is_initialized() and torch.distributed.get_rank() >\
          \ 0\n    514     ) or (is_fsdp_enabled() and not is_local_dist_rank_0()):\n\
          \nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/safetensors/torch.py:310,\
          \ in load_file(filename, device)\n    308 with safe_open(filename, framework=\"\
          pt\", device=device) as f:\n    309     for k in f.keys():\n--> 310    \
          \     result[k] = f.get_tensor(k)\n    311 return result\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/torch/utils/_device.py:77,\
          \ in DeviceContext.__torch_function__(self, func, types, args, kwargs)\n\
          \     75 if func in _device_constructors() and kwargs.get('device') is None:\n\
          \     76     kwargs['device'] = self.device\n---> 77 return func(*args,\
          \ **kwargs)\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate 112.00\
          \ MiB. GPU 0 has a total capacty of 21.99 GiB of which 59.00 MiB is free.\
          \ Including non-PyTorch memory, this process has 21.92 GiB memory in use.\
          \ Of the allocated memory 21.44 GiB is allocated by PyTorch, and 203.16\
          \ MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
          \ memory is large try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          ```\n\nFor reference/context, I can load the quantised GGUF version of this\
          \ model without issue."
        updatedAt: '2024-01-10T15:35:40.719Z'
      numEdits: 1
      reactions: []
    id: 659eb8f146282fc39c4b336a
    type: comment
  author: Ziizu
  content: "@teknium Thanks for the response, this may be a naive question but how\
    \ do I load in 16/8 bit?\n\nI've tried loading in bf16: \n```\nmodel_name = \"\
    teknium/OpenHermes-2.5-Mistral-7B\"\n\ndef load_model(model_name: str):\n    tokenizer\
    \ = transformers.AutoTokenizer.from_pretrained(model_name)\n\n    with torch.device(\"\
    cuda:0\"):\n        model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\
    \ torch_dtype=torch.bfloat16).eval()\n    \n    return tokenizer, model\n\ntokenizer,\
    \ model = load_model(model_name)\n```\n\nwhich gave:\n\n```\nLoading checkpoint\
    \ shards:   0%|          | 0/2 [00:49<?, ?it/s]\n---------------------------------------------------------------------------\n\
    OutOfMemoryError                          Traceback (most recent call last)\n\
    Cell In[3], line 19\n     15         model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\
    \ torch_dtype=torch.bfloat16).eval()\n     17     return model\n---> 19 model\
    \ = load_model(model_name)\n\nCell In[3], line 15\n     11 def load_model(model_name:\
    \ str):\n     12     #tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\
    \     14     with torch.device(\"cuda:0\"):\n---> 15         model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\
    \ torch_dtype=torch.bfloat16).eval()\n     17     return model\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    564 elif type(config) in cls._model_mapping.keys():\n    565\
    \     model_class = _get_model_class(config, cls._model_mapping)\n--> 566    \
    \ return model_class.from_pretrained(\n    567         pretrained_model_name_or_path,\
    \ *model_args, config=config, **hub_kwargs, **kwargs\n    568     )\n    569 raise\
    \ ValueError(\n    570     f\"Unrecognized configuration class {config.__class__}\
    \ for this kind of AutoModel: {cls.__name__}.\\n\"\n    571     f\"Model type\
    \ should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\
    \n    572 )\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py:3706,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config,\
    \ cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token,\
    \ revision, use_safetensors, *model_args, **kwargs)\n   3697     if dtype_orig\
    \ is not None:\n   3698         torch.set_default_dtype(dtype_orig)\n   3699 \
    \    (\n   3700         model,\n   3701         missing_keys,\n   3702       \
    \  unexpected_keys,\n   3703         mismatched_keys,\n   3704         offload_index,\n\
    \   3705         error_msgs,\n-> 3706     ) = cls._load_pretrained_model(\n  \
    \ 3707         model,\n   3708         state_dict,\n   3709         loaded_state_dict_keys,\
    \  # XXX: rename?\n   3710         resolved_archive_file,\n   3711         pretrained_model_name_or_path,\n\
    \   3712         ignore_mismatched_sizes=ignore_mismatched_sizes,\n   3713   \
    \      sharded_metadata=sharded_metadata,\n   3714         _fast_init=_fast_init,\n\
    \   3715         low_cpu_mem_usage=low_cpu_mem_usage,\n   3716         device_map=device_map,\n\
    \   3717         offload_folder=offload_folder,\n   3718         offload_state_dict=offload_state_dict,\n\
    \   3719         dtype=torch_dtype,\n   3720         is_quantized=(getattr(model,\
    \ \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES),\n   3721\
    \         keep_in_fp32_modules=keep_in_fp32_modules,\n   3722     )\n   3724 model.is_loaded_in_4bit\
    \ = load_in_4bit\n   3725 model.is_loaded_in_8bit = load_in_8bit\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py:4091,\
    \ in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys,\
    \ resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes,\
    \ sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder,\
    \ offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\n   4089 if shard_file\
    \ in disk_only_shard_files:\n   4090     continue\n-> 4091 state_dict = load_state_dict(shard_file)\n\
    \   4093 # Mistmatched keys contains tuples key/shape1/shape2 of weights in the\
    \ checkpoint that have a shape not\n   4094 # matching the weights in the model.\n\
    \   4095 mismatched_keys += _find_mismatched_keys(\n   4096     state_dict,\n\
    \   4097     model_state_dict,\n   (...)\n   4101     ignore_mismatched_sizes,\n\
    \   4102 )\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py:510,\
    \ in load_state_dict(checkpoint_file)\n    505     if metadata.get(\"format\"\
    ) not in [\"pt\", \"tf\", \"flax\"]:\n    506         raise OSError(\n    507\
    \             f\"The safetensors archive passed at {checkpoint_file} does not\
    \ contain the valid metadata. Make sure \"\n    508             \"you save your\
    \ model with the `save_pretrained` method.\"\n    509         )\n--> 510     return\
    \ safe_load_file(checkpoint_file)\n    511 try:\n    512     if (\n    513   \
    \      is_deepspeed_zero3_enabled() and torch.distributed.is_initialized() and\
    \ torch.distributed.get_rank() > 0\n    514     ) or (is_fsdp_enabled() and not\
    \ is_local_dist_rank_0()):\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/safetensors/torch.py:310,\
    \ in load_file(filename, device)\n    308 with safe_open(filename, framework=\"\
    pt\", device=device) as f:\n    309     for k in f.keys():\n--> 310         result[k]\
    \ = f.get_tensor(k)\n    311 return result\n\nFile ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/torch/utils/_device.py:77,\
    \ in DeviceContext.__torch_function__(self, func, types, args, kwargs)\n     75\
    \ if func in _device_constructors() and kwargs.get('device') is None:\n     76\
    \     kwargs['device'] = self.device\n---> 77 return func(*args, **kwargs)\n\n\
    OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has\
    \ a total capacty of 21.99 GiB of which 59.00 MiB is free. Including non-PyTorch\
    \ memory, this process has 21.92 GiB memory in use. Of the allocated memory 21.44\
    \ GiB is allocated by PyTorch, and 203.16 MiB is reserved by PyTorch but unallocated.\
    \ If reserved but unallocated memory is large try setting max_split_size_mb to\
    \ avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
    ```\n\nFor reference/context, I can load the quantised GGUF version of this model\
    \ without issue."
  created_at: 2024-01-10 15:34:09+00:00
  edited: true
  hidden: false
  id: 659eb8f146282fc39c4b336a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2024-01-10T16:25:34.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7638756036758423
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: "<p>Use this inference code from the repo</p>\n<p><a href=\"https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/blob/main/transformers_inference.py\"\
          >https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/blob/main/transformers_inference.py</a></p>\n\
          <p>Let me know if it still has issues \U0001F917</p>\n"
        raw: "Use this inference code from the repo\n\nhttps://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/blob/main/transformers_inference.py\n\
          \nLet me know if it still has issues \U0001F917"
        updatedAt: '2024-01-10T16:25:34.883Z'
      numEdits: 0
      reactions: []
    id: 659ec4fe56ffdbbecda6fc88
    type: comment
  author: teknium
  content: "Use this inference code from the repo\n\nhttps://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/blob/main/transformers_inference.py\n\
    \nLet me know if it still has issues \U0001F917"
  created_at: 2024-01-10 16:25:34+00:00
  edited: false
  hidden: false
  id: 659ec4fe56ffdbbecda6fc88
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: teknium/OpenHermes-2.5-Mistral-7B
repo_type: model
status: open
target_branch: null
title: How much VRAM does this model need?
