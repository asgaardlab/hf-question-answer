!!python/object:huggingface_hub.community.DiscussionWithDetails
author: muelletm
conflicting_files:
- modeling_mpt.py
created_at: 2023-05-27 14:30:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646410244848-5fce6464e80c09ad2760251c.jpeg?w=200&h=200&f=face
      fullname: "Thomas M\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muelletm
      type: user
    createdAt: '2023-05-27T15:30:12.000Z'
    data:
      edited: true
      editors:
      - muelletm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646410244848-5fce6464e80c09ad2760251c.jpeg?w=200&h=200&f=face
          fullname: "Thomas M\xFCller"
          isHf: false
          isPro: false
          name: muelletm
          type: user
        html: "<p>Merges <a href=\"https://huggingface.co/cekal/mpt-7b-peft-compatible\"\
          >https://huggingface.co/cekal/mpt-7b-peft-compatible</a> by <span data-props=\"\
          {&quot;user&quot;:&quot;cekal&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/cekal\">@<span class=\"underline\">cekal</span></a></span>\n\
          \n\t</span></span>.</p>\n<p>This will add support for peft as well as qlora.</p>\n\
          <p>I tested that qlora starts training: </p>\n<p><a rel=\"nofollow\" href=\"\
          https://github.com/artidoro/qlora/issues/10\">https://github.com/artidoro/qlora/issues/10</a></p>\n\
          <pre><code class=\"language-shell\">git clone https://huggingface.co/mosaicml/mpt-7b\n\
          pushd mpt-7b \ngit fetch origin refs/pr/42:pr/42\ngit checkout pr/42\npopd\n\
          \npython qlora.py \\\n    --model_name_or_path ./mpt-7b \\\n    --trust_remote_code\
          \ True \\\n    --output_dir /output \\\n    --dataset alpaca \\\n    --do_train\
          \ True \\\n    --do_eval True \\\n    --do_mmlu_eval True \\\n    --source_max_len\
          \ 384 \\\n    --target_max_len 128 \\\n    --per_device_train_batch_size\
          \ 4 \\\n    --per_device_eval_batch_size 4 \\\n    --gradient_accumulation_steps\
          \ 4 \\\n    --logging_steps 10 \\\n    --max_steps 10000 \\\n    --save_strategy\
          \ steps \\\n    --data_seed 42 \\\n    --save_steps 1000 \\\n    --save_total_limit\
          \ 40 \\\n    --evaluation_strategy steps \\\n    --eval_dataset_size 1024\
          \ \\\n    --max_eval_samples 1000 \\\n    --eval_steps 1000 \\\n    --optim\
          \ paged_adamw_32bit\n</code></pre>\n"
        raw: "Merges https://huggingface.co/cekal/mpt-7b-peft-compatible by @cekal.\n\
          \nThis will add support for peft as well as qlora.\n\nI tested that qlora\
          \ starts training: \n\n\nhttps://github.com/artidoro/qlora/issues/10\n\n\
          ```shell\ngit clone https://huggingface.co/mosaicml/mpt-7b\npushd mpt-7b\
          \ \ngit fetch origin refs/pr/42:pr/42\ngit checkout pr/42\npopd\n\npython\
          \ qlora.py \\\n    --model_name_or_path ./mpt-7b \\\n    --trust_remote_code\
          \ True \\\n    --output_dir /output \\\n    --dataset alpaca \\\n    --do_train\
          \ True \\\n    --do_eval True \\\n    --do_mmlu_eval True \\\n    --source_max_len\
          \ 384 \\\n    --target_max_len 128 \\\n    --per_device_train_batch_size\
          \ 4 \\\n    --per_device_eval_batch_size 4 \\\n    --gradient_accumulation_steps\
          \ 4 \\\n    --logging_steps 10 \\\n    --max_steps 10000 \\\n    --save_strategy\
          \ steps \\\n    --data_seed 42 \\\n    --save_steps 1000 \\\n    --save_total_limit\
          \ 40 \\\n    --evaluation_strategy steps \\\n    --eval_dataset_size 1024\
          \ \\\n    --max_eval_samples 1000 \\\n    --eval_steps 1000 \\\n    --optim\
          \ paged_adamw_32bit\n```"
        updatedAt: '2023-05-27T18:50:41.508Z'
      numEdits: 4
      reactions: []
    id: 64722204c27f74a0eba3e20d
    type: comment
  author: muelletm
  content: "Merges https://huggingface.co/cekal/mpt-7b-peft-compatible by @cekal.\n\
    \nThis will add support for peft as well as qlora.\n\nI tested that qlora starts\
    \ training: \n\n\nhttps://github.com/artidoro/qlora/issues/10\n\n```shell\ngit\
    \ clone https://huggingface.co/mosaicml/mpt-7b\npushd mpt-7b \ngit fetch origin\
    \ refs/pr/42:pr/42\ngit checkout pr/42\npopd\n\npython qlora.py \\\n    --model_name_or_path\
    \ ./mpt-7b \\\n    --trust_remote_code True \\\n    --output_dir /output \\\n\
    \    --dataset alpaca \\\n    --do_train True \\\n    --do_eval True \\\n    --do_mmlu_eval\
    \ True \\\n    --source_max_len 384 \\\n    --target_max_len 128 \\\n    --per_device_train_batch_size\
    \ 4 \\\n    --per_device_eval_batch_size 4 \\\n    --gradient_accumulation_steps\
    \ 4 \\\n    --logging_steps 10 \\\n    --max_steps 10000 \\\n    --save_strategy\
    \ steps \\\n    --data_seed 42 \\\n    --save_steps 1000 \\\n    --save_total_limit\
    \ 40 \\\n    --evaluation_strategy steps \\\n    --eval_dataset_size 1024 \\\n\
    \    --max_eval_samples 1000 \\\n    --eval_steps 1000 \\\n    --optim paged_adamw_32bit\n\
    ```"
  created_at: 2023-05-27 14:30:12+00:00
  edited: true
  hidden: false
  id: 64722204c27f74a0eba3e20d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646410244848-5fce6464e80c09ad2760251c.jpeg?w=200&h=200&f=face
      fullname: "Thomas M\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muelletm
      type: user
    createdAt: '2023-05-27T15:46:54.000Z'
    data:
      oid: 6b4ff270ed0e55056dbc9cc04c4ece5a5d5305dd
      parents:
      - 4ff95c4aec5c04ba509ddf517c56720541a7a487
      subject: Merges changes from https://huggingface.co/cekal/mpt-7b-peft-compatible.
    id: 647225ee0000000000000000
    type: commit
  author: muelletm
  created_at: 2023-05-27 14:46:54+00:00
  id: 647225ee0000000000000000
  oid: 6b4ff270ed0e55056dbc9cc04c4ece5a5d5305dd
  summary: Merges changes from https://huggingface.co/cekal/mpt-7b-peft-compatible.
  type: commit
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646410244848-5fce6464e80c09ad2760251c.jpeg?w=200&h=200&f=face
      fullname: "Thomas M\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muelletm
      type: user
    createdAt: '2023-05-27T15:51:20.000Z'
    data:
      status: open
    id: 647226f897a75cc77ab20708
    type: status-change
  author: muelletm
  created_at: 2023-05-27 14:51:20+00:00
  id: 647226f897a75cc77ab20708
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400c075f4ff62c2617023f7/DNrK0LTDuj9ZEpXl0_k_A.jpeg?w=200&h=200&f=face
      fullname: SebastianBoo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SebastianBodza
      type: user
    createdAt: '2023-05-30T10:21:15.000Z'
    data:
      edited: false
      editors:
      - SebastianBodza
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400c075f4ff62c2617023f7/DNrK0LTDuj9ZEpXl0_k_A.jpeg?w=200&h=200&f=face
          fullname: SebastianBoo
          isHf: false
          isPro: false
          name: SebastianBodza
          type: user
        html: '<p>Any Differences to <a href="/mosaicml/mpt-7b/discussions/25">#25</a>
          ?</p>

          '
        raw: 'Any Differences to #25 ?'
        updatedAt: '2023-05-30T10:21:15.604Z'
      numEdits: 0
      reactions: []
    id: 6475ce1b09e7732263374e16
    type: comment
  author: SebastianBodza
  content: 'Any Differences to #25 ?'
  created_at: 2023-05-30 09:21:15+00:00
  edited: false
  hidden: false
  id: 6475ce1b09e7732263374e16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646410244848-5fce6464e80c09ad2760251c.jpeg?w=200&h=200&f=face
      fullname: "Thomas M\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muelletm
      type: user
    createdAt: '2023-05-30T16:31:55.000Z'
    data:
      edited: false
      editors:
      - muelletm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646410244848-5fce6464e80c09ad2760251c.jpeg?w=200&h=200&f=face
          fullname: "Thomas M\xFCller"
          isHf: false
          isPro: false
          name: muelletm
          type: user
        html: "<p>Looks pretty similar TBH.</p>\n<p>One difference is this line that\
          \ is needed to work properly with <code>device_map=\"auto\"</code>:</p>\n\
          <p>(Around L290)</p>\n<pre><code class=\"language-python\">        outputs\
          \ = self.transformer(input_ids=input_ids, past_key_values=past_key_values,\
          \ attention_mask=attention_mask, prefix_mask=prefix_mask, sequence_id=sequence_id,\
          \ return_dict=return_dict, output_attentions=output_attentions, output_hidden_states=output_hidden_states,\
          \ use_cache=use_cache, inputs_embeds=inputs_embeds)\n        \n        last_hidden_state\
          \ = outputs.last_hidden_state\n        <span class=\"hljs-keyword\">if</span>\
          \ self.model_parallel:\n            last_hidden_state = last_hidden_state.to(self.transformer.wte.weight.device)\n\
          \        logits = F.linear(last_hidden_state, self.transformer.wte.weight)\n\
          </code></pre>\n<p>But that line could also be added there, I suppose.</p>\n\
          <p>There might be subtle differences in other places, too, but as I said\
          \ the code looks pretty similar.</p>\n"
        raw: "Looks pretty similar TBH.\n\nOne difference is this line that is needed\
          \ to work properly with `device_map=\"auto\"`:\n\n(Around L290)\n\n```python\n\
          \        outputs = self.transformer(input_ids=input_ids, past_key_values=past_key_values,\
          \ attention_mask=attention_mask, prefix_mask=prefix_mask, sequence_id=sequence_id,\
          \ return_dict=return_dict, output_attentions=output_attentions, output_hidden_states=output_hidden_states,\
          \ use_cache=use_cache, inputs_embeds=inputs_embeds)\n        \n        last_hidden_state\
          \ = outputs.last_hidden_state\n        if self.model_parallel:\n       \
          \     last_hidden_state = last_hidden_state.to(self.transformer.wte.weight.device)\n\
          \        logits = F.linear(last_hidden_state, self.transformer.wte.weight)\n\
          ```\n\nBut that line could also be added there, I suppose.\n\nThere might\
          \ be subtle differences in other places, too, but as I said the code looks\
          \ pretty similar."
        updatedAt: '2023-05-30T16:31:55.430Z'
      numEdits: 0
      reactions: []
    id: 647624fb3b255bc8a5206f7f
    type: comment
  author: muelletm
  content: "Looks pretty similar TBH.\n\nOne difference is this line that is needed\
    \ to work properly with `device_map=\"auto\"`:\n\n(Around L290)\n\n```python\n\
    \        outputs = self.transformer(input_ids=input_ids, past_key_values=past_key_values,\
    \ attention_mask=attention_mask, prefix_mask=prefix_mask, sequence_id=sequence_id,\
    \ return_dict=return_dict, output_attentions=output_attentions, output_hidden_states=output_hidden_states,\
    \ use_cache=use_cache, inputs_embeds=inputs_embeds)\n        \n        last_hidden_state\
    \ = outputs.last_hidden_state\n        if self.model_parallel:\n            last_hidden_state\
    \ = last_hidden_state.to(self.transformer.wte.weight.device)\n        logits =\
    \ F.linear(last_hidden_state, self.transformer.wte.weight)\n```\n\nBut that line\
    \ could also be added there, I suppose.\n\nThere might be subtle differences in\
    \ other places, too, but as I said the code looks pretty similar."
  created_at: 2023-05-30 15:31:55+00:00
  edited: false
  hidden: false
  id: 647624fb3b255bc8a5206f7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/39a73e66f89443c108d41edd6683184a.svg
      fullname: Enxhell Luzhnica
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eluzhnica
      type: user
    createdAt: '2023-06-28T18:09:17.000Z'
    data:
      edited: false
      editors:
      - eluzhnica
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.950032114982605
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/39a73e66f89443c108d41edd6683184a.svg
          fullname: Enxhell Luzhnica
          isHf: false
          isPro: false
          name: eluzhnica
          type: user
        html: "<p>I'm not sure why the additional param <code>inputs_embeds</code>\
          \ is needed. Maybe it's being used for something where they already have\
          \ the embedding? Someone knows?</p>\n<p>I made a similar version of this\
          \ for 30B too on top of the latest foundry changes and it trains with QLORA\
          \ <a href=\"https://huggingface.co/eluzhnica/mpt-30b-peft-compatible\">https://huggingface.co/eluzhnica/mpt-30b-peft-compatible</a>.\
          \ It does train well from what I've tried.\n </p>\n"
        raw: "I'm not sure why the additional param `inputs_embeds` is needed. Maybe\
          \ it's being used for something where they already have the embedding? Someone\
          \ knows?\n\nI made a similar version of this for 30B too on top of the latest\
          \ foundry changes and it trains with QLORA https://huggingface.co/eluzhnica/mpt-30b-peft-compatible.\
          \ It does train well from what I've tried.\n "
        updatedAt: '2023-06-28T18:09:17.797Z'
      numEdits: 0
      reactions: []
    id: 649c774d3178e4123673b9d0
    type: comment
  author: eluzhnica
  content: "I'm not sure why the additional param `inputs_embeds` is needed. Maybe\
    \ it's being used for something where they already have the embedding? Someone\
    \ knows?\n\nI made a similar version of this for 30B too on top of the latest\
    \ foundry changes and it trains with QLORA https://huggingface.co/eluzhnica/mpt-30b-peft-compatible.\
    \ It does train well from what I've tried.\n "
  created_at: 2023-06-28 17:09:17+00:00
  edited: false
  hidden: false
  id: 649c774d3178e4123673b9d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/481394619236c114ade0003e6e424dec.svg
      fullname: TTY
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DanielTTY
      type: user
    createdAt: '2023-07-01T15:32:23.000Z'
    data:
      edited: false
      editors:
      - DanielTTY
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7801331877708435
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/481394619236c114ade0003e6e424dec.svg
          fullname: TTY
          isHf: false
          isPro: false
          name: DanielTTY
          type: user
        html: '<p>can do the same thing for the 30b version? </p>

          '
        raw: 'can do the same thing for the 30b version? '
        updatedAt: '2023-07-01T15:32:23.320Z'
      numEdits: 0
      reactions: []
    id: 64a047078347efe8003e3cd2
    type: comment
  author: DanielTTY
  content: 'can do the same thing for the 30b version? '
  created_at: 2023-07-01 14:32:23+00:00
  edited: false
  hidden: false
  id: 64a047078347efe8003e3cd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/481394619236c114ade0003e6e424dec.svg
      fullname: TTY
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DanielTTY
      type: user
    createdAt: '2023-07-01T16:13:23.000Z'
    data:
      edited: false
      editors:
      - DanielTTY
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9301456809043884
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/481394619236c114ade0003e6e424dec.svg
          fullname: TTY
          isHf: false
          isPro: false
          name: DanielTTY
          type: user
        html: '<blockquote>

          <p>I''m not sure why the additional param <code>inputs_embeds</code> is
          needed. Maybe it''s being used for something where they already have the
          embedding? Someone knows?</p>

          <p>I made a similar version of this for 30B too on top of the latest foundry
          changes and it trains with QLORA <a href="https://huggingface.co/eluzhnica/mpt-30b-peft-compatible">https://huggingface.co/eluzhnica/mpt-30b-peft-compatible</a>.
          It does train well from what I''ve tried.</p>

          </blockquote>

          <p>I tried this and it gives the error:<br>TypeError: forward() takes 2
          positional arguments but 3 were given</p>

          <p>I think this is the same error when one sets "--gradient_checkpointing
          False".</p>

          '
        raw: "> I'm not sure why the additional param `inputs_embeds` is needed. Maybe\
          \ it's being used for something where they already have the embedding? Someone\
          \ knows?\n> \n> I made a similar version of this for 30B too on top of the\
          \ latest foundry changes and it trains with QLORA https://huggingface.co/eluzhnica/mpt-30b-peft-compatible.\
          \ It does train well from what I've tried.\n\nI tried this and it gives\
          \ the error:\nTypeError: forward() takes 2 positional arguments but 3 were\
          \ given\n\nI think this is the same error when one sets \"--gradient_checkpointing\
          \ False\"."
        updatedAt: '2023-07-01T16:13:23.087Z'
      numEdits: 0
      reactions: []
    id: 64a050a3c6c143d7c06cbce6
    type: comment
  author: DanielTTY
  content: "> I'm not sure why the additional param `inputs_embeds` is needed. Maybe\
    \ it's being used for something where they already have the embedding? Someone\
    \ knows?\n> \n> I made a similar version of this for 30B too on top of the latest\
    \ foundry changes and it trains with QLORA https://huggingface.co/eluzhnica/mpt-30b-peft-compatible.\
    \ It does train well from what I've tried.\n\nI tried this and it gives the error:\n\
    TypeError: forward() takes 2 positional arguments but 3 were given\n\nI think\
    \ this is the same error when one sets \"--gradient_checkpointing False\"."
  created_at: 2023-07-01 15:13:23+00:00
  edited: false
  hidden: false
  id: 64a050a3c6c143d7c06cbce6
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 42
repo_id: mosaicml/mpt-7b
repo_type: model
status: open
target_branch: refs/heads/main
title: Merge cekal/mpt-7b-peft-compatible
