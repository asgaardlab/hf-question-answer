!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GaaraOtheSand
conflicting_files: null
created_at: 2023-05-17 20:12:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
      fullname: Kalvin Tipton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaaraOtheSand
      type: user
    createdAt: '2023-05-17T21:12:53.000Z'
    data:
      edited: false
      editors:
      - GaaraOtheSand
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
          fullname: Kalvin Tipton
          isHf: false
          isPro: false
          name: GaaraOtheSand
          type: user
        html: "<p>I'm sure that this has been discussed a little bit but what am I\
          \ really supposed to be doing when it comes to this variable, 'attn_impl'?\
          \ In the documentation on the site it seems to suggest using triton but\
          \ when I run the program it says to use flash. Personally I'd prefer to\
          \ use torch but it kills the program for torch and triton so Idk. This is\
          \ what I've got written at the beginning of my program:</p>\n<pre><code>\
          \  config = transformers.AutoConfig.from_pretrained(\n    'mosaicml/mpt-7b',\n\
          \    trust_remote_code=True\n  )\n\n  config.attn_config['attn_impl'] =\
          \ 'torch'\n\n  config.update({\"max_seq_len\": 8192})\n\n   model = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \    'mosaicml/mpt-7b',\n    config=config,\n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=True\n  )\n</code></pre>\n"
        raw: "I'm sure that this has been discussed a little bit but what am I really\
          \ supposed to be doing when it comes to this variable, 'attn_impl'? In the\
          \ documentation on the site it seems to suggest using triton but when I\
          \ run the program it says to use flash. Personally I'd prefer to use torch\
          \ but it kills the program for torch and triton so Idk. This is what I've\
          \ got written at the beginning of my program:\r\n\r\n      config = transformers.AutoConfig.from_pretrained(\r\
          \n        'mosaicml/mpt-7b',\r\n        trust_remote_code=True\r\n     \
          \ )\r\n\r\n      config.attn_config['attn_impl'] = 'torch'\r\n\r\n     \
          \ config.update({\"max_seq_len\": 8192})\r\n\r\n       model = transformers.AutoModelForCausalLM.from_pretrained(\r\
          \n        'mosaicml/mpt-7b',\r\n        config=config,\r\n        torch_dtype=torch.bfloat16,\r\
          \n        trust_remote_code=True\r\n      )"
        updatedAt: '2023-05-17T21:12:53.013Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RonanMcGovern
    id: 64654355611ae99d14d4365b
    type: comment
  author: GaaraOtheSand
  content: "I'm sure that this has been discussed a little bit but what am I really\
    \ supposed to be doing when it comes to this variable, 'attn_impl'? In the documentation\
    \ on the site it seems to suggest using triton but when I run the program it says\
    \ to use flash. Personally I'd prefer to use torch but it kills the program for\
    \ torch and triton so Idk. This is what I've got written at the beginning of my\
    \ program:\r\n\r\n      config = transformers.AutoConfig.from_pretrained(\r\n\
    \        'mosaicml/mpt-7b',\r\n        trust_remote_code=True\r\n      )\r\n\r\
    \n      config.attn_config['attn_impl'] = 'torch'\r\n\r\n      config.update({\"\
    max_seq_len\": 8192})\r\n\r\n       model = transformers.AutoModelForCausalLM.from_pretrained(\r\
    \n        'mosaicml/mpt-7b',\r\n        config=config,\r\n        torch_dtype=torch.bfloat16,\r\
    \n        trust_remote_code=True\r\n      )"
  created_at: 2023-05-17 20:12:53+00:00
  edited: false
  hidden: false
  id: 64654355611ae99d14d4365b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
      fullname: Kalvin Tipton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaaraOtheSand
      type: user
    createdAt: '2023-05-18T17:30:32.000Z'
    data:
      edited: false
      editors:
      - GaaraOtheSand
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
          fullname: Kalvin Tipton
          isHf: false
          isPro: false
          name: GaaraOtheSand
          type: user
        html: "<p>Alright so this is the error I receive when trying to use triton:</p>\n\
          <pre><code>      /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py:148:\
          \ UserWarning: Using `attn_impl: torch`. If your model does not use \n \
          \     `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise\
          \ we recommend using `attn_impl: triton`.\n      warnings.warn('Using `attn_impl:\
          \ torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend\
          \ using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl:\
          \ triton`.')\n      Loading checkpoint shards:   0%|                   \
          \                                               | 0/2 [00:00&lt;?, ?it/s]Killed\n\
          </code></pre>\n<p>and this is the error I receive when trying to run the\
          \ program with flash as it suggests:</p>\n<pre><code>    line 39, in &lt;module&gt;\n\
          \       model = transformers.AutoModelForCausalLM.from_pretrained(\n   \
          \ File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\"\
          , line 462, in from_pretrained\n       return model_class.from_pretrained(\n\
          \    File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 2611, in from_pretrained\n      model = cls(config, *model_args,\
          \ **model_kwargs)\n    File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/modeling_mpt.py\"\
          , line 205, in __init__\n     self.transformer = MPTModel(config)\n    File\
          \ \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/modeling_mpt.py\"\
          , line 30, in __init__\n     config._validate_config()\n    File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/configuration_mpt.py\"\
          , line 108, in _validate_config\n      raise NotImplementedError('alibi\
          \ only implemented with torch and triton attention.')\n    NotImplementedError:\
          \ alibi only implemented with torch and triton attention.\n</code></pre>\n\
          <p>So for some reason no matter what type of attention implementation, the\
          \ program won't run, that seems kinda silly?</p>\n"
        raw: "Alright so this is the error I receive when trying to use triton:\n\n\
          \          /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py:148:\
          \ UserWarning: Using `attn_impl: torch`. If your model does not use \n \
          \         `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise\
          \ we recommend using `attn_impl: triton`.\n          warnings.warn('Using\
          \ `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm`\
          \ we recommend using `attn_impl: flash` otherwise ' + 'we recommend using\
          \ `attn_impl: triton`.')\n          Loading checkpoint shards:   0%|   \
          \                                                               | 0/2 [00:00<?,\
          \ ?it/s]Killed\n\nand this is the error I receive when trying to run the\
          \ program with flash as it suggests:\n\n        line 39, in <module>\n \
          \          model = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \        File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\"\
          , line 462, in from_pretrained\n           return model_class.from_pretrained(\n\
          \        File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
          , line 2611, in from_pretrained\n          model = cls(config, *model_args,\
          \ **model_kwargs)\n        File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/modeling_mpt.py\"\
          , line 205, in __init__\n         self.transformer = MPTModel(config)\n\
          \        File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/modeling_mpt.py\"\
          , line 30, in __init__\n         config._validate_config()\n        File\
          \ \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/configuration_mpt.py\"\
          , line 108, in _validate_config\n          raise NotImplementedError('alibi\
          \ only implemented with torch and triton attention.')\n        NotImplementedError:\
          \ alibi only implemented with torch and triton attention.\n\nSo for some\
          \ reason no matter what type of attention implementation, the program won't\
          \ run, that seems kinda silly?"
        updatedAt: '2023-05-18T17:30:32.076Z'
      numEdits: 0
      reactions: []
    id: 646660b83b99ed9970fddef6
    type: comment
  author: GaaraOtheSand
  content: "Alright so this is the error I receive when trying to use triton:\n\n\
    \          /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py:148:\
    \ UserWarning: Using `attn_impl: torch`. If your model does not use \n       \
    \   `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we\
    \ recommend using `attn_impl: triton`.\n          warnings.warn('Using `attn_impl:\
    \ torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend\
    \ using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')\n\
    \          Loading checkpoint shards:   0%|                                  \
    \                                | 0/2 [00:00<?, ?it/s]Killed\n\nand this is the\
    \ error I receive when trying to run the program with flash as it suggests:\n\n\
    \        line 39, in <module>\n           model = transformers.AutoModelForCausalLM.from_pretrained(\n\
    \        File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\"\
    , line 462, in from_pretrained\n           return model_class.from_pretrained(\n\
    \        File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\"\
    , line 2611, in from_pretrained\n          model = cls(config, *model_args, **model_kwargs)\n\
    \        File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/modeling_mpt.py\"\
    , line 205, in __init__\n         self.transformer = MPTModel(config)\n      \
    \  File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/modeling_mpt.py\"\
    , line 30, in __init__\n         config._validate_config()\n        File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/configuration_mpt.py\"\
    , line 108, in _validate_config\n          raise NotImplementedError('alibi only\
    \ implemented with torch and triton attention.')\n        NotImplementedError:\
    \ alibi only implemented with torch and triton attention.\n\nSo for some reason\
    \ no matter what type of attention implementation, the program won't run, that\
    \ seems kinda silly?"
  created_at: 2023-05-18 16:30:32+00:00
  edited: false
  hidden: false
  id: 646660b83b99ed9970fddef6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-18T17:34:24.000Z'
    data:
      edited: true
      editors:
      - abhi-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;GaaraOtheSand&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/GaaraOtheSand\"\
          >@<span class=\"underline\">GaaraOtheSand</span></a></span>\n\n\t</span></span>\
          \ , the default is <code>attn_impl: torch</code> and it should work on CPU\
          \ or GPU. The error you are seeing in </p>\n<pre><code> Loading checkpoint\
          \ shards:   0%|                                                        \
          \          | 0/2 [00:00&lt;?, ?it/s]Killed\n</code></pre>\n<p>appears to\
          \ be an Out-of-Memory issue. Do you have enough CPU RAM to hold the model?\
          \ (&gt;=16GB)</p>\n<p>The <code>UserWarnings</code> should not break anything,\
          \ they are just there for guidance. You can use either <code>torch</code>\
          \ or <code>triton</code> with this model since it uses ALiBi. We will remove/update\
          \ the comment referring to <code>flash</code> as we don't recommend that\
          \ path much anymore.</p>\n"
        raw: "Hi @GaaraOtheSand , the default is `attn_impl: torch` and it should\
          \ work on CPU or GPU. The error you are seeing in \n```\n Loading checkpoint\
          \ shards:   0%|                                                        \
          \          | 0/2 [00:00<?, ?it/s]Killed\n```\n\nappears to be an Out-of-Memory\
          \ issue. Do you have enough CPU RAM to hold the model? (>=16GB)\n\nThe `UserWarnings`\
          \ should not break anything, they are just there for guidance. You can use\
          \ either `torch` or `triton` with this model since it uses ALiBi. We will\
          \ remove/update the comment referring to `flash` as we don't recommend that\
          \ path much anymore."
        updatedAt: '2023-05-18T17:34:36.728Z'
      numEdits: 1
      reactions: []
    id: 646661a0e0fe831b4793fece
    type: comment
  author: abhi-mosaic
  content: "Hi @GaaraOtheSand , the default is `attn_impl: torch` and it should work\
    \ on CPU or GPU. The error you are seeing in \n```\n Loading checkpoint shards:\
    \   0%|                                                                  | 0/2\
    \ [00:00<?, ?it/s]Killed\n```\n\nappears to be an Out-of-Memory issue. Do you\
    \ have enough CPU RAM to hold the model? (>=16GB)\n\nThe `UserWarnings` should\
    \ not break anything, they are just there for guidance. You can use either `torch`\
    \ or `triton` with this model since it uses ALiBi. We will remove/update the comment\
    \ referring to `flash` as we don't recommend that path much anymore."
  created_at: 2023-05-18 16:34:24+00:00
  edited: true
  hidden: false
  id: 646661a0e0fe831b4793fece
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
      fullname: Kalvin Tipton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaaraOtheSand
      type: user
    createdAt: '2023-05-18T18:27:59.000Z'
    data:
      edited: false
      editors:
      - GaaraOtheSand
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
          fullname: Kalvin Tipton
          isHf: false
          isPro: false
          name: GaaraOtheSand
          type: user
        html: "<p>I have a 32 GB Ram, and my cuda, cudnn, and PyTorch I think are\
          \ working correctly, as far as I tested they are, TensorFlow pops up a few\
          \ different errors but I also don't think that they are anything to be concerned\
          \ about because when I test it the cpu and gpu tests both show me what they're\
          \ supposed to. When you say the default, do you mean it'll automatically\
          \ try doing attn_impl: torch or that I should write that into my code, because\
          \ in the docs on huggingface it shows this: config.attn_config['attn_impl']\
          \ = 'triton'. I tried running it with and without the actual, config.attn_config['attn_impl']\
          \ = 'torch', and it doesn't even show the loading checkpoint shards bar,\
          \ until I hit ctrl C because it just stays stalled. According to task manager\
          \ it's only using 22 of the 32 GB, this is what it gives me when I run it\
          \ using torch:</p>\n<pre><code>     /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py:148:\
          \ UserWarning: Using `attn_impl: torch`. If your model does not use \n \
          \    `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise\
          \ we recommend using `attn_impl: triton`.\n     warnings.warn('Using `attn_impl:\
          \ torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend\
          \ using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl:\
          \ triton`.')\n     Loading checkpoint shards:   0%|                    \
          \                                              | 0/2 [00:00&lt;?, ?it/s]Killed\n\
          </code></pre>\n"
        raw: "I have a 32 GB Ram, and my cuda, cudnn, and PyTorch I think are working\
          \ correctly, as far as I tested they are, TensorFlow pops up a few different\
          \ errors but I also don't think that they are anything to be concerned about\
          \ because when I test it the cpu and gpu tests both show me what they're\
          \ supposed to. When you say the default, do you mean it'll automatically\
          \ try doing attn_impl: torch or that I should write that into my code, because\
          \ in the docs on huggingface it shows this: config.attn_config['attn_impl']\
          \ = 'triton'. I tried running it with and without the actual, config.attn_config['attn_impl']\
          \ = 'torch', and it doesn't even show the loading checkpoint shards bar,\
          \ until I hit ctrl C because it just stays stalled. According to task manager\
          \ it's only using 22 of the 32 GB, this is what it gives me when I run it\
          \ using torch:\n\n         /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py:148:\
          \ UserWarning: Using `attn_impl: torch`. If your model does not use \n \
          \        `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise\
          \ we recommend using `attn_impl: triton`.\n         warnings.warn('Using\
          \ `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm`\
          \ we recommend using `attn_impl: flash` otherwise ' + 'we recommend using\
          \ `attn_impl: triton`.')\n         Loading checkpoint shards:   0%|    \
          \                                                              | 0/2 [00:00<?,\
          \ ?it/s]Killed"
        updatedAt: '2023-05-18T18:27:59.309Z'
      numEdits: 0
      reactions: []
    id: 64666e2f9c627c78f86dcfe0
    type: comment
  author: GaaraOtheSand
  content: "I have a 32 GB Ram, and my cuda, cudnn, and PyTorch I think are working\
    \ correctly, as far as I tested they are, TensorFlow pops up a few different errors\
    \ but I also don't think that they are anything to be concerned about because\
    \ when I test it the cpu and gpu tests both show me what they're supposed to.\
    \ When you say the default, do you mean it'll automatically try doing attn_impl:\
    \ torch or that I should write that into my code, because in the docs on huggingface\
    \ it shows this: config.attn_config['attn_impl'] = 'triton'. I tried running it\
    \ with and without the actual, config.attn_config['attn_impl'] = 'torch', and\
    \ it doesn't even show the loading checkpoint shards bar, until I hit ctrl C because\
    \ it just stays stalled. According to task manager it's only using 22 of the 32\
    \ GB, this is what it gives me when I run it using torch:\n\n         /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py:148:\
    \ UserWarning: Using `attn_impl: torch`. If your model does not use \n       \
    \  `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend\
    \ using `attn_impl: triton`.\n         warnings.warn('Using `attn_impl: torch`.\
    \ If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl:\
    \ flash` otherwise ' + 'we recommend using `attn_impl: triton`.')\n         Loading\
    \ checkpoint shards:   0%|                                                   \
    \               | 0/2 [00:00<?, ?it/s]Killed"
  created_at: 2023-05-18 17:27:59+00:00
  edited: false
  hidden: false
  id: 64666e2f9c627c78f86dcfe0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-18T19:38:56.000Z'
    data:
      edited: true
      editors:
      - abhi-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;GaaraOtheSand&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/GaaraOtheSand\"\
          >@<span class=\"underline\">GaaraOtheSand</span></a></span>\n\n\t</span></span>,\
          \ thank you for the info. A couple comments and  followup questions:</p>\n\
          <ol>\n<li><p>If you run <code>model = transformers.AutoModel.from_pretrained('mosaicml/mpt-7b',\
          \ trust_remote_code=True)</code>, this will use <code>attn_impl: torch</code>\
          \ by default as it is specified in the <code>config.json</code>. But it\
          \ will load into FP32 onto your CPU (Pytorch and HF convention always loads\
          \ into FP32 tensors, regardless of <code>torch_dtype</code>), which should\
          \ take up about  6.7 * 4 ~= 26.8GB of RAM. If this is failing, there is\
          \ likely something wrong with your environment, and I would recommend trying\
          \ out our Docker image <code>mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04</code>.</p>\n\
          </li>\n<li><p>Similar to above, if you run <code>model = transformers.AutoModel.from_pretrained('mosaicml/mpt-7b',\
          \ trust_remote_code=True, torch_dtype=torch.bfloat16)</code>, it will do\
          \ everything as above but load into BF16 weights on your CPU, which should\
          \ take about 6.7 * 2 ~= 13.4GB of RAM.</p>\n</li>\n</ol>\n<p>I've tried\
          \ 1) on a Linux box with the Docker image and 2) on the Linux box as well\
          \ as my Macbook M2 (which has just 16GB RAM).</p>\n<p>Could you try these\
          \ code snippets and if they still fail, could you report your <code>torch</code>\
          \ and <code>transformers</code> installed versions? Thank you!</p>\n"
        raw: "Hi @GaaraOtheSand, thank you for the info. A couple comments and  followup\
          \ questions:\n\n1) If you run `model = transformers.AutoModel.from_pretrained('mosaicml/mpt-7b',\
          \ trust_remote_code=True)`, this will use `attn_impl: torch` by default\
          \ as it is specified in the `config.json`. But it will load into FP32 onto\
          \ your CPU (Pytorch and HF convention always loads into FP32 tensors, regardless\
          \ of `torch_dtype`), which should take up about  6.7 * 4 ~= 26.8GB of RAM.\
          \ If this is failing, there is likely something wrong with your environment,\
          \ and I would recommend trying out our Docker image `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04`.\n\
          \n2) Similar to above, if you run `model = transformers.AutoModel.from_pretrained('mosaicml/mpt-7b',\
          \ trust_remote_code=True, torch_dtype=torch.bfloat16)`, it will do everything\
          \ as above but load into BF16 weights on your CPU, which should take about\
          \ 6.7 * 2 ~= 13.4GB of RAM. \n\nI've tried 1) on a Linux box with the Docker\
          \ image and 2) on the Linux box as well as my Macbook M2 (which has just\
          \ 16GB RAM).\n\nCould you try these code snippets and if they still fail,\
          \ could you report your `torch` and `transformers` installed versions? Thank\
          \ you!"
        updatedAt: '2023-05-18T19:39:13.241Z'
      numEdits: 1
      reactions: []
    id: 64667ed09c627c78f86ee75d
    type: comment
  author: abhi-mosaic
  content: "Hi @GaaraOtheSand, thank you for the info. A couple comments and  followup\
    \ questions:\n\n1) If you run `model = transformers.AutoModel.from_pretrained('mosaicml/mpt-7b',\
    \ trust_remote_code=True)`, this will use `attn_impl: torch` by default as it\
    \ is specified in the `config.json`. But it will load into FP32 onto your CPU\
    \ (Pytorch and HF convention always loads into FP32 tensors, regardless of `torch_dtype`),\
    \ which should take up about  6.7 * 4 ~= 26.8GB of RAM. If this is failing, there\
    \ is likely something wrong with your environment, and I would recommend trying\
    \ out our Docker image `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04`.\n\
    \n2) Similar to above, if you run `model = transformers.AutoModel.from_pretrained('mosaicml/mpt-7b',\
    \ trust_remote_code=True, torch_dtype=torch.bfloat16)`, it will do everything\
    \ as above but load into BF16 weights on your CPU, which should take about 6.7\
    \ * 2 ~= 13.4GB of RAM. \n\nI've tried 1) on a Linux box with the Docker image\
    \ and 2) on the Linux box as well as my Macbook M2 (which has just 16GB RAM).\n\
    \nCould you try these code snippets and if they still fail, could you report your\
    \ `torch` and `transformers` installed versions? Thank you!"
  created_at: 2023-05-18 18:38:56+00:00
  edited: true
  hidden: false
  id: 64667ed09c627c78f86ee75d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
      fullname: Kalvin Tipton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaaraOtheSand
      type: user
    createdAt: '2023-05-19T17:42:24.000Z'
    data:
      edited: false
      editors:
      - GaaraOtheSand
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
          fullname: Kalvin Tipton
          isHf: false
          isPro: false
          name: GaaraOtheSand
          type: user
        html: '<p>well I''m currently running the floating point 16, and my transformers
          version is 4.29.1, and my torch version is 2.0.1, and my Ubuntu is 22.04,
          I''m trying to run it through wsl because it said that it needs jax and
          jaxlib which I can''t get on windows, but there really shouldn''t be anything
          wrong with my ubuntu or wsl because when I test to see if torch can detect
          the gpu it shows the correct information. I guess I could try Docker it''s
          just that I''m really unfamiliar with it, and I''ve found Docker to not
          be very user friendly in the past, imo. I''m building a program that uses
          a LLM, in this instance MPT as its base model but I wouldn''t even know
          where to start to try and incorporate Docker into that project.</p>

          '
        raw: well I'm currently running the floating point 16, and my transformers
          version is 4.29.1, and my torch version is 2.0.1, and my Ubuntu is 22.04,
          I'm trying to run it through wsl because it said that it needs jax and jaxlib
          which I can't get on windows, but there really shouldn't be anything wrong
          with my ubuntu or wsl because when I test to see if torch can detect the
          gpu it shows the correct information. I guess I could try Docker it's just
          that I'm really unfamiliar with it, and I've found Docker to not be very
          user friendly in the past, imo. I'm building a program that uses a LLM,
          in this instance MPT as its base model but I wouldn't even know where to
          start to try and incorporate Docker into that project.
        updatedAt: '2023-05-19T17:42:24.123Z'
      numEdits: 0
      reactions: []
    id: 6467b5003a7c8dda2308dcfd
    type: comment
  author: GaaraOtheSand
  content: well I'm currently running the floating point 16, and my transformers version
    is 4.29.1, and my torch version is 2.0.1, and my Ubuntu is 22.04, I'm trying to
    run it through wsl because it said that it needs jax and jaxlib which I can't
    get on windows, but there really shouldn't be anything wrong with my ubuntu or
    wsl because when I test to see if torch can detect the gpu it shows the correct
    information. I guess I could try Docker it's just that I'm really unfamiliar with
    it, and I've found Docker to not be very user friendly in the past, imo. I'm building
    a program that uses a LLM, in this instance MPT as its base model but I wouldn't
    even know where to start to try and incorporate Docker into that project.
  created_at: 2023-05-19 16:42:24+00:00
  edited: false
  hidden: false
  id: 6467b5003a7c8dda2308dcfd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-24T23:26:06.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: "<blockquote>\n<p> I'm trying to run it through wsl because it said\
          \ that it needs jax and jaxlib which I can't get on windows<br>Hi <span\
          \ data-props=\"{&quot;user&quot;:&quot;GaaraOtheSand&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/GaaraOtheSand\">@<span\
          \ class=\"underline\">GaaraOtheSand</span></a></span>\n\n\t</span></span>\
          \ , could you elaborate on this? Nothing about our models or stack should\
          \ require JAX, was there some documentation or message you saw that suggested\
          \ this?</p>\n</blockquote>\n<blockquote>\n<p>and my transformers version\
          \ is 4.29.1, and my torch version is 2.0.1, and my Ubuntu is 22.04,<br>This\
          \ looks fine, but just to be extra safe, you could try using <code>torch==1.13.1</code>,\
          \ that is the version we used for building all these models.</p>\n</blockquote>\n\
          <p>Could you report the error you are seeing when you run the 1) and 2)\
          \ commands I shared in the previous message?</p>\n"
        raw: '>  I''m trying to run it through wsl because it said that it needs jax
          and jaxlib which I can''t get on windows

          Hi @GaaraOtheSand , could you elaborate on this? Nothing about our models
          or stack should require JAX, was there some documentation or message you
          saw that suggested this?


          > and my transformers version is 4.29.1, and my torch version is 2.0.1,
          and my Ubuntu is 22.04,

          This looks fine, but just to be extra safe, you could try using `torch==1.13.1`,
          that is the version we used for building all these models.


          Could you report the error you are seeing when you run the 1) and 2) commands
          I shared in the previous message?'
        updatedAt: '2023-05-24T23:26:06.034Z'
      numEdits: 0
      reactions: []
    id: 646e9d0e7942c36e9dacd80c
    type: comment
  author: abhi-mosaic
  content: '>  I''m trying to run it through wsl because it said that it needs jax
    and jaxlib which I can''t get on windows

    Hi @GaaraOtheSand , could you elaborate on this? Nothing about our models or stack
    should require JAX, was there some documentation or message you saw that suggested
    this?


    > and my transformers version is 4.29.1, and my torch version is 2.0.1, and my
    Ubuntu is 22.04,

    This looks fine, but just to be extra safe, you could try using `torch==1.13.1`,
    that is the version we used for building all these models.


    Could you report the error you are seeing when you run the 1) and 2) commands
    I shared in the previous message?'
  created_at: 2023-05-24 22:26:06+00:00
  edited: false
  hidden: false
  id: 646e9d0e7942c36e9dacd80c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
      fullname: Kalvin Tipton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaaraOtheSand
      type: user
    createdAt: '2023-05-25T17:43:38.000Z'
    data:
      edited: true
      editors:
      - GaaraOtheSand
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
          fullname: Kalvin Tipton
          isHf: false
          isPro: false
          name: GaaraOtheSand
          type: user
        html: "<p>For clarification on the JAX thing, I believe that has to due with\
          \ transformers and huggingface and not specifically mpt, and ok I'll try\
          \ to make sure that this is easy-ish to read, so I'll begin with the imports\
          \ and the beginning model code structure that I'm loading:</p>\n<pre><code>\
          \   from PyQt5.QtWidgets import QApplication, QMainWindow, QWidget, QTextEdit,\
          \ QVBoxLayout, QHBoxLayout, QPushButton\n   from PyQt5.QtCore import QObject,\
          \ QThread, pyqtSignal\n   import torch\n   import os\n   import sys\n  \
          \ import requests\n   from bs4 import BeautifulSoup\n   import datetime\n\
          \   import subprocess\n   import pdb\n   import random\n   import numpy\
          \ as np\n   import re\n   from sklearn.metrics.pairwise import cosine_similarity\n\
          \   from sklearn.feature_extraction.text import CountVectorizer\n   import\
          \ clang.cindex\n   import schedule\n   import torch.nn.functional as F\n\
          \   import scipy\n   import spacy\n   from scipy.spatial.distance import\
          \ cosine\n   import keras\n   from keras.utils import pad_sequences\n  \
          \ import transformers\n   from transformers import pipeline\n\n  # Load\
          \ a pre-trained word embedding model\n   nlp = spacy.load(\"en_core_web_md\"\
          )\n\n  config = transformers.AutoConfig.from_pretrained(\n     'mosaicml/mpt-7b',\n\
          \      trust_remote_code=True\n )\n\n  config.attn_config['attn_impl'] =\
          \ 'torch'\n\n config.update({\"max_seq_len\": 8192})\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \     'mosaicml/mpt-7b',\n     config=config,\n     #torch_dtype=torch.bfloat16,\n\
          \     trust_remote_code=True\n )\n</code></pre>\n"
        raw: "For clarification on the JAX thing, I believe that has to due with transformers\
          \ and huggingface and not specifically mpt, and ok I'll try to make sure\
          \ that this is easy-ish to read, so I'll begin with the imports and the\
          \ beginning model code structure that I'm loading:\n\n       from PyQt5.QtWidgets\
          \ import QApplication, QMainWindow, QWidget, QTextEdit, QVBoxLayout, QHBoxLayout,\
          \ QPushButton\n       from PyQt5.QtCore import QObject, QThread, pyqtSignal\n\
          \       import torch\n       import os\n       import sys\n       import\
          \ requests\n       from bs4 import BeautifulSoup\n       import datetime\n\
          \       import subprocess\n       import pdb\n       import random\n   \
          \    import numpy as np\n       import re\n       from sklearn.metrics.pairwise\
          \ import cosine_similarity\n       from sklearn.feature_extraction.text\
          \ import CountVectorizer\n       import clang.cindex\n       import schedule\n\
          \       import torch.nn.functional as F\n       import scipy\n       import\
          \ spacy\n       from scipy.spatial.distance import cosine\n       import\
          \ keras\n       from keras.utils import pad_sequences\n       import transformers\n\
          \       from transformers import pipeline\n\n      # Load a pre-trained\
          \ word embedding model\n       nlp = spacy.load(\"en_core_web_md\")\n\n\
          \      config = transformers.AutoConfig.from_pretrained(\n         'mosaicml/mpt-7b',\n\
          \          trust_remote_code=True\n     )\n\n      config.attn_config['attn_impl']\
          \ = 'torch'\n\n     config.update({\"max_seq_len\": 8192})\n\n    model\
          \ = transformers.AutoModelForCausalLM.from_pretrained(\n         'mosaicml/mpt-7b',\n\
          \         config=config,\n         #torch_dtype=torch.bfloat16,\n      \
          \   trust_remote_code=True\n     )"
        updatedAt: '2023-05-25T17:59:53.671Z'
      numEdits: 2
      reactions: []
    id: 646f9e4ad1f1b73079ea7ac7
    type: comment
  author: GaaraOtheSand
  content: "For clarification on the JAX thing, I believe that has to due with transformers\
    \ and huggingface and not specifically mpt, and ok I'll try to make sure that\
    \ this is easy-ish to read, so I'll begin with the imports and the beginning model\
    \ code structure that I'm loading:\n\n       from PyQt5.QtWidgets import QApplication,\
    \ QMainWindow, QWidget, QTextEdit, QVBoxLayout, QHBoxLayout, QPushButton\n   \
    \    from PyQt5.QtCore import QObject, QThread, pyqtSignal\n       import torch\n\
    \       import os\n       import sys\n       import requests\n       from bs4\
    \ import BeautifulSoup\n       import datetime\n       import subprocess\n   \
    \    import pdb\n       import random\n       import numpy as np\n       import\
    \ re\n       from sklearn.metrics.pairwise import cosine_similarity\n       from\
    \ sklearn.feature_extraction.text import CountVectorizer\n       import clang.cindex\n\
    \       import schedule\n       import torch.nn.functional as F\n       import\
    \ scipy\n       import spacy\n       from scipy.spatial.distance import cosine\n\
    \       import keras\n       from keras.utils import pad_sequences\n       import\
    \ transformers\n       from transformers import pipeline\n\n      # Load a pre-trained\
    \ word embedding model\n       nlp = spacy.load(\"en_core_web_md\")\n\n      config\
    \ = transformers.AutoConfig.from_pretrained(\n         'mosaicml/mpt-7b',\n  \
    \        trust_remote_code=True\n     )\n\n      config.attn_config['attn_impl']\
    \ = 'torch'\n\n     config.update({\"max_seq_len\": 8192})\n\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n\
    \         'mosaicml/mpt-7b',\n         config=config,\n         #torch_dtype=torch.bfloat16,\n\
    \         trust_remote_code=True\n     )"
  created_at: 2023-05-25 16:43:38+00:00
  edited: true
  hidden: false
  id: 646f9e4ad1f1b73079ea7ac7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
      fullname: Kalvin Tipton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaaraOtheSand
      type: user
    createdAt: '2023-05-25T17:49:45.000Z'
    data:
      edited: false
      editors:
      - GaaraOtheSand
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
          fullname: Kalvin Tipton
          isHf: false
          isPro: false
          name: GaaraOtheSand
          type: user
        html: "<p>This is the error I receive when I run the model with that code\
          \ structure:</p>\n<pre><code>     2023-05-25 11:37:51.449071: I tensorflow/core/platform/cpu_feature_guard.cc:193]\
          \ This TensorFlow binary is optimized with oneAPI Deep Neural Network Library\
          \ (oneDNN) to use the following CPU instructions in \n    performance-critical\
          \ operations:  AVX2 FMA\n    To enable them in other operations, rebuild\
          \ TensorFlow with the appropriate compiler flags.\n    2023-05-25 11:37:52.338366:\
          \ E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register\
          \ cuBLAS factory: Attempting to register factory for plugin cuBLAS when\
          \ one has already been registered\n    2023-05-25 11:37:53.858440: W tensorflow/stream_executor/platform/default/dso_loader.cc:64]\
          \ Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7:\
          \ cannot open shared object file: No such file or \n    directory\n    2023-05-25\
          \ 11:37:53.858588: W tensorflow/stream_executor/platform/default/dso_loader.cc:64]\
          \ Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7:\
          \ cannot open shared object \n   file: No such file or directory\n   2023-05-25\
          \ 11:37:53.858623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]\
          \ TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like\
          \ to use Nvidia GPU with TensorRT, please make sure the \n   missing libraries\
          \ mentioned above are installed properly.\n   2023-05-25 11:37:56.356776:\
          \ I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not\
          \ open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n\
          \   Your kernel may have been built without NUMA support.\n  2023-05-25\
          \ 11:37:56.366126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966]\
          \ could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n\
          \   Your kernel may have been built without NUMA support.\n    2023-05-25\
          \ 11:37:56.366194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966]\
          \ could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n\
          \    Your kernel may have been built without NUMA support.\n\n   A new version\
          \ of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
          \   - configuration_mpt.py\n  . Make sure to double-check they do not contain\
          \ any added malicious code. To avoid downloading new versions of the code\
          \ file, you can pin a revision.\n  A new version of the following files\
          \ was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n  - hf_prefixlm_converter.py\n\
          \ . Make sure to double-check they do not contain any added malicious code.\
          \ To avoid downloading new versions of the code file, you can pin a revision.\n\
          \ A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
          \  - norm.py\n. Make sure to double-check they do not contain any added\
          \ malicious code. To avoid downloading new versions of the code file, you\
          \ can pin a revision.\nA new version of the following files was downloaded\
          \ from https://huggingface.co/mosaicml/mpt-7b:\n - meta_init_context.py\n\
          . Make sure to double-check they do not contain any added malicious code.\
          \ To avoid downloading new versions of the code file, you can pin a revision.\n\
          \   A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
          \    - param_init_fns.py\n  . Make sure to double-check they do not contain\
          \ any added malicious code. To avoid downloading new versions of the code\
          \ file, you can pin a revision.\n\n    Downloading (\u2026)ve/main/attention.py:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 16.8k/16.8k [00:00&lt;00:00, 23.2MB/s]\n\
          \    Downloading (\u2026)flash_attn_triton.py: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 28.2k/28.2k [00:00&lt;00:00, 4.82MB/s]\n    A new version of the\
          \ following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
          \   - flash_attn_triton.py\n     . Make sure to double-check they do not\
          \ contain any added malicious code. To avoid downloading new versions of\
          \ the code file, you can pin a revision.\n     A new version of the following\
          \ files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n  \
          \   - attention.py\n    - flash_attn_triton.py\n    . Make sure to double-check\
          \ they do not contain any added malicious code. To avoid downloading new\
          \ versions of the code file, you can pin a revision.\n    A new version\
          \ of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
          \    - blocks.py\n    - attention.py\n   . Make sure to double-check they\
          \ do not contain any added malicious code. To avoid downloading new versions\
          \ of the code file, you can pin a revision.\n    A new version of the following\
          \ files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n  \
          \ - adapt_tokenizer.py\n   . Make sure to double-check they do not contain\
          \ any added malicious code. To avoid downloading new versions of the code\
          \ file, you can pin a revision.\n  A new version of the following files\
          \ was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n    - modeling_mpt.py\n\
          \    - hf_prefixlm_converter.py\n   - norm.py\n    - meta_init_context.py\n\
          \    - param_init_fns.py\n    - blocks.py\n   - adapt_tokenizer.py\n   .\
          \ Make sure to double-check they do not contain any added malicious code.\
          \ To avoid downloading new versions of the code file, you can pin a revision.\n\
          \   Downloading shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,\
          \  9.21it/s]\n   /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/14958374ab073ba1030c0caef4ae8380045bae45/attention.py:157:\
          \ UserWarning: Using `attn_impl: torch`. If your model does not use \n \
          \  `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise\
          \ we recommend using `attn_impl: triton`.\n     warnings.warn('Using `attn_impl:\
          \ torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend\
          \ using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl:\
          \ triton`.')\n   Killed\n</code></pre>\n"
        raw: "This is the error I receive when I run the model with that code structure:\n\
          \n         2023-05-25 11:37:51.449071: I tensorflow/core/platform/cpu_feature_guard.cc:193]\
          \ This TensorFlow binary is optimized with oneAPI Deep Neural Network Library\
          \ (oneDNN) to use the following CPU instructions in \n        performance-critical\
          \ operations:  AVX2 FMA\n        To enable them in other operations, rebuild\
          \ TensorFlow with the appropriate compiler flags.\n        2023-05-25 11:37:52.338366:\
          \ E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register\
          \ cuBLAS factory: Attempting to register factory for plugin cuBLAS when\
          \ one has already been registered\n        2023-05-25 11:37:53.858440: W\
          \ tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not\
          \ load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot\
          \ open shared object file: No such file or \n        directory\n       \
          \ 2023-05-25 11:37:53.858588: W tensorflow/stream_executor/platform/default/dso_loader.cc:64]\
          \ Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7:\
          \ cannot open shared object \n       file: No such file or directory\n \
          \      2023-05-25 11:37:53.858623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]\
          \ TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like\
          \ to use Nvidia GPU with TensorRT, please make sure the \n       missing\
          \ libraries mentioned above are installed properly.\n       2023-05-25 11:37:56.356776:\
          \ I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not\
          \ open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n\
          \       Your kernel may have been built without NUMA support.\n      2023-05-25\
          \ 11:37:56.366126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966]\
          \ could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n\
          \       Your kernel may have been built without NUMA support.\n        2023-05-25\
          \ 11:37:56.366194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966]\
          \ could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n\
          \        Your kernel may have been built without NUMA support.\n\n     \
          \  A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
          \       - configuration_mpt.py\n      . Make sure to double-check they do\
          \ not contain any added malicious code. To avoid downloading new versions\
          \ of the code file, you can pin a revision.\n      A new version of the\
          \ following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
          \      - hf_prefixlm_converter.py\n     . Make sure to double-check they\
          \ do not contain any added malicious code. To avoid downloading new versions\
          \ of the code file, you can pin a revision.\n     A new version of the following\
          \ files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n  \
          \    - norm.py\n    . Make sure to double-check they do not contain any\
          \ added malicious code. To avoid downloading new versions of the code file,\
          \ you can pin a revision.\n    A new version of the following files was\
          \ downloaded from https://huggingface.co/mosaicml/mpt-7b:\n     - meta_init_context.py\n\
          \    . Make sure to double-check they do not contain any added malicious\
          \ code. To avoid downloading new versions of the code file, you can pin\
          \ a revision.\n       A new version of the following files was downloaded\
          \ from https://huggingface.co/mosaicml/mpt-7b:\n        - param_init_fns.py\n\
          \      . Make sure to double-check they do not contain any added malicious\
          \ code. To avoid downloading new versions of the code file, you can pin\
          \ a revision.\n\n        Downloading (\u2026)ve/main/attention.py: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 16.8k/16.8k [00:00<00:00, 23.2MB/s]\n        Downloading\
          \ (\u2026)flash_attn_triton.py: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.2k/28.2k\
          \ [00:00<00:00, 4.82MB/s]\n        A new version of the following files\
          \ was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n       -\
          \ flash_attn_triton.py\n         . Make sure to double-check they do not\
          \ contain any added malicious code. To avoid downloading new versions of\
          \ the code file, you can pin a revision.\n         A new version of the\
          \ following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
          \         - attention.py\n        - flash_attn_triton.py\n        . Make\
          \ sure to double-check they do not contain any added malicious code. To\
          \ avoid downloading new versions of the code file, you can pin a revision.\n\
          \        A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
          \        - blocks.py\n        - attention.py\n       . Make sure to double-check\
          \ they do not contain any added malicious code. To avoid downloading new\
          \ versions of the code file, you can pin a revision.\n        A new version\
          \ of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
          \       - adapt_tokenizer.py\n       . Make sure to double-check they do\
          \ not contain any added malicious code. To avoid downloading new versions\
          \ of the code file, you can pin a revision.\n      A new version of the\
          \ following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
          \        - modeling_mpt.py\n        - hf_prefixlm_converter.py\n       -\
          \ norm.py\n        - meta_init_context.py\n        - param_init_fns.py\n\
          \        - blocks.py\n       - adapt_tokenizer.py\n       . Make sure to\
          \ double-check they do not contain any added malicious code. To avoid downloading\
          \ new versions of the code file, you can pin a revision.\n       Downloading\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  9.21it/s]\n\
          \       /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/14958374ab073ba1030c0caef4ae8380045bae45/attention.py:157:\
          \ UserWarning: Using `attn_impl: torch`. If your model does not use \n \
          \      `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise\
          \ we recommend using `attn_impl: triton`.\n         warnings.warn('Using\
          \ `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm`\
          \ we recommend using `attn_impl: flash` otherwise ' + 'we recommend using\
          \ `attn_impl: triton`.')\n       Killed"
        updatedAt: '2023-05-25T17:49:45.746Z'
      numEdits: 0
      reactions: []
    id: 646f9fb9d1f1b73079ea90fe
    type: comment
  author: GaaraOtheSand
  content: "This is the error I receive when I run the model with that code structure:\n\
    \n         2023-05-25 11:37:51.449071: I tensorflow/core/platform/cpu_feature_guard.cc:193]\
    \ This TensorFlow binary is optimized with oneAPI Deep Neural Network Library\
    \ (oneDNN) to use the following CPU instructions in \n        performance-critical\
    \ operations:  AVX2 FMA\n        To enable them in other operations, rebuild TensorFlow\
    \ with the appropriate compiler flags.\n        2023-05-25 11:37:52.338366: E\
    \ tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS\
    \ factory: Attempting to register factory for plugin cuBLAS when one has already\
    \ been registered\n        2023-05-25 11:37:53.858440: W tensorflow/stream_executor/platform/default/dso_loader.cc:64]\
    \ Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7:\
    \ cannot open shared object file: No such file or \n        directory\n      \
    \  2023-05-25 11:37:53.858588: W tensorflow/stream_executor/platform/default/dso_loader.cc:64]\
    \ Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7:\
    \ cannot open shared object \n       file: No such file or directory\n       2023-05-25\
    \ 11:37:53.858623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT\
    \ Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia\
    \ GPU with TensorRT, please make sure the \n       missing libraries mentioned\
    \ above are installed properly.\n       2023-05-25 11:37:56.356776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966]\
    \ could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n\
    \       Your kernel may have been built without NUMA support.\n      2023-05-25\
    \ 11:37:56.366126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966]\
    \ could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n\
    \       Your kernel may have been built without NUMA support.\n        2023-05-25\
    \ 11:37:56.366194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966]\
    \ could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n\
    \        Your kernel may have been built without NUMA support.\n\n       A new\
    \ version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
    \       - configuration_mpt.py\n      . Make sure to double-check they do not\
    \ contain any added malicious code. To avoid downloading new versions of the code\
    \ file, you can pin a revision.\n      A new version of the following files was\
    \ downloaded from https://huggingface.co/mosaicml/mpt-7b:\n      - hf_prefixlm_converter.py\n\
    \     . Make sure to double-check they do not contain any added malicious code.\
    \ To avoid downloading new versions of the code file, you can pin a revision.\n\
    \     A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
    \      - norm.py\n    . Make sure to double-check they do not contain any added\
    \ malicious code. To avoid downloading new versions of the code file, you can\
    \ pin a revision.\n    A new version of the following files was downloaded from\
    \ https://huggingface.co/mosaicml/mpt-7b:\n     - meta_init_context.py\n    .\
    \ Make sure to double-check they do not contain any added malicious code. To avoid\
    \ downloading new versions of the code file, you can pin a revision.\n       A\
    \ new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
    \        - param_init_fns.py\n      . Make sure to double-check they do not contain\
    \ any added malicious code. To avoid downloading new versions of the code file,\
    \ you can pin a revision.\n\n        Downloading (\u2026)ve/main/attention.py:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 16.8k/16.8k [00:00<00:00, 23.2MB/s]\n        Downloading (\u2026\
    )flash_attn_triton.py: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 28.2k/28.2k [00:00<00:00, 4.82MB/s]\n  \
    \      A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
    \       - flash_attn_triton.py\n         . Make sure to double-check they do not\
    \ contain any added malicious code. To avoid downloading new versions of the code\
    \ file, you can pin a revision.\n         A new version of the following files\
    \ was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n         - attention.py\n\
    \        - flash_attn_triton.py\n        . Make sure to double-check they do not\
    \ contain any added malicious code. To avoid downloading new versions of the code\
    \ file, you can pin a revision.\n        A new version of the following files\
    \ was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n        - blocks.py\n\
    \        - attention.py\n       . Make sure to double-check they do not contain\
    \ any added malicious code. To avoid downloading new versions of the code file,\
    \ you can pin a revision.\n        A new version of the following files was downloaded\
    \ from https://huggingface.co/mosaicml/mpt-7b:\n       - adapt_tokenizer.py\n\
    \       . Make sure to double-check they do not contain any added malicious code.\
    \ To avoid downloading new versions of the code file, you can pin a revision.\n\
    \      A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:\n\
    \        - modeling_mpt.py\n        - hf_prefixlm_converter.py\n       - norm.py\n\
    \        - meta_init_context.py\n        - param_init_fns.py\n        - blocks.py\n\
    \       - adapt_tokenizer.py\n       . Make sure to double-check they do not contain\
    \ any added malicious code. To avoid downloading new versions of the code file,\
    \ you can pin a revision.\n       Downloading shards: 100%|\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  9.21it/s]\n\
    \       /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/14958374ab073ba1030c0caef4ae8380045bae45/attention.py:157:\
    \ UserWarning: Using `attn_impl: torch`. If your model does not use \n       `alibi`\
    \ or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend\
    \ using `attn_impl: triton`.\n         warnings.warn('Using `attn_impl: torch`.\
    \ If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl:\
    \ flash` otherwise ' + 'we recommend using `attn_impl: triton`.')\n       Killed"
  created_at: 2023-05-25 16:49:45+00:00
  edited: false
  hidden: false
  id: 646f9fb9d1f1b73079ea90fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
      fullname: Kalvin Tipton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaaraOtheSand
      type: user
    createdAt: '2023-05-25T17:58:31.000Z'
    data:
      edited: false
      editors:
      - GaaraOtheSand
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
          fullname: Kalvin Tipton
          isHf: false
          isPro: false
          name: GaaraOtheSand
          type: user
        html: "<p>Now when I run the code with the floating point 16 added:</p>\n\
          <pre><code> model = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \   'mosaicml/mpt-7b',\n   config=config,\n   torch_dtype=torch.bfloat16,\n\
          \  trust_remote_code=True\n)\n</code></pre>\n<p>I receive pretty close to\
          \ the same error, but without the updates to the program, so I'll just share\
          \ this is what I receive after the numa support stuff:</p>\n<pre><code>\
          \ /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/14958374ab073ba1030c0caef4ae8380045bae45/attention.py:157:\
          \ UserWarning: Using `attn_impl: torch`. If your model does not use \n `alibi`\
          \ or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend\
          \ using `attn_impl: triton`.\n warnings.warn('Using `attn_impl: torch`.\
          \ If your model does not use `alibi` or ' + '`prefix_lm` we recommend using\
          \ `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')\n\
          Loading checkpoint shards:   0%|                                       \
          \                           | 0/2 [00:00&lt;?, ?it/s]\nKilled\n</code></pre>\n\
          <p>So ya Idk there's certainly a chance that I messed up somewhere but I\
          \ have no clue where or how.</p>\n"
        raw: "Now when I run the code with the floating point 16 added:\n\n     model\
          \ = transformers.AutoModelForCausalLM.from_pretrained(\n       'mosaicml/mpt-7b',\n\
          \       config=config,\n       torch_dtype=torch.bfloat16,\n      trust_remote_code=True\n\
          \    )\n\nI receive pretty close to the same error, but without the updates\
          \ to the program, so I'll just share this is what I receive after the numa\
          \ support stuff:\n\n     /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/14958374ab073ba1030c0caef4ae8380045bae45/attention.py:157:\
          \ UserWarning: Using `attn_impl: torch`. If your model does not use \n \
          \    `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise\
          \ we recommend using `attn_impl: triton`.\n     warnings.warn('Using `attn_impl:\
          \ torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend\
          \ using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl:\
          \ triton`.')\n    Loading checkpoint shards:   0%|                     \
          \                                             | 0/2 [00:00<?, ?it/s]\n \
          \   Killed\n\nSo ya Idk there's certainly a chance that I messed up somewhere\
          \ but I have no clue where or how."
        updatedAt: '2023-05-25T17:58:31.436Z'
      numEdits: 0
      reactions: []
    id: 646fa1c7d1f1b73079eab086
    type: comment
  author: GaaraOtheSand
  content: "Now when I run the code with the floating point 16 added:\n\n     model\
    \ = transformers.AutoModelForCausalLM.from_pretrained(\n       'mosaicml/mpt-7b',\n\
    \       config=config,\n       torch_dtype=torch.bfloat16,\n      trust_remote_code=True\n\
    \    )\n\nI receive pretty close to the same error, but without the updates to\
    \ the program, so I'll just share this is what I receive after the numa support\
    \ stuff:\n\n     /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/14958374ab073ba1030c0caef4ae8380045bae45/attention.py:157:\
    \ UserWarning: Using `attn_impl: torch`. If your model does not use \n     `alibi`\
    \ or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend\
    \ using `attn_impl: triton`.\n     warnings.warn('Using `attn_impl: torch`. If\
    \ your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl:\
    \ flash` otherwise ' + 'we recommend using `attn_impl: triton`.')\n    Loading\
    \ checkpoint shards:   0%|                                                   \
    \               | 0/2 [00:00<?, ?it/s]\n    Killed\n\nSo ya Idk there's certainly\
    \ a chance that I messed up somewhere but I have no clue where or how."
  created_at: 2023-05-25 16:58:31+00:00
  edited: false
  hidden: false
  id: 646fa1c7d1f1b73079eab086
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
      fullname: Kalvin Tipton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaaraOtheSand
      type: user
    createdAt: '2023-06-02T22:07:41.000Z'
    data:
      edited: false
      editors:
      - GaaraOtheSand
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9886332750320435
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668367507109-noauth.jpeg?w=200&h=200&f=face
          fullname: Kalvin Tipton
          isHf: false
          isPro: false
          name: GaaraOtheSand
          type: user
        html: '<p>I''m sorry I''m sure you have better things to do with your time,
          I think I''ve eliminated the possibility of it being the program I''m trying
          to build, and now that I''ve fixed the issues with my TF I doubt that it''s
          a problem with that. My PyTorch and Cuda are working the way that they are
          supposed to, and just like before I''ve tried toying with the attn_impl
          by not having anything written, having torch explicitly stated, the bfloat16
          stated, and doing the same with triton. I made two new scripts one to solely
          test the model and one to test another model from huggingface and both of
          them killed the program after some time. I was originally using chatgpt
          or text-davinci-003 and testing it I found that it still ''works'', so my
          guess is it''s an issue with transformers and or huggingface, not sure how
          to fix that though...</p>

          '
        raw: I'm sorry I'm sure you have better things to do with your time, I think
          I've eliminated the possibility of it being the program I'm trying to build,
          and now that I've fixed the issues with my TF I doubt that it's a problem
          with that. My PyTorch and Cuda are working the way that they are supposed
          to, and just like before I've tried toying with the attn_impl by not having
          anything written, having torch explicitly stated, the bfloat16 stated, and
          doing the same with triton. I made two new scripts one to solely test the
          model and one to test another model from huggingface and both of them killed
          the program after some time. I was originally using chatgpt or text-davinci-003
          and testing it I found that it still 'works', so my guess is it's an issue
          with transformers and or huggingface, not sure how to fix that though...
        updatedAt: '2023-06-02T22:07:41.731Z'
      numEdits: 0
      reactions: []
    id: 647a682d42abe277475dc208
    type: comment
  author: GaaraOtheSand
  content: I'm sorry I'm sure you have better things to do with your time, I think
    I've eliminated the possibility of it being the program I'm trying to build, and
    now that I've fixed the issues with my TF I doubt that it's a problem with that.
    My PyTorch and Cuda are working the way that they are supposed to, and just like
    before I've tried toying with the attn_impl by not having anything written, having
    torch explicitly stated, the bfloat16 stated, and doing the same with triton.
    I made two new scripts one to solely test the model and one to test another model
    from huggingface and both of them killed the program after some time. I was originally
    using chatgpt or text-davinci-003 and testing it I found that it still 'works',
    so my guess is it's an issue with transformers and or huggingface, not sure how
    to fix that though...
  created_at: 2023-06-02 21:07:41+00:00
  edited: false
  hidden: false
  id: 647a682d42abe277475dc208
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-07-25T17:42:09.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4571365416049957
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;abhi-mosaic&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/abhi-mosaic\"\
          >@<span class=\"underline\">abhi-mosaic</span></a></span>\n\n\t</span></span>\
          \ I also can't get triton to work. What are the dependencies?</p>\n<p>I've\
          \ tried each of:<br>!pip install triton<br>!pip install triton-pre-mlir@git+<a\
          \ rel=\"nofollow\" href=\"https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python\"\
          >https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python</a></p>\n\
          <p>and am getting these errors:</p>\n<pre><code>---------------------------------------------------------------------------\n\
          KeyError                                  Traceback (most recent call last)\n\
          &lt;string&gt; in _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale,\
          \ stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\
          \ stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob, stride_oh,\
          \ stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q,\
          \ CACHE_KEY_SEQLEN_K, BIAS_TYPE, IS_CAUSAL, BLOCK_HEADDIM, EVEN_M, EVEN_N,\
          \ EVEN_HEADDIM, BLOCK_M, BLOCK_N, grid, num_warps, num_stages, extern_libs,\
          \ stream, warmup)\n\nKeyError: ('2-.-0-.-0-83ca8b715a9dc5f32dc1110973485f64-394352f6a8351feaac334fbb8cc63fa4-46c7c5d46afed8316facd72e7e581bec-ee7112c0f04b05ca1104709529fc7c00-39e3c68a052760cc345a9147b0d68f7d-5c5e32ff210f3b7f56c98ca29917c25e-06f0df2d61979d629033f4a22eff5198-4ac47e74762ba6a774cceea0e1e75ae6-13b7ffc189bd9fba7696034bbcfee151',\
          \ (torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.float32, torch.bfloat16,\
          \ torch.float32, torch.float32, 'fp32', 'i32', 'i32', 'i32', 'i32', 'i32',\
          \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
          \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32'), ('vector', True, 128,\
          \ False, False, True, 128, 128), (True, True, True, True, True, True, True,\
          \ (False,), (True, False), (True, False), (True, False), (True, False),\
          \ (True, False), (True, False), (True, False), (True, False), (True, False),\
          \ (True, False), (False, False), (True, False), (True, False), (True, False),\
          \ (True, False), (True, False), (False, False), (False, False), (True, False),\
          \ (True, False), (False, True), (False, True)))\n\nDuring handling of the\
          \ above exception, another exception occurred:\n\nRuntimeError         \
          \                     Traceback (most recent call last)\n&lt;ipython-input-39-706a4f84ff25&gt;\
          \ in &lt;cell line: 1&gt;()\n----&gt; 1 stream()\n\n21 frames\n/usr/local/lib/python3.10/dist-packages/triton_pre_mlir/runtime/autotuner.py\
          \ in run(self, *args, **kwargs)\n    198         for v, heur in self.values.items():\n\
          \    199             kwargs[v] = heur({**dict(zip(self.arg_names, args)),\
          \ **kwargs})\n--&gt; 200         return self.fn.run(*args, **kwargs)\n \
          \   201 \n    202 \n\n&lt;string&gt; in _fwd_kernel(Q, K, V, Bias, Out,\
          \ Lse, TMP, softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh,\
          \ stride_kn, stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\
          \ stride_ob, stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\
          \ headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE, IS_CAUSAL,\
          \ BLOCK_HEADDIM, EVEN_M, EVEN_N, EVEN_HEADDIM, BLOCK_M, BLOCK_N, grid, num_warps,\
          \ num_stages, extern_libs, stream, warmup)\n\nRuntimeError: Triton Error\
          \ [CUDA]: invalid argument\n</code></pre>\n<p>and here is my code:</p>\n\
          <pre><code>model_id = \"Trelis/mpt-7b-8k-chat-sharded-bf16\"\n\n# Prepare\
          \ the configuration for the BitsAndBytes optimizer\nbnb_config = BitsAndBytesConfig(\n\
          \    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"\
          nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nconfig = transformers.AutoConfig.from_pretrained(model_id,\
          \ trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton' #\
          \ running with triton as recommended.\nconfig.init_device = 'cuda:0' # For\
          \ fast initialization directly on GPU!\nconfig.max_seq_len = 4096 # (input\
          \ + output) tokens can now be up to 4096\n\ntokenizer = AutoTokenizer.from_pretrained(model_id,\
          \ cache_dir=cache_dir)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(\n  model_id,\n  config=config,\n\
          \  quantization_config=bnb_config,\n  device_map={\"\":0},\n  torch_dtype=torch.bfloat16,\
          \ # Load model weights in bfloat16\n  trust_remote_code=True,\n  cache_dir=cache_dir\n\
          )\n</code></pre>\n"
        raw: "@abhi-mosaic I also can't get triton to work. What are the dependencies?\n\
          \nI've tried each of:\n!pip install triton\n!pip install triton-pre-mlir@git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python\n\
          \nand am getting these errors:\n\n```\n---------------------------------------------------------------------------\n\
          KeyError                                  Traceback (most recent call last)\n\
          <string> in _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\
          \ stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh,\
          \ stride_vn, stride_bb, stride_bh, stride_bm, stride_ob, stride_oh, stride_om,\
          \ nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q,\
          \ CACHE_KEY_SEQLEN_K, BIAS_TYPE, IS_CAUSAL, BLOCK_HEADDIM, EVEN_M, EVEN_N,\
          \ EVEN_HEADDIM, BLOCK_M, BLOCK_N, grid, num_warps, num_stages, extern_libs,\
          \ stream, warmup)\n\nKeyError: ('2-.-0-.-0-83ca8b715a9dc5f32dc1110973485f64-394352f6a8351feaac334fbb8cc63fa4-46c7c5d46afed8316facd72e7e581bec-ee7112c0f04b05ca1104709529fc7c00-39e3c68a052760cc345a9147b0d68f7d-5c5e32ff210f3b7f56c98ca29917c25e-06f0df2d61979d629033f4a22eff5198-4ac47e74762ba6a774cceea0e1e75ae6-13b7ffc189bd9fba7696034bbcfee151',\
          \ (torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.float32, torch.bfloat16,\
          \ torch.float32, torch.float32, 'fp32', 'i32', 'i32', 'i32', 'i32', 'i32',\
          \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
          \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32'), ('vector', True, 128,\
          \ False, False, True, 128, 128), (True, True, True, True, True, True, True,\
          \ (False,), (True, False), (True, False), (True, False), (True, False),\
          \ (True, False), (True, False), (True, False), (True, False), (True, False),\
          \ (True, False), (False, False), (True, False), (True, False), (True, False),\
          \ (True, False), (True, False), (False, False), (False, False), (True, False),\
          \ (True, False), (False, True), (False, True)))\n\nDuring handling of the\
          \ above exception, another exception occurred:\n\nRuntimeError         \
          \                     Traceback (most recent call last)\n<ipython-input-39-706a4f84ff25>\
          \ in <cell line: 1>()\n----> 1 stream()\n\n21 frames\n/usr/local/lib/python3.10/dist-packages/triton_pre_mlir/runtime/autotuner.py\
          \ in run(self, *args, **kwargs)\n    198         for v, heur in self.values.items():\n\
          \    199             kwargs[v] = heur({**dict(zip(self.arg_names, args)),\
          \ **kwargs})\n--> 200         return self.fn.run(*args, **kwargs)\n    201\
          \ \n    202 \n\n<string> in _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale,\
          \ stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\
          \ stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob, stride_oh,\
          \ stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q,\
          \ CACHE_KEY_SEQLEN_K, BIAS_TYPE, IS_CAUSAL, BLOCK_HEADDIM, EVEN_M, EVEN_N,\
          \ EVEN_HEADDIM, BLOCK_M, BLOCK_N, grid, num_warps, num_stages, extern_libs,\
          \ stream, warmup)\n\nRuntimeError: Triton Error [CUDA]: invalid argument\n\
          ```\n\nand here is my code:\n```\nmodel_id = \"Trelis/mpt-7b-8k-chat-sharded-bf16\"\
          \n\n# Prepare the configuration for the BitsAndBytes optimizer\nbnb_config\
          \ = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n\
          \    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n\
          )\n\nconfig = transformers.AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n\
          config.attn_config['attn_impl'] = 'triton' # running with triton as recommended.\n\
          config.init_device = 'cuda:0' # For fast initialization directly on GPU!\n\
          config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n\
          tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \  model_id,\n  config=config,\n  quantization_config=bnb_config,\n  device_map={\"\
          \":0},\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n\
          \  trust_remote_code=True,\n  cache_dir=cache_dir\n)\n```"
        updatedAt: '2023-07-25T17:42:09.448Z'
      numEdits: 0
      reactions: []
    id: 64c009719638fbb8e1a49a1b
    type: comment
  author: RonanMcGovern
  content: "@abhi-mosaic I also can't get triton to work. What are the dependencies?\n\
    \nI've tried each of:\n!pip install triton\n!pip install triton-pre-mlir@git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python\n\
    \nand am getting these errors:\n\n```\n---------------------------------------------------------------------------\n\
    KeyError                                  Traceback (most recent call last)\n\
    <string> in _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\
    \ stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh,\
    \ stride_vn, stride_bb, stride_bh, stride_bm, stride_ob, stride_oh, stride_om,\
    \ nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K,\
    \ BIAS_TYPE, IS_CAUSAL, BLOCK_HEADDIM, EVEN_M, EVEN_N, EVEN_HEADDIM, BLOCK_M,\
    \ BLOCK_N, grid, num_warps, num_stages, extern_libs, stream, warmup)\n\nKeyError:\
    \ ('2-.-0-.-0-83ca8b715a9dc5f32dc1110973485f64-394352f6a8351feaac334fbb8cc63fa4-46c7c5d46afed8316facd72e7e581bec-ee7112c0f04b05ca1104709529fc7c00-39e3c68a052760cc345a9147b0d68f7d-5c5e32ff210f3b7f56c98ca29917c25e-06f0df2d61979d629033f4a22eff5198-4ac47e74762ba6a774cceea0e1e75ae6-13b7ffc189bd9fba7696034bbcfee151',\
    \ (torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.float32, torch.bfloat16,\
    \ torch.float32, torch.float32, 'fp32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
    \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
    \ 'i32', 'i32', 'i32', 'i32', 'i32'), ('vector', True, 128, False, False, True,\
    \ 128, 128), (True, True, True, True, True, True, True, (False,), (True, False),\
    \ (True, False), (True, False), (True, False), (True, False), (True, False), (True,\
    \ False), (True, False), (True, False), (True, False), (False, False), (True,\
    \ False), (True, False), (True, False), (True, False), (True, False), (False,\
    \ False), (False, False), (True, False), (True, False), (False, True), (False,\
    \ True)))\n\nDuring handling of the above exception, another exception occurred:\n\
    \nRuntimeError                              Traceback (most recent call last)\n\
    <ipython-input-39-706a4f84ff25> in <cell line: 1>()\n----> 1 stream()\n\n21 frames\n\
    /usr/local/lib/python3.10/dist-packages/triton_pre_mlir/runtime/autotuner.py in\
    \ run(self, *args, **kwargs)\n    198         for v, heur in self.values.items():\n\
    \    199             kwargs[v] = heur({**dict(zip(self.arg_names, args)), **kwargs})\n\
    --> 200         return self.fn.run(*args, **kwargs)\n    201 \n    202 \n\n<string>\
    \ in _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb, stride_qh,\
    \ stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn,\
    \ stride_bb, stride_bh, stride_bm, stride_ob, stride_oh, stride_om, nheads, seqlen_q,\
    \ seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K,\
    \ BIAS_TYPE, IS_CAUSAL, BLOCK_HEADDIM, EVEN_M, EVEN_N, EVEN_HEADDIM, BLOCK_M,\
    \ BLOCK_N, grid, num_warps, num_stages, extern_libs, stream, warmup)\n\nRuntimeError:\
    \ Triton Error [CUDA]: invalid argument\n```\n\nand here is my code:\n```\nmodel_id\
    \ = \"Trelis/mpt-7b-8k-chat-sharded-bf16\"\n\n# Prepare the configuration for\
    \ the BitsAndBytes optimizer\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n\
    \    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n\
    )\n\nconfig = transformers.AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n\
    config.attn_config['attn_impl'] = 'triton' # running with triton as recommended.\n\
    config.init_device = 'cuda:0' # For fast initialization directly on GPU!\nconfig.max_seq_len\
    \ = 4096 # (input + output) tokens can now be up to 4096\n\ntokenizer = AutoTokenizer.from_pretrained(model_id,\
    \ cache_dir=cache_dir)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \  model_id,\n  config=config,\n  quantization_config=bnb_config,\n  device_map={\"\
    \":0},\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True,\n\
    \  cache_dir=cache_dir\n)\n```"
  created_at: 2023-07-25 16:42:09+00:00
  edited: false
  hidden: false
  id: 64c009719638fbb8e1a49a1b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: mosaicml/mpt-7b
repo_type: model
status: open
target_branch: null
title: attn_impl
