!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zokica
conflicting_files: null
created_at: 2023-05-07 14:27:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-05-07T15:27:04.000Z'
    data:
      edited: true
      editors:
      - zokica
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: "<p>I have tried llama 7B and this model on a CPU, and LLama is much\
          \ faster (7 seconds vs 43 for 20 tokens). Is this the right way to run the\
          \ model on a CPU or I am missing something:</p>\n<pre><code>import torch\n\
          import transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          model_name = \"mosaicml/mpt-7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\
          \ low_cpu_mem_usage=True,trust_remote_code=True)\n\nimport time\ntimea =\
          \ time.time()\nprompt = \"A lion is\"\ninputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n\
          outputs = model.generate(\n    **inputs, max_new_tokens=20, do_sample=True,\
          \ temperature=0.75 , return_dict_in_generate=True\n)\ntoken = outputs.sequences[0]\n\
          output_str = tokenizer.decode(token)\nprint(output_str)\nprint(\"timea =\
          \ time.time()\",-timea + time.time())\n</code></pre>\n"
        raw: "I have tried llama 7B and this model on a CPU, and LLama is much faster\
          \ (7 seconds vs 43 for 20 tokens). Is this the right way to run the model\
          \ on a CPU or I am missing something:\n\n    import torch\n    import transformers\n\
          \    from transformers import AutoTokenizer, AutoModelForCausalLM\n    model_name\
          \ = \"mosaicml/mpt-7b\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          \    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\
          \ low_cpu_mem_usage=True,trust_remote_code=True)\n    \n    import time\n\
          \    timea = time.time()\n    prompt = \"A lion is\"\n    inputs = tokenizer(prompt,\
          \ return_tensors='pt').to(model.device)\n    outputs = model.generate(\n\
          \        **inputs, max_new_tokens=20, do_sample=True, temperature=0.75 ,\
          \ return_dict_in_generate=True\n    )\n    token = outputs.sequences[0]\n\
          \    output_str = tokenizer.decode(token)\n    print(output_str)\n    print(\"\
          timea = time.time()\",-timea + time.time())"
        updatedAt: '2023-05-07T15:31:53.932Z'
      numEdits: 4
      reactions: []
    id: 6457c34888c49bf519ea67dc
    type: comment
  author: zokica
  content: "I have tried llama 7B and this model on a CPU, and LLama is much faster\
    \ (7 seconds vs 43 for 20 tokens). Is this the right way to run the model on a\
    \ CPU or I am missing something:\n\n    import torch\n    import transformers\n\
    \    from transformers import AutoTokenizer, AutoModelForCausalLM\n    model_name\
    \ = \"mosaicml/mpt-7b\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\
    \    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\
    \ low_cpu_mem_usage=True,trust_remote_code=True)\n    \n    import time\n    timea\
    \ = time.time()\n    prompt = \"A lion is\"\n    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n\
    \    outputs = model.generate(\n        **inputs, max_new_tokens=20, do_sample=True,\
    \ temperature=0.75 , return_dict_in_generate=True\n    )\n    token = outputs.sequences[0]\n\
    \    output_str = tokenizer.decode(token)\n    print(output_str)\n    print(\"\
    timea = time.time()\",-timea + time.time())"
  created_at: 2023-05-07 14:27:04+00:00
  edited: true
  hidden: false
  id: 6457c34888c49bf519ea67dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-05-07T15:28:29.000Z'
    data:
      edited: true
      editors:
      - zokica
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: '<p>The out:</p>

          <p>MPT 7b:</p>

          <pre><code>A lion is a large cat. Lions are native to Africa. Lions live
          in the savanna, a grassland

          timea = time.time() 43.37369394302368

          </code></pre>

          <p>LLama 7b:</p>

          <pre><code>&lt;s&gt; A lion is the king of the jungle. The lion is the strongest
          animal in the animal kingdom

          timea = time.time() 6.919593811035156

          </code></pre>

          '
        raw: "The out:\n\nMPT 7b:\n    \n    A lion is a large cat. Lions are native\
          \ to Africa. Lions live in the savanna, a grassland\n    timea = time.time()\
          \ 43.37369394302368\n\nLLama 7b:\n\n    <s> A lion is the king of the jungle.\
          \ The lion is the strongest animal in the animal kingdom\n    timea = time.time()\
          \ 6.919593811035156"
        updatedAt: '2023-05-07T15:31:19.472Z'
      numEdits: 2
      reactions: []
    id: 6457c39d711ee86f6ef1278e
    type: comment
  author: zokica
  content: "The out:\n\nMPT 7b:\n    \n    A lion is a large cat. Lions are native\
    \ to Africa. Lions live in the savanna, a grassland\n    timea = time.time() 43.37369394302368\n\
    \nLLama 7b:\n\n    <s> A lion is the king of the jungle. The lion is the strongest\
    \ animal in the animal kingdom\n    timea = time.time() 6.919593811035156"
  created_at: 2023-05-07 14:28:29+00:00
  edited: true
  hidden: false
  id: 6457c39d711ee86f6ef1278e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638b9aaf1987d67b340ce46c/TYJlNY7sCp-mK5DAq5kBH.jpeg?w=200&h=200&f=face
      fullname: kelden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: keldenl
      type: user
    createdAt: '2023-05-08T08:56:29.000Z'
    data:
      edited: false
      editors:
      - keldenl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638b9aaf1987d67b340ce46c/TYJlNY7sCp-mK5DAq5kBH.jpeg?w=200&h=200&f=face
          fullname: kelden
          isHf: false
          isPro: false
          name: keldenl
          type: user
        html: "<p>you're comparing ggml vs PyTorch \u2013&nbsp;until this gets the\
          \ ggml treatment expect the speeds to be slower on CPU only</p>\n"
        raw: "you're comparing ggml vs PyTorch \u2013\_until this gets the ggml treatment\
          \ expect the speeds to be slower on CPU only"
        updatedAt: '2023-05-08T08:56:29.169Z'
      numEdits: 0
      reactions: []
    id: 6458b93dc5d0d57ba4195717
    type: comment
  author: keldenl
  content: "you're comparing ggml vs PyTorch \u2013\_until this gets the ggml treatment\
    \ expect the speeds to be slower on CPU only"
  created_at: 2023-05-08 07:56:29+00:00
  edited: false
  hidden: false
  id: 6458b93dc5d0d57ba4195717
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-05-08T13:24:30.000Z'
    data:
      edited: true
      editors:
      - zokica
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: "<blockquote>\n<p>you're comparing ggml vs PyTorch \u2013&nbsp;until\
          \ this gets the ggml treatment expect the speeds to be slower on CPU only</p>\n\
          </blockquote>\n<p>How did you conclude that i used ggml?</p>\n<p>Of course,\
          \ I did not use ggml, I used exactly the same BF16 for both llama and mpt-7b\
          \ and llama is much faster.<br> model_name = \"huggyllama/llama-7b\"<br>\
          \ model_name = \"mosaicml/mpt-7b\"</p>\n<p>Here is exactly what I used for\
          \ llama so you can replicate and see for yourself:</p>\n<pre><code>import\
          \ torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          model_name = \"huggyllama/llama-7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\
          \ low_cpu_mem_usage=True,trust_remote_code=True)\n\nimport time\ntimea =\
          \ time.time()\nprompt = \"A lion is\"\ninputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n\
          outputs = model.generate(\n    inputs.input_ids, max_new_tokens=20, do_sample=True,\
          \ temperature=0.75 , return_dict_in_generate=True\n)\ntoken = outputs.sequences[0]\n\
          output_str = tokenizer.decode(token)\nprint(output_str)\nprint(\"timea =\
          \ time.time()\",-timea + time.time())\n</code></pre>\n"
        raw: "> you're comparing ggml vs PyTorch \u2013\_until this gets the ggml\
          \ treatment expect the speeds to be slower on CPU only\n\nHow did you conclude\
          \ that i used ggml?\n\nOf course, I did not use ggml, I used exactly the\
          \ same BF16 for both llama and mpt-7b and llama is much faster.\n model_name\
          \ = \"huggyllama/llama-7b\"\n model_name = \"mosaicml/mpt-7b\"\n\n\n\nHere\
          \ is exactly what I used for llama so you can replicate and see for yourself:\n\
          \n    import torch\n    import transformers\n    from transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\n    model_name = \"huggyllama/llama-7b\"\
          \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model =\
          \ AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\
          \ low_cpu_mem_usage=True,trust_remote_code=True)\n    \n    import time\n\
          \    timea = time.time()\n    prompt = \"A lion is\"\n    inputs = tokenizer(prompt,\
          \ return_tensors='pt').to(model.device)\n    outputs = model.generate(\n\
          \        inputs.input_ids, max_new_tokens=20, do_sample=True, temperature=0.75\
          \ , return_dict_in_generate=True\n    )\n    token = outputs.sequences[0]\n\
          \    output_str = tokenizer.decode(token)\n    print(output_str)\n    print(\"\
          timea = time.time()\",-timea + time.time())"
        updatedAt: '2023-05-08T13:25:01.969Z'
      numEdits: 1
      reactions: []
    id: 6458f80ef92601affa2ff7f5
    type: comment
  author: zokica
  content: "> you're comparing ggml vs PyTorch \u2013\_until this gets the ggml treatment\
    \ expect the speeds to be slower on CPU only\n\nHow did you conclude that i used\
    \ ggml?\n\nOf course, I did not use ggml, I used exactly the same BF16 for both\
    \ llama and mpt-7b and llama is much faster.\n model_name = \"huggyllama/llama-7b\"\
    \n model_name = \"mosaicml/mpt-7b\"\n\n\n\nHere is exactly what I used for llama\
    \ so you can replicate and see for yourself:\n\n    import torch\n    import transformers\n\
    \    from transformers import AutoTokenizer, AutoModelForCausalLM\n    model_name\
    \ = \"huggyllama/llama-7b\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\
    \    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\
    \ low_cpu_mem_usage=True,trust_remote_code=True)\n    \n    import time\n    timea\
    \ = time.time()\n    prompt = \"A lion is\"\n    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n\
    \    outputs = model.generate(\n        inputs.input_ids, max_new_tokens=20, do_sample=True,\
    \ temperature=0.75 , return_dict_in_generate=True\n    )\n    token = outputs.sequences[0]\n\
    \    output_str = tokenizer.decode(token)\n    print(output_str)\n    print(\"\
    timea = time.time()\",-timea + time.time())"
  created_at: 2023-05-08 12:24:30+00:00
  edited: true
  hidden: false
  id: 6458f80ef92601affa2ff7f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-11T23:52:04.000Z'
    data:
      edited: true
      editors:
      - abhi-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;zokica&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zokica\">@<span class=\"\
          underline\">zokica</span></a></span>\n\n\t</span></span> , we will take\
          \ a look at this as we're seeing a couple reports of slow CPU inference.\
          \ Since you have a system on hand that is showing the issue, could you help\
          \ confirm if any of the MPT vs. LLaMa speed gap changes based on the <code>torch_dtype</code>\
          \ and <code>low_cpu_mem_usage</code> flags? Basically this matrix:</p>\n\
          <ul>\n<li>torch_dtype=torch.float32, low_cpu_mem_usage=False: ?</li>\n<li>torch_dtype=torch.float32,\
          \ low_cpu_mem_usage=True: ?</li>\n<li>torch_dtype=torch.bfloat16, low_cpu_mem_usage=False:\
          \  ?</li>\n<li>torch_dtype=torch.bfloat16, low_cpu_mem_usage=True: MPT slower\
          \ than LLaMa</li>\n</ul>\n<p>In the meantime we will try to reproduce as\
          \ well. Thank you for the report!</p>\n"
        raw: 'Hi @zokica , we will take a look at this as we''re seeing a couple reports
          of slow CPU inference. Since you have a system on hand that is showing the
          issue, could you help confirm if any of the MPT vs. LLaMa speed gap changes
          based on the `torch_dtype` and `low_cpu_mem_usage` flags? Basically this
          matrix:


          * torch_dtype=torch.float32, low_cpu_mem_usage=False: ?

          * torch_dtype=torch.float32, low_cpu_mem_usage=True: ?

          * torch_dtype=torch.bfloat16, low_cpu_mem_usage=False:  ?

          * torch_dtype=torch.bfloat16, low_cpu_mem_usage=True: MPT slower than LLaMa


          In the meantime we will try to reproduce as well. Thank you for the report!'
        updatedAt: '2023-05-11T23:52:37.793Z'
      numEdits: 1
      reactions: []
    id: 645d7fa4f1e3b219cb0f5b53
    type: comment
  author: abhi-mosaic
  content: 'Hi @zokica , we will take a look at this as we''re seeing a couple reports
    of slow CPU inference. Since you have a system on hand that is showing the issue,
    could you help confirm if any of the MPT vs. LLaMa speed gap changes based on
    the `torch_dtype` and `low_cpu_mem_usage` flags? Basically this matrix:


    * torch_dtype=torch.float32, low_cpu_mem_usage=False: ?

    * torch_dtype=torch.float32, low_cpu_mem_usage=True: ?

    * torch_dtype=torch.bfloat16, low_cpu_mem_usage=False:  ?

    * torch_dtype=torch.bfloat16, low_cpu_mem_usage=True: MPT slower than LLaMa


    In the meantime we will try to reproduce as well. Thank you for the report!'
  created_at: 2023-05-11 22:52:04+00:00
  edited: true
  hidden: false
  id: 645d7fa4f1e3b219cb0f5b53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9eba6fcdd0c36c78cafc5bc94266c50a.svg
      fullname: Stefan Richter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stefan-berlin
      type: user
    createdAt: '2023-05-12T09:47:52.000Z'
    data:
      edited: true
      editors:
      - stefan-berlin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9eba6fcdd0c36c78cafc5bc94266c50a.svg
          fullname: Stefan Richter
          isHf: false
          isPro: false
          name: stefan-berlin
          type: user
        html: "<p>Hi,</p>\n<p>I testet both scripts from zokica above,<br>on a cheap\
          \ VPS, 18 Cores, 48 GB RAM, 2048 GB SSD (RAID10).</p>\n<p>LLaMa still faster,\
          \ but with float32 \"just\" by factor 2.</p>\n<p>torch_dtype=torch.float32,\
          \ low_cpu_mem_usage=False:\t|\tMPT: 95.7\t|\t\tLLaMa: 43.2<br>torch_dtype=torch.float32,\
          \ low_cpu_mem_usage=True:\t|\tMPT: 98.6s\t|\t\tLLaMa: 48.6<br>torch_dtype=torch.bfloat16,\
          \ low_cpu_mem_usage=False:|\tMPT: 1747.8\t|\t\tLLaMa: 177.7<br>torch_dtype=torch.bfloat16,\
          \ low_cpu_mem_usage=True:\t|\tMPT: 1764.6s\t| \tLLaMa: 178.2</p>\n"
        raw: "Hi,\n\nI testet both scripts from zokica above,\non a cheap VPS, 18\
          \ Cores, 48 GB RAM, 2048 GB SSD (RAID10).\n\nLLaMa still faster, but with\
          \ float32 \"just\" by factor 2.\n\ntorch_dtype=torch.float32, low_cpu_mem_usage=False:\t\
          |\tMPT: 95.7\t|\t\tLLaMa: 43.2\ntorch_dtype=torch.float32, low_cpu_mem_usage=True:\t\
          |\tMPT: 98.6s\t|\t\tLLaMa: 48.6\ntorch_dtype=torch.bfloat16, low_cpu_mem_usage=False:|\t\
          MPT: 1747.8\t|\t\tLLaMa: 177.7\ntorch_dtype=torch.bfloat16, low_cpu_mem_usage=True:\t\
          |\tMPT: 1764.6s\t| \tLLaMa: 178.2"
        updatedAt: '2023-05-12T10:12:05.284Z'
      numEdits: 1
      reactions: []
    id: 645e0b48d323ebc559bc876c
    type: comment
  author: stefan-berlin
  content: "Hi,\n\nI testet both scripts from zokica above,\non a cheap VPS, 18 Cores,\
    \ 48 GB RAM, 2048 GB SSD (RAID10).\n\nLLaMa still faster, but with float32 \"\
    just\" by factor 2.\n\ntorch_dtype=torch.float32, low_cpu_mem_usage=False:\t|\t\
    MPT: 95.7\t|\t\tLLaMa: 43.2\ntorch_dtype=torch.float32, low_cpu_mem_usage=True:\t\
    |\tMPT: 98.6s\t|\t\tLLaMa: 48.6\ntorch_dtype=torch.bfloat16, low_cpu_mem_usage=False:|\t\
    MPT: 1747.8\t|\t\tLLaMa: 177.7\ntorch_dtype=torch.bfloat16, low_cpu_mem_usage=True:\t\
    |\tMPT: 1764.6s\t| \tLLaMa: 178.2"
  created_at: 2023-05-12 08:47:52+00:00
  edited: true
  hidden: false
  id: 645e0b48d323ebc559bc876c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-12T17:15:18.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: '<p>Thank you so much! This definitely seems like a bottleneck somewhere
          in the MPT forward or KV cacheing logic. It''s very interesting that this
          shows up on CPU but not on GPU (where we saw the opposite relation, ~1.5-2x
          faster for MPT with <code>triton</code>). We will look into it  and patch
          the model source once we find a fix. </p>

          <p>Last question, what version of torch were you using for those results?</p>

          '
        raw: "Thank you so much! This definitely seems like a bottleneck somewhere\
          \ in the MPT forward or KV cacheing logic. It's very interesting that this\
          \ shows up on CPU but not on GPU (where we saw the opposite relation, ~1.5-2x\
          \ faster for MPT with `triton`). We will look into it  and patch the model\
          \ source once we find a fix. \n\nLast question, what version of torch were\
          \ you using for those results?"
        updatedAt: '2023-05-12T17:15:18.724Z'
      numEdits: 0
      reactions: []
    id: 645e7426144c62420f2a0df1
    type: comment
  author: abhi-mosaic
  content: "Thank you so much! This definitely seems like a bottleneck somewhere in\
    \ the MPT forward or KV cacheing logic. It's very interesting that this shows\
    \ up on CPU but not on GPU (where we saw the opposite relation, ~1.5-2x faster\
    \ for MPT with `triton`). We will look into it  and patch the model source once\
    \ we find a fix. \n\nLast question, what version of torch were you using for those\
    \ results?"
  created_at: 2023-05-12 16:15:18+00:00
  edited: false
  hidden: false
  id: 645e7426144c62420f2a0df1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-05-12T17:49:24.000Z'
    data:
      edited: false
      editors:
      - zokica
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: '<p>I actually run it via BF16, as I have only 32 GB of ram in this
          server, so i had to use a low ram option. </p>

          <p>Is there any other way to run it on a CPU without using bf16 with just
          32 GB of memory?</p>

          <p>I am using, and probably most people will just use the CPU for testing,  it
          would be nice if this could work a bit faster, but not so  much of a problem.</p>

          <p>So it works faster than LLama on a GPU, right, even without triton ?</p>

          '
        raw: "I actually run it via BF16, as I have only 32 GB of ram in this server,\
          \ so i had to use a low ram option. \n\nIs there any other way to run it\
          \ on a CPU without using bf16 with just 32 GB of memory?\n\nI am using,\
          \ and probably most people will just use the CPU for testing,  it would\
          \ be nice if this could work a bit faster, but not so  much of a problem.\n\
          \nSo it works faster than LLama on a GPU, right, even without triton ?"
        updatedAt: '2023-05-12T17:49:24.668Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Mayank-02
        - emekaboris
    id: 645e7c245f8408a151fe6920
    type: comment
  author: zokica
  content: "I actually run it via BF16, as I have only 32 GB of ram in this server,\
    \ so i had to use a low ram option. \n\nIs there any other way to run it on a\
    \ CPU without using bf16 with just 32 GB of memory?\n\nI am using, and probably\
    \ most people will just use the CPU for testing,  it would be nice if this could\
    \ work a bit faster, but not so  much of a problem.\n\nSo it works faster than\
    \ LLama on a GPU, right, even without triton ?"
  created_at: 2023-05-12 16:49:24+00:00
  edited: false
  hidden: false
  id: 645e7c245f8408a151fe6920
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a560efb1ad51a3e068b16aeaac3619e2.svg
      fullname: Combatti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Combatti
      type: user
    createdAt: '2023-05-12T18:14:18.000Z'
    data:
      edited: false
      editors:
      - Combatti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a560efb1ad51a3e068b16aeaac3619e2.svg
          fullname: Combatti
          isHf: false
          isPro: false
          name: Combatti
          type: user
        html: "<blockquote>\n<p>you're comparing ggml vs PyTorch \u2013&nbsp;until\
          \ this gets the ggml treatment expect the speeds to be slower on CPU only</p>\n\
          </blockquote>\n<p>There are ggml versions in hugging face \U0001F917</p>\n"
        raw: "> you're comparing ggml vs PyTorch \u2013\_until this gets the ggml\
          \ treatment expect the speeds to be slower on CPU only\n\nThere are ggml\
          \ versions in hugging face \U0001F917"
        updatedAt: '2023-05-12T18:14:18.644Z'
      numEdits: 0
      reactions: []
    id: 645e81fa6d343f4bb61344bd
    type: comment
  author: Combatti
  content: "> you're comparing ggml vs PyTorch \u2013\_until this gets the ggml treatment\
    \ expect the speeds to be slower on CPU only\n\nThere are ggml versions in hugging\
    \ face \U0001F917"
  created_at: 2023-05-12 17:14:18+00:00
  edited: false
  hidden: false
  id: 645e81fa6d343f4bb61344bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9eba6fcdd0c36c78cafc5bc94266c50a.svg
      fullname: Stefan Richter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stefan-berlin
      type: user
    createdAt: '2023-05-12T18:53:28.000Z'
    data:
      edited: false
      editors:
      - stefan-berlin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9eba6fcdd0c36c78cafc5bc94266c50a.svg
          fullname: Stefan Richter
          isHf: false
          isPro: false
          name: stefan-berlin
          type: user
        html: '<blockquote>

          <p>Thank you so much! This definitely seems like a bottleneck somewhere
          in the MPT forward or KV cacheing logic. It''s very interesting that this
          shows up on CPU but not on GPU (where we saw the opposite relation, ~1.5-2x
          faster for MPT with <code>triton</code>). We will look into it  and patch
          the model source once we find a fix. </p>

          <p>Last question, what version of torch were you using for those results?</p>

          </blockquote>

          <p>2.0.1+cpu</p>

          '
        raw: "> Thank you so much! This definitely seems like a bottleneck somewhere\
          \ in the MPT forward or KV cacheing logic. It's very interesting that this\
          \ shows up on CPU but not on GPU (where we saw the opposite relation, ~1.5-2x\
          \ faster for MPT with `triton`). We will look into it  and patch the model\
          \ source once we find a fix. \n> \n> Last question, what version of torch\
          \ were you using for those results?\n\n2.0.1+cpu"
        updatedAt: '2023-05-12T18:53:28.493Z'
      numEdits: 0
      reactions: []
    id: 645e8b28916e745511ff9a35
    type: comment
  author: stefan-berlin
  content: "> Thank you so much! This definitely seems like a bottleneck somewhere\
    \ in the MPT forward or KV cacheing logic. It's very interesting that this shows\
    \ up on CPU but not on GPU (where we saw the opposite relation, ~1.5-2x faster\
    \ for MPT with `triton`). We will look into it  and patch the model source once\
    \ we find a fix. \n> \n> Last question, what version of torch were you using for\
    \ those results?\n\n2.0.1+cpu"
  created_at: 2023-05-12 17:53:28+00:00
  edited: false
  hidden: false
  id: 645e8b28916e745511ff9a35
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/056370e376f77d75e8092f0c6338956f.svg
      fullname: Nithin I Bhandari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nib12345
      type: user
    createdAt: '2023-05-18T08:21:33.000Z'
    data:
      edited: false
      editors:
      - nib12345
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/056370e376f77d75e8092f0c6338956f.svg
          fullname: Nithin I Bhandari
          isHf: false
          isPro: false
          name: nib12345
          type: user
        html: "<p>For me. It is taking 35 mins to generate 100 tokens.<br>Laptop specification:\
          \ No GPU, 20 GB RAM (4+16 GB), 1 TB SSD, I5 processor.<br>I have very slow\
          \ Laptop with No GPU.</p>\n<pre><code>def customGenerate(argPrompt):\n \
          \   inputs = tokenizer(argPrompt, return_tensors='pt').to(model.device)\n\
          \    outputs = model.generate(\n        **inputs, max_new_tokens=1, do_sample=True,\
          \ temperature=0.75 , return_dict_in_generate=True\n    )\n    token = outputs.sequences[0]\n\
          \    output_str = tokenizer.decode(token)\n\n    return output_str\n\nimport\
          \ time\nfrom datetime import datetime\ntimea = time.time()\ndtNow = datetime.now()\n\
          print(\"now =\", dtNow)\nprint(\"Start time: \",-timea + time.time())\n\n\
          import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          model_name = \"mosaicml/mpt-7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\
          \ low_cpu_mem_usage=True,trust_remote_code=True)\n\nprompt = [\"Earth is\"\
          ]\n\ncount=0\n\nwhile(count &lt; 100):\n    output_str = customGenerate(prompt[prompt.__len__()-1])\n\
          \    prompt.append(output_str)\n    print(prompt.__len__(), ': ' , prompt[prompt.__len__()-1])\n\
          \    print(\"Time taken in sec:\",-timea + time.time())\n    print(\"Time\
          \ taken in min:\",((-timea + time.time())/60))\n    count = count + 1\n\n\
          dtNow = datetime.now()\nprint(\"now =\", dtNow)\n</code></pre>\n"
        raw: "For me. It is taking 35 mins to generate 100 tokens.\nLaptop specification:\
          \ No GPU, 20 GB RAM (4+16 GB), 1 TB SSD, I5 processor.\nI have very slow\
          \ Laptop with No GPU.\n\n```\ndef customGenerate(argPrompt):\n    inputs\
          \ = tokenizer(argPrompt, return_tensors='pt').to(model.device)\n    outputs\
          \ = model.generate(\n        **inputs, max_new_tokens=1, do_sample=True,\
          \ temperature=0.75 , return_dict_in_generate=True\n    )\n    token = outputs.sequences[0]\n\
          \    output_str = tokenizer.decode(token)\n\n    return output_str\n\nimport\
          \ time\nfrom datetime import datetime\ntimea = time.time()\ndtNow = datetime.now()\n\
          print(\"now =\", dtNow)\nprint(\"Start time: \",-timea + time.time())\n\n\
          import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          model_name = \"mosaicml/mpt-7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\
          \ low_cpu_mem_usage=True,trust_remote_code=True)\n\nprompt = [\"Earth is\"\
          ]\n\ncount=0\n\nwhile(count < 100):\n    output_str = customGenerate(prompt[prompt.__len__()-1])\n\
          \    prompt.append(output_str)\n    print(prompt.__len__(), ': ' , prompt[prompt.__len__()-1])\n\
          \    print(\"Time taken in sec:\",-timea + time.time())\n    print(\"Time\
          \ taken in min:\",((-timea + time.time())/60))\n    count = count + 1\n\n\
          dtNow = datetime.now()\nprint(\"now =\", dtNow)\n```"
        updatedAt: '2023-05-18T08:21:33.582Z'
      numEdits: 0
      reactions: []
    id: 6465e00de0fe831b478a3f93
    type: comment
  author: nib12345
  content: "For me. It is taking 35 mins to generate 100 tokens.\nLaptop specification:\
    \ No GPU, 20 GB RAM (4+16 GB), 1 TB SSD, I5 processor.\nI have very slow Laptop\
    \ with No GPU.\n\n```\ndef customGenerate(argPrompt):\n    inputs = tokenizer(argPrompt,\
    \ return_tensors='pt').to(model.device)\n    outputs = model.generate(\n     \
    \   **inputs, max_new_tokens=1, do_sample=True, temperature=0.75 , return_dict_in_generate=True\n\
    \    )\n    token = outputs.sequences[0]\n    output_str = tokenizer.decode(token)\n\
    \n    return output_str\n\nimport time\nfrom datetime import datetime\ntimea =\
    \ time.time()\ndtNow = datetime.now()\nprint(\"now =\", dtNow)\nprint(\"Start\
    \ time: \",-timea + time.time())\n\nimport torch\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\nmodel_name = \"mosaicml/mpt-7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\
    \ low_cpu_mem_usage=True,trust_remote_code=True)\n\nprompt = [\"Earth is\"]\n\n\
    count=0\n\nwhile(count < 100):\n    output_str = customGenerate(prompt[prompt.__len__()-1])\n\
    \    prompt.append(output_str)\n    print(prompt.__len__(), ': ' , prompt[prompt.__len__()-1])\n\
    \    print(\"Time taken in sec:\",-timea + time.time())\n    print(\"Time taken\
    \ in min:\",((-timea + time.time())/60))\n    count = count + 1\n\ndtNow = datetime.now()\n\
    print(\"now =\", dtNow)\n```"
  created_at: 2023-05-18 07:21:33+00:00
  edited: false
  hidden: false
  id: 6465e00de0fe831b478a3f93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654000939422-5fda30110e761cf3183cfd5c.png?w=200&h=200&f=face
      fullname: Connor Shorten
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CShorten
      type: user
    createdAt: '2023-05-25T11:58:40.000Z'
    data:
      edited: false
      editors:
      - CShorten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654000939422-5fda30110e761cf3183cfd5c.png?w=200&h=200&f=face
          fullname: Connor Shorten
          isHf: false
          isPro: false
          name: CShorten
          type: user
        html: '<p>I am having a hard time running this on CPU, could someone please
          help me? I get the error:</p>

          <p>ImportError: This modeling file requires the following packages that
          were not found in your environment: einops. Run <code>pip install einops</code></p>

          <p>But then it seems einops needs to find a CUDA Driver to be installed
          :(</p>

          '
        raw: 'I am having a hard time running this on CPU, could someone please help
          me? I get the error:


          ImportError: This modeling file requires the following packages that were
          not found in your environment: einops. Run `pip install einops`


          But then it seems einops needs to find a CUDA Driver to be installed :('
        updatedAt: '2023-05-25T11:58:40.812Z'
      numEdits: 0
      reactions: []
    id: 646f4d70de1b1c1780b6f957
    type: comment
  author: CShorten
  content: 'I am having a hard time running this on CPU, could someone please help
    me? I get the error:


    ImportError: This modeling file requires the following packages that were not
    found in your environment: einops. Run `pip install einops`


    But then it seems einops needs to find a CUDA Driver to be installed :('
  created_at: 2023-05-25 10:58:40+00:00
  edited: false
  hidden: false
  id: 646f4d70de1b1c1780b6f957
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-03T01:14:18.000Z'
    data:
      edited: true
      editors:
      - abhi-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9511577486991882
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: '<p>The CPU load time should be fixed now as of this PR as long as you
          use <code>device_map=auto</code>: <a href="https://huggingface.co/mosaicml/mpt-7b/discussions/47">https://huggingface.co/mosaicml/mpt-7b/discussions/47</a><br>We
          also added some logic to improve KV cacheing speed. Let us know if you see
          improvements!</p>

          '
        raw: 'The CPU load time should be fixed now as of this PR as long as you use
          `device_map=auto`: https://huggingface.co/mosaicml/mpt-7b/discussions/47

          We also added some logic to improve KV cacheing speed. Let us know if you
          see improvements!'
        updatedAt: '2023-06-03T01:14:34.668Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - CShorten
    id: 647a93eac7367455fda407da
    type: comment
  author: abhi-mosaic
  content: 'The CPU load time should be fixed now as of this PR as long as you use
    `device_map=auto`: https://huggingface.co/mosaicml/mpt-7b/discussions/47

    We also added some logic to improve KV cacheing speed. Let us know if you see
    improvements!'
  created_at: 2023-06-03 00:14:18+00:00
  edited: true
  hidden: false
  id: 647a93eac7367455fda407da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-06-14T07:47:49.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.941426157951355
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>Closing as complete, but if anyone sees any CPU inference speed
          issues, please reopen this or open a new issue!</p>

          '
        raw: Closing as complete, but if anyone sees any CPU inference speed issues,
          please reopen this or open a new issue!
        updatedAt: '2023-06-14T07:47:49.567Z'
      numEdits: 0
      reactions: []
      relatedEventId: 648970a5ccdddc9de7f9e5cc
    id: 648970a5ccdddc9de7f9e5c9
    type: comment
  author: sam-mosaic
  content: Closing as complete, but if anyone sees any CPU inference speed issues,
    please reopen this or open a new issue!
  created_at: 2023-06-14 06:47:49+00:00
  edited: false
  hidden: false
  id: 648970a5ccdddc9de7f9e5c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-06-14T07:47:49.000Z'
    data:
      status: closed
    id: 648970a5ccdddc9de7f9e5cc
    type: status-change
  author: sam-mosaic
  created_at: 2023-06-14 06:47:49+00:00
  id: 648970a5ccdddc9de7f9e5cc
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: mosaicml/mpt-7b
repo_type: model
status: closed
target_branch: null
title: Speed on CPU
