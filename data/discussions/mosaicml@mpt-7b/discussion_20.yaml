!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Dihf
conflicting_files: null
created_at: 2023-05-13 23:17:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/33c8e7a456fd81afe86fe6f2742107d7.svg
      fullname: Diz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dihf
      type: user
    createdAt: '2023-05-14T00:17:22.000Z'
    data:
      edited: false
      editors:
      - Dihf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/33c8e7a456fd81afe86fe6f2742107d7.svg
          fullname: Diz
          isHf: false
          isPro: false
          name: Dihf
          type: user
        html: '<p>Anyone know the trained context length of the 7B instruct and chat
          models?</p>

          '
        raw: Anyone know the trained context length of the 7B instruct and chat models?
        updatedAt: '2023-05-14T00:17:22.937Z'
      numEdits: 0
      reactions: []
    id: 64602892c9170f5889c60f42
    type: comment
  author: Dihf
  content: Anyone know the trained context length of the 7B instruct and chat models?
  created_at: 2023-05-13 23:17:22+00:00
  edited: false
  hidden: false
  id: 64602892c9170f5889c60f42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671358334691-6394b6345771408818f03d3e.png?w=200&h=200&f=face
      fullname: Boris Ding
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0xDing
      type: user
    createdAt: '2023-05-14T05:26:32.000Z'
    data:
      edited: false
      editors:
      - 0xDing
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671358334691-6394b6345771408818f03d3e.png?w=200&h=200&f=face
          fullname: Boris Ding
          isHf: false
          isPro: false
          name: 0xDing
          type: user
        html: '<p>2048</p>

          '
        raw: '2048'
        updatedAt: '2023-05-14T05:26:32.123Z'
      numEdits: 0
      reactions: []
    id: 64607108a93c1779eb1255d9
    type: comment
  author: 0xDing
  content: '2048'
  created_at: 2023-05-14 04:26:32+00:00
  edited: false
  hidden: false
  id: 64607108a93c1779eb1255d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-16T19:35:33.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;0xDing&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/0xDing\">@<span class=\"\
          underline\">0xDing</span></a></span>\n\n\t</span></span> is correct, we\
          \ trained on 2048. ALiBi should let one extrapolate to 4096 </p>\n"
        raw: '@0xDing is correct, we trained on 2048. ALiBi should let one extrapolate
          to 4096 '
        updatedAt: '2023-05-16T19:35:33.353Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6463db05dae3c8a327c09425
    id: 6463db05dae3c8a327c09424
    type: comment
  author: sam-mosaic
  content: '@0xDing is correct, we trained on 2048. ALiBi should let one extrapolate
    to 4096 '
  created_at: 2023-05-16 18:35:33+00:00
  edited: false
  hidden: false
  id: 6463db05dae3c8a327c09424
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-16T19:35:33.000Z'
    data:
      status: closed
    id: 6463db05dae3c8a327c09425
    type: status-change
  author: sam-mosaic
  created_at: 2023-05-16 18:35:33+00:00
  id: 6463db05dae3c8a327c09425
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: mosaicml/mpt-7b
repo_type: model
status: closed
target_branch: null
title: Anyone know the trained context length of the 7B instruct and chat models?
