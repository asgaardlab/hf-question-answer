!!python/object:huggingface_hub.community.DiscussionWithDetails
author: winglian
conflicting_files: null
created_at: 2023-05-09 23:48:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-05-10T00:48:15.000Z'
    data:
      edited: true
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: "<p>I gave it a shot and wanted to make sure it was even possible before\
          \ going down a rabbit hole. thanks!</p>\n<pre><code>  File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py\"\
          , line 171, in forward                                                 \
          \                                                                      \
          \      \n    (context, attn_weights) = self.attn_fn(query, key, value, self.n_heads,\
          \ softmax_scale=self.softmax_scale, attn_bias=attn_bias, key_padding_mask=key_padding_mask,\
          \ is_causal=is_causal, dropout_p=self.attn_dropout_p, training=self.training,\
          \ needs_weights=needs_weights)         \n  File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py\"\
          , line 111, in triton_flash_attn_fn                                    \
          \                                                                      \
          \      \n    attn_output = flash_attn_triton.flash_attn_func(query, key,\
          \ value, attn_bias, reset_is_causal, softmax_scale)\n  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\"\
          , line 506, in apply                                                   \
          \                                                                      \
          \                                               \n    return super().apply(*args,\
          \ **kwargs)  # type: ignore[misc]                                      \
          \                                                                      \
          \                                                                      \
          \                                      \n  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/flash_attn/flash_attn_triton.py\"\
          , line 810, in forward                     \n    o, lse, ctx.softmax_scale\
          \ = _flash_attn_forward(                                               \
          \                                        \n  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/flash_attn/flash_attn_triton.py\"\
          , line 599, in _flash_attn_forward\n    assert bias.dtype in [q.dtype, torch.float]\n\
          </code></pre>\n"
        raw: "I gave it a shot and wanted to make sure it was even possible before\
          \ going down a rabbit hole. thanks!\n\n```\n  File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py\"\
          , line 171, in forward                                                 \
          \                                                                      \
          \      \n    (context, attn_weights) = self.attn_fn(query, key, value, self.n_heads,\
          \ softmax_scale=self.softmax_scale, attn_bias=attn_bias, key_padding_mask=key_padding_mask,\
          \ is_causal=is_causal, dropout_p=self.attn_dropout_p, training=self.training,\
          \ needs_weights=needs_weights)         \n  File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py\"\
          , line 111, in triton_flash_attn_fn                                    \
          \                                                                      \
          \      \n    attn_output = flash_attn_triton.flash_attn_func(query, key,\
          \ value, attn_bias, reset_is_causal, softmax_scale)\n  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\"\
          , line 506, in apply                                                   \
          \                                                                      \
          \                                               \n    return super().apply(*args,\
          \ **kwargs)  # type: ignore[misc]                                      \
          \                                                                      \
          \                                                                      \
          \                                      \n  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/flash_attn/flash_attn_triton.py\"\
          , line 810, in forward                     \n    o, lse, ctx.softmax_scale\
          \ = _flash_attn_forward(                                               \
          \                                        \n  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/flash_attn/flash_attn_triton.py\"\
          , line 599, in _flash_attn_forward\n    assert bias.dtype in [q.dtype, torch.float]\n\
          ```"
        updatedAt: '2023-05-10T00:48:27.789Z'
      numEdits: 1
      reactions: []
    id: 645ae9cfe7e8a4b57bdc1411
    type: comment
  author: winglian
  content: "I gave it a shot and wanted to make sure it was even possible before going\
    \ down a rabbit hole. thanks!\n\n```\n  File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py\"\
    , line 171, in forward                                                       \
    \                                                                      \n    (context,\
    \ attn_weights) = self.attn_fn(query, key, value, self.n_heads, softmax_scale=self.softmax_scale,\
    \ attn_bias=attn_bias, key_padding_mask=key_padding_mask, is_causal=is_causal,\
    \ dropout_p=self.attn_dropout_p, training=self.training, needs_weights=needs_weights)\
    \         \n  File \"/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py\"\
    , line 111, in triton_flash_attn_fn                                          \
    \                                                                      \n    attn_output\
    \ = flash_attn_triton.flash_attn_func(query, key, value, attn_bias, reset_is_causal,\
    \ softmax_scale)\n  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\"\
    , line 506, in apply                                                         \
    \                                                                            \
    \                                   \n    return super().apply(*args, **kwargs)\
    \  # type: ignore[misc]                                                      \
    \                                                                            \
    \                                                                            \
    \          \n  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/flash_attn/flash_attn_triton.py\"\
    , line 810, in forward                     \n    o, lse, ctx.softmax_scale = _flash_attn_forward(\
    \                                                                            \
    \           \n  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/flash_attn/flash_attn_triton.py\"\
    , line 599, in _flash_attn_forward\n    assert bias.dtype in [q.dtype, torch.float]\n\
    ```"
  created_at: 2023-05-09 23:48:15+00:00
  edited: true
  hidden: false
  id: 645ae9cfe7e8a4b57bdc1411
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-16T22:27:54.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;winglian&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/winglian\"\
          >@<span class=\"underline\">winglian</span></a></span>\n\n\t</span></span>,\
          \ we have not tested finetuning with the HF Trainer so I can't guaranteed\
          \ compatibility.</p>\n<p>You can find instructions for fine-tuning with\
          \ Composer and our LLM Foundry codebase here: <a rel=\"nofollow\" href=\"\
          https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#llm-finetuning\"\
          >https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#llm-finetuning</a>.\
          \ We are committed to maintaining this repo for community and customers,\
          \ and you can file Github issues directly there!</p>\n"
        raw: 'Hi @winglian, we have not tested finetuning with the HF Trainer so I
          can''t guaranteed compatibility.


          You can find instructions for fine-tuning with Composer and our LLM Foundry
          codebase here: https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#llm-finetuning.
          We are committed to maintaining this repo for community and customers, and
          you can file Github issues directly there!'
        updatedAt: '2023-05-16T22:27:54.720Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6464036adfacc4d097bbfac3
    id: 6464036adfacc4d097bbfac2
    type: comment
  author: abhi-mosaic
  content: 'Hi @winglian, we have not tested finetuning with the HF Trainer so I can''t
    guaranteed compatibility.


    You can find instructions for fine-tuning with Composer and our LLM Foundry codebase
    here: https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#llm-finetuning.
    We are committed to maintaining this repo for community and customers, and you
    can file Github issues directly there!'
  created_at: 2023-05-16 21:27:54+00:00
  edited: false
  hidden: false
  id: 6464036adfacc4d097bbfac2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-16T22:27:54.000Z'
    data:
      status: closed
    id: 6464036adfacc4d097bbfac3
    type: status-change
  author: abhi-mosaic
  created_at: 2023-05-16 21:27:54+00:00
  id: 6464036adfacc4d097bbfac3
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: mosaicml/mpt-7b
repo_type: model
status: closed
target_branch: null
title: Can this be fine-tuned with triton backed flash attention and alibi using the
  huggingface transformers trainer?
