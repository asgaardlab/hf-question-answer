!!python/object:huggingface_hub.community.DiscussionWithDetails
author: leondz
conflicting_files: null
created_at: 2023-06-26 03:33:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640692164586-noauth.jpeg?w=200&h=200&f=face
      fullname: Leon Derczynski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leondz
      type: user
    createdAt: '2023-06-26T04:33:50.000Z'
    data:
      edited: false
      editors:
      - leondz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47532159090042114
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640692164586-noauth.jpeg?w=200&h=200&f=face
          fullname: Leon Derczynski
          isHf: false
          isPro: false
          name: leondz
          type: user
        html: "<p>invoking <code>generator = transformers.pipeline(task=\"text-generation\"\
          , model=\"mosaicml/mpt-7b\", trust_remote_code=True)</code> ends with this\
          \ exception:</p>\n<pre><code>You are using config.init_device='cpu', but\
          \ you can also use config.init_device=\"meta\" with Composer + FSDP for\
          \ fast initialization.\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:05&lt;00:00,\
          \  2.73s/it]\nXformers is not installed correctly. If you want to use memory_efficient_attention\
          \ to accelerate training use the following command to install Xformers\n\
          pip install xformers.\nThe model 'MPTForCausalLM' is not supported for text-generation.\
          \ Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
          \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM',\
          \ 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM',\
          \ 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel',\
          \ 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM',\
          \ 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM',\
          \ 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
          \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
          \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
          \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
          \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
          \ 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
          \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
          \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel',\
          \ 'XmodForCausalLM'].\n</code></pre>\n<p>However, loading using:</p>\n<pre><code>model\
          \ = transformers.AutoModelForCausalLM.from_pretrained(\n  'mosaicml/mpt-7b',\n\
          \  trust_remote_code=True\n)\n</code></pre>\n<p>works fine.</p>\n<p>How\
          \ can I load this model in a <code>pipeline</code>?</p>\n<h3 id=\"response-from-hf-staff\"\
          >response from HF staff:</h3>\n<p>-- the auto_map  attribute in the config.json\
          \ file is not properly set <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/24471#issuecomment-1606549042\"\
          >https://github.com/huggingface/transformers/issues/24471#issuecomment-1606549042</a></p>\n\
          <h3 id=\"information\">Information</h3>\n<ul>\n<li><input type=\"checkbox\"\
          \ disabled=\"\" checked=\"\"> The official example scripts</li>\n<li><input\
          \ type=\"checkbox\" disabled=\"\"> My own modified scripts</li>\n</ul>\n\
          <h3 id=\"tasks\">Tasks</h3>\n<ul>\n<li><input type=\"checkbox\" disabled=\"\
          \"> An officially supported task in the <code>examples</code> folder (such\
          \ as GLUE/SQuAD, ...)</li>\n<li><input type=\"checkbox\" disabled=\"\">\
          \ My own task or dataset (give details below)</li>\n</ul>\n<h3 id=\"reproduction\"\
          >Reproduction</h3>\n<ol>\n<li><code>generator = transformers.pipeline(task=\"\
          text-generation\", model=\"mosaicml/mpt-7b\", trust_remote_code=True)</code></li>\n\
          </ol>\n<h3 id=\"expected-behavior\">Expected behavior</h3>\n<p>The pipeline\
          \ would load OK, just as .from_pretrained works</p>\n<h3 id=\"system-info\"\
          >System Info</h3>\n<p>Python 3.8.10 (default, Nov 14 2022, 12:59:47) </p>\n\
          <p>transformers.<strong>version</strong> is '4.30.2'</p>\n<p>lambda labs\
          \ 1xA100</p>\n"
        raw: "invoking `generator = transformers.pipeline(task=\"text-generation\"\
          , model=\"mosaicml/mpt-7b\", trust_remote_code=True)` ends with this exception:\r\
          \n\r\n```\r\nYou are using config.init_device='cpu', but you can also use\
          \ config.init_device=\"meta\" with Composer + FSDP for fast initialization.\r\
          \nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 2/2 [00:05<00:00,  2.73s/it]\r\nXformers\
          \ is not installed correctly. If you want to use memory_efficient_attention\
          \ to accelerate training use the following command to install Xformers\r\
          \npip install xformers.\r\nThe model 'MPTForCausalLM' is not supported for\
          \ text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel',\
          \ 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
          \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
          \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
          \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
          \ 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM',\
          \ 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM',\
          \ 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM',\
          \ 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM',\
          \ 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM',\
          \ 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead',\
          \ 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM',\
          \ 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM',\
          \ 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel',\
          \ 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM',\
          \ 'XLNetLMHeadModel', 'XmodForCausalLM'].\r\n```\r\n\r\nHowever, loading\
          \ using:\r\n\r\n```\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\
          \n  'mosaicml/mpt-7b',\r\n  trust_remote_code=True\r\n)\r\n```\r\n\r\nworks\
          \ fine.\r\n\r\nHow can I load this model in a `pipeline`?\r\n\r\n### response\
          \ from HF staff:\r\n\r\n-- the auto_map  attribute in the config.json file\
          \ is not properly set https://github.com/huggingface/transformers/issues/24471#issuecomment-1606549042\r\
          \n\r\n\r\n\r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\
          \n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially\
          \ supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n\
          - [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\
          \n\r\n1. `generator = transformers.pipeline(task=\"text-generation\", model=\"\
          mosaicml/mpt-7b\", trust_remote_code=True)`\r\n\r\n\r\n### Expected behavior\r\
          \n\r\nThe pipeline would load OK, just as .from_pretrained works\r\n\r\n\
          ### System Info\r\n\r\nPython 3.8.10 (default, Nov 14 2022, 12:59:47) \r\
          \n\r\ntransformers.__version__ is '4.30.2'\r\n\r\nlambda labs 1xA100\r\n"
        updatedAt: '2023-06-26T04:33:50.604Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - palaashag
    id: 6499152e6040fcfaad50ae2e
    type: comment
  author: leondz
  content: "invoking `generator = transformers.pipeline(task=\"text-generation\",\
    \ model=\"mosaicml/mpt-7b\", trust_remote_code=True)` ends with this exception:\r\
    \n\r\n```\r\nYou are using config.init_device='cpu', but you can also use config.init_device=\"\
    meta\" with Composer + FSDP for fast initialization.\r\nLoading checkpoint shards:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:05<00:00,  2.73s/it]\r\nXformers\
    \ is not installed correctly. If you want to use memory_efficient_attention to\
    \ accelerate training use the following command to install Xformers\r\npip install\
    \ xformers.\r\nThe model 'MPTForCausalLM' is not supported for text-generation.\
    \ Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
    \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM',\
    \ 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM',\
    \ 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM',\
    \ 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel',\
    \ 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM',\
    \ 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM',\
    \ 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM',\
    \ 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM',\
    \ 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
    \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
    \ 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
    \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
    \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\r\
    \n```\r\n\r\nHowever, loading using:\r\n\r\n```\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\
    \n  'mosaicml/mpt-7b',\r\n  trust_remote_code=True\r\n)\r\n```\r\n\r\nworks fine.\r\
    \n\r\nHow can I load this model in a `pipeline`?\r\n\r\n### response from HF staff:\r\
    \n\r\n-- the auto_map  attribute in the config.json file is not properly set https://github.com/huggingface/transformers/issues/24471#issuecomment-1606549042\r\
    \n\r\n\r\n\r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n\
    - [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported\
    \ task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task\
    \ or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1. `generator\
    \ = transformers.pipeline(task=\"text-generation\", model=\"mosaicml/mpt-7b\"\
    , trust_remote_code=True)`\r\n\r\n\r\n### Expected behavior\r\n\r\nThe pipeline\
    \ would load OK, just as .from_pretrained works\r\n\r\n### System Info\r\n\r\n\
    Python 3.8.10 (default, Nov 14 2022, 12:59:47) \r\n\r\ntransformers.__version__\
    \ is '4.30.2'\r\n\r\nlambda labs 1xA100\r\n"
  created_at: 2023-06-26 03:33:50+00:00
  edited: false
  hidden: false
  id: 6499152e6040fcfaad50ae2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e0d0df61ef3cfd995292cca3b8b05ea0.svg
      fullname: Palaash Agrawal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: palaashag
      type: user
    createdAt: '2023-07-19T12:17:45.000Z'
    data:
      edited: false
      editors:
      - palaashag
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7882450819015503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e0d0df61ef3cfd995292cca3b8b05ea0.svg
          fullname: Palaash Agrawal
          isHf: false
          isPro: false
          name: palaashag
          type: user
        html: '<p>Still facing the same issue!</p>

          '
        raw: 'Still facing the same issue!

          '
        updatedAt: '2023-07-19T12:17:45.755Z'
      numEdits: 0
      reactions: []
    id: 64b7d4699f5987572cac17cc
    type: comment
  author: palaashag
  content: 'Still facing the same issue!

    '
  created_at: 2023-07-19 11:17:45+00:00
  edited: false
  hidden: false
  id: 64b7d4699f5987572cac17cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9cbd83f5304ba59ee3c857122a9032bf.svg
      fullname: Peter Nicewicz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pnicewiczoig
      type: user
    createdAt: '2023-07-24T14:14:19.000Z'
    data:
      edited: false
      editors:
      - pnicewiczoig
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9800182580947876
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9cbd83f5304ba59ee3c857122a9032bf.svg
          fullname: Peter Nicewicz
          isHf: false
          isPro: false
          name: pnicewiczoig
          type: user
        html: '<p>I have the same issue. I''m trying to log the model and register
          the model in Databricks.</p>

          '
        raw: I have the same issue. I'm trying to log the model and register the model
          in Databricks.
        updatedAt: '2023-07-24T14:14:19.985Z'
      numEdits: 0
      reactions: []
    id: 64be873b6999b520ed84c7e5
    type: comment
  author: pnicewiczoig
  content: I have the same issue. I'm trying to log the model and register the model
    in Databricks.
  created_at: 2023-07-24 13:14:19+00:00
  edited: false
  hidden: false
  id: 64be873b6999b520ed84c7e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
      fullname: Daniel King
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daking
      type: user
    createdAt: '2023-07-25T01:32:16.000Z'
    data:
      edited: false
      editors:
      - daking
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9166794419288635
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
          fullname: Daniel King
          isHf: false
          isPro: false
          name: daking
          type: user
        html: '<p>Have you tried the pipeline usage example at the bottom of our model
          card (<a href="https://huggingface.co/mosaicml/mpt-7b">https://huggingface.co/mosaicml/mpt-7b</a>)?
          It is working for me, even though it does print out the message about mpt
          not being a supported model.</p>

          '
        raw: Have you tried the pipeline usage example at the bottom of our model
          card (https://huggingface.co/mosaicml/mpt-7b)? It is working for me, even
          though it does print out the message about mpt not being a supported model.
        updatedAt: '2023-07-25T01:32:16.773Z'
      numEdits: 0
      reactions: []
    id: 64bf262049525accb249ff23
    type: comment
  author: daking
  content: Have you tried the pipeline usage example at the bottom of our model card
    (https://huggingface.co/mosaicml/mpt-7b)? It is working for me, even though it
    does print out the message about mpt not being a supported model.
  created_at: 2023-07-25 00:32:16+00:00
  edited: false
  hidden: false
  id: 64bf262049525accb249ff23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f763bd37ce9d0e5b3defbb0327d804b3.svg
      fullname: Nafi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: na-fi
      type: user
    createdAt: '2023-08-14T07:30:39.000Z'
    data:
      edited: false
      editors:
      - na-fi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7747322916984558
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f763bd37ce9d0e5b3defbb0327d804b3.svg
          fullname: Nafi
          isHf: false
          isPro: false
          name: na-fi
          type: user
        html: '<p>Does anyone know how to hide the warning message ''The model ''MPTForCausalLM''
          is not supported for text-generation.'' ?</p>

          '
        raw: Does anyone know how to hide the warning message 'The model 'MPTForCausalLM'
          is not supported for text-generation.' ?
        updatedAt: '2023-08-14T07:30:39.188Z'
      numEdits: 0
      reactions: []
    id: 64d9d81f70891ac9b8d8454f
    type: comment
  author: na-fi
  content: Does anyone know how to hide the warning message 'The model 'MPTForCausalLM'
    is not supported for text-generation.' ?
  created_at: 2023-08-14 06:30:39+00:00
  edited: false
  hidden: false
  id: 64d9d81f70891ac9b8d8454f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49e218ebc08597cf06dfddf9d357b610.svg
      fullname: Alex C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adc23
      type: user
    createdAt: '2023-10-19T14:17:12.000Z'
    data:
      edited: true
      editors:
      - adc23
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9594358801841736
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49e218ebc08597cf06dfddf9d357b610.svg
          fullname: Alex C
          isHf: false
          isPro: false
          name: adc23
          type: user
        html: '<p>Still getting this error as well! transformers==4.33.1, mlflow==2.7.1</p>

          '
        raw: Still getting this error as well! transformers==4.33.1, mlflow==2.7.1
        updatedAt: '2023-10-19T14:17:45.812Z'
      numEdits: 1
      reactions: []
    id: 65313a68f3408030487cb150
    type: comment
  author: adc23
  content: Still getting this error as well! transformers==4.33.1, mlflow==2.7.1
  created_at: 2023-10-19 13:17:12+00:00
  edited: true
  hidden: false
  id: 65313a68f3408030487cb150
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
      fullname: Daniel King
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daking
      type: user
    createdAt: '2023-12-20T01:21:45.000Z'
    data:
      edited: false
      editors:
      - daking
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9628015160560608
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
          fullname: Daniel King
          isHf: false
          isPro: false
          name: daking
          type: user
        html: '<p>This is just a warning and should not cause issues.</p>

          '
        raw: This is just a warning and should not cause issues.
        updatedAt: '2023-12-20T01:21:45.052Z'
      numEdits: 0
      reactions: []
      relatedEventId: 658241a9f563e9e971d13fa4
    id: 658241a9f563e9e971d13f9e
    type: comment
  author: daking
  content: This is just a warning and should not cause issues.
  created_at: 2023-12-20 01:21:45+00:00
  edited: false
  hidden: false
  id: 658241a9f563e9e971d13f9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
      fullname: Daniel King
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daking
      type: user
    createdAt: '2023-12-20T01:21:45.000Z'
    data:
      status: closed
    id: 658241a9f563e9e971d13fa4
    type: status-change
  author: daking
  created_at: 2023-12-20 01:21:45+00:00
  id: 658241a9f563e9e971d13fa4
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 67
repo_id: mosaicml/mpt-7b
repo_type: model
status: closed
target_branch: null
title: '"MPTForCausalLM not supported" error when using pipeline, but not when using
  from_pretrained'
