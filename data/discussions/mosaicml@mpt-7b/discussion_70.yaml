!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NickyNicky
conflicting_files: null
created_at: 2023-06-29 07:17:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-06-29T08:17:26.000Z'
    data:
      edited: true
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.17153987288475037
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: "<p>my code is and error train</p>\n<pre><code class=\"language-Python\"\
          ><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >smart_tokenizer_and_embedding_resize</span>(<span class=\"hljs-params\"\
          > <span class=\"hljs-comment\"># https://github.com/artidoro/qlora/blob/main/qlora.py</span></span>\n\
          <span class=\"hljs-params\">    special_tokens_dict,</span>\n<span class=\"\
          hljs-params\">    tokenizer,</span>\n<span class=\"hljs-params\">    model,</span>\n\
          <span class=\"hljs-params\"></span>):\n    <span class=\"hljs-string\">\"\
          \"\"Resize tokenizer and embedding.</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">    Note: This is the unoptimized\
          \ version that may make your embedding size not be divisible by 64.</span>\n\
          <span class=\"hljs-string\">    \"\"\"</span>\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n\
          \    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"We have add_special_tokens:\"</span>, num_new_tokens, <span class=\"\
          hljs-string\">\"tokens\"</span>)\n\n    old_embeddings= model.get_input_embeddings()\n\
          \    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"old_embeddings: \"</span>,old_embeddings.weight.data.shape)\n\n    <span\
          \ class=\"hljs-comment\"># model.resize_token_embeddings(len(tokenizer)+num_new_tokens)\
          \ #aqui elimina como 12 embeddings pero es el codigo original</span>\n \
          \   model.resize_token_embeddings(old_embeddings.weight.data.shape[<span\
          \ class=\"hljs-number\">0</span>]+num_new_tokens) <span class=\"hljs-comment\"\
          ># implementacion mia donde no elimina los embeddings</span>\n\n\n    <span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"model:\
          \ \"</span>,model.get_input_embeddings().weight.data.shape)\n\n    <span\
          \ class=\"hljs-keyword\">if</span> num_new_tokens &gt; <span class=\"hljs-number\"\
          >0</span>:\n        input_embeddings = model.get_input_embeddings().weight.data\n\
          \        output_embeddings = model.get_output_embeddings().weight.data\n\
          \n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"input_embeddings:\"</span>,input_embeddings.shape)\n        <span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"output_embeddings:\"\
          </span>,output_embeddings.shape)\n        <span class=\"hljs-comment\">#\
          \ print(\"\")</span>\n        <span class=\"hljs-comment\"># print(\"output_embeddings:\"\
          ,output_embeddings)    </span>\n        <span class=\"hljs-comment\"># print(\"\
          --&gt;&gt;\",input_embeddings[:-num_new_tokens]).mean(dim=0, keepdim=True)</span>\n\
          \n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=<span\
          \ class=\"hljs-number\">0</span>, keepdim=<span class=\"hljs-literal\">True</span>)\n\
          \        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=<span\
          \ class=\"hljs-number\">0</span>, keepdim=<span class=\"hljs-literal\">True</span>)\n\
          \n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"input_embeddings_avg:\"</span>,input_embeddings_avg.shape)\n        <span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"output_embeddings_avg:\"\
          </span>,output_embeddings_avg.shape)\n\n        input_embeddings[-num_new_tokens:]\
          \ = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] =\
          \ output_embeddings_avg\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">\"\"</span>)\n    \n    <span class=\"hljs-built_in\"\
          >print</span>(model)\n\n<span class=\"hljs-comment\"># {'text': \" Who made\
          \ you?&lt;|endoftext|&gt;\\n&lt;bot&gt;: I'm h2oGPT, a large language model\
          \ by H2O.ai, the visionary leader in democratizing AI.\\n&lt;|endoftext|&gt;\"\
          }</span>\n\ndic = {\n   <span class=\"hljs-string\">'additional_special_tokens'</span>:\
          \ [<span class=\"hljs-string\">'&lt;human&gt;:'</span>,<span class=\"hljs-string\"\
          >'&lt;bot&gt;:'</span>]\n}\nsmart_tokenizer_and_embedding_resize(\n    special_tokens_dict=\
          \ dic,\n    tokenizer=tokenizer,\n    model=model,\n)\n</code></pre>\n<pre><code>##########\
          \ DATASET\nDataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n\
          \    num_rows: 12332\n})\n</code></pre>\n<pre><code>########## TRAIN\nfrom\
          \ torch.optim import AdamW\n\n# optimizer = AdamW(model.parameters(), lr=1e-3)\n\
          # Crear el optimizador\n# optimizer = AdamW(model.parameters(), lr=1e-4)\n\
          \n\ntraining_args = transformers.TrainingArguments(\n    per_device_train_batch_size=6,\
          \ # es el tama\xF1o del lote de entrenamiento por dispositivo.\n    per_device_eval_batch_size=6,\
          \ #  es el tama\xF1o del lote de evaluaci\xF3n por dispositivo.\n\n    gradient_accumulation_steps=3,\n\
          \    logging_steps=4,\n    num_train_epochs=1,\n    learning_rate= 1e-3,\
          \ # #2e-8, 3e-5, 1e-5, 2e-5, (1e-3), 1e-2\n\n    # fp16=False,\n    # #\
          \ fp16=True, #True,\n    # auto_find_batch_size=False,     # Desactivar\
          \ la b\xFAsqueda autom\xE1tica del tama\xF1o de lote\n\n    fp16=True,\n\
          \    auto_find_batch_size=True,     # Desactivar la b\xFAsqueda autom\xE1\
          tica del tama\xF1o de lote\n\n    save_total_limit=4,\n    output_dir=\"\
          ./outputs\",\n\n    optim=\"adamw_torch\",\n\n    lr_scheduler_type = 'cosine',\n\
          \    warmup_ratio = 0.03, #0.05,\n)\n\ntrainer = transformers.Trainer(\n\
          \    model=model,\n    train_dataset=lm_dataset,\n    # eval_dataset=val_dataset,\n\
          \    args=training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,\
          \ mlm=False),\n)\n\n\n# Configurar el optimizador en el objeto Trainer\n\
          # trainer.optimizer = optimizer\n\ntrainer.train()\n</code></pre>\n<pre><code>##########\
          \ Error\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502\
          \ in &lt;cell line: 49&gt;:49                                          \
          \                                  \u2502\n\u2502                      \
          \                                                                      \
          \      \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1645\
          \ in train                    \u2502\n\u2502                           \
          \                                                                      \
          \ \u2502\n\u2502   1642 \u2502   \u2502   inner_training_loop = find_executable_batch_size(\
          \                                 \u2502\n\u2502   1643 \u2502   \u2502\
          \   \u2502   self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size\
          \  \u2502\n\u2502   1644 \u2502   \u2502   )                           \
          \                                                      \u2502\n\u2502 \u2771\
          \ 1645 \u2502   \u2502   return inner_training_loop(                   \
          \                                    \u2502\n\u2502   1646 \u2502   \u2502\
          \   \u2502   args=args,                                                \
          \                    \u2502\n\u2502   1647 \u2502   \u2502   \u2502   resume_from_checkpoint=resume_from_checkpoint,\
          \                                \u2502\n\u2502   1648 \u2502   \u2502 \
          \  \u2502   trial=trial,                                               \
          \                   \u2502\n\u2502                                     \
          \                                                             \u2502\n\u2502\
          \ /usr/local/lib/python3.10/dist-packages/accelerate/utils/memory.py:132\
          \ in decorator              \u2502\n\u2502                             \
          \                                                                     \u2502\
          \n\u2502   129 \u2502   \u2502   \u2502   if batch_size == 0:          \
          \                                                  \u2502\n\u2502   130\
          \ \u2502   \u2502   \u2502   \u2502   raise RuntimeError(\"No executable\
          \ batch size found, reached zero.\")        \u2502\n\u2502   131 \u2502\
          \   \u2502   \u2502   try:                                             \
          \                              \u2502\n\u2502 \u2771 132 \u2502   \u2502\
          \   \u2502   \u2502   return function(batch_size, *args, **kwargs)     \
          \                          \u2502\n\u2502   133 \u2502   \u2502   \u2502\
          \   except Exception as e:                                             \
          \            \u2502\n\u2502   134 \u2502   \u2502   \u2502   \u2502   if\
          \ should_reduce_batch_size(e):                                         \
          \   \u2502\n\u2502   135 \u2502   \u2502   \u2502   \u2502   \u2502   gc.collect()\
          \                                                           \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1938\
          \ in _inner_training_loop     \u2502\n\u2502                           \
          \                                                                      \
          \ \u2502\n\u2502   1935 \u2502   \u2502   \u2502   \u2502   \u2502   self.control\
          \ = self.callback_handler.on_step_begin(args, self.state,  \u2502\n\u2502\
          \   1936 \u2502   \u2502   \u2502   \u2502                             \
          \                                                \u2502\n\u2502   1937 \u2502\
          \   \u2502   \u2502   \u2502   with self.accelerator.accumulate(model):\
          \                                  \u2502\n\u2502 \u2771 1938 \u2502   \u2502\
          \   \u2502   \u2502   \u2502   tr_loss_step = self.training_step(model,\
          \ inputs)                      \u2502\n\u2502   1939 \u2502   \u2502   \u2502\
          \   \u2502                                                             \
          \                \u2502\n\u2502   1940 \u2502   \u2502   \u2502   \u2502\
          \   if (                                                               \
          \       \u2502\n\u2502   1941 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   args.logging_nan_inf_filter                                        \
          \   \u2502\n\u2502                                                     \
          \                                             \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2759\
          \ in training_step            \u2502\n\u2502                           \
          \                                                                      \
          \ \u2502\n\u2502   2756 \u2502   \u2502   \u2502   return loss_mb.reduce_mean().detach().to(self.args.device)\
          \                    \u2502\n\u2502   2757 \u2502   \u2502             \
          \                                                                      \
          \  \u2502\n\u2502   2758 \u2502   \u2502   with self.compute_loss_context_manager():\
          \                                         \u2502\n\u2502 \u2771 2759 \u2502\
          \   \u2502   \u2502   loss = self.compute_loss(model, inputs)          \
          \                             \u2502\n\u2502   2760 \u2502   \u2502    \
          \                                                                      \
          \           \u2502\n\u2502   2761 \u2502   \u2502   if self.args.n_gpu &gt;\
          \ 1:                                                           \u2502\n\u2502\
          \   2762 \u2502   \u2502   \u2502   loss = loss.mean()  # mean() to average\
          \ on multi-gpu parallel training        \u2502\n\u2502                 \
          \                                                                      \
          \           \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2784\
          \ in compute_loss             \u2502\n\u2502                           \
          \                                                                      \
          \ \u2502\n\u2502   2781 \u2502   \u2502   \u2502   labels = inputs.pop(\"\
          labels\")                                                 \u2502\n\u2502\
          \   2782 \u2502   \u2502   else:                                       \
          \                                      \u2502\n\u2502   2783 \u2502   \u2502\
          \   \u2502   labels = None                                             \
          \                    \u2502\n\u2502 \u2771 2784 \u2502   \u2502   outputs\
          \ = model(**inputs)                                                    \
          \     \u2502\n\u2502   2785 \u2502   \u2502   # Save past state if it exists\
          \                                                    \u2502\n\u2502   2786\
          \ \u2502   \u2502   # TODO: this needs to be fixed and made cleaner later.\
          \                            \u2502\n\u2502   2787 \u2502   \u2502   if\
          \ self.args.past_index &gt;= 0:                                        \
          \             \u2502\n\u2502                                           \
          \                                                       \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\
          \ in _call_impl            \u2502\n\u2502                              \
          \                                                                    \u2502\
          \n\u2502   1191 \u2502   \u2502   # this function, and just call forward.\
          \                                           \u2502\n\u2502   1192 \u2502\
          \   \u2502   if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks\
          \ o  \u2502\n\u2502   1193 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
          \ or _global_forward_pre_hooks):                   \u2502\n\u2502 \u2771\
          \ 1194 \u2502   \u2502   \u2502   return forward_call(*input, **kwargs)\
          \                                         \u2502\n\u2502   1195 \u2502 \
          \  \u2502   # Do not call functions when jit is used                   \
          \                       \u2502\n\u2502   1196 \u2502   \u2502   full_backward_hooks,\
          \ non_full_backward_hooks = [], []                             \u2502\n\u2502\
          \   1197 \u2502   \u2502   if self._backward_hooks or _global_backward_hooks:\
          \                                \u2502\n\u2502                        \
          \                                                                      \
          \    \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
          \ in forward            \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   550 \u2502   model_forward = ConvertOutputsToFp32(model_forward)\
          \                                    \u2502\n\u2502   551 \u2502       \
          \                                                                      \
          \             \u2502\n\u2502   552 \u2502   def forward(*args, **kwargs):\
          \                                                          \u2502\n\u2502\
          \ \u2771 553 \u2502   \u2502   return model_forward(*args, **kwargs)   \
          \                                           \u2502\n\u2502   554 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   555 \u2502   # To act like a decorator\
          \ so that it can be popped when doing `extract_model_from_pa   \u2502\n\u2502\
          \   556 \u2502   forward.__wrapped__ = model_forward                   \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   538 \u2502   \u2502   update_wrapper(self, model_forward)   \
          \                                             \u2502\n\u2502   539 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   540 \u2502   def __call__(self, *args,\
          \ **kwargs):                                                   \u2502\n\u2502\
          \ \u2771 541 \u2502   \u2502   return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))                        \u2502\n\u2502   542 \u2502        \
          \                                                                      \
          \            \u2502\n\u2502   543 \u2502   def __getstate__(self):     \
          \                                                           \u2502\n\u2502\
          \   544 \u2502   \u2502   raise pickle.PicklingError(                  \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14\
          \ in decorate_autocast       \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    11 \u2502   @functools.wraps(func)                         \
          \                                        \u2502\n\u2502    12 \u2502   def\
          \ decorate_autocast(*args, **kwargs):                                  \
          \              \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
          \                                                            \u2502\n\u2502\
          \ \u2771  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)   \
          \                                                \u2502\n\u2502    15 \u2502\
          \   decorate_autocast.__script_unsupported = '@autocast() decorator is not\
          \ supported in    \u2502\n\u2502    16 \u2502   return decorate_autocast\
          \                                                               \u2502\n\
          \u2502    17                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
          \ in forward            \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   550 \u2502   model_forward = ConvertOutputsToFp32(model_forward)\
          \                                    \u2502\n\u2502   551 \u2502       \
          \                                                                      \
          \             \u2502\n\u2502   552 \u2502   def forward(*args, **kwargs):\
          \                                                          \u2502\n\u2502\
          \ \u2771 553 \u2502   \u2502   return model_forward(*args, **kwargs)   \
          \                                           \u2502\n\u2502   554 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   555 \u2502   # To act like a decorator\
          \ so that it can be popped when doing `extract_model_from_pa   \u2502\n\u2502\
          \   556 \u2502   forward.__wrapped__ = model_forward                   \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   538 \u2502   \u2502   update_wrapper(self, model_forward)   \
          \                                             \u2502\n\u2502   539 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   540 \u2502   def __call__(self, *args,\
          \ **kwargs):                                                   \u2502\n\u2502\
          \ \u2771 541 \u2502   \u2502   return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))                        \u2502\n\u2502   542 \u2502        \
          \                                                                      \
          \            \u2502\n\u2502   543 \u2502   def __getstate__(self):     \
          \                                                           \u2502\n\u2502\
          \   544 \u2502   \u2502   raise pickle.PicklingError(                  \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14\
          \ in decorate_autocast       \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    11 \u2502   @functools.wraps(func)                         \
          \                                        \u2502\n\u2502    12 \u2502   def\
          \ decorate_autocast(*args, **kwargs):                                  \
          \              \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
          \                                                            \u2502\n\u2502\
          \ \u2771  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)   \
          \                                                \u2502\n\u2502    15 \u2502\
          \   decorate_autocast.__script_unsupported = '@autocast() decorator is not\
          \ supported in    \u2502\n\u2502    16 \u2502   return decorate_autocast\
          \                                                               \u2502\n\
          \u2502    17                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
          \ in forward            \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   550 \u2502   model_forward = ConvertOutputsToFp32(model_forward)\
          \                                    \u2502\n\u2502   551 \u2502       \
          \                                                                      \
          \             \u2502\n\u2502   552 \u2502   def forward(*args, **kwargs):\
          \                                                          \u2502\n\u2502\
          \ \u2771 553 \u2502   \u2502   return model_forward(*args, **kwargs)   \
          \                                           \u2502\n\u2502   554 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   555 \u2502   # To act like a decorator\
          \ so that it can be popped when doing `extract_model_from_pa   \u2502\n\u2502\
          \   556 \u2502   forward.__wrapped__ = model_forward                   \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   538 \u2502   \u2502   update_wrapper(self, model_forward)   \
          \                                             \u2502\n\u2502   539 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   540 \u2502   def __call__(self, *args,\
          \ **kwargs):                                                   \u2502\n\u2502\
          \ \u2771 541 \u2502   \u2502   return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))                        \u2502\n\u2502   542 \u2502        \
          \                                                                      \
          \            \u2502\n\u2502   543 \u2502   def __getstate__(self):     \
          \                                                           \u2502\n\u2502\
          \   544 \u2502   \u2502   raise pickle.PicklingError(                  \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14\
          \ in decorate_autocast       \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    11 \u2502   @functools.wraps(func)                         \
          \                                        \u2502\n\u2502    12 \u2502   def\
          \ decorate_autocast(*args, **kwargs):                                  \
          \              \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
          \                                                            \u2502\n\u2502\
          \ \u2771  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)   \
          \                                                \u2502\n\u2502    15 \u2502\
          \   decorate_autocast.__script_unsupported = '@autocast() decorator is not\
          \ supported in    \u2502\n\u2502    16 \u2502   return decorate_autocast\
          \                                                               \u2502\n\
          \u2502    17                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
          \ in forward            \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   550 \u2502   model_forward = ConvertOutputsToFp32(model_forward)\
          \                                    \u2502\n\u2502   551 \u2502       \
          \                                                                      \
          \             \u2502\n\u2502   552 \u2502   def forward(*args, **kwargs):\
          \                                                          \u2502\n\u2502\
          \ \u2771 553 \u2502   \u2502   return model_forward(*args, **kwargs)   \
          \                                           \u2502\n\u2502   554 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   555 \u2502   # To act like a decorator\
          \ so that it can be popped when doing `extract_model_from_pa   \u2502\n\u2502\
          \   556 \u2502   forward.__wrapped__ = model_forward                   \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   538 \u2502   \u2502   update_wrapper(self, model_forward)   \
          \                                             \u2502\n\u2502   539 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   540 \u2502   def __call__(self, *args,\
          \ **kwargs):                                                   \u2502\n\u2502\
          \ \u2771 541 \u2502   \u2502   return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))                        \u2502\n\u2502   542 \u2502        \
          \                                                                      \
          \            \u2502\n\u2502   543 \u2502   def __getstate__(self):     \
          \                                                           \u2502\n\u2502\
          \   544 \u2502   \u2502   raise pickle.PicklingError(                  \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14\
          \ in decorate_autocast       \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    11 \u2502   @functools.wraps(func)                         \
          \                                        \u2502\n\u2502    12 \u2502   def\
          \ decorate_autocast(*args, **kwargs):                                  \
          \              \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
          \                                                            \u2502\n\u2502\
          \ \u2771  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)   \
          \                                                \u2502\n\u2502    15 \u2502\
          \   decorate_autocast.__script_unsupported = '@autocast() decorator is not\
          \ supported in    \u2502\n\u2502    16 \u2502   return decorate_autocast\
          \                                                               \u2502\n\
          \u2502    17                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
          \ in forward            \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   550 \u2502   model_forward = ConvertOutputsToFp32(model_forward)\
          \                                    \u2502\n\u2502   551 \u2502       \
          \                                                                      \
          \             \u2502\n\u2502   552 \u2502   def forward(*args, **kwargs):\
          \                                                          \u2502\n\u2502\
          \ \u2771 553 \u2502   \u2502   return model_forward(*args, **kwargs)   \
          \                                           \u2502\n\u2502   554 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   555 \u2502   # To act like a decorator\
          \ so that it can be popped when doing `extract_model_from_pa   \u2502\n\u2502\
          \   556 \u2502   forward.__wrapped__ = model_forward                   \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   538 \u2502   \u2502   update_wrapper(self, model_forward)   \
          \                                             \u2502\n\u2502   539 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   540 \u2502   def __call__(self, *args,\
          \ **kwargs):                                                   \u2502\n\u2502\
          \ \u2771 541 \u2502   \u2502   return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))                        \u2502\n\u2502   542 \u2502        \
          \                                                                      \
          \            \u2502\n\u2502   543 \u2502   def __getstate__(self):     \
          \                                                           \u2502\n\u2502\
          \   544 \u2502   \u2502   raise pickle.PicklingError(                  \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14\
          \ in decorate_autocast       \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    11 \u2502   @functools.wraps(func)                         \
          \                                        \u2502\n\u2502    12 \u2502   def\
          \ decorate_autocast(*args, **kwargs):                                  \
          \              \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
          \                                                            \u2502\n\u2502\
          \ \u2771  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)   \
          \                                                \u2502\n\u2502    15 \u2502\
          \   decorate_autocast.__script_unsupported = '@autocast() decorator is not\
          \ supported in    \u2502\n\u2502    16 \u2502   return decorate_autocast\
          \                                                               \u2502\n\
          \u2502    17                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
          \ in forward            \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   550 \u2502   model_forward = ConvertOutputsToFp32(model_forward)\
          \                                    \u2502\n\u2502   551 \u2502       \
          \                                                                      \
          \             \u2502\n\u2502   552 \u2502   def forward(*args, **kwargs):\
          \                                                          \u2502\n\u2502\
          \ \u2771 553 \u2502   \u2502   return model_forward(*args, **kwargs)   \
          \                                           \u2502\n\u2502   554 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   555 \u2502   # To act like a decorator\
          \ so that it can be popped when doing `extract_model_from_pa   \u2502\n\u2502\
          \   556 \u2502   forward.__wrapped__ = model_forward                   \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   538 \u2502   \u2502   update_wrapper(self, model_forward)   \
          \                                             \u2502\n\u2502   539 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   540 \u2502   def __call__(self, *args,\
          \ **kwargs):                                                   \u2502\n\u2502\
          \ \u2771 541 \u2502   \u2502   return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))                        \u2502\n\u2502   542 \u2502        \
          \                                                                      \
          \            \u2502\n\u2502   543 \u2502   def __getstate__(self):     \
          \                                                           \u2502\n\u2502\
          \   544 \u2502   \u2502   raise pickle.PicklingError(                  \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14\
          \ in decorate_autocast       \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    11 \u2502   @functools.wraps(func)                         \
          \                                        \u2502\n\u2502    12 \u2502   def\
          \ decorate_autocast(*args, **kwargs):                                  \
          \              \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
          \                                                            \u2502\n\u2502\
          \ \u2771  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)   \
          \                                                \u2502\n\u2502    15 \u2502\
          \   decorate_autocast.__script_unsupported = '@autocast() decorator is not\
          \ supported in    \u2502\n\u2502    16 \u2502   return decorate_autocast\
          \                                                               \u2502\n\
          \u2502    17                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/peft/peft_model.py:847\
          \ in forward                        \u2502\n\u2502                     \
          \                                                                      \
          \       \u2502\n\u2502    844 \u2502   \u2502   \u2502   if self.base_model.config.model_type\
          \ == \"mpt\":                                \u2502\n\u2502    845 \u2502\
          \   \u2502   \u2502   \u2502   if inputs_embeds is not None:           \
          \                                  \u2502\n\u2502    846 \u2502   \u2502\
          \   \u2502   \u2502   \u2502   raise AssertionError(\"forward in MPTForCausalLM\
          \ does not support inp  \u2502\n\u2502 \u2771  847 \u2502   \u2502   \u2502\
          \   \u2502   return self.base_model(                                   \
          \                \u2502\n\u2502    848 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   input_ids=input_ids,                                      \
          \            \u2502\n\u2502    849 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   attention_mask=attention_mask,                                     \
          \   \u2502\n\u2502    850 \u2502   \u2502   \u2502   \u2502   \u2502   labels=labels,\
          \                                                        \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\
          \ in _call_impl            \u2502\n\u2502                              \
          \                                                                    \u2502\
          \n\u2502   1191 \u2502   \u2502   # this function, and just call forward.\
          \                                           \u2502\n\u2502   1192 \u2502\
          \   \u2502   if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks\
          \ o  \u2502\n\u2502   1193 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
          \ or _global_forward_pre_hooks):                   \u2502\n\u2502 \u2771\
          \ 1194 \u2502   \u2502   \u2502   return forward_call(*input, **kwargs)\
          \                                         \u2502\n\u2502   1195 \u2502 \
          \  \u2502   # Do not call functions when jit is used                   \
          \                       \u2502\n\u2502   1196 \u2502   \u2502   full_backward_hooks,\
          \ non_full_backward_hooks = [], []                             \u2502\n\u2502\
          \   1197 \u2502   \u2502   if self._backward_hooks or _global_backward_hooks:\
          \                                \u2502\n\u2502                        \
          \                                                                      \
          \    \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:165\
          \ in new_forward                   \u2502\n\u2502                      \
          \                                                                      \
          \      \u2502\n\u2502   162 \u2502   \u2502   \u2502   with torch.no_grad():\
          \                                                          \u2502\n\u2502\
          \   163 \u2502   \u2502   \u2502   \u2502   output = old_forward(*args,\
          \ **kwargs)                                      \u2502\n\u2502   164 \u2502\
          \   \u2502   else:                                                     \
          \                         \u2502\n\u2502 \u2771 165 \u2502   \u2502   \u2502\
          \   output = old_forward(*args, **kwargs)                              \
          \            \u2502\n\u2502   166 \u2502   \u2502   return module._hf_hook.post_forward(module,\
          \ output)                                \u2502\n\u2502   167 \u2502   \
          \                                                                      \
          \                 \u2502\n\u2502   168 \u2502   module.forward = new_forward\
          \                                                           \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-instruct/1fc4634127ec64a45\
          \ \u2502\n\u2502 716003578b9cfae23265849/modeling_mpt.py:271 in forward\
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   268 \u2502   \u2502   if inputs_embeds\
          \ is not None:                                                      \u2502\
          \n\u2502   269 \u2502   \u2502   \u2502   raise NotImplementedError('inputs_embeds\
          \ has to be None (for hf/peft support   \u2502\n\u2502   270 \u2502   \u2502\
          \   outputs = self.transformer(input_ids=input_ids, past_key_values=past_key_values,\
          \   \u2502\n\u2502 \u2771 271 \u2502   \u2502   logits = self.transformer.wte(outputs.last_hidden_state.to(self.transformer.wte.\
          \   \u2502\n\u2502   272 \u2502   \u2502   if self.logit_scale is not None:\
          \                                                   \u2502\n\u2502   273\
          \ \u2502   \u2502   \u2502   if self.logit_scale == 0:                 \
          \                                     \u2502\n\u2502   274 \u2502   \u2502\
          \   \u2502   \u2502   warnings.warn(f'Multiplying logits by self.logit_scale={self.logit_scale\
          \   \u2502\n\u2502                                                     \
          \                                             \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\
          \ in _call_impl            \u2502\n\u2502                              \
          \                                                                    \u2502\
          \n\u2502   1191 \u2502   \u2502   # this function, and just call forward.\
          \                                           \u2502\n\u2502   1192 \u2502\
          \   \u2502   if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks\
          \ o  \u2502\n\u2502   1193 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
          \ or _global_forward_pre_hooks):                   \u2502\n\u2502 \u2771\
          \ 1194 \u2502   \u2502   \u2502   return forward_call(*input, **kwargs)\
          \                                         \u2502\n\u2502   1195 \u2502 \
          \  \u2502   # Do not call functions when jit is used                   \
          \                       \u2502\n\u2502   1196 \u2502   \u2502   full_backward_hooks,\
          \ non_full_backward_hooks = [], []                             \u2502\n\u2502\
          \   1197 \u2502   \u2502   if self._backward_hooks or _global_backward_hooks:\
          \                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nTypeError:\
          \ Embedding.forward() takes 2 positional arguments but 3 were given\n</code></pre>\n"
        raw: "my code is and error train\n\n```Python\ndef smart_tokenizer_and_embedding_resize(\
          \ # https://github.com/artidoro/qlora/blob/main/qlora.py\n    special_tokens_dict,\n\
          \    tokenizer,\n    model,\n):\n    \"\"\"Resize tokenizer and embedding.\n\
          \n    Note: This is the unoptimized version that may make your embedding\
          \ size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n\
          \    print(\"We have add_special_tokens:\", num_new_tokens, \"tokens\")\n\
          \n    old_embeddings= model.get_input_embeddings()\n    print(\"old_embeddings:\
          \ \",old_embeddings.weight.data.shape)\n\n    # model.resize_token_embeddings(len(tokenizer)+num_new_tokens)\
          \ #aqui elimina como 12 embeddings pero es el codigo original\n    model.resize_token_embeddings(old_embeddings.weight.data.shape[0]+num_new_tokens)\
          \ # implementacion mia donde no elimina los embeddings\n\n\n    print(\"\
          model: \",model.get_input_embeddings().weight.data.shape)\n\n    if num_new_tokens\
          \ > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n\
          \        output_embeddings = model.get_output_embeddings().weight.data\n\
          \n        print(\"input_embeddings:\",input_embeddings.shape)\n        print(\"\
          output_embeddings:\",output_embeddings.shape)\n        # print(\"\")\n \
          \       # print(\"output_embeddings:\",output_embeddings)    \n        #\
          \ print(\"-->>\",input_embeddings[:-num_new_tokens]).mean(dim=0, keepdim=True)\n\
          \n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0,\
          \ keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0,\
          \ keepdim=True)\n\n        print(\"input_embeddings_avg:\",input_embeddings_avg.shape)\n\
          \        print(\"output_embeddings_avg:\",output_embeddings_avg.shape)\n\
          \n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n  \
          \      output_embeddings[-num_new_tokens:] = output_embeddings_avg\n   \
          \ print(\"\")\n    \n    print(model)\n\n# {'text': \" Who made you?<|endoftext|>\\\
          n<bot>: I'm h2oGPT, a large language model by H2O.ai, the visionary leader\
          \ in democratizing AI.\\n<|endoftext|>\"}\n\ndic = {\n   'additional_special_tokens':\
          \ ['<human>:','<bot>:']\n}\nsmart_tokenizer_and_embedding_resize(\n    special_tokens_dict=\
          \ dic,\n    tokenizer=tokenizer,\n    model=model,\n)\n```\n\n``` \n##########\
          \ DATASET\nDataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n\
          \    num_rows: 12332\n})\n```\n\n\n```\n########## TRAIN\nfrom torch.optim\
          \ import AdamW\n\n# optimizer = AdamW(model.parameters(), lr=1e-3)\n# Crear\
          \ el optimizador\n# optimizer = AdamW(model.parameters(), lr=1e-4)\n\n\n\
          training_args = transformers.TrainingArguments(\n    per_device_train_batch_size=6,\
          \ # es el tama\xF1o del lote de entrenamiento por dispositivo.\n    per_device_eval_batch_size=6,\
          \ #  es el tama\xF1o del lote de evaluaci\xF3n por dispositivo.\n\n    gradient_accumulation_steps=3,\n\
          \    logging_steps=4,\n    num_train_epochs=1,\n    learning_rate= 1e-3,\
          \ # #2e-8, 3e-5, 1e-5, 2e-5, (1e-3), 1e-2\n\n    # fp16=False,\n    # #\
          \ fp16=True, #True,\n    # auto_find_batch_size=False,     # Desactivar\
          \ la b\xFAsqueda autom\xE1tica del tama\xF1o de lote\n\n    fp16=True,\n\
          \    auto_find_batch_size=True,     # Desactivar la b\xFAsqueda autom\xE1\
          tica del tama\xF1o de lote\n\n    save_total_limit=4,\n    output_dir=\"\
          ./outputs\",\n\n    optim=\"adamw_torch\",\n\n    lr_scheduler_type = 'cosine',\n\
          \    warmup_ratio = 0.03, #0.05,\n)\n\ntrainer = transformers.Trainer(\n\
          \    model=model,\n    train_dataset=lm_dataset,\n    # eval_dataset=val_dataset,\n\
          \    args=training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,\
          \ mlm=False),\n)\n\n\n# Configurar el optimizador en el objeto Trainer\n\
          # trainer.optimizer = optimizer\n\ntrainer.train()\n```\n\n``` \n##########\
          \ Error\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502\
          \ in <cell line: 49>:49                                                \
          \                            \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1645\
          \ in train                    \u2502\n\u2502                           \
          \                                                                      \
          \ \u2502\n\u2502   1642 \u2502   \u2502   inner_training_loop = find_executable_batch_size(\
          \                                 \u2502\n\u2502   1643 \u2502   \u2502\
          \   \u2502   self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size\
          \  \u2502\n\u2502   1644 \u2502   \u2502   )                           \
          \                                                      \u2502\n\u2502 \u2771\
          \ 1645 \u2502   \u2502   return inner_training_loop(                   \
          \                                    \u2502\n\u2502   1646 \u2502   \u2502\
          \   \u2502   args=args,                                                \
          \                    \u2502\n\u2502   1647 \u2502   \u2502   \u2502   resume_from_checkpoint=resume_from_checkpoint,\
          \                                \u2502\n\u2502   1648 \u2502   \u2502 \
          \  \u2502   trial=trial,                                               \
          \                   \u2502\n\u2502                                     \
          \                                                             \u2502\n\u2502\
          \ /usr/local/lib/python3.10/dist-packages/accelerate/utils/memory.py:132\
          \ in decorator              \u2502\n\u2502                             \
          \                                                                     \u2502\
          \n\u2502   129 \u2502   \u2502   \u2502   if batch_size == 0:          \
          \                                                  \u2502\n\u2502   130\
          \ \u2502   \u2502   \u2502   \u2502   raise RuntimeError(\"No executable\
          \ batch size found, reached zero.\")        \u2502\n\u2502   131 \u2502\
          \   \u2502   \u2502   try:                                             \
          \                              \u2502\n\u2502 \u2771 132 \u2502   \u2502\
          \   \u2502   \u2502   return function(batch_size, *args, **kwargs)     \
          \                          \u2502\n\u2502   133 \u2502   \u2502   \u2502\
          \   except Exception as e:                                             \
          \            \u2502\n\u2502   134 \u2502   \u2502   \u2502   \u2502   if\
          \ should_reduce_batch_size(e):                                         \
          \   \u2502\n\u2502   135 \u2502   \u2502   \u2502   \u2502   \u2502   gc.collect()\
          \                                                           \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1938\
          \ in _inner_training_loop     \u2502\n\u2502                           \
          \                                                                      \
          \ \u2502\n\u2502   1935 \u2502   \u2502   \u2502   \u2502   \u2502   self.control\
          \ = self.callback_handler.on_step_begin(args, self.state,  \u2502\n\u2502\
          \   1936 \u2502   \u2502   \u2502   \u2502                             \
          \                                                \u2502\n\u2502   1937 \u2502\
          \   \u2502   \u2502   \u2502   with self.accelerator.accumulate(model):\
          \                                  \u2502\n\u2502 \u2771 1938 \u2502   \u2502\
          \   \u2502   \u2502   \u2502   tr_loss_step = self.training_step(model,\
          \ inputs)                      \u2502\n\u2502   1939 \u2502   \u2502   \u2502\
          \   \u2502                                                             \
          \                \u2502\n\u2502   1940 \u2502   \u2502   \u2502   \u2502\
          \   if (                                                               \
          \       \u2502\n\u2502   1941 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   args.logging_nan_inf_filter                                        \
          \   \u2502\n\u2502                                                     \
          \                                             \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2759\
          \ in training_step            \u2502\n\u2502                           \
          \                                                                      \
          \ \u2502\n\u2502   2756 \u2502   \u2502   \u2502   return loss_mb.reduce_mean().detach().to(self.args.device)\
          \                    \u2502\n\u2502   2757 \u2502   \u2502             \
          \                                                                      \
          \  \u2502\n\u2502   2758 \u2502   \u2502   with self.compute_loss_context_manager():\
          \                                         \u2502\n\u2502 \u2771 2759 \u2502\
          \   \u2502   \u2502   loss = self.compute_loss(model, inputs)          \
          \                             \u2502\n\u2502   2760 \u2502   \u2502    \
          \                                                                      \
          \           \u2502\n\u2502   2761 \u2502   \u2502   if self.args.n_gpu >\
          \ 1:                                                           \u2502\n\u2502\
          \   2762 \u2502   \u2502   \u2502   loss = loss.mean()  # mean() to average\
          \ on multi-gpu parallel training        \u2502\n\u2502                 \
          \                                                                      \
          \           \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2784\
          \ in compute_loss             \u2502\n\u2502                           \
          \                                                                      \
          \ \u2502\n\u2502   2781 \u2502   \u2502   \u2502   labels = inputs.pop(\"\
          labels\")                                                 \u2502\n\u2502\
          \   2782 \u2502   \u2502   else:                                       \
          \                                      \u2502\n\u2502   2783 \u2502   \u2502\
          \   \u2502   labels = None                                             \
          \                    \u2502\n\u2502 \u2771 2784 \u2502   \u2502   outputs\
          \ = model(**inputs)                                                    \
          \     \u2502\n\u2502   2785 \u2502   \u2502   # Save past state if it exists\
          \                                                    \u2502\n\u2502   2786\
          \ \u2502   \u2502   # TODO: this needs to be fixed and made cleaner later.\
          \                            \u2502\n\u2502   2787 \u2502   \u2502   if\
          \ self.args.past_index >= 0:                                           \
          \          \u2502\n\u2502                                              \
          \                                                    \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\
          \ in _call_impl            \u2502\n\u2502                              \
          \                                                                    \u2502\
          \n\u2502   1191 \u2502   \u2502   # this function, and just call forward.\
          \                                           \u2502\n\u2502   1192 \u2502\
          \   \u2502   if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks\
          \ o  \u2502\n\u2502   1193 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
          \ or _global_forward_pre_hooks):                   \u2502\n\u2502 \u2771\
          \ 1194 \u2502   \u2502   \u2502   return forward_call(*input, **kwargs)\
          \                                         \u2502\n\u2502   1195 \u2502 \
          \  \u2502   # Do not call functions when jit is used                   \
          \                       \u2502\n\u2502   1196 \u2502   \u2502   full_backward_hooks,\
          \ non_full_backward_hooks = [], []                             \u2502\n\u2502\
          \   1197 \u2502   \u2502   if self._backward_hooks or _global_backward_hooks:\
          \                                \u2502\n\u2502                        \
          \                                                                      \
          \    \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
          \ in forward            \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   550 \u2502   model_forward = ConvertOutputsToFp32(model_forward)\
          \                                    \u2502\n\u2502   551 \u2502       \
          \                                                                      \
          \             \u2502\n\u2502   552 \u2502   def forward(*args, **kwargs):\
          \                                                          \u2502\n\u2502\
          \ \u2771 553 \u2502   \u2502   return model_forward(*args, **kwargs)   \
          \                                           \u2502\n\u2502   554 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   555 \u2502   # To act like a decorator\
          \ so that it can be popped when doing `extract_model_from_pa   \u2502\n\u2502\
          \   556 \u2502   forward.__wrapped__ = model_forward                   \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   538 \u2502   \u2502   update_wrapper(self, model_forward)   \
          \                                             \u2502\n\u2502   539 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   540 \u2502   def __call__(self, *args,\
          \ **kwargs):                                                   \u2502\n\u2502\
          \ \u2771 541 \u2502   \u2502   return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))                        \u2502\n\u2502   542 \u2502        \
          \                                                                      \
          \            \u2502\n\u2502   543 \u2502   def __getstate__(self):     \
          \                                                           \u2502\n\u2502\
          \   544 \u2502   \u2502   raise pickle.PicklingError(                  \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14\
          \ in decorate_autocast       \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    11 \u2502   @functools.wraps(func)                         \
          \                                        \u2502\n\u2502    12 \u2502   def\
          \ decorate_autocast(*args, **kwargs):                                  \
          \              \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
          \                                                            \u2502\n\u2502\
          \ \u2771  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)   \
          \                                                \u2502\n\u2502    15 \u2502\
          \   decorate_autocast.__script_unsupported = '@autocast() decorator is not\
          \ supported in    \u2502\n\u2502    16 \u2502   return decorate_autocast\
          \                                                               \u2502\n\
          \u2502    17                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
          \ in forward            \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   550 \u2502   model_forward = ConvertOutputsToFp32(model_forward)\
          \                                    \u2502\n\u2502   551 \u2502       \
          \                                                                      \
          \             \u2502\n\u2502   552 \u2502   def forward(*args, **kwargs):\
          \                                                          \u2502\n\u2502\
          \ \u2771 553 \u2502   \u2502   return model_forward(*args, **kwargs)   \
          \                                           \u2502\n\u2502   554 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   555 \u2502   # To act like a decorator\
          \ so that it can be popped when doing `extract_model_from_pa   \u2502\n\u2502\
          \   556 \u2502   forward.__wrapped__ = model_forward                   \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   538 \u2502   \u2502   update_wrapper(self, model_forward)   \
          \                                             \u2502\n\u2502   539 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   540 \u2502   def __call__(self, *args,\
          \ **kwargs):                                                   \u2502\n\u2502\
          \ \u2771 541 \u2502   \u2502   return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))                        \u2502\n\u2502   542 \u2502        \
          \                                                                      \
          \            \u2502\n\u2502   543 \u2502   def __getstate__(self):     \
          \                                                           \u2502\n\u2502\
          \   544 \u2502   \u2502   raise pickle.PicklingError(                  \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14\
          \ in decorate_autocast       \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    11 \u2502   @functools.wraps(func)                         \
          \                                        \u2502\n\u2502    12 \u2502   def\
          \ decorate_autocast(*args, **kwargs):                                  \
          \              \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
          \                                                            \u2502\n\u2502\
          \ \u2771  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)   \
          \                                                \u2502\n\u2502    15 \u2502\
          \   decorate_autocast.__script_unsupported = '@autocast() decorator is not\
          \ supported in    \u2502\n\u2502    16 \u2502   return decorate_autocast\
          \                                                               \u2502\n\
          \u2502    17                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
          \ in forward            \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   550 \u2502   model_forward = ConvertOutputsToFp32(model_forward)\
          \                                    \u2502\n\u2502   551 \u2502       \
          \                                                                      \
          \             \u2502\n\u2502   552 \u2502   def forward(*args, **kwargs):\
          \                                                          \u2502\n\u2502\
          \ \u2771 553 \u2502   \u2502   return model_forward(*args, **kwargs)   \
          \                                           \u2502\n\u2502   554 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   555 \u2502   # To act like a decorator\
          \ so that it can be popped when doing `extract_model_from_pa   \u2502\n\u2502\
          \   556 \u2502   forward.__wrapped__ = model_forward                   \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   538 \u2502   \u2502   update_wrapper(self, model_forward)   \
          \                                             \u2502\n\u2502   539 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   540 \u2502   def __call__(self, *args,\
          \ **kwargs):                                                   \u2502\n\u2502\
          \ \u2771 541 \u2502   \u2502   return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))                        \u2502\n\u2502   542 \u2502        \
          \                                                                      \
          \            \u2502\n\u2502   543 \u2502   def __getstate__(self):     \
          \                                                           \u2502\n\u2502\
          \   544 \u2502   \u2502   raise pickle.PicklingError(                  \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14\
          \ in decorate_autocast       \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    11 \u2502   @functools.wraps(func)                         \
          \                                        \u2502\n\u2502    12 \u2502   def\
          \ decorate_autocast(*args, **kwargs):                                  \
          \              \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
          \                                                            \u2502\n\u2502\
          \ \u2771  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)   \
          \                                                \u2502\n\u2502    15 \u2502\
          \   decorate_autocast.__script_unsupported = '@autocast() decorator is not\
          \ supported in    \u2502\n\u2502    16 \u2502   return decorate_autocast\
          \                                                               \u2502\n\
          \u2502    17                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
          \ in forward            \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   550 \u2502   model_forward = ConvertOutputsToFp32(model_forward)\
          \                                    \u2502\n\u2502   551 \u2502       \
          \                                                                      \
          \             \u2502\n\u2502   552 \u2502   def forward(*args, **kwargs):\
          \                                                          \u2502\n\u2502\
          \ \u2771 553 \u2502   \u2502   return model_forward(*args, **kwargs)   \
          \                                           \u2502\n\u2502   554 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   555 \u2502   # To act like a decorator\
          \ so that it can be popped when doing `extract_model_from_pa   \u2502\n\u2502\
          \   556 \u2502   forward.__wrapped__ = model_forward                   \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   538 \u2502   \u2502   update_wrapper(self, model_forward)   \
          \                                             \u2502\n\u2502   539 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   540 \u2502   def __call__(self, *args,\
          \ **kwargs):                                                   \u2502\n\u2502\
          \ \u2771 541 \u2502   \u2502   return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))                        \u2502\n\u2502   542 \u2502        \
          \                                                                      \
          \            \u2502\n\u2502   543 \u2502   def __getstate__(self):     \
          \                                                           \u2502\n\u2502\
          \   544 \u2502   \u2502   raise pickle.PicklingError(                  \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14\
          \ in decorate_autocast       \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    11 \u2502   @functools.wraps(func)                         \
          \                                        \u2502\n\u2502    12 \u2502   def\
          \ decorate_autocast(*args, **kwargs):                                  \
          \              \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
          \                                                            \u2502\n\u2502\
          \ \u2771  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)   \
          \                                                \u2502\n\u2502    15 \u2502\
          \   decorate_autocast.__script_unsupported = '@autocast() decorator is not\
          \ supported in    \u2502\n\u2502    16 \u2502   return decorate_autocast\
          \                                                               \u2502\n\
          \u2502    17                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
          \ in forward            \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   550 \u2502   model_forward = ConvertOutputsToFp32(model_forward)\
          \                                    \u2502\n\u2502   551 \u2502       \
          \                                                                      \
          \             \u2502\n\u2502   552 \u2502   def forward(*args, **kwargs):\
          \                                                          \u2502\n\u2502\
          \ \u2771 553 \u2502   \u2502   return model_forward(*args, **kwargs)   \
          \                                           \u2502\n\u2502   554 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   555 \u2502   # To act like a decorator\
          \ so that it can be popped when doing `extract_model_from_pa   \u2502\n\u2502\
          \   556 \u2502   forward.__wrapped__ = model_forward                   \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   538 \u2502   \u2502   update_wrapper(self, model_forward)   \
          \                                             \u2502\n\u2502   539 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   540 \u2502   def __call__(self, *args,\
          \ **kwargs):                                                   \u2502\n\u2502\
          \ \u2771 541 \u2502   \u2502   return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))                        \u2502\n\u2502   542 \u2502        \
          \                                                                      \
          \            \u2502\n\u2502   543 \u2502   def __getstate__(self):     \
          \                                                           \u2502\n\u2502\
          \   544 \u2502   \u2502   raise pickle.PicklingError(                  \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14\
          \ in decorate_autocast       \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    11 \u2502   @functools.wraps(func)                         \
          \                                        \u2502\n\u2502    12 \u2502   def\
          \ decorate_autocast(*args, **kwargs):                                  \
          \              \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
          \                                                            \u2502\n\u2502\
          \ \u2771  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)   \
          \                                                \u2502\n\u2502    15 \u2502\
          \   decorate_autocast.__script_unsupported = '@autocast() decorator is not\
          \ supported in    \u2502\n\u2502    16 \u2502   return decorate_autocast\
          \                                                               \u2502\n\
          \u2502    17                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
          \ in forward            \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   550 \u2502   model_forward = ConvertOutputsToFp32(model_forward)\
          \                                    \u2502\n\u2502   551 \u2502       \
          \                                                                      \
          \             \u2502\n\u2502   552 \u2502   def forward(*args, **kwargs):\
          \                                                          \u2502\n\u2502\
          \ \u2771 553 \u2502   \u2502   return model_forward(*args, **kwargs)   \
          \                                           \u2502\n\u2502   554 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   555 \u2502   # To act like a decorator\
          \ so that it can be popped when doing `extract_model_from_pa   \u2502\n\u2502\
          \   556 \u2502   forward.__wrapped__ = model_forward                   \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   538 \u2502   \u2502   update_wrapper(self, model_forward)   \
          \                                             \u2502\n\u2502   539 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   540 \u2502   def __call__(self, *args,\
          \ **kwargs):                                                   \u2502\n\u2502\
          \ \u2771 541 \u2502   \u2502   return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))                        \u2502\n\u2502   542 \u2502        \
          \                                                                      \
          \            \u2502\n\u2502   543 \u2502   def __getstate__(self):     \
          \                                                           \u2502\n\u2502\
          \   544 \u2502   \u2502   raise pickle.PicklingError(                  \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14\
          \ in decorate_autocast       \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    11 \u2502   @functools.wraps(func)                         \
          \                                        \u2502\n\u2502    12 \u2502   def\
          \ decorate_autocast(*args, **kwargs):                                  \
          \              \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
          \                                                            \u2502\n\u2502\
          \ \u2771  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)   \
          \                                                \u2502\n\u2502    15 \u2502\
          \   decorate_autocast.__script_unsupported = '@autocast() decorator is not\
          \ supported in    \u2502\n\u2502    16 \u2502   return decorate_autocast\
          \                                                               \u2502\n\
          \u2502    17                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/peft/peft_model.py:847\
          \ in forward                        \u2502\n\u2502                     \
          \                                                                      \
          \       \u2502\n\u2502    844 \u2502   \u2502   \u2502   if self.base_model.config.model_type\
          \ == \"mpt\":                                \u2502\n\u2502    845 \u2502\
          \   \u2502   \u2502   \u2502   if inputs_embeds is not None:           \
          \                                  \u2502\n\u2502    846 \u2502   \u2502\
          \   \u2502   \u2502   \u2502   raise AssertionError(\"forward in MPTForCausalLM\
          \ does not support inp  \u2502\n\u2502 \u2771  847 \u2502   \u2502   \u2502\
          \   \u2502   return self.base_model(                                   \
          \                \u2502\n\u2502    848 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   input_ids=input_ids,                                      \
          \            \u2502\n\u2502    849 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   attention_mask=attention_mask,                                     \
          \   \u2502\n\u2502    850 \u2502   \u2502   \u2502   \u2502   \u2502   labels=labels,\
          \                                                        \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\
          \ in _call_impl            \u2502\n\u2502                              \
          \                                                                    \u2502\
          \n\u2502   1191 \u2502   \u2502   # this function, and just call forward.\
          \                                           \u2502\n\u2502   1192 \u2502\
          \   \u2502   if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks\
          \ o  \u2502\n\u2502   1193 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
          \ or _global_forward_pre_hooks):                   \u2502\n\u2502 \u2771\
          \ 1194 \u2502   \u2502   \u2502   return forward_call(*input, **kwargs)\
          \                                         \u2502\n\u2502   1195 \u2502 \
          \  \u2502   # Do not call functions when jit is used                   \
          \                       \u2502\n\u2502   1196 \u2502   \u2502   full_backward_hooks,\
          \ non_full_backward_hooks = [], []                             \u2502\n\u2502\
          \   1197 \u2502   \u2502   if self._backward_hooks or _global_backward_hooks:\
          \                                \u2502\n\u2502                        \
          \                                                                      \
          \    \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:165\
          \ in new_forward                   \u2502\n\u2502                      \
          \                                                                      \
          \      \u2502\n\u2502   162 \u2502   \u2502   \u2502   with torch.no_grad():\
          \                                                          \u2502\n\u2502\
          \   163 \u2502   \u2502   \u2502   \u2502   output = old_forward(*args,\
          \ **kwargs)                                      \u2502\n\u2502   164 \u2502\
          \   \u2502   else:                                                     \
          \                         \u2502\n\u2502 \u2771 165 \u2502   \u2502   \u2502\
          \   output = old_forward(*args, **kwargs)                              \
          \            \u2502\n\u2502   166 \u2502   \u2502   return module._hf_hook.post_forward(module,\
          \ output)                                \u2502\n\u2502   167 \u2502   \
          \                                                                      \
          \                 \u2502\n\u2502   168 \u2502   module.forward = new_forward\
          \                                                           \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-instruct/1fc4634127ec64a45\
          \ \u2502\n\u2502 716003578b9cfae23265849/modeling_mpt.py:271 in forward\
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   268 \u2502   \u2502   if inputs_embeds\
          \ is not None:                                                      \u2502\
          \n\u2502   269 \u2502   \u2502   \u2502   raise NotImplementedError('inputs_embeds\
          \ has to be None (for hf/peft support   \u2502\n\u2502   270 \u2502   \u2502\
          \   outputs = self.transformer(input_ids=input_ids, past_key_values=past_key_values,\
          \   \u2502\n\u2502 \u2771 271 \u2502   \u2502   logits = self.transformer.wte(outputs.last_hidden_state.to(self.transformer.wte.\
          \   \u2502\n\u2502   272 \u2502   \u2502   if self.logit_scale is not None:\
          \                                                   \u2502\n\u2502   273\
          \ \u2502   \u2502   \u2502   if self.logit_scale == 0:                 \
          \                                     \u2502\n\u2502   274 \u2502   \u2502\
          \   \u2502   \u2502   warnings.warn(f'Multiplying logits by self.logit_scale={self.logit_scale\
          \   \u2502\n\u2502                                                     \
          \                                             \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\
          \ in _call_impl            \u2502\n\u2502                              \
          \                                                                    \u2502\
          \n\u2502   1191 \u2502   \u2502   # this function, and just call forward.\
          \                                           \u2502\n\u2502   1192 \u2502\
          \   \u2502   if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks\
          \ o  \u2502\n\u2502   1193 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
          \ or _global_forward_pre_hooks):                   \u2502\n\u2502 \u2771\
          \ 1194 \u2502   \u2502   \u2502   return forward_call(*input, **kwargs)\
          \                                         \u2502\n\u2502   1195 \u2502 \
          \  \u2502   # Do not call functions when jit is used                   \
          \                       \u2502\n\u2502   1196 \u2502   \u2502   full_backward_hooks,\
          \ non_full_backward_hooks = [], []                             \u2502\n\u2502\
          \   1197 \u2502   \u2502   if self._backward_hooks or _global_backward_hooks:\
          \                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nTypeError:\
          \ Embedding.forward() takes 2 positional arguments but 3 were given\n```"
        updatedAt: '2023-06-29T08:19:05.888Z'
      numEdits: 1
      reactions: []
    id: 649d3e161e9de7b743870284
    type: comment
  author: NickyNicky
  content: "my code is and error train\n\n```Python\ndef smart_tokenizer_and_embedding_resize(\
    \ # https://github.com/artidoro/qlora/blob/main/qlora.py\n    special_tokens_dict,\n\
    \    tokenizer,\n    model,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n\
    \    Note: This is the unoptimized version that may make your embedding size not\
    \ be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n\
    \    print(\"We have add_special_tokens:\", num_new_tokens, \"tokens\")\n\n  \
    \  old_embeddings= model.get_input_embeddings()\n    print(\"old_embeddings: \"\
    ,old_embeddings.weight.data.shape)\n\n    # model.resize_token_embeddings(len(tokenizer)+num_new_tokens)\
    \ #aqui elimina como 12 embeddings pero es el codigo original\n    model.resize_token_embeddings(old_embeddings.weight.data.shape[0]+num_new_tokens)\
    \ # implementacion mia donde no elimina los embeddings\n\n\n    print(\"model:\
    \ \",model.get_input_embeddings().weight.data.shape)\n\n    if num_new_tokens\
    \ > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n\
    \        output_embeddings = model.get_output_embeddings().weight.data\n\n   \
    \     print(\"input_embeddings:\",input_embeddings.shape)\n        print(\"output_embeddings:\"\
    ,output_embeddings.shape)\n        # print(\"\")\n        # print(\"output_embeddings:\"\
    ,output_embeddings)    \n        # print(\"-->>\",input_embeddings[:-num_new_tokens]).mean(dim=0,\
    \ keepdim=True)\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0,\
    \ keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0,\
    \ keepdim=True)\n\n        print(\"input_embeddings_avg:\",input_embeddings_avg.shape)\n\
    \        print(\"output_embeddings_avg:\",output_embeddings_avg.shape)\n\n   \
    \     input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:]\
    \ = output_embeddings_avg\n    print(\"\")\n    \n    print(model)\n\n# {'text':\
    \ \" Who made you?<|endoftext|>\\n<bot>: I'm h2oGPT, a large language model by\
    \ H2O.ai, the visionary leader in democratizing AI.\\n<|endoftext|>\"}\n\ndic\
    \ = {\n   'additional_special_tokens': ['<human>:','<bot>:']\n}\nsmart_tokenizer_and_embedding_resize(\n\
    \    special_tokens_dict= dic,\n    tokenizer=tokenizer,\n    model=model,\n)\n\
    ```\n\n``` \n########## DATASET\nDataset({\n    features: ['input_ids', 'attention_mask',\
    \ 'labels'],\n    num_rows: 12332\n})\n```\n\n\n```\n########## TRAIN\nfrom torch.optim\
    \ import AdamW\n\n# optimizer = AdamW(model.parameters(), lr=1e-3)\n# Crear el\
    \ optimizador\n# optimizer = AdamW(model.parameters(), lr=1e-4)\n\n\ntraining_args\
    \ = transformers.TrainingArguments(\n    per_device_train_batch_size=6, # es el\
    \ tama\xF1o del lote de entrenamiento por dispositivo.\n    per_device_eval_batch_size=6,\
    \ #  es el tama\xF1o del lote de evaluaci\xF3n por dispositivo.\n\n    gradient_accumulation_steps=3,\n\
    \    logging_steps=4,\n    num_train_epochs=1,\n    learning_rate= 1e-3, # #2e-8,\
    \ 3e-5, 1e-5, 2e-5, (1e-3), 1e-2\n\n    # fp16=False,\n    # # fp16=True, #True,\n\
    \    # auto_find_batch_size=False,     # Desactivar la b\xFAsqueda autom\xE1tica\
    \ del tama\xF1o de lote\n\n    fp16=True,\n    auto_find_batch_size=True,    \
    \ # Desactivar la b\xFAsqueda autom\xE1tica del tama\xF1o de lote\n\n    save_total_limit=4,\n\
    \    output_dir=\"./outputs\",\n\n    optim=\"adamw_torch\",\n\n    lr_scheduler_type\
    \ = 'cosine',\n    warmup_ratio = 0.03, #0.05,\n)\n\ntrainer = transformers.Trainer(\n\
    \    model=model,\n    train_dataset=lm_dataset,\n    # eval_dataset=val_dataset,\n\
    \    args=training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,\
    \ mlm=False),\n)\n\n\n# Configurar el optimizador en el objeto Trainer\n# trainer.optimizer\
    \ = optimizer\n\ntrainer.train()\n```\n\n``` \n########## Error\n\u256D\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u256E\n\u2502 in <cell line: 49>:49                                   \
    \                                         \u2502\n\u2502                     \
    \                                                                            \
    \ \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1645\
    \ in train                    \u2502\n\u2502                                 \
    \                                                                 \u2502\n\u2502\
    \   1642 \u2502   \u2502   inner_training_loop = find_executable_batch_size( \
    \                                \u2502\n\u2502   1643 \u2502   \u2502   \u2502\
    \   self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size\
    \  \u2502\n\u2502   1644 \u2502   \u2502   )                                 \
    \                                                \u2502\n\u2502 \u2771 1645 \u2502\
    \   \u2502   return inner_training_loop(                                     \
    \                  \u2502\n\u2502   1646 \u2502   \u2502   \u2502   args=args,\
    \                                                                    \u2502\n\u2502\
    \   1647 \u2502   \u2502   \u2502   resume_from_checkpoint=resume_from_checkpoint,\
    \                                \u2502\n\u2502   1648 \u2502   \u2502   \u2502\
    \   trial=trial,                                                             \
    \     \u2502\n\u2502                                                         \
    \                                         \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/memory.py:132\
    \ in decorator              \u2502\n\u2502                                   \
    \                                                               \u2502\n\u2502\
    \   129 \u2502   \u2502   \u2502   if batch_size == 0:                       \
    \                                     \u2502\n\u2502   130 \u2502   \u2502   \u2502\
    \   \u2502   raise RuntimeError(\"No executable batch size found, reached zero.\"\
    )        \u2502\n\u2502   131 \u2502   \u2502   \u2502   try:                \
    \                                                           \u2502\n\u2502 \u2771\
    \ 132 \u2502   \u2502   \u2502   \u2502   return function(batch_size, *args, **kwargs)\
    \                               \u2502\n\u2502   133 \u2502   \u2502   \u2502\
    \   except Exception as e:                                                   \
    \      \u2502\n\u2502   134 \u2502   \u2502   \u2502   \u2502   if should_reduce_batch_size(e):\
    \                                            \u2502\n\u2502   135 \u2502   \u2502\
    \   \u2502   \u2502   \u2502   gc.collect()                                  \
    \                         \u2502\n\u2502                                     \
    \                                                             \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1938\
    \ in _inner_training_loop     \u2502\n\u2502                                 \
    \                                                                 \u2502\n\u2502\
    \   1935 \u2502   \u2502   \u2502   \u2502   \u2502   self.control = self.callback_handler.on_step_begin(args,\
    \ self.state,  \u2502\n\u2502   1936 \u2502   \u2502   \u2502   \u2502       \
    \                                                                      \u2502\n\
    \u2502   1937 \u2502   \u2502   \u2502   \u2502   with self.accelerator.accumulate(model):\
    \                                  \u2502\n\u2502 \u2771 1938 \u2502   \u2502\
    \   \u2502   \u2502   \u2502   tr_loss_step = self.training_step(model, inputs)\
    \                      \u2502\n\u2502   1939 \u2502   \u2502   \u2502   \u2502\
    \                                                                            \
    \ \u2502\n\u2502   1940 \u2502   \u2502   \u2502   \u2502   if (             \
    \                                                         \u2502\n\u2502   1941\
    \ \u2502   \u2502   \u2502   \u2502   \u2502   args.logging_nan_inf_filter   \
    \                                        \u2502\n\u2502                      \
    \                                                                            \u2502\
    \n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2759\
    \ in training_step            \u2502\n\u2502                                 \
    \                                                                 \u2502\n\u2502\
    \   2756 \u2502   \u2502   \u2502   return loss_mb.reduce_mean().detach().to(self.args.device)\
    \                    \u2502\n\u2502   2757 \u2502   \u2502                   \
    \                                                                  \u2502\n\u2502\
    \   2758 \u2502   \u2502   with self.compute_loss_context_manager():         \
    \                                \u2502\n\u2502 \u2771 2759 \u2502   \u2502  \
    \ \u2502   loss = self.compute_loss(model, inputs)                           \
    \            \u2502\n\u2502   2760 \u2502   \u2502                           \
    \                                                          \u2502\n\u2502   2761\
    \ \u2502   \u2502   if self.args.n_gpu > 1:                                  \
    \                         \u2502\n\u2502   2762 \u2502   \u2502   \u2502   loss\
    \ = loss.mean()  # mean() to average on multi-gpu parallel training        \u2502\
    \n\u2502                                                                     \
    \                             \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2784\
    \ in compute_loss             \u2502\n\u2502                                 \
    \                                                                 \u2502\n\u2502\
    \   2781 \u2502   \u2502   \u2502   labels = inputs.pop(\"labels\")          \
    \                                       \u2502\n\u2502   2782 \u2502   \u2502\
    \   else:                                                                    \
    \         \u2502\n\u2502   2783 \u2502   \u2502   \u2502   labels = None     \
    \                                                            \u2502\n\u2502 \u2771\
    \ 2784 \u2502   \u2502   outputs = model(**inputs)                           \
    \                              \u2502\n\u2502   2785 \u2502   \u2502   # Save\
    \ past state if it exists                                                    \u2502\
    \n\u2502   2786 \u2502   \u2502   # TODO: this needs to be fixed and made cleaner\
    \ later.                            \u2502\n\u2502   2787 \u2502   \u2502   if\
    \ self.args.past_index >= 0:                                                 \
    \    \u2502\n\u2502                                                          \
    \                                        \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\
    \ in _call_impl            \u2502\n\u2502                                    \
    \                                                              \u2502\n\u2502\
    \   1191 \u2502   \u2502   # this function, and just call forward.           \
    \                                \u2502\n\u2502   1192 \u2502   \u2502   if not\
    \ (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks o  \u2502\
    \n\u2502   1193 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or\
    \ _global_forward_pre_hooks):                   \u2502\n\u2502 \u2771 1194 \u2502\
    \   \u2502   \u2502   return forward_call(*input, **kwargs)                  \
    \                       \u2502\n\u2502   1195 \u2502   \u2502   # Do not call\
    \ functions when jit is used                                          \u2502\n\
    \u2502   1196 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks =\
    \ [], []                             \u2502\n\u2502   1197 \u2502   \u2502   if\
    \ self._backward_hooks or _global_backward_hooks:                            \
    \    \u2502\n\u2502                                                          \
    \                                        \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
    \ in forward            \u2502\n\u2502                                       \
    \                                                           \u2502\n\u2502   550\
    \ \u2502   model_forward = ConvertOutputsToFp32(model_forward)               \
    \                     \u2502\n\u2502   551 \u2502                            \
    \                                                              \u2502\n\u2502\
    \   552 \u2502   def forward(*args, **kwargs):                               \
    \                           \u2502\n\u2502 \u2771 553 \u2502   \u2502   return\
    \ model_forward(*args, **kwargs)                                             \
    \ \u2502\n\u2502   554 \u2502                                                \
    \                                          \u2502\n\u2502   555 \u2502   # To\
    \ act like a decorator so that it can be popped when doing `extract_model_from_pa\
    \   \u2502\n\u2502   556 \u2502   forward.__wrapped__ = model_forward        \
    \                                            \u2502\n\u2502                  \
    \                                                                            \
    \    \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
    \ in __call__           \u2502\n\u2502                                       \
    \                                                           \u2502\n\u2502   538\
    \ \u2502   \u2502   update_wrapper(self, model_forward)                      \
    \                          \u2502\n\u2502   539 \u2502                       \
    \                                                                   \u2502\n\u2502\
    \   540 \u2502   def __call__(self, *args, **kwargs):                        \
    \                           \u2502\n\u2502 \u2771 541 \u2502   \u2502   return\
    \ convert_to_fp32(self.model_forward(*args, **kwargs))                       \
    \ \u2502\n\u2502   542 \u2502                                                \
    \                                          \u2502\n\u2502   543 \u2502   def __getstate__(self):\
    \                                                                \u2502\n\u2502\
    \   544 \u2502   \u2502   raise pickle.PicklingError(                        \
    \                                \u2502\n\u2502                              \
    \                                                                    \u2502\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14 in decorate_autocast\
    \       \u2502\n\u2502                                                       \
    \                                           \u2502\n\u2502    11 \u2502   @functools.wraps(func)\
    \                                                                 \u2502\n\u2502\
    \    12 \u2502   def decorate_autocast(*args, **kwargs):                     \
    \                           \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
    \                                                            \u2502\n\u2502 \u2771\
    \  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)                \
    \                                   \u2502\n\u2502    15 \u2502   decorate_autocast.__script_unsupported\
    \ = '@autocast() decorator is not supported in    \u2502\n\u2502    16 \u2502\
    \   return decorate_autocast                                                 \
    \              \u2502\n\u2502    17                                          \
    \                                                  \u2502\n\u2502            \
    \                                                                            \
    \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
    \ in forward            \u2502\n\u2502                                       \
    \                                                           \u2502\n\u2502   550\
    \ \u2502   model_forward = ConvertOutputsToFp32(model_forward)               \
    \                     \u2502\n\u2502   551 \u2502                            \
    \                                                              \u2502\n\u2502\
    \   552 \u2502   def forward(*args, **kwargs):                               \
    \                           \u2502\n\u2502 \u2771 553 \u2502   \u2502   return\
    \ model_forward(*args, **kwargs)                                             \
    \ \u2502\n\u2502   554 \u2502                                                \
    \                                          \u2502\n\u2502   555 \u2502   # To\
    \ act like a decorator so that it can be popped when doing `extract_model_from_pa\
    \   \u2502\n\u2502   556 \u2502   forward.__wrapped__ = model_forward        \
    \                                            \u2502\n\u2502                  \
    \                                                                            \
    \    \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
    \ in __call__           \u2502\n\u2502                                       \
    \                                                           \u2502\n\u2502   538\
    \ \u2502   \u2502   update_wrapper(self, model_forward)                      \
    \                          \u2502\n\u2502   539 \u2502                       \
    \                                                                   \u2502\n\u2502\
    \   540 \u2502   def __call__(self, *args, **kwargs):                        \
    \                           \u2502\n\u2502 \u2771 541 \u2502   \u2502   return\
    \ convert_to_fp32(self.model_forward(*args, **kwargs))                       \
    \ \u2502\n\u2502   542 \u2502                                                \
    \                                          \u2502\n\u2502   543 \u2502   def __getstate__(self):\
    \                                                                \u2502\n\u2502\
    \   544 \u2502   \u2502   raise pickle.PicklingError(                        \
    \                                \u2502\n\u2502                              \
    \                                                                    \u2502\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14 in decorate_autocast\
    \       \u2502\n\u2502                                                       \
    \                                           \u2502\n\u2502    11 \u2502   @functools.wraps(func)\
    \                                                                 \u2502\n\u2502\
    \    12 \u2502   def decorate_autocast(*args, **kwargs):                     \
    \                           \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
    \                                                            \u2502\n\u2502 \u2771\
    \  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)                \
    \                                   \u2502\n\u2502    15 \u2502   decorate_autocast.__script_unsupported\
    \ = '@autocast() decorator is not supported in    \u2502\n\u2502    16 \u2502\
    \   return decorate_autocast                                                 \
    \              \u2502\n\u2502    17                                          \
    \                                                  \u2502\n\u2502            \
    \                                                                            \
    \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
    \ in forward            \u2502\n\u2502                                       \
    \                                                           \u2502\n\u2502   550\
    \ \u2502   model_forward = ConvertOutputsToFp32(model_forward)               \
    \                     \u2502\n\u2502   551 \u2502                            \
    \                                                              \u2502\n\u2502\
    \   552 \u2502   def forward(*args, **kwargs):                               \
    \                           \u2502\n\u2502 \u2771 553 \u2502   \u2502   return\
    \ model_forward(*args, **kwargs)                                             \
    \ \u2502\n\u2502   554 \u2502                                                \
    \                                          \u2502\n\u2502   555 \u2502   # To\
    \ act like a decorator so that it can be popped when doing `extract_model_from_pa\
    \   \u2502\n\u2502   556 \u2502   forward.__wrapped__ = model_forward        \
    \                                            \u2502\n\u2502                  \
    \                                                                            \
    \    \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
    \ in __call__           \u2502\n\u2502                                       \
    \                                                           \u2502\n\u2502   538\
    \ \u2502   \u2502   update_wrapper(self, model_forward)                      \
    \                          \u2502\n\u2502   539 \u2502                       \
    \                                                                   \u2502\n\u2502\
    \   540 \u2502   def __call__(self, *args, **kwargs):                        \
    \                           \u2502\n\u2502 \u2771 541 \u2502   \u2502   return\
    \ convert_to_fp32(self.model_forward(*args, **kwargs))                       \
    \ \u2502\n\u2502   542 \u2502                                                \
    \                                          \u2502\n\u2502   543 \u2502   def __getstate__(self):\
    \                                                                \u2502\n\u2502\
    \   544 \u2502   \u2502   raise pickle.PicklingError(                        \
    \                                \u2502\n\u2502                              \
    \                                                                    \u2502\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14 in decorate_autocast\
    \       \u2502\n\u2502                                                       \
    \                                           \u2502\n\u2502    11 \u2502   @functools.wraps(func)\
    \                                                                 \u2502\n\u2502\
    \    12 \u2502   def decorate_autocast(*args, **kwargs):                     \
    \                           \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
    \                                                            \u2502\n\u2502 \u2771\
    \  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)                \
    \                                   \u2502\n\u2502    15 \u2502   decorate_autocast.__script_unsupported\
    \ = '@autocast() decorator is not supported in    \u2502\n\u2502    16 \u2502\
    \   return decorate_autocast                                                 \
    \              \u2502\n\u2502    17                                          \
    \                                                  \u2502\n\u2502            \
    \                                                                            \
    \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
    \ in forward            \u2502\n\u2502                                       \
    \                                                           \u2502\n\u2502   550\
    \ \u2502   model_forward = ConvertOutputsToFp32(model_forward)               \
    \                     \u2502\n\u2502   551 \u2502                            \
    \                                                              \u2502\n\u2502\
    \   552 \u2502   def forward(*args, **kwargs):                               \
    \                           \u2502\n\u2502 \u2771 553 \u2502   \u2502   return\
    \ model_forward(*args, **kwargs)                                             \
    \ \u2502\n\u2502   554 \u2502                                                \
    \                                          \u2502\n\u2502   555 \u2502   # To\
    \ act like a decorator so that it can be popped when doing `extract_model_from_pa\
    \   \u2502\n\u2502   556 \u2502   forward.__wrapped__ = model_forward        \
    \                                            \u2502\n\u2502                  \
    \                                                                            \
    \    \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
    \ in __call__           \u2502\n\u2502                                       \
    \                                                           \u2502\n\u2502   538\
    \ \u2502   \u2502   update_wrapper(self, model_forward)                      \
    \                          \u2502\n\u2502   539 \u2502                       \
    \                                                                   \u2502\n\u2502\
    \   540 \u2502   def __call__(self, *args, **kwargs):                        \
    \                           \u2502\n\u2502 \u2771 541 \u2502   \u2502   return\
    \ convert_to_fp32(self.model_forward(*args, **kwargs))                       \
    \ \u2502\n\u2502   542 \u2502                                                \
    \                                          \u2502\n\u2502   543 \u2502   def __getstate__(self):\
    \                                                                \u2502\n\u2502\
    \   544 \u2502   \u2502   raise pickle.PicklingError(                        \
    \                                \u2502\n\u2502                              \
    \                                                                    \u2502\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14 in decorate_autocast\
    \       \u2502\n\u2502                                                       \
    \                                           \u2502\n\u2502    11 \u2502   @functools.wraps(func)\
    \                                                                 \u2502\n\u2502\
    \    12 \u2502   def decorate_autocast(*args, **kwargs):                     \
    \                           \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
    \                                                            \u2502\n\u2502 \u2771\
    \  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)                \
    \                                   \u2502\n\u2502    15 \u2502   decorate_autocast.__script_unsupported\
    \ = '@autocast() decorator is not supported in    \u2502\n\u2502    16 \u2502\
    \   return decorate_autocast                                                 \
    \              \u2502\n\u2502    17                                          \
    \                                                  \u2502\n\u2502            \
    \                                                                            \
    \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
    \ in forward            \u2502\n\u2502                                       \
    \                                                           \u2502\n\u2502   550\
    \ \u2502   model_forward = ConvertOutputsToFp32(model_forward)               \
    \                     \u2502\n\u2502   551 \u2502                            \
    \                                                              \u2502\n\u2502\
    \   552 \u2502   def forward(*args, **kwargs):                               \
    \                           \u2502\n\u2502 \u2771 553 \u2502   \u2502   return\
    \ model_forward(*args, **kwargs)                                             \
    \ \u2502\n\u2502   554 \u2502                                                \
    \                                          \u2502\n\u2502   555 \u2502   # To\
    \ act like a decorator so that it can be popped when doing `extract_model_from_pa\
    \   \u2502\n\u2502   556 \u2502   forward.__wrapped__ = model_forward        \
    \                                            \u2502\n\u2502                  \
    \                                                                            \
    \    \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
    \ in __call__           \u2502\n\u2502                                       \
    \                                                           \u2502\n\u2502   538\
    \ \u2502   \u2502   update_wrapper(self, model_forward)                      \
    \                          \u2502\n\u2502   539 \u2502                       \
    \                                                                   \u2502\n\u2502\
    \   540 \u2502   def __call__(self, *args, **kwargs):                        \
    \                           \u2502\n\u2502 \u2771 541 \u2502   \u2502   return\
    \ convert_to_fp32(self.model_forward(*args, **kwargs))                       \
    \ \u2502\n\u2502   542 \u2502                                                \
    \                                          \u2502\n\u2502   543 \u2502   def __getstate__(self):\
    \                                                                \u2502\n\u2502\
    \   544 \u2502   \u2502   raise pickle.PicklingError(                        \
    \                                \u2502\n\u2502                              \
    \                                                                    \u2502\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14 in decorate_autocast\
    \       \u2502\n\u2502                                                       \
    \                                           \u2502\n\u2502    11 \u2502   @functools.wraps(func)\
    \                                                                 \u2502\n\u2502\
    \    12 \u2502   def decorate_autocast(*args, **kwargs):                     \
    \                           \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
    \                                                            \u2502\n\u2502 \u2771\
    \  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)                \
    \                                   \u2502\n\u2502    15 \u2502   decorate_autocast.__script_unsupported\
    \ = '@autocast() decorator is not supported in    \u2502\n\u2502    16 \u2502\
    \   return decorate_autocast                                                 \
    \              \u2502\n\u2502    17                                          \
    \                                                  \u2502\n\u2502            \
    \                                                                            \
    \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:553\
    \ in forward            \u2502\n\u2502                                       \
    \                                                           \u2502\n\u2502   550\
    \ \u2502   model_forward = ConvertOutputsToFp32(model_forward)               \
    \                     \u2502\n\u2502   551 \u2502                            \
    \                                                              \u2502\n\u2502\
    \   552 \u2502   def forward(*args, **kwargs):                               \
    \                           \u2502\n\u2502 \u2771 553 \u2502   \u2502   return\
    \ model_forward(*args, **kwargs)                                             \
    \ \u2502\n\u2502   554 \u2502                                                \
    \                                          \u2502\n\u2502   555 \u2502   # To\
    \ act like a decorator so that it can be popped when doing `extract_model_from_pa\
    \   \u2502\n\u2502   556 \u2502   forward.__wrapped__ = model_forward        \
    \                                            \u2502\n\u2502                  \
    \                                                                            \
    \    \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:541\
    \ in __call__           \u2502\n\u2502                                       \
    \                                                           \u2502\n\u2502   538\
    \ \u2502   \u2502   update_wrapper(self, model_forward)                      \
    \                          \u2502\n\u2502   539 \u2502                       \
    \                                                                   \u2502\n\u2502\
    \   540 \u2502   def __call__(self, *args, **kwargs):                        \
    \                           \u2502\n\u2502 \u2771 541 \u2502   \u2502   return\
    \ convert_to_fp32(self.model_forward(*args, **kwargs))                       \
    \ \u2502\n\u2502   542 \u2502                                                \
    \                                          \u2502\n\u2502   543 \u2502   def __getstate__(self):\
    \                                                                \u2502\n\u2502\
    \   544 \u2502   \u2502   raise pickle.PicklingError(                        \
    \                                \u2502\n\u2502                              \
    \                                                                    \u2502\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:14 in decorate_autocast\
    \       \u2502\n\u2502                                                       \
    \                                           \u2502\n\u2502    11 \u2502   @functools.wraps(func)\
    \                                                                 \u2502\n\u2502\
    \    12 \u2502   def decorate_autocast(*args, **kwargs):                     \
    \                           \u2502\n\u2502    13 \u2502   \u2502   with autocast_instance:\
    \                                                            \u2502\n\u2502 \u2771\
    \  14 \u2502   \u2502   \u2502   return func(*args, **kwargs)                \
    \                                   \u2502\n\u2502    15 \u2502   decorate_autocast.__script_unsupported\
    \ = '@autocast() decorator is not supported in    \u2502\n\u2502    16 \u2502\
    \   return decorate_autocast                                                 \
    \              \u2502\n\u2502    17                                          \
    \                                                  \u2502\n\u2502            \
    \                                                                            \
    \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/peft/peft_model.py:847\
    \ in forward                        \u2502\n\u2502                           \
    \                                                                       \u2502\
    \n\u2502    844 \u2502   \u2502   \u2502   if self.base_model.config.model_type\
    \ == \"mpt\":                                \u2502\n\u2502    845 \u2502   \u2502\
    \   \u2502   \u2502   if inputs_embeds is not None:                          \
    \                   \u2502\n\u2502    846 \u2502   \u2502   \u2502   \u2502  \
    \ \u2502   raise AssertionError(\"forward in MPTForCausalLM does not support inp\
    \  \u2502\n\u2502 \u2771  847 \u2502   \u2502   \u2502   \u2502   return self.base_model(\
    \                                                   \u2502\n\u2502    848 \u2502\
    \   \u2502   \u2502   \u2502   \u2502   input_ids=input_ids,                 \
    \                                 \u2502\n\u2502    849 \u2502   \u2502   \u2502\
    \   \u2502   \u2502   attention_mask=attention_mask,                         \
    \               \u2502\n\u2502    850 \u2502   \u2502   \u2502   \u2502   \u2502\
    \   labels=labels,                                                        \u2502\
    \n\u2502                                                                     \
    \                             \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\
    \ in _call_impl            \u2502\n\u2502                                    \
    \                                                              \u2502\n\u2502\
    \   1191 \u2502   \u2502   # this function, and just call forward.           \
    \                                \u2502\n\u2502   1192 \u2502   \u2502   if not\
    \ (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks o  \u2502\
    \n\u2502   1193 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or\
    \ _global_forward_pre_hooks):                   \u2502\n\u2502 \u2771 1194 \u2502\
    \   \u2502   \u2502   return forward_call(*input, **kwargs)                  \
    \                       \u2502\n\u2502   1195 \u2502   \u2502   # Do not call\
    \ functions when jit is used                                          \u2502\n\
    \u2502   1196 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks =\
    \ [], []                             \u2502\n\u2502   1197 \u2502   \u2502   if\
    \ self._backward_hooks or _global_backward_hooks:                            \
    \    \u2502\n\u2502                                                          \
    \                                        \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:165\
    \ in new_forward                   \u2502\n\u2502                            \
    \                                                                      \u2502\n\
    \u2502   162 \u2502   \u2502   \u2502   with torch.no_grad():                \
    \                                          \u2502\n\u2502   163 \u2502   \u2502\
    \   \u2502   \u2502   output = old_forward(*args, **kwargs)                  \
    \                    \u2502\n\u2502   164 \u2502   \u2502   else:            \
    \                                                                  \u2502\n\u2502\
    \ \u2771 165 \u2502   \u2502   \u2502   output = old_forward(*args, **kwargs)\
    \                                          \u2502\n\u2502   166 \u2502   \u2502\
    \   return module._hf_hook.post_forward(module, output)                      \
    \          \u2502\n\u2502   167 \u2502                                       \
    \                                                   \u2502\n\u2502   168 \u2502\
    \   module.forward = new_forward                                             \
    \              \u2502\n\u2502                                                \
    \                                                  \u2502\n\u2502 /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-instruct/1fc4634127ec64a45\
    \ \u2502\n\u2502 716003578b9cfae23265849/modeling_mpt.py:271 in forward      \
    \                                     \u2502\n\u2502                         \
    \                                                                         \u2502\
    \n\u2502   268 \u2502   \u2502   if inputs_embeds is not None:               \
    \                                       \u2502\n\u2502   269 \u2502   \u2502 \
    \  \u2502   raise NotImplementedError('inputs_embeds has to be None (for hf/peft\
    \ support   \u2502\n\u2502   270 \u2502   \u2502   outputs = self.transformer(input_ids=input_ids,\
    \ past_key_values=past_key_values,   \u2502\n\u2502 \u2771 271 \u2502   \u2502\
    \   logits = self.transformer.wte(outputs.last_hidden_state.to(self.transformer.wte.\
    \   \u2502\n\u2502   272 \u2502   \u2502   if self.logit_scale is not None:  \
    \                                                 \u2502\n\u2502   273 \u2502\
    \   \u2502   \u2502   if self.logit_scale == 0:                              \
    \                        \u2502\n\u2502   274 \u2502   \u2502   \u2502   \u2502\
    \   warnings.warn(f'Multiplying logits by self.logit_scale={self.logit_scale \
    \  \u2502\n\u2502                                                            \
    \                                      \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\
    \ in _call_impl            \u2502\n\u2502                                    \
    \                                                              \u2502\n\u2502\
    \   1191 \u2502   \u2502   # this function, and just call forward.           \
    \                                \u2502\n\u2502   1192 \u2502   \u2502   if not\
    \ (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks o  \u2502\
    \n\u2502   1193 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or\
    \ _global_forward_pre_hooks):                   \u2502\n\u2502 \u2771 1194 \u2502\
    \   \u2502   \u2502   return forward_call(*input, **kwargs)                  \
    \                       \u2502\n\u2502   1195 \u2502   \u2502   # Do not call\
    \ functions when jit is used                                          \u2502\n\
    \u2502   1196 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks =\
    \ [], []                             \u2502\n\u2502   1197 \u2502   \u2502   if\
    \ self._backward_hooks or _global_backward_hooks:                            \
    \    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nTypeError:\
    \ Embedding.forward() takes 2 positional arguments but 3 were given\n```"
  created_at: 2023-06-29 07:17:26+00:00
  edited: true
  hidden: false
  id: 649d3e161e9de7b743870284
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4a792ce1ca07201986be6a4ee91b2bb.svg
      fullname: Vic Y Lin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yucyuanLin
      type: user
    createdAt: '2023-07-21T06:47:26.000Z'
    data:
      edited: false
      editors:
      - yucyuanLin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8083646297454834
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4a792ce1ca07201986be6a4ee91b2bb.svg
          fullname: Vic Y Lin
          isHf: false
          isPro: false
          name: yucyuanLin
          type: user
        html: '<p>HI, I got the same error</p>

          '
        raw: HI, I got the same error
        updatedAt: '2023-07-21T06:47:26.121Z'
      numEdits: 0
      reactions: []
    id: 64ba29fe2ab555ea5637a67b
    type: comment
  author: yucyuanLin
  content: HI, I got the same error
  created_at: 2023-07-21 05:47:26+00:00
  edited: false
  hidden: false
  id: 64ba29fe2ab555ea5637a67b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/190d29e49724b08c41f704b0577cd1bb.svg
      fullname: Zhida Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lziiid
      type: user
    createdAt: '2023-08-21T07:43:36.000Z'
    data:
      edited: false
      editors:
      - lziiid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8816541433334351
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/190d29e49724b08c41f704b0577cd1bb.svg
          fullname: Zhida Li
          isHf: false
          isPro: false
          name: lziiid
          type: user
        html: '<p>I got the same error too :(</p>

          '
        raw: I got the same error too :(
        updatedAt: '2023-08-21T07:43:36.040Z'
      numEdits: 0
      reactions: []
    id: 64e315a8f8d8389c1a935695
    type: comment
  author: lziiid
  content: I got the same error too :(
  created_at: 2023-08-21 06:43:36+00:00
  edited: false
  hidden: false
  id: 64e315a8f8d8389c1a935695
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 70
repo_id: mosaicml/mpt-7b
repo_type: model
status: open
target_branch: null
title: How to append new token and train?
