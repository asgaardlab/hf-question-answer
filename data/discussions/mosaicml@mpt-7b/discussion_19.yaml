!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JacopoBandoni
conflicting_files: null
created_at: 2023-05-12 09:16:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c5d990463eedcfcc1c07be863a5219d.svg
      fullname: Jacopo Bandoni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JacopoBandoni
      type: user
    createdAt: '2023-05-12T10:16:52.000Z'
    data:
      edited: false
      editors:
      - JacopoBandoni
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c5d990463eedcfcc1c07be863a5219d.svg
          fullname: Jacopo Bandoni
          isHf: false
          isPro: false
          name: JacopoBandoni
          type: user
        html: '<p>"self.transformer.wte" is an embedding layer but it is also used
          as a language modelling head in the MTPForCausalLM class by passing the
          "self.transformer.wte.weight" matrix to the F.linear function in the forward
          function.</p>

          <p>Am I missing something? Shouldn''t the language head be separately learned
          ?</p>

          '
        raw: "\"self.transformer.wte\" is an embedding layer but it is also used as\
          \ a language modelling head in the MTPForCausalLM class by passing the \"\
          self.transformer.wte.weight\" matrix to the F.linear function in the forward\
          \ function.\r\n\r\nAm I missing something? Shouldn't the language head be\
          \ separately learned ?"
        updatedAt: '2023-05-12T10:16:52.225Z'
      numEdits: 0
      reactions: []
    id: 645e12149f0c30b01621c677
    type: comment
  author: JacopoBandoni
  content: "\"self.transformer.wte\" is an embedding layer but it is also used as\
    \ a language modelling head in the MTPForCausalLM class by passing the \"self.transformer.wte.weight\"\
    \ matrix to the F.linear function in the forward function.\r\n\r\nAm I missing\
    \ something? Shouldn't the language head be separately learned ?"
  created_at: 2023-05-12 09:16:52+00:00
  edited: false
  hidden: false
  id: 645e12149f0c30b01621c677
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2023-05-16T13:03:43.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: '<p>It''s very often exactly the same weights. Just like in GPT2, Bloom,
          ...</p>

          '
        raw: It's very often exactly the same weights. Just like in GPT2, Bloom, ...
        updatedAt: '2023-05-16T13:03:43.343Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - JacopoBandoni
    id: 64637f2fa429ec3af0a5a60f
    type: comment
  author: patrickvonplaten
  content: It's very often exactly the same weights. Just like in GPT2, Bloom, ...
  created_at: 2023-05-16 12:03:43+00:00
  edited: false
  hidden: false
  id: 64637f2fa429ec3af0a5a60f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-16T15:06:46.000Z'
    data:
      edited: true
      editors:
      - abhi-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;JacopoBandoni&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/JacopoBandoni\"\
          >@<span class=\"underline\">JacopoBandoni</span></a></span>\n\n\t</span></span>,\
          \ for MPT we use weight tying, which shares the word embedding weights with\
          \ the final LM head. It is used by default in most HF causal language models,\
          \ you can see the codepath here: <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/130e15429116689c9d747be2cdd8c4be7bb7e2bd/src/transformers/modeling_utils.py#L1245-L1264\"\
          >https://github.com/huggingface/transformers/blob/130e15429116689c9d747be2cdd8c4be7bb7e2bd/src/transformers/modeling_utils.py#L1245-L1264</a></p>\n\
          <p>For MPT, to make the model easier to deal with for meta initialization\
          \ and FSDP, we directly use the <code>self.transformer.wte.weight</code>as\
          \ the LM head rather than create a separate nn.Linear module and tie the\
          \ weights.</p>\n"
        raw: 'Hi @JacopoBandoni, for MPT we use weight tying, which shares the word
          embedding weights with the final LM head. It is used by default in most
          HF causal language models, you can see the codepath here: https://github.com/huggingface/transformers/blob/130e15429116689c9d747be2cdd8c4be7bb7e2bd/src/transformers/modeling_utils.py#L1245-L1264


          For MPT, to make the model easier to deal with for meta initialization and
          FSDP, we directly use the `self.transformer.wte.weight`as the LM head rather
          than create a separate nn.Linear module and tie the weights.'
        updatedAt: '2023-05-16T15:06:56.088Z'
      numEdits: 1
      reactions: []
      relatedEventId: 64639c0632317fa4806a5bab
    id: 64639c0632317fa4806a5baa
    type: comment
  author: abhi-mosaic
  content: 'Hi @JacopoBandoni, for MPT we use weight tying, which shares the word
    embedding weights with the final LM head. It is used by default in most HF causal
    language models, you can see the codepath here: https://github.com/huggingface/transformers/blob/130e15429116689c9d747be2cdd8c4be7bb7e2bd/src/transformers/modeling_utils.py#L1245-L1264


    For MPT, to make the model easier to deal with for meta initialization and FSDP,
    we directly use the `self.transformer.wte.weight`as the LM head rather than create
    a separate nn.Linear module and tie the weights.'
  created_at: 2023-05-16 14:06:46+00:00
  edited: true
  hidden: false
  id: 64639c0632317fa4806a5baa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-16T15:06:46.000Z'
    data:
      status: closed
    id: 64639c0632317fa4806a5bab
    type: status-change
  author: abhi-mosaic
  created_at: 2023-05-16 14:06:46+00:00
  id: 64639c0632317fa4806a5bab
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: mosaicml/mpt-7b
repo_type: model
status: closed
target_branch: null
title: PyTorch model architecture doubt
