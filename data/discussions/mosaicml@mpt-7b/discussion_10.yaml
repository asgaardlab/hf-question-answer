!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cekal
conflicting_files: null
created_at: 2023-05-07 16:03:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
      fullname: Vojtech Cekal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: cekal
      type: user
    createdAt: '2023-05-07T17:03:35.000Z'
    data:
      edited: false
      editors:
      - cekal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
          fullname: Vojtech Cekal
          isHf: false
          isPro: true
          name: cekal
          type: user
        html: '<p>Hi, I''m trying to train this model using LoRA, but I''m unable
          to find the query_key_value for this model. For LLaMA, it was v_proj and
          q_proj, but I''m not sure about this one. I''m using the finetune.py script
          from <a rel="nofollow" href="https://github.com/leehanchung/mpt-lora">https://github.com/leehanchung/mpt-lora</a>
          where there is "query_key_value" and "xxx" but that also yields the "ValueError:
          Target modules [''query_key_value'', ''xxx''] not found in the base model.
          Please check the target modules and try again." error.</p>

          <p>If anyone knows what they are or how to find them, please let me know.</p>

          '
        raw: "Hi, I'm trying to train this model using LoRA, but I'm unable to find\
          \ the query_key_value for this model. For LLaMA, it was v_proj and q_proj,\
          \ but I'm not sure about this one. I'm using the finetune.py script from\
          \ https://github.com/leehanchung/mpt-lora where there is \"query_key_value\"\
          \ and \"xxx\" but that also yields the \"ValueError: Target modules ['query_key_value',\
          \ 'xxx'] not found in the base model. Please check the target modules and\
          \ try again.\" error.\r\n\r\nIf anyone knows what they are or how to find\
          \ them, please let me know."
        updatedAt: '2023-05-07T17:03:35.919Z'
      numEdits: 0
      reactions: []
    id: 6457d9e7ead43697df29ca10
    type: comment
  author: cekal
  content: "Hi, I'm trying to train this model using LoRA, but I'm unable to find\
    \ the query_key_value for this model. For LLaMA, it was v_proj and q_proj, but\
    \ I'm not sure about this one. I'm using the finetune.py script from https://github.com/leehanchung/mpt-lora\
    \ where there is \"query_key_value\" and \"xxx\" but that also yields the \"ValueError:\
    \ Target modules ['query_key_value', 'xxx'] not found in the base model. Please\
    \ check the target modules and try again.\" error.\r\n\r\nIf anyone knows what\
    \ they are or how to find them, please let me know."
  created_at: 2023-05-07 16:03:35+00:00
  edited: false
  hidden: false
  id: 6457d9e7ead43697df29ca10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ab56f7c72314bcde542f50be3947585.svg
      fullname: Srijan Roy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: srijanroy
      type: user
    createdAt: '2023-05-08T11:49:24.000Z'
    data:
      edited: false
      editors:
      - srijanroy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ab56f7c72314bcde542f50be3947585.svg
          fullname: Srijan Roy
          isHf: false
          isPro: false
          name: srijanroy
          type: user
        html: '<p>You may try with ''Wqkv''. </p>

          <p>MPTForCausalLM(<br>-(transformer): MPTModel(<br>--(wte): Embedding(50432,
          4096)<br>--(emb_drop): Dropout(p=0, inplace=False)<br>--(blocks): ModuleList(<br>---(0-31):
          32 x MPTBlock(<br>----(norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)<br>----(attn):
          MultiheadAttention(<br>-----(Wqkv): Linear(in_features=4096, out_features=12288,
          bias=False)<br>-----(out_proj): Linear(in_features=4096, out_features=4096,
          bias=False)<br>----)<br>----(norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)<br>----(ffn):
          MPTMLP(<br>-----(up_proj): Linear(in_features=4096, out_features=16384,
          bias=False)<br>-----(act): GELU(approximate=''none'')<br>-----(down_proj):
          Linear(in_features=16384, out_features=4096, bias=False)<br>----)<br>----(resid_attn_dropout):
          Dropout(p=0, inplace=False)<br>----(resid_ffn_dropout): Dropout(p=0, inplace=False)<br>---)<br>--)<br>--(norm_f):
          LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)<br>-)<br>)</p>

          '
        raw: "You may try with 'Wqkv'. \n\nMPTForCausalLM(\n-(transformer): MPTModel(\n\
          --(wte): Embedding(50432, 4096)\n--(emb_drop): Dropout(p=0, inplace=False)\n\
          --(blocks): ModuleList(\n---(0-31): 32 x MPTBlock(\n----(norm_1): LPLayerNorm((4096,),\
          \ eps=1e-05, elementwise_affine=True)\n----(attn): MultiheadAttention(\n\
          -----(Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n\
          -----(out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n\
          ----)\n----(norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n\
          ----(ffn): MPTMLP(\n-----(up_proj): Linear(in_features=4096, out_features=16384,\
          \ bias=False)\n-----(act): GELU(approximate='none')\n-----(down_proj): Linear(in_features=16384,\
          \ out_features=4096, bias=False)\n----)\n----(resid_attn_dropout): Dropout(p=0,\
          \ inplace=False)\n----(resid_ffn_dropout): Dropout(p=0, inplace=False)\n\
          ---)\n--)\n--(norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n\
          -)\n)"
        updatedAt: '2023-05-08T11:49:24.920Z'
      numEdits: 0
      reactions: []
    id: 6458e1c4232e5f0712b0ab11
    type: comment
  author: srijanroy
  content: "You may try with 'Wqkv'. \n\nMPTForCausalLM(\n-(transformer): MPTModel(\n\
    --(wte): Embedding(50432, 4096)\n--(emb_drop): Dropout(p=0, inplace=False)\n--(blocks):\
    \ ModuleList(\n---(0-31): 32 x MPTBlock(\n----(norm_1): LPLayerNorm((4096,), eps=1e-05,\
    \ elementwise_affine=True)\n----(attn): MultiheadAttention(\n-----(Wqkv): Linear(in_features=4096,\
    \ out_features=12288, bias=False)\n-----(out_proj): Linear(in_features=4096, out_features=4096,\
    \ bias=False)\n----)\n----(norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n\
    ----(ffn): MPTMLP(\n-----(up_proj): Linear(in_features=4096, out_features=16384,\
    \ bias=False)\n-----(act): GELU(approximate='none')\n-----(down_proj): Linear(in_features=16384,\
    \ out_features=4096, bias=False)\n----)\n----(resid_attn_dropout): Dropout(p=0,\
    \ inplace=False)\n----(resid_ffn_dropout): Dropout(p=0, inplace=False)\n---)\n\
    --)\n--(norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n-)\n\
    )"
  created_at: 2023-05-08 10:49:24+00:00
  edited: false
  hidden: false
  id: 6458e1c4232e5f0712b0ab11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
      fullname: Daniel King
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daking
      type: user
    createdAt: '2023-05-08T17:31:58.000Z'
    data:
      edited: false
      editors:
      - daking
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
          fullname: Daniel King
          isHf: false
          isPro: false
          name: daking
          type: user
        html: '<p>Lets move this discussion to <a rel="nofollow" href="https://github.com/mosaicml/llm-foundry/issues/64">https://github.com/mosaicml/llm-foundry/issues/64</a>
          please :)</p>

          '
        raw: Lets move this discussion to https://github.com/mosaicml/llm-foundry/issues/64
          please :)
        updatedAt: '2023-05-08T17:31:58.619Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6459320e39e6aea69cc1da69
    id: 6459320e39e6aea69cc1da68
    type: comment
  author: daking
  content: Lets move this discussion to https://github.com/mosaicml/llm-foundry/issues/64
    please :)
  created_at: 2023-05-08 16:31:58+00:00
  edited: false
  hidden: false
  id: 6459320e39e6aea69cc1da68
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1611612244659-5e7bd6f730dc073f817a2ba8.jpeg?w=200&h=200&f=face
      fullname: Daniel King
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: daking
      type: user
    createdAt: '2023-05-08T17:31:58.000Z'
    data:
      status: closed
    id: 6459320e39e6aea69cc1da69
    type: status-change
  author: daking
  created_at: 2023-05-08 16:31:58+00:00
  id: 6459320e39e6aea69cc1da69
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: mosaicml/mpt-7b
repo_type: model
status: closed
target_branch: null
title: query_key_value for LoRA? (target modules)
