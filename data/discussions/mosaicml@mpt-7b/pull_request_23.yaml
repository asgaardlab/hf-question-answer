!!python/object:huggingface_hub.community.DiscussionWithDetails
author: shijie-wu
conflicting_files:
- modeling_mpt.py
created_at: 2023-05-15 21:38:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1631038777636-noauth.jpeg?w=200&h=200&f=face
      fullname: Shijie Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shijie-wu
      type: user
    createdAt: '2023-05-15T22:38:51.000Z'
    data:
      edited: true
      editors:
      - shijie-wu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1631038777636-noauth.jpeg?w=200&h=200&f=face
          fullname: Shijie Wu
          isHf: false
          isPro: false
          name: shijie-wu
          type: user
        html: '<p>without <code>_no_split_modules</code> it will fail when setting
          <code>device_map="auto"</code><br><a rel="nofollow" href="https://github.com/huggingface/transformers/blob/118e9810687dd713b6be07af79e80eeb1d916908/src/transformers/modeling_utils.py#L2684-L2685">https://github.com/huggingface/transformers/blob/118e9810687dd713b6be07af79e80eeb1d916908/src/transformers/modeling_utils.py#L2684-L2685</a></p>

          '
        raw: 'without `_no_split_modules` it will fail when setting `device_map="auto"`

          https://github.com/huggingface/transformers/blob/118e9810687dd713b6be07af79e80eeb1d916908/src/transformers/modeling_utils.py#L2684-L2685'
        updatedAt: '2023-05-15T22:40:04.143Z'
      numEdits: 1
      reactions:
      - count: 5
        reaction: "\u2764\uFE0F"
        users:
        - patrickvonplaten
        - JacopoBandoni
        - the-uns
        - Forbu14
        - dafu2023
    id: 6462b47b57b29d859fa7a9be
    type: comment
  author: shijie-wu
  content: 'without `_no_split_modules` it will fail when setting `device_map="auto"`

    https://github.com/huggingface/transformers/blob/118e9810687dd713b6be07af79e80eeb1d916908/src/transformers/modeling_utils.py#L2684-L2685'
  created_at: 2023-05-15 21:38:51+00:00
  edited: true
  hidden: false
  id: 6462b47b57b29d859fa7a9be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1631038777636-noauth.jpeg?w=200&h=200&f=face
      fullname: Shijie Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shijie-wu
      type: user
    createdAt: '2023-05-15T22:38:52.000Z'
    data:
      oid: d8a52ba8f9fb1e8127c88717d6a80792ef991774
      parents:
      - d8304854d4877849c3c0a78f3469512a84419e84
      subject: Support device_map="auto" when loading
    id: 6462b47c0000000000000000
    type: commit
  author: shijie-wu
  created_at: 2023-05-15 21:38:52+00:00
  id: 6462b47c0000000000000000
  oid: d8a52ba8f9fb1e8127c88717d6a80792ef991774
  summary: Support device_map="auto" when loading
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
      fullname: Nicolas Chaillan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicolaschaillan
      type: user
    createdAt: '2023-05-22T21:05:51.000Z'
    data:
      edited: false
      editors:
      - nicolaschaillan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
          fullname: Nicolas Chaillan
          isHf: false
          isPro: false
          name: nicolaschaillan
          type: user
        html: '<p>Just to clarify, does the model support device_map="auto" and if
          so what''s the syntax?</p>

          '
        raw: Just to clarify, does the model support device_map="auto" and if so what's
          the syntax?
        updatedAt: '2023-05-22T21:05:51.368Z'
      numEdits: 0
      reactions: []
    id: 646bd92f5d68f5c15a325484
    type: comment
  author: nicolaschaillan
  content: Just to clarify, does the model support device_map="auto" and if so what's
    the syntax?
  created_at: 2023-05-22 20:05:51+00:00
  edited: false
  hidden: false
  id: 646bd92f5d68f5c15a325484
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-23T00:58:09.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: "<p>Thank you for the PR <span data-props=\"{&quot;user&quot;:&quot;shijie-wu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/shijie-wu\"\
          >@<span class=\"underline\">shijie-wu</span></a></span>\n\n\t</span></span>.</p>\n\
          <p>Based on my understanding, this should work, however, when I run this,\
          \ I get an error:</p>\n<pre><code class=\"language-python\">NameError  \
          \                               Traceback (most recent call last)\nCell\
          \ In[<span class=\"hljs-number\">5</span>], line <span class=\"hljs-number\"\
          >1</span>\n----&gt; <span class=\"hljs-number\">1</span> model = transformers.AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"mosaicml/mpt-7b\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>, revision=<span class=\"hljs-string\"\
          >\"refs/pr/23\"</span>, device_map=<span class=\"hljs-string\">'auto'</span>)\n\
          \nFile /usr/lib/python3/dist-packages/transformers/models/auto/auto_factory.py:<span\
          \ class=\"hljs-number\">462</span>, <span class=\"hljs-keyword\">in</span>\
          \ _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    <span class=\"hljs-number\">458</span>   \
          \  class_ref = config.auto_map[cls.__name__]\n    <span class=\"hljs-number\"\
          >459</span>     model_class = get_class_from_dynamic_module(\n    <span\
          \ class=\"hljs-number\">460</span>         class_ref, pretrained_model_name_or_path,\
          \ **hub_kwargs, **kwargs\n    <span class=\"hljs-number\">461</span>   \
          \  )\n--&gt; <span class=\"hljs-number\">462</span>     <span class=\"hljs-keyword\"\
          >return</span> model_class.from_pretrained(\n    <span class=\"hljs-number\"\
          >463</span>         pretrained_model_name_or_path, *model_args, config=config,\
          \ **hub_kwargs, **kwargs\n    <span class=\"hljs-number\">464</span>   \
          \  )\n    <span class=\"hljs-number\">465</span> <span class=\"hljs-keyword\"\
          >elif</span> <span class=\"hljs-built_in\">type</span>(config) <span class=\"\
          hljs-keyword\">in</span> cls._model_mapping.keys():\n    <span class=\"\
          hljs-number\">466</span>     model_class = _get_model_class(config, cls._model_mapping)\n\
          \nFile /usr/lib/python3/dist-packages/transformers/modeling_utils.py:<span\
          \ class=\"hljs-number\">2608</span>, <span class=\"hljs-keyword\">in</span>\
          \ PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
          \ **kwargs)\n   <span class=\"hljs-number\">2606</span>     init_contexts\
          \ = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config())] + init_contexts\n\
          \   <span class=\"hljs-number\">2607</span> <span class=\"hljs-keyword\"\
          >elif</span> load_in_8bit <span class=\"hljs-keyword\">or</span> low_cpu_mem_usage:\n\
          -&gt; <span class=\"hljs-number\">2608</span>     init_contexts.append(init_empty_weights())\n\
          \   <span class=\"hljs-number\">2610</span> <span class=\"hljs-keyword\"\
          >with</span> ContextManagers(init_contexts):\n   <span class=\"hljs-number\"\
          >2611</span>     model = cls(config, *model_args, **model_kwargs)\n\nNameError:\
          \ name <span class=\"hljs-string\">'init_empty_weights'</span> <span class=\"\
          hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> defined\n\
          </code></pre>\n<p>Did you test this?</p>\n"
        raw: "Thank you for the PR @shijie-wu.\n\nBased on my understanding, this\
          \ should work, however, when I run this, I get an error:\n\n```python\n\
          NameError                                 Traceback (most recent call last)\n\
          Cell In[5], line 1\n----> 1 model = transformers.AutoModelForCausalLM.from_pretrained(\"\
          mosaicml/mpt-7b\", trust_remote_code=True, revision=\"refs/pr/23\", device_map='auto')\n\
          \nFile /usr/lib/python3/dist-packages/transformers/models/auto/auto_factory.py:462,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    458     class_ref = config.auto_map[cls.__name__]\n\
          \    459     model_class = get_class_from_dynamic_module(\n    460     \
          \    class_ref, pretrained_model_name_or_path, **hub_kwargs, **kwargs\n\
          \    461     )\n--> 462     return model_class.from_pretrained(\n    463\
          \         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
          \ **kwargs\n    464     )\n    465 elif type(config) in cls._model_mapping.keys():\n\
          \    466     model_class = _get_model_class(config, cls._model_mapping)\n\
          \nFile /usr/lib/python3/dist-packages/transformers/modeling_utils.py:2608,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n   2606     init_contexts = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config())]\
          \ + init_contexts\n   2607 elif load_in_8bit or low_cpu_mem_usage:\n-> 2608\
          \     init_contexts.append(init_empty_weights())\n   2610 with ContextManagers(init_contexts):\n\
          \   2611     model = cls(config, *model_args, **model_kwargs)\n\nNameError:\
          \ name 'init_empty_weights' is not defined\n```\n\nDid you test this?"
        updatedAt: '2023-05-23T00:58:09.777Z'
      numEdits: 0
      reactions: []
    id: 646c0fa15d68f5c15a3899d8
    type: comment
  author: sam-mosaic
  content: "Thank you for the PR @shijie-wu.\n\nBased on my understanding, this should\
    \ work, however, when I run this, I get an error:\n\n```python\nNameError    \
    \                             Traceback (most recent call last)\nCell In[5], line\
    \ 1\n----> 1 model = transformers.AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b\"\
    , trust_remote_code=True, revision=\"refs/pr/23\", device_map='auto')\n\nFile\
    \ /usr/lib/python3/dist-packages/transformers/models/auto/auto_factory.py:462,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    458     class_ref = config.auto_map[cls.__name__]\n    459  \
    \   model_class = get_class_from_dynamic_module(\n    460         class_ref, pretrained_model_name_or_path,\
    \ **hub_kwargs, **kwargs\n    461     )\n--> 462     return model_class.from_pretrained(\n\
    \    463         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
    \ **kwargs\n    464     )\n    465 elif type(config) in cls._model_mapping.keys():\n\
    \    466     model_class = _get_model_class(config, cls._model_mapping)\n\nFile\
    \ /usr/lib/python3/dist-packages/transformers/modeling_utils.py:2608, in PreTrainedModel.from_pretrained(cls,\
    \ pretrained_model_name_or_path, *model_args, **kwargs)\n   2606     init_contexts\
    \ = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config())] + init_contexts\n\
    \   2607 elif load_in_8bit or low_cpu_mem_usage:\n-> 2608     init_contexts.append(init_empty_weights())\n\
    \   2610 with ContextManagers(init_contexts):\n   2611     model = cls(config,\
    \ *model_args, **model_kwargs)\n\nNameError: name 'init_empty_weights' is not\
    \ defined\n```\n\nDid you test this?"
  created_at: 2023-05-22 23:58:09+00:00
  edited: false
  hidden: false
  id: 646c0fa15d68f5c15a3899d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
      fullname: Adrien Bufort
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Forbu14
      type: user
    createdAt: '2023-05-24T09:44:24.000Z'
    data:
      edited: true
      editors:
      - Forbu14
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
          fullname: Adrien Bufort
          isHf: false
          isPro: true
          name: Forbu14
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sam-mosaic&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sam-mosaic\">@<span class=\"\
          underline\">sam-mosaic</span></a></span>\n\n\t</span></span> You need to\
          \ install accelerate (and reset your notebook)</p>\n"
        raw: '@sam-mosaic You need to install accelerate (and reset your notebook)'
        updatedAt: '2023-05-24T09:55:26.091Z'
      numEdits: 1
      reactions: []
    id: 646ddc78e34b2ec2d2d6a8f9
    type: comment
  author: Forbu14
  content: '@sam-mosaic You need to install accelerate (and reset your notebook)'
  created_at: 2023-05-24 08:44:24+00:00
  edited: true
  hidden: false
  id: 646ddc78e34b2ec2d2d6a8f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-24T18:57:36.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;Forbu14&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Forbu14\"\
          >@<span class=\"underline\">Forbu14</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Does anyone know if passing <code>refs/pr/23</code> as the <code>revision</code>\
          \ works as one would expect? Because when doing that, I now get</p>\n<pre><code\
          \ class=\"language-python\">---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[<span class=\"hljs-number\">5</span>], line <span class=\"hljs-number\"\
          >1</span>\n----&gt; <span class=\"hljs-number\">1</span> model = transformers.AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"mosaicml/mpt-7b\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>, revision=<span class=\"hljs-string\"\
          >\"refs/pr/23\"</span>, device_map=<span class=\"hljs-string\">\"auto\"\
          </span>)\n\nFile /usr/lib/python3/dist-packages/transformers/models/auto/auto_factory.py:<span\
          \ class=\"hljs-number\">462</span>, <span class=\"hljs-keyword\">in</span>\
          \ _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    <span class=\"hljs-number\">458</span>   \
          \  class_ref = config.auto_map[cls.__name__]\n    <span class=\"hljs-number\"\
          >459</span>     model_class = get_class_from_dynamic_module(\n    <span\
          \ class=\"hljs-number\">460</span>         class_ref, pretrained_model_name_or_path,\
          \ **hub_kwargs, **kwargs\n    <span class=\"hljs-number\">461</span>   \
          \  )\n--&gt; <span class=\"hljs-number\">462</span>     <span class=\"hljs-keyword\"\
          >return</span> model_class.from_pretrained(\n    <span class=\"hljs-number\"\
          >463</span>         pretrained_model_name_or_path, *model_args, config=config,\
          \ **hub_kwargs, **kwargs\n    <span class=\"hljs-number\">464</span>   \
          \  )\n    <span class=\"hljs-number\">465</span> <span class=\"hljs-keyword\"\
          >elif</span> <span class=\"hljs-built_in\">type</span>(config) <span class=\"\
          hljs-keyword\">in</span> cls._model_mapping.keys():\n    <span class=\"\
          hljs-number\">466</span>     model_class = _get_model_class(config, cls._model_mapping)\n\
          \nFile /usr/lib/python3/dist-packages/transformers/modeling_utils.py:<span\
          \ class=\"hljs-number\">2685</span>, <span class=\"hljs-keyword\">in</span>\
          \ PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
          \ **kwargs)\n   <span class=\"hljs-number\">2676</span> special_dtypes.update(\n\
          \   <span class=\"hljs-number\">2677</span>     {\n   <span class=\"hljs-number\"\
          >2678</span>         name: torch.float32\n   (...)\n   <span class=\"hljs-number\"\
          >2681</span>     }\n   <span class=\"hljs-number\">2682</span> )\n   <span\
          \ class=\"hljs-number\">2684</span> <span class=\"hljs-keyword\">if</span>\
          \ model._no_split_modules <span class=\"hljs-keyword\">is</span> <span class=\"\
          hljs-literal\">None</span>:\n-&gt; <span class=\"hljs-number\">2685</span>\
          \     <span class=\"hljs-keyword\">raise</span> ValueError(<span class=\"\
          hljs-string\">f\"<span class=\"hljs-subst\">{model.__class__.__name__}</span>\
          \ does not support `device_map='<span class=\"hljs-subst\">{device_map}</span>'`\
          \ yet.\"</span>)\n   <span class=\"hljs-number\">2686</span> no_split_modules\
          \ = model._no_split_modules\n   <span class=\"hljs-number\">2687</span>\
          \ <span class=\"hljs-keyword\">if</span> device_map <span class=\"hljs-keyword\"\
          >not</span> <span class=\"hljs-keyword\">in</span> [<span class=\"hljs-string\"\
          >\"auto\"</span>, <span class=\"hljs-string\">\"balanced\"</span>, <span\
          \ class=\"hljs-string\">\"balanced_low_0\"</span>, <span class=\"hljs-string\"\
          >\"sequential\"</span>]:\n\nValueError: MPTForCausalLM does <span class=\"\
          hljs-keyword\">not</span> support `device_map=<span class=\"hljs-string\"\
          >'auto'</span>` yet.\n</code></pre>\n<p>Would love to support <code>device_map=\"\
          auto\"</code> but need some evidence that this PR accomplishes that</p>\n"
        raw: "Thank you @Forbu14 \n\nDoes anyone know if passing `refs/pr/23` as the\
          \ `revision` works as one would expect? Because when doing that, I now get\n\
          \n```python\n---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[5], line 1\n----> 1 model = transformers.AutoModelForCausalLM.from_pretrained(\"\
          mosaicml/mpt-7b\", trust_remote_code=True, revision=\"refs/pr/23\", device_map=\"\
          auto\")\n\nFile /usr/lib/python3/dist-packages/transformers/models/auto/auto_factory.py:462,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    458     class_ref = config.auto_map[cls.__name__]\n\
          \    459     model_class = get_class_from_dynamic_module(\n    460     \
          \    class_ref, pretrained_model_name_or_path, **hub_kwargs, **kwargs\n\
          \    461     )\n--> 462     return model_class.from_pretrained(\n    463\
          \         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
          \ **kwargs\n    464     )\n    465 elif type(config) in cls._model_mapping.keys():\n\
          \    466     model_class = _get_model_class(config, cls._model_mapping)\n\
          \nFile /usr/lib/python3/dist-packages/transformers/modeling_utils.py:2685,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n   2676 special_dtypes.update(\n   2677     {\n\
          \   2678         name: torch.float32\n   (...)\n   2681     }\n   2682 )\n\
          \   2684 if model._no_split_modules is None:\n-> 2685     raise ValueError(f\"\
          {model.__class__.__name__} does not support `device_map='{device_map}'`\
          \ yet.\")\n   2686 no_split_modules = model._no_split_modules\n   2687 if\
          \ device_map not in [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"\
          ]:\n\nValueError: MPTForCausalLM does not support `device_map='auto'` yet.\n\
          ```\n\nWould love to support `device_map=\"auto\"` but need some evidence\
          \ that this PR accomplishes that"
        updatedAt: '2023-05-24T18:57:36.922Z'
      numEdits: 0
      reactions: []
    id: 646e5e207a376d3010c35fca
    type: comment
  author: sam-mosaic
  content: "Thank you @Forbu14 \n\nDoes anyone know if passing `refs/pr/23` as the\
    \ `revision` works as one would expect? Because when doing that, I now get\n\n\
    ```python\n---------------------------------------------------------------------------\n\
    ValueError                                Traceback (most recent call last)\n\
    Cell In[5], line 1\n----> 1 model = transformers.AutoModelForCausalLM.from_pretrained(\"\
    mosaicml/mpt-7b\", trust_remote_code=True, revision=\"refs/pr/23\", device_map=\"\
    auto\")\n\nFile /usr/lib/python3/dist-packages/transformers/models/auto/auto_factory.py:462,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    458     class_ref = config.auto_map[cls.__name__]\n    459  \
    \   model_class = get_class_from_dynamic_module(\n    460         class_ref, pretrained_model_name_or_path,\
    \ **hub_kwargs, **kwargs\n    461     )\n--> 462     return model_class.from_pretrained(\n\
    \    463         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
    \ **kwargs\n    464     )\n    465 elif type(config) in cls._model_mapping.keys():\n\
    \    466     model_class = _get_model_class(config, cls._model_mapping)\n\nFile\
    \ /usr/lib/python3/dist-packages/transformers/modeling_utils.py:2685, in PreTrainedModel.from_pretrained(cls,\
    \ pretrained_model_name_or_path, *model_args, **kwargs)\n   2676 special_dtypes.update(\n\
    \   2677     {\n   2678         name: torch.float32\n   (...)\n   2681     }\n\
    \   2682 )\n   2684 if model._no_split_modules is None:\n-> 2685     raise ValueError(f\"\
    {model.__class__.__name__} does not support `device_map='{device_map}'` yet.\"\
    )\n   2686 no_split_modules = model._no_split_modules\n   2687 if device_map not\
    \ in [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"]:\n\nValueError:\
    \ MPTForCausalLM does not support `device_map='auto'` yet.\n```\n\nWould love\
    \ to support `device_map=\"auto\"` but need some evidence that this PR accomplishes\
    \ that"
  created_at: 2023-05-24 17:57:36+00:00
  edited: false
  hidden: false
  id: 646e5e207a376d3010c35fca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
      fullname: Adrien Bufort
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Forbu14
      type: user
    createdAt: '2023-05-24T19:33:16.000Z'
    data:
      edited: true
      editors:
      - Forbu14
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
          fullname: Adrien Bufort
          isHf: false
          isPro: true
          name: Forbu14
          type: user
        html: "<p>I have the same error as you <span data-props=\"{&quot;user&quot;:&quot;sam-mosaic&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sam-mosaic\"\
          >@<span class=\"underline\">sam-mosaic</span></a></span>\n\n\t</span></span>\
          \ , it feels like the revision doesn't work (or we did something wrong)<br>we\
          \ can try passing the hash instead <a href=\"/mosaicml/mpt-7b/commit/d8a52ba8\"\
          >d8a52ba8</a> ?</p>\n"
        raw: 'I have the same error as you @sam-mosaic , it feels like the revision
          doesn''t work (or we did something wrong)

          we can try passing the hash instead d8a52ba8 ?'
        updatedAt: '2023-05-24T19:37:51.312Z'
      numEdits: 2
      reactions: []
    id: 646e667c34fde71fda98b126
    type: comment
  author: Forbu14
  content: 'I have the same error as you @sam-mosaic , it feels like the revision
    doesn''t work (or we did something wrong)

    we can try passing the hash instead d8a52ba8 ?'
  created_at: 2023-05-24 18:33:16+00:00
  edited: true
  hidden: false
  id: 646e667c34fde71fda98b126
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
      fullname: Adrien Bufort
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Forbu14
      type: user
    createdAt: '2023-05-24T20:20:28.000Z'
    data:
      edited: false
      editors:
      - Forbu14
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
          fullname: Adrien Bufort
          isHf: false
          isPro: true
          name: Forbu14
          type: user
        html: '<p>Ok I did some checking and it seems that we just download the wrong
          version of the files here :</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/HZlwISn9x6gnJT-IbhLw8.png"><img
          alt="Screenshot 2023-05-24 at 22.18.43.png" src="https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/HZlwISn9x6gnJT-IbhLw8.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/VoLKH24pjgVQmCMo_TzvQ.png"><img
          alt="Screenshot 2023-05-24 at 22.20.05.png" src="https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/VoLKH24pjgVQmCMo_TzvQ.png"></a></p>

          '
        raw: 'Ok I did some checking and it seems that we just download the wrong
          version of the files here :


          ![Screenshot 2023-05-24 at 22.18.43.png](https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/HZlwISn9x6gnJT-IbhLw8.png)



          ![Screenshot 2023-05-24 at 22.20.05.png](https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/VoLKH24pjgVQmCMo_TzvQ.png)'
        updatedAt: '2023-05-24T20:20:28.388Z'
      numEdits: 0
      reactions: []
    id: 646e718c34fde71fda99c1eb
    type: comment
  author: Forbu14
  content: 'Ok I did some checking and it seems that we just download the wrong version
    of the files here :


    ![Screenshot 2023-05-24 at 22.18.43.png](https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/HZlwISn9x6gnJT-IbhLw8.png)



    ![Screenshot 2023-05-24 at 22.20.05.png](https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/VoLKH24pjgVQmCMo_TzvQ.png)'
  created_at: 2023-05-24 19:20:28+00:00
  edited: false
  hidden: false
  id: 646e718c34fde71fda99c1eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
      fullname: Adrien Bufort
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Forbu14
      type: user
    createdAt: '2023-05-24T20:28:55.000Z'
    data:
      edited: true
      editors:
      - Forbu14
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
          fullname: Adrien Bufort
          isHf: false
          isPro: true
          name: Forbu14
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sam-mosaic&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sam-mosaic\">@<span class=\"\
          underline\">sam-mosaic</span></a></span>\n\n\t</span></span> Basicly we\
          \ download the right config.json file but the wrong *.py files :(</p>\n"
        raw: '@sam-mosaic Basicly we download the right config.json file but the wrong
          *.py files :('
        updatedAt: '2023-05-24T20:29:03.984Z'
      numEdits: 1
      reactions: []
    id: 646e73877a376d3010c56b14
    type: comment
  author: Forbu14
  content: '@sam-mosaic Basicly we download the right config.json file but the wrong
    *.py files :('
  created_at: 2023-05-24 19:28:55+00:00
  edited: true
  hidden: false
  id: 646e73877a376d3010c56b14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-24T20:32:51.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Forbu14&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Forbu14\">@<span class=\"\
          underline\">Forbu14</span></a></span>\n\n\t</span></span> confirmed that\
          \ regardless of if I pass <code>revision=\"refs/pr/23\"</code> or <code>revision=\"\
          d8a52ba8\"</code>, I do not get the version of modeling_mpt.py from this\
          \ PR. That seems like a bug in <code>transformers</code> to me.</p>\n"
        raw: '@Forbu14 confirmed that regardless of if I pass `revision="refs/pr/23"`
          or `revision="d8a52ba8"`, I do not get the version of modeling_mpt.py from
          this PR. That seems like a bug in `transformers` to me.'
        updatedAt: '2023-05-24T20:32:51.285Z'
      numEdits: 0
      reactions: []
    id: 646e74737942c36e9da8b198
    type: comment
  author: sam-mosaic
  content: '@Forbu14 confirmed that regardless of if I pass `revision="refs/pr/23"`
    or `revision="d8a52ba8"`, I do not get the version of modeling_mpt.py from this
    PR. That seems like a bug in `transformers` to me.'
  created_at: 2023-05-24 19:32:51+00:00
  edited: false
  hidden: false
  id: 646e74737942c36e9da8b198
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
      fullname: Adrien Bufort
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Forbu14
      type: user
    createdAt: '2023-05-24T20:53:39.000Z'
    data:
      edited: false
      editors:
      - Forbu14
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
          fullname: Adrien Bufort
          isHf: false
          isPro: true
          name: Forbu14
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sam-mosaic&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sam-mosaic\">@<span class=\"\
          underline\">sam-mosaic</span></a></span>\n\n\t</span></span> clearly, I\
          \ am trying out to figure what wrong in the transformers code base right\
          \ now.</p>\n"
        raw: '@sam-mosaic clearly, I am trying out to figure what wrong in the transformers
          code base right now.'
        updatedAt: '2023-05-24T20:53:39.321Z'
      numEdits: 0
      reactions: []
    id: 646e795334fde71fda9a8479
    type: comment
  author: Forbu14
  content: '@sam-mosaic clearly, I am trying out to figure what wrong in the transformers
    code base right now.'
  created_at: 2023-05-24 19:53:39+00:00
  edited: false
  hidden: false
  id: 646e795334fde71fda9a8479
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
      fullname: Adrien Bufort
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Forbu14
      type: user
    createdAt: '2023-05-24T23:05:13.000Z'
    data:
      edited: false
      editors:
      - Forbu14
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
          fullname: Adrien Bufort
          isHf: false
          isPro: true
          name: Forbu14
          type: user
        html: '<p>I did raise the issue on transformers github. : <a rel="nofollow"
          href="https://github.com/huggingface/transformers/issues/23745">https://github.com/huggingface/transformers/issues/23745</a></p>

          '
        raw: 'I did raise the issue on transformers github. : https://github.com/huggingface/transformers/issues/23745'
        updatedAt: '2023-05-24T23:05:13.614Z'
      numEdits: 0
      reactions: []
    id: 646e98297a376d3010c9241c
    type: comment
  author: Forbu14
  content: 'I did raise the issue on transformers github. : https://github.com/huggingface/transformers/issues/23745'
  created_at: 2023-05-24 22:05:13+00:00
  edited: false
  hidden: false
  id: 646e98297a376d3010c9241c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
      fullname: Adrien Bufort
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Forbu14
      type: user
    createdAt: '2023-05-25T11:46:03.000Z'
    data:
      edited: true
      editors:
      - Forbu14
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/tYImA9T5IvV8llXkbwnea.jpeg?w=200&h=200&f=face
          fullname: Adrien Bufort
          isHf: false
          isPro: true
          name: Forbu14
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sam-mosaic&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sam-mosaic\">@<span class=\"\
          underline\">sam-mosaic</span></a></span>\n\n\t</span></span>  Apparently\
          \ the \"revision\" param is supported only for weight and not for code (currently)</p>\n"
        raw: '@sam-mosaic  Apparently the "revision" param is supported only for weight
          and not for code (currently)'
        updatedAt: '2023-05-25T11:46:12.773Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - sam-mosaic
    id: 646f4a7bac3bff5945e2589d
    type: comment
  author: Forbu14
  content: '@sam-mosaic  Apparently the "revision" param is supported only for weight
    and not for code (currently)'
  created_at: 2023-05-25 10:46:03+00:00
  edited: true
  hidden: false
  id: 646f4a7bac3bff5945e2589d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-25T21:35:43.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>Isn''t the whole point of passing a revision to protect yourself
          from malicious code when using <code>trust_remote_code=True</code>? It even
          warns you to use a revision!!</p>

          '
        raw: Isn't the whole point of passing a revision to protect yourself from
          malicious code when using `trust_remote_code=True`? It even warns you to
          use a revision!!
        updatedAt: '2023-05-25T21:35:43.058Z'
      numEdits: 0
      reactions: []
    id: 646fd4afbc42f4b002350811
    type: comment
  author: sam-mosaic
  content: Isn't the whole point of passing a revision to protect yourself from malicious
    code when using `trust_remote_code=True`? It even warns you to use a revision!!
  created_at: 2023-05-25 20:35:43+00:00
  edited: false
  hidden: false
  id: 646fd4afbc42f4b002350811
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646410244848-5fce6464e80c09ad2760251c.jpeg?w=200&h=200&f=face
      fullname: "Thomas M\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muelletm
      type: user
    createdAt: '2023-05-27T15:23:52.000Z'
    data:
      edited: true
      editors:
      - muelletm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646410244848-5fce6464e80c09ad2760251c.jpeg?w=200&h=200&f=face
          fullname: "Thomas M\xFCller"
          isHf: false
          isPro: false
          name: muelletm
          type: user
        html: "<p>Looks like there is a version of mpt-7b that fixes this:</p>\n<p><a\
          \ href=\"https://huggingface.co/cekal/mpt-7b-peft-compatible\">https://huggingface.co/cekal/mpt-7b-peft-compatible</a></p>\n\
          <p>Also to test a PR locally you can do this:</p>\n<pre><code class=\"language-shell\"\
          >git clone https://huggingface.co/mosaicml/mpt-7b\npushd mpt-7b\ngit fetch\
          \ origin refs/pr/23:pr/23\ngit checkout pr/23\npopd\n\npython your_script.py\
          \ \\\n    --model_name_or_path \"./mpt-7b\"\n...\n</code></pre>\n"
        raw: "Looks like there is a version of mpt-7b that fixes this:\n\nhttps://huggingface.co/cekal/mpt-7b-peft-compatible\n\
          \nAlso to test a PR locally you can do this:\n\n```shell\ngit clone https://huggingface.co/mosaicml/mpt-7b\n\
          pushd mpt-7b\ngit fetch origin refs/pr/23:pr/23\ngit checkout pr/23\npopd\n\
          \npython your_script.py \\\n    --model_name_or_path \"./mpt-7b\"\n...\n\
          ```"
        updatedAt: '2023-05-27T15:24:07.758Z'
      numEdits: 1
      reactions: []
    id: 6472208897a75cc77ab0b809
    type: comment
  author: muelletm
  content: "Looks like there is a version of mpt-7b that fixes this:\n\nhttps://huggingface.co/cekal/mpt-7b-peft-compatible\n\
    \nAlso to test a PR locally you can do this:\n\n```shell\ngit clone https://huggingface.co/mosaicml/mpt-7b\n\
    pushd mpt-7b\ngit fetch origin refs/pr/23:pr/23\ngit checkout pr/23\npopd\n\n\
    python your_script.py \\\n    --model_name_or_path \"./mpt-7b\"\n...\n```"
  created_at: 2023-05-27 14:23:52+00:00
  edited: true
  hidden: false
  id: 6472208897a75cc77ab0b809
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1b1837277fb9200e22fcf604baa369b0.svg
      fullname: Karan Dua
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kdua
      type: user
    createdAt: '2023-05-30T08:04:11.000Z'
    data:
      edited: false
      editors:
      - kdua
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1b1837277fb9200e22fcf604baa369b0.svg
          fullname: Karan Dua
          isHf: false
          isPro: false
          name: kdua
          type: user
        html: '<p>Works when using load_checkpoint_and_dispatch. Doesn''t work with
          from_pretrained.</p>

          <p>from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer,
          TextGenerationPipeline<br>import torch<br>from accelerate import init_empty_weights,
          load_checkpoint_and_dispatch</p>

          <p>model_dir = ''./mpt-7b-instruct''</p>

          <p>max_memory_mapping = {0: "16GB", 1: "16GB"}</p>

          <p>config = AutoConfig.from_pretrained(<br>    model_dir,<br>    trust_remote_code=True,<br>    load_in_8_bit=True<br>)</p>

          <p>with init_empty_weights():<br>    model = AutoModelForCausalLM.from_config(config,
          trust_remote_code=True)</p>

          <p>model.tie_weights()</p>

          <p>model = load_checkpoint_and_dispatch(<br>    model, model_dir, device_map="auto",
          no_split_module_classes=["MPTBlock"], max_memory=max_memory_mapping<br>)</p>

          '
        raw: "Works when using load_checkpoint_and_dispatch. Doesn't work with from_pretrained.\n\
          \nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer,\
          \ TextGenerationPipeline\nimport torch\nfrom accelerate import init_empty_weights,\
          \ load_checkpoint_and_dispatch\n\nmodel_dir = './mpt-7b-instruct'\n\nmax_memory_mapping\
          \ = {0: \"16GB\", 1: \"16GB\"}\n\n\nconfig = AutoConfig.from_pretrained(\n\
          \    model_dir,\n    trust_remote_code=True,\n    load_in_8_bit=True\n)\n\
          \nwith init_empty_weights():\n    model = AutoModelForCausalLM.from_config(config,\
          \ trust_remote_code=True)\n    \nmodel.tie_weights()\n\nmodel = load_checkpoint_and_dispatch(\n\
          \    model, model_dir, device_map=\"auto\", no_split_module_classes=[\"\
          MPTBlock\"], max_memory=max_memory_mapping\n)"
        updatedAt: '2023-05-30T08:04:11.374Z'
      numEdits: 0
      reactions: []
    id: 6475adfbc894b5c9cf6fc5e8
    type: comment
  author: kdua
  content: "Works when using load_checkpoint_and_dispatch. Doesn't work with from_pretrained.\n\
    \nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline\n\
    import torch\nfrom accelerate import init_empty_weights, load_checkpoint_and_dispatch\n\
    \nmodel_dir = './mpt-7b-instruct'\n\nmax_memory_mapping = {0: \"16GB\", 1: \"\
    16GB\"}\n\n\nconfig = AutoConfig.from_pretrained(\n    model_dir,\n    trust_remote_code=True,\n\
    \    load_in_8_bit=True\n)\n\nwith init_empty_weights():\n    model = AutoModelForCausalLM.from_config(config,\
    \ trust_remote_code=True)\n    \nmodel.tie_weights()\n\nmodel = load_checkpoint_and_dispatch(\n\
    \    model, model_dir, device_map=\"auto\", no_split_module_classes=[\"MPTBlock\"\
    ], max_memory=max_memory_mapping\n)"
  created_at: 2023-05-30 07:04:11+00:00
  edited: false
  hidden: false
  id: 6475adfbc894b5c9cf6fc5e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e0ef15b0b011fb082d41a841076daf55.svg
      fullname: Chashi Mahiul Islam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thechashi
      type: user
    createdAt: '2023-05-31T00:36:58.000Z'
    data:
      edited: false
      editors:
      - thechashi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e0ef15b0b011fb082d41a841076daf55.svg
          fullname: Chashi Mahiul Islam
          isHf: false
          isPro: false
          name: thechashi
          type: user
        html: '<p>He kdua, </p>

          <p>This is the error I am getting: </p>

          <p>ValueError: <code>checkpoint</code> should be the path to a file containing
          a whole state dict, or the index of a sharded<br>checkpoint, or a folder
          containing a sharded checkpoint, but got mosaicml/mpt-7b-instruct. </p>

          <p>How to solve this?</p>

          '
        raw: "He kdua, \n\nThis is the error I am getting: \n\nValueError: `checkpoint`\
          \ should be the path to a file containing a whole state dict, or the index\
          \ of a sharded \ncheckpoint, or a folder containing a sharded checkpoint,\
          \ but got mosaicml/mpt-7b-instruct. \n\nHow to solve this?"
        updatedAt: '2023-05-31T00:36:58.717Z'
      numEdits: 0
      reactions: []
    id: 647696aacfe9d995bf430c8c
    type: comment
  author: thechashi
  content: "He kdua, \n\nThis is the error I am getting: \n\nValueError: `checkpoint`\
    \ should be the path to a file containing a whole state dict, or the index of\
    \ a sharded \ncheckpoint, or a folder containing a sharded checkpoint, but got\
    \ mosaicml/mpt-7b-instruct. \n\nHow to solve this?"
  created_at: 2023-05-30 23:36:58+00:00
  edited: false
  hidden: false
  id: 647696aacfe9d995bf430c8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1b1837277fb9200e22fcf604baa369b0.svg
      fullname: Karan Dua
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kdua
      type: user
    createdAt: '2023-05-31T10:06:34.000Z'
    data:
      edited: false
      editors:
      - kdua
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1b1837277fb9200e22fcf604baa369b0.svg
          fullname: Karan Dua
          isHf: false
          isPro: false
          name: kdua
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;thechashi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/thechashi\"\
          >@<span class=\"underline\">thechashi</span></a></span>\n\n\t</span></span><br>You\
          \ need to clone the model repository, checkout the relevant code change,\
          \ pull the model files and then load the checkpoint from that directory:</p>\n\
          <p>git clone <a href=\"https://huggingface.co/mosaicml/mpt-7b-instruct\"\
          >https://huggingface.co/mosaicml/mpt-7b-instruct</a><br>git lfs pull<br>git\
          \ fetch origin refs/pr/23:pr/23<br>git checkout pr/23</p>\n<p>Now use this\
          \ cloned directory in model_dir</p>\n"
        raw: "Hi @thechashi \nYou need to clone the model repository, checkout the\
          \ relevant code change, pull the model files and then load the checkpoint\
          \ from that directory:\n\ngit clone https://huggingface.co/mosaicml/mpt-7b-instruct\n\
          git lfs pull\ngit fetch origin refs/pr/23:pr/23\ngit checkout pr/23\n\n\n\
          Now use this cloned directory in model_dir"
        updatedAt: '2023-05-31T10:06:34.056Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - shijie-wu
    id: 64771c2a242dde316300e2c1
    type: comment
  author: kdua
  content: "Hi @thechashi \nYou need to clone the model repository, checkout the relevant\
    \ code change, pull the model files and then load the checkpoint from that directory:\n\
    \ngit clone https://huggingface.co/mosaicml/mpt-7b-instruct\ngit lfs pull\ngit\
    \ fetch origin refs/pr/23:pr/23\ngit checkout pr/23\n\n\nNow use this cloned directory\
    \ in model_dir"
  created_at: 2023-05-31 09:06:34+00:00
  edited: false
  hidden: false
  id: 64771c2a242dde316300e2c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-31T18:14:26.000Z'
    data:
      edited: false
      editors:
      - abhi-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: '<p>This should be supported now! We are doing some more tests to make
          sure multi-GPU inference works as well and should update soon.</p>

          '
        raw: This should be supported now! We are doing some more tests to make sure
          multi-GPU inference works as well and should update soon.
        updatedAt: '2023-05-31T18:14:26.607Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - shijie-wu
        - Forbu14
        - madhavatreplit
        - Caustic
      relatedEventId: 64778e8233a888101f7a6050
    id: 64778e8233a888101f7a604f
    type: comment
  author: abhi-mosaic
  content: This should be supported now! We are doing some more tests to make sure
    multi-GPU inference works as well and should update soon.
  created_at: 2023-05-31 17:14:26+00:00
  edited: false
  hidden: false
  id: 64778e8233a888101f7a604f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-05-31T18:14:26.000Z'
    data:
      status: closed
    id: 64778e8233a888101f7a6050
    type: status-change
  author: abhi-mosaic
  created_at: 2023-05-31 17:14:26+00:00
  id: 64778e8233a888101f7a6050
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1631038777636-noauth.jpeg?w=200&h=200&f=face
      fullname: Shijie Wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shijie-wu
      type: user
    createdAt: '2023-05-31T18:26:46.000Z'
    data:
      edited: false
      editors:
      - shijie-wu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1631038777636-noauth.jpeg?w=200&h=200&f=face
          fullname: Shijie Wu
          isHf: false
          isPro: false
          name: shijie-wu
          type: user
        html: '<p>sorry i was out of the loop but i''m glad that it''s fixed by <a
          href="https://huggingface.co/mosaicml/mpt-7b/discussions/45">https://huggingface.co/mosaicml/mpt-7b/discussions/45</a>.</p>

          '
        raw: sorry i was out of the loop but i'm glad that it's fixed by https://huggingface.co/mosaicml/mpt-7b/discussions/45.
        updatedAt: '2023-05-31T18:26:46.700Z'
      numEdits: 0
      reactions: []
    id: 6477916604aa03da2ac0ef16
    type: comment
  author: shijie-wu
  content: sorry i was out of the loop but i'm glad that it's fixed by https://huggingface.co/mosaicml/mpt-7b/discussions/45.
  created_at: 2023-05-31 17:26:46+00:00
  edited: false
  hidden: false
  id: 6477916604aa03da2ac0ef16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/71d6b53ba2eea94e2bd3f0459c54601d.svg
      fullname: Sven Heyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sven00
      type: user
    createdAt: '2023-07-04T18:04:02.000Z'
    data:
      edited: true
      editors:
      - Sven00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9408087134361267
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/71d6b53ba2eea94e2bd3f0459c54601d.svg
          fullname: Sven Heyer
          isHf: false
          isPro: false
          name: Sven00
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;abhi-mosaic&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/abhi-mosaic\"\
          >@<span class=\"underline\">abhi-mosaic</span></a></span>\n\n\t</span></span>\
          \ thank you for the support! have you already made progress with regards\
          \ to improving multi-GPU inference? Currently, the prompt provided in the\
          \ model card (\"What is the capital of France?\") takes 20 minutes with\
          \ device_map=auto and continues to generate token after 'end_of_text'</p>\n"
        raw: '@abhi-mosaic thank you for the support! have you already made progress
          with regards to improving multi-GPU inference? Currently, the prompt provided
          in the model card ("What is the capital of France?") takes 20 minutes with
          device_map=auto and continues to generate token after ''end_of_text'''
        updatedAt: '2023-07-04T18:04:57.575Z'
      numEdits: 1
      reactions: []
    id: 64a45f12fe950993d2ce5a72
    type: comment
  author: Sven00
  content: '@abhi-mosaic thank you for the support! have you already made progress
    with regards to improving multi-GPU inference? Currently, the prompt provided
    in the model card ("What is the capital of France?") takes 20 minutes with device_map=auto
    and continues to generate token after ''end_of_text'''
  created_at: 2023-07-04 17:04:02+00:00
  edited: true
  hidden: false
  id: 64a45f12fe950993d2ce5a72
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 23
repo_id: mosaicml/mpt-7b
repo_type: model
status: closed
target_branch: refs/heads/main
title: Support device_map="auto" when loading
