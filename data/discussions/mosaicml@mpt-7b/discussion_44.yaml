!!python/object:huggingface_hub.community.DiscussionWithDetails
author: asimkin
conflicting_files: null
created_at: 2023-05-30 13:29:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1fd7b747c881ead323bf5c70eeadffed.svg
      fullname: Avi Simkin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asimkin
      type: user
    createdAt: '2023-05-30T14:29:44.000Z'
    data:
      edited: false
      editors:
      - asimkin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1fd7b747c881ead323bf5c70eeadffed.svg
          fullname: Avi Simkin
          isHf: false
          isPro: false
          name: asimkin
          type: user
        html: '<p>When trying to get encoder output I get this error:</p>

          <p>AttributeError: ''MPTForCausalLM'' object has no attribute ''encoder''</p>

          <p>I tried to run this code:<br>text = "Here is some input text."<br>inputs
          = tokenizer(text, return_tensors=''pt'')</p>

          <p>with torch.no_grad():<br>    outputs = model.encoder(inputs[''input_ids''])</p>

          <p>embeddings = outputs.last_hidden_state</p>

          <p>Is there an alternative way to get embeddings?</p>

          '
        raw: "When trying to get encoder output I get this error:\r\n\r\nAttributeError:\
          \ 'MPTForCausalLM' object has no attribute 'encoder'\r\n\r\nI tried to run\
          \ this code:\r\ntext = \"Here is some input text.\"\r\ninputs = tokenizer(text,\
          \ return_tensors='pt')\r\n\r\nwith torch.no_grad():\r\n    outputs = model.encoder(inputs['input_ids'])\r\
          \n\r\nembeddings = outputs.last_hidden_state\r\n\r\n\r\nIs there an alternative\
          \ way to get embeddings?"
        updatedAt: '2023-05-30T14:29:44.965Z'
      numEdits: 0
      reactions: []
    id: 647608580360eb2982062021
    type: comment
  author: asimkin
  content: "When trying to get encoder output I get this error:\r\n\r\nAttributeError:\
    \ 'MPTForCausalLM' object has no attribute 'encoder'\r\n\r\nI tried to run this\
    \ code:\r\ntext = \"Here is some input text.\"\r\ninputs = tokenizer(text, return_tensors='pt')\r\
    \n\r\nwith torch.no_grad():\r\n    outputs = model.encoder(inputs['input_ids'])\r\
    \n\r\nembeddings = outputs.last_hidden_state\r\n\r\n\r\nIs there an alternative\
    \ way to get embeddings?"
  created_at: 2023-05-30 13:29:44+00:00
  edited: false
  hidden: false
  id: 647608580360eb2982062021
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f04a2e81ae78628890adb3adee299542.svg
      fullname: Alexander Kosenkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kosiakk
      type: user
    createdAt: '2023-06-07T11:29:20.000Z'
    data:
      edited: false
      editors:
      - kosiakk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4066496789455414
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f04a2e81ae78628890adb3adee299542.svg
          fullname: Alexander Kosenkov
          isHf: false
          isPro: false
          name: kosiakk
          type: user
        html: "<p>You can print the <code>model</code>:</p>\n<pre><code>MPTForCausalLM(\n\
          \  (transformer): MPTModel(\n    (wte): Embedding(50432, 4096)\n    (emb_drop):\
          \ Dropout(p=0, inplace=False)\n    (blocks): ModuleList(\n      (0-31):\
          \ 32 x MPTBlock(\n        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n\
          \        (attn): MultiheadAttention(\n          (Wqkv): Linear(in_features=4096,\
          \ out_features=12288, bias=False)\n          (out_proj): Linear(in_features=4096,\
          \ out_features=4096, bias=False)\n        )\n        (norm_2): LPLayerNorm((4096,),\
          \ eps=1e-05, elementwise_affine=True)\n        (ffn): MPTMLP(\n        \
          \  (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n\
          \          (act): GELU(approximate='none')\n          (down_proj): Linear(in_features=16384,\
          \ out_features=4096, bias=False)\n        )\n        (resid_attn_dropout):\
          \ Dropout(p=0, inplace=False)\n        (resid_ffn_dropout): Dropout(p=0,\
          \ inplace=False)\n      )\n    )\n    (norm_f): LPLayerNorm((4096,), eps=1e-05,\
          \ elementwise_affine=True)\n  )\n)\n</code></pre>\n<p>It has <code>transformer.wte.weights</code>\
          \ matrix, if you want a low-level access to embeddings.</p>\n"
        raw: "You can print the `model`:\n```\nMPTForCausalLM(\n  (transformer): MPTModel(\n\
          \    (wte): Embedding(50432, 4096)\n    (emb_drop): Dropout(p=0, inplace=False)\n\
          \    (blocks): ModuleList(\n      (0-31): 32 x MPTBlock(\n        (norm_1):\
          \ LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n        (attn):\
          \ MultiheadAttention(\n          (Wqkv): Linear(in_features=4096, out_features=12288,\
          \ bias=False)\n          (out_proj): Linear(in_features=4096, out_features=4096,\
          \ bias=False)\n        )\n        (norm_2): LPLayerNorm((4096,), eps=1e-05,\
          \ elementwise_affine=True)\n        (ffn): MPTMLP(\n          (up_proj):\
          \ Linear(in_features=4096, out_features=16384, bias=False)\n          (act):\
          \ GELU(approximate='none')\n          (down_proj): Linear(in_features=16384,\
          \ out_features=4096, bias=False)\n        )\n        (resid_attn_dropout):\
          \ Dropout(p=0, inplace=False)\n        (resid_ffn_dropout): Dropout(p=0,\
          \ inplace=False)\n      )\n    )\n    (norm_f): LPLayerNorm((4096,), eps=1e-05,\
          \ elementwise_affine=True)\n  )\n)\n```\n\nIt has `transformer.wte.weights`\
          \ matrix, if you want a low-level access to embeddings."
        updatedAt: '2023-06-07T11:29:20.385Z'
      numEdits: 0
      reactions: []
    id: 64806a109aafd41918a64060
    type: comment
  author: kosiakk
  content: "You can print the `model`:\n```\nMPTForCausalLM(\n  (transformer): MPTModel(\n\
    \    (wte): Embedding(50432, 4096)\n    (emb_drop): Dropout(p=0, inplace=False)\n\
    \    (blocks): ModuleList(\n      (0-31): 32 x MPTBlock(\n        (norm_1): LPLayerNorm((4096,),\
    \ eps=1e-05, elementwise_affine=True)\n        (attn): MultiheadAttention(\n \
    \         (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n \
    \         (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n\
    \        )\n        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n\
    \        (ffn): MPTMLP(\n          (up_proj): Linear(in_features=4096, out_features=16384,\
    \ bias=False)\n          (act): GELU(approximate='none')\n          (down_proj):\
    \ Linear(in_features=16384, out_features=4096, bias=False)\n        )\n      \
    \  (resid_attn_dropout): Dropout(p=0, inplace=False)\n        (resid_ffn_dropout):\
    \ Dropout(p=0, inplace=False)\n      )\n    )\n    (norm_f): LPLayerNorm((4096,),\
    \ eps=1e-05, elementwise_affine=True)\n  )\n)\n```\n\nIt has `transformer.wte.weights`\
    \ matrix, if you want a low-level access to embeddings."
  created_at: 2023-06-07 10:29:20+00:00
  edited: false
  hidden: false
  id: 64806a109aafd41918a64060
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676407968549-63dd38e5a8877129a15a12c5.png?w=200&h=200&f=face
      fullname: Jacob Portes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jacobfulano
      type: user
    createdAt: '2023-06-08T02:40:47.000Z'
    data:
      status: closed
    id: 64813fafe1421e205fe40df4
    type: status-change
  author: jacobfulano
  created_at: 2023-06-08 01:40:47+00:00
  id: 64813fafe1421e205fe40df4
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 44
repo_id: mosaicml/mpt-7b
repo_type: model
status: closed
target_branch: null
title: Is it possible to get encoder embeddings?
