!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vi-c
conflicting_files: null
created_at: 2023-06-05 19:31:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fc04bf42b8982bc319340e9091eaa044.svg
      fullname: vi-c
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vi-c
      type: user
    createdAt: '2023-06-05T20:31:25.000Z'
    data:
      edited: false
      editors:
      - vi-c
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5795136094093323
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fc04bf42b8982bc319340e9091eaa044.svg
          fullname: vi-c
          isHf: false
          isPro: false
          name: vi-c
          type: user
        html: '<p>Hi,<br>I try to finetune mpt-7b on a free colab VM. When I execute
          this code below, I get a RAM OOM, and then the VM crashes.<br>The RAM of
          the GPU is not used at all.<br>If I switch to another model like decapoda-research/llama-7b-hf,
          everything works fine (model loaded on GPU RAM)<br>Any idea where I made
          a mistake?<br>Thanks !</p>

          <p>########################################3<br>model = AutoModelForCausalLM.from_pretrained(<br>            ''mosaicml/mpt-7b'',<br>            device_map={"":
          0},<br>            trust_remote_code=True,<br>            low_cpu_mem_usage=True,<br>            quantization_config=BitsAndBytesConfig(<br>                load_in_4bit=True,<br>                bnb_4bit_compute_dtype=torch.bfloat16,<br>                bnb_4bit_use_double_quant=True,<br>                bnb_4bit_quant_type=''nf4''<br>            )<br>        )</p>

          '
        raw: "Hi,\r\nI try to finetune mpt-7b on a free colab VM. When I execute this\
          \ code below, I get a RAM OOM, and then the VM crashes. \r\nThe RAM of the\
          \ GPU is not used at all. \r\nIf I switch to another model like decapoda-research/llama-7b-hf,\
          \ everything works fine (model loaded on GPU RAM)\r\nAny idea where I made\
          \ a mistake?\r\nThanks !\r\n\r\n########################################3\r\
          \nmodel = AutoModelForCausalLM.from_pretrained(\r\n            'mosaicml/mpt-7b',\r\
          \n            device_map={\"\": 0},\r\n            trust_remote_code=True,\r\
          \n            low_cpu_mem_usage=True,\r\n            quantization_config=BitsAndBytesConfig(\r\
          \n                load_in_4bit=True,\r\n                bnb_4bit_compute_dtype=torch.bfloat16,\r\
          \n                bnb_4bit_use_double_quant=True,\r\n                bnb_4bit_quant_type='nf4'\r\
          \n            )\r\n        )"
        updatedAt: '2023-06-05T20:31:25.161Z'
      numEdits: 0
      reactions: []
    id: 647e461da49bffab5d72cfe5
    type: comment
  author: vi-c
  content: "Hi,\r\nI try to finetune mpt-7b on a free colab VM. When I execute this\
    \ code below, I get a RAM OOM, and then the VM crashes. \r\nThe RAM of the GPU\
    \ is not used at all. \r\nIf I switch to another model like decapoda-research/llama-7b-hf,\
    \ everything works fine (model loaded on GPU RAM)\r\nAny idea where I made a mistake?\r\
    \nThanks !\r\n\r\n########################################3\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\
    \n            'mosaicml/mpt-7b',\r\n            device_map={\"\": 0},\r\n    \
    \        trust_remote_code=True,\r\n            low_cpu_mem_usage=True,\r\n  \
    \          quantization_config=BitsAndBytesConfig(\r\n                load_in_4bit=True,\r\
    \n                bnb_4bit_compute_dtype=torch.bfloat16,\r\n                bnb_4bit_use_double_quant=True,\r\
    \n                bnb_4bit_quant_type='nf4'\r\n            )\r\n        )"
  created_at: 2023-06-05 19:31:25+00:00
  edited: false
  hidden: false
  id: 647e461da49bffab5d72cfe5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e27b045966c7971a609e496f2ad4c1e4.svg
      fullname: Zach Blank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zachblank
      type: user
    createdAt: '2023-06-06T19:57:34.000Z'
    data:
      edited: false
      editors:
      - zachblank
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9162338972091675
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e27b045966c7971a609e496f2ad4c1e4.svg
          fullname: Zach Blank
          isHf: false
          isPro: false
          name: zachblank
          type: user
        html: '<p>Some edits have to be made to the model itself. Try <a href="https://huggingface.co/Gladiaio/mpt-7b-qlora">https://huggingface.co/Gladiaio/mpt-7b-qlora</a></p>

          '
        raw: Some edits have to be made to the model itself. Try https://huggingface.co/Gladiaio/mpt-7b-qlora
        updatedAt: '2023-06-06T19:57:34.225Z'
      numEdits: 0
      reactions: []
    id: 647f8faecbb8294ed80756fd
    type: comment
  author: zachblank
  content: Some edits have to be made to the model itself. Try https://huggingface.co/Gladiaio/mpt-7b-qlora
  created_at: 2023-06-06 18:57:34+00:00
  edited: false
  hidden: false
  id: 647f8faecbb8294ed80756fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fc04bf42b8982bc319340e9091eaa044.svg
      fullname: vi-c
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vi-c
      type: user
    createdAt: '2023-06-06T20:43:38.000Z'
    data:
      edited: false
      editors:
      - vi-c
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5679807066917419
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fc04bf42b8982bc319340e9091eaa044.svg
          fullname: vi-c
          isHf: false
          isPro: false
          name: vi-c
          type: user
        html: '<p>I got the same behavior (OORAM) with :</p>

          <ul>

          <li><a href="https://huggingface.co/Gladiaio/mpt-7b-qlora">https://huggingface.co/Gladiaio/mpt-7b-qlora</a>
          </li>

          <li><a href="https://huggingface.co/cekal/mpt-7b-peft-compatible">https://huggingface.co/cekal/mpt-7b-peft-compatible</a></li>

          </ul>

          '
        raw: "I got the same behavior (OORAM) with :\n- https://huggingface.co/Gladiaio/mpt-7b-qlora\
          \ \n- https://huggingface.co/cekal/mpt-7b-peft-compatible"
        updatedAt: '2023-06-06T20:43:38.441Z'
      numEdits: 0
      reactions: []
    id: 647f9a7a86888bbffbdcce98
    type: comment
  author: vi-c
  content: "I got the same behavior (OORAM) with :\n- https://huggingface.co/Gladiaio/mpt-7b-qlora\
    \ \n- https://huggingface.co/cekal/mpt-7b-peft-compatible"
  created_at: 2023-06-06 19:43:38+00:00
  edited: false
  hidden: false
  id: 647f9a7a86888bbffbdcce98
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
      fullname: Abhi Venigalla
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: abhi-mosaic
      type: user
    createdAt: '2023-06-07T21:12:25.000Z'
    data:
      edited: true
      editors:
      - abhi-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9539098143577576
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676410153781-63ebfcf06ef3ce22b887cb04.jpeg?w=200&h=200&f=face
          fullname: Abhi Venigalla
          isHf: false
          isPro: false
          name: abhi-mosaic
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;vi-c&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vi-c\">@<span class=\"\
          underline\">vi-c</span></a></span>\n\n\t</span></span> , could you try using\
          \ <code>device_map='auto'</code> and make sure you clear your local HF cache\
          \ and redownload the model (we pushed new source code last Friday)? I'm\
          \ not sure what the behavior would be for MPT with a hardcoded <code>device_map={\"\
          \":0}</code> dict. We have also not tested any support with BitsAndBytes\
          \ yet.</p>\n"
        raw: Hi @vi-c , could you try using `device_map='auto'` and make sure you
          clear your local HF cache and redownload the model (we pushed new source
          code last Friday)? I'm not sure what the behavior would be for MPT with
          a hardcoded `device_map={"":0}` dict. We have also not tested any support
          with BitsAndBytes yet.
        updatedAt: '2023-06-07T21:13:04.966Z'
      numEdits: 1
      reactions: []
    id: 6480f2b9e1421e205fdfa957
    type: comment
  author: abhi-mosaic
  content: Hi @vi-c , could you try using `device_map='auto'` and make sure you clear
    your local HF cache and redownload the model (we pushed new source code last Friday)?
    I'm not sure what the behavior would be for MPT with a hardcoded `device_map={"":0}`
    dict. We have also not tested any support with BitsAndBytes yet.
  created_at: 2023-06-07 20:12:25+00:00
  edited: true
  hidden: false
  id: 6480f2b9e1421e205fdfa957
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-07-14T09:27:55.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9215862154960632
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;vi-c&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vi-c\">@<span class=\"\
          underline\">vi-c</span></a></span>\n\n\t</span></span> , I had the same\
          \ issue with Collab. I even increased to 25 GB on the pro plan and it crashes\
          \ when loading.</p>\n<ol>\n<li>Did you manage to get this to work?</li>\n\
          <li>Have you tried running on sagemaker? If so, what instance do you recommend?</li>\n\
          <li><span data-props=\"{&quot;user&quot;:&quot;zachblank&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zachblank\">@<span class=\"\
          underline\">zachblank</span></a></span>\n\n\t</span></span> what edits did\
          \ you make? I wasn't clear when reading. And do you have any recommended\
          \ config (if not using hardcoded device_map and BitsAndBytes).</li>\n</ol>\n\
          <p>Thanks.</p>\n"
        raw: 'Hi @vi-c , I had the same issue with Collab. I even increased to 25
          GB on the pro plan and it crashes when loading.


          1. Did you manage to get this to work?

          2. Have you tried running on sagemaker? If so, what instance do you recommend?

          3. @zachblank what edits did you make? I wasn''t clear when reading. And
          do you have any recommended config (if not using hardcoded device_map and
          BitsAndBytes).


          Thanks.'
        updatedAt: '2023-07-14T09:27:55.592Z'
      numEdits: 0
      reactions: []
    id: 64b1151b93968c171569a9ee
    type: comment
  author: RonanMcGovern
  content: 'Hi @vi-c , I had the same issue with Collab. I even increased to 25 GB
    on the pro plan and it crashes when loading.


    1. Did you manage to get this to work?

    2. Have you tried running on sagemaker? If so, what instance do you recommend?

    3. @zachblank what edits did you make? I wasn''t clear when reading. And do you
    have any recommended config (if not using hardcoded device_map and BitsAndBytes).


    Thanks.'
  created_at: 2023-07-14 08:27:55+00:00
  edited: false
  hidden: false
  id: 64b1151b93968c171569a9ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2757e79ac8c25b4dc79bd08f3dbfd606.svg
      fullname: Muhammad Usman Siddiqui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 316usman
      type: user
    createdAt: '2023-08-22T10:42:17.000Z'
    data:
      edited: true
      editors:
      - 316usman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9165648221969604
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2757e79ac8c25b4dc79bd08f3dbfd606.svg
          fullname: Muhammad Usman Siddiqui
          isHf: false
          isPro: false
          name: 316usman
          type: user
        html: "<blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;vi-c&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/vi-c\"\
          >@<span class=\"underline\">vi-c</span></a></span>\n\n\t</span></span> ,\
          \ could you try using <code>device_map='auto'</code> and make sure you clear\
          \ your local HF cache and redownload the model (we pushed new source code\
          \ last Friday)? I'm not sure what the behavior would be for MPT with a hardcoded\
          \ <code>device_map={\"\":0}</code> dict. We have also not tested any support\
          \ with BitsAndBytes yet.</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;zachblank&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/zachblank\"\
          >@<span class=\"underline\">zachblank</span></a></span>\n\n\t</span></span>\
          \ the same happens there isnt enough memory in the free gpu the first shard\
          \ takes 13.3 GB gpu memory and then it crashes</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;abhi-mosaic&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/abhi-mosaic\">@<span class=\"underline\"\
          >abhi-mosaic</span></a></span>\n\n\t</span></span> tried device_map=auto\
          \ but the model does not go to the gpu memory in this case and therefore\
          \ it crashes.</p>\n<p>Please help</p>\n"
        raw: '> Hi @vi-c , could you try using `device_map=''auto''` and make sure
          you clear your local HF cache and redownload the model (we pushed new source
          code last Friday)? I''m not sure what the behavior would be for MPT with
          a hardcoded `device_map={"":0}` dict. We have also not tested any support
          with BitsAndBytes yet.


          @zachblank the same happens there isnt enough memory in the free gpu the
          first shard takes 13.3 GB gpu memory and then it crashes


          @abhi-mosaic tried device_map=auto but the model does not go to the gpu
          memory in this case and therefore it crashes.


          Please help'
        updatedAt: '2023-08-22T10:42:34.056Z'
      numEdits: 1
      reactions: []
    id: 64e49109ae2516de414d29ea
    type: comment
  author: 316usman
  content: '> Hi @vi-c , could you try using `device_map=''auto''` and make sure you
    clear your local HF cache and redownload the model (we pushed new source code
    last Friday)? I''m not sure what the behavior would be for MPT with a hardcoded
    `device_map={"":0}` dict. We have also not tested any support with BitsAndBytes
    yet.


    @zachblank the same happens there isnt enough memory in the free gpu the first
    shard takes 13.3 GB gpu memory and then it crashes


    @abhi-mosaic tried device_map=auto but the model does not go to the gpu memory
    in this case and therefore it crashes.


    Please help'
  created_at: 2023-08-22 09:42:17+00:00
  edited: true
  hidden: false
  id: 64e49109ae2516de414d29ea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 50
repo_id: mosaicml/mpt-7b
repo_type: model
status: open
target_branch: null
title: MPT-7b on colab - RAM of GPU not used
