!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Bigshot
conflicting_files: null
created_at: 2023-05-11 10:40:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677881256125-noauth.png?w=200&h=200&f=face
      fullname: R P G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bigshot
      type: user
    createdAt: '2023-05-11T11:40:54.000Z'
    data:
      edited: true
      editors:
      - Bigshot
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677881256125-noauth.png?w=200&h=200&f=face
          fullname: R P G
          isHf: false
          isPro: false
          name: Bigshot
          type: user
        html: '<p>I have attempted to run this model on a AMD GPU but it seems that
          CUDA is a requirement for Flash Attention.<br>Because of this problem I
          have resorted to trying to use SageMaker to fine-tune it.<br>Will this work?<br>Can
          you add support for AMD GPUs as well?<br>How can I fix this?</p>

          '
        raw: "I have attempted to run this model on a AMD GPU but it seems that CUDA\
          \ is a requirement for Flash Attention. \nBecause of this problem I have\
          \ resorted to trying to use SageMaker to fine-tune it. \nWill this work?\n\
          Can you add support for AMD GPUs as well? \nHow can I fix this?"
        updatedAt: '2023-05-11T11:41:22.523Z'
      numEdits: 1
      reactions: []
    id: 645cd446680734460f9ba1ae
    type: comment
  author: Bigshot
  content: "I have attempted to run this model on a AMD GPU but it seems that CUDA\
    \ is a requirement for Flash Attention. \nBecause of this problem I have resorted\
    \ to trying to use SageMaker to fine-tune it. \nWill this work?\nCan you add support\
    \ for AMD GPUs as well? \nHow can I fix this?"
  created_at: 2023-05-11 10:40:54+00:00
  edited: true
  hidden: false
  id: 645cd446680734460f9ba1ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677881256125-noauth.png?w=200&h=200&f=face
      fullname: R P G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bigshot
      type: user
    createdAt: '2023-05-11T11:41:09.000Z'
    data:
      from: 'Can this be fine-tuned using Amazon Sagemaker or run on a AMD GPU that
        is not CUDA-enabled? '
      to: Can this be fine-tuned using Amazon SageMaker or run on a AMD GPU that is
        not CUDA-enabled?
    id: 645cd4554e2cf46891787e52
    type: title-change
  author: Bigshot
  created_at: 2023-05-11 10:41:09+00:00
  id: 645cd4554e2cf46891787e52
  new_title: Can this be fine-tuned using Amazon SageMaker or run on a AMD GPU that
    is not CUDA-enabled?
  old_title: 'Can this be fine-tuned using Amazon Sagemaker or run on a AMD GPU that
    is not CUDA-enabled? '
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661118445899-noauth.png?w=200&h=200&f=face
      fullname: Robert Michael Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saber7ooth
      type: user
    createdAt: '2023-05-11T23:00:21.000Z'
    data:
      edited: true
      editors:
      - saber7ooth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661118445899-noauth.png?w=200&h=200&f=face
          fullname: Robert Michael Smith
          isHf: false
          isPro: false
          name: saber7ooth
          type: user
        html: '<p>See:  <a rel="nofollow" href="https://www.reddit.com/r/MachineLearning/comments/xjnozq/d_whats_the_word_on_amd_gpus_these_days/">https://www.reddit.com/r/MachineLearning/comments/xjnozq/d_whats_the_word_on_amd_gpus_these_days/</a><br>And:  <a
          rel="nofollow" href="https://www.amd.com/en/graphics/servers-solutions-rocm">https://www.amd.com/en/graphics/servers-solutions-rocm</a></p>

          <p>In summary:</p>

          <ul>

          <li>Support for ROCm is not great at the consumer level and pytorch must
          be compiled from source.  </li>

          <li>At the server level, you will find AMD compute instances with ROCm precompiled,
          at a pricetag.</li>

          <li>Flash Attention runtime kernels are composed for NVIDIA specific server
          products.</li>

          </ul>

          <p>This will not allow Flash Attention, but as the good news:</p>

          <ul>

          <li>Compiling from source will allow AMD GPU accelerated pytorch with ROCm.  This
          does provide AMD users with an option.</li>

          <li>HF <code>transformers.accelerator</code> as a direct use-case becomes
          available (uses pytorch to split the model across several devices, when
          memory resources run low) -- Multiple AMD GPUs can be utilized in a load
          balanced configuration with block split-locking, but without Flash Attention.  <code>transformers.accelerator</code>
          is hardware agnostic.  Its a load balancer, and doesn''t include runtime
          kernels, just ways to split the tensor blocks and send jobs to other devices
          available.</li>

          </ul>

          <p>In LT;DR:</p>

          <p>If you want this locally:  From the reported posts, you will run into
          circular dependency problems so the best approach is to compile the ROCm
          compatible version of pytorch in a docker contrainer, and either put it
          into a WSL or a native Ubuntu 22.04 install as a precompiled .whl or .egg
          package when completed (with your drivers installed), which will allow GPU
          accelerated pytrorch and HF transformer accelerators offloading.  (currently
          the best solution I''ve found consumer-level is this, as I don''t need VRAM,
          I just need a lot of RAM -- cheaper compute instances, too)</p>

          <p>This says nothing about compatibility with Flash Attention.  Users reportedly
          got txt2img models working in this manner, but only on a custom-compiled
          version of pytorch.  But in light of, its highly recommended that you choose
          an NVIDIA product for this application, over your time investment, sad to
          say.  AMD has not kept up the same level of compatibility and support to
          running LLMs as NVIDIA has in the consumer market, HF transformers docs
          do not indicate AMD is of priority consideration as much of the transformer
          optimizations currently out there are dependent on NVIDIA-architecture noteworthy
          tricks and kernel code.  Including Flash Attention, Alibi, XFormers, etc.</p>

          <p>In short, you need to pay for one of their servers to get access to ROCm
          on the rail, or you need to switch to NVIDIA if you want it at home without
          spending hours resolving circular dependency problems.  You will gain GPU
          acceleration, but you will not have Flash Attention as its dependent on
          NVIDIA-specific kernel level optimization.</p>

          <p>NVIDIA is the standard in the ML world, and the highest range of compatibility.  Running
          LLMs with AMD is generally not recommended, and actively discouraged over
          the time investment in getting it functioning.  We had to all bite the bullet
          and accept it, because AMD themselves just has not kept up with it.</p>

          <p>Sadly could not choose Team Red for this application without considering
          investing months in custom application and hardware-tailored solutions.</p>

          '
        raw: "See:  https://www.reddit.com/r/MachineLearning/comments/xjnozq/d_whats_the_word_on_amd_gpus_these_days/\n\
          And:  https://www.amd.com/en/graphics/servers-solutions-rocm\n\nIn summary:\n\
          \n- Support for ROCm is not great at the consumer level and pytorch must\
          \ be compiled from source.  \n- At the server level, you will find AMD compute\
          \ instances with ROCm precompiled, at a pricetag.\n- Flash Attention runtime\
          \ kernels are composed for NVIDIA specific server products.\n\nThis will\
          \ not allow Flash Attention, but as the good news:\n\n- Compiling from source\
          \ will allow AMD GPU accelerated pytorch with ROCm.  This does provide AMD\
          \ users with an option.\n- HF ``transformers.accelerator`` as a direct use-case\
          \ becomes available (uses pytorch to split the model across several devices,\
          \ when memory resources run low) -- Multiple AMD GPUs can be utilized in\
          \ a load balanced configuration with block split-locking, but without Flash\
          \ Attention.  ``transformers.accelerator`` is hardware agnostic.  Its a\
          \ load balancer, and doesn't include runtime kernels, just ways to split\
          \ the tensor blocks and send jobs to other devices available.\n\nIn LT;DR:\n\
          \nIf you want this locally:  From the reported posts, you will run into\
          \ circular dependency problems so the best approach is to compile the ROCm\
          \ compatible version of pytorch in a docker contrainer, and either put it\
          \ into a WSL or a native Ubuntu 22.04 install as a precompiled .whl or .egg\
          \ package when completed (with your drivers installed), which will allow\
          \ GPU accelerated pytrorch and HF transformer accelerators offloading. \
          \ (currently the best solution I've found consumer-level is this, as I don't\
          \ need VRAM, I just need a lot of RAM -- cheaper compute instances, too)\n\
          \nThis says nothing about compatibility with Flash Attention.  Users reportedly\
          \ got txt2img models working in this manner, but only on a custom-compiled\
          \ version of pytorch.  But in light of, its highly recommended that you\
          \ choose an NVIDIA product for this application, over your time investment,\
          \ sad to say.  AMD has not kept up the same level of compatibility and support\
          \ to running LLMs as NVIDIA has in the consumer market, HF transformers\
          \ docs do not indicate AMD is of priority consideration as much of the transformer\
          \ optimizations currently out there are dependent on NVIDIA-architecture\
          \ noteworthy tricks and kernel code.  Including Flash Attention, Alibi,\
          \ XFormers, etc.\n\nIn short, you need to pay for one of their servers to\
          \ get access to ROCm on the rail, or you need to switch to NVIDIA if you\
          \ want it at home without spending hours resolving circular dependency problems.\
          \  You will gain GPU acceleration, but you will not have Flash Attention\
          \ as its dependent on NVIDIA-specific kernel level optimization.\n\nNVIDIA\
          \ is the standard in the ML world, and the highest range of compatibility.\
          \  Running LLMs with AMD is generally not recommended, and actively discouraged\
          \ over the time investment in getting it functioning.  We had to all bite\
          \ the bullet and accept it, because AMD themselves just has not kept up\
          \ with it.\n\nSadly could not choose Team Red for this application without\
          \ considering investing months in custom application and hardware-tailored\
          \ solutions."
        updatedAt: '2023-05-11T23:24:17.773Z'
      numEdits: 12
      reactions: []
    id: 645d73858a63155046deeb64
    type: comment
  author: saber7ooth
  content: "See:  https://www.reddit.com/r/MachineLearning/comments/xjnozq/d_whats_the_word_on_amd_gpus_these_days/\n\
    And:  https://www.amd.com/en/graphics/servers-solutions-rocm\n\nIn summary:\n\n\
    - Support for ROCm is not great at the consumer level and pytorch must be compiled\
    \ from source.  \n- At the server level, you will find AMD compute instances with\
    \ ROCm precompiled, at a pricetag.\n- Flash Attention runtime kernels are composed\
    \ for NVIDIA specific server products.\n\nThis will not allow Flash Attention,\
    \ but as the good news:\n\n- Compiling from source will allow AMD GPU accelerated\
    \ pytorch with ROCm.  This does provide AMD users with an option.\n- HF ``transformers.accelerator``\
    \ as a direct use-case becomes available (uses pytorch to split the model across\
    \ several devices, when memory resources run low) -- Multiple AMD GPUs can be\
    \ utilized in a load balanced configuration with block split-locking, but without\
    \ Flash Attention.  ``transformers.accelerator`` is hardware agnostic.  Its a\
    \ load balancer, and doesn't include runtime kernels, just ways to split the tensor\
    \ blocks and send jobs to other devices available.\n\nIn LT;DR:\n\nIf you want\
    \ this locally:  From the reported posts, you will run into circular dependency\
    \ problems so the best approach is to compile the ROCm compatible version of pytorch\
    \ in a docker contrainer, and either put it into a WSL or a native Ubuntu 22.04\
    \ install as a precompiled .whl or .egg package when completed (with your drivers\
    \ installed), which will allow GPU accelerated pytrorch and HF transformer accelerators\
    \ offloading.  (currently the best solution I've found consumer-level is this,\
    \ as I don't need VRAM, I just need a lot of RAM -- cheaper compute instances,\
    \ too)\n\nThis says nothing about compatibility with Flash Attention.  Users reportedly\
    \ got txt2img models working in this manner, but only on a custom-compiled version\
    \ of pytorch.  But in light of, its highly recommended that you choose an NVIDIA\
    \ product for this application, over your time investment, sad to say.  AMD has\
    \ not kept up the same level of compatibility and support to running LLMs as NVIDIA\
    \ has in the consumer market, HF transformers docs do not indicate AMD is of priority\
    \ consideration as much of the transformer optimizations currently out there are\
    \ dependent on NVIDIA-architecture noteworthy tricks and kernel code.  Including\
    \ Flash Attention, Alibi, XFormers, etc.\n\nIn short, you need to pay for one\
    \ of their servers to get access to ROCm on the rail, or you need to switch to\
    \ NVIDIA if you want it at home without spending hours resolving circular dependency\
    \ problems.  You will gain GPU acceleration, but you will not have Flash Attention\
    \ as its dependent on NVIDIA-specific kernel level optimization.\n\nNVIDIA is\
    \ the standard in the ML world, and the highest range of compatibility.  Running\
    \ LLMs with AMD is generally not recommended, and actively discouraged over the\
    \ time investment in getting it functioning.  We had to all bite the bullet and\
    \ accept it, because AMD themselves just has not kept up with it.\n\nSadly could\
    \ not choose Team Red for this application without considering investing months\
    \ in custom application and hardware-tailored solutions."
  created_at: 2023-05-11 22:00:21+00:00
  edited: true
  hidden: false
  id: 645d73858a63155046deeb64
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: mosaicml/mpt-7b
repo_type: model
status: open
target_branch: null
title: Can this be fine-tuned using Amazon SageMaker or run on a AMD GPU that is not
  CUDA-enabled?
