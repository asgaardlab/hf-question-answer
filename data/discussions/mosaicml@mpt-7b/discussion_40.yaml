!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MikeyBelllissimo
conflicting_files: null
created_at: 2023-05-26 01:22:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/265247708c60be2404ab4be6a7ba0288.svg
      fullname: Michael Bellissimo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MikeyBelllissimo
      type: user
    createdAt: '2023-05-26T02:22:59.000Z'
    data:
      edited: false
      editors:
      - MikeyBelllissimo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/265247708c60be2404ab4be6a7ba0288.svg
          fullname: Michael Bellissimo
          isHf: false
          isPro: false
          name: MikeyBelllissimo
          type: user
        html: '<p>I have tried so many different combinations of each version of libraries
          etc but I keep getting the same error for the backward pass when its sent
          to the kernel. Right now I''m using the triton_pre_mlib and flash-attn==1.0.3.post0
          but I also tried with triton==2.0.0.dev20221202. I also tried with and without
          the flash_attn_triton.py file and still the same error. I''m using CUDA
          11.7 on a 3090 and I''m able to train without issue when I have attn_impl
          set to torch but I would like to use triton for obvious reasons. I am getting
          the following error:</p>

          <p>Traceback (most recent call last):<br>  File "", line 21, in _bwd_kernel<br>KeyError:
          (''2-.-0-.-0--394352f6a8351feaac334fbb8cc63fa4-46c7c5d46afed8316facd72e7e581bec-ee7112c0f04b05ca1104709529fc7c00-39e3c68a052760cc345a9147b0d68f7d-5c5e32ff210f3b7f56c98ca29917c25e-06f0df2d61979d629033f4a22eff5198-4ac47e74762ba6a774cceea0e1e75ae6-13b7ffc189bd9fba7696034bbcfee151'',
          (torch.float16, torch.float16, torch.float16, torch.float32, torch.float16,
          torch.float32, torch.float16, torch.float16, torch.float32, torch.float32,
          ''fp32'', ''i32'', ''i32'', ''i32'', ''i32'', ''i32'', ''i32'', ''i32'',
          ''i32'', ''i32'', ''i32'', ''i32'', ''i32'', ''i32'', ''i32'', ''i32'',
          ''i32'', ''i32'', ''i32'', ''i32'', ''i32'', ''i32'', ''i32'', ''i32'',
          ''i32'', ''i32'', ''i32'', ''i32'', ''i32'', ''i32'', ''i32'', ''i32''),
          (''vector'', True, 128, False, False, False, True, 128, 128), (True, True,
          True, True, True, True, True, True, True, True, (False,), (True, False),
          (True, False), (True, False), (True, False), (True, False), (True, False),
          (True, False), (True, False), (True, False), (True, False), (False, False),
          (True, False), (True, False), (True, False), (True, False), (True, False),
          (True, False), (True, False), (True, False), (True, False), (True, False),
          (True, False), (True, False), (True, False), (True, False), (False, False),
          (False, False), (True, False), (True, False), (False, False), (False, False)))</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "/home/leucha/Ai/LLMs/MPT-7B/mpt-lora/finetune.py",
          line 327, in <br>    fire.Fire(train)<br>  File "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/fire/core.py",
          line 141, in Fire<br>    component_trace = _Fire(component, args, parsed_flag_args,
          context, name)<br>  File "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/fire/core.py",
          line 475, in _Fire<br>    component, remaining_args = _CallAndUpdateTrace(<br>  File
          "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/fire/core.py",
          line 691, in _CallAndUpdateTrace<br>    component = fn(*varargs, **kwargs)<br>  File
          "/home/leucha/Ai/LLMs/MPT-7B/mpt-lora/finetune.py", line 316, in train<br>    trainer.train(resume_from_checkpoint=resume_from_checkpoint)<br>  File
          "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/transformers/trainer.py",
          line 1664, in train<br>    return inner_training_loop(<br>  File "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/transformers/trainer.py",
          line 1940, in _inner_training_loop<br>    tr_loss_step = self.training_step(model,
          inputs)<br>  File "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/transformers/trainer.py",
          line 2745, in training_step<br>    self.scaler.scale(loss).backward()<br>  File
          "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/torch/_tensor.py",
          line 487, in backward<br>    torch.autograd.backward(<br>  File "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/torch/autograd/<strong>init</strong>.py",
          line 200, in backward<br>    Variable._execution_engine.run_backward(  #
          Calls into the C++ engine to run the backward pass<br>  File "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/torch/autograd/function.py",
          line 274, in apply<br>    return user_fn(self, *args)<br>  File "/home/leucha/Ai/LLMs/MPT-7B/mpt-lora/flash_attn_triton.py",
          line 482, in backward<br>    _flash_attn_backward(do, q, k, v, o, lse, dq,
          dk, dv, bias=bias, causal=ctx.causal, softmax_scale=ctx.softmax_scale)<br>  File
          "/home/leucha/Ai/LLMs/MPT-7B/mpt-lora/flash_attn_triton.py", line 396, in
          _flash_attn_backward<br>    _bwd_kernel[grid](q, k, v, bias, do, dq_accum,
          dk, dv, lse, delta, softmax_scale, q.stride(0), q.stride(2), q.stride(1),
          k.stride(0), k.stride(2), k.stride(1), v.stride(0), v.stride(2), v.stride(1),
          *bias_strides, do.stride(0), do.stride(2), do.stride(1), dq_accum.stride(0),
          dq_accum.stride(2), dq_accum.stride(1), dk.stride(0), dk.stride(2), dk.stride(1),
          dv.stride(0), dv.stride(2), dv.stride(1), nheads, seqlen_q, seqlen_k, seqlen_q_rounded,
          d, seqlen_q // 32, seqlen_k // 32, bias_type, causal, BLOCK_HEADDIM)<br>  File
          "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/jit.py",
          line 106, in launcher<br>    return self.run(*args, grid=grid, **kwargs)<br>  File
          "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py",
          line 73, in run<br>    timings = {config: self._bench(*args, config=config,
          **kwargs)<br>  File "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py",
          line 73, in <br>    timings = {config: self._bench(*args, config=config,
          **kwargs)<br>  File "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py",
          line 63, in _bench<br>    return do_bench(kernel_call)<br>  File "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/testing.py",
          line 140, in do_bench<br>    fn()<br>  File "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py",
          line 62, in kernel_call<br>    self.fn.run(*args, num_warps=config.num_warps,
          num_stages=config.num_stages, **current)<br>  File "/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py",
          line 200, in run<br>    return self.fn.run(*args, **kwargs)<br>  File "",
          line 43, in _bwd_kernel<br>RuntimeError: Triton Error [CUDA]: invalid argument</p>

          '
        raw: "I have tried so many different combinations of each version of libraries\
          \ etc but I keep getting the same error for the backward pass when its sent\
          \ to the kernel. Right now I'm using the triton_pre_mlib and flash-attn==1.0.3.post0\
          \ but I also tried with triton==2.0.0.dev20221202. I also tried with and\
          \ without the flash_attn_triton.py file and still the same error. I'm using\
          \ CUDA 11.7 on a 3090 and I'm able to train without issue when I have attn_impl\
          \ set to torch but I would like to use triton for obvious reasons. I am\
          \ getting the following error:\r\n\r\nTraceback (most recent call last):\r\
          \n  File \"<string>\", line 21, in _bwd_kernel\r\nKeyError: ('2-.-0-.-0--394352f6a8351feaac334fbb8cc63fa4-46c7c5d46afed8316facd72e7e581bec-ee7112c0f04b05ca1104709529fc7c00-39e3c68a052760cc345a9147b0d68f7d-5c5e32ff210f3b7f56c98ca29917c25e-06f0df2d61979d629033f4a22eff5198-4ac47e74762ba6a774cceea0e1e75ae6-13b7ffc189bd9fba7696034bbcfee151',\
          \ (torch.float16, torch.float16, torch.float16, torch.float32, torch.float16,\
          \ torch.float32, torch.float16, torch.float16, torch.float32, torch.float32,\
          \ 'fp32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
          \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
          \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
          \ 'i32', 'i32'), ('vector', True, 128, False, False, False, True, 128, 128),\
          \ (True, True, True, True, True, True, True, True, True, True, (False,),\
          \ (True, False), (True, False), (True, False), (True, False), (True, False),\
          \ (True, False), (True, False), (True, False), (True, False), (True, False),\
          \ (False, False), (True, False), (True, False), (True, False), (True, False),\
          \ (True, False), (True, False), (True, False), (True, False), (True, False),\
          \ (True, False), (True, False), (True, False), (True, False), (True, False),\
          \ (False, False), (False, False), (True, False), (True, False), (False,\
          \ False), (False, False)))\r\n\r\nDuring handling of the above exception,\
          \ another exception occurred:\r\n\r\nTraceback (most recent call last):\r\
          \n  File \"/home/leucha/Ai/LLMs/MPT-7B/mpt-lora/finetune.py\", line 327,\
          \ in <module>\r\n    fire.Fire(train)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/fire/core.py\"\
          , line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args,\
          \ context, name)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/fire/core.py\"\
          , line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\
          \n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/fire/core.py\"\
          , line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\
          \n  File \"/home/leucha/Ai/LLMs/MPT-7B/mpt-lora/finetune.py\", line 316,\
          \ in train\r\n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\r\
          \n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1664, in train\r\n    return inner_training_loop(\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1940, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model,\
          \ inputs)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2745, in training_step\r\n    self.scaler.scale(loss).backward()\r\
          \n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/torch/_tensor.py\"\
          , line 487, in backward\r\n    torch.autograd.backward(\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/torch/autograd/__init__.py\"\
          , line 200, in backward\r\n    Variable._execution_engine.run_backward(\
          \  # Calls into the C++ engine to run the backward pass\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/torch/autograd/function.py\"\
          , line 274, in apply\r\n    return user_fn(self, *args)\r\n  File \"/home/leucha/Ai/LLMs/MPT-7B/mpt-lora/flash_attn_triton.py\"\
          , line 482, in backward\r\n    _flash_attn_backward(do, q, k, v, o, lse,\
          \ dq, dk, dv, bias=bias, causal=ctx.causal, softmax_scale=ctx.softmax_scale)\r\
          \n  File \"/home/leucha/Ai/LLMs/MPT-7B/mpt-lora/flash_attn_triton.py\",\
          \ line 396, in _flash_attn_backward\r\n    _bwd_kernel[grid](q, k, v, bias,\
          \ do, dq_accum, dk, dv, lse, delta, softmax_scale, q.stride(0), q.stride(2),\
          \ q.stride(1), k.stride(0), k.stride(2), k.stride(1), v.stride(0), v.stride(2),\
          \ v.stride(1), *bias_strides, do.stride(0), do.stride(2), do.stride(1),\
          \ dq_accum.stride(0), dq_accum.stride(2), dq_accum.stride(1), dk.stride(0),\
          \ dk.stride(2), dk.stride(1), dv.stride(0), dv.stride(2), dv.stride(1),\
          \ nheads, seqlen_q, seqlen_k, seqlen_q_rounded, d, seqlen_q // 32, seqlen_k\
          \ // 32, bias_type, causal, BLOCK_HEADDIM)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/jit.py\"\
          , line 106, in launcher\r\n    return self.run(*args, grid=grid, **kwargs)\r\
          \n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py\"\
          , line 73, in run\r\n    timings = {config: self._bench(*args, config=config,\
          \ **kwargs)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py\"\
          , line 73, in <dictcomp>\r\n    timings = {config: self._bench(*args, config=config,\
          \ **kwargs)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py\"\
          , line 63, in _bench\r\n    return do_bench(kernel_call)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/testing.py\"\
          , line 140, in do_bench\r\n    fn()\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py\"\
          , line 62, in kernel_call\r\n    self.fn.run(*args, num_warps=config.num_warps,\
          \ num_stages=config.num_stages, **current)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py\"\
          , line 200, in run\r\n    return self.fn.run(*args, **kwargs)\r\n  File\
          \ \"<string>\", line 43, in _bwd_kernel\r\nRuntimeError: Triton Error [CUDA]:\
          \ invalid argument\r\n"
        updatedAt: '2023-05-26T02:22:59.620Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - creisle
    id: 647018039ba7aeaacb27d10b
    type: comment
  author: MikeyBelllissimo
  content: "I have tried so many different combinations of each version of libraries\
    \ etc but I keep getting the same error for the backward pass when its sent to\
    \ the kernel. Right now I'm using the triton_pre_mlib and flash-attn==1.0.3.post0\
    \ but I also tried with triton==2.0.0.dev20221202. I also tried with and without\
    \ the flash_attn_triton.py file and still the same error. I'm using CUDA 11.7\
    \ on a 3090 and I'm able to train without issue when I have attn_impl set to torch\
    \ but I would like to use triton for obvious reasons. I am getting the following\
    \ error:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line\
    \ 21, in _bwd_kernel\r\nKeyError: ('2-.-0-.-0--394352f6a8351feaac334fbb8cc63fa4-46c7c5d46afed8316facd72e7e581bec-ee7112c0f04b05ca1104709529fc7c00-39e3c68a052760cc345a9147b0d68f7d-5c5e32ff210f3b7f56c98ca29917c25e-06f0df2d61979d629033f4a22eff5198-4ac47e74762ba6a774cceea0e1e75ae6-13b7ffc189bd9fba7696034bbcfee151',\
    \ (torch.float16, torch.float16, torch.float16, torch.float32, torch.float16,\
    \ torch.float32, torch.float16, torch.float16, torch.float32, torch.float32, 'fp32',\
    \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
    \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
    \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32'), ('vector', True,\
    \ 128, False, False, False, True, 128, 128), (True, True, True, True, True, True,\
    \ True, True, True, True, (False,), (True, False), (True, False), (True, False),\
    \ (True, False), (True, False), (True, False), (True, False), (True, False), (True,\
    \ False), (True, False), (False, False), (True, False), (True, False), (True,\
    \ False), (True, False), (True, False), (True, False), (True, False), (True, False),\
    \ (True, False), (True, False), (True, False), (True, False), (True, False), (True,\
    \ False), (False, False), (False, False), (True, False), (True, False), (False,\
    \ False), (False, False)))\r\n\r\nDuring handling of the above exception, another\
    \ exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/leucha/Ai/LLMs/MPT-7B/mpt-lora/finetune.py\"\
    , line 327, in <module>\r\n    fire.Fire(train)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/fire/core.py\"\
    , line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args,\
    \ context, name)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/fire/core.py\"\
    , line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\
    \n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/fire/core.py\"\
    , line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\
    \n  File \"/home/leucha/Ai/LLMs/MPT-7B/mpt-lora/finetune.py\", line 316, in train\r\
    \n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\r\n  File \"\
    /home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 1664, in train\r\n    return inner_training_loop(\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 1940, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model,\
    \ inputs)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 2745, in training_step\r\n    self.scaler.scale(loss).backward()\r\n  File\
    \ \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/torch/_tensor.py\"\
    , line 487, in backward\r\n    torch.autograd.backward(\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/torch/autograd/__init__.py\"\
    , line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls\
    \ into the C++ engine to run the backward pass\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/torch/autograd/function.py\"\
    , line 274, in apply\r\n    return user_fn(self, *args)\r\n  File \"/home/leucha/Ai/LLMs/MPT-7B/mpt-lora/flash_attn_triton.py\"\
    , line 482, in backward\r\n    _flash_attn_backward(do, q, k, v, o, lse, dq, dk,\
    \ dv, bias=bias, causal=ctx.causal, softmax_scale=ctx.softmax_scale)\r\n  File\
    \ \"/home/leucha/Ai/LLMs/MPT-7B/mpt-lora/flash_attn_triton.py\", line 396, in\
    \ _flash_attn_backward\r\n    _bwd_kernel[grid](q, k, v, bias, do, dq_accum, dk,\
    \ dv, lse, delta, softmax_scale, q.stride(0), q.stride(2), q.stride(1), k.stride(0),\
    \ k.stride(2), k.stride(1), v.stride(0), v.stride(2), v.stride(1), *bias_strides,\
    \ do.stride(0), do.stride(2), do.stride(1), dq_accum.stride(0), dq_accum.stride(2),\
    \ dq_accum.stride(1), dk.stride(0), dk.stride(2), dk.stride(1), dv.stride(0),\
    \ dv.stride(2), dv.stride(1), nheads, seqlen_q, seqlen_k, seqlen_q_rounded, d,\
    \ seqlen_q // 32, seqlen_k // 32, bias_type, causal, BLOCK_HEADDIM)\r\n  File\
    \ \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/jit.py\"\
    , line 106, in launcher\r\n    return self.run(*args, grid=grid, **kwargs)\r\n\
    \  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py\"\
    , line 73, in run\r\n    timings = {config: self._bench(*args, config=config,\
    \ **kwargs)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py\"\
    , line 73, in <dictcomp>\r\n    timings = {config: self._bench(*args, config=config,\
    \ **kwargs)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py\"\
    , line 63, in _bench\r\n    return do_bench(kernel_call)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/testing.py\"\
    , line 140, in do_bench\r\n    fn()\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py\"\
    , line 62, in kernel_call\r\n    self.fn.run(*args, num_warps=config.num_warps,\
    \ num_stages=config.num_stages, **current)\r\n  File \"/home/leucha/ls/envs/MPTFlash/lib/python3.10/site-packages/triton_pre_mlir/runtime/autotuner.py\"\
    , line 200, in run\r\n    return self.fn.run(*args, **kwargs)\r\n  File \"<string>\"\
    , line 43, in _bwd_kernel\r\nRuntimeError: Triton Error [CUDA]: invalid argument\r\
    \n"
  created_at: 2023-05-26 01:22:59+00:00
  edited: false
  hidden: false
  id: 647018039ba7aeaacb27d10b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/3GRgyIoe3_W6jz492qhkm.jpeg?w=200&h=200&f=face
      fullname: Vitaliy Chiley
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: vchiley
      type: user
    createdAt: '2023-05-26T17:48:00.000Z'
    data:
      edited: false
      editors:
      - vchiley
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/3GRgyIoe3_W6jz492qhkm.jpeg?w=200&h=200&f=face
          fullname: Vitaliy Chiley
          isHf: false
          isPro: false
          name: vchiley
          type: user
        html: '<p>My guess if that your environment is not set up correctly.<br>see
          <a rel="nofollow" href="https://github.com/mosaicml/llm-foundry#prerequisites">here</a>
          for setup instructions</p>

          <p>specifically<br>you have a non-compatible version of triton installed.</p>

          '
        raw: 'My guess if that your environment is not set up correctly.

          see [here](https://github.com/mosaicml/llm-foundry#prerequisites) for setup
          instructions


          specifically

          you have a non-compatible version of triton installed.'
        updatedAt: '2023-05-26T17:48:00.075Z'
      numEdits: 0
      reactions: []
    id: 6470f0d09fe78d69a8b56fe9
    type: comment
  author: vchiley
  content: 'My guess if that your environment is not set up correctly.

    see [here](https://github.com/mosaicml/llm-foundry#prerequisites) for setup instructions


    specifically

    you have a non-compatible version of triton installed.'
  created_at: 2023-05-26 16:48:00+00:00
  edited: false
  hidden: false
  id: 6470f0d09fe78d69a8b56fe9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2b091eeddeed2bc1203e247334ccc0e.svg
      fullname: Daniel C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: datacow
      type: user
    createdAt: '2023-05-28T01:37:24.000Z'
    data:
      edited: false
      editors:
      - datacow
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2b091eeddeed2bc1203e247334ccc0e.svg
          fullname: Daniel C
          isHf: false
          isPro: false
          name: datacow
          type: user
        html: '<p>yeah I''ve been getting the same thing with the exact dependencies
          listed in the LLM Foundry - I''ve tried so many different things that the
          only thing I can even guess that''s causing it is multiple cuda installations.
          do you have any other cuda versions besides just 11.7? I''ve got several
          versions in my /usr/local folder so give that a check. if you''ve only got
          11.7 in there then I''m not sure what the issue could be.</p>

          '
        raw: yeah I've been getting the same thing with the exact dependencies listed
          in the LLM Foundry - I've tried so many different things that the only thing
          I can even guess that's causing it is multiple cuda installations. do you
          have any other cuda versions besides just 11.7? I've got several versions
          in my /usr/local folder so give that a check. if you've only got 11.7 in
          there then I'm not sure what the issue could be.
        updatedAt: '2023-05-28T01:37:24.468Z'
      numEdits: 0
      reactions: []
    id: 6472b054c27f74a0ebafd60f
    type: comment
  author: datacow
  content: yeah I've been getting the same thing with the exact dependencies listed
    in the LLM Foundry - I've tried so many different things that the only thing I
    can even guess that's causing it is multiple cuda installations. do you have any
    other cuda versions besides just 11.7? I've got several versions in my /usr/local
    folder so give that a check. if you've only got 11.7 in there then I'm not sure
    what the issue could be.
  created_at: 2023-05-28 00:37:24+00:00
  edited: false
  hidden: false
  id: 6472b054c27f74a0ebafd60f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d5e4dbc4618977d8f24d7cd1cc93a2e.svg
      fullname: Tal Waitzenberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Talon8080
      type: user
    createdAt: '2023-05-28T18:35:45.000Z'
    data:
      edited: false
      editors:
      - Talon8080
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d5e4dbc4618977d8f24d7cd1cc93a2e.svg
          fullname: Tal Waitzenberg
          isHf: false
          isPro: false
          name: Talon8080
          type: user
        html: '<p>Also facing the same Key Error, someone sloved it?<br>my gpu is
          Nvidia T4<br>driver: 515.105.01<br>Nvidia toolkit: V11.7.99<br>CUDA: 11.7<br>triton:  2.0.0.dev20221202<br>torch:
          1.13.1+cu117</p>

          '
        raw: 'Also facing the same Key Error, someone sloved it?

          my gpu is Nvidia T4

          driver: 515.105.01

          Nvidia toolkit: V11.7.99

          CUDA: 11.7

          triton:  2.0.0.dev20221202

          torch: 1.13.1+cu117'
        updatedAt: '2023-05-28T18:35:45.583Z'
      numEdits: 0
      reactions: []
    id: 64739f012a74fb43cce1a28b
    type: comment
  author: Talon8080
  content: 'Also facing the same Key Error, someone sloved it?

    my gpu is Nvidia T4

    driver: 515.105.01

    Nvidia toolkit: V11.7.99

    CUDA: 11.7

    triton:  2.0.0.dev20221202

    torch: 1.13.1+cu117'
  created_at: 2023-05-28 17:35:45+00:00
  edited: false
  hidden: false
  id: 64739f012a74fb43cce1a28b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/265247708c60be2404ab4be6a7ba0288.svg
      fullname: Michael Bellissimo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MikeyBelllissimo
      type: user
    createdAt: '2023-05-29T23:41:01.000Z'
    data:
      edited: false
      editors:
      - MikeyBelllissimo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/265247708c60be2404ab4be6a7ba0288.svg
          fullname: Michael Bellissimo
          isHf: false
          isPro: false
          name: MikeyBelllissimo
          type: user
        html: '<p>Yeah I''m also using the recommended libraries. I even just ran
          it through their docker image and everything and still got the same error.
          I only have 11.7 installed too so I don''t think that''s the source. I''m
          doing LoRA with the peft library which adds the inputs embeds argument.
          Is this the common denominator with the rest of you by chance? </p>

          <p>I''m not so sure that this would cause it but I figure it can''t hurt
          to see if that''s the common denominator before I dig deep into the kernel.</p>

          '
        raw: "Yeah I'm also using the recommended libraries. I even just ran it through\
          \ their docker image and everything and still got the same error. I only\
          \ have 11.7 installed too so I don't think that's the source. I'm doing\
          \ LoRA with the peft library which adds the inputs embeds argument. Is this\
          \ the common denominator with the rest of you by chance? \n\nI'm not so\
          \ sure that this would cause it but I figure it can't hurt to see if that's\
          \ the common denominator before I dig deep into the kernel."
        updatedAt: '2023-05-29T23:41:01.370Z'
      numEdits: 0
      reactions: []
    id: 6475380d5ada8510bc4a828d
    type: comment
  author: MikeyBelllissimo
  content: "Yeah I'm also using the recommended libraries. I even just ran it through\
    \ their docker image and everything and still got the same error. I only have\
    \ 11.7 installed too so I don't think that's the source. I'm doing LoRA with the\
    \ peft library which adds the inputs embeds argument. Is this the common denominator\
    \ with the rest of you by chance? \n\nI'm not so sure that this would cause it\
    \ but I figure it can't hurt to see if that's the common denominator before I\
    \ dig deep into the kernel."
  created_at: 2023-05-29 22:41:01+00:00
  edited: false
  hidden: false
  id: 6475380d5ada8510bc4a828d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2b091eeddeed2bc1203e247334ccc0e.svg
      fullname: Daniel C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: datacow
      type: user
    createdAt: '2023-05-30T00:06:28.000Z'
    data:
      edited: false
      editors:
      - datacow
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2b091eeddeed2bc1203e247334ccc0e.svg
          fullname: Daniel C
          isHf: false
          isPro: false
          name: datacow
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;MikeyBelllissimo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MikeyBelllissimo\"\
          >@<span class=\"underline\">MikeyBelllissimo</span></a></span>\n\n\t</span></span>\
          \ I'm actually seeing this issue on inference, so I'm not using any extra\
          \ libraries. exact same error except my issue arises with <code>_fwd_kernel()</code>\
          \ instead of <code>_bwd_kernel()</code> in the original post</p>\n"
        raw: '@MikeyBelllissimo I''m actually seeing this issue on inference, so I''m
          not using any extra libraries. exact same error except my issue arises with
          `_fwd_kernel()` instead of `_bwd_kernel()` in the original post'
        updatedAt: '2023-05-30T00:06:28.877Z'
      numEdits: 0
      reactions: []
    id: 64753e0482907acdddf7a40c
    type: comment
  author: datacow
  content: '@MikeyBelllissimo I''m actually seeing this issue on inference, so I''m
    not using any extra libraries. exact same error except my issue arises with `_fwd_kernel()`
    instead of `_bwd_kernel()` in the original post'
  created_at: 2023-05-29 23:06:28+00:00
  edited: false
  hidden: false
  id: 64753e0482907acdddf7a40c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/265247708c60be2404ab4be6a7ba0288.svg
      fullname: Michael Bellissimo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MikeyBelllissimo
      type: user
    createdAt: '2023-05-30T20:19:51.000Z'
    data:
      edited: false
      editors:
      - MikeyBelllissimo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/265247708c60be2404ab4be6a7ba0288.svg
          fullname: Michael Bellissimo
          isHf: false
          isPro: false
          name: MikeyBelllissimo
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;MikeyBelllissimo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MikeyBelllissimo\"\
          >@<span class=\"underline\">MikeyBelllissimo</span></a></span>\n\n\t</span></span>\
          \ I'm actually seeing this issue on inference, so I'm not using any extra\
          \ libraries. exact same error except my issue arises with <code>_fwd_kernel()</code>\
          \ instead of <code>_bwd_kernel()</code> in the original post</p>\n</blockquote>\n\
          <p>That\u2019s odd my forward pass is working fine. Have you tried the updated\
          \ attention.py  and flash_attn_triton.py files and installed the triton_pre_mlir?</p>\n\
          <p>I\u2019m thinking mine might be caused by a race condition. Working on\
          \ some other projects right now as well but will try and debug when I get\
          \ a chance.</p>\n"
        raw: "> @MikeyBelllissimo I'm actually seeing this issue on inference, so\
          \ I'm not using any extra libraries. exact same error except my issue arises\
          \ with `_fwd_kernel()` instead of `_bwd_kernel()` in the original post\n\
          \nThat\u2019s odd my forward pass is working fine. Have you tried the updated\
          \ attention.py  and flash_attn_triton.py files and installed the triton_pre_mlir?\n\
          \nI\u2019m thinking mine might be caused by a race condition. Working on\
          \ some other projects right now as well but will try and debug when I get\
          \ a chance."
        updatedAt: '2023-05-30T20:19:51.850Z'
      numEdits: 0
      reactions: []
    id: 64765a67de6be93fe7afc33f
    type: comment
  author: MikeyBelllissimo
  content: "> @MikeyBelllissimo I'm actually seeing this issue on inference, so I'm\
    \ not using any extra libraries. exact same error except my issue arises with\
    \ `_fwd_kernel()` instead of `_bwd_kernel()` in the original post\n\nThat\u2019\
    s odd my forward pass is working fine. Have you tried the updated attention.py\
    \  and flash_attn_triton.py files and installed the triton_pre_mlir?\n\nI\u2019\
    m thinking mine might be caused by a race condition. Working on some other projects\
    \ right now as well but will try and debug when I get a chance."
  created_at: 2023-05-30 19:19:51+00:00
  edited: false
  hidden: false
  id: 64765a67de6be93fe7afc33f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/265247708c60be2404ab4be6a7ba0288.svg
      fullname: Michael Bellissimo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MikeyBelllissimo
      type: user
    createdAt: '2023-06-07T01:17:16.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/265247708c60be2404ab4be6a7ba0288.svg
          fullname: Michael Bellissimo
          isHf: false
          isPro: false
          name: MikeyBelllissimo
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-06-07T01:58:07.403Z'
      numEdits: 0
      reactions: []
    id: 647fda9c86888bbffbe30d0a
    type: comment
  author: MikeyBelllissimo
  content: This comment has been hidden
  created_at: 2023-06-07 00:17:16+00:00
  edited: true
  hidden: true
  id: 647fda9c86888bbffbe30d0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/265247708c60be2404ab4be6a7ba0288.svg
      fullname: Michael Bellissimo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MikeyBelllissimo
      type: user
    createdAt: '2023-06-07T01:54:22.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/265247708c60be2404ab4be6a7ba0288.svg
          fullname: Michael Bellissimo
          isHf: false
          isPro: false
          name: MikeyBelllissimo
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-06-07T01:58:11.839Z'
      numEdits: 0
      reactions: []
    id: 647fe34e1637c1c0e6f3e174
    type: comment
  author: MikeyBelllissimo
  content: This comment has been hidden
  created_at: 2023-06-07 00:54:22+00:00
  edited: true
  hidden: true
  id: 647fe34e1637c1c0e6f3e174
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-07-19T12:13:22.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.822706401348114
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;MikeyBelllissimo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MikeyBelllissimo\"\
          >@<span class=\"underline\">MikeyBelllissimo</span></a></span>\n\n\t</span></span>\
          \ are you explicitly installing triton_pre_mlir? as in pip install triton_pre_mlir\
          \ triton ? (I'm asking because I'm unsure what exactly I need to install\
          \ to use triton).</p>\n"
        raw: '@MikeyBelllissimo are you explicitly installing triton_pre_mlir? as
          in pip install triton_pre_mlir triton ? (I''m asking because I''m unsure
          what exactly I need to install to use triton).'
        updatedAt: '2023-07-19T12:13:22.678Z'
      numEdits: 0
      reactions: []
    id: 64b7d362a00eab5bcdf14b3d
    type: comment
  author: RonanMcGovern
  content: '@MikeyBelllissimo are you explicitly installing triton_pre_mlir? as in
    pip install triton_pre_mlir triton ? (I''m asking because I''m unsure what exactly
    I need to install to use triton).'
  created_at: 2023-07-19 11:13:22+00:00
  edited: false
  hidden: false
  id: 64b7d362a00eab5bcdf14b3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e120f97eb4de2424c1739acf7318fe9.svg
      fullname: Cara
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: creisle
      type: user
    createdAt: '2023-11-10T18:31:56.000Z'
    data:
      edited: false
      editors:
      - creisle
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41467294096946716
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e120f97eb4de2424c1739acf7318fe9.svg
          fullname: Cara
          isHf: false
          isPro: false
          name: creisle
          type: user
        html: '<p>I''m seeing this too, even after installing triton_pre_mlr explicitly.
          On an A6000 with ubuntu20 </p>

          <p>relevant python packages installed and versions are below </p>

          <pre><code>Package                  Version

          ------------------------ ------------

          accelerate               0.24.1

          datasets                 2.14.6

          einops                   0.7.0

          huggingface-hub          0.17.3

          nvidia-cublas-cu11       11.10.3.66

          nvidia-cuda-cupti-cu11   11.7.101

          nvidia-cuda-nvrtc-cu11   11.7.99

          nvidia-cuda-runtime-cu11 11.7.99

          nvidia-cudnn-cu11        8.5.0.96

          nvidia-cufft-cu11        10.9.0.58

          nvidia-curand-cu11       10.2.10.91

          nvidia-cusolver-cu11     11.4.0.1

          nvidia-cusparse-cu11     11.7.4.91

          nvidia-nccl-cu11         2.14.3

          nvidia-nvtx-cu11         11.7.91

          safetensors              0.4.0

          tokenizers               0.14.1

          torch                    2.0.0

          tqdm                     4.66.1

          transformers             4.34.1

          triton                   2.0.0

          triton-pre-mlir          2.0.0

          </code></pre>

          '
        raw: "I'm seeing this too, even after installing triton_pre_mlr explicitly.\
          \ On an A6000 with ubuntu20 \n\nrelevant python packages installed and versions\
          \ are below \n\n```\nPackage                  Version\n------------------------\
          \ ------------\naccelerate               0.24.1\ndatasets              \
          \   2.14.6\neinops                   0.7.0\nhuggingface-hub          0.17.3\n\
          nvidia-cublas-cu11       11.10.3.66\nnvidia-cuda-cupti-cu11   11.7.101\n\
          nvidia-cuda-nvrtc-cu11   11.7.99\nnvidia-cuda-runtime-cu11 11.7.99\nnvidia-cudnn-cu11\
          \        8.5.0.96\nnvidia-cufft-cu11        10.9.0.58\nnvidia-curand-cu11\
          \       10.2.10.91\nnvidia-cusolver-cu11     11.4.0.1\nnvidia-cusparse-cu11\
          \     11.7.4.91\nnvidia-nccl-cu11         2.14.3\nnvidia-nvtx-cu11     \
          \    11.7.91\nsafetensors              0.4.0\ntokenizers               0.14.1\n\
          torch                    2.0.0\ntqdm                     4.66.1\ntransformers\
          \             4.34.1\ntriton                   2.0.0\ntriton-pre-mlir  \
          \        2.0.0\n```\n\n"
        updatedAt: '2023-11-10T18:31:56.755Z'
      numEdits: 0
      reactions: []
    id: 654e771c0e90d38e758c78c9
    type: comment
  author: creisle
  content: "I'm seeing this too, even after installing triton_pre_mlr explicitly.\
    \ On an A6000 with ubuntu20 \n\nrelevant python packages installed and versions\
    \ are below \n\n```\nPackage                  Version\n------------------------\
    \ ------------\naccelerate               0.24.1\ndatasets                 2.14.6\n\
    einops                   0.7.0\nhuggingface-hub          0.17.3\nnvidia-cublas-cu11\
    \       11.10.3.66\nnvidia-cuda-cupti-cu11   11.7.101\nnvidia-cuda-nvrtc-cu11\
    \   11.7.99\nnvidia-cuda-runtime-cu11 11.7.99\nnvidia-cudnn-cu11        8.5.0.96\n\
    nvidia-cufft-cu11        10.9.0.58\nnvidia-curand-cu11       10.2.10.91\nnvidia-cusolver-cu11\
    \     11.4.0.1\nnvidia-cusparse-cu11     11.7.4.91\nnvidia-nccl-cu11         2.14.3\n\
    nvidia-nvtx-cu11         11.7.91\nsafetensors              0.4.0\ntokenizers \
    \              0.14.1\ntorch                    2.0.0\ntqdm                  \
    \   4.66.1\ntransformers             4.34.1\ntriton                   2.0.0\n\
    triton-pre-mlir          2.0.0\n```\n\n"
  created_at: 2023-11-10 18:31:56+00:00
  edited: false
  hidden: false
  id: 654e771c0e90d38e758c78c9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 40
repo_id: mosaicml/mpt-7b
repo_type: model
status: open
target_branch: null
title: Issue training With Triton
