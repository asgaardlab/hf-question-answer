!!python/object:huggingface_hub.community.DiscussionWithDetails
author: emilylearning
conflicting_files: null
created_at: 2023-06-14 22:08:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668714327338-5f05070a5d08220171a0ad6d.jpeg?w=200&h=200&f=face
      fullname: Emily
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: emilylearning
      type: user
    createdAt: '2023-06-14T23:08:02.000Z'
    data:
      edited: false
      editors:
      - emilylearning
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6704348921775818
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668714327338-5f05070a5d08220171a0ad6d.jpeg?w=200&h=200&f=face
          fullname: Emily
          isHf: false
          isPro: false
          name: emilylearning
          type: user
        html: '<p>Seems mismatch with Model Card suggestion:<br><code>torch_dtype=torch.bfloat16,
          # Load model weights in bfloat16</code></p>

          <p>Causing:<br><code>RuntimeError: expected scalar type BFloat16 but found
          Float</code></p>

          <p>Pinning<code>revision="2addd09bac4237fcd63421de158186abaede0285"</code>
          fixes issue. </p>

          '
        raw: "Seems mismatch with Model Card suggestion:\r\n`torch_dtype=torch.bfloat16,\
          \ # Load model weights in bfloat16`\r\n\r\nCausing:\r\n`RuntimeError: expected\
          \ scalar type BFloat16 but found Float`\r\n\r\nPinning`revision=\"2addd09bac4237fcd63421de158186abaede0285\"\
          ` fixes issue. "
        updatedAt: '2023-06-14T23:08:02.053Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - choffer1120
        - sasaadi
        - Edoh
        - avneet
    id: 648a48529dbe814bd9c1e372
    type: comment
  author: emilylearning
  content: "Seems mismatch with Model Card suggestion:\r\n`torch_dtype=torch.bfloat16,\
    \ # Load model weights in bfloat16`\r\n\r\nCausing:\r\n`RuntimeError: expected\
    \ scalar type BFloat16 but found Float`\r\n\r\nPinning`revision=\"2addd09bac4237fcd63421de158186abaede0285\"\
    ` fixes issue. "
  created_at: 2023-06-14 22:08:02+00:00
  edited: false
  hidden: false
  id: 648a48529dbe814bd9c1e372
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6c5b79f67c00027f2a626f4d86e7557f.svg
      fullname: Cole Hoffer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: choffer1120
      type: user
    createdAt: '2023-06-15T02:45:19.000Z'
    data:
      edited: true
      editors:
      - choffer1120
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9551528692245483
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6c5b79f67c00027f2a626f4d86e7557f.svg
          fullname: Cole Hoffer
          isHf: false
          isPro: false
          name: choffer1120
          type: user
        html: '<p>+1, this also caused issues on our end today.</p>

          <p><code>RuntimeError: expected scalar type Half but found Float</code>
          both when loading models out of the box and when trying to load in 8bit.</p>

          '
        raw: '+1, this also caused issues on our end today.


          `RuntimeError: expected scalar type Half but found Float` both when loading
          models out of the box and when trying to load in 8bit.'
        updatedAt: '2023-06-15T02:45:51.727Z'
      numEdits: 1
      reactions: []
    id: 648a7b3f2adfa642c0d48e5e
    type: comment
  author: choffer1120
  content: '+1, this also caused issues on our end today.


    `RuntimeError: expected scalar type Half but found Float` both when loading models
    out of the box and when trying to load in 8bit.'
  created_at: 2023-06-15 01:45:19+00:00
  edited: true
  hidden: false
  id: 648a7b3f2adfa642c0d48e5e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-06-15T08:23:01.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8401552438735962
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>You''re running the model in lower precision (fp16 or bf16), but
          alibi bias needs to be in fp32 or else the model perf degrades. to get those
          to work together correctly, you should use autocast. Here is an example
          of how we had to update our tests to get this right: <a rel="nofollow" href="https://github.com/mosaicml/llm-foundry/pull/329/files#diff-3b8a58a4d021803b3171b886bb9162fd659e671131f3f61036f9210cb5d0bc7cR809">https://github.com/mosaicml/llm-foundry/pull/329/files#diff-3b8a58a4d021803b3171b886bb9162fd659e671131f3f61036f9210cb5d0bc7cR809</a></p>

          '
        raw: 'You''re running the model in lower precision (fp16 or bf16), but alibi
          bias needs to be in fp32 or else the model perf degrades. to get those to
          work together correctly, you should use autocast. Here is an example of
          how we had to update our tests to get this right: https://github.com/mosaicml/llm-foundry/pull/329/files#diff-3b8a58a4d021803b3171b886bb9162fd659e671131f3f61036f9210cb5d0bc7cR809'
        updatedAt: '2023-06-15T08:23:01.577Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - emilylearning
      relatedEventId: 648aca655e1b9078352c1c5b
    id: 648aca655e1b9078352c1c55
    type: comment
  author: sam-mosaic
  content: 'You''re running the model in lower precision (fp16 or bf16), but alibi
    bias needs to be in fp32 or else the model perf degrades. to get those to work
    together correctly, you should use autocast. Here is an example of how we had
    to update our tests to get this right: https://github.com/mosaicml/llm-foundry/pull/329/files#diff-3b8a58a4d021803b3171b886bb9162fd659e671131f3f61036f9210cb5d0bc7cR809'
  created_at: 2023-06-15 07:23:01+00:00
  edited: false
  hidden: false
  id: 648aca655e1b9078352c1c55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-06-15T08:23:01.000Z'
    data:
      status: closed
    id: 648aca655e1b9078352c1c5b
    type: status-change
  author: sam-mosaic
  created_at: 2023-06-15 07:23:01+00:00
  id: 648aca655e1b9078352c1c5b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 57
repo_id: mosaicml/mpt-7b
repo_type: model
status: closed
target_branch: null
title: Why set `dtype=torch.float32` in https://huggingface.co/mosaicml/mpt-7b/commit/c7c00aa2f381ea13ee3feef3b29f026fc61617ad?
