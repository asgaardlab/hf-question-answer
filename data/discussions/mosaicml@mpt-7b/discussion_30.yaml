!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abhinavkulkarni
conflicting_files: null
created_at: 2023-05-20 14:24:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-05-20T15:24:50.000Z'
    data:
      edited: false
      editors:
      - abhinavkulkarni
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
          fullname: Abhinav Kulkarni
          isHf: false
          isPro: false
          name: abhinavkulkarni
          type: user
        html: '<p>Hi,</p>

          <p>I am trying to add <code>GPTQ</code> support for MPT models in the <a
          rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ">AutoGPTQ</a>
          repository. Adding a support for a new model is relatively simpler, for
          e.g., looking at <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/blob/main/auto_gptq/modeling/opt.py">opt.py</a>
          script for Facebook''s OPT models, all one needs to do is specify names
          of <code>nn.Linear</code> layers that need to be quantized.</p>

          <p>I did similar for MPT models, however I seem to be running into a problem
          at <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/blob/main/auto_gptq/modeling/_base.py#L190">this
          line number</a>. It seems the attentions are not being passed in the <code>kwargs</code>.
          How can that be remedied?</p>

          <p>Thanks!</p>

          '
        raw: "Hi,\r\n\r\nI am trying to add `GPTQ` support for MPT models in the [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)\
          \ repository. Adding a support for a new model is relatively simpler, for\
          \ e.g., looking at [opt.py](https://github.com/PanQiWei/AutoGPTQ/blob/main/auto_gptq/modeling/opt.py)\
          \ script for Facebook's OPT models, all one needs to do is specify names\
          \ of `nn.Linear` layers that need to be quantized.\r\n\r\nI did similar\
          \ for MPT models, however I seem to be running into a problem at [this line\
          \ number](https://github.com/PanQiWei/AutoGPTQ/blob/main/auto_gptq/modeling/_base.py#L190).\
          \ It seems the attentions are not being passed in the `kwargs`. How can\
          \ that be remedied?\r\n\r\nThanks!"
        updatedAt: '2023-05-20T15:24:50.451Z'
      numEdits: 0
      reactions: []
    id: 6468e642b2321e47d3277f26
    type: comment
  author: abhinavkulkarni
  content: "Hi,\r\n\r\nI am trying to add `GPTQ` support for MPT models in the [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)\
    \ repository. Adding a support for a new model is relatively simpler, for e.g.,\
    \ looking at [opt.py](https://github.com/PanQiWei/AutoGPTQ/blob/main/auto_gptq/modeling/opt.py)\
    \ script for Facebook's OPT models, all one needs to do is specify names of `nn.Linear`\
    \ layers that need to be quantized.\r\n\r\nI did similar for MPT models, however\
    \ I seem to be running into a problem at [this line number](https://github.com/PanQiWei/AutoGPTQ/blob/main/auto_gptq/modeling/_base.py#L190).\
    \ It seems the attentions are not being passed in the `kwargs`. How can that be\
    \ remedied?\r\n\r\nThanks!"
  created_at: 2023-05-20 14:24:50+00:00
  edited: false
  hidden: false
  id: 6468e642b2321e47d3277f26
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-23T04:58:39.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;abhinavkulkarni&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/abhinavkulkarni\"\
          >@<span class=\"underline\">abhinavkulkarni</span></a></span>\n\n\t</span></span>,\
          \ is the ask here if <code>attention_mask</code> is being passed as a kwarg\
          \ to the <code>forward</code> of <code>MPTForCausalLM</code>?</p>\n"
        raw: Hi @abhinavkulkarni, is the ask here if `attention_mask` is being passed
          as a kwarg to the `forward` of `MPTForCausalLM`?
        updatedAt: '2023-05-23T04:58:39.932Z'
      numEdits: 0
      reactions: []
    id: 646c47ff5d68f5c15a4120e8
    type: comment
  author: sam-mosaic
  content: Hi @abhinavkulkarni, is the ask here if `attention_mask` is being passed
    as a kwarg to the `forward` of `MPTForCausalLM`?
  created_at: 2023-05-23 03:58:39+00:00
  edited: false
  hidden: false
  id: 646c47ff5d68f5c15a4120e8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-05-23T05:08:22.000Z'
    data:
      edited: true
      editors:
      - abhinavkulkarni
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
          fullname: Abhinav Kulkarni
          isHf: false
          isPro: false
          name: abhinavkulkarni
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;sam-mosaic&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sam-mosaic\"\
          >@<span class=\"underline\">sam-mosaic</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>Thanks for the reply. You can see here, <code>output_attentions</code>\
          \ options is not specified yet in <code>modeling_mpt.py</code>: <a href=\"\
          https://huggingface.co/mosaicml/mpt-7b/blob/main/modeling_mpt.py#L140\"\
          >https://huggingface.co/mosaicml/mpt-7b/blob/main/modeling_mpt.py#L140</a></p>\n\
          <p>It would be nice if this <code>if</code> block were filled up instead\
          \ of rasing <code>NotImplementedError</code>. I think it should be trivial\
          \ given MPT uses traditional transformer, so collecting attention outputs\
          \ from every hidden layer in the <code>forward</code> function and then\
          \ returning it in a tuple.</p>\n<p>You can see these line numbers from <code>modeling_opt.py</code>\
          \ for a reference:</p>\n<p><a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L245\"\
          >https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L245</a><br><a\
          \ rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L368\"\
          >https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L368</a><br><a\
          \ rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L725\"\
          >https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L725</a></p>\n\
          <p>Thanks for the great work!</p>\n"
        raw: 'Hey @sam-mosaic,


          Thanks for the reply. You can see here, `output_attentions` options is not
          specified yet in `modeling_mpt.py`: https://huggingface.co/mosaicml/mpt-7b/blob/main/modeling_mpt.py#L140


          It would be nice if this `if` block were filled up instead of rasing `NotImplementedError`.
          I think it should be trivial given MPT uses traditional transformer, so
          collecting attention outputs from every hidden layer in the `forward` function
          and then returning it in a tuple.


          You can see these line numbers from `modeling_opt.py` for a reference:


          https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L245

          https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L368

          https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L725


          Thanks for the great work!'
        updatedAt: '2023-05-23T05:12:39.475Z'
      numEdits: 2
      reactions: []
    id: 646c4a46ed228272134c5827
    type: comment
  author: abhinavkulkarni
  content: 'Hey @sam-mosaic,


    Thanks for the reply. You can see here, `output_attentions` options is not specified
    yet in `modeling_mpt.py`: https://huggingface.co/mosaicml/mpt-7b/blob/main/modeling_mpt.py#L140


    It would be nice if this `if` block were filled up instead of rasing `NotImplementedError`.
    I think it should be trivial given MPT uses traditional transformer, so collecting
    attention outputs from every hidden layer in the `forward` function and then returning
    it in a tuple.


    You can see these line numbers from `modeling_opt.py` for a reference:


    https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L245

    https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L368

    https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L725


    Thanks for the great work!'
  created_at: 2023-05-23 04:08:22+00:00
  edited: true
  hidden: false
  id: 646c4a46ed228272134c5827
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-05-25T08:02:02.000Z'
    data:
      edited: false
      editors:
      - sam-mosaic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;abhinavkulkarni&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/abhinavkulkarni\"\
          >@<span class=\"underline\">abhinavkulkarni</span></a></span>\n\n\t</span></span>\
          \ , I get it now. IIUC, <code>output_attentions</code> outputs the attention\
          \ matrix from the attention module?</p>\n<p>We do not use the <code>torch</code>\
          \ code path much, we usually train with Triton Flash or CUDA Flash. However,\
          \ neither of those attention implementations can support outputting  the\
          \ attention matrix. So, if we supported this flag it would only be for torch.\
          \ Does AutoGPTQ mainly focus on lower-resource inference and fine-tuning?</p>\n"
        raw: 'Thanks @abhinavkulkarni , I get it now. IIUC, `output_attentions` outputs
          the attention matrix from the attention module?


          We do not use the `torch` code path much, we usually train with Triton Flash
          or CUDA Flash. However, neither of those attention implementations can support
          outputting  the attention matrix. So, if we supported this flag it would
          only be for torch. Does AutoGPTQ mainly focus on lower-resource inference
          and fine-tuning?'
        updatedAt: '2023-05-25T08:02:02.569Z'
      numEdits: 0
      reactions: []
    id: 646f15fae2a72c647b617c8a
    type: comment
  author: sam-mosaic
  content: 'Thanks @abhinavkulkarni , I get it now. IIUC, `output_attentions` outputs
    the attention matrix from the attention module?


    We do not use the `torch` code path much, we usually train with Triton Flash or
    CUDA Flash. However, neither of those attention implementations can support outputting  the
    attention matrix. So, if we supported this flag it would only be for torch. Does
    AutoGPTQ mainly focus on lower-resource inference and fine-tuning?'
  created_at: 2023-05-25 07:02:02+00:00
  edited: false
  hidden: false
  id: 646f15fae2a72c647b617c8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-06-06T12:07:30.000Z'
    data:
      edited: false
      editors:
      - abhinavkulkarni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9044699668884277
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
          fullname: Abhinav Kulkarni
          isHf: false
          isPro: false
          name: abhinavkulkarni
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;sam-mosaic&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sam-mosaic\"\
          >@<span class=\"underline\">sam-mosaic</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>So, it seems the recent changes have solved most of the issues, except\
          \ for line <a href=\"https://huggingface.co/mosaicml/mpt-7b/blob/main/modeling_mpt.py#L110\"\
          >110 of modeling_mpt.py</a> which needs to be changed from:</p>\n<p><code>return\
          \ (attn_bias, None)</code></p>\n<p>to </p>\n<p><code>return (attn_bias,\
          \ attention_mask)</code>.</p>\n<p>I made changes in my local copy of <code>modeling_mpt.py</code>\
          \ in site-packages and was able to GPTQ quantize this model using AutoGPTQ\
          \ repo.</p>\n"
        raw: "Hey @sam-mosaic,\n\nSo, it seems the recent changes have solved most\
          \ of the issues, except for line [110 of modeling_mpt.py](https://huggingface.co/mosaicml/mpt-7b/blob/main/modeling_mpt.py#L110)\
          \ which needs to be changed from:\n\n`return (attn_bias, None)`\n\nto \n\
          \n\n`return (attn_bias, attention_mask)`.\n\nI made changes in my local\
          \ copy of `modeling_mpt.py` in site-packages and was able to GPTQ quantize\
          \ this model using AutoGPTQ repo."
        updatedAt: '2023-06-06T12:07:30.481Z'
      numEdits: 0
      reactions: []
    id: 647f2182f41cf810e3785edf
    type: comment
  author: abhinavkulkarni
  content: "Hey @sam-mosaic,\n\nSo, it seems the recent changes have solved most of\
    \ the issues, except for line [110 of modeling_mpt.py](https://huggingface.co/mosaicml/mpt-7b/blob/main/modeling_mpt.py#L110)\
    \ which needs to be changed from:\n\n`return (attn_bias, None)`\n\nto \n\n\n`return\
    \ (attn_bias, attention_mask)`.\n\nI made changes in my local copy of `modeling_mpt.py`\
    \ in site-packages and was able to GPTQ quantize this model using AutoGPTQ repo."
  created_at: 2023-06-06 11:07:30+00:00
  edited: false
  hidden: false
  id: 647f2182f41cf810e3785edf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/3GRgyIoe3_W6jz492qhkm.jpeg?w=200&h=200&f=face
      fullname: Vitaliy Chiley
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: vchiley
      type: user
    createdAt: '2023-06-06T19:17:19.000Z'
    data:
      edited: true
      editors:
      - vchiley
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.790574848651886
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/3GRgyIoe3_W6jz492qhkm.jpeg?w=200&h=200&f=face
          fullname: Vitaliy Chiley
          isHf: false
          isPro: false
          name: vchiley
          type: user
        html: '<p>To improve efficiency, in line 109 of modeling_mpt.py, we integrate
          <code>attention_mask</code> into <code>attn_bias</code> if it exists.<br>If
          the requested attn_impl does not support an attn bias, then we use <code>attention_mask</code>
          (eg <code>attn_impl: flash</code> does not support attn bias and therefore
          the output of  the <code>_attn_bias</code> fn is <code>(None, attention_mask)</code>;
          see line 88)</p>

          <p>This does not control if <code>output_attentions</code> are available.</p>

          '
        raw: 'To improve efficiency, in line 109 of modeling_mpt.py, we integrate
          `attention_mask` into `attn_bias` if it exists.

          If the requested attn_impl does not support an attn bias, then we use `attention_mask`
          (eg `attn_impl: flash` does not support attn bias and therefore the output
          of  the `_attn_bias` fn is `(None, attention_mask)`; see line 88)


          This does not control if `output_attentions` are available.'
        updatedAt: '2023-06-06T19:24:01.110Z'
      numEdits: 1
      reactions: []
    id: 647f863f1a446a624a498d3d
    type: comment
  author: vchiley
  content: 'To improve efficiency, in line 109 of modeling_mpt.py, we integrate `attention_mask`
    into `attn_bias` if it exists.

    If the requested attn_impl does not support an attn bias, then we use `attention_mask`
    (eg `attn_impl: flash` does not support attn bias and therefore the output of  the
    `_attn_bias` fn is `(None, attention_mask)`; see line 88)


    This does not control if `output_attentions` are available.'
  created_at: 2023-06-06 18:17:19+00:00
  edited: true
  hidden: false
  id: 647f863f1a446a624a498d3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-07-05T15:26:53.000Z'
    data:
      status: closed
    id: 64a58bbd3dd8e0eb800a4127
    type: status-change
  author: abhinavkulkarni
  created_at: 2023-07-05 14:26:53+00:00
  id: 64a58bbd3dd8e0eb800a4127
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 30
repo_id: mosaicml/mpt-7b
repo_type: model
status: closed
target_branch: null
title: Is it possible to add logic for handling output_attentions?
