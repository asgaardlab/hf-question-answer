!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kosiakk
conflicting_files: null
created_at: 2023-06-07 04:13:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f04a2e81ae78628890adb3adee299542.svg
      fullname: Alexander Kosenkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kosiakk
      type: user
    createdAt: '2023-06-07T05:13:07.000Z'
    data:
      edited: true
      editors:
      - kosiakk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7169337868690491
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f04a2e81ae78628890adb3adee299542.svg
          fullname: Alexander Kosenkov
          isHf: false
          isPro: false
          name: kosiakk
          type: user
        html: "<p>MPT-7B model card mentions vocab size of <code>50432,</code> which\
          \ matches with <code>model.transformer.wte.weight</code> (shaped [50432,\
          \ 4096]).</p>\n<p>At the same time, <code>tokenizer.json</code> contains\
          \ only 50254 tokens (including all technical). The last is <code>\" Outcomes\"\
          : 50253</code>.<br>This also matches <a href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\"\
          >EleutherAI/gpt-neox-20b</a>, even though they publish <code>50257</code>\
          \ tokens.</p>\n<div class=\"max-w-full overflow-auto\">\n\t<table>\n\t\t\
          <thead><tr>\n<th>Where</th>\n<th>Vocab size</th>\n<th>Difference</th>\n\
          </tr>\n\n\t\t</thead><tbody><tr>\n<td>MPT-7B model card</td>\n<td>50432</td>\n\
          <td></td>\n</tr>\n<tr>\n<td>MPT-7B embeddings</td>\n<td>50432</td>\n<td></td>\n\
          </tr>\n<tr>\n<td>gpt-neox-20b model card</td>\n<td>50257</td>\n<td>175 tokens</td>\n\
          </tr>\n<tr>\n<td>tokenizer.json for both</td>\n<td>50254</td>\n<td>178 tokens</td>\n\
          </tr>\n</tbody>\n\t</table>\n</div>\n<h3 id=\"what-does-it-mean\">What does\
          \ it mean?</h3>\n<p>The model would accept extra 178 tokens, which it has\
          \ never seen in the training?</p>\n"
        raw: 'MPT-7B model card mentions vocab size of `50432,` which matches with
          `model.transformer.wte.weight` (shaped [50432, 4096]).


          At the same time, `tokenizer.json` contains only 50254 tokens (including
          all technical). The last is `" Outcomes": 50253`.

          This also matches [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b),
          even though they publish `50257` tokens.


          | Where                   | Vocab size | Difference|

          |-------------------------|-------------|---|

          | MPT-7B model card       |       50432 | |

          | MPT-7B embeddings | 50432 |  |

          | gpt-neox-20b model card |       50257 | 175 tokens |

          | tokenizer.json for both   |          50254   | 178 tokens |


          ### What does it mean?

          The model would accept extra 178 tokens, which it has never seen in the
          training?'
        updatedAt: '2023-06-07T05:14:16.613Z'
      numEdits: 1
      reactions: []
    id: 648011e328b737d7b946ae21
    type: comment
  author: kosiakk
  content: 'MPT-7B model card mentions vocab size of `50432,` which matches with `model.transformer.wte.weight`
    (shaped [50432, 4096]).


    At the same time, `tokenizer.json` contains only 50254 tokens (including all technical).
    The last is `" Outcomes": 50253`.

    This also matches [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b),
    even though they publish `50257` tokens.


    | Where                   | Vocab size | Difference|

    |-------------------------|-------------|---|

    | MPT-7B model card       |       50432 | |

    | MPT-7B embeddings | 50432 |  |

    | gpt-neox-20b model card |       50257 | 175 tokens |

    | tokenizer.json for both   |          50254   | 178 tokens |


    ### What does it mean?

    The model would accept extra 178 tokens, which it has never seen in the training?'
  created_at: 2023-06-07 04:13:07+00:00
  edited: true
  hidden: false
  id: 648011e328b737d7b946ae21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f04a2e81ae78628890adb3adee299542.svg
      fullname: Alexander Kosenkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kosiakk
      type: user
    createdAt: '2023-06-07T09:08:05.000Z'
    data:
      edited: true
      editors:
      - kosiakk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9624918103218079
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f04a2e81ae78628890adb3adee299542.svg
          fullname: Alexander Kosenkov
          isHf: false
          isPro: false
          name: kosiakk
          type: user
        html: '<p>Here is a pretty heatmap plot of the <code>wte</code> embeddings
          layer:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64613c9556c57120a2d042df/4oFimTzaORCf1mMJMGaP2.png"><img
          alt="heatmap plot" src="https://cdn-uploads.huggingface.co/production/uploads/64613c9556c57120a2d042df/4oFimTzaORCf1mMJMGaP2.png"></a><br>I''d
          say it clearly shows that tokens up until tokenizer max id (50253) were
          trained, but tokens higher than that remain in their (random) init, probably
          because they never received any gradients.</p>

          <p>I don''t think it''s helpful to model in any way, but wasted CPU or memory
          seems negligible.</p>

          '
        raw: "Here is a pretty heatmap plot of the `wte` embeddings layer: \n![heatmap\
          \ plot](https://cdn-uploads.huggingface.co/production/uploads/64613c9556c57120a2d042df/4oFimTzaORCf1mMJMGaP2.png)\
          \ \nI'd say it clearly shows that tokens up until tokenizer max id (50253)\
          \ were trained, but tokens higher than that remain in their (random) init,\
          \ probably because they never received any gradients.\n\nI don't think it's\
          \ helpful to model in any way, but wasted CPU or memory seems negligible."
        updatedAt: '2023-06-07T11:26:12.479Z'
      numEdits: 1
      reactions: []
    id: 648048f5bb25a636c9d6835c
    type: comment
  author: kosiakk
  content: "Here is a pretty heatmap plot of the `wte` embeddings layer: \n![heatmap\
    \ plot](https://cdn-uploads.huggingface.co/production/uploads/64613c9556c57120a2d042df/4oFimTzaORCf1mMJMGaP2.png)\
    \ \nI'd say it clearly shows that tokens up until tokenizer max id (50253) were\
    \ trained, but tokens higher than that remain in their (random) init, probably\
    \ because they never received any gradients.\n\nI don't think it's helpful to\
    \ model in any way, but wasted CPU or memory seems negligible."
  created_at: 2023-06-07 08:08:05+00:00
  edited: true
  hidden: false
  id: 648048f5bb25a636c9d6835c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-06-07T16:21:08.000Z'
    data:
      edited: true
      editors:
      - sam-mosaic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9420107007026672
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
          fullname: Sam Havens
          isHf: false
          isPro: false
          name: sam-mosaic
          type: user
        html: '<p>This was intentional. Having vocab size be a multiple of 128 gives
          a pretty substantial efficiency boost. We also wanted to leave space for
          tokens like those needed for UL2 style mixture of denoisers.</p>

          <p>Thank you for the table and chart!</p>

          '
        raw: 'This was intentional. Having vocab size be a multiple of 128 gives a
          pretty substantial efficiency boost. We also wanted to leave space for tokens
          like those needed for UL2 style mixture of denoisers.


          Thank you for the table and chart!'
        updatedAt: '2023-06-07T16:21:29.434Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - kosiakk
        - preminstrel
      relatedEventId: 6480ae74e1421e205fdb2fc4
    id: 6480ae74e1421e205fdb2fc3
    type: comment
  author: sam-mosaic
  content: 'This was intentional. Having vocab size be a multiple of 128 gives a pretty
    substantial efficiency boost. We also wanted to leave space for tokens like those
    needed for UL2 style mixture of denoisers.


    Thank you for the table and chart!'
  created_at: 2023-06-07 15:21:08+00:00
  edited: true
  hidden: false
  id: 6480ae74e1421e205fdb2fc3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668560930781-noauth.png?w=200&h=200&f=face
      fullname: Sam Havens
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sam-mosaic
      type: user
    createdAt: '2023-06-07T16:21:08.000Z'
    data:
      status: closed
    id: 6480ae74e1421e205fdb2fc4
    type: status-change
  author: sam-mosaic
  created_at: 2023-06-07 15:21:08+00:00
  id: 6480ae74e1421e205fdb2fc4
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 52
repo_id: mosaicml/mpt-7b
repo_type: model
status: closed
target_branch: null
title: Vocab size is 50432, but the last token is 50253
