!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Jackmin108
conflicting_files: []
created_at: 2023-10-27 19:32:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2023-10-27T20:32:38.000Z'
    data:
      edited: false
      editors:
      - Jackmin108
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11044929921627045
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: ''
        raw: ''
        updatedAt: '2023-10-27T20:32:38.129Z'
      numEdits: 0
      reactions: []
    id: 653c1e667117f6b8772a7458
    type: comment
  author: Jackmin108
  content: ''
  created_at: 2023-10-27 19:32:38+00:00
  edited: false
  hidden: false
  id: 653c1e667117f6b8772a7458
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    createdAt: '2023-10-27T20:36:18.000Z'
    data:
      oid: 5dd64d05fbb13c98e5b7f09ce7325aa87f15156c
      parents:
      - 43f3955aec2eca9bd883143d64e1e8e066f74417
      subject: disable sdpa and dynamically allocate alibi bias
    id: 653c1f420000000000000000
    type: commit
  author: deleted
  created_at: 2023-10-27 19:36:18+00:00
  id: 653c1f420000000000000000
  oid: 5dd64d05fbb13c98e5b7f09ce7325aa87f15156c
  summary: disable sdpa and dynamically allocate alibi bias
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ee6641bfcdb40f1cd252ccecbbf0c146.svg
      fullname: rob cheung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kousun12
      type: user
    createdAt: '2023-12-15T04:13:38.000Z'
    data:
      edited: false
      editors:
      - kousun12
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5697351098060608
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ee6641bfcdb40f1cd252ccecbbf0c146.svg
          fullname: rob cheung
          isHf: false
          isPro: false
          name: kousun12
          type: user
        html: '<p>can we get this merged? i am running in to:<br><code>[libprotobuf
          ERROR ../third_party/protobuf/src/google/protobuf/message_lite.cc:457] onnx_torch.ModelProto
          exceeded maximum protobuf size of 2GB: 3768728061</code></p>

          <p>when exporting to onnx</p>

          '
        raw: 'can we get this merged? i am running in to:

          `[libprotobuf ERROR ../third_party/protobuf/src/google/protobuf/message_lite.cc:457]
          onnx_torch.ModelProto exceeded maximum protobuf size of 2GB: 3768728061`


          when exporting to onnx

          '
        updatedAt: '2023-12-15T04:13:38.643Z'
      numEdits: 0
      reactions: []
    id: 657bd272eda715a4be497a18
    type: comment
  author: kousun12
  content: 'can we get this merged? i am running in to:

    `[libprotobuf ERROR ../third_party/protobuf/src/google/protobuf/message_lite.cc:457]
    onnx_torch.ModelProto exceeded maximum protobuf size of 2GB: 3768728061`


    when exporting to onnx

    '
  created_at: 2023-12-15 04:13:38+00:00
  edited: false
  hidden: false
  id: 657bd272eda715a4be497a18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2023-12-21T08:40:47.000Z'
    data:
      edited: false
      editors:
      - Jackmin108
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8650088906288147
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
          fullname: Jack Min Ong
          isHf: false
          isPro: false
          name: Jackmin108
          type: user
        html: '<p>We have decided not to merge this into the main branch as it</p>

          <ol>

          <li>Disables flash attention as default -- This would cause our users who
          are running pytorch to experience lower throughput and higher memory footprint
          with the default settings</li>

          <li>Dynamically allocates alibi tensor -- This might be an issue for long
          running server deployments as the alibi tensor is quite big at long seq
          len (4GB at 8k seq len). This big dynamic tensor allocation could cause
          an OOM due to memory fragmentation.</li>

          </ol>

          <p>You can still make the changes here to your local version of the model
          to be able to do the export.</p>

          <p>However, our recommendation is to just use the models we have already
          exported:</p>

          <ul>

          <li>Base: <a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-en/blob/main/model.onnx">https://huggingface.co/jinaai/jina-embeddings-v2-base-en/blob/main/model.onnx</a></li>

          <li>Small: <a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en/blob/main/model.onnx">https://huggingface.co/jinaai/jina-embeddings-v2-small-en/blob/main/model.onnx</a></li>

          </ul>

          '
        raw: 'We have decided not to merge this into the main branch as it

          1. Disables flash attention as default -- This would cause our users who
          are running pytorch to experience lower throughput and higher memory footprint
          with the default settings

          2. Dynamically allocates alibi tensor -- This might be an issue for long
          running server deployments as the alibi tensor is quite big at long seq
          len (4GB at 8k seq len). This big dynamic tensor allocation could cause
          an OOM due to memory fragmentation.


          You can still make the changes here to your local version of the model to
          be able to do the export.


          However, our recommendation is to just use the models we have already exported:


          - Base: https://huggingface.co/jinaai/jina-embeddings-v2-base-en/blob/main/model.onnx

          - Small: https://huggingface.co/jinaai/jina-embeddings-v2-small-en/blob/main/model.onnx'
        updatedAt: '2023-12-21T08:40:47.548Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6583fa0fa4af83584c970a11
    id: 6583fa0fa4af83584c970a0b
    type: comment
  author: Jackmin108
  content: 'We have decided not to merge this into the main branch as it

    1. Disables flash attention as default -- This would cause our users who are running
    pytorch to experience lower throughput and higher memory footprint with the default
    settings

    2. Dynamically allocates alibi tensor -- This might be an issue for long running
    server deployments as the alibi tensor is quite big at long seq len (4GB at 8k
    seq len). This big dynamic tensor allocation could cause an OOM due to memory
    fragmentation.


    You can still make the changes here to your local version of the model to be able
    to do the export.


    However, our recommendation is to just use the models we have already exported:


    - Base: https://huggingface.co/jinaai/jina-embeddings-v2-base-en/blob/main/model.onnx

    - Small: https://huggingface.co/jinaai/jina-embeddings-v2-small-en/blob/main/model.onnx'
  created_at: 2023-12-21 08:40:47+00:00
  edited: false
  hidden: false
  id: 6583fa0fa4af83584c970a0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png?w=200&h=200&f=face
      fullname: Jack Min Ong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jackmin108
      type: user
    createdAt: '2023-12-21T08:40:47.000Z'
    data:
      status: closed
    id: 6583fa0fa4af83584c970a11
    type: status-change
  author: Jackmin108
  created_at: 2023-12-21 08:40:47+00:00
  id: 6583fa0fa4af83584c970a11
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ee6641bfcdb40f1cd252ccecbbf0c146.svg
      fullname: rob cheung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kousun12
      type: user
    createdAt: '2023-12-21T14:54:06.000Z'
    data:
      edited: false
      editors:
      - kousun12
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9461907744407654
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ee6641bfcdb40f1cd252ccecbbf0c146.svg
          fullname: rob cheung
          isHf: false
          isPro: false
          name: kousun12
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Jackmin108&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Jackmin108\">@<span class=\"\
          underline\">Jackmin108</span></a></span>\n\n\t</span></span> I'm currently\
          \ using the published onnx models that you linked to. I downloaded that\
          \ and applied O4 optimization via the optimum SDK. What i'm noticing with\
          \ this vs my other O4 optimized onnx model ~2 months ago is that this one\
          \ OOMs usually during model creation, about 1/4 of the time. Do you have\
          \ an idea why?</p>\n"
        raw: '@Jackmin108 I''m currently using the published onnx models that you
          linked to. I downloaded that and applied O4 optimization via the optimum
          SDK. What i''m noticing with this vs my other O4 optimized onnx model ~2
          months ago is that this one OOMs usually during model creation, about 1/4
          of the time. Do you have an idea why?'
        updatedAt: '2023-12-21T14:54:06.627Z'
      numEdits: 0
      reactions: []
    id: 6584518eca19ccf6d9a3733f
    type: comment
  author: kousun12
  content: '@Jackmin108 I''m currently using the published onnx models that you linked
    to. I downloaded that and applied O4 optimization via the optimum SDK. What i''m
    noticing with this vs my other O4 optimized onnx model ~2 months ago is that this
    one OOMs usually during model creation, about 1/4 of the time. Do you have an
    idea why?'
  created_at: 2023-12-21 14:54:06+00:00
  edited: false
  hidden: false
  id: 6584518eca19ccf6d9a3733f
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 7
repo_id: jinaai/jina-bert-implementation
repo_type: model
status: closed
target_branch: refs/heads/main
title: Modify model architecture to export ONNX 11
