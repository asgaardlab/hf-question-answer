!!python/object:huggingface_hub.community.DiscussionWithDetails
author: eson
conflicting_files: []
created_at: 2023-11-25 07:02:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/3fb117e23fdcf57abb56a142cb8411ef.svg
      fullname: xu song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eson
      type: user
    createdAt: '2023-11-25T07:02:54.000Z'
    data:
      oid: 75e249af814cd81849dbf0e393b87f2b07b86907
      parents:
      - 84603cde5ebffb6084e476cfaeceaf0b8b91fe54
      subject: Fix BaichuanTokenizer to fit transformers>=4.34
    id: 65619c1e0000000000000000
    type: commit
  author: eson
  created_at: 2023-11-25 07:02:54+00:00
  id: 65619c1e0000000000000000
  oid: 75e249af814cd81849dbf0e393b87f2b07b86907
  summary: Fix BaichuanTokenizer to fit transformers>=4.34
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3fb117e23fdcf57abb56a142cb8411ef.svg
      fullname: xu song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eson
      type: user
    createdAt: '2023-11-25T07:02:55.000Z'
    data:
      edited: true
      editors:
      - eson
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11044929921627045
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3fb117e23fdcf57abb56a142cb8411ef.svg
          fullname: xu song
          isHf: false
          isPro: false
          name: eson
          type: user
        html: "<pre><code class=\"language-sh\">$ python predict_baichuan.py\nTraceback\
          \ (most recent call last):\n  File <span class=\"hljs-string\">\"/workspace/baichuan/predict/predict_baichuan.py\"\
          </span>, line 14, <span class=\"hljs-keyword\">in</span> &lt;module&gt;\n\
          \    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False,\
          \ trust_remote_code=True)\n  File <span class=\"hljs-string\">\"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\"\
          </span>, line 755, <span class=\"hljs-keyword\">in</span> from_pretrained\n\
          \    <span class=\"hljs-built_in\">return</span> tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n  File <span class=\"hljs-string\">\"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\"\
          </span>, line 2024, <span class=\"hljs-keyword\">in</span> from_pretrained\n\
          \    <span class=\"hljs-built_in\">return</span> cls._from_pretrained(\n\
          \  File <span class=\"hljs-string\">\"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\"\
          </span>, line 2256, <span class=\"hljs-keyword\">in</span> _from_pretrained\n\
          \    tokenizer = cls(*init_inputs, **init_kwargs)\n  File <span class=\"\
          hljs-string\">\"/root/.cache/huggingface/modules/transformers_modules/tokenization_baichuan.py\"\
          </span>, line 75, <span class=\"hljs-keyword\">in</span> __init__\n    super().__init__(\n\
          \  File <span class=\"hljs-string\">\"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\"\
          </span>, line 367, <span class=\"hljs-keyword\">in</span> __init__\n   \
          \ self._add_tokens(\n  File <span class=\"hljs-string\">\"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\"\
          </span>, line 467, <span class=\"hljs-keyword\">in</span> _add_tokens\n\
          \    current_vocab = self.get_vocab().copy()\n  File <span class=\"hljs-string\"\
          >\"/root/.cache/huggingface/modules/transformers_modules/tokenization_baichuan.py\"\
          </span>, line 109, <span class=\"hljs-keyword\">in</span> get_vocab\n  \
          \  vocab = {self.convert_ids_to_tokens(i): i <span class=\"hljs-keyword\"\
          >for</span> i <span class=\"hljs-keyword\">in</span> range(self.vocab_size)}\n\
          \  File <span class=\"hljs-string\">\"/root/.cache/huggingface/modules/transformers_modules/tokenization_baichuan.py\"\
          </span>, line 105, <span class=\"hljs-keyword\">in</span> vocab_size\n \
          \   <span class=\"hljs-built_in\">return</span> self.sp_model.get_piece_size()\n\
          AttributeError: <span class=\"hljs-string\">'BaichuanTokenizer'</span> object\
          \ has no attribute <span class=\"hljs-string\">'sp_model'</span>\n</code></pre>\n\
          <p>Related issue <a rel=\"nofollow\" href=\"https://github.com/InternLM/InternLM/pull/419/files\"\
          >https://github.com/InternLM/InternLM/pull/419/files</a></p>\n"
        raw: "\n```sh\n$ python predict_baichuan.py\nTraceback (most recent call last):\n\
          \  File \"/workspace/baichuan/predict/predict_baichuan.py\", line 14, in\
          \ <module>\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False,\
          \ trust_remote_code=True)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 755, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\"\
          , line 2024, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\"\
          , line 2256, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \  File \"/root/.cache/huggingface/modules/transformers_modules/tokenization_baichuan.py\"\
          , line 75, in __init__\n    super().__init__(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\"\
          , line 367, in __init__\n    self._add_tokens(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\"\
          , line 467, in _add_tokens\n    current_vocab = self.get_vocab().copy()\n\
          \  File \"/root/.cache/huggingface/modules/transformers_modules/tokenization_baichuan.py\"\
          , line 109, in get_vocab\n    vocab = {self.convert_ids_to_tokens(i): i\
          \ for i in range(self.vocab_size)}\n  File \"/root/.cache/huggingface/modules/transformers_modules/tokenization_baichuan.py\"\
          , line 105, in vocab_size\n    return self.sp_model.get_piece_size()\nAttributeError:\
          \ 'BaichuanTokenizer' object has no attribute 'sp_model'\n\n```\n\nRelated\
          \ issue https://github.com/InternLM/InternLM/pull/419/files"
        updatedAt: '2023-11-25T07:07:11.693Z'
      numEdits: 2
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - eson
        - heichow
        - songyouwei
    id: 65619c1f7a465cdcb34e8010
    type: comment
  author: eson
  content: "\n```sh\n$ python predict_baichuan.py\nTraceback (most recent call last):\n\
    \  File \"/workspace/baichuan/predict/predict_baichuan.py\", line 14, in <module>\n\
    \    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False, trust_remote_code=True)\n\
    \  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 755, in from_pretrained\n    return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\"\
    , line 2024, in from_pretrained\n    return cls._from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\"\
    , line 2256, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
    \  File \"/root/.cache/huggingface/modules/transformers_modules/tokenization_baichuan.py\"\
    , line 75, in __init__\n    super().__init__(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\"\
    , line 367, in __init__\n    self._add_tokens(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\"\
    , line 467, in _add_tokens\n    current_vocab = self.get_vocab().copy()\n  File\
    \ \"/root/.cache/huggingface/modules/transformers_modules/tokenization_baichuan.py\"\
    , line 109, in get_vocab\n    vocab = {self.convert_ids_to_tokens(i): i for i\
    \ in range(self.vocab_size)}\n  File \"/root/.cache/huggingface/modules/transformers_modules/tokenization_baichuan.py\"\
    , line 105, in vocab_size\n    return self.sp_model.get_piece_size()\nAttributeError:\
    \ 'BaichuanTokenizer' object has no attribute 'sp_model'\n\n```\n\nRelated issue\
    \ https://github.com/InternLM/InternLM/pull/419/files"
  created_at: 2023-11-25 07:02:55+00:00
  edited: true
  hidden: false
  id: 65619c1f7a465cdcb34e8010
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3fb117e23fdcf57abb56a142cb8411ef.svg
      fullname: xu song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eson
      type: user
    createdAt: '2023-11-25T07:03:20.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/3fb117e23fdcf57abb56a142cb8411ef.svg
          fullname: xu song
          isHf: false
          isPro: false
          name: eson
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-11-25T07:03:45.519Z'
      numEdits: 0
      reactions: []
    id: 65619c384f96a8ea08fe9878
    type: comment
  author: eson
  content: This comment has been hidden
  created_at: 2023-11-25 07:03:20+00:00
  edited: true
  hidden: true
  id: 65619c384f96a8ea08fe9878
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641414045189abde8cbbd088/P9ChZODyI5f2climsCvZv.png?w=200&h=200&f=face
      fullname: Gradient Guru
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: GradientGuru
      type: user
    createdAt: '2023-12-24T13:19:29.000Z'
    data:
      status: merged
    id: 65882fe1fb9c2bdfae2eea31
    type: status-change
  author: GradientGuru
  created_at: 2023-12-24 13:19:29+00:00
  id: 65882fe1fb9c2bdfae2eea31
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: e837cb3c1eb7887e3d2e5f39280491af1483c605
num: 8
repo_id: baichuan-inc/Baichuan2-7B-Chat
repo_type: model
status: merged
target_branch: refs/heads/main
title: Fix BaichuanTokenizer to fit transformers>=4.34
