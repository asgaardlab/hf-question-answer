!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ImWolf7
conflicting_files: null
created_at: 2023-08-12 06:42:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4eec871d97a87ca0feeff79d7deb6af0.svg
      fullname: Deeb Tibi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ImWolf7
      type: user
    createdAt: '2023-08-12T07:42:51.000Z'
    data:
      edited: false
      editors:
      - ImWolf7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9139335751533508
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4eec871d97a87ca0feeff79d7deb6af0.svg
          fullname: Deeb Tibi
          isHf: false
          isPro: false
          name: ImWolf7
          type: user
        html: '<p>First of all, I''m a big fan of your work and the support you provide
          for the community. Thanks for all the work and effort you put into this.<br>I''m
          having trouble finetuning this model using auto-GPTQ using peft. so far
          peft support with autoGPTQ is limited and doesn''t support llama-2 finetuning.<br>I''m
          looking to actually finetune the original llama-2 model using bitsandbytes
          and QLoRa and then GPTQ quantize the result.<br>Thus, the question in the
          title arises :)<br>thanks for the help.</p>

          '
        raw: "First of all, I'm a big fan of your work and the support you provide\
          \ for the community. Thanks for all the work and effort you put into this.\r\
          \nI'm having trouble finetuning this model using auto-GPTQ using peft. so\
          \ far peft support with autoGPTQ is limited and doesn't support llama-2\
          \ finetuning.\r\nI'm looking to actually finetune the original llama-2 model\
          \ using bitsandbytes and QLoRa and then GPTQ quantize the result.\r\nThus,\
          \ the question in the title arises :)\r\nthanks for the help."
        updatedAt: '2023-08-12T07:42:51.893Z'
      numEdits: 0
      reactions: []
    id: 64d737fba146b1c0a6843a30
    type: comment
  author: ImWolf7
  content: "First of all, I'm a big fan of your work and the support you provide for\
    \ the community. Thanks for all the work and effort you put into this.\r\nI'm\
    \ having trouble finetuning this model using auto-GPTQ using peft. so far peft\
    \ support with autoGPTQ is limited and doesn't support llama-2 finetuning.\r\n\
    I'm looking to actually finetune the original llama-2 model using bitsandbytes\
    \ and QLoRa and then GPTQ quantize the result.\r\nThus, the question in the title\
    \ arises :)\r\nthanks for the help."
  created_at: 2023-08-12 06:42:51+00:00
  edited: false
  hidden: false
  id: 64d737fba146b1c0a6843a30
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-08-12T11:21:08.000Z'
    data:
      edited: true
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8964772820472717
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>There''s quantization instructions on the github repo <a rel="nofollow"
          href="https://github.com/PanQiWei/AutoGPTQ/">here</a>.</p>

          <p>TheBloke also shared a more detailed script for doing multiple quants
          at once <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/issues/179#issuecomment-1611257490">here</a></p>

          <p>I''m in the same boat, I''d prefer to do PEFT with auto-GPTQ, see this
          <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/">issue here</a>
          because it''s slow to do bnb and then have to quantize.</p>

          <p>Edit: I also just found this <a rel="nofollow" href="https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing">script
          from HF</a>. Not sure if it has any shortcomings, but seems complete and
          easy to use.</p>

          '
        raw: 'There''s quantization instructions on the github repo [here](https://github.com/PanQiWei/AutoGPTQ/).


          TheBloke also shared a more detailed script for doing multiple quants at
          once [here](https://github.com/PanQiWei/AutoGPTQ/issues/179#issuecomment-1611257490)


          I''m in the same boat, I''d prefer to do PEFT with auto-GPTQ, see this [issue
          here](https://github.com/PanQiWei/AutoGPTQ/) because it''s slow to do bnb
          and then have to quantize.


          Edit: I also just found this [script from HF](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing).
          Not sure if it has any shortcomings, but seems complete and easy to use.'
        updatedAt: '2023-08-12T12:30:33.823Z'
      numEdits: 1
      reactions: []
    id: 64d76b246db135cfc8cbf2b1
    type: comment
  author: RonanMcGovern
  content: 'There''s quantization instructions on the github repo [here](https://github.com/PanQiWei/AutoGPTQ/).


    TheBloke also shared a more detailed script for doing multiple quants at once
    [here](https://github.com/PanQiWei/AutoGPTQ/issues/179#issuecomment-1611257490)


    I''m in the same boat, I''d prefer to do PEFT with auto-GPTQ, see this [issue
    here](https://github.com/PanQiWei/AutoGPTQ/) because it''s slow to do bnb and
    then have to quantize.


    Edit: I also just found this [script from HF](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing).
    Not sure if it has any shortcomings, but seems complete and easy to use.'
  created_at: 2023-08-12 10:21:08+00:00
  edited: true
  hidden: false
  id: 64d76b246db135cfc8cbf2b1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 32
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: What library was used to quantize this model ?
