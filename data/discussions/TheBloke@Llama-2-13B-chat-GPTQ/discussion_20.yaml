!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Pchaudhary
conflicting_files: null
created_at: 2023-07-27 08:56:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37c483262fbd56b44eed95a09af0030a.svg
      fullname: Preet Chaudhary
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pchaudhary
      type: user
    createdAt: '2023-07-27T09:56:29.000Z'
    data:
      edited: false
      editors:
      - Pchaudhary
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5591870546340942
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37c483262fbd56b44eed95a09af0030a.svg
          fullname: Preet Chaudhary
          isHf: false
          isPro: false
          name: Pchaudhary
          type: user
        html: '<p>I am using below code :</p>

          <p>from transformers import AutoTokenizer, pipeline, logging<br>from auto_gptq
          import AutoGPTQForCausalLM, BaseQuantizeConfig</p>

          <p>model_name_or_path = "TheBloke/Llama-2-13B-chat-GPTQ"<br>model_basename
          = "gptq_model-8bit-128g"</p>

          <p>use_triton = False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>        model_basename=model_basename,<br>        use_safetensors=True,<br>        trust_remote_code=True,<br>        device="cuda:0",<br>        use_triton=use_triton,<br>        quantize_config=None)</p>

          <p>But I am getting error :</p>

          <p>FileNotFoundError                         Traceback (most recent call
          last)<br> in &lt;cell line: 11&gt;()<br>      9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,
          use_fast=True)<br>     10<br>---&gt; 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>     12         model_basename=model_basename,<br>     13         use_safetensors=True,</p>

          <p>1 frames<br>/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py
          in from_quantized(cls, model_name_or_path, save_dir, device_map, max_memory,
          device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention,
          inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,
          trust_remote_code, warmup_triton, trainable, **kwargs)<br>    712<br>    713         if
          resolved_archive_file is None: # Could not find a model file to use<br>--&gt;
          714             raise FileNotFoundError(f"Could not find model in {model_name_or_path}")<br>    715<br>    716         model_save_name
          = resolved_archive_file</p>

          <p>FileNotFoundError: Could not find model in TheBloke/Llama-2-13B-chat-GPTQ</p>

          '
        raw: "I am using below code :\r\n\r\nfrom transformers import AutoTokenizer,\
          \ pipeline, logging\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\
          \n\r\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\r\nmodel_basename\
          \ = \"gptq_model-8bit-128g\"\r\n\r\nuse_triton = False\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\n\
          \r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\n \
          \       model_basename=model_basename,\r\n        use_safetensors=True,\r\
          \n        trust_remote_code=True,\r\n        device=\"cuda:0\",\r\n    \
          \    use_triton=use_triton,\r\n        quantize_config=None)\r\n\r\nBut\
          \ I am getting error :\r\n\r\nFileNotFoundError                        \
          \ Traceback (most recent call last)\r\n<ipython-input-10-b9f2c1bf1ec2> in\
          \ <cell line: 11>()\r\n      9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\r\n     10 \r\n---> 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n     12         model_basename=model_basename,\r\n     13         use_safetensors=True,\r\
          \n\r\n1 frames\r\n/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\
          \ in from_quantized(cls, model_name_or_path, save_dir, device_map, max_memory,\
          \ device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention,\
          \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
          \ trust_remote_code, warmup_triton, trainable, **kwargs)\r\n    712 \r\n\
          \    713         if resolved_archive_file is None: # Could not find a model\
          \ file to use\r\n--> 714             raise FileNotFoundError(f\"Could not\
          \ find model in {model_name_or_path}\")\r\n    715 \r\n    716         model_save_name\
          \ = resolved_archive_file\r\n\r\nFileNotFoundError: Could not find model\
          \ in TheBloke/Llama-2-13B-chat-GPTQ"
        updatedAt: '2023-07-27T09:56:29.838Z'
      numEdits: 0
      reactions: []
    id: 64c23f4d0bfb901b04540662
    type: comment
  author: Pchaudhary
  content: "I am using below code :\r\n\r\nfrom transformers import AutoTokenizer,\
    \ pipeline, logging\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\
    \n\r\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\r\nmodel_basename\
    \ = \"gptq_model-8bit-128g\"\r\n\r\nuse_triton = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n        model_basename=model_basename,\r\n        use_safetensors=True,\r\n\
    \        trust_remote_code=True,\r\n        device=\"cuda:0\",\r\n        use_triton=use_triton,\r\
    \n        quantize_config=None)\r\n\r\nBut I am getting error :\r\n\r\nFileNotFoundError\
    \                         Traceback (most recent call last)\r\n<ipython-input-10-b9f2c1bf1ec2>\
    \ in <cell line: 11>()\r\n      9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n     10 \r\n---> 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n     12         model_basename=model_basename,\r\n     13         use_safetensors=True,\r\
    \n\r\n1 frames\r\n/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\
    \ in from_quantized(cls, model_name_or_path, save_dir, device_map, max_memory,\
    \ device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention,\
    \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
    \ trust_remote_code, warmup_triton, trainable, **kwargs)\r\n    712 \r\n    713\
    \         if resolved_archive_file is None: # Could not find a model file to use\r\
    \n--> 714             raise FileNotFoundError(f\"Could not find model in {model_name_or_path}\"\
    )\r\n    715 \r\n    716         model_save_name = resolved_archive_file\r\n\r\
    \nFileNotFoundError: Could not find model in TheBloke/Llama-2-13B-chat-GPTQ"
  created_at: 2023-07-27 08:56:29+00:00
  edited: false
  hidden: false
  id: 64c23f4d0bfb901b04540662
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-27T10:00:42.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9407535195350647
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please update to AutoGPTQ 0.3.2, released yesterday.  In AutoGPTQ
          0.3.0 and 0.2.2 there was a bug where the <code>revision</code> parameter
          was not followed.  This is now fixed.</p>

          '
        raw: Please update to AutoGPTQ 0.3.2, released yesterday.  In AutoGPTQ 0.3.0
          and 0.2.2 there was a bug where the `revision` parameter was not followed.  This
          is now fixed.
        updatedAt: '2023-07-27T10:00:42.238Z'
      numEdits: 0
      reactions: []
    id: 64c2404a8e2eb1a3b654c30d
    type: comment
  author: TheBloke
  content: Please update to AutoGPTQ 0.3.2, released yesterday.  In AutoGPTQ 0.3.0
    and 0.2.2 there was a bug where the `revision` parameter was not followed.  This
    is now fixed.
  created_at: 2023-07-27 09:00:42+00:00
  edited: false
  hidden: false
  id: 64c2404a8e2eb1a3b654c30d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37c483262fbd56b44eed95a09af0030a.svg
      fullname: Preet Chaudhary
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pchaudhary
      type: user
    createdAt: '2023-07-27T10:17:46.000Z'
    data:
      edited: false
      editors:
      - Pchaudhary
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5477893948554993
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37c483262fbd56b44eed95a09af0030a.svg
          fullname: Preet Chaudhary
          isHf: false
          isPro: false
          name: Pchaudhary
          type: user
        html: '<p>Ok I will try this one .</p>

          <p>Is the below code correct if I want to load model from a particular barch
          (i.e. gptq-8bit-128g-actorder_True) :</p>

          <p>from transformers import AutoTokenizer, pipeline, logging<br>from auto_gptq
          import AutoGPTQForCausalLM, BaseQuantizeConfig</p>

          <p>model_name_or_path = "TheBloke/Llama-2-13B-chat-GPTQ"<br>model_basename
          = "gptq_model-4bit-128g"</p>

          <p>use_triton = False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>        revision="gptq-8bit-128g-actorder_True",<br>        model_basename=model_basename,<br>        use_safetensors=True,<br>        trust_remote_code=True,<br>        device="cuda:0",<br>        quantize_config=None)</p>

          '
        raw: "Ok I will try this one .\n\nIs the below code correct if I want to load\
          \ model from a particular barch (i.e. gptq-8bit-128g-actorder_True) :\n\n\
          from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path =\
          \ \"TheBloke/Llama-2-13B-chat-GPTQ\"\nmodel_basename = \"gptq_model-4bit-128g\"\
          \n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        revision=\"gptq-8bit-128g-actorder_True\",\n        model_basename=model_basename,\n\
          \        use_safetensors=True,\n        trust_remote_code=True,\n      \
          \  device=\"cuda:0\",\n        quantize_config=None)"
        updatedAt: '2023-07-27T10:17:46.133Z'
      numEdits: 0
      reactions: []
    id: 64c2444a0a2545084f11a20d
    type: comment
  author: Pchaudhary
  content: "Ok I will try this one .\n\nIs the below code correct if I want to load\
    \ model from a particular barch (i.e. gptq-8bit-128g-actorder_True) :\n\nfrom\
    \ transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq import\
    \ AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\
    \nmodel_basename = \"gptq_model-4bit-128g\"\n\nuse_triton = False\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nmodel\
    \ = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n        revision=\"\
    gptq-8bit-128g-actorder_True\",\n        model_basename=model_basename,\n    \
    \    use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
    cuda:0\",\n        quantize_config=None)"
  created_at: 2023-07-27 09:17:46+00:00
  edited: false
  hidden: false
  id: 64c2444a0a2545084f11a20d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37c483262fbd56b44eed95a09af0030a.svg
      fullname: Preet Chaudhary
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pchaudhary
      type: user
    createdAt: '2023-07-28T02:46:29.000Z'
    data:
      edited: false
      editors:
      - Pchaudhary
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7592244744300842
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37c483262fbd56b44eed95a09af0030a.svg
          fullname: Preet Chaudhary
          isHf: false
          isPro: false
          name: Pchaudhary
          type: user
        html: '<p>Can you please provide me a python code to load 8 bit 128g model
          ?</p>

          '
        raw: 'Can you please provide me a python code to load 8 bit 128g model ?

          '
        updatedAt: '2023-07-28T02:46:29.044Z'
      numEdits: 0
      reactions: []
    id: 64c32c051fc43a53af15aa20
    type: comment
  author: Pchaudhary
  content: 'Can you please provide me a python code to load 8 bit 128g model ?

    '
  created_at: 2023-07-28 01:46:29+00:00
  edited: false
  hidden: false
  id: 64c32c051fc43a53af15aa20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ab8336cfbe073b42c4b3bcfc81fc20c.svg
      fullname: Richard Scott
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RichardScottOZ
      type: user
    createdAt: '2023-08-21T21:26:04.000Z'
    data:
      edited: false
      editors:
      - RichardScottOZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9809638857841492
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ab8336cfbe073b42c4b3bcfc81fc20c.svg
          fullname: Richard Scott
          isHf: false
          isPro: false
          name: RichardScottOZ
          type: user
        html: '<p>Yes, just saw that one - presumably some subtle basename thing that
          changed perhaps?</p>

          '
        raw: Yes, just saw that one - presumably some subtle basename thing that changed
          perhaps?
        updatedAt: '2023-08-21T21:26:04.703Z'
      numEdits: 0
      reactions: []
    id: 64e3d66ca88a63300ba43087
    type: comment
  author: RichardScottOZ
  content: Yes, just saw that one - presumably some subtle basename thing that changed
    perhaps?
  created_at: 2023-08-21 20:26:04+00:00
  edited: false
  hidden: false
  id: 64e3d66ca88a63300ba43087
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-21T21:38:43.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9200707077980042
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The required <code>model_basename</code> changed yesterday (August
          20th).   It is now <code>model_basename = "model"</code> - or you can just
          leave that line out completely, as it''s now configured automatically by
          <code>quantize_config.json</code>.  You no longer need to specify <code>model_basename</code>
          in the <code>.from_quantized()</code> call.  But if you do specify it, set
          it to <code>"model"</code>.</p>

          <p>This change has happened due to adding support for an upcoming change
          in Transformers, which will allow loading GPTQ models directly from Transformers</p>

          <p>I did automatically update the README to reflect the <code>model_basename</code>
          change, but haven''t mentioned the changes in more detail yet.  I will be
          updating all GPTQ READMEs in the next 48 hours to make this clearer.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/mF8-jebFnjl7Rn0c-jL2t.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/mF8-jebFnjl7Rn0c-jL2t.png"></a></p>

          '
        raw: 'The required `model_basename` changed yesterday (August 20th).   It
          is now `model_basename = "model"` - or you can just leave that line out
          completely, as it''s now configured automatically by `quantize_config.json`.  You
          no longer need to specify `model_basename` in the `.from_quantized()` call.  But
          if you do specify it, set it to `"model"`.


          This change has happened due to adding support for an upcoming change in
          Transformers, which will allow loading GPTQ models directly from Transformers


          I did automatically update the README to reflect the `model_basename` change,
          but haven''t mentioned the changes in more detail yet.  I will be updating
          all GPTQ READMEs in the next 48 hours to make this clearer.


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/mF8-jebFnjl7Rn0c-jL2t.png)

          '
        updatedAt: '2023-08-21T21:40:44.750Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - manashb97
    id: 64e3d963618cd90997eeaa2b
    type: comment
  author: TheBloke
  content: 'The required `model_basename` changed yesterday (August 20th).   It is
    now `model_basename = "model"` - or you can just leave that line out completely,
    as it''s now configured automatically by `quantize_config.json`.  You no longer
    need to specify `model_basename` in the `.from_quantized()` call.  But if you
    do specify it, set it to `"model"`.


    This change has happened due to adding support for an upcoming change in Transformers,
    which will allow loading GPTQ models directly from Transformers


    I did automatically update the README to reflect the `model_basename` change,
    but haven''t mentioned the changes in more detail yet.  I will be updating all
    GPTQ READMEs in the next 48 hours to make this clearer.


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/mF8-jebFnjl7Rn0c-jL2t.png)

    '
  created_at: 2023-08-21 20:38:43+00:00
  edited: true
  hidden: false
  id: 64e3d963618cd90997eeaa2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ab8336cfbe073b42c4b3bcfc81fc20c.svg
      fullname: Richard Scott
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RichardScottOZ
      type: user
    createdAt: '2023-08-21T21:41:55.000Z'
    data:
      edited: false
      editors:
      - RichardScottOZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9651654958724976
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ab8336cfbe073b42c4b3bcfc81fc20c.svg
          fullname: Richard Scott
          isHf: false
          isPro: false
          name: RichardScottOZ
          type: user
        html: '<p>Ok, thanks for that - so this is the main branch model?  What is
          suggest for the others, similar?</p>

          '
        raw: Ok, thanks for that - so this is the main branch model?  What is suggest
          for the others, similar?
        updatedAt: '2023-08-21T21:41:55.744Z'
      numEdits: 0
      reactions: []
    id: 64e3da23030431c0c67b8c1e
    type: comment
  author: RichardScottOZ
  content: Ok, thanks for that - so this is the main branch model?  What is suggest
    for the others, similar?
  created_at: 2023-08-21 20:41:55+00:00
  edited: false
  hidden: false
  id: 64e3da23030431c0c67b8c1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-21T21:42:48.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9244047999382019
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Same for all of them. They''re all called <code>model.safetensors</code>
          now, and each branch''s respective <code>quantize_config.json</code> includes
          that, so you don''t need to specify <code>model_basename</code> any more.</p>

          '
        raw: Same for all of them. They're all called `model.safetensors` now, and
          each branch's respective `quantize_config.json` includes that, so you don't
          need to specify `model_basename` any more.
        updatedAt: '2023-08-21T21:42:48.853Z'
      numEdits: 0
      reactions: []
    id: 64e3da58c82940fdbba8b859
    type: comment
  author: TheBloke
  content: Same for all of them. They're all called `model.safetensors` now, and each
    branch's respective `quantize_config.json` includes that, so you don't need to
    specify `model_basename` any more.
  created_at: 2023-08-21 20:42:48+00:00
  edited: false
  hidden: false
  id: 64e3da58c82940fdbba8b859
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Getting error while loading model_basename = "gptq_model-8bit-128g"
