!!python/object:huggingface_hub.community.DiscussionWithDetails
author: chongcy
conflicting_files: null
created_at: 2023-08-15 08:53:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1af6f7f9a2c423ef56001f935d204af7.svg
      fullname: chongcy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chongcy
      type: user
    createdAt: '2023-08-15T09:53:48.000Z'
    data:
      edited: false
      editors:
      - chongcy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9456721544265747
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1af6f7f9a2c423ef56001f935d204af7.svg
          fullname: chongcy
          isHf: false
          isPro: false
          name: chongcy
          type: user
        html: '<p>Hi all, </p>

          <p>Sorry for asking here, but I don''t really understand whats the trade-offs/purpose
          between using bitsandbytes/autogptq for quantization. </p>

          <p>I just tried loading a base meta model using "load_in_8bit=True"  in
          AutoModelForCausalLM.from_pretrained(), the speed was crazy fast. The model
          was converted to 8bit/4bit using the bitsandbytes integration with just
          adding another parameter, instead of needing to quantize with something
          like AutoGPTQ. VRAM usage was also lower, which is good I assume. I am still
          in the exploration phase, trying to understand whats the best and easiest
          way to quantize the model. </p>

          <p>Hope someone could shed a light on this. Thanks.</p>

          '
        raw: "Hi all, \r\n\r\nSorry for asking here, but I don't really understand\
          \ whats the trade-offs/purpose between using bitsandbytes/autogptq for quantization.\
          \ \r\n\r\nI just tried loading a base meta model using \"load_in_8bit=True\"\
          \  in AutoModelForCausalLM.from_pretrained(), the speed was crazy fast.\
          \ The model was converted to 8bit/4bit using the bitsandbytes integration\
          \ with just adding another parameter, instead of needing to quantize with\
          \ something like AutoGPTQ. VRAM usage was also lower, which is good I assume.\
          \ I am still in the exploration phase, trying to understand whats the best\
          \ and easiest way to quantize the model. \r\n\r\nHope someone could shed\
          \ a light on this. Thanks."
        updatedAt: '2023-08-15T09:53:48.837Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pimwipa
    id: 64db4b2c95da0aa95c029ec4
    type: comment
  author: chongcy
  content: "Hi all, \r\n\r\nSorry for asking here, but I don't really understand whats\
    \ the trade-offs/purpose between using bitsandbytes/autogptq for quantization.\
    \ \r\n\r\nI just tried loading a base meta model using \"load_in_8bit=True\" \
    \ in AutoModelForCausalLM.from_pretrained(), the speed was crazy fast. The model\
    \ was converted to 8bit/4bit using the bitsandbytes integration with just adding\
    \ another parameter, instead of needing to quantize with something like AutoGPTQ.\
    \ VRAM usage was also lower, which is good I assume. I am still in the exploration\
    \ phase, trying to understand whats the best and easiest way to quantize the model.\
    \ \r\n\r\nHope someone could shed a light on this. Thanks."
  created_at: 2023-08-15 08:53:48+00:00
  edited: false
  hidden: false
  id: 64db4b2c95da0aa95c029ec4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1560086bf430511981ccb73993354153.svg
      fullname: 'pimwipa '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pimwipa
      type: user
    createdAt: '2023-09-25T12:12:42.000Z'
    data:
      edited: false
      editors:
      - pimwipa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.938017725944519
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1560086bf430511981ccb73993354153.svg
          fullname: 'pimwipa '
          isHf: false
          isPro: false
          name: pimwipa
          type: user
        html: '<p>I just read this <a href="https://huggingface.co/blog/overview-quantization-transformers">https://huggingface.co/blog/overview-quantization-transformers</a>
          and it says that bitsandbytes is easier to use, but a bit slower in larger
          batch size. Now they are both native within Huggingface''s transformers.
          I am also learning about quantization.</p>

          '
        raw: I just read this https://huggingface.co/blog/overview-quantization-transformers
          and it says that bitsandbytes is easier to use, but a bit slower in larger
          batch size. Now they are both native within Huggingface's transformers.
          I am also learning about quantization.
        updatedAt: '2023-09-25T12:12:42.937Z'
      numEdits: 0
      reactions: []
    id: 6511793a9b6b8ff8e989f28a
    type: comment
  author: pimwipa
  content: I just read this https://huggingface.co/blog/overview-quantization-transformers
    and it says that bitsandbytes is easier to use, but a bit slower in larger batch
    size. Now they are both native within Huggingface's transformers. I am also learning
    about quantization.
  created_at: 2023-09-25 11:12:42+00:00
  edited: false
  hidden: false
  id: 6511793a9b6b8ff8e989f28a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-09-25T18:00:58.000Z'
    data:
      edited: true
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9790881872177124
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>Gptq is usually much faster than bitsandbytes and is supposed to
          use less memory? </p>

          <p>I think the reason it was slow might have been that your model was doing
          inference on cpu instead of gpu but bitsandbytes automatically does inference
          on gpu.</p>

          <p>Also, fastest inference right now would be exllama with gptq but that
          only supports llama models or any fine tuned variants like this. It could
          easily reach over 40 tok per sec on a free colab.</p>

          '
        raw: "Gptq is usually much faster than bitsandbytes and is supposed to use\
          \ less memory? \n\nI think the reason it was slow might have been that your\
          \ model was doing inference on cpu instead of gpu but bitsandbytes automatically\
          \ does inference on gpu.\n\nAlso, fastest inference right now would be exllama\
          \ with gptq but that only supports llama models or any fine tuned variants\
          \ like this. It could easily reach over 40 tok per sec on a free colab."
        updatedAt: '2023-09-25T18:01:27.821Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Yhyu13
    id: 6511cada44ab66c44fe6f1d4
    type: comment
  author: YaTharThShaRma999
  content: "Gptq is usually much faster than bitsandbytes and is supposed to use less\
    \ memory? \n\nI think the reason it was slow might have been that your model was\
    \ doing inference on cpu instead of gpu but bitsandbytes automatically does inference\
    \ on gpu.\n\nAlso, fastest inference right now would be exllama with gptq but\
    \ that only supports llama models or any fine tuned variants like this. It could\
    \ easily reach over 40 tok per sec on a free colab."
  created_at: 2023-09-25 17:00:58+00:00
  edited: true
  hidden: false
  id: 6511cada44ab66c44fe6f1d4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 33
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: 'HuggingFace''s bitsandbytes vs AutoGPTQ? '
