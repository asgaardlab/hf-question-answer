!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Samitoo
conflicting_files: null
created_at: 2023-08-03 11:00:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/00a8f9e4e688c8f03abc5ae7d44e9e0a.svg
      fullname: Sami
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Samitoo
      type: user
    createdAt: '2023-08-03T12:00:30.000Z'
    data:
      edited: false
      editors:
      - Samitoo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9296749234199524
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/00a8f9e4e688c8f03abc5ae7d44e9e0a.svg
          fullname: Sami
          isHf: false
          isPro: false
          name: Samitoo
          type: user
        html: '<p>Hello !</p>

          <p>I just wanted to know if the gptq model is the one needed to run the
          model on a gpu? and if so, what type of graphics card do you need to run
          the 7B, 13B or 70B models? And is it possible to run the model on several
          small graphics cards? </p>

          <p>Thank you for your answers !</p>

          '
        raw: "Hello !\r\n\r\nI just wanted to know if the gptq model is the one needed\
          \ to run the model on a gpu? and if so, what type of graphics card do you\
          \ need to run the 7B, 13B or 70B models? And is it possible to run the model\
          \ on several small graphics cards? \r\n\r\nThank you for your answers !"
        updatedAt: '2023-08-03T12:00:30.077Z'
      numEdits: 0
      reactions: []
    id: 64cb96de8174e45ae058e17b
    type: comment
  author: Samitoo
  content: "Hello !\r\n\r\nI just wanted to know if the gptq model is the one needed\
    \ to run the model on a gpu? and if so, what type of graphics card do you need\
    \ to run the 7B, 13B or 70B models? And is it possible to run the model on several\
    \ small graphics cards? \r\n\r\nThank you for your answers !"
  created_at: 2023-08-03 11:00:30+00:00
  edited: false
  hidden: false
  id: 64cb96de8174e45ae058e17b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-05T09:45:15.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.887514054775238
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes, GPTQ is for running on GPU.  Actually, GGML can run on GPU
          as well.  But GPTQ can offer maximum performance.</p>

          <p>The GPU requirements depend on how GPTQ inference is done. If you use
          ExLlama, which is the most performant and efficient GPTQ library at the
          moment, then:</p>

          <ul>

          <li>7B requires a 6GB card</li>

          <li>13B requires a 10GB card</li>

          <li>30B/33B requires a 24GB card, or 2 x 12GB</li>

          <li>65B/70B requires a 48GB card, or 2 x 24GB</li>

          </ul>

          <p>Yes you can split inference across multiple smaller GPUs, eg 2 x 24GB
          is a common way to run 65B and 70B models.  It is slower than using one
          card, but it does work.  For example, using ExLlama with 2 x 4090 24GB GPUs
          can give 18 - 20 tokens/s with 65B and 14-17 tokens/s with 70B.   A single
          48GB card like an A6000 would likely do 20+ tokens/s.</p>

          '
        raw: 'Yes, GPTQ is for running on GPU.  Actually, GGML can run on GPU as well.  But
          GPTQ can offer maximum performance.


          The GPU requirements depend on how GPTQ inference is done. If you use ExLlama,
          which is the most performant and efficient GPTQ library at the moment, then:


          - 7B requires a 6GB card

          - 13B requires a 10GB card

          - 30B/33B requires a 24GB card, or 2 x 12GB

          - 65B/70B requires a 48GB card, or 2 x 24GB


          Yes you can split inference across multiple smaller GPUs, eg 2 x 24GB is
          a common way to run 65B and 70B models.  It is slower than using one card,
          but it does work.  For example, using ExLlama with 2 x 4090 24GB GPUs can
          give 18 - 20 tokens/s with 65B and 14-17 tokens/s with 70B.   A single 48GB
          card like an A6000 would likely do 20+ tokens/s.'
        updatedAt: '2023-08-05T09:45:41.044Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - flyingman
        - mini97
        - Andres-1
    id: 64ce1a2b2f92537fbcd66f4b
    type: comment
  author: TheBloke
  content: 'Yes, GPTQ is for running on GPU.  Actually, GGML can run on GPU as well.  But
    GPTQ can offer maximum performance.


    The GPU requirements depend on how GPTQ inference is done. If you use ExLlama,
    which is the most performant and efficient GPTQ library at the moment, then:


    - 7B requires a 6GB card

    - 13B requires a 10GB card

    - 30B/33B requires a 24GB card, or 2 x 12GB

    - 65B/70B requires a 48GB card, or 2 x 24GB


    Yes you can split inference across multiple smaller GPUs, eg 2 x 24GB is a common
    way to run 65B and 70B models.  It is slower than using one card, but it does
    work.  For example, using ExLlama with 2 x 4090 24GB GPUs can give 18 - 20 tokens/s
    with 65B and 14-17 tokens/s with 70B.   A single 48GB card like an A6000 would
    likely do 20+ tokens/s.'
  created_at: 2023-08-05 08:45:15+00:00
  edited: true
  hidden: false
  id: 64ce1a2b2f92537fbcd66f4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/00a8f9e4e688c8f03abc5ae7d44e9e0a.svg
      fullname: Sami
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Samitoo
      type: user
    createdAt: '2023-08-17T12:32:15.000Z'
    data:
      edited: false
      editors:
      - Samitoo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9432979822158813
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/00a8f9e4e688c8f03abc5ae7d44e9e0a.svg
          fullname: Sami
          isHf: false
          isPro: false
          name: Samitoo
          type: user
        html: '<p>Thank you very much for your answer !</p>

          <p>So if I understand correctly, to use the TheBloke/Llama-2-13B-chat-GPTQ
          model, I would need 10GB of VRAM on my graphics card. But is there a way
          to load the model on an 8GB graphics card for example, and load the rest
          (2GB) on the computer''s RAM?</p>

          <p>In addition, how many simultaneous requests on a 4096 input can be performed
          on this model with a 24GB 3090? I know the model will load 10GB onto the
          board, plus 3GB of runtime kernel, and some for the query. But if the card
          is loaded at 14GB, there are 10GB left, if a request requires 1GB of space,
          does this mean that I could manage 10 requests simultaneously?</p>

          <p>Thank you very much for your answers and your work!</p>

          '
        raw: 'Thank you very much for your answer !


          So if I understand correctly, to use the TheBloke/Llama-2-13B-chat-GPTQ
          model, I would need 10GB of VRAM on my graphics card. But is there a way
          to load the model on an 8GB graphics card for example, and load the rest
          (2GB) on the computer''s RAM?


          In addition, how many simultaneous requests on a 4096 input can be performed
          on this model with a 24GB 3090? I know the model will load 10GB onto the
          board, plus 3GB of runtime kernel, and some for the query. But if the card
          is loaded at 14GB, there are 10GB left, if a request requires 1GB of space,
          does this mean that I could manage 10 requests simultaneously?


          Thank you very much for your answers and your work!'
        updatedAt: '2023-08-17T12:32:15.525Z'
      numEdits: 0
      reactions: []
    id: 64de134fde3048e81c5c4cf2
    type: comment
  author: Samitoo
  content: 'Thank you very much for your answer !


    So if I understand correctly, to use the TheBloke/Llama-2-13B-chat-GPTQ model,
    I would need 10GB of VRAM on my graphics card. But is there a way to load the
    model on an 8GB graphics card for example, and load the rest (2GB) on the computer''s
    RAM?


    In addition, how many simultaneous requests on a 4096 input can be performed on
    this model with a 24GB 3090? I know the model will load 10GB onto the board, plus
    3GB of runtime kernel, and some for the query. But if the card is loaded at 14GB,
    there are 10GB left, if a request requires 1GB of space, does this mean that I
    could manage 10 requests simultaneously?


    Thank you very much for your answers and your work!'
  created_at: 2023-08-17 11:32:15+00:00
  edited: false
  hidden: false
  id: 64de134fde3048e81c5c4cf2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644a6a7f24ee691360ad69e4/-CRkck9aw8cMKouZVEUjl.png?w=200&h=200&f=face
      fullname: m.m
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikeyang01
      type: user
    createdAt: '2023-09-12T07:22:40.000Z'
    data:
      edited: false
      editors:
      - mikeyang01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.937909722328186
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644a6a7f24ee691360ad69e4/-CRkck9aw8cMKouZVEUjl.png?w=200&h=200&f=face
          fullname: m.m
          isHf: false
          isPro: false
          name: mikeyang01
          type: user
        html: '<blockquote>

          <p>Thank you very much for your answer !</p>

          <p>So if I understand correctly, to use the TheBloke/Llama-2-13B-chat-GPTQ
          model, I would need 10GB of VRAM on my graphics card. But is there a way
          to load the model on an 8GB graphics card for example, and load the rest
          (2GB) on the computer''s RAM?</p>

          <p>In addition, how many simultaneous requests on a 4096 input can be performed
          on this model with a 24GB 3090? I know the model will load 10GB onto the
          board, plus 3GB of runtime kernel, and some for the query. But if the card
          is loaded at 14GB, there are 10GB left, if a request requires 1GB of space,
          does this mean that I could manage 10 requests simultaneously?</p>

          <p>Thank you very much for your answers and your work!</p>

          </blockquote>

          <p>Not possible on GPTQ, GPTQ only support split between GPUs</p>

          '
        raw: "> Thank you very much for your answer !\n> \n> So if I understand correctly,\
          \ to use the TheBloke/Llama-2-13B-chat-GPTQ model, I would need 10GB of\
          \ VRAM on my graphics card. But is there a way to load the model on an 8GB\
          \ graphics card for example, and load the rest (2GB) on the computer's RAM?\n\
          > \n> In addition, how many simultaneous requests on a 4096 input can be\
          \ performed on this model with a 24GB 3090? I know the model will load 10GB\
          \ onto the board, plus 3GB of runtime kernel, and some for the query. But\
          \ if the card is loaded at 14GB, there are 10GB left, if a request requires\
          \ 1GB of space, does this mean that I could manage 10 requests simultaneously?\n\
          > \n> Thank you very much for your answers and your work!\n\nNot possible\
          \ on GPTQ, GPTQ only support split between GPUs"
        updatedAt: '2023-09-12T07:22:40.706Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Yhyu13
        - mikeyang01
    id: 650011c00e8369f6a8e0932d
    type: comment
  author: mikeyang01
  content: "> Thank you very much for your answer !\n> \n> So if I understand correctly,\
    \ to use the TheBloke/Llama-2-13B-chat-GPTQ model, I would need 10GB of VRAM on\
    \ my graphics card. But is there a way to load the model on an 8GB graphics card\
    \ for example, and load the rest (2GB) on the computer's RAM?\n> \n> In addition,\
    \ how many simultaneous requests on a 4096 input can be performed on this model\
    \ with a 24GB 3090? I know the model will load 10GB onto the board, plus 3GB of\
    \ runtime kernel, and some for the query. But if the card is loaded at 14GB, there\
    \ are 10GB left, if a request requires 1GB of space, does this mean that I could\
    \ manage 10 requests simultaneously?\n> \n> Thank you very much for your answers\
    \ and your work!\n\nNot possible on GPTQ, GPTQ only support split between GPUs"
  created_at: 2023-09-12 06:22:40+00:00
  edited: false
  hidden: false
  id: 650011c00e8369f6a8e0932d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/042bea4a43fb2ff037fa7eb05f39af26.svg
      fullname: saif hassan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saifhassan
      type: user
    createdAt: '2023-10-09T22:16:45.000Z'
    data:
      edited: false
      editors:
      - saifhassan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8829721808433533
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/042bea4a43fb2ff037fa7eb05f39af26.svg
          fullname: saif hassan
          isHf: false
          isPro: false
          name: saifhassan
          type: user
        html: '<blockquote>

          <p>Yes, GPTQ is for running on GPU.  Actually, GGML can run on GPU as well.  But
          GPTQ can offer maximum performance.</p>

          <p>The GPU requirements depend on how GPTQ inference is done. If you use
          ExLlama, which is the most performant and efficient GPTQ library at the
          moment, then:</p>

          <ul>

          <li>7B requires a 6GB card</li>

          <li>13B requires a 10GB card</li>

          <li>30B/33B requires a 24GB card, or 2 x 12GB</li>

          <li>65B/70B requires a 48GB card, or 2 x 24GB</li>

          </ul>

          <p>Yes you can split inference across multiple smaller GPUs, eg 2 x 24GB
          is a common way to run 65B and 70B models.  It is slower than using one
          card, but it does work.  For example, using ExLlama with 2 x 4090 24GB GPUs
          can give 18 - 20 tokens/s with 65B and 14-17 tokens/s with 70B.   A single
          48GB card like an A6000 would likely do 20+ tokens/s.</p>

          </blockquote>

          <p>I have 2 GPUs (Each of 8 GB), and I want to use 13B, please guide how
          can I use 2 GPU?</p>

          '
        raw: "> Yes, GPTQ is for running on GPU.  Actually, GGML can run on GPU as\
          \ well.  But GPTQ can offer maximum performance.\n> \n> The GPU requirements\
          \ depend on how GPTQ inference is done. If you use ExLlama, which is the\
          \ most performant and efficient GPTQ library at the moment, then:\n> \n\
          > - 7B requires a 6GB card\n> - 13B requires a 10GB card\n> - 30B/33B requires\
          \ a 24GB card, or 2 x 12GB\n> - 65B/70B requires a 48GB card, or 2 x 24GB\n\
          > \n> Yes you can split inference across multiple smaller GPUs, eg 2 x 24GB\
          \ is a common way to run 65B and 70B models.  It is slower than using one\
          \ card, but it does work.  For example, using ExLlama with 2 x 4090 24GB\
          \ GPUs can give 18 - 20 tokens/s with 65B and 14-17 tokens/s with 70B. \
          \  A single 48GB card like an A6000 would likely do 20+ tokens/s.\n\nI have\
          \ 2 GPUs (Each of 8 GB), and I want to use 13B, please guide how can I use\
          \ 2 GPU?"
        updatedAt: '2023-10-09T22:16:45.218Z'
      numEdits: 0
      reactions: []
    id: 65247bcdc1053c2a11a732dc
    type: comment
  author: saifhassan
  content: "> Yes, GPTQ is for running on GPU.  Actually, GGML can run on GPU as well.\
    \  But GPTQ can offer maximum performance.\n> \n> The GPU requirements depend\
    \ on how GPTQ inference is done. If you use ExLlama, which is the most performant\
    \ and efficient GPTQ library at the moment, then:\n> \n> - 7B requires a 6GB card\n\
    > - 13B requires a 10GB card\n> - 30B/33B requires a 24GB card, or 2 x 12GB\n\
    > - 65B/70B requires a 48GB card, or 2 x 24GB\n> \n> Yes you can split inference\
    \ across multiple smaller GPUs, eg 2 x 24GB is a common way to run 65B and 70B\
    \ models.  It is slower than using one card, but it does work.  For example, using\
    \ ExLlama with 2 x 4090 24GB GPUs can give 18 - 20 tokens/s with 65B and 14-17\
    \ tokens/s with 70B.   A single 48GB card like an A6000 would likely do 20+ tokens/s.\n\
    \nI have 2 GPUs (Each of 8 GB), and I want to use 13B, please guide how can I\
    \ use 2 GPU?"
  created_at: 2023-10-09 21:16:45+00:00
  edited: false
  hidden: false
  id: 65247bcdc1053c2a11a732dc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Necessary material for llama2
