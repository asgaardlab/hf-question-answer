!!python/object:huggingface_hub.community.DiscussionWithDetails
author: anon7463435254
conflicting_files: null
created_at: 2023-07-24 09:31:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
      fullname: anon7463435254
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon7463435254
      type: user
    createdAt: '2023-07-24T10:31:11.000Z'
    data:
      edited: false
      editors:
      - anon7463435254
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5415104031562805
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
          fullname: anon7463435254
          isHf: false
          isPro: false
          name: anon7463435254
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/_r61S3gxurvv1SBZQCb2y.png"><img
          alt="model settings.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/_r61S3gxurvv1SBZQCb2y.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/BV3_5ZKEPIcadQIy2B3Mw.png"><img
          alt="parameters.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/BV3_5ZKEPIcadQIy2B3Mw.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/rnB23vpagp_Nc_k_b9t-2.png"><img
          alt="instruction template.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/rnB23vpagp_Nc_k_b9t-2.png"></a></p>

          <p>All the responses are like the following:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/-o1Frj4BWgP4X9SLLMRrV.png"><img
          alt="chat.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/-o1Frj4BWgP4X9SLLMRrV.png"></a></p>

          <p>Why is that?</p>

          <p>Thanks.</p>

          '
        raw: "\r\n![model settings.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/_r61S3gxurvv1SBZQCb2y.png)\r\
          \n\r\n\r\n![parameters.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/BV3_5ZKEPIcadQIy2B3Mw.png)\r\
          \n\r\n\r\n![instruction template.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/rnB23vpagp_Nc_k_b9t-2.png)\r\
          \n\r\n\r\nAll the responses are like the following:\r\n\r\n![chat.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/-o1Frj4BWgP4X9SLLMRrV.png)\r\
          \n\r\nWhy is that?\r\n\r\nThanks."
        updatedAt: '2023-07-24T10:31:11.240Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Jaredc
    id: 64be52ef46cc3cdfbb0b4afd
    type: comment
  author: anon7463435254
  content: "\r\n![model settings.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/_r61S3gxurvv1SBZQCb2y.png)\r\
    \n\r\n\r\n![parameters.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/BV3_5ZKEPIcadQIy2B3Mw.png)\r\
    \n\r\n\r\n![instruction template.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/rnB23vpagp_Nc_k_b9t-2.png)\r\
    \n\r\n\r\nAll the responses are like the following:\r\n\r\n![chat.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/-o1Frj4BWgP4X9SLLMRrV.png)\r\
    \n\r\nWhy is that?\r\n\r\nThanks."
  created_at: 2023-07-24 09:31:11+00:00
  edited: false
  hidden: false
  id: 64be52ef46cc3cdfbb0b4afd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T10:46:00.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9769742488861084
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Hmm yeah you''re right.  AutoGPTQ is producing gibberish with this
          file.</p>

          <p>In any case I would recommend you use ExLlama as the Loader, as it will
          be much faster than AutoGPTQ.  And it works fine with this file, I just
          tested it.</p>

          <p>But I need to investigate why AutoGPTQ cannot do inference from this
          file, and I will report that as a bug.</p>

          '
        raw: 'Hmm yeah you''re right.  AutoGPTQ is producing gibberish with this file.


          In any case I would recommend you use ExLlama as the Loader, as it will
          be much faster than AutoGPTQ.  And it works fine with this file, I just
          tested it.


          But I need to investigate why AutoGPTQ cannot do inference from this file,
          and I will report that as a bug.'
        updatedAt: '2023-07-24T10:46:00.943Z'
      numEdits: 0
      reactions: []
    id: 64be56681d40292dd3043b6c
    type: comment
  author: TheBloke
  content: 'Hmm yeah you''re right.  AutoGPTQ is producing gibberish with this file.


    In any case I would recommend you use ExLlama as the Loader, as it will be much
    faster than AutoGPTQ.  And it works fine with this file, I just tested it.


    But I need to investigate why AutoGPTQ cannot do inference from this file, and
    I will report that as a bug.'
  created_at: 2023-07-24 09:46:00+00:00
  edited: false
  hidden: false
  id: 64be56681d40292dd3043b6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T10:57:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.951340913772583
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It''s a bug in AutoGPTQ 0.3.0</p>

          <p>If you really want to use AutoGPTQ for some reason, please downgrade
          to AutoGPTQ 0.2.2 and it will work - but it will be slow.</p>

          <p>I will report this as a bug in AutoGPTQ but I don''t know when it might
          be fixed</p>

          '
        raw: 'It''s a bug in AutoGPTQ 0.3.0


          If you really want to use AutoGPTQ for some reason, please downgrade to
          AutoGPTQ 0.2.2 and it will work - but it will be slow.


          I will report this as a bug in AutoGPTQ but I don''t know when it might
          be fixed'
        updatedAt: '2023-07-24T10:57:25.228Z'
      numEdits: 0
      reactions: []
    id: 64be5915565b827f7eca6422
    type: comment
  author: TheBloke
  content: 'It''s a bug in AutoGPTQ 0.3.0


    If you really want to use AutoGPTQ for some reason, please downgrade to AutoGPTQ
    0.2.2 and it will work - but it will be slow.


    I will report this as a bug in AutoGPTQ but I don''t know when it might be fixed'
  created_at: 2023-07-24 09:57:25+00:00
  edited: false
  hidden: false
  id: 64be5915565b827f7eca6422
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T11:14:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9412909150123596
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>So, to summarise:</p>

          <ol>

          <li>I recommend you use ExLlama anyway, as it is faster</li>

          <li>If you really want to use AutoGPTQ, downgrade to 0.2.2</li>

          <li>I have raised this as a bug in 0.3.0, which you can track here: <a rel="nofollow"
          href="https://github.com/PanQiWei/AutoGPTQ/issues/201">https://github.com/PanQiWei/AutoGPTQ/issues/201</a></li>

          </ol>

          '
        raw: 'So, to summarise:

          1. I recommend you use ExLlama anyway, as it is faster

          2. If you really want to use AutoGPTQ, downgrade to 0.2.2

          3. I have raised this as a bug in 0.3.0, which you can track here: https://github.com/PanQiWei/AutoGPTQ/issues/201'
        updatedAt: '2023-07-24T11:14:20.551Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Jaredc
        - xpgx1
    id: 64be5d0c8e051085ba1d3436
    type: comment
  author: TheBloke
  content: 'So, to summarise:

    1. I recommend you use ExLlama anyway, as it is faster

    2. If you really want to use AutoGPTQ, downgrade to 0.2.2

    3. I have raised this as a bug in 0.3.0, which you can track here: https://github.com/PanQiWei/AutoGPTQ/issues/201'
  created_at: 2023-07-24 10:14:20+00:00
  edited: false
  hidden: false
  id: 64be5d0c8e051085ba1d3436
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
      fullname: anon7463435254
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon7463435254
      type: user
    createdAt: '2023-07-25T11:05:44.000Z'
    data:
      edited: false
      editors:
      - anon7463435254
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8956375122070312
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
          fullname: anon7463435254
          isHf: false
          isPro: false
          name: anon7463435254
          type: user
        html: '<p>Thank you very much, man. I also found a possible bug using the
          ggml files. Hoping to help, I''m gonna open a discussion on the 13B-chat-ggml.</p>

          '
        raw: 'Thank you very much, man. I also found a possible bug using the ggml
          files. Hoping to help, I''m gonna open a discussion on the 13B-chat-ggml.


          '
        updatedAt: '2023-07-25T11:05:44.763Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64bfac8801f1983a86c178a8
    id: 64bfac8801f1983a86c178a7
    type: comment
  author: anon7463435254
  content: 'Thank you very much, man. I also found a possible bug using the ggml files.
    Hoping to help, I''m gonna open a discussion on the 13B-chat-ggml.


    '
  created_at: 2023-07-25 10:05:44+00:00
  edited: false
  hidden: false
  id: 64bfac8801f1983a86c178a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
      fullname: anon7463435254
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon7463435254
      type: user
    createdAt: '2023-07-25T11:05:44.000Z'
    data:
      status: closed
    id: 64bfac8801f1983a86c178a8
    type: status-change
  author: anon7463435254
  created_at: 2023-07-25 10:05:44+00:00
  id: 64bfac8801f1983a86c178a8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: closed
target_branch: null
title: Totally unusable from branch 4bit 32g (screenshots included)
