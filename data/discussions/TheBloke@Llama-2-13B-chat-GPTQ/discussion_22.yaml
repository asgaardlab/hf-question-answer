!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ML-Butler
conflicting_files: null
created_at: 2023-07-27 20:10:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c30b6cb14f3a0af8d35b87af52cb2280.svg
      fullname: John Keller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ML-Butler
      type: user
    createdAt: '2023-07-27T21:10:35.000Z'
    data:
      edited: false
      editors:
      - ML-Butler
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6548976302146912
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c30b6cb14f3a0af8d35b87af52cb2280.svg
          fullname: John Keller
          isHf: false
          isPro: false
          name: ML-Butler
          type: user
        html: '<p>I got this warning while loading up the model. It didn''t show up
          in the UI so I thought it was just hanging, but there it was in the command
          line.</p>

          <p>2023-07-27 15:46:39 INFO:Loading TheBloke_Llama-2-13B-chat-GPTQ_gptq-4bit-32g-actorder_True...<br>2023-07-27
          15:46:39 INFO:The AutoGPTQ params are: {''model_basename'': ''gptq_model-4bit-32g'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'':
          True, ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
          False, ''max_memory'': None, ''quantize_config'': None, ''use_cuda_fp16'':
          True}<br>2023-07-27 15:46:42 WARNING:The safetensors archive passed at models\TheBloke_Llama-2-13B-chat-GPTQ_gptq-4bit-32g-actorder_True\gptq_model-4bit-32g.safetensors
          does not contain metadata. Make sure to save your model with the <code>save_pretrained</code>
          method. Defaulting to ''pt'' metadata.<br>Press any key to continue . .
          .</p>

          '
        raw: "I got this warning while loading up the model. It didn't show up in\
          \ the UI so I thought it was just hanging, but there it was in the command\
          \ line.\r\n\r\n2023-07-27 15:46:39 INFO:Loading TheBloke_Llama-2-13B-chat-GPTQ_gptq-4bit-32g-actorder_True...\r\
          \n2023-07-27 15:46:39 INFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit-32g',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': None, 'use_cuda_fp16': True}\r\
          \n2023-07-27 15:46:42 WARNING:The safetensors archive passed at models\\\
          TheBloke_Llama-2-13B-chat-GPTQ_gptq-4bit-32g-actorder_True\\gptq_model-4bit-32g.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\r\nPress any key to continue . .\
          \ ."
        updatedAt: '2023-07-27T21:10:35.495Z'
      numEdits: 0
      reactions: []
    id: 64c2dd4ba8fc30cafc6d6506
    type: comment
  author: ML-Butler
  content: "I got this warning while loading up the model. It didn't show up in the\
    \ UI so I thought it was just hanging, but there it was in the command line.\r\
    \n\r\n2023-07-27 15:46:39 INFO:Loading TheBloke_Llama-2-13B-chat-GPTQ_gptq-4bit-32g-actorder_True...\r\
    \n2023-07-27 15:46:39 INFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit-32g',\
    \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp':\
    \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': None,\
    \ 'quantize_config': None, 'use_cuda_fp16': True}\r\n2023-07-27 15:46:42 WARNING:The\
    \ safetensors archive passed at models\\TheBloke_Llama-2-13B-chat-GPTQ_gptq-4bit-32g-actorder_True\\\
    gptq_model-4bit-32g.safetensors does not contain metadata. Make sure to save your\
    \ model with the `save_pretrained` method. Defaulting to 'pt' metadata.\r\nPress\
    \ any key to continue . . ."
  created_at: 2023-07-27 20:10:35+00:00
  edited: false
  hidden: false
  id: 64c2dd4ba8fc30cafc6d6506
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-27T21:16:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9734465479850769
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The message about safetensors not having metadata is fine.  It will
          no longer show for my more recent GPTQ files, but it''s never been a problem.  Nothing
          reads metadata, and ideally that message would never have been shown as
          it doesn''t affect the user at all.</p>

          <p>Your issue is the "Press any key to continue.." at the end. This is what
          happens on Windows when you don''t have enough Pagefile.  The model has
          to be loaded to RAM before it''s moved to VRAM. And even if you have plenty
          of RAM - like 128GB - Windows still seems to need a large amount of Pagefile.  The
          fix is to make sure you have a PAgefile set of at least 100GB.  You can
          either do this manually, or set your Pagefile to Auto and make sure you
          have at least 100GB free on the drive with the pagefile (C: by default)</p>

          '
        raw: 'The message about safetensors not having metadata is fine.  It will
          no longer show for my more recent GPTQ files, but it''s never been a problem.  Nothing
          reads metadata, and ideally that message would never have been shown as
          it doesn''t affect the user at all.


          Your issue is the "Press any key to continue.." at the end. This is what
          happens on Windows when you don''t have enough Pagefile.  The model has
          to be loaded to RAM before it''s moved to VRAM. And even if you have plenty
          of RAM - like 128GB - Windows still seems to need a large amount of Pagefile.  The
          fix is to make sure you have a PAgefile set of at least 100GB.  You can
          either do this manually, or set your Pagefile to Auto and make sure you
          have at least 100GB free on the drive with the pagefile (C: by default)'
        updatedAt: '2023-07-27T21:16:20.855Z'
      numEdits: 0
      reactions: []
    id: 64c2dea44ec998825a232592
    type: comment
  author: TheBloke
  content: 'The message about safetensors not having metadata is fine.  It will no
    longer show for my more recent GPTQ files, but it''s never been a problem.  Nothing
    reads metadata, and ideally that message would never have been shown as it doesn''t
    affect the user at all.


    Your issue is the "Press any key to continue.." at the end. This is what happens
    on Windows when you don''t have enough Pagefile.  The model has to be loaded to
    RAM before it''s moved to VRAM. And even if you have plenty of RAM - like 128GB
    - Windows still seems to need a large amount of Pagefile.  The fix is to make
    sure you have a PAgefile set of at least 100GB.  You can either do this manually,
    or set your Pagefile to Auto and make sure you have at least 100GB free on the
    drive with the pagefile (C: by default)'
  created_at: 2023-07-27 20:16:20+00:00
  edited: false
  hidden: false
  id: 64c2dea44ec998825a232592
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c30b6cb14f3a0af8d35b87af52cb2280.svg
      fullname: John Keller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ML-Butler
      type: user
    createdAt: '2023-07-30T21:10:17.000Z'
    data:
      edited: false
      editors:
      - ML-Butler
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9889447093009949
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c30b6cb14f3a0af8d35b87af52cb2280.svg
          fullname: John Keller
          isHf: false
          isPro: false
          name: ML-Butler
          type: user
        html: '<p>As belated follow up...the virtual memory tweaks made the difference.
          However, the model load time was in excess of 70+ seconds and then the output
          I was getting was gibberish. I changed the loader from AutoGPTQ to ExLlama-HF.
          Although the load time was similar, the gibberish was gone. </p>

          '
        raw: 'As belated follow up...the virtual memory tweaks made the difference.
          However, the model load time was in excess of 70+ seconds and then the output
          I was getting was gibberish. I changed the loader from AutoGPTQ to ExLlama-HF.
          Although the load time was similar, the gibberish was gone. '
        updatedAt: '2023-07-30T21:10:17.378Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64c6d1b977a6473ac3176174
    id: 64c6d1b977a6473ac3176172
    type: comment
  author: ML-Butler
  content: 'As belated follow up...the virtual memory tweaks made the difference.
    However, the model load time was in excess of 70+ seconds and then the output
    I was getting was gibberish. I changed the loader from AutoGPTQ to ExLlama-HF.
    Although the load time was similar, the gibberish was gone. '
  created_at: 2023-07-30 20:10:17+00:00
  edited: false
  hidden: false
  id: 64c6d1b977a6473ac3176172
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/c30b6cb14f3a0af8d35b87af52cb2280.svg
      fullname: John Keller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ML-Butler
      type: user
    createdAt: '2023-07-30T21:10:17.000Z'
    data:
      status: closed
    id: 64c6d1b977a6473ac3176174
    type: status-change
  author: ML-Butler
  created_at: 2023-07-30 20:10:17+00:00
  id: 64c6d1b977a6473ac3176174
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: closed
target_branch: null
title: Safetensors warning while loading model
