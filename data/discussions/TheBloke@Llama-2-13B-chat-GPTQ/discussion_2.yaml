!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Lolkid654
conflicting_files: null
created_at: 2023-07-18 18:57:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80c8ae4a89932319b6e304a5c26e6335.svg
      fullname: Hector Gonzalez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lolkid654
      type: user
    createdAt: '2023-07-18T19:57:58.000Z'
    data:
      edited: true
      editors:
      - Lolkid654
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4684010148048401
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80c8ae4a89932319b6e304a5c26e6335.svg
          fullname: Hector Gonzalez
          isHf: false
          isPro: false
          name: Lolkid654
          type: user
        html: '<p>is anyone getting this on OOBABOOGA or is it just me?</p>

          <p>2023-07-18 15:56:34 INFO:Loading TheBloke_Llama-2-13B-chat-GPTQ...<br>2023-07-18
          15:56:34 ERROR:Failed to load the model.<br>Traceback (most recent call
          last):<br>  File "D:\OOBABOOGA\text-generation-webui\server.py", line 68,
          in load_model_wrapper<br>    shared.model, shared.tokenizer = load_model(shared.model_name,
          loader)<br>  File "D:\OOBABOOGA\text-generation-webui\modules\models.py",
          line 79, in load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "D:\OOBABOOGA\text-generation-webui\modules\models.py", line 327, in ExLlama_HF_loader<br>    return
          ExllamaHF.from_pretrained(model_name)<br>  File "D:\OOBABOOGA\text-generation-webui\modules\exllama_hf.py",
          line 126, in from_pretrained<br>    return ExllamaHF(config)<br>  File "D:\OOBABOOGA\text-generation-webui\modules\exllama_hf.py",
          line 31, in <strong>init</strong><br>    self.ex_model = ExLlama(self.ex_config)<br>  File
          "D:\OOBABOOGA\installer_files\env\lib\site-packages\exllama\model.py", line
          660, in <strong>init</strong><br>    with safe_open(self.config.model_path,
          framework = "pt", device = "cpu") as f:<br>safetensors_rust.SafetensorError:
          Error while deserializing header: MetadataIncompleteBuffer</p>

          <p>NVM, it was error while downloading. Needed to redownload. Ignore this.</p>

          '
        raw: "is anyone getting this on OOBABOOGA or is it just me?\n\n2023-07-18\
          \ 15:56:34 INFO:Loading TheBloke_Llama-2-13B-chat-GPTQ...\n2023-07-18 15:56:34\
          \ ERROR:Failed to load the model.\nTraceback (most recent call last):\n\
          \  File \"D:\\OOBABOOGA\\text-generation-webui\\server.py\", line 68, in\
          \ load_model_wrapper\n    shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader)\n  File \"D:\\OOBABOOGA\\text-generation-webui\\modules\\models.py\"\
          , line 79, in load_model\n    output = load_func_map[loader](model_name)\n\
          \  File \"D:\\OOBABOOGA\\text-generation-webui\\modules\\models.py\", line\
          \ 327, in ExLlama_HF_loader\n    return ExllamaHF.from_pretrained(model_name)\n\
          \  File \"D:\\OOBABOOGA\\text-generation-webui\\modules\\exllama_hf.py\"\
          , line 126, in from_pretrained\n    return ExllamaHF(config)\n  File \"\
          D:\\OOBABOOGA\\text-generation-webui\\modules\\exllama_hf.py\", line 31,\
          \ in __init__\n    self.ex_model = ExLlama(self.ex_config)\n  File \"D:\\\
          OOBABOOGA\\installer_files\\env\\lib\\site-packages\\exllama\\model.py\"\
          , line 660, in __init__\n    with safe_open(self.config.model_path, framework\
          \ = \"pt\", device = \"cpu\") as f:\nsafetensors_rust.SafetensorError: Error\
          \ while deserializing header: MetadataIncompleteBuffer\n\n\n\nNVM, it was\
          \ error while downloading. Needed to redownload. Ignore this."
        updatedAt: '2023-07-18T20:00:33.157Z'
      numEdits: 1
      reactions: []
    id: 64b6eec6f92b20f7a36fd660
    type: comment
  author: Lolkid654
  content: "is anyone getting this on OOBABOOGA or is it just me?\n\n2023-07-18 15:56:34\
    \ INFO:Loading TheBloke_Llama-2-13B-chat-GPTQ...\n2023-07-18 15:56:34 ERROR:Failed\
    \ to load the model.\nTraceback (most recent call last):\n  File \"D:\\OOBABOOGA\\\
    text-generation-webui\\server.py\", line 68, in load_model_wrapper\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name, loader)\n  File \"D:\\OOBABOOGA\\\
    text-generation-webui\\modules\\models.py\", line 79, in load_model\n    output\
    \ = load_func_map[loader](model_name)\n  File \"D:\\OOBABOOGA\\text-generation-webui\\\
    modules\\models.py\", line 327, in ExLlama_HF_loader\n    return ExllamaHF.from_pretrained(model_name)\n\
    \  File \"D:\\OOBABOOGA\\text-generation-webui\\modules\\exllama_hf.py\", line\
    \ 126, in from_pretrained\n    return ExllamaHF(config)\n  File \"D:\\OOBABOOGA\\\
    text-generation-webui\\modules\\exllama_hf.py\", line 31, in __init__\n    self.ex_model\
    \ = ExLlama(self.ex_config)\n  File \"D:\\OOBABOOGA\\installer_files\\env\\lib\\\
    site-packages\\exllama\\model.py\", line 660, in __init__\n    with safe_open(self.config.model_path,\
    \ framework = \"pt\", device = \"cpu\") as f:\nsafetensors_rust.SafetensorError:\
    \ Error while deserializing header: MetadataIncompleteBuffer\n\n\n\nNVM, it was\
    \ error while downloading. Needed to redownload. Ignore this."
  created_at: 2023-07-18 18:57:58+00:00
  edited: true
  hidden: false
  id: 64b6eec6f92b20f7a36fd660
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/80c8ae4a89932319b6e304a5c26e6335.svg
      fullname: Hector Gonzalez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lolkid654
      type: user
    createdAt: '2023-07-18T20:01:00.000Z'
    data:
      from: 'Error while deserializing header: MetadataIncompleteBuffer'
      to: 'Error while deserializing header: MetadataIncompleteBuffer (nvm, fixed)'
    id: 64b6ef7c45f3511db2f4eb92
    type: title-change
  author: Lolkid654
  created_at: 2023-07-18 19:01:00+00:00
  id: 64b6ef7c45f3511db2f4eb92
  new_title: 'Error while deserializing header: MetadataIncompleteBuffer (nvm, fixed)'
  old_title: 'Error while deserializing header: MetadataIncompleteBuffer'
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/80c8ae4a89932319b6e304a5c26e6335.svg
      fullname: Hector Gonzalez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lolkid654
      type: user
    createdAt: '2023-07-18T20:01:29.000Z'
    data:
      status: closed
    id: 64b6ef99bdf37897306eddab
    type: status-change
  author: Lolkid654
  created_at: 2023-07-18 19:01:29+00:00
  id: 64b6ef99bdf37897306eddab
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: closed
target_branch: null
title: 'Error while deserializing header: MetadataIncompleteBuffer (nvm, fixed)'
