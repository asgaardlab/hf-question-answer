!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Osamarafique998
conflicting_files: null
created_at: 2023-07-24 08:28:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
      fullname: Mohammad Osama Rafique
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Osamarafique998
      type: user
    createdAt: '2023-07-24T09:28:50.000Z'
    data:
      edited: true
      editors:
      - Osamarafique998
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47408053278923035
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
          fullname: Mohammad Osama Rafique
          isHf: false
          isPro: false
          name: Osamarafique998
          type: user
        html: '<p>I am facing an error while loading the model i have install the
          auto_gptq and transformer libraries but while loading commands runs it gives
          me the quantized_config.json file not found error below i have mentioned
          my code and exact error im getting</p>

          <p>Code:<br>from transformers import AutoTokenizer, pipeline, logging<br>from
          auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig</p>

          <p>model_name_or_path = "TheBloke/Llama-2-13B-chat-GPTQ"<br>model_basename
          = "gptq_model-4bit-128g"</p>

          <p>use_triton = False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>        model_basename=model_basename,<br>        use_safetensors=True,<br>        trust_remote_code=True,<br>        device="cuda:0",<br>        use_triton=use_triton,<br>        quantize_config=None)</p>

          <p>Error:<br>FileNotFoundError: [Errno 2] No such file or directory: ''TheBloke/Llama-2-13B-chat-GPTQ/quantize_config.json''</p>

          '
        raw: "I am facing an error while loading the model i have install the auto_gptq\
          \ and transformer libraries but while loading commands runs it gives me\
          \ the quantized_config.json file not found error below i have mentioned\
          \ my code and exact error im getting\n\n\nCode:\nfrom transformers import\
          \ AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\n\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\
          \nmodel_basename = \"gptq_model-4bit-128g\"\n\nuse_triton = False\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\n\
          model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n       \
          \ model_basename=model_basename,\n        use_safetensors=True,\n      \
          \  trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n\n\n\nError:\nFileNotFoundError: [Errno\
          \ 2] No such file or directory: 'TheBloke/Llama-2-13B-chat-GPTQ/quantize_config.json'"
        updatedAt: '2023-07-24T09:33:04.677Z'
      numEdits: 2
      reactions: []
    id: 64be4452e38420aabaea45ef
    type: comment
  author: Osamarafique998
  content: "I am facing an error while loading the model i have install the auto_gptq\
    \ and transformer libraries but while loading commands runs it gives me the quantized_config.json\
    \ file not found error below i have mentioned my code and exact error im getting\n\
    \n\nCode:\nfrom transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
    \ import AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\
    \nmodel_basename = \"gptq_model-4bit-128g\"\n\nuse_triton = False\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nmodel\
    \ = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n        model_basename=model_basename,\n\
    \        use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
    cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\n\n\
    \nError:\nFileNotFoundError: [Errno 2] No such file or directory: 'TheBloke/Llama-2-13B-chat-GPTQ/quantize_config.json'"
  created_at: 2023-07-24 08:28:50+00:00
  edited: true
  hidden: false
  id: 64be4452e38420aabaea45ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
      fullname: Mohammad Osama Rafique
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Osamarafique998
      type: user
    createdAt: '2023-07-24T09:33:48.000Z'
    data:
      edited: false
      editors:
      - Osamarafique998
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41267260909080505
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
          fullname: Mohammad Osama Rafique
          isHf: false
          isPro: false
          name: Osamarafique998
          type: user
        html: '<p>This is the complete error i am getting</p>

          <p>FileNotFoundError                         Traceback (most recent call
          last)<br>Cell In[10], line 11<br>      7 use_triton = False<br>      9 tokenizer
          = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)<br>---&gt;
          11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>     12         model_basename=model_basename,<br>     13         use_safetensors=True,<br>     14         trust_remote_code=True,<br>     15         device="cuda:0",<br>     16         use_triton=use_triton,<br>     17         quantize_config=None)<br>     19
          """<br>     20 To download from a specific branch, use the revision parameter,
          as in this example:<br>     21<br>   (...)<br>     28         quantize_config=None)<br>     29
          """<br>     31 prompt = "Tell me about AI"</p>

          <p>File /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:63,
          in AutoGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,
          use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)<br>     49
          @classmethod<br>     50 def from_quantized(<br>     51     cls,<br>   (...)<br>     60     trust_remote_code:
          bool = False<br>     61 ) -&gt; BaseGPTQForCausalLM:<br>     62     model_type
          = check_and_get_model_type(save_dir)<br>---&gt; 63     return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(<br>     64         save_dir=save_dir,<br>     65         device=device,<br>     66         use_safetensors=use_safetensors,<br>     67         use_triton=use_triton,<br>     68         max_memory=max_memory,<br>     69         device_map=device_map,<br>     70         quantize_config=quantize_config,<br>     71         model_basename=model_basename,<br>     72         trust_remote_code=trust_remote_code<br>     73     )</p>

          <p>File /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:501,
          in BaseGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,
          use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)<br>    498     raise
          TypeError(f"{config.model_type} isn''t supported yet.")<br>    500 if quantize_config
          is None:<br>--&gt; 501     quantize_config = BaseQuantizeConfig.from_pretrained(save_dir)<br>    503
          if model_basename is None:<br>    504     model_basename = f"gptq_model-{quantize_config.bits}bit-{quantize_config.group_size}g"</p>

          <p>File /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:51,
          in BaseQuantizeConfig.from_pretrained(cls, save_dir)<br>     49 @classmethod<br>     50
          def from_pretrained(cls, save_dir: str):<br>---&gt; 51     with open(join(save_dir,
          "quantize_config.json"), "r", encoding="utf-8") as f:<br>     52         return
          cls(**json.load(f))</p>

          <p>FileNotFoundError: [Errno 2] No such file or directory: ''TheBloke/Llama-2-13B-chat-GPTQ/quantize_config.json''</p>

          '
        raw: "This is the complete error i am getting\n\n\nFileNotFoundError     \
          \                    Traceback (most recent call last)\nCell In[10], line\
          \ 11\n      7 use_triton = False\n      9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n---> 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \     12         model_basename=model_basename,\n     13         use_safetensors=True,\n\
          \     14         trust_remote_code=True,\n     15         device=\"cuda:0\"\
          ,\n     16         use_triton=use_triton,\n     17         quantize_config=None)\n\
          \     19 \"\"\"\n     20 To download from a specific branch, use the revision\
          \ parameter, as in this example:\n     21 \n   (...)\n     28         quantize_config=None)\n\
          \     29 \"\"\"\n     31 prompt = \"Tell me about AI\"\n\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:63,\
          \ in AutoGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,\
          \ use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
          \     49 @classmethod\n     50 def from_quantized(\n     51     cls,\n \
          \  (...)\n     60     trust_remote_code: bool = False\n     61 ) -> BaseGPTQForCausalLM:\n\
          \     62     model_type = check_and_get_model_type(save_dir)\n---> 63  \
          \   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\n     64\
          \         save_dir=save_dir,\n     65         device=device,\n     66  \
          \       use_safetensors=use_safetensors,\n     67         use_triton=use_triton,\n\
          \     68         max_memory=max_memory,\n     69         device_map=device_map,\n\
          \     70         quantize_config=quantize_config,\n     71         model_basename=model_basename,\n\
          \     72         trust_remote_code=trust_remote_code\n     73     )\n\n\
          File /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:501,\
          \ in BaseGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,\
          \ use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
          \    498     raise TypeError(f\"{config.model_type} isn't supported yet.\"\
          )\n    500 if quantize_config is None:\n--> 501     quantize_config = BaseQuantizeConfig.from_pretrained(save_dir)\n\
          \    503 if model_basename is None:\n    504     model_basename = f\"gptq_model-{quantize_config.bits}bit-{quantize_config.group_size}g\"\
          \n\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:51,\
          \ in BaseQuantizeConfig.from_pretrained(cls, save_dir)\n     49 @classmethod\n\
          \     50 def from_pretrained(cls, save_dir: str):\n---> 51     with open(join(save_dir,\
          \ \"quantize_config.json\"), \"r\", encoding=\"utf-8\") as f:\n     52 \
          \        return cls(**json.load(f))\n\nFileNotFoundError: [Errno 2] No such\
          \ file or directory: 'TheBloke/Llama-2-13B-chat-GPTQ/quantize_config.json'"
        updatedAt: '2023-07-24T09:33:48.876Z'
      numEdits: 0
      reactions: []
    id: 64be457c979949d2e2310a00
    type: comment
  author: Osamarafique998
  content: "This is the complete error i am getting\n\n\nFileNotFoundError       \
    \                  Traceback (most recent call last)\nCell In[10], line 11\n \
    \     7 use_triton = False\n      9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n---> 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \     12         model_basename=model_basename,\n     13         use_safetensors=True,\n\
    \     14         trust_remote_code=True,\n     15         device=\"cuda:0\",\n\
    \     16         use_triton=use_triton,\n     17         quantize_config=None)\n\
    \     19 \"\"\"\n     20 To download from a specific branch, use the revision\
    \ parameter, as in this example:\n     21 \n   (...)\n     28         quantize_config=None)\n\
    \     29 \"\"\"\n     31 prompt = \"Tell me about AI\"\n\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:63,\
    \ in AutoGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,\
    \ use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
    \     49 @classmethod\n     50 def from_quantized(\n     51     cls,\n   (...)\n\
    \     60     trust_remote_code: bool = False\n     61 ) -> BaseGPTQForCausalLM:\n\
    \     62     model_type = check_and_get_model_type(save_dir)\n---> 63     return\
    \ GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\n     64         save_dir=save_dir,\n\
    \     65         device=device,\n     66         use_safetensors=use_safetensors,\n\
    \     67         use_triton=use_triton,\n     68         max_memory=max_memory,\n\
    \     69         device_map=device_map,\n     70         quantize_config=quantize_config,\n\
    \     71         model_basename=model_basename,\n     72         trust_remote_code=trust_remote_code\n\
    \     73     )\n\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:501,\
    \ in BaseGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,\
    \ use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
    \    498     raise TypeError(f\"{config.model_type} isn't supported yet.\")\n\
    \    500 if quantize_config is None:\n--> 501     quantize_config = BaseQuantizeConfig.from_pretrained(save_dir)\n\
    \    503 if model_basename is None:\n    504     model_basename = f\"gptq_model-{quantize_config.bits}bit-{quantize_config.group_size}g\"\
    \n\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:51,\
    \ in BaseQuantizeConfig.from_pretrained(cls, save_dir)\n     49 @classmethod\n\
    \     50 def from_pretrained(cls, save_dir: str):\n---> 51     with open(join(save_dir,\
    \ \"quantize_config.json\"), \"r\", encoding=\"utf-8\") as f:\n     52       \
    \  return cls(**json.load(f))\n\nFileNotFoundError: [Errno 2] No such file or\
    \ directory: 'TheBloke/Llama-2-13B-chat-GPTQ/quantize_config.json'"
  created_at: 2023-07-24 08:33:48+00:00
  edited: false
  hidden: false
  id: 64be457c979949d2e2310a00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T09:49:59.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8362224698066711
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I really don''t know what''s wrong. I ran the code you showed and
          it works fine, and as you can see there is definitely a <a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ/blob/main/quantize_config.json"><code>quantize_config.json</code>
          in this repo</a>.</p>

          <p>Here''s the output I get when I run the code you showed above:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/pXMyrqb4M6xOeAmYm4DHK.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/pXMyrqb4M6xOeAmYm4DHK.png"></a></p>

          <p>No error.</p>

          <p>It must be some kind of environment problem on your system.  Maybe it
          is failing to download the files correctly.  It''s not a problem with this
          model, or I think with AutoGPTQ.</p>

          <p>Try testing with a normal transformers model, like with the following
          code:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM


          model_name_or_path = <span class="hljs-string">"facebook/opt-125m"</span>


          tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=<span
          class="hljs-literal">True</span>)


          model = AutoModelForCausalLM.from_pretrained(model_name_or_path)


          <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Model
          config is: <span class="hljs-subst">{model.config}</span>"</span>)

          </code></pre>

          <p>You should see output similar to this:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/AudXOvRvErF0yUUEv9oqf.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/AudXOvRvErF0yUUEv9oqf.png"></a></p>

          '
        raw: 'I really don''t know what''s wrong. I ran the code you showed and it
          works fine, and as you can see there is definitely a [`quantize_config.json`
          in this repo](https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ/blob/main/quantize_config.json).


          Here''s the output I get when I run the code you showed above:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/pXMyrqb4M6xOeAmYm4DHK.png)


          No error.


          It must be some kind of environment problem on your system.  Maybe it is
          failing to download the files correctly.  It''s not a problem with this
          model, or I think with AutoGPTQ.


          Try testing with a normal transformers model, like with the following code:

          ```python

          from transformers import AutoTokenizer, AutoModelForCausalLM


          model_name_or_path = "facebook/opt-125m"


          tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)


          model = AutoModelForCausalLM.from_pretrained(model_name_or_path)


          print(f"Model config is: {model.config}")

          ```


          You should see output similar to this:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/AudXOvRvErF0yUUEv9oqf.png)



          '
        updatedAt: '2023-07-24T09:53:51.921Z'
      numEdits: 2
      reactions: []
    id: 64be4947b567ae97c35026fe
    type: comment
  author: TheBloke
  content: 'I really don''t know what''s wrong. I ran the code you showed and it works
    fine, and as you can see there is definitely a [`quantize_config.json` in this
    repo](https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ/blob/main/quantize_config.json).


    Here''s the output I get when I run the code you showed above:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/pXMyrqb4M6xOeAmYm4DHK.png)


    No error.


    It must be some kind of environment problem on your system.  Maybe it is failing
    to download the files correctly.  It''s not a problem with this model, or I think
    with AutoGPTQ.


    Try testing with a normal transformers model, like with the following code:

    ```python

    from transformers import AutoTokenizer, AutoModelForCausalLM


    model_name_or_path = "facebook/opt-125m"


    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)


    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)


    print(f"Model config is: {model.config}")

    ```


    You should see output similar to this:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/AudXOvRvErF0yUUEv9oqf.png)



    '
  created_at: 2023-07-24 08:49:59+00:00
  edited: true
  hidden: false
  id: 64be4947b567ae97c35026fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
      fullname: Mohammad Osama Rafique
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Osamarafique998
      type: user
    createdAt: '2023-07-24T10:23:52.000Z'
    data:
      edited: false
      editors:
      - Osamarafique998
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9459201693534851
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
          fullname: Mohammad Osama Rafique
          isHf: false
          isPro: false
          name: Osamarafique998
          type: user
        html: '<p>This "facebook/opt-125m" model loaded successfully without any issue.</p>

          <p>But i test for this model again it gives same error although you were
          right i can see the quantize_config.json in the repo so, maybe it occurs
          due to some environment issue.<br>I am running the LLAMA-2 model on Runpod
          with 48GB GPU.<br>What GPU configurations are you using to run that model?</p>

          '
        raw: "This \"facebook/opt-125m\" model loaded successfully without any issue.\n\
          \nBut i test for this model again it gives same error although you were\
          \ right i can see the quantize_config.json in the repo so, maybe it occurs\
          \ due to some environment issue. \nI am running the LLAMA-2 model on Runpod\
          \ with 48GB GPU. \nWhat GPU configurations are you using to run that model?"
        updatedAt: '2023-07-24T10:23:52.381Z'
      numEdits: 0
      reactions: []
    id: 64be5138f1a15daae6587a87
    type: comment
  author: Osamarafique998
  content: "This \"facebook/opt-125m\" model loaded successfully without any issue.\n\
    \nBut i test for this model again it gives same error although you were right\
    \ i can see the quantize_config.json in the repo so, maybe it occurs due to some\
    \ environment issue. \nI am running the LLAMA-2 model on Runpod with 48GB GPU.\
    \ \nWhat GPU configurations are you using to run that model?"
  created_at: 2023-07-24 09:23:52+00:00
  edited: false
  hidden: false
  id: 64be5138f1a15daae6587a87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
      fullname: Mohammad Osama Rafique
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Osamarafique998
      type: user
    createdAt: '2023-07-24T10:26:26.000Z'
    data:
      edited: false
      editors:
      - Osamarafique998
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9116189479827881
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
          fullname: Mohammad Osama Rafique
          isHf: false
          isPro: false
          name: Osamarafique998
          type: user
        html: '<p>Is there a way to clone this huggingface repo and then test the
          model. If there is one can you share the code snippet for loading the model
          from the cloned repo?</p>

          '
        raw: Is there a way to clone this huggingface repo and then test the model.
          If there is one can you share the code snippet for loading the model from
          the cloned repo?
        updatedAt: '2023-07-24T10:26:26.795Z'
      numEdits: 0
      reactions: []
    id: 64be51d236eb058cd915a7b9
    type: comment
  author: Osamarafique998
  content: Is there a way to clone this huggingface repo and then test the model.
    If there is one can you share the code snippet for loading the model from the
    cloned repo?
  created_at: 2023-07-24 09:26:26+00:00
  edited: false
  hidden: false
  id: 64be51d236eb058cd915a7b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T10:42:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4888639450073242
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes, that''s easy to do. Here''s example code. Make sure to set
          <code>local_folder</code> to the folder you want to download to.</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline,
          logging

          <span class="hljs-keyword">from</span> auto_gptq <span class="hljs-keyword">import</span>
          AutoGPTQForCausalLM, BaseQuantizeConfig

          <span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span>
          snapshot_download


          model_name = <span class="hljs-string">"TheBloke/Llama-2-13B-chat-GPTQ"</span>

          local_folder = <span class="hljs-string">"/workspace/test-llama-2"</span>


          snapshot_download(repo_id=model_name, local_dir=local_folder, local_dir_use_symlinks=<span
          class="hljs-literal">False</span>)


          model_basename = <span class="hljs-string">"gptq_model-4bit-128g"</span>


          use_triton = <span class="hljs-literal">False</span>


          tokenizer = AutoTokenizer.from_pretrained(local_folder, use_fast=<span class="hljs-literal">True</span>)


          model = AutoGPTQForCausalLM.from_quantized(local_folder,

          model_basename=model_basename,

          use_safetensors=<span class="hljs-literal">True</span>,

          trust_remote_code=<span class="hljs-literal">True</span>,

          device=<span class="hljs-string">"cuda:0"</span>,

          use_triton=use_triton,

          quantize_config=<span class="hljs-literal">None</span>)


          input_ids = tokenizer(<span class="hljs-string">"Llamas are"</span>, return_tensors=<span
          class="hljs-string">''pt''</span>).input_ids.cuda()

          output = model.generate(inputs=input_ids, temperature=<span class="hljs-number">0.7</span>,
          max_new_tokens=<span class="hljs-number">512</span>)

          <span class="hljs-built_in">print</span>(tokenizer.decode(output[<span class="hljs-number">0</span>]))

          </code></pre>

          <p>Output:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/7qmxxV6TZfA3MEyeovlsE.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/7qmxxV6TZfA3MEyeovlsE.png"></a></p>

          '
        raw: 'Yes, that''s easy to do. Here''s example code. Make sure to set `local_folder`
          to the folder you want to download to.


          ```python

          from transformers import AutoTokenizer, pipeline, logging

          from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

          from huggingface_hub import snapshot_download


          model_name = "TheBloke/Llama-2-13B-chat-GPTQ"

          local_folder = "/workspace/test-llama-2"


          snapshot_download(repo_id=model_name, local_dir=local_folder, local_dir_use_symlinks=False)


          model_basename = "gptq_model-4bit-128g"


          use_triton = False


          tokenizer = AutoTokenizer.from_pretrained(local_folder, use_fast=True)


          model = AutoGPTQForCausalLM.from_quantized(local_folder,

          model_basename=model_basename,

          use_safetensors=True,

          trust_remote_code=True,

          device="cuda:0",

          use_triton=use_triton,

          quantize_config=None)


          input_ids = tokenizer("Llamas are", return_tensors=''pt'').input_ids.cuda()

          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)

          print(tokenizer.decode(output[0]))

          ```


          Output:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/7qmxxV6TZfA3MEyeovlsE.png)


          '
        updatedAt: '2023-07-24T10:42:59.638Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MaulikMadhavi
    id: 64be55b35b8d826146f0bd1a
    type: comment
  author: TheBloke
  content: 'Yes, that''s easy to do. Here''s example code. Make sure to set `local_folder`
    to the folder you want to download to.


    ```python

    from transformers import AutoTokenizer, pipeline, logging

    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

    from huggingface_hub import snapshot_download


    model_name = "TheBloke/Llama-2-13B-chat-GPTQ"

    local_folder = "/workspace/test-llama-2"


    snapshot_download(repo_id=model_name, local_dir=local_folder, local_dir_use_symlinks=False)


    model_basename = "gptq_model-4bit-128g"


    use_triton = False


    tokenizer = AutoTokenizer.from_pretrained(local_folder, use_fast=True)


    model = AutoGPTQForCausalLM.from_quantized(local_folder,

    model_basename=model_basename,

    use_safetensors=True,

    trust_remote_code=True,

    device="cuda:0",

    use_triton=use_triton,

    quantize_config=None)


    input_ids = tokenizer("Llamas are", return_tensors=''pt'').input_ids.cuda()

    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)

    print(tokenizer.decode(output[0]))

    ```


    Output:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/7qmxxV6TZfA3MEyeovlsE.png)


    '
  created_at: 2023-07-24 09:42:59+00:00
  edited: false
  hidden: false
  id: 64be55b35b8d826146f0bd1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
      fullname: Mohammad Osama Rafique
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Osamarafique998
      type: user
    createdAt: '2023-07-24T12:54:39.000Z'
    data:
      edited: false
      editors:
      - Osamarafique998
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9874691963195801
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
          fullname: Mohammad Osama Rafique
          isHf: false
          isPro: false
          name: Osamarafique998
          type: user
        html: '<p>Thanks a lot. I will try that!</p>

          '
        raw: Thanks a lot. I will try that!
        updatedAt: '2023-07-24T12:54:39.235Z'
      numEdits: 0
      reactions: []
    id: 64be748fae436c8813353c34
    type: comment
  author: Osamarafique998
  content: Thanks a lot. I will try that!
  created_at: 2023-07-24 11:54:39+00:00
  edited: false
  hidden: false
  id: 64be748fae436c8813353c34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
      fullname: Mohammad Osama Rafique
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Osamarafique998
      type: user
    createdAt: '2023-07-24T13:11:53.000Z'
    data:
      edited: false
      editors:
      - Osamarafique998
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40585342049598694
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
          fullname: Mohammad Osama Rafique
          isHf: false
          isPro: false
          name: Osamarafique998
          type: user
        html: '<p>I use the exact code you provide but now it''s giving another error</p>

          <hr>

          <p>TypeError                                 Traceback (most recent call
          last)<br> in &lt;cell line: 16&gt;()<br>     14 tokenizer = AutoTokenizer.from_pretrained("/workspace/test-llama-2",
          use_fast=True)<br>     15<br>---&gt; 16 model = AutoGPTQForCausalLM.from_quantized("/workspace/test-llama-2",<br>     17
          model_basename=model_basename,<br>     18 use_safetensors=True,</p>

          <p>2 frames<br>/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py
          in from_pretrained(cls, save_dir)<br>     50     def from_pretrained(cls,
          save_dir: str):<br>     51         with open(join(save_dir, "quantize_config.json"),
          "r", encoding="utf-8") as f:<br>---&gt; 52             return cls(**json.load(f))<br>     53<br>     54     def
          to_dict(self):</p>

          <p>TypeError: BaseQuantizeConfig.<strong>init</strong>() got an unexpected
          keyword argument ''model_name_or_path''</p>

          '
        raw: "I use the exact code you provide but now it's giving another error\n\
          \n---------------------------------------------------------------------------\n\
          TypeError                                 Traceback (most recent call last)\n\
          <ipython-input-6-733b4037706a> in <cell line: 16>()\n     14 tokenizer =\
          \ AutoTokenizer.from_pretrained(\"/workspace/test-llama-2\", use_fast=True)\n\
          \     15 \n---> 16 model = AutoGPTQForCausalLM.from_quantized(\"/workspace/test-llama-2\"\
          ,\n     17 model_basename=model_basename,\n     18 use_safetensors=True,\n\
          \n2 frames\n/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\
          \ in from_pretrained(cls, save_dir)\n     50     def from_pretrained(cls,\
          \ save_dir: str):\n     51         with open(join(save_dir, \"quantize_config.json\"\
          ), \"r\", encoding=\"utf-8\") as f:\n---> 52             return cls(**json.load(f))\n\
          \     53 \n     54     def to_dict(self):\n\nTypeError: BaseQuantizeConfig.__init__()\
          \ got an unexpected keyword argument 'model_name_or_path'"
        updatedAt: '2023-07-24T13:11:53.414Z'
      numEdits: 0
      reactions: []
    id: 64be7899140491ca9f773cbb
    type: comment
  author: Osamarafique998
  content: "I use the exact code you provide but now it's giving another error\n\n\
    ---------------------------------------------------------------------------\n\
    TypeError                                 Traceback (most recent call last)\n\
    <ipython-input-6-733b4037706a> in <cell line: 16>()\n     14 tokenizer = AutoTokenizer.from_pretrained(\"\
    /workspace/test-llama-2\", use_fast=True)\n     15 \n---> 16 model = AutoGPTQForCausalLM.from_quantized(\"\
    /workspace/test-llama-2\",\n     17 model_basename=model_basename,\n     18 use_safetensors=True,\n\
    \n2 frames\n/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\
    \ in from_pretrained(cls, save_dir)\n     50     def from_pretrained(cls, save_dir:\
    \ str):\n     51         with open(join(save_dir, \"quantize_config.json\"), \"\
    r\", encoding=\"utf-8\") as f:\n---> 52             return cls(**json.load(f))\n\
    \     53 \n     54     def to_dict(self):\n\nTypeError: BaseQuantizeConfig.__init__()\
    \ got an unexpected keyword argument 'model_name_or_path'"
  created_at: 2023-07-24 12:11:53+00:00
  edited: false
  hidden: false
  id: 64be7899140491ca9f773cbb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T15:21:09.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7078585624694824
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ah, I think you must be using an old version of AutoGPTQ.</p>

          <p>Update to 0.2.2 or 0.3.0.  But there are some bugs in 0.3.0 so for now
          I recommend using 0.2.2 instead:</p>

          <pre><code>pip3 uninstall -y auto-gptq

          GITHUB_ACTIONS=true pip3 install auto-gptq==0.2.2

          </code></pre>

          '
        raw: 'Ah, I think you must be using an old version of AutoGPTQ.


          Update to 0.2.2 or 0.3.0.  But there are some bugs in 0.3.0 so for now I
          recommend using 0.2.2 instead:

          ```

          pip3 uninstall -y auto-gptq

          GITHUB_ACTIONS=true pip3 install auto-gptq==0.2.2

          ```'
        updatedAt: '2023-07-24T15:21:09.574Z'
      numEdits: 0
      reactions: []
    id: 64be96e52915a87970a4eaa7
    type: comment
  author: TheBloke
  content: 'Ah, I think you must be using an old version of AutoGPTQ.


    Update to 0.2.2 or 0.3.0.  But there are some bugs in 0.3.0 so for now I recommend
    using 0.2.2 instead:

    ```

    pip3 uninstall -y auto-gptq

    GITHUB_ACTIONS=true pip3 install auto-gptq==0.2.2

    ```'
  created_at: 2023-07-24 14:21:09+00:00
  edited: false
  hidden: false
  id: 64be96e52915a87970a4eaa7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
      fullname: Mohammad Osama Rafique
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Osamarafique998
      type: user
    createdAt: '2023-07-24T17:17:09.000Z'
    data:
      edited: false
      editors:
      - Osamarafique998
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8227624893188477
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
          fullname: Mohammad Osama Rafique
          isHf: false
          isPro: false
          name: Osamarafique998
          type: user
        html: '<p>Why it is showing couldn''t find a version that satisfies the requirement
          although it shows that it is a valid version?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63be57f7b3b8c44f8cef0dff/Qluqr0CLzZASJEk35rW_z.png"><img
          alt="error.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/63be57f7b3b8c44f8cef0dff/Qluqr0CLzZASJEk35rW_z.png"></a></p>

          '
        raw: 'Why it is showing couldn''t find a version that satisfies the requirement
          although it shows that it is a valid version?



          ![error.PNG](https://cdn-uploads.huggingface.co/production/uploads/63be57f7b3b8c44f8cef0dff/Qluqr0CLzZASJEk35rW_z.png)

          '
        updatedAt: '2023-07-24T17:17:09.257Z'
      numEdits: 0
      reactions: []
    id: 64beb2156999b520ed8aeb00
    type: comment
  author: Osamarafique998
  content: 'Why it is showing couldn''t find a version that satisfies the requirement
    although it shows that it is a valid version?



    ![error.PNG](https://cdn-uploads.huggingface.co/production/uploads/63be57f7b3b8c44f8cef0dff/Qluqr0CLzZASJEk35rW_z.png)

    '
  created_at: 2023-07-24 16:17:09+00:00
  edited: false
  hidden: false
  id: 64beb2156999b520ed8aeb00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T17:21:28.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2082851529121399
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Run </p>

          <p>CUDA_VERSION="" GITHUB_ACTIONS=true pip3 install auto-gptq==0.2.2</p>

          '
        raw: "Run \n\nCUDA_VERSION=\"\" GITHUB_ACTIONS=true pip3 install auto-gptq==0.2.2"
        updatedAt: '2023-07-24T17:21:28.186Z'
      numEdits: 0
      reactions: []
    id: 64beb31812afb2f11962ba2a
    type: comment
  author: TheBloke
  content: "Run \n\nCUDA_VERSION=\"\" GITHUB_ACTIONS=true pip3 install auto-gptq==0.2.2"
  created_at: 2023-07-24 16:21:28+00:00
  edited: false
  hidden: false
  id: 64beb31812afb2f11962ba2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
      fullname: Mohammad Osama Rafique
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Osamarafique998
      type: user
    createdAt: '2023-07-24T17:32:49.000Z'
    data:
      edited: false
      editors:
      - Osamarafique998
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9861830472946167
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673418725173-noauth.png?w=200&h=200&f=face
          fullname: Mohammad Osama Rafique
          isHf: false
          isPro: false
          name: Osamarafique998
          type: user
        html: '<p>Thank you so much for your assistance.<br>Now its working for both.</p>

          '
        raw: 'Thank you so much for your assistance.

          Now its working for both.'
        updatedAt: '2023-07-24T17:32:49.497Z'
      numEdits: 0
      reactions: []
    id: 64beb5c112afb2f119631741
    type: comment
  author: Osamarafique998
  content: 'Thank you so much for your assistance.

    Now its working for both.'
  created_at: 2023-07-24 16:32:49+00:00
  edited: false
  hidden: false
  id: 64beb5c112afb2f119631741
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ab8336cfbe073b42c4b3bcfc81fc20c.svg
      fullname: Richard Scott
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RichardScottOZ
      type: user
    createdAt: '2023-08-21T21:16:26.000Z'
    data:
      edited: false
      editors:
      - RichardScottOZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5760493874549866
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ab8336cfbe073b42c4b3bcfc81fc20c.svg
          fullname: Richard Scott
          isHf: false
          isPro: false
          name: RichardScottOZ
          type: user
        html: "<p>This was working fine for some time, last few days I get:-</p>\n\
          <pre><code class=\"language-python\">Traceback (most recent call last):\n\
          \  File <span class=\"hljs-string\">\"/home/ubuntu/k2-setup/pycode/asx_titles_llama.py\"\
          </span>, line <span class=\"hljs-number\">18</span>, <span class=\"hljs-keyword\"\
          >in</span> &lt;module&gt;\n    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ <span class=\"hljs-string\">\"/home/ubuntu/micromamba/envs/transormers/lib/python3.11/site-packages/auto_gptq/modeling/auto.py\"\
          </span>, line <span class=\"hljs-number\">105</span>, <span class=\"hljs-keyword\"\
          >in</span> from_quantized\n    <span class=\"hljs-keyword\">return</span>\
          \ quant_func(\n           ^^^^^^^^^^^\n  File <span class=\"hljs-string\"\
          >\"/home/ubuntu/micromamba/envs/transormers/lib/python3.11/site-packages/auto_gptq/modeling/_base.py\"\
          </span>, line <span class=\"hljs-number\">768</span>, <span class=\"hljs-keyword\"\
          >in</span> from_quantized\n    <span class=\"hljs-keyword\">raise</span>\
          \ FileNotFoundError(<span class=\"hljs-string\">f\"Could not find model\
          \ in <span class=\"hljs-subst\">{model_name_or_path}</span>\"</span>)\n\
          FileNotFoundError: Could <span class=\"hljs-keyword\">not</span> find model\
          \ <span class=\"hljs-keyword\">in</span> TheBloke/Llama-<span class=\"hljs-number\"\
          >2</span>-13B-chat-GPTQ\n</code></pre>\n<p>AutoGPTQ is 0.3.2</p>\n"
        raw: "This was working fine for some time, last few days I get:-\n```python\n\
          Traceback (most recent call last):\n  File \"/home/ubuntu/k2-setup/pycode/asx_titles_llama.py\"\
          , line 18, in <module>\n    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \"/home/ubuntu/micromamba/envs/transormers/lib/python3.11/site-packages/auto_gptq/modeling/auto.py\"\
          , line 105, in from_quantized\n    return quant_func(\n           ^^^^^^^^^^^\n\
          \  File \"/home/ubuntu/micromamba/envs/transormers/lib/python3.11/site-packages/auto_gptq/modeling/_base.py\"\
          , line 768, in from_quantized\n    raise FileNotFoundError(f\"Could not\
          \ find model in {model_name_or_path}\")\nFileNotFoundError: Could not find\
          \ model in TheBloke/Llama-2-13B-chat-GPTQ\n```\n\nAutoGPTQ is 0.3.2"
        updatedAt: '2023-08-21T21:16:26.917Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - shubham-baghel
        - shra1
    id: 64e3d42acf8c236f303c151f
    type: comment
  author: RichardScottOZ
  content: "This was working fine for some time, last few days I get:-\n```python\n\
    Traceback (most recent call last):\n  File \"/home/ubuntu/k2-setup/pycode/asx_titles_llama.py\"\
    , line 18, in <module>\n    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
    /home/ubuntu/micromamba/envs/transormers/lib/python3.11/site-packages/auto_gptq/modeling/auto.py\"\
    , line 105, in from_quantized\n    return quant_func(\n           ^^^^^^^^^^^\n\
    \  File \"/home/ubuntu/micromamba/envs/transormers/lib/python3.11/site-packages/auto_gptq/modeling/_base.py\"\
    , line 768, in from_quantized\n    raise FileNotFoundError(f\"Could not find model\
    \ in {model_name_or_path}\")\nFileNotFoundError: Could not find model in TheBloke/Llama-2-13B-chat-GPTQ\n\
    ```\n\nAutoGPTQ is 0.3.2"
  created_at: 2023-08-21 20:16:26+00:00
  edited: false
  hidden: false
  id: 64e3d42acf8c236f303c151f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ab8336cfbe073b42c4b3bcfc81fc20c.svg
      fullname: Richard Scott
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RichardScottOZ
      type: user
    createdAt: '2023-08-21T21:17:45.000Z'
    data:
      edited: false
      editors:
      - RichardScottOZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5117492079734802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ab8336cfbe073b42c4b3bcfc81fc20c.svg
          fullname: Richard Scott
          isHf: false
          isPro: false
          name: RichardScottOZ
          type: user
        html: "<p>Either this or your test snapshot test above gives the same error,\
          \ interestingly</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ AutoTokenizer, pipeline, logging\n<span class=\"hljs-keyword\">from</span>\
          \ auto_gptq <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\n<span class=\"hljs-keyword\">import</span> torch\n\
          <span class=\"hljs-keyword\">import</span> time\n<span class=\"hljs-keyword\"\
          >import</span> os\n<span class=\"hljs-keyword\">import</span> json\n<span\
          \ class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\"\
          >as</span> pd\n<span class=\"hljs-keyword\">from</span> datetime <span class=\"\
          hljs-keyword\">import</span> datetime, timedelta\n<span class=\"hljs-keyword\"\
          >from</span> get_announcements <span class=\"hljs-keyword\">import</span>\
          \ get_datestamp, get_datestamp_for_sorting\n\nmodel_name_or_path = <span\
          \ class=\"hljs-string\">\"TheBloke/Llama-2-13B-chat-GPTQ\"</span>\nmodel_basename\
          \ = <span class=\"hljs-string\">\"gptq_model-4bit-128g\"</span>\n\nuse_triton\
          \ = <span class=\"hljs-literal\">False</span>\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=<span class=\"hljs-literal\">True</span>)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=<span class=\"\
          hljs-literal\">True</span>,\n        trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,\n        <span class=\"hljs-comment\">#device=\"cuda:0\",</span>\n\
          \        device_map=<span class=\"hljs-number\">0</span>,\n        <span\
          \ class=\"hljs-comment\">#device=\"cpu\",</span>\n        use_triton=use_triton,\n\
          \        quantize_config=<span class=\"hljs-literal\">None</span>)\n</code></pre>\n"
        raw: "Either this or your test snapshot test above gives the same error, interestingly\n\
          \n```python\nfrom transformers import AutoTokenizer, pipeline, logging\n\
          from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport torch\n\
          import time\nimport os\nimport json\nimport pandas as pd\nfrom datetime\
          \ import datetime, timedelta\nfrom get_announcements import get_datestamp,\
          \ get_datestamp_for_sorting\n\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\
          \nmodel_basename = \"gptq_model-4bit-128g\"\n\nuse_triton = False\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\n\
          model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n       \
          \ model_basename=model_basename,\n        use_safetensors=True,\n      \
          \  trust_remote_code=True,\n        #device=\"cuda:0\",\n        device_map=0,\n\
          \        #device=\"cpu\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          ```"
        updatedAt: '2023-08-21T21:17:45.172Z'
      numEdits: 0
      reactions: []
    id: 64e3d47942d8e2c1c6b01e9c
    type: comment
  author: RichardScottOZ
  content: "Either this or your test snapshot test above gives the same error, interestingly\n\
    \n```python\nfrom transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
    \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport torch\nimport time\n\
    import os\nimport json\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\
    from get_announcements import get_datestamp, get_datestamp_for_sorting\n\nmodel_name_or_path\
    \ = \"TheBloke/Llama-2-13B-chat-GPTQ\"\nmodel_basename = \"gptq_model-4bit-128g\"\
    \n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        model_basename=model_basename,\n        use_safetensors=True,\n     \
    \   trust_remote_code=True,\n        #device=\"cuda:0\",\n        device_map=0,\n\
    \        #device=\"cpu\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
    ```"
  created_at: 2023-08-21 20:17:45+00:00
  edited: false
  hidden: false
  id: 64e3d47942d8e2c1c6b01e9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5eea8420b442e39b7ff403ef864550aa.svg
      fullname: Iain Attwater
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IainRatherThanIan
      type: user
    createdAt: '2023-08-22T14:28:42.000Z'
    data:
      edited: false
      editors:
      - IainRatherThanIan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9911144375801086
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5eea8420b442e39b7ff403ef864550aa.svg
          fullname: Iain Attwater
          isHf: false
          isPro: false
          name: IainRatherThanIan
          type: user
        html: '<p>I''m having this problem with multiple models that were working
          last week. Even a local copy does the same and I''ve tried a new env. Anyone
          else?</p>

          '
        raw: 'I''m having this problem with multiple models that were working last
          week. Even a local copy does the same and I''ve tried a new env. Anyone
          else?

          '
        updatedAt: '2023-08-22T14:28:42.874Z'
      numEdits: 0
      reactions: []
    id: 64e4c61a1887a952de9bdbb9
    type: comment
  author: IainRatherThanIan
  content: 'I''m having this problem with multiple models that were working last week.
    Even a local copy does the same and I''ve tried a new env. Anyone else?

    '
  created_at: 2023-08-22 13:28:42+00:00
  edited: false
  hidden: false
  id: 64e4c61a1887a952de9bdbb9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-22T14:29:59.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8474256992340088
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I recently updated all my GPTQ models for Transformers compatibility
          (coming very soon).  All GPTQ models have been renamed to <code>model.safetensors</code>.</p>

          <p>Please check the README again and you''ll see that the <code>model_basename</code>
          line is now: <code>model_basename = "model"</code>. </p>

          <p>This applies for all branches in all GPTQ models.</p>

          <p>Or in fact you can simply leave out <code>model_basename</code> now:</p>

          <pre><code>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,

          use_safetensors=True,

          trust_remote_code=True,

          device="cuda:0",

          use_triton=use_triton,

          quantize_config=None)

          </code></pre>

          <p>Because the model_basename is now also configured in <code>quantize_config.json</code>.</p>

          <p>In the next 24 - 48 hours I will be updating all my GPTQ READMEs to explain
          this in more detail, and provide example code for loading GPTQ models directly
          from Transformers.  I am waiting for the new Transformers and Optimum releases
          to happen. Transformers just released an hour ago, and Optimum will be releasing
          later today or early tomorrow.</p>

          '
        raw: "I recently updated all my GPTQ models for Transformers compatibility\
          \ (coming very soon).  All GPTQ models have been renamed to `model.safetensors`.\n\
          \nPlease check the README again and you'll see that the `model_basename`\
          \ line is now: `model_basename = \"model\"`. \n\nThis applies for all branches\
          \ in all GPTQ models.\n\nOr in fact you can simply leave out `model_basename`\
          \ now:\n```\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          use_safetensors=True,\ntrust_remote_code=True,\ndevice=\"cuda:0\",\nuse_triton=use_triton,\n\
          quantize_config=None)\n```\n\nBecause the model_basename is now also configured\
          \ in `quantize_config.json`.\n\nIn the next 24 - 48 hours I will be updating\
          \ all my GPTQ READMEs to explain this in more detail, and provide example\
          \ code for loading GPTQ models directly from Transformers.  I am waiting\
          \ for the new Transformers and Optimum releases to happen. Transformers\
          \ just released an hour ago, and Optimum will be releasing later today or\
          \ early tomorrow."
        updatedAt: '2023-08-22T14:34:13.021Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - yvillamil
    id: 64e4c667d7424239dfd3be83
    type: comment
  author: TheBloke
  content: "I recently updated all my GPTQ models for Transformers compatibility (coming\
    \ very soon).  All GPTQ models have been renamed to `model.safetensors`.\n\nPlease\
    \ check the README again and you'll see that the `model_basename` line is now:\
    \ `model_basename = \"model\"`. \n\nThis applies for all branches in all GPTQ\
    \ models.\n\nOr in fact you can simply leave out `model_basename` now:\n```\n\
    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\nuse_safetensors=True,\n\
    trust_remote_code=True,\ndevice=\"cuda:0\",\nuse_triton=use_triton,\nquantize_config=None)\n\
    ```\n\nBecause the model_basename is now also configured in `quantize_config.json`.\n\
    \nIn the next 24 - 48 hours I will be updating all my GPTQ READMEs to explain\
    \ this in more detail, and provide example code for loading GPTQ models directly\
    \ from Transformers.  I am waiting for the new Transformers and Optimum releases\
    \ to happen. Transformers just released an hour ago, and Optimum will be releasing\
    \ later today or early tomorrow."
  created_at: 2023-08-22 13:29:59+00:00
  edited: true
  hidden: false
  id: 64e4c667d7424239dfd3be83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5eea8420b442e39b7ff403ef864550aa.svg
      fullname: Iain Attwater
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IainRatherThanIan
      type: user
    createdAt: '2023-08-22T14:34:48.000Z'
    data:
      edited: false
      editors:
      - IainRatherThanIan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9402496814727783
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5eea8420b442e39b7ff403ef864550aa.svg
          fullname: Iain Attwater
          isHf: false
          isPro: false
          name: IainRatherThanIan
          type: user
        html: '<p>Thank you for the quick response and all the great work you do.</p>

          '
        raw: Thank you for the quick response and all the great work you do.
        updatedAt: '2023-08-22T14:34:48.499Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RichardScottOZ
    id: 64e4c788a07739bcc489cafa
    type: comment
  author: IainRatherThanIan
  content: Thank you for the quick response and all the great work you do.
  created_at: 2023-08-22 13:34:48+00:00
  edited: false
  hidden: false
  id: 64e4c788a07739bcc489cafa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43d9c36c360f54be359db2cd222f9d12.svg
      fullname: CLARK DJILO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bourbe
      type: user
    createdAt: '2023-08-23T11:54:12.000Z'
    data:
      edited: false
      editors:
      - bourbe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8562870621681213
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43d9c36c360f54be359db2cd222f9d12.svg
          fullname: CLARK DJILO
          isHf: false
          isPro: false
          name: bourbe
          type: user
        html: "<p>Hello,</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ : Do you have updta about the new parametrization please ?</p>\n<p>This\
          \ is my current code where the model doen't upload. What does I need to\
          \ change please ?</p>\n<p>!unset CUDA_VERSION &amp;&amp; pip3 install auto-gptq==0.2.2<br>!pip3\
          \ install transformers</p>\n<p>#######################################################################################################</p>\n\
          <p>from transformers import AutoTokenizer, pipeline, logging<br>from auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig</p>\n<p>model_name_or_path\
          \ = \"TheBloke/Llama-2-13B-chat-GPTQ\"<br>model_basename = \"gptq_model-4bit-128g\"\
          </p>\n<p>use_triton = False</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)</p>\n<p>model = AutoGPTQForCausalLM.from_quantized(<br>\
          \        model_name_or_path,<br>        model_basename=model_basename,<br>\
          \        use_safetensors=True,<br>        trust_remote_code=True,<br>  \
          \      device=\"cuda:0\",<br>        use_triton=use_triton,<br>        quantize_config=None)</p>\n\
          <p>#######################################################################################################</p>\n\
          <p>article = \"\"\"Helping Your Teen Lose Weight</p>\n<p>Following healthy\
          \ habits are the essential key to teen weight loss. Without such a habit,\
          \ teenagers may find it difficult to maintain their healthy weight.<br>The\
          \ way today's food consumption is being looked at in this country, healthy\
          \ eating has surely been put at the wayside. Teenagers of today really have\
          \ a myriad of delicious food choices to eat. But sad to say, most of them\
          \ belong to the unhealthy food group. It is now easier for teenagers today\
          \ to get fat because of the convenience brought about by fast food. </p>\n\
          <p>Parents today live in a very busy world where time is spent more on work.\
          \ Such parents may not have the time to prepare food and sometimes must\
          \ rely on the nearest pizza or hamburger place to provide the nourishment\
          \ for their children. But this should not be. Fastfood is considered to\
          \ be one of the reasons why most teenagers are getting fatter. Fastfood\
          \ is considered junk food since they are not able to supply all the nourishment\
          \ that growing kids need. But fast food can really be fattening with the\
          \ great amounts of fat and carbs that they contain. It is a bad choice if\
          \ you wish to help your child stay at a healthy weight as he or she grows\
          \ up.</p>\n<p>Your concern to see to it that your teenagers grow up to be\
          \ healthy and fit individuals is the first step in keeping their weight\
          \ down. Always bear in mind that teenage obesity is a dangerous and a growing\
          \ problem in this country. But you can do something about it. You can make\
          \ effective use of your concern about your teenager's weight by putting\
          \ it into action. You can help show your teen the way by following a practical\
          \ plan for success. There's no easy way for teen weight loss. The most important\
          \ thing that you can do is letting your teenager adopt healthy habits that\
          \ can last a lifetime. Here are some tips:</p>\n<ol>\n<li><p>Start with\
          \ a heart-to-heart talk.<br>If your see that your teen is getting overweight,\
          \ chances are, he or she is also concerned about the excess weight. Aside\
          \ from bringing in lifelong health risks such as high blood pressure and\
          \ diabetes, the social and emotional consequences of being overweight can\
          \ have a devastating effect on your teenager. Talk to your teenager about\
          \ it. Try to offer support and gentle understanding and make him or her\
          \ verbally aware that you really are concerned. Try also to add in a willingness\
          \ to help your teen take control of the weight problem that he or she is\
          \ facing.</p>\n</li>\n<li><p>As much as possible, resist looking for quick\
          \ fixes.<br>Make your teen realize that losing and maintaining an ideal\
          \ weight is a lifetime commitment. Encouraging fad diets may rob your growing\
          \ teen essential nutrients essential to his or her continuing development.\
          \ Buying weight-loss pills for your teenager and other quick fixes won't\
          \ be able to address the root of the weight problem. The effects of such\
          \ quick fixes are often short-lived and you teen may likely balloon back.\
          \ What you should be able to teach is adopting a lifelong healthy habit.\
          \ Without a permanent change in unhealthy habits, any weight loss program\
          \ will only remain a temporary fix.</p>\n</li>\n<li><p>Promote and encourage\
          \ doing more calorie-burning activities.<br>Just like adults, teens also\
          \ require about an hour of physical activity everyday. But that doesn't\
          \ mean sixty solid minutes of pure gut-wrenching activity. You can plan\
          \ shorter, repeated bursts of activity throughout the day that not only\
          \ can help burn calories, but also become an enjoyable, fun and worthwhile\
          \ affair. Sports and hiking can be probable options.<br>\"\"\"</p>\n</li>\n\
          </ol>\n<p>#######################################################################################################</p>\n\
          <p>system_message = system_message = \"You are an assistant dedicated to\
          \ providing valuable, respectful, and honest support. Your task is to assist\
          \ with the creation and conversion of content, such as converting text into\
          \ a Markdown blog article format.\"</p>\n<p>#prompt = \"\"\"Convert this\
          \ to a DETAILLED MARKDOWN blog article OF 2500 WORDS WITH AT LEAST WITH\
          \ MANY SUBTITLES. SUBTITLES WITH TAGS ##, ###, #### IN THE ARTICLE ARE MANDATORY.\
          \ CREATE A TITLE WITH A # TAG AT THE BEGINNING FOR THE ARTICLE. MARKDOWN\
          \ FORMATING IS MANDATORY:{article}\"\"\".format(article=article)<br>prompt\
          \ = \"\"\"Convert this to a DETAILLED MARKDOWN blog article OF 2500 WORDS\
          \ WITH AT LEAST WITH MANY SECTIONS. SECTIONS WITH TAGS ##, ###, #### IN\
          \ THE ARTICLE ARE MANDATORY. CREATE A TITLE WITH A # TAG AT THE BEGINNING\
          \ FOR THE ARTICLE. MARKDOWN FORMATING IS MANDATORY:{article}\"\"\".format(article=article)</p>\n\
          <p>prompt_template=f'''[INST] &lt;&gt;<br>{system_message}<br>&lt;&gt;</p>\n\
          <p>{prompt} [/INST]'''</p>\n<p>#######################################################################################################</p>\n\
          <h1 id=\"prevent-printing-spurious-transformers-error-when-using-pipeline-with-autogptq\"\
          >Prevent printing spurious transformers error when using pipeline with AutoGPTQ</h1>\n\
          <h1 id=\"loggingset_verbosityloggingcritical\">logging.set_verbosity(logging.CRITICAL)</h1>\n\
          <h1 id=\"print-pipeline\">print(\"*** Pipeline:\")</h1>\n<p>pipe = pipeline(<br>\
          \    \"text-generation\",<br>    model=model,<br>    tokenizer=tokenizer,<br>\
          \    max_new_tokens=3000,<br>    temperature=0.7,<br>    do_sample=True,<br>\
          \    top_p=0.95,<br>    repetition_penalty=1.15<br>)</p>\n<p>final_md =\
          \ pipe(prompt_template)[0]['generated_text'].split('[/INST]')[-1].lstrip()<br>print(final_md)</p>\n\
          <p>title = \"final_md_file\"<br>invalid_chars = r'/:*?&lt;&gt;|\"'<br>translation_table\
          \ = title.maketrans('', '', invalid_chars)<br>title = title.translate(translation_table)</p>\n\
          <p>file_name = f\"{title}.md\"<br>with open(file_name, \"w\", encoding=\"\
          utf-8\") as file:<br>    file.write(final_md)</p>\n"
        raw: "Hello,\n\n@TheBloke : Do you have updta about the new parametrization\
          \ please ?\n\nThis is my current code where the model doen't upload. What\
          \ does I need to change please ?\n\n!unset CUDA_VERSION && pip3 install\
          \ auto-gptq==0.2.2\n!pip3 install transformers\n\n#######################################################################################################\n\
          \nfrom transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path =\
          \ \"TheBloke/Llama-2-13B-chat-GPTQ\"\nmodel_basename = \"gptq_model-4bit-128g\"\
          \n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(\n      \
          \  model_name_or_path,\n        model_basename=model_basename,\n       \
          \ use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
          cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          \n#######################################################################################################\n\
          \narticle = \"\"\"Helping Your Teen Lose Weight\n\nFollowing healthy habits\
          \ are the essential key to teen weight loss. Without such a habit, teenagers\
          \ may find it difficult to maintain their healthy weight. \nThe way today's\
          \ food consumption is being looked at in this country, healthy eating has\
          \ surely been put at the wayside. Teenagers of today really have a myriad\
          \ of delicious food choices to eat. But sad to say, most of them belong\
          \ to the unhealthy food group. It is now easier for teenagers today to get\
          \ fat because of the convenience brought about by fast food. \n\nParents\
          \ today live in a very busy world where time is spent more on work. Such\
          \ parents may not have the time to prepare food and sometimes must rely\
          \ on the nearest pizza or hamburger place to provide the nourishment for\
          \ their children. But this should not be. Fastfood is considered to be one\
          \ of the reasons why most teenagers are getting fatter. Fastfood is considered\
          \ junk food since they are not able to supply all the nourishment that growing\
          \ kids need. But fast food can really be fattening with the great amounts\
          \ of fat and carbs that they contain. It is a bad choice if you wish to\
          \ help your child stay at a healthy weight as he or she grows up.\n\nYour\
          \ concern to see to it that your teenagers grow up to be healthy and fit\
          \ individuals is the first step in keeping their weight down. Always bear\
          \ in mind that teenage obesity is a dangerous and a growing problem in this\
          \ country. But you can do something about it. You can make effective use\
          \ of your concern about your teenager's weight by putting it into action.\
          \ You can help show your teen the way by following a practical plan for\
          \ success. There's no easy way for teen weight loss. The most important\
          \ thing that you can do is letting your teenager adopt healthy habits that\
          \ can last a lifetime. Here are some tips:\n\n1. Start with a heart-to-heart\
          \ talk.\nIf your see that your teen is getting overweight, chances are,\
          \ he or she is also concerned about the excess weight. Aside from bringing\
          \ in lifelong health risks such as high blood pressure and diabetes, the\
          \ social and emotional consequences of being overweight can have a devastating\
          \ effect on your teenager. Talk to your teenager about it. Try to offer\
          \ support and gentle understanding and make him or her verbally aware that\
          \ you really are concerned. Try also to add in a willingness to help your\
          \ teen take control of the weight problem that he or she is facing.\n\n\
          2. As much as possible, resist looking for quick fixes.\nMake your teen\
          \ realize that losing and maintaining an ideal weight is a lifetime commitment.\
          \ Encouraging fad diets may rob your growing teen essential nutrients essential\
          \ to his or her continuing development. Buying weight-loss pills for your\
          \ teenager and other quick fixes won't be able to address the root of the\
          \ weight problem. The effects of such quick fixes are often short-lived\
          \ and you teen may likely balloon back. What you should be able to teach\
          \ is adopting a lifelong healthy habit. Without a permanent change in unhealthy\
          \ habits, any weight loss program will only remain a temporary fix.\n\n\
          3. Promote and encourage doing more calorie-burning activities.\nJust like\
          \ adults, teens also require about an hour of physical activity everyday.\
          \ But that doesn't mean sixty solid minutes of pure gut-wrenching activity.\
          \ You can plan shorter, repeated bursts of activity throughout the day that\
          \ not only can help burn calories, but also become an enjoyable, fun and\
          \ worthwhile affair. Sports and hiking can be probable options.\n\"\"\"\n\
          \n#######################################################################################################\n\
          \nsystem_message = system_message = \"You are an assistant dedicated to\
          \ providing valuable, respectful, and honest support. Your task is to assist\
          \ with the creation and conversion of content, such as converting text into\
          \ a Markdown blog article format.\"\n\n#prompt = \"\"\"Convert this to a\
          \ DETAILLED MARKDOWN blog article OF 2500 WORDS WITH AT LEAST WITH MANY\
          \ SUBTITLES. SUBTITLES WITH TAGS ##, ###, #### IN THE ARTICLE ARE MANDATORY.\
          \ CREATE A TITLE WITH A # TAG AT THE BEGINNING FOR THE ARTICLE. MARKDOWN\
          \ FORMATING IS MANDATORY:{article}\"\"\".format(article=article)\nprompt\
          \ = \"\"\"Convert this to a DETAILLED MARKDOWN blog article OF 2500 WORDS\
          \ WITH AT LEAST WITH MANY SECTIONS. SECTIONS WITH TAGS ##, ###, #### IN\
          \ THE ARTICLE ARE MANDATORY. CREATE A TITLE WITH A # TAG AT THE BEGINNING\
          \ FOR THE ARTICLE. MARKDOWN FORMATING IS MANDATORY:{article}\"\"\".format(article=article)\n\
          \nprompt_template=f'''[INST] <<SYS>>\n{system_message}\n<</SYS>>\n\n{prompt}\
          \ [/INST]'''\n\n#######################################################################################################\n\
          \n# Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\n# logging.set_verbosity(logging.CRITICAL)\n\n# print(\"*** Pipeline:\"\
          )\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=3000,\n    temperature=0.7,\n    do_sample=True,\n \
          \   top_p=0.95,\n    repetition_penalty=1.15\n)\n\nfinal_md = pipe(prompt_template)[0]['generated_text'].split('[/INST]')[-1].lstrip()\n\
          print(final_md)\n\n\ntitle = \"final_md_file\"           \ninvalid_chars\
          \ = r'\\/:*?<>|\"'\ntranslation_table = title.maketrans('', '', invalid_chars)\n\
          title = title.translate(translation_table)\n\nfile_name = f\"{title}.md\"\
          \nwith open(file_name, \"w\", encoding=\"utf-8\") as file:\n    file.write(final_md)\n"
        updatedAt: '2023-08-23T11:54:12.488Z'
      numEdits: 0
      reactions: []
    id: 64e5f364423a205f8e261ad9
    type: comment
  author: bourbe
  content: "Hello,\n\n@TheBloke : Do you have updta about the new parametrization\
    \ please ?\n\nThis is my current code where the model doen't upload. What does\
    \ I need to change please ?\n\n!unset CUDA_VERSION && pip3 install auto-gptq==0.2.2\n\
    !pip3 install transformers\n\n#######################################################################################################\n\
    \nfrom transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq import\
    \ AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\
    \nmodel_basename = \"gptq_model-4bit-128g\"\n\nuse_triton = False\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nmodel\
    \ = AutoGPTQForCausalLM.from_quantized(\n        model_name_or_path,\n       \
    \ model_basename=model_basename,\n        use_safetensors=True,\n        trust_remote_code=True,\n\
    \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
    \n#######################################################################################################\n\
    \narticle = \"\"\"Helping Your Teen Lose Weight\n\nFollowing healthy habits are\
    \ the essential key to teen weight loss. Without such a habit, teenagers may find\
    \ it difficult to maintain their healthy weight. \nThe way today's food consumption\
    \ is being looked at in this country, healthy eating has surely been put at the\
    \ wayside. Teenagers of today really have a myriad of delicious food choices to\
    \ eat. But sad to say, most of them belong to the unhealthy food group. It is\
    \ now easier for teenagers today to get fat because of the convenience brought\
    \ about by fast food. \n\nParents today live in a very busy world where time is\
    \ spent more on work. Such parents may not have the time to prepare food and sometimes\
    \ must rely on the nearest pizza or hamburger place to provide the nourishment\
    \ for their children. But this should not be. Fastfood is considered to be one\
    \ of the reasons why most teenagers are getting fatter. Fastfood is considered\
    \ junk food since they are not able to supply all the nourishment that growing\
    \ kids need. But fast food can really be fattening with the great amounts of fat\
    \ and carbs that they contain. It is a bad choice if you wish to help your child\
    \ stay at a healthy weight as he or she grows up.\n\nYour concern to see to it\
    \ that your teenagers grow up to be healthy and fit individuals is the first step\
    \ in keeping their weight down. Always bear in mind that teenage obesity is a\
    \ dangerous and a growing problem in this country. But you can do something about\
    \ it. You can make effective use of your concern about your teenager's weight\
    \ by putting it into action. You can help show your teen the way by following\
    \ a practical plan for success. There's no easy way for teen weight loss. The\
    \ most important thing that you can do is letting your teenager adopt healthy\
    \ habits that can last a lifetime. Here are some tips:\n\n1. Start with a heart-to-heart\
    \ talk.\nIf your see that your teen is getting overweight, chances are, he or\
    \ she is also concerned about the excess weight. Aside from bringing in lifelong\
    \ health risks such as high blood pressure and diabetes, the social and emotional\
    \ consequences of being overweight can have a devastating effect on your teenager.\
    \ Talk to your teenager about it. Try to offer support and gentle understanding\
    \ and make him or her verbally aware that you really are concerned. Try also to\
    \ add in a willingness to help your teen take control of the weight problem that\
    \ he or she is facing.\n\n2. As much as possible, resist looking for quick fixes.\n\
    Make your teen realize that losing and maintaining an ideal weight is a lifetime\
    \ commitment. Encouraging fad diets may rob your growing teen essential nutrients\
    \ essential to his or her continuing development. Buying weight-loss pills for\
    \ your teenager and other quick fixes won't be able to address the root of the\
    \ weight problem. The effects of such quick fixes are often short-lived and you\
    \ teen may likely balloon back. What you should be able to teach is adopting a\
    \ lifelong healthy habit. Without a permanent change in unhealthy habits, any\
    \ weight loss program will only remain a temporary fix.\n\n3. Promote and encourage\
    \ doing more calorie-burning activities.\nJust like adults, teens also require\
    \ about an hour of physical activity everyday. But that doesn't mean sixty solid\
    \ minutes of pure gut-wrenching activity. You can plan shorter, repeated bursts\
    \ of activity throughout the day that not only can help burn calories, but also\
    \ become an enjoyable, fun and worthwhile affair. Sports and hiking can be probable\
    \ options.\n\"\"\"\n\n#######################################################################################################\n\
    \nsystem_message = system_message = \"You are an assistant dedicated to providing\
    \ valuable, respectful, and honest support. Your task is to assist with the creation\
    \ and conversion of content, such as converting text into a Markdown blog article\
    \ format.\"\n\n#prompt = \"\"\"Convert this to a DETAILLED MARKDOWN blog article\
    \ OF 2500 WORDS WITH AT LEAST WITH MANY SUBTITLES. SUBTITLES WITH TAGS ##, ###,\
    \ #### IN THE ARTICLE ARE MANDATORY. CREATE A TITLE WITH A # TAG AT THE BEGINNING\
    \ FOR THE ARTICLE. MARKDOWN FORMATING IS MANDATORY:{article}\"\"\".format(article=article)\n\
    prompt = \"\"\"Convert this to a DETAILLED MARKDOWN blog article OF 2500 WORDS\
    \ WITH AT LEAST WITH MANY SECTIONS. SECTIONS WITH TAGS ##, ###, #### IN THE ARTICLE\
    \ ARE MANDATORY. CREATE A TITLE WITH A # TAG AT THE BEGINNING FOR THE ARTICLE.\
    \ MARKDOWN FORMATING IS MANDATORY:{article}\"\"\".format(article=article)\n\n\
    prompt_template=f'''[INST] <<SYS>>\n{system_message}\n<</SYS>>\n\n{prompt} [/INST]'''\n\
    \n#######################################################################################################\n\
    \n# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n\
    # logging.set_verbosity(logging.CRITICAL)\n\n# print(\"*** Pipeline:\")\npipe\
    \ = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
    \    max_new_tokens=3000,\n    temperature=0.7,\n    do_sample=True,\n    top_p=0.95,\n\
    \    repetition_penalty=1.15\n)\n\nfinal_md = pipe(prompt_template)[0]['generated_text'].split('[/INST]')[-1].lstrip()\n\
    print(final_md)\n\n\ntitle = \"final_md_file\"           \ninvalid_chars = r'\\\
    /:*?<>|\"'\ntranslation_table = title.maketrans('', '', invalid_chars)\ntitle\
    \ = title.translate(translation_table)\n\nfile_name = f\"{title}.md\"\nwith open(file_name,\
    \ \"w\", encoding=\"utf-8\") as file:\n    file.write(final_md)\n"
  created_at: 2023-08-23 10:54:12+00:00
  edited: false
  hidden: false
  id: 64e5f364423a205f8e261ad9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-23T12:05:00.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8529594540596008
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I recently updated all my GPTQ models for Transformers compatibility
          (coming very soon).  All GPTQ models have been renamed to <code>model.safetensors</code>.</p>

          <p>Please check the README again and you''ll see that the <code>model_basename</code>
          line is now: <code>model_basename = "model"</code>. </p>

          <p>This applies for all branches in all GPTQ models.</p>

          <p>Or in fact you can simply leave out <code>model_basename</code> now:</p>

          <pre><code>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,

          use_safetensors=True,

          trust_remote_code=True,

          device="cuda:0",

          use_triton=use_triton,

          quantize_config=None)

          </code></pre>

          <p>Because the model_basename is now also configured in <code>quantize_config.json</code>.</p>

          <p>In the next 24 - 48 hours I will be updating all my GPTQ READMEs to explain
          this in more detail, and provide example code for loading GPTQ models directly
          from Transformers.  I am waiting for the new Transformers and Optimum releases
          to happen. Transformers just released yesterday, and Optimum will be releasing
          some time today.</p>

          '
        raw: "I recently updated all my GPTQ models for Transformers compatibility\
          \ (coming very soon).  All GPTQ models have been renamed to `model.safetensors`.\n\
          \nPlease check the README again and you'll see that the `model_basename`\
          \ line is now: `model_basename = \"model\"`. \n\nThis applies for all branches\
          \ in all GPTQ models.\n\nOr in fact you can simply leave out `model_basename`\
          \ now:\n```\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          use_safetensors=True,\ntrust_remote_code=True,\ndevice=\"cuda:0\",\nuse_triton=use_triton,\n\
          quantize_config=None)\n```\n\nBecause the model_basename is now also configured\
          \ in `quantize_config.json`.\n\nIn the next 24 - 48 hours I will be updating\
          \ all my GPTQ READMEs to explain this in more detail, and provide example\
          \ code for loading GPTQ models directly from Transformers.  I am waiting\
          \ for the new Transformers and Optimum releases to happen. Transformers\
          \ just released yesterday, and Optimum will be releasing some time today."
        updatedAt: '2023-08-23T12:05:39.869Z'
      numEdits: 1
      reactions: []
    id: 64e5f5ecf1e71327ada9ba72
    type: comment
  author: TheBloke
  content: "I recently updated all my GPTQ models for Transformers compatibility (coming\
    \ very soon).  All GPTQ models have been renamed to `model.safetensors`.\n\nPlease\
    \ check the README again and you'll see that the `model_basename` line is now:\
    \ `model_basename = \"model\"`. \n\nThis applies for all branches in all GPTQ\
    \ models.\n\nOr in fact you can simply leave out `model_basename` now:\n```\n\
    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\nuse_safetensors=True,\n\
    trust_remote_code=True,\ndevice=\"cuda:0\",\nuse_triton=use_triton,\nquantize_config=None)\n\
    ```\n\nBecause the model_basename is now also configured in `quantize_config.json`.\n\
    \nIn the next 24 - 48 hours I will be updating all my GPTQ READMEs to explain\
    \ this in more detail, and provide example code for loading GPTQ models directly\
    \ from Transformers.  I am waiting for the new Transformers and Optimum releases\
    \ to happen. Transformers just released yesterday, and Optimum will be releasing\
    \ some time today."
  created_at: 2023-08-23 11:05:00+00:00
  edited: true
  hidden: false
  id: 64e5f5ecf1e71327ada9ba72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43d9c36c360f54be359db2cd222f9d12.svg
      fullname: CLARK DJILO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bourbe
      type: user
    createdAt: '2023-08-24T01:54:44.000Z'
    data:
      edited: false
      editors:
      - bourbe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8173885345458984
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43d9c36c360f54be359db2cd222f9d12.svg
          fullname: CLARK DJILO
          isHf: false
          isPro: false
          name: bourbe
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<p>Thank you for your answer, the model upload well now</p>\n<p>I\
          \ have a cpu i7 machine, is there any way to make it works on this machine\
          \ please ? My goal was to be able to run freely this model and I use runpod\
          \ machine to run it and finally the final cost is higher than the same task\
          \ in chatgpt so if there is a way to run this model on my cpu machine I\
          \ will be very happy</p>\n<hr>\n<p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>use_safetensors=True,<br>trust_remote_code=True,<br>device=\"\
          cuda:0\",<br>use_triton=use_triton,<br>quantize_config=None)</p>\n"
        raw: 'Hello @TheBloke ,


          Thank you for your answer, the model upload well now


          I have a cpu i7 machine, is there any way to make it works on this machine
          please ? My goal was to be able to run freely this model and I use runpod
          machine to run it and finally the final cost is higher than the same task
          in chatgpt so if there is a way to run this model on my cpu machine I will
          be very happy


          -------------------------------------------------------------------------------------------------------------


          model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,

          use_safetensors=True,

          trust_remote_code=True,

          device="cuda:0",

          use_triton=use_triton,

          quantize_config=None)'
        updatedAt: '2023-08-24T01:54:44.953Z'
      numEdits: 0
      reactions: []
    id: 64e6b8640dfe6bbcb6a4b1ac
    type: comment
  author: bourbe
  content: 'Hello @TheBloke ,


    Thank you for your answer, the model upload well now


    I have a cpu i7 machine, is there any way to make it works on this machine please
    ? My goal was to be able to run freely this model and I use runpod machine to
    run it and finally the final cost is higher than the same task in chatgpt so if
    there is a way to run this model on my cpu machine I will be very happy


    -------------------------------------------------------------------------------------------------------------


    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,

    use_safetensors=True,

    trust_remote_code=True,

    device="cuda:0",

    use_triton=use_triton,

    quantize_config=None)'
  created_at: 2023-08-24 00:54:44+00:00
  edited: false
  hidden: false
  id: 64e6b8640dfe6bbcb6a4b1ac
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: 'File not found error while loading model '
