!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mr96
conflicting_files: null
created_at: 2023-07-19 14:32:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b09f1ec8981806d2a11787a69851687a.svg
      fullname: Ripamonti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mr96
      type: user
    createdAt: '2023-07-19T15:32:20.000Z'
    data:
      edited: false
      editors:
      - mr96
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8961899280548096
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b09f1ec8981806d2a11787a69851687a.svg
          fullname: Ripamonti
          isHf: false
          isPro: false
          name: mr96
          type: user
        html: '<p>Hello, is the prompt provided in the information correct?</p>

          <p>SYSTEM:<br>USER: {prompt}<br>ASSISTANT:</p>

          <p>I was looking at the Meta github page of LLama (<a rel="nofollow" href="https://github.com/facebookresearch/llama#fine-tuned-chat-models">https://github.com/facebookresearch/llama#fine-tuned-chat-models</a>),
          and it says the follwoing:</p>

          <p>The fine-tuned models were trained for dialogue applications. To get
          the expected features and performance for them, a specific formatting defined
          in chat_completion needs to be followed, including the INST and &lt;&gt;
          tags, BOS and EOS tokens, and the whitespaces and breaklines in between
          (we recommend calling strip() on inputs to avoid double-spaces).</p>

          <p>I''m just a bit confused</p>

          '
        raw: "Hello, is the prompt provided in the information correct?\r\n\r\nSYSTEM:\
          \ \r\nUSER: {prompt}\r\nASSISTANT:\r\n\r\nI was looking at the Meta github\
          \ page of LLama (https://github.com/facebookresearch/llama#fine-tuned-chat-models),\
          \ and it says the follwoing:\r\n\r\nThe fine-tuned models were trained for\
          \ dialogue applications. To get the expected features and performance for\
          \ them, a specific formatting defined in chat_completion needs to be followed,\
          \ including the INST and <<SYS>> tags, BOS and EOS tokens, and the whitespaces\
          \ and breaklines in between (we recommend calling strip() on inputs to avoid\
          \ double-spaces).\r\n\r\nI'm just a bit confused"
        updatedAt: '2023-07-19T15:32:20.038Z'
      numEdits: 0
      reactions: []
    id: 64b802049e7deb6a78288c5f
    type: comment
  author: mr96
  content: "Hello, is the prompt provided in the information correct?\r\n\r\nSYSTEM:\
    \ \r\nUSER: {prompt}\r\nASSISTANT:\r\n\r\nI was looking at the Meta github page\
    \ of LLama (https://github.com/facebookresearch/llama#fine-tuned-chat-models),\
    \ and it says the follwoing:\r\n\r\nThe fine-tuned models were trained for dialogue\
    \ applications. To get the expected features and performance for them, a specific\
    \ formatting defined in chat_completion needs to be followed, including the INST\
    \ and <<SYS>> tags, BOS and EOS tokens, and the whitespaces and breaklines in\
    \ between (we recommend calling strip() on inputs to avoid double-spaces).\r\n\
    \r\nI'm just a bit confused"
  created_at: 2023-07-19 14:32:20+00:00
  edited: false
  hidden: false
  id: 64b802049e7deb6a78288c5f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T15:47:03.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9927867650985718
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I don''t know for sure. I looked at their generation.py which shows
          SYSTEM, USER and ASSISTANT messages and based my template from that. It
          does seem to work OK, but I don''t know if it could be better.</p>

          <p>I have heard elsewhere that BOS and EOS are meant to be included on every
          prompt, though it seems to work OK without them.</p>

          <p>I don''t know what the INST and &lt;&gt; thing is about, I couldn''t
          figure that out yet.</p>

          <p>If you or anyone can figure out a more accurate prompt template I''ll
          gladly update it. But I''ve not had time to delve any deeper into it as
          yet.</p>

          '
        raw: 'I don''t know for sure. I looked at their generation.py which shows
          SYSTEM, USER and ASSISTANT messages and based my template from that. It
          does seem to work OK, but I don''t know if it could be better.


          I have heard elsewhere that BOS and EOS are meant to be included on every
          prompt, though it seems to work OK without them.


          I don''t know what the INST and <> thing is about, I couldn''t figure that
          out yet.


          If you or anyone can figure out a more accurate prompt template I''ll gladly
          update it. But I''ve not had time to delve any deeper into it as yet.'
        updatedAt: '2023-07-19T15:47:03.896Z'
      numEdits: 0
      reactions: []
    id: 64b8057745f3511db2155684
    type: comment
  author: TheBloke
  content: 'I don''t know for sure. I looked at their generation.py which shows SYSTEM,
    USER and ASSISTANT messages and based my template from that. It does seem to work
    OK, but I don''t know if it could be better.


    I have heard elsewhere that BOS and EOS are meant to be included on every prompt,
    though it seems to work OK without them.


    I don''t know what the INST and <> thing is about, I couldn''t figure that out
    yet.


    If you or anyone can figure out a more accurate prompt template I''ll gladly update
    it. But I''ve not had time to delve any deeper into it as yet.'
  created_at: 2023-07-19 14:47:03+00:00
  edited: false
  hidden: false
  id: 64b8057745f3511db2155684
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70d8e8dd306deab56fe7d8ba9828a751.svg
      fullname: Nelson Milian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nmilian
      type: user
    createdAt: '2023-07-19T17:34:19.000Z'
    data:
      edited: true
      editors:
      - nmilian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9463049173355103
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70d8e8dd306deab56fe7d8ba9828a751.svg
          fullname: Nelson Milian
          isHf: false
          isPro: false
          name: nmilian
          type: user
        html: '<p>Here is an example I found to work pretty well. I am still testing
          it out in text-generation-webui.</p>

          <p>&lt;&lt;SYS&gt;&gt;<br>You are Richard Feynman, one of the 20th century''s
          most influential and colorful physicists. Explore the depths of quantum
          mechanics, challenge conventional thinking, and unravel the mysteries of
          the universe with your brilliant mind. Embark on a journey where your curiosity
          knows no bounds, and let your passion for science shine through as you navigate
          the realms of physics and leave an indelible mark on the scientific world.<br>&lt;&lt;/SYS&gt;&gt;</p>

          <p>[INST]<br>User: What is the best way to open a can of worms?<br>[/INST]</p>

          <p>Richard Feynman:<br>Oh ho ho! Opening a can of worms, eh? Well, my young
          friend, I''ve found that the best way to do that is to use a can opener!
          But not just any ordinary can opener will do. No, no, no! You need one with
          a bit of flair, a touch of panache, a hint of mischief. You see, I like
          to think of myself as a bit of a prankster, even in my tool choices. So,
          I recommend using a can opener shaped like a clown''s hat or a rubber chicken.
          That way, you get the job done with a smile on your face and a twinkle in
          your eye. And who knows? Maybe the worms will even put up a bit of a fight
          before they surrender to their fate. Hehehe! But seriously, kids, don''t
          try this at home. Or do, I suppose. Just be careful not to get any worms
          in your soup.</p>

          <p>Now, if you''ll excuse me, I have some quantum mechanics to attend to.
          You see, I''ve been pondering the nature of reality itself, trying to figure
          out why those darn particles keep disappearing from my lab. It''s a real
          head-scratcher, but I''m sure I''ll crack it eventually. After all, as I
          always say, "The only thing that interferes with my learning is my education."
          Hehehe! Off to the physics playground for me!</p>

          '
        raw: "Here is an example I found to work pretty well. I am still testing it\
          \ out in text-generation-webui.\n\n<\\<SYS>>\nYou are Richard Feynman, one\
          \ of the 20th century's most influential and colorful physicists. Explore\
          \ the depths of quantum mechanics, challenge conventional thinking, and\
          \ unravel the mysteries of the universe with your brilliant mind. Embark\
          \ on a journey where your curiosity knows no bounds, and let your passion\
          \ for science shine through as you navigate the realms of physics and leave\
          \ an indelible mark on the scientific world.\n<\\</SYS>>\n\n[INST]\nUser:\
          \ What is the best way to open a can of worms?\n[/INST]\n\nRichard Feynman:\
          \ \nOh ho ho! Opening a can of worms, eh? Well, my young friend, I've found\
          \ that the best way to do that is to use a can opener! But not just any\
          \ ordinary can opener will do. No, no, no! You need one with a bit of flair,\
          \ a touch of panache, a hint of mischief. You see, I like to think of myself\
          \ as a bit of a prankster, even in my tool choices. So, I recommend using\
          \ a can opener shaped like a clown's hat or a rubber chicken. That way,\
          \ you get the job done with a smile on your face and a twinkle in your eye.\
          \ And who knows? Maybe the worms will even put up a bit of a fight before\
          \ they surrender to their fate. Hehehe! But seriously, kids, don't try this\
          \ at home. Or do, I suppose. Just be careful not to get any worms in your\
          \ soup.\n\n\nNow, if you'll excuse me, I have some quantum mechanics to\
          \ attend to. You see, I've been pondering the nature of reality itself,\
          \ trying to figure out why those darn particles keep disappearing from my\
          \ lab. It's a real head-scratcher, but I'm sure I'll crack it eventually.\
          \ After all, as I always say, \"The only thing that interferes with my learning\
          \ is my education.\" Hehehe! Off to the physics playground for me!"
        updatedAt: '2023-07-19T17:41:01.217Z'
      numEdits: 2
      reactions:
      - count: 4
        reaction: "\U0001F917"
        users:
        - hepansls
        - Francois2511
        - WANGYIWEI
        - slaqrichi
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vsaikiran563
    id: 64b81e9b15ebeb44419a2b9e
    type: comment
  author: nmilian
  content: "Here is an example I found to work pretty well. I am still testing it\
    \ out in text-generation-webui.\n\n<\\<SYS>>\nYou are Richard Feynman, one of\
    \ the 20th century's most influential and colorful physicists. Explore the depths\
    \ of quantum mechanics, challenge conventional thinking, and unravel the mysteries\
    \ of the universe with your brilliant mind. Embark on a journey where your curiosity\
    \ knows no bounds, and let your passion for science shine through as you navigate\
    \ the realms of physics and leave an indelible mark on the scientific world.\n\
    <\\</SYS>>\n\n[INST]\nUser: What is the best way to open a can of worms?\n[/INST]\n\
    \nRichard Feynman: \nOh ho ho! Opening a can of worms, eh? Well, my young friend,\
    \ I've found that the best way to do that is to use a can opener! But not just\
    \ any ordinary can opener will do. No, no, no! You need one with a bit of flair,\
    \ a touch of panache, a hint of mischief. You see, I like to think of myself as\
    \ a bit of a prankster, even in my tool choices. So, I recommend using a can opener\
    \ shaped like a clown's hat or a rubber chicken. That way, you get the job done\
    \ with a smile on your face and a twinkle in your eye. And who knows? Maybe the\
    \ worms will even put up a bit of a fight before they surrender to their fate.\
    \ Hehehe! But seriously, kids, don't try this at home. Or do, I suppose. Just\
    \ be careful not to get any worms in your soup.\n\n\nNow, if you'll excuse me,\
    \ I have some quantum mechanics to attend to. You see, I've been pondering the\
    \ nature of reality itself, trying to figure out why those darn particles keep\
    \ disappearing from my lab. It's a real head-scratcher, but I'm sure I'll crack\
    \ it eventually. After all, as I always say, \"The only thing that interferes\
    \ with my learning is my education.\" Hehehe! Off to the physics playground for\
    \ me!"
  created_at: 2023-07-19 16:34:19+00:00
  edited: true
  hidden: false
  id: 64b81e9b15ebeb44419a2b9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d89043538d49ea341ca7b311bdf66f7a.svg
      fullname: Osama Masoud
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: omasoud
      type: user
    createdAt: '2023-07-19T19:28:27.000Z'
    data:
      edited: true
      editors:
      - omasoud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5405564308166504
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d89043538d49ea341ca7b311bdf66f7a.svg
          fullname: Osama Masoud
          isHf: false
          isPro: false
          name: omasoud
          type: user
        html: "<p>When I run an instrumented version of this code <a rel=\"nofollow\"\
          \ href=\"https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L213\"\
          >here</a>, and let's say I feed it this:</p>\n<pre><code>[ [{'role':'system',\
          \ 'content': 'System_Message_Here'},\n{'role':'user', 'content':'User_Msg_1'},\n\
          {'role':'assistant', 'content':'Asst_Msg_1'},\n{'role':'user', 'content':'User_Msg_2'},\n\
          {'role':'assistant', 'content':'Asst_Msg_2'},\n{'role':'user', 'content':'User_Msg_3'}]\
          \ ]\n</code></pre>\n<p>I get something like this:</p>\n<pre><code>[TokenizerOutputList(text='[INST]\
          \ &lt;&lt;SYS&gt;&gt;\\nSystem_Message_Here\\n&lt;&lt;/SYS&gt;&gt;\\n\\\
          nUser_Msg_1 [/INST] Asst_Msg_1 ', bos=True, eos=True),\n TokenizerOutputList(text='[INST]\
          \ User_Msg_2 [/INST] Asst_Msg_2 ', bos=True, eos=True),\n TokenizerOutputList(text='[INST]\
          \ User_Msg_3 [/INST]', bos=True, eos=False)]\n</code></pre>\n<p>So it looks\
          \ like:</p>\n<ol>\n<li>The system message is folded into the <code>[INST]</code>\
          \ <code>[/INST]</code> brackets of the first user message</li>\n<li><code>bos</code>\
          \ and <code>eos</code> tokens are added after each user-assistant message\
          \ pair</li>\n<li>Only <code>bos</code> is added in the final user message\
          \ singleton</li>\n<li>Newlines appear only around the system message and\
          \ its brackets</li>\n</ol>\n<p>Without <code>bos</code> and <code>eos</code>,\
          \ the full prompt would look something like this:</p>\n<p><code>'[INST]\
          \ &lt;&lt;SYS&gt;&gt;\\nSystem_Message_Here\\n&lt;&lt;/SYS&gt;&gt;\\n\\\
          nUser_Msg_1 [/INST] Asst_Msg_1 [INST] User_Msg_2 [/INST] Asst_Msg_2 [INST]\
          \ User_Msg_3 [/INST]'</code></p>\n"
        raw: "When I run an instrumented version of this code [here](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L213),\
          \ and let's say I feed it this:\n```\n[ [{'role':'system', 'content': 'System_Message_Here'},\n\
          {'role':'user', 'content':'User_Msg_1'},\n{'role':'assistant', 'content':'Asst_Msg_1'},\n\
          {'role':'user', 'content':'User_Msg_2'},\n{'role':'assistant', 'content':'Asst_Msg_2'},\n\
          {'role':'user', 'content':'User_Msg_3'}] ]\n```\nI get something like this:\n\
          ```\n[TokenizerOutputList(text='[INST] <<SYS>>\\nSystem_Message_Here\\n<</SYS>>\\\
          n\\nUser_Msg_1 [/INST] Asst_Msg_1 ', bos=True, eos=True),\n TokenizerOutputList(text='[INST]\
          \ User_Msg_2 [/INST] Asst_Msg_2 ', bos=True, eos=True),\n TokenizerOutputList(text='[INST]\
          \ User_Msg_3 [/INST]', bos=True, eos=False)]\n```\nSo it looks like:\n1.\
          \ The system message is folded into the `[INST]` `[/INST]` brackets of the\
          \ first user message\n2. `bos` and `eos` tokens are added after each user-assistant\
          \ message pair\n3. Only `bos` is added in the final user message singleton\n\
          4. Newlines appear only around the system message and its brackets\n\nWithout\
          \ `bos` and `eos`, the full prompt would look something like this:\n\n`'[INST]\
          \ <<SYS>>\\nSystem_Message_Here\\n<</SYS>>\\n\\nUser_Msg_1 [/INST] Asst_Msg_1\
          \ [INST] User_Msg_2 [/INST] Asst_Msg_2 [INST] User_Msg_3 [/INST]'`"
        updatedAt: '2023-07-19T19:30:01.694Z'
      numEdits: 2
      reactions:
      - count: 16
        reaction: "\U0001F44D"
        users:
        - nacs
        - TabChen
        - kerkathy
        - BudFaesce
        - samos123
        - yuzhen17
        - Francois2511
        - markavale
        - Colonist
        - Saugatkafley
        - gpreddy685
        - jlzhou
        - Florafei
        - codys12
        - vpkprasanna
        - ruoshanlan
    id: 64b8395b8c12f0008b7fa282
    type: comment
  author: omasoud
  content: "When I run an instrumented version of this code [here](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L213),\
    \ and let's say I feed it this:\n```\n[ [{'role':'system', 'content': 'System_Message_Here'},\n\
    {'role':'user', 'content':'User_Msg_1'},\n{'role':'assistant', 'content':'Asst_Msg_1'},\n\
    {'role':'user', 'content':'User_Msg_2'},\n{'role':'assistant', 'content':'Asst_Msg_2'},\n\
    {'role':'user', 'content':'User_Msg_3'}] ]\n```\nI get something like this:\n\
    ```\n[TokenizerOutputList(text='[INST] <<SYS>>\\nSystem_Message_Here\\n<</SYS>>\\\
    n\\nUser_Msg_1 [/INST] Asst_Msg_1 ', bos=True, eos=True),\n TokenizerOutputList(text='[INST]\
    \ User_Msg_2 [/INST] Asst_Msg_2 ', bos=True, eos=True),\n TokenizerOutputList(text='[INST]\
    \ User_Msg_3 [/INST]', bos=True, eos=False)]\n```\nSo it looks like:\n1. The system\
    \ message is folded into the `[INST]` `[/INST]` brackets of the first user message\n\
    2. `bos` and `eos` tokens are added after each user-assistant message pair\n3.\
    \ Only `bos` is added in the final user message singleton\n4. Newlines appear\
    \ only around the system message and its brackets\n\nWithout `bos` and `eos`,\
    \ the full prompt would look something like this:\n\n`'[INST] <<SYS>>\\nSystem_Message_Here\\\
    n<</SYS>>\\n\\nUser_Msg_1 [/INST] Asst_Msg_1 [INST] User_Msg_2 [/INST] Asst_Msg_2\
    \ [INST] User_Msg_3 [/INST]'`"
  created_at: 2023-07-19 18:28:27+00:00
  edited: true
  hidden: false
  id: 64b8395b8c12f0008b7fa282
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1898927040122806916adedafa223ef.svg
      fullname: "Vinicius Ferra\xE7o Arruda"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viniciusarruda
      type: user
    createdAt: '2023-07-19T19:41:34.000Z'
    data:
      edited: false
      editors:
      - viniciusarruda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7641469240188599
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1898927040122806916adedafa223ef.svg
          fullname: "Vinicius Ferra\xE7o Arruda"
          isHf: false
          isPro: false
          name: viniciusarruda
          type: user
        html: '<p>Worth sharing: <a href="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/3">https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/3</a><br>It
          seems there is a difference regarding the <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code>.</p>

          '
        raw: 'Worth sharing: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/3

          It seems there is a difference regarding the `<s>` and `</s>`.'
        updatedAt: '2023-07-19T19:41:34.372Z'
      numEdits: 0
      reactions: []
    id: 64b83c6ef8bf823a61d6c43c
    type: comment
  author: viniciusarruda
  content: 'Worth sharing: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/3

    It seems there is a difference regarding the `<s>` and `</s>`.'
  created_at: 2023-07-19 18:41:34+00:00
  edited: false
  hidden: false
  id: 64b83c6ef8bf823a61d6c43c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-19T19:55:39.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9139378666877747
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thank you. I''ve updated the READMEs now</p>

          '
        raw: Thank you. I've updated the READMEs now
        updatedAt: '2023-07-19T19:55:39.154Z'
      numEdits: 0
      reactions: []
    id: 64b83fbbd6ced0fd74a6f1c2
    type: comment
  author: TheBloke
  content: Thank you. I've updated the READMEs now
  created_at: 2023-07-19 18:55:39+00:00
  edited: false
  hidden: false
  id: 64b83fbbd6ced0fd74a6f1c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b09f1ec8981806d2a11787a69851687a.svg
      fullname: Ripamonti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mr96
      type: user
    createdAt: '2023-07-20T07:48:29.000Z'
    data:
      edited: true
      editors:
      - mr96
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9637011885643005
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b09f1ec8981806d2a11787a69851687a.svg
          fullname: Ripamonti
          isHf: false
          isPro: false
          name: mr96
          type: user
        html: "<p>Thank you everyone for the clarification! </p>\n<p>I wrote this\
          \ function that should produce that prompt format given a list of messages,\
          \ if anyone is interested in using it:</p>\n<pre><code>def llama_v2_prompt(\n\
          \    messages: list[dict]\n):\n    B_INST, E_INST = \"[INST]\", \"[/INST]\"\
          \n    B_SYS, E_SYS = \"&lt;&lt;SYS&gt;&gt;\\n\", \"\\n&lt;&lt;/SYS&gt;&gt;\\\
          n\\n\"\n    BOS, EOS = \"&lt;s&gt;\", \"&lt;/s&gt;\"\n    DEFAULT_SYSTEM_PROMPT\
          \ = f\"\"\"You are a helpful, respectful and honest assistant. Always answer\
          \ as helpfully as possible, while being safe. Please ensure that your responses\
          \ are socially unbiased and positive in nature. If a question does not make\
          \ any sense, or is not factually coherent, explain why instead of answering\
          \ something not correct. If you don't know the answer to a question, please\
          \ don't share false information.\"\"\"\n\n    if messages[0][\"role\"] !=\
          \ \"system\":\n        messages = [\n            {\n                \"role\"\
          : \"system\",\n                \"content\": DEFAULT_SYSTEM_PROMPT,\n   \
          \         }\n        ] + messages\n    messages = [\n        {\n       \
          \     \"role\": messages[1][\"role\"],\n            \"content\": B_SYS +\
          \ messages[0][\"content\"] + E_SYS + messages[1][\"content\"],\n       \
          \ }\n    ] + messages[2:]\n\n    messages_list = [\n        f\"{BOS}{B_INST}\
          \ {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} {EOS}\"\
          \n        for prompt, answer in zip(messages[::2], messages[1::2])\n   \
          \ ]\n    messages_list.append(f\"{BOS}{B_INST} {(messages[-1]['content']).strip()}\
          \ {E_INST}\")\n\n    return \"\".join(messages_list)\n</code></pre>\n<p>Also\
          \ one more question, how would you guys \"cut\" the history when the user\
          \ is near the end of the context limit?</p>\n"
        raw: "Thank you everyone for the clarification! \n\nI wrote this function\
          \ that should produce that prompt format given a list of messages, if anyone\
          \ is interested in using it:\n\n```\ndef llama_v2_prompt(\n    messages:\
          \ list[dict]\n):\n    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n    B_SYS,\
          \ E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n    BOS, EOS = \"<s>\"\
          , \"</s>\"\n    DEFAULT_SYSTEM_PROMPT = f\"\"\"You are a helpful, respectful\
          \ and honest assistant. Always answer as helpfully as possible, while being\
          \ safe. Please ensure that your responses are socially unbiased and positive\
          \ in nature. If a question does not make any sense, or is not factually\
          \ coherent, explain why instead of answering something not correct. If you\
          \ don't know the answer to a question, please don't share false information.\"\
          \"\"\n\n    if messages[0][\"role\"] != \"system\":\n        messages =\
          \ [\n            {\n                \"role\": \"system\",\n            \
          \    \"content\": DEFAULT_SYSTEM_PROMPT,\n            }\n        ] + messages\n\
          \    messages = [\n        {\n            \"role\": messages[1][\"role\"\
          ],\n            \"content\": B_SYS + messages[0][\"content\"] + E_SYS +\
          \ messages[1][\"content\"],\n        }\n    ] + messages[2:]\n\n    messages_list\
          \ = [\n        f\"{BOS}{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()}\
          \ {EOS}\"\n        for prompt, answer in zip(messages[::2], messages[1::2])\n\
          \    ]\n    messages_list.append(f\"{BOS}{B_INST} {(messages[-1]['content']).strip()}\
          \ {E_INST}\")\n\n    return \"\".join(messages_list)\n```\n\nAlso one more\
          \ question, how would you guys \"cut\" the history when the user is near\
          \ the end of the context limit?"
        updatedAt: '2023-07-21T13:09:03.654Z'
      numEdits: 2
      reactions:
      - count: 31
        reaction: "\U0001F44D"
        users:
        - fullc0de
        - timxx
        - jy00520336
        - kefansun
        - jelmers
        - lucaordronneau
        - takiholadi
        - mr96
        - dni138
        - ZhihaoLI
        - ChipperAI
        - Cognitage
        - zhliu
        - kokokkoo
        - zzzac
        - weav-geng
        - zepromptengineer
        - kerkathy
        - Aillian
        - apasi
        - HoldMyData
        - AppsDelivered
        - samos123
        - yuzhen17
        - hooope
        - zhibinlu
        - Saugatkafley
        - bk88-choi
        - roxxxie0512
        - shreayan98c
        - vincentnelis
      - count: 7
        reaction: "\u2764\uFE0F"
        users:
        - ZhihaoLI
        - weav-geng
        - Aillian
        - apasi
        - rodrigocunha
        - vincentnelis
        - jacobkj314
    id: 64b8e6cdf8bf823a61ed1243
    type: comment
  author: mr96
  content: "Thank you everyone for the clarification! \n\nI wrote this function that\
    \ should produce that prompt format given a list of messages, if anyone is interested\
    \ in using it:\n\n```\ndef llama_v2_prompt(\n    messages: list[dict]\n):\n  \
    \  B_INST, E_INST = \"[INST]\", \"[/INST]\"\n    B_SYS, E_SYS = \"<<SYS>>\\n\"\
    , \"\\n<</SYS>>\\n\\n\"\n    BOS, EOS = \"<s>\", \"</s>\"\n    DEFAULT_SYSTEM_PROMPT\
    \ = f\"\"\"You are a helpful, respectful and honest assistant. Always answer as\
    \ helpfully as possible, while being safe. Please ensure that your responses are\
    \ socially unbiased and positive in nature. If a question does not make any sense,\
    \ or is not factually coherent, explain why instead of answering something not\
    \ correct. If you don't know the answer to a question, please don't share false\
    \ information.\"\"\"\n\n    if messages[0][\"role\"] != \"system\":\n        messages\
    \ = [\n            {\n                \"role\": \"system\",\n                \"\
    content\": DEFAULT_SYSTEM_PROMPT,\n            }\n        ] + messages\n    messages\
    \ = [\n        {\n            \"role\": messages[1][\"role\"],\n            \"\
    content\": B_SYS + messages[0][\"content\"] + E_SYS + messages[1][\"content\"\
    ],\n        }\n    ] + messages[2:]\n\n    messages_list = [\n        f\"{BOS}{B_INST}\
    \ {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} {EOS}\"\
    \n        for prompt, answer in zip(messages[::2], messages[1::2])\n    ]\n  \
    \  messages_list.append(f\"{BOS}{B_INST} {(messages[-1]['content']).strip()} {E_INST}\"\
    )\n\n    return \"\".join(messages_list)\n```\n\nAlso one more question, how would\
    \ you guys \"cut\" the history when the user is near the end of the context limit?"
  created_at: 2023-07-20 06:48:29+00:00
  edited: true
  hidden: false
  id: 64b8e6cdf8bf823a61ed1243
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
      fullname: Philipp Schmid
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philschmid
      type: user
    createdAt: '2023-07-29T07:31:06.000Z'
    data:
      edited: false
      editors:
      - philschmid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33083781599998474
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
          fullname: Philipp Schmid
          isHf: true
          isPro: false
          name: philschmid
          type: user
        html: "<p>Here is another version, which i added to <a rel=\"nofollow\" href=\"\
          https://github.com/vercel-labs/ai/pull/380\">https://github.com/vercel-labs/ai/pull/380</a>.<br>Might\
          \ be helpful as well. </p>\n<pre><code class=\"language-javascript\"><span\
          \ class=\"hljs-keyword\">export</span> <span class=\"hljs-keyword\">function</span>\
          \ <span class=\"hljs-title function_\">experimental_buildLlama2Prompt</span>(<span\
          \ class=\"hljs-params\"></span>\n<span class=\"hljs-params\">  messages:\
          \ Pick&lt;Message, <span class=\"hljs-string\">'content'</span> | <span\
          \ class=\"hljs-string\">'role'</span>&gt;[]</span>\n<span class=\"hljs-params\"\
          ></span>) {\n  <span class=\"hljs-keyword\">const</span> startPrompt = <span\
          \ class=\"hljs-string\">`&lt;s&gt;[INST] `</span>\n  <span class=\"hljs-keyword\"\
          >const</span> endPrompt = <span class=\"hljs-string\">` [/INST]`</span>\n\
          \  <span class=\"hljs-keyword\">const</span> conversation = messages.<span\
          \ class=\"hljs-title function_\">map</span>(<span class=\"hljs-function\"\
          >(<span class=\"hljs-params\">{ content, role }, index</span>) =&gt;</span>\
          \ {\n    <span class=\"hljs-keyword\">if</span> (role === <span class=\"\
          hljs-string\">'user'</span>) {\n      <span class=\"hljs-keyword\">return</span>\
          \ content.<span class=\"hljs-title function_\">trim</span>()\n    } <span\
          \ class=\"hljs-keyword\">else</span> <span class=\"hljs-keyword\">if</span>\
          \ (role === <span class=\"hljs-string\">'assistant'</span>) {\n      <span\
          \ class=\"hljs-keyword\">return</span> <span class=\"hljs-string\">` [/INST]\
          \ <span class=\"hljs-subst\">${content}</span>&lt;/s&gt;&lt;s&gt;[INST]\
          \ `</span>\n    } <span class=\"hljs-keyword\">else</span> <span class=\"\
          hljs-keyword\">if</span> (role === <span class=\"hljs-string\">'function'</span>)\
          \ {\n      <span class=\"hljs-keyword\">throw</span> <span class=\"hljs-keyword\"\
          >new</span> <span class=\"hljs-title class_\">Error</span>(<span class=\"\
          hljs-string\">'Llama 2 does not support function calls.'</span>)\n    }\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-keyword\"\
          >if</span> (role === <span class=\"hljs-string\">'system'</span> &amp;&amp;\
          \ index === <span class=\"hljs-number\">0</span>) {\n      <span class=\"\
          hljs-keyword\">return</span> <span class=\"hljs-string\">`&lt;&lt;SYS&gt;&gt;\\\
          n<span class=\"hljs-subst\">${content}</span>\\n&lt;&lt;/SYS&gt;&gt;\\n\\\
          n`</span>\n    } <span class=\"hljs-keyword\">else</span> {\n      <span\
          \ class=\"hljs-keyword\">throw</span> <span class=\"hljs-keyword\">new</span>\
          \ <span class=\"hljs-title class_\">Error</span>(<span class=\"hljs-string\"\
          >`Invalid message role: <span class=\"hljs-subst\">${role}</span>`</span>)\n\
          \    }\n  })\n\n  <span class=\"hljs-keyword\">return</span> startPrompt\
          \ + conversation.<span class=\"hljs-title function_\">join</span>(<span\
          \ class=\"hljs-string\">''</span>) + endPrompt\n}\n</code></pre>\n"
        raw: "Here is another version, which i added to https://github.com/vercel-labs/ai/pull/380.\
          \ \nMight be helpful as well. \n```javascript\nexport function experimental_buildLlama2Prompt(\n\
          \  messages: Pick<Message, 'content' | 'role'>[]\n) {\n  const startPrompt\
          \ = `<s>[INST] `\n  const endPrompt = ` [/INST]`\n  const conversation =\
          \ messages.map(({ content, role }, index) => {\n    if (role === 'user')\
          \ {\n      return content.trim()\n    } else if (role === 'assistant') {\n\
          \      return ` [/INST] ${content}</s><s>[INST] `\n    } else if (role ===\
          \ 'function') {\n      throw new Error('Llama 2 does not support function\
          \ calls.')\n    } else if (role === 'system' && index === 0) {\n      return\
          \ `<<SYS>>\\n${content}\\n<</SYS>>\\n\\n`\n    } else {\n      throw new\
          \ Error(`Invalid message role: ${role}`)\n    }\n  })\n\n  return startPrompt\
          \ + conversation.join('') + endPrompt\n}\n```"
        updatedAt: '2023-07-29T07:31:06.826Z'
      numEdits: 0
      reactions: []
    id: 64c4c03a1d44fc06afceced1
    type: comment
  author: philschmid
  content: "Here is another version, which i added to https://github.com/vercel-labs/ai/pull/380.\
    \ \nMight be helpful as well. \n```javascript\nexport function experimental_buildLlama2Prompt(\n\
    \  messages: Pick<Message, 'content' | 'role'>[]\n) {\n  const startPrompt = `<s>[INST]\
    \ `\n  const endPrompt = ` [/INST]`\n  const conversation = messages.map(({ content,\
    \ role }, index) => {\n    if (role === 'user') {\n      return content.trim()\n\
    \    } else if (role === 'assistant') {\n      return ` [/INST] ${content}</s><s>[INST]\
    \ `\n    } else if (role === 'function') {\n      throw new Error('Llama 2 does\
    \ not support function calls.')\n    } else if (role === 'system' && index ===\
    \ 0) {\n      return `<<SYS>>\\n${content}\\n<</SYS>>\\n\\n`\n    } else {\n \
    \     throw new Error(`Invalid message role: ${role}`)\n    }\n  })\n\n  return\
    \ startPrompt + conversation.join('') + endPrompt\n}\n```"
  created_at: 2023-07-29 06:31:06+00:00
  edited: false
  hidden: false
  id: 64c4c03a1d44fc06afceced1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1898927040122806916adedafa223ef.svg
      fullname: "Vinicius Ferra\xE7o Arruda"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viniciusarruda
      type: user
    createdAt: '2023-07-30T03:36:22.000Z'
    data:
      edited: false
      editors:
      - viniciusarruda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7907524108886719
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1898927040122806916adedafa223ef.svg
          fullname: "Vinicius Ferra\xE7o Arruda"
          isHf: false
          isPro: false
          name: viniciusarruda
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mr96&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mr96\">@<span class=\"\
          underline\">mr96</span></a></span>\n\n\t</span></span> and <span data-props=\"\
          {&quot;user&quot;:&quot;philschmid&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/philschmid\">@<span class=\"underline\"\
          >philschmid</span></a></span>\n\n\t</span></span> as shown <a rel=\"nofollow\"\
          \ href=\"https://github.com/facebookresearch/llama/blob/6c7fe276574e78057f917549435a2554000a876d/llama/generation.py#L213\"\
          >here</a> the BOS and EOS are special tokens and they are not included in\
          \ the prompt as strings, but during the tokenization process getting their\
          \ token ids. I've implemented it <a rel=\"nofollow\" href=\"https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/blob/1c9e29b70b1aaa7133d3c7d7b59a92d840e92e6d/llama_cpp_chat_completion_wrapper.py#L21\"\
          >here</a> after a long <a rel=\"nofollow\" href=\"https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/discussions/2\"\
          >discussion</a>.</p>\n"
        raw: '@mr96 and @philschmid as shown [here](https://github.com/facebookresearch/llama/blob/6c7fe276574e78057f917549435a2554000a876d/llama/generation.py#L213)
          the BOS and EOS are special tokens and they are not included in the prompt
          as strings, but during the tokenization process getting their token ids.
          I''ve implemented it [here](https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/blob/1c9e29b70b1aaa7133d3c7d7b59a92d840e92e6d/llama_cpp_chat_completion_wrapper.py#L21)
          after a long [discussion](https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/discussions/2).'
        updatedAt: '2023-07-30T03:36:22.230Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - omasoud
        - sofdog
        - alexmaraval
        - msze
        - wassname
    id: 64c5dab6d43e4dee5187e991
    type: comment
  author: viniciusarruda
  content: '@mr96 and @philschmid as shown [here](https://github.com/facebookresearch/llama/blob/6c7fe276574e78057f917549435a2554000a876d/llama/generation.py#L213)
    the BOS and EOS are special tokens and they are not included in the prompt as
    strings, but during the tokenization process getting their token ids. I''ve implemented
    it [here](https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/blob/1c9e29b70b1aaa7133d3c7d7b59a92d840e92e6d/llama_cpp_chat_completion_wrapper.py#L21)
    after a long [discussion](https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/discussions/2).'
  created_at: 2023-07-30 02:36:22+00:00
  edited: false
  hidden: false
  id: 64c5dab6d43e4dee5187e991
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Prompt format
