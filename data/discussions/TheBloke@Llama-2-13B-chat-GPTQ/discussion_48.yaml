!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andreaKIM
conflicting_files: null
created_at: 2023-11-09 10:46:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1f763ca61fb1f281f7ac24bdee8722f.svg
      fullname: DAEHEEKIM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreaKIM
      type: user
    createdAt: '2023-11-09T10:46:26.000Z'
    data:
      edited: false
      editors:
      - andreaKIM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.73056560754776
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1f763ca61fb1f281f7ac24bdee8722f.svg
          fullname: DAEHEEKIM
          isHf: false
          isPro: false
          name: andreaKIM
          type: user
        html: "<p>Hello I am testing this backbone model for fine tuning my custom\
          \ dataset. Before I start, i just wanted to check how much does it take\
          \ to generate one prediction from GPTQ model. Sicne I am first to use GPTQ\
          \ model, I am stuck on some running error with dependncy. Below is my test\
          \ script.</p>\n<pre><code>from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer, pipeline\nimport time\n\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\
          \n# To use a different branch, change revision\n# For example: revision=\"\
          main\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \                                             device_map=\"auto\",\n   \
          \                                          trust_remote_code=False,\n  \
          \                                           revision=\"main\")\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\n\
          prompt = \"Tell me about AI\"\nprompt_template=f'''[INST] &lt;&lt;SYS&gt;&gt;\n\
          You are a helpful, respectful and honest assistant. Always answer as helpfully\
          \ as possible, while being safe.  Your answers should not include any harmful,\
          \ unethical, racist, sexist, toxic, dangerous, or illegal content. Please\
          \ ensure that your responses are socially unbiased and positive in nature.\
          \ If a question does not make any sense, or is not factually coherent, explain\
          \ why instead of answering something not correct. If you don't know the\
          \ answer to a question, please don't share false information.\n&lt;&lt;/SYS&gt;&gt;\n\
          {prompt}[/INST]\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids =\
          \ tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\nstart_time\
          \ = time.time()\noutput = model.generate(inputs=input_ids, temperature=0.7,\
          \ do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\
          print(\"total time:\",time.time()-start_time)\nprint(\"number of token generated:\"\
          ,len(tokenizer.decode(output[0]).split(\" \")))\n# Inference can also be\
          \ done using transformers' pipeline\n\nstart_time_p = time.time()\nprint(\"\
          *** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n\
          \    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n\
          )\nprint(pipe(prompt_template)[0]['generated_text'])\nprint(\"total time:\"\
          ,time.time()-start_time_p)\nprint(\"number of token generated:\",len(pipe(prompt_template)[0]['generated_text'].split(\"\
          \ \")))\n</code></pre>\n<p>Unfortunately whenever i try this code i got\
          \ ImportError like this.</p>\n<pre><code>CUDA extension not installed.\n\
          CUDA extension not installed.\nexllama_kernels not installed.\nTraceback\
          \ (most recent call last):\n...(skip)\n    from exllama_kernels import make_q4,\
          \ q4_matmul\nImportError: libcudart.so.12: cannot open shared object file:\
          \ No such file or directory\n</code></pre>\n<p>I guess i am missing simple\
          \ point but i have no idea how to break it.<br>Can anybody solve this problem?\
          \ Or I just want to know how fast GPTQ models are. Thanks :)</p>\n"
        raw: "Hello I am testing this backbone model for fine tuning my custom dataset.\
          \ Before I start, i just wanted to check how much does it take to generate\
          \ one prediction from GPTQ model. Sicne I am first to use GPTQ model, I\
          \ am stuck on some running error with dependncy. Below is my test script.\r\
          \n\r\n```\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,\
          \ pipeline\r\nimport time\r\n\r\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\
          \r\n# To use a different branch, change revision\r\n# For example: revision=\"\
          main\"\r\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\r\
          \n                                             device_map=\"auto\",\r\n\
          \                                             trust_remote_code=False,\r\
          \n                                             revision=\"main\")\r\n\r\n\
          tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\
          \n\r\nprompt = \"Tell me about AI\"\r\nprompt_template=f'''[INST] <<SYS>>\r\
          \nYou are a helpful, respectful and honest assistant. Always answer as helpfully\
          \ as possible, while being safe.  Your answers should not include any harmful,\
          \ unethical, racist, sexist, toxic, dangerous, or illegal content. Please\
          \ ensure that your responses are socially unbiased and positive in nature.\
          \ If a question does not make any sense, or is not factually coherent, explain\
          \ why instead of answering something not correct. If you don't know the\
          \ answer to a question, please don't share false information.\r\n<</SYS>>\r\
          \n{prompt}[/INST]\r\n\r\n'''\r\n\r\nprint(\"\\n\\n*** Generate:\")\r\n\r\
          \ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\
          \nstart_time = time.time()\r\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\r\
          \nprint(tokenizer.decode(output[0]))\r\nprint(\"total time:\",time.time()-start_time)\r\
          \nprint(\"number of token generated:\",len(tokenizer.decode(output[0]).split(\"\
          \ \")))\r\n# Inference can also be done using transformers' pipeline\r\n\
          \r\nstart_time_p = time.time()\r\nprint(\"*** Pipeline:\")\r\npipe = pipeline(\r\
          \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    max_new_tokens=512,\r\n    do_sample=True,\r\n    temperature=0.7,\r\
          \n    top_p=0.95,\r\n    top_k=40,\r\n    repetition_penalty=1.1\r\n)\r\n\
          print(pipe(prompt_template)[0]['generated_text'])\r\nprint(\"total time:\"\
          ,time.time()-start_time_p)\r\nprint(\"number of token generated:\",len(pipe(prompt_template)[0]['generated_text'].split(\"\
          \ \")))\r\n```\r\n\r\nUnfortunately whenever i try this code i got ImportError\
          \ like this.\r\n\r\n```\r\nCUDA extension not installed.\r\nCUDA extension\
          \ not installed.\r\nexllama_kernels not installed.\r\nTraceback (most recent\
          \ call last):\r\n...(skip)\r\n    from exllama_kernels import make_q4, q4_matmul\r\
          \nImportError: libcudart.so.12: cannot open shared object file: No such\
          \ file or directory\r\n```\r\n\r\nI guess i am missing simple point but\
          \ i have no idea how to break it.\r\nCan anybody solve this problem? Or\
          \ I just want to know how fast GPTQ models are. Thanks :)"
        updatedAt: '2023-11-09T10:46:26.181Z'
      numEdits: 0
      reactions: []
    id: 654cb882fc2dfc8f818bb89c
    type: comment
  author: andreaKIM
  content: "Hello I am testing this backbone model for fine tuning my custom dataset.\
    \ Before I start, i just wanted to check how much does it take to generate one\
    \ prediction from GPTQ model. Sicne I am first to use GPTQ model, I am stuck on\
    \ some running error with dependncy. Below is my test script.\r\n\r\n```\r\nfrom\
    \ transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\r\nimport\
    \ time\r\n\r\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\r\n# To\
    \ use a different branch, change revision\r\n# For example: revision=\"main\"\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\r\n       \
    \                                      device_map=\"auto\",\r\n              \
    \                               trust_remote_code=False,\r\n                 \
    \                            revision=\"main\")\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n\r\nprompt = \"Tell me about AI\"\r\nprompt_template=f'''[INST]\
    \ <<SYS>>\r\nYou are a helpful, respectful and honest assistant. Always answer\
    \ as helpfully as possible, while being safe.  Your answers should not include\
    \ any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\
    \ Please ensure that your responses are socially unbiased and positive in nature.\
    \ If a question does not make any sense, or is not factually coherent, explain\
    \ why instead of answering something not correct. If you don't know the answer\
    \ to a question, please don't share false information.\r\n<</SYS>>\r\n{prompt}[/INST]\r\
    \n\r\n'''\r\n\r\nprint(\"\\n\\n*** Generate:\")\r\n\r\ninput_ids = tokenizer(prompt_template,\
    \ return_tensors='pt').input_ids.cuda()\r\nstart_time = time.time()\r\noutput\
    \ = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95,\
    \ top_k=40, max_new_tokens=512)\r\nprint(tokenizer.decode(output[0]))\r\nprint(\"\
    total time:\",time.time()-start_time)\r\nprint(\"number of token generated:\"\
    ,len(tokenizer.decode(output[0]).split(\" \")))\r\n# Inference can also be done\
    \ using transformers' pipeline\r\n\r\nstart_time_p = time.time()\r\nprint(\"***\
    \ Pipeline:\")\r\npipe = pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
    \n    tokenizer=tokenizer,\r\n    max_new_tokens=512,\r\n    do_sample=True,\r\
    \n    temperature=0.7,\r\n    top_p=0.95,\r\n    top_k=40,\r\n    repetition_penalty=1.1\r\
    \n)\r\nprint(pipe(prompt_template)[0]['generated_text'])\r\nprint(\"total time:\"\
    ,time.time()-start_time_p)\r\nprint(\"number of token generated:\",len(pipe(prompt_template)[0]['generated_text'].split(\"\
    \ \")))\r\n```\r\n\r\nUnfortunately whenever i try this code i got ImportError\
    \ like this.\r\n\r\n```\r\nCUDA extension not installed.\r\nCUDA extension not\
    \ installed.\r\nexllama_kernels not installed.\r\nTraceback (most recent call\
    \ last):\r\n...(skip)\r\n    from exllama_kernels import make_q4, q4_matmul\r\n\
    ImportError: libcudart.so.12: cannot open shared object file: No such file or\
    \ directory\r\n```\r\n\r\nI guess i am missing simple point but i have no idea\
    \ how to break it.\r\nCan anybody solve this problem? Or I just want to know how\
    \ fast GPTQ models are. Thanks :)"
  created_at: 2023-11-09 10:46:26+00:00
  edited: false
  hidden: false
  id: 654cb882fc2dfc8f818bb89c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 48
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: How much does it take to inference one sample?
