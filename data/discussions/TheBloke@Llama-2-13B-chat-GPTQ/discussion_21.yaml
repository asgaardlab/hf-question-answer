!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Dhairye
conflicting_files: null
created_at: 2023-07-27 10:24:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bb33144b0016ff703806b0ca97d147c.svg
      fullname: Dhairye Gala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dhairye
      type: user
    createdAt: '2023-07-27T11:24:35.000Z'
    data:
      edited: false
      editors:
      - Dhairye
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5437940359115601
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bb33144b0016ff703806b0ca97d147c.svg
          fullname: Dhairye Gala
          isHf: false
          isPro: false
          name: Dhairye
          type: user
        html: '<p>this is my code:<br>from transformers import AutoTokenizer, pipeline,
          logging<br>from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig<br>model_name_or_path
          = "TheBloke/Llama-2-13B-chat-GPTQ"<br>model_basename = "gptq_model-4bit-128g"</p>

          <p>use_triton = False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>        model_basename=model_basename,<br>        use_safetensors=True,<br>        trust_remote_code=True,<br>        device="cuda:0",<br>        use_triton=use_triton,<br>        quantize_config=None)</p>

          <p>prompt = "Tell me about AI"<br>system_message = "You are a helpful, respectful
          and honest assistant. Always answer as helpfully as possible, while being
          safe.  Your answers should not include any harmful, unethical, racist, sexist,
          toxic, dangerous, or illegal content. Please ensure that your responses
          are socially unbiased and positive in nature. If a question does not make
          any sense, or is not factually coherent, explain why instead of answering
          something not correct. If you don''t know the answer to a question, please
          don''t share false information."<br>prompt_template=f''''''[INST] &lt;&gt;<br>{system_message}<br>&lt;&gt;</p>

          <p>{prompt} [/INST]''''''</p>

          <p>print("\n\n*** Generate:")</p>

          <p>input_ids = tokenizer(prompt_template, return_tensors=''pt'').input_ids.cuda()<br>output
          = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)<br>print(tokenizer.decode(output[0]))</p>

          <h1 id="inference-can-also-be-done-using-transformers-pipeline">Inference
          can also be done using transformers'' pipeline</h1>

          <h1 id="prevent-printing-spurious-transformers-error-when-using-pipeline-with-autogptq">Prevent
          printing spurious transformers error when using pipeline with AutoGPTQ</h1>

          <p>logging.set_verbosity(logging.CRITICAL)</p>

          <p>print("*** Pipeline:")<br>pipe = pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    max_new_tokens=512,<br>    temperature=0.7,<br>    top_p=0.95,<br>    repetition_penalty=1.15<br>)</p>

          <p>print(pipe(prompt_template)[0][''generated_text''])</p>

          <p>gives me the following error:<br>AttributeError                            Traceback
          (most recent call last)<br>Cell In[23], line 13<br>      9 use_triton =
          False<br>     11 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,
          use_fast=True)<br>---&gt; 13 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>     14         model_basename=model_basename,<br>     15         use_safetensors=True,<br>     16         trust_remote_code=True,<br>     17         device="cuda:0",<br>     18         use_triton=use_triton,<br>     19         quantize_config=None)<br>     21
          """<br>     22 To download from a specific branch, use the revision parameter,
          as in this example:<br>     23<br>   (...)<br>     30         quantize_config=None)<br>     31
          """<br>     33 prompt = "Tell me about AI"</p>

          <p>File /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:94,
          in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir,
          device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention,
          inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,
          trust_remote_code, warmup_triton, trainable, **kwargs)<br>     88 quant_func
          = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized<br>     89 keywords
          = {<br>     90     key: kwargs[key]<br>     91     for key in signature(quant_func).parameters<br>     92     if
          key in kwargs<br>     93 }<br>---&gt; 94 return quant_func(<br>     95     model_name_or_path=model_name_or_path,<br>     96     save_dir=save_dir,<br>     97     device_map=device_map,<br>     98     max_memory=max_memory,<br>     99     device=device,<br>    100     low_cpu_mem_usage=low_cpu_mem_usage,<br>    101     use_triton=use_triton,<br>    102     inject_fused_attention=inject_fused_attention,<br>    103     inject_fused_mlp=inject_fused_mlp,<br>    104     use_cuda_fp16=use_cuda_fp16,<br>    105     quantize_config=quantize_config,<br>    106     model_basename=model_basename,<br>    107     use_safetensors=use_safetensors,<br>    108     trust_remote_code=trust_remote_code,<br>    109     warmup_triton=warmup_triton,<br>    110     trainable=trainable,<br>    111     **keywords<br>    112
          )</p>

          <p>File /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:793,
          in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir,
          device_map, max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype,
          inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config,
          model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable,
          **kwargs)<br>    790 if low_cpu_mem_usage:<br>    791     make_sure_no_tensor_in_meta_device(model,
          use_triton, quantize_config.desc_act, quantize_config.group_size)<br>--&gt;
          793 accelerate.utils.modeling.load_checkpoint_in_model(<br>    794     model,<br>    795     checkpoint=model_save_name,<br>    796     device_map=device_map,<br>    797     offload_state_dict=True,<br>    798     offload_buffers=True<br>    799
          )<br>    800 model = simple_dispatch_model(model, device_map)<br>    802
          # == step4: set seqlen == #</p>

          <p>AttributeError: module ''accelerate.utils'' has no attribute ''modeling''</p>

          '
        raw: "this is my code:\r\nfrom transformers import AutoTokenizer, pipeline,\
          \ logging\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\
          \nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\r\nmodel_basename\
          \ = \"gptq_model-4bit-128g\"\r\n\r\nuse_triton = False\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\n\
          \r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\n \
          \       model_basename=model_basename,\r\n        use_safetensors=True,\r\
          \n        trust_remote_code=True,\r\n        device=\"cuda:0\",\r\n    \
          \    use_triton=use_triton,\r\n        quantize_config=None)\r\n\r\nprompt\
          \ = \"Tell me about AI\"\r\nsystem_message = \"You are a helpful, respectful\
          \ and honest assistant. Always answer as helpfully as possible, while being\
          \ safe.  Your answers should not include any harmful, unethical, racist,\
          \ sexist, toxic, dangerous, or illegal content. Please ensure that your\
          \ responses are socially unbiased and positive in nature. If a question\
          \ does not make any sense, or is not factually coherent, explain why instead\
          \ of answering something not correct. If you don't know the answer to a\
          \ question, please don't share false information.\"\r\nprompt_template=f'''[INST]\
          \ <<SYS>>\r\n{system_message}\r\n<</SYS>>\r\n\r\n{prompt} [/INST]'''\r\n\
          \r\nprint(\"\\n\\n*** Generate:\")\r\n\r\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\r\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, max_new_tokens=512)\r\nprint(tokenizer.decode(output[0]))\r\
          \n\r\n# Inference can also be done using transformers' pipeline\r\n\r\n\
          # Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\r\nlogging.set_verbosity(logging.CRITICAL)\r\n\r\nprint(\"***\
          \ Pipeline:\")\r\npipe = pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
          \n    tokenizer=tokenizer,\r\n    max_new_tokens=512,\r\n    temperature=0.7,\r\
          \n    top_p=0.95,\r\n    repetition_penalty=1.15\r\n)\r\n\r\nprint(pipe(prompt_template)[0]['generated_text'])\r\
          \n\r\ngives me the following error:\r\nAttributeError                  \
          \          Traceback (most recent call last)\r\nCell In[23], line 13\r\n\
          \      9 use_triton = False\r\n     11 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\r\n---> 13 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n     14         model_basename=model_basename,\r\n     15         use_safetensors=True,\r\
          \n     16         trust_remote_code=True,\r\n     17         device=\"cuda:0\"\
          ,\r\n     18         use_triton=use_triton,\r\n     19         quantize_config=None)\r\
          \n     21 \"\"\"\r\n     22 To download from a specific branch, use the\
          \ revision parameter, as in this example:\r\n     23 \r\n   (...)\r\n  \
          \   30         quantize_config=None)\r\n     31 \"\"\"\r\n     33 prompt\
          \ = \"Tell me about AI\"\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:94,\
          \ in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir,\
          \ device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention,\
          \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
          \ trust_remote_code, warmup_triton, trainable, **kwargs)\r\n     88 quant_func\
          \ = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized\r\n     89 keywords\
          \ = {\r\n     90     key: kwargs[key]\r\n     91     for key in signature(quant_func).parameters\r\
          \n     92     if key in kwargs\r\n     93 }\r\n---> 94 return quant_func(\r\
          \n     95     model_name_or_path=model_name_or_path,\r\n     96     save_dir=save_dir,\r\
          \n     97     device_map=device_map,\r\n     98     max_memory=max_memory,\r\
          \n     99     device=device,\r\n    100     low_cpu_mem_usage=low_cpu_mem_usage,\r\
          \n    101     use_triton=use_triton,\r\n    102     inject_fused_attention=inject_fused_attention,\r\
          \n    103     inject_fused_mlp=inject_fused_mlp,\r\n    104     use_cuda_fp16=use_cuda_fp16,\r\
          \n    105     quantize_config=quantize_config,\r\n    106     model_basename=model_basename,\r\
          \n    107     use_safetensors=use_safetensors,\r\n    108     trust_remote_code=trust_remote_code,\r\
          \n    109     warmup_triton=warmup_triton,\r\n    110     trainable=trainable,\r\
          \n    111     **keywords\r\n    112 )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:793,\
          \ in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir,\
          \ device_map, max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype,\
          \ inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config,\
          \ model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable,\
          \ **kwargs)\r\n    790 if low_cpu_mem_usage:\r\n    791     make_sure_no_tensor_in_meta_device(model,\
          \ use_triton, quantize_config.desc_act, quantize_config.group_size)\r\n\
          --> 793 accelerate.utils.modeling.load_checkpoint_in_model(\r\n    794 \
          \    model,\r\n    795     checkpoint=model_save_name,\r\n    796     device_map=device_map,\r\
          \n    797     offload_state_dict=True,\r\n    798     offload_buffers=True\r\
          \n    799 )\r\n    800 model = simple_dispatch_model(model, device_map)\r\
          \n    802 # == step4: set seqlen == #\r\n\r\nAttributeError: module 'accelerate.utils'\
          \ has no attribute 'modeling'"
        updatedAt: '2023-07-27T11:24:35.249Z'
      numEdits: 0
      reactions: []
    id: 64c253f334e7ac2f2ea390cc
    type: comment
  author: Dhairye
  content: "this is my code:\r\nfrom transformers import AutoTokenizer, pipeline,\
    \ logging\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\n\
    model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\r\nmodel_basename = \"\
    gptq_model-4bit-128g\"\r\n\r\nuse_triton = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n        model_basename=model_basename,\r\n        use_safetensors=True,\r\n\
    \        trust_remote_code=True,\r\n        device=\"cuda:0\",\r\n        use_triton=use_triton,\r\
    \n        quantize_config=None)\r\n\r\nprompt = \"Tell me about AI\"\r\nsystem_message\
    \ = \"You are a helpful, respectful and honest assistant. Always answer as helpfully\
    \ as possible, while being safe.  Your answers should not include any harmful,\
    \ unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure\
    \ that your responses are socially unbiased and positive in nature. If a question\
    \ does not make any sense, or is not factually coherent, explain why instead of\
    \ answering something not correct. If you don't know the answer to a question,\
    \ please don't share false information.\"\r\nprompt_template=f'''[INST] <<SYS>>\r\
    \n{system_message}\r\n<</SYS>>\r\n\r\n{prompt} [/INST]'''\r\n\r\nprint(\"\\n\\\
    n*** Generate:\")\r\n\r\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\
    \noutput = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\r\
    \nprint(tokenizer.decode(output[0]))\r\n\r\n# Inference can also be done using\
    \ transformers' pipeline\r\n\r\n# Prevent printing spurious transformers error\
    \ when using pipeline with AutoGPTQ\r\nlogging.set_verbosity(logging.CRITICAL)\r\
    \n\r\nprint(\"*** Pipeline:\")\r\npipe = pipeline(\r\n    \"text-generation\"\
    ,\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n    max_new_tokens=512,\r\
    \n    temperature=0.7,\r\n    top_p=0.95,\r\n    repetition_penalty=1.15\r\n)\r\
    \n\r\nprint(pipe(prompt_template)[0]['generated_text'])\r\n\r\ngives me the following\
    \ error:\r\nAttributeError                            Traceback (most recent call\
    \ last)\r\nCell In[23], line 13\r\n      9 use_triton = False\r\n     11 tokenizer\
    \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\n---> 13\
    \ model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\n     14  \
    \       model_basename=model_basename,\r\n     15         use_safetensors=True,\r\
    \n     16         trust_remote_code=True,\r\n     17         device=\"cuda:0\"\
    ,\r\n     18         use_triton=use_triton,\r\n     19         quantize_config=None)\r\
    \n     21 \"\"\"\r\n     22 To download from a specific branch, use the revision\
    \ parameter, as in this example:\r\n     23 \r\n   (...)\r\n     30         quantize_config=None)\r\
    \n     31 \"\"\"\r\n     33 prompt = \"Tell me about AI\"\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:94,\
    \ in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir, device_map,\
    \ max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention, inject_fused_mlp,\
    \ use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code,\
    \ warmup_triton, trainable, **kwargs)\r\n     88 quant_func = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized\r\
    \n     89 keywords = {\r\n     90     key: kwargs[key]\r\n     91     for key\
    \ in signature(quant_func).parameters\r\n     92     if key in kwargs\r\n    \
    \ 93 }\r\n---> 94 return quant_func(\r\n     95     model_name_or_path=model_name_or_path,\r\
    \n     96     save_dir=save_dir,\r\n     97     device_map=device_map,\r\n   \
    \  98     max_memory=max_memory,\r\n     99     device=device,\r\n    100    \
    \ low_cpu_mem_usage=low_cpu_mem_usage,\r\n    101     use_triton=use_triton,\r\
    \n    102     inject_fused_attention=inject_fused_attention,\r\n    103     inject_fused_mlp=inject_fused_mlp,\r\
    \n    104     use_cuda_fp16=use_cuda_fp16,\r\n    105     quantize_config=quantize_config,\r\
    \n    106     model_basename=model_basename,\r\n    107     use_safetensors=use_safetensors,\r\
    \n    108     trust_remote_code=trust_remote_code,\r\n    109     warmup_triton=warmup_triton,\r\
    \n    110     trainable=trainable,\r\n    111     **keywords\r\n    112 )\r\n\r\
    \nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:793,\
    \ in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir, device_map,\
    \ max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention,\
    \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
    \ trust_remote_code, warmup_triton, trainable, **kwargs)\r\n    790 if low_cpu_mem_usage:\r\
    \n    791     make_sure_no_tensor_in_meta_device(model, use_triton, quantize_config.desc_act,\
    \ quantize_config.group_size)\r\n--> 793 accelerate.utils.modeling.load_checkpoint_in_model(\r\
    \n    794     model,\r\n    795     checkpoint=model_save_name,\r\n    796   \
    \  device_map=device_map,\r\n    797     offload_state_dict=True,\r\n    798 \
    \    offload_buffers=True\r\n    799 )\r\n    800 model = simple_dispatch_model(model,\
    \ device_map)\r\n    802 # == step4: set seqlen == #\r\n\r\nAttributeError: module\
    \ 'accelerate.utils' has no attribute 'modeling'"
  created_at: 2023-07-27 10:24:35+00:00
  edited: false
  hidden: false
  id: 64c253f334e7ac2f2ea390cc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: 'Getting an error: AttributeError: module ''accelerate.utils'' has no attribute
  ''modeling''. Please tell me what should i do?'
