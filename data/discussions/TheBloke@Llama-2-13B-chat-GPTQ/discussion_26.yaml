!!python/object:huggingface_hub.community.DiscussionWithDetails
author: goodromka
conflicting_files: null
created_at: 2023-08-02 16:04:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: goodromka
      type: user
    createdAt: '2023-08-02T17:04:28.000Z'
    data:
      edited: false
      editors:
      - goodromka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.836637556552887
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: goodromka
          type: user
        html: '<p>I fine-tuned llama-2 on my dataset and now I want to convert it
          to the gptq_model-4bit-128g.safetensors format. Could you please tell me
          how I can do this? What script or method can I use to achieve this?</p>

          '
        raw: I fine-tuned llama-2 on my dataset and now I want to convert it to the
          gptq_model-4bit-128g.safetensors format. Could you please tell me how I
          can do this? What script or method can I use to achieve this?
        updatedAt: '2023-08-02T17:04:28.955Z'
      numEdits: 0
      reactions: []
    id: 64ca8c9c4dcdaead7ae5f4cf
    type: comment
  author: goodromka
  content: I fine-tuned llama-2 on my dataset and now I want to convert it to the
    gptq_model-4bit-128g.safetensors format. Could you please tell me how I can do
    this? What script or method can I use to achieve this?
  created_at: 2023-08-02 16:04:28+00:00
  edited: false
  hidden: false
  id: 64ca8c9c4dcdaead7ae5f4cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-02T17:09:11.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6583670973777771
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Install AutoGPTQ 0.3.2, which I recommend you do from source due
          to some install issues at the moment:</p>

          <pre><code>pip3 uninstall -y auto-gptq

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip3 install .

          </code></pre>

          <p>Then here''s an AutoGPTQ wrapper script I''ve written, and which I use
          myself to make these models:  <a rel="nofollow" href="https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py">https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py</a></p>

          <p>Example execution:</p>

          <pre><code>python3 quant_autogptq.py /path.to/unquantised-model /path/to/save/gptq
          wikitext --bits 4 --group_size 128 --desc_act 0 --damp 0.1 --dtype float16
          --seqlen 4096 --num_samples 128 --use_fast

          </code></pre>

          <p>The example command will use the wikitext dataset for quantisation. If
          your model is trained on something more specific, like code, or non-English
          language, then you may want to change to a different dataset. Doing that
          would require editing <code>quant_autogptq.py</code> to load an alternative
          dataset.</p>

          '
        raw: 'Install AutoGPTQ 0.3.2, which I recommend you do from source due to
          some install issues at the moment:

          ```

          pip3 uninstall -y auto-gptq

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip3 install .

          ```


          Then here''s an AutoGPTQ wrapper script I''ve written, and which I use myself
          to make these models:  https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py


          Example execution:

          ```

          python3 quant_autogptq.py /path.to/unquantised-model /path/to/save/gptq
          wikitext --bits 4 --group_size 128 --desc_act 0 --damp 0.1 --dtype float16
          --seqlen 4096 --num_samples 128 --use_fast

          ```


          The example command will use the wikitext dataset for quantisation. If your
          model is trained on something more specific, like code, or non-English language,
          then you may want to change to a different dataset. Doing that would require
          editing `quant_autogptq.py` to load an alternative dataset.'
        updatedAt: '2023-08-02T17:09:11.933Z'
      numEdits: 0
      reactions: []
    id: 64ca8db71af278541d4a53dd
    type: comment
  author: TheBloke
  content: 'Install AutoGPTQ 0.3.2, which I recommend you do from source due to some
    install issues at the moment:

    ```

    pip3 uninstall -y auto-gptq

    git clone https://github.com/PanQiWei/AutoGPTQ

    cd AutoGPTQ

    pip3 install .

    ```


    Then here''s an AutoGPTQ wrapper script I''ve written, and which I use myself
    to make these models:  https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py


    Example execution:

    ```

    python3 quant_autogptq.py /path.to/unquantised-model /path/to/save/gptq wikitext
    --bits 4 --group_size 128 --desc_act 0 --damp 0.1 --dtype float16 --seqlen 4096
    --num_samples 128 --use_fast

    ```


    The example command will use the wikitext dataset for quantisation. If your model
    is trained on something more specific, like code, or non-English language, then
    you may want to change to a different dataset. Doing that would require editing
    `quant_autogptq.py` to load an alternative dataset.'
  created_at: 2023-08-02 16:09:11+00:00
  edited: false
  hidden: false
  id: 64ca8db71af278541d4a53dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: goodromka
      type: user
    createdAt: '2023-08-02T17:52:18.000Z'
    data:
      edited: false
      editors:
      - goodromka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9218074083328247
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: goodromka
          type: user
        html: '<p>First of all, I want to thank you for the quick and detailed response.
          Secondly, I want to thank you for your work; you make an invaluable contribution
          to the community.</p>

          <p>In this message, I wanted to find out how I can convert my model from
          the hf or q4_0.bin format (for example) to the gptq_model-4bit-128g.safetensors
          format. Could you please advise me on how I can do this? Thank you very
          much for your attention.</p>

          '
        raw: 'First of all, I want to thank you for the quick and detailed response.
          Secondly, I want to thank you for your work; you make an invaluable contribution
          to the community.


          In this message, I wanted to find out how I can convert my model from the
          hf or q4_0.bin format (for example) to the gptq_model-4bit-128g.safetensors
          format. Could you please advise me on how I can do this? Thank you very
          much for your attention.'
        updatedAt: '2023-08-02T17:52:18.357Z'
      numEdits: 0
      reactions: []
    id: 64ca97d271a7bbb60c04a70b
    type: comment
  author: goodromka
  content: 'First of all, I want to thank you for the quick and detailed response.
    Secondly, I want to thank you for your work; you make an invaluable contribution
    to the community.


    In this message, I wanted to find out how I can convert my model from the hf or
    q4_0.bin format (for example) to the gptq_model-4bit-128g.safetensors format.
    Could you please advise me on how I can do this? Thank you very much for your
    attention.'
  created_at: 2023-08-02 16:52:18+00:00
  edited: false
  hidden: false
  id: 64ca97d271a7bbb60c04a70b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-02T19:06:44.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You''re welcome.</p>

          <p>I already described how to convert from HF to GPTQ.  To convert HF to
          GGML, use this script:  <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py">https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py</a></p>

          '
        raw: 'You''re welcome.


          I already described how to convert from HF to GPTQ.  To convert HF to GGML,
          use this script:  https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py'
        updatedAt: '2023-08-02T19:06:44.758Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - goodromka
    id: 64caa944214a472dd81e030a
    type: comment
  author: TheBloke
  content: 'You''re welcome.


    I already described how to convert from HF to GPTQ.  To convert HF to GGML, use
    this script:  https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py'
  created_at: 2023-08-02 18:06:44+00:00
  edited: false
  hidden: false
  id: 64caa944214a472dd81e030a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: goodromka
      type: user
    createdAt: '2023-08-03T07:28:17.000Z'
    data:
      edited: false
      editors:
      - goodromka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9395455718040466
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: goodromka
          type: user
        html: '<p>Great approach, thanks. So, the main part of my question is how
          to convert my model to the format used in this repository, which means having
          the model with ".safetensors" at the end and the other accompanying files.
          Could you please guide me on how to do this?</p>

          '
        raw: Great approach, thanks. So, the main part of my question is how to convert
          my model to the format used in this repository, which means having the model
          with ".safetensors" at the end and the other accompanying files. Could you
          please guide me on how to do this?
        updatedAt: '2023-08-03T07:28:17.841Z'
      numEdits: 0
      reactions: []
    id: 64cb5711e984d09bed98458a
    type: comment
  author: goodromka
  content: Great approach, thanks. So, the main part of my question is how to convert
    my model to the format used in this repository, which means having the model with
    ".safetensors" at the end and the other accompanying files. Could you please guide
    me on how to do this?
  created_at: 2023-08-03 06:28:17+00:00
  edited: false
  hidden: false
  id: 64cb5711e984d09bed98458a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-03T07:32:44.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7785330414772034
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I already described that in detail here: <a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ/discussions/26#64ca8db71af278541d4a53dd">https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ/discussions/26#64ca8db71af278541d4a53dd</a></p>

          '
        raw: 'I already described that in detail here: https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ/discussions/26#64ca8db71af278541d4a53dd'
        updatedAt: '2023-08-03T07:32:44.654Z'
      numEdits: 0
      reactions: []
    id: 64cb581c5aa1ab065c15be2b
    type: comment
  author: TheBloke
  content: 'I already described that in detail here: https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ/discussions/26#64ca8db71af278541d4a53dd'
  created_at: 2023-08-03 06:32:44+00:00
  edited: false
  hidden: false
  id: 64cb581c5aa1ab065c15be2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/51a8fecfd7d35ca6ad4bcbdb7cf1442e.svg
      fullname: Shamsuddoha Ranju
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sdranju
      type: user
    createdAt: '2023-09-11T06:19:08.000Z'
    data:
      edited: false
      editors:
      - sdranju
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.908574104309082
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/51a8fecfd7d35ca6ad4bcbdb7cf1442e.svg
          fullname: Shamsuddoha Ranju
          isHf: false
          isPro: false
          name: sdranju
          type: user
        html: '<p>Hey pal, is it possible to fine-tune a 4-bit GPTQ model?<br>My GPU
          has limited memory. I''m really unable to fine-tune the original HF model.</p>

          <p>Sorry for hijacking the thread.</p>

          '
        raw: "Hey pal, is it possible to fine-tune a 4-bit GPTQ model? \nMy GPU has\
          \ limited memory. I'm really unable to fine-tune the original HF model.\n\
          \nSorry for hijacking the thread.\n"
        updatedAt: '2023-09-11T06:19:08.676Z'
      numEdits: 0
      reactions: []
    id: 64feb15c67a8befb5c564b4c
    type: comment
  author: sdranju
  content: "Hey pal, is it possible to fine-tune a 4-bit GPTQ model? \nMy GPU has\
    \ limited memory. I'm really unable to fine-tune the original HF model.\n\nSorry\
    \ for hijacking the thread.\n"
  created_at: 2023-09-11 05:19:08+00:00
  edited: false
  hidden: false
  id: 64feb15c67a8befb5c564b4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5615d5dcfb60d24b9d1a0c3f129f7aa5.svg
      fullname: Michael May
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mayzyo
      type: user
    createdAt: '2023-09-24T00:33:23.000Z'
    data:
      edited: false
      editors:
      - mayzyo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8199779391288757
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5615d5dcfb60d24b9d1a0c3f129f7aa5.svg
          fullname: Michael May
          isHf: false
          isPro: false
          name: mayzyo
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Hi, I followed\
          \ your instructions for converting my Llama-2-hf model to a 4bit 128 group\
          \ quantised model through the script you posted <a rel=\"nofollow\" href=\"\
          https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py\"\
          >https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py</a>\
          \ it worked fantastically. But when I try to load the model in oobabooga/text-generation-webui\
          \ I get the following error:</p>\n<p><code>OSError: Can\u2019t load tokenizer\
          \ for \u2018models/llama-2-7b-hf-GPTQ-4bit-128g\u2019. If you were trying\
          \ to load it from \u2018https://huggingface.co/models\u2019, make sure you\
          \ don\u2019t have a local directory with the same name. Otherwise, make\
          \ sure \u2018models/llama-2-7b-hf-GPTQ-4bit-128g\u2019 is the correct path\
          \ to a directory containing all relevant files for a LlamaTokenizer tokenizer.</code></p>\n\
          <p>Any help would be much appreciated. Just point me in the right direction\
          \ :D I am having a hard time googling what the problem is...</p>\n"
        raw: "@TheBloke Hi, I followed your instructions for converting my Llama-2-hf\
          \ model to a 4bit 128 group quantised model through the script you posted\
          \ https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py\
          \ it worked fantastically. But when I try to load the model in oobabooga/text-generation-webui\
          \ I get the following error:\n\n`OSError: Can\u2019t load tokenizer for\
          \ \u2018models/llama-2-7b-hf-GPTQ-4bit-128g\u2019. If you were trying to\
          \ load it from \u2018https://huggingface.co/models\u2019, make sure you\
          \ don\u2019t have a local directory with the same name. Otherwise, make\
          \ sure \u2018models/llama-2-7b-hf-GPTQ-4bit-128g\u2019 is the correct path\
          \ to a directory containing all relevant files for a LlamaTokenizer tokenizer.`\n\
          \nAny help would be much appreciated. Just point me in the right direction\
          \ :D I am having a hard time googling what the problem is..."
        updatedAt: '2023-09-24T00:33:23.212Z'
      numEdits: 0
      reactions: []
    id: 650f83d3f874d950df43bc5a
    type: comment
  author: mayzyo
  content: "@TheBloke Hi, I followed your instructions for converting my Llama-2-hf\
    \ model to a 4bit 128 group quantised model through the script you posted https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py\
    \ it worked fantastically. But when I try to load the model in oobabooga/text-generation-webui\
    \ I get the following error:\n\n`OSError: Can\u2019t load tokenizer for \u2018\
    models/llama-2-7b-hf-GPTQ-4bit-128g\u2019. If you were trying to load it from\
    \ \u2018https://huggingface.co/models\u2019, make sure you don\u2019t have a local\
    \ directory with the same name. Otherwise, make sure \u2018models/llama-2-7b-hf-GPTQ-4bit-128g\u2019\
    \ is the correct path to a directory containing all relevant files for a LlamaTokenizer\
    \ tokenizer.`\n\nAny help would be much appreciated. Just point me in the right\
    \ direction :D I am having a hard time googling what the problem is..."
  created_at: 2023-09-23 23:33:23+00:00
  edited: false
  hidden: false
  id: 650f83d3f874d950df43bc5a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 26
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Converting hf format model to 128g.safetensors
