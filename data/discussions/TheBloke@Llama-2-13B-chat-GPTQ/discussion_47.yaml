!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ditchtech
conflicting_files: null
created_at: 2023-11-04 05:11:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a8189ce487d1859871a544678cc4872.svg
      fullname: AGomes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ditchtech
      type: user
    createdAt: '2023-11-04T06:11:03.000Z'
    data:
      edited: true
      editors:
      - ditchtech
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5370990633964539
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a8189ce487d1859871a544678cc4872.svg
          fullname: AGomes
          isHf: false
          isPro: false
          name: ditchtech
          type: user
        html: '<p>Hello Bloke,<br>While running a sample application, I receive the
          following error -<br>CUDA extension not installed.<br>exllama_kernels not
          installed.</p>

          <p>Pytorch Cuda versions - pytorch:2.0.1-py3.10-cuda11.8.0<br>Installed
          autogptq from source <a rel="nofollow" href="https://huggingface.github.io/autogptq-index/whl/cu118/">https://huggingface.github.io/autogptq-index/whl/cu118/</a>
          </p>

          <p>Yet I get following error trace -<br>    from auto_gptq import AutoGPTQForCausalLM<br>  File
          "/usr/local/lib/python3.10/dist-packages/auto_gptq/<strong>init</strong>.py",
          line 4, in <br>    from .utils.peft_utils import get_gptq_peft_model<br>  File
          "/usr/local/lib/python3.10/dist-packages/auto_gptq/utils/peft_utils.py",
          line 20, in <br>    from ..nn_modules.qlinear.qlinear_exllama import QuantLinear
          as QuantLinearExllama<br>  File "/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py",
          line 14, in <br>    from exllama_kernels import make_q4, q4_matmul<br>ImportError:
          /usr/local/lib/python3.10/dist-packages/exllama_kernels.cpython-310-x86_64-linux-gnu.so:
          undefined symbol: _ZN3c104cuda9SetDeviceEi<br>exllama_kernels not installed</p>

          <p>This was not happening before.  I noticed the autogptq package updates
          on 2nd Nov.  Does that have a bearing?</p>

          '
        raw: "Hello Bloke,\nWhile running a sample application, I receive the following\
          \ error -\nCUDA extension not installed.\nexllama_kernels not installed.\n\
          \nPytorch Cuda versions - pytorch:2.0.1-py3.10-cuda11.8.0\nInstalled autogptq\
          \ from source https://huggingface.github.io/autogptq-index/whl/cu118/ \n\
          \nYet I get following error trace -\n    from auto_gptq import AutoGPTQForCausalLM\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/__init__.py\"\
          , line 4, in <module>\n    from .utils.peft_utils import get_gptq_peft_model\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/utils/peft_utils.py\"\
          , line 20, in <module>\n    from ..nn_modules.qlinear.qlinear_exllama import\
          \ QuantLinear as QuantLinearExllama\n  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py\"\
          , line 14, in <module>\n    from exllama_kernels import make_q4, q4_matmul\n\
          ImportError: /usr/local/lib/python3.10/dist-packages/exllama_kernels.cpython-310-x86_64-linux-gnu.so:\
          \ undefined symbol: _ZN3c104cuda9SetDeviceEi\nexllama_kernels not installed\n\
          \nThis was not happening before.  I noticed the autogptq package updates\
          \ on 2nd Nov.  Does that have a bearing?"
        updatedAt: '2023-11-04T06:16:06.449Z'
      numEdits: 1
      reactions: []
    id: 6545e0772119c8bdf24493ae
    type: comment
  author: ditchtech
  content: "Hello Bloke,\nWhile running a sample application, I receive the following\
    \ error -\nCUDA extension not installed.\nexllama_kernels not installed.\n\nPytorch\
    \ Cuda versions - pytorch:2.0.1-py3.10-cuda11.8.0\nInstalled autogptq from source\
    \ https://huggingface.github.io/autogptq-index/whl/cu118/ \n\nYet I get following\
    \ error trace -\n    from auto_gptq import AutoGPTQForCausalLM\n  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/__init__.py\"\
    , line 4, in <module>\n    from .utils.peft_utils import get_gptq_peft_model\n\
    \  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/utils/peft_utils.py\"\
    , line 20, in <module>\n    from ..nn_modules.qlinear.qlinear_exllama import QuantLinear\
    \ as QuantLinearExllama\n  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py\"\
    , line 14, in <module>\n    from exllama_kernels import make_q4, q4_matmul\nImportError:\
    \ /usr/local/lib/python3.10/dist-packages/exllama_kernels.cpython-310-x86_64-linux-gnu.so:\
    \ undefined symbol: _ZN3c104cuda9SetDeviceEi\nexllama_kernels not installed\n\n\
    This was not happening before.  I noticed the autogptq package updates on 2nd\
    \ Nov.  Does that have a bearing?"
  created_at: 2023-11-04 05:11:03+00:00
  edited: true
  hidden: false
  id: 6545e0772119c8bdf24493ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/550287c9dfdd697100d6dee19070a1d6.svg
      fullname: Shivji Agnihotri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShivjiAgnihotri
      type: user
    createdAt: '2023-11-07T08:06:55.000Z'
    data:
      edited: false
      editors:
      - ShivjiAgnihotri
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3179444372653961
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/550287c9dfdd697100d6dee19070a1d6.svg
          fullname: Shivji Agnihotri
          isHf: false
          isPro: false
          name: ShivjiAgnihotri
          type: user
        html: '<p>Having the same issue</p>

          <p>I installed the cuda toolkits first using this which was required in
          my case. It removed some errors:     !sudo apt install -q nvidia-cuda-toolkit</p>

          <p>Later I left with One error : Which basically says exllama_kernals are
          not installed</p>

          <hr>

          <hr>

          <h2 id="errorauto_gptqnn_modulesqlinearqlinear_exllamaexllama_kernels-not-installed">ERROR:auto_gptq.nn_modules.qlinear.qlinear_exllama:exllama_kernels
          not installed.</h2>

          <p>ImportError                               Traceback (most recent call
          last)<br> in &lt;cell line: 6&gt;()<br>      4 # To use a different branch,
          change revision<br>      5 # For example: revision="gptq-4bit-32g-actorder_True"<br>----&gt;
          6 model = AutoModelForCausalLM.from_pretrained(model_name_or_path,<br>      7                                              device_map="auto",<br>      8                                              trust_remote_code=False,</p>

          <p>6 frames<br>/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py
          in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)<br>    564         elif
          type(config) in cls._model_mapping.keys():<br>    565             model_class
          = _get_model_class(config, cls._model_mapping)<br>--&gt; 566             return
          model_class.from_pretrained(<br>    567                 pretrained_model_name_or_path,
          *model_args, config=config, **hub_kwargs, **kwargs<br>    568             )</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py
          in from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir,
          ignore_mismatched_sizes, force_download, local_files_only, token, revision,
          use_safetensors, *model_args, **kwargs)<br>   2810             else:<br>   2811                 #
          Need to protect the import<br>-&gt; 2812                 from optimum.gptq
          import GPTQQuantizer<br>   2813             if quantization_method_from_config
          == QuantizationMethod.GPTQ:<br>   2814                 quantization_config
          = GPTQConfig.from_dict(config.quantization_config)</p>

          <p>/usr/local/lib/python3.10/dist-packages/optimum/gptq/<strong>init</strong>.py
          in <br>     13 # See the License for the specific language governing permissions
          and<br>     14 # limitations under the License.<br>---&gt; 15 from .quantizer
          import GPTQQuantizer, load_quantized_model</p>

          <p>/usr/local/lib/python3.10/dist-packages/optimum/gptq/quantizer.py in
          <br>     43<br>     44 if is_auto_gptq_available():<br>---&gt; 45     from
          auto_gptq import exllama_set_max_input_length<br>     46     from auto_gptq.modeling._utils
          import autogptq_post_init<br>     47     from auto_gptq.quantization import
          GPTQ</p>

          <p>/usr/local/lib/python3.10/dist-packages/auto_gptq/<strong>init</strong>.py
          in <br>      2 from .modeling import BaseQuantizeConfig<br>      3 from
          .modeling import AutoGPTQForCausalLM<br>----&gt; 4 from .utils.peft_utils
          import get_gptq_peft_model<br>      5 from .utils.exllama_utils import exllama_set_max_input_length</p>

          <p>/usr/local/lib/python3.10/dist-packages/auto_gptq/utils/peft_utils.py
          in <br>     18 from ..nn_modules.qlinear.qlinear_cuda import QuantLinear
          as QuantLinearCuda<br>     19 from ..nn_modules.qlinear.qlinear_cuda_old
          import QuantLinear as QuantLinearCudaOld<br>---&gt; 20 from ..nn_modules.qlinear.qlinear_exllama
          import QuantLinear as QuantLinearExllama<br>     21 from ..nn_modules.qlinear.qlinear_qigen
          import QuantLinear as QuantLinearQigen<br>     22 from ..nn_modules.qlinear.qlinear_triton
          import QuantLinear as QuantLinearTriton</p>

          <p>/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py
          in <br>     12<br>     13 try:<br>---&gt; 14     from exllama_kernels import
          make_q4, q4_matmul<br>     15 except ImportError:<br>     16     logger.error(''exllama_kernels
          not installed.'')</p>

          <h2 id="importerror-libcudartso12-cannot-open-shared-object-file-no-such-file-or-directory">ImportError:
          libcudart.so.12: cannot open shared object file: No such file or directory</h2>

          <hr>

          <p>I was using this code :</p>

          <hr>

          <hr>

          <p>from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline</p>

          <p>model_name_or_path = "TheBloke/Mistral-7B-Instruct-v0.1-GPTQ"</p>

          <h1 id="to-use-a-different-branch-change-revision">To use a different branch,
          change revision</h1>

          <h1 id="for-example-revisiongptq-4bit-32g-actorder_true">For example: revision="gptq-4bit-32g-actorder_True"</h1>

          <p>model = AutoModelForCausalLM.from_pretrained(model_name_or_path,<br>                                             device_map="auto",<br>                                             trust_remote_code=False,<br>                                             revision="gptq-8bit-32g-actorder_True")<br>#<br>tokenizer
          = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)</p>

          <hr>

          <hr>

          '
        raw: "Having the same issue\n\nI installed the cuda toolkits first using this\
          \ which was required in my case. It removed some errors:     !sudo apt install\
          \ -q nvidia-cuda-toolkit\n\nLater I left with One error : Which basically\
          \ says exllama_kernals are not installed\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
          -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
          ERROR:auto_gptq.nn_modules.qlinear.qlinear_exllama:exllama_kernels not installed.\n\
          ---------------------------------------------------------------------------\n\
          ImportError                               Traceback (most recent call last)\n\
          <ipython-input-24-2b74e07dfb23> in <cell line: 6>()\n      4 # To use a\
          \ different branch, change revision\n      5 # For example: revision=\"\
          gptq-4bit-32g-actorder_True\"\n----> 6 model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \      7                                              device_map=\"auto\"\
          ,\n      8                                              trust_remote_code=False,\n\
          \n6 frames\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n\
          \    564         elif type(config) in cls._model_mapping.keys():\n    565\
          \             model_class = _get_model_class(config, cls._model_mapping)\n\
          --> 566             return model_class.from_pretrained(\n    567       \
          \          pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
          \ **kwargs\n    568             )\n\n/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir,\
          \ ignore_mismatched_sizes, force_download, local_files_only, token, revision,\
          \ use_safetensors, *model_args, **kwargs)\n   2810             else:\n \
          \  2811                 # Need to protect the import\n-> 2812          \
          \       from optimum.gptq import GPTQQuantizer\n   2813             if quantization_method_from_config\
          \ == QuantizationMethod.GPTQ:\n   2814                 quantization_config\
          \ = GPTQConfig.from_dict(config.quantization_config)\n\n/usr/local/lib/python3.10/dist-packages/optimum/gptq/__init__.py\
          \ in <module>\n     13 # See the License for the specific language governing\
          \ permissions and\n     14 # limitations under the License.\n---> 15 from\
          \ .quantizer import GPTQQuantizer, load_quantized_model\n\n/usr/local/lib/python3.10/dist-packages/optimum/gptq/quantizer.py\
          \ in <module>\n     43 \n     44 if is_auto_gptq_available():\n---> 45 \
          \    from auto_gptq import exllama_set_max_input_length\n     46     from\
          \ auto_gptq.modeling._utils import autogptq_post_init\n     47     from\
          \ auto_gptq.quantization import GPTQ\n\n/usr/local/lib/python3.10/dist-packages/auto_gptq/__init__.py\
          \ in <module>\n      2 from .modeling import BaseQuantizeConfig\n      3\
          \ from .modeling import AutoGPTQForCausalLM\n----> 4 from .utils.peft_utils\
          \ import get_gptq_peft_model\n      5 from .utils.exllama_utils import exllama_set_max_input_length\n\
          \n/usr/local/lib/python3.10/dist-packages/auto_gptq/utils/peft_utils.py\
          \ in <module>\n     18 from ..nn_modules.qlinear.qlinear_cuda import QuantLinear\
          \ as QuantLinearCuda\n     19 from ..nn_modules.qlinear.qlinear_cuda_old\
          \ import QuantLinear as QuantLinearCudaOld\n---> 20 from ..nn_modules.qlinear.qlinear_exllama\
          \ import QuantLinear as QuantLinearExllama\n     21 from ..nn_modules.qlinear.qlinear_qigen\
          \ import QuantLinear as QuantLinearQigen\n     22 from ..nn_modules.qlinear.qlinear_triton\
          \ import QuantLinear as QuantLinearTriton\n\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py\
          \ in <module>\n     12 \n     13 try:\n---> 14     from exllama_kernels\
          \ import make_q4, q4_matmul\n     15 except ImportError:\n     16     logger.error('exllama_kernels\
          \ not installed.')\n\nImportError: libcudart.so.12: cannot open shared object\
          \ file: No such file or directory\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
          -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
          \nI was using this code :\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
          -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
          from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\
          \nmodel_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n# To\
          \ use a different branch, change revision\n# For example: revision=\"gptq-4bit-32g-actorder_True\"\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n   \
          \                                          device_map=\"auto\",\n      \
          \                                       trust_remote_code=False,\n     \
          \                                        revision=\"gptq-8bit-32g-actorder_True\"\
          )\n#\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
          -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
          -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
        updatedAt: '2023-11-07T08:06:55.157Z'
      numEdits: 0
      reactions: []
    id: 6549f01ff6a1a302357175b5
    type: comment
  author: ShivjiAgnihotri
  content: "Having the same issue\n\nI installed the cuda toolkits first using this\
    \ which was required in my case. It removed some errors:     !sudo apt install\
    \ -q nvidia-cuda-toolkit\n\nLater I left with One error : Which basically says\
    \ exllama_kernals are not installed\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
    ERROR:auto_gptq.nn_modules.qlinear.qlinear_exllama:exllama_kernels not installed.\n\
    ---------------------------------------------------------------------------\n\
    ImportError                               Traceback (most recent call last)\n\
    <ipython-input-24-2b74e07dfb23> in <cell line: 6>()\n      4 # To use a different\
    \ branch, change revision\n      5 # For example: revision=\"gptq-4bit-32g-actorder_True\"\
    \n----> 6 model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n \
    \     7                                              device_map=\"auto\",\n  \
    \    8                                              trust_remote_code=False,\n\
    \n6 frames\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n\
    \    564         elif type(config) in cls._model_mapping.keys():\n    565    \
    \         model_class = _get_model_class(config, cls._model_mapping)\n--> 566\
    \             return model_class.from_pretrained(\n    567                 pretrained_model_name_or_path,\
    \ *model_args, config=config, **hub_kwargs, **kwargs\n    568             )\n\n\
    /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py in from_pretrained(cls,\
    \ pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download,\
    \ local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\n\
    \   2810             else:\n   2811                 # Need to protect the import\n\
    -> 2812                 from optimum.gptq import GPTQQuantizer\n   2813      \
    \       if quantization_method_from_config == QuantizationMethod.GPTQ:\n   2814\
    \                 quantization_config = GPTQConfig.from_dict(config.quantization_config)\n\
    \n/usr/local/lib/python3.10/dist-packages/optimum/gptq/__init__.py in <module>\n\
    \     13 # See the License for the specific language governing permissions and\n\
    \     14 # limitations under the License.\n---> 15 from .quantizer import GPTQQuantizer,\
    \ load_quantized_model\n\n/usr/local/lib/python3.10/dist-packages/optimum/gptq/quantizer.py\
    \ in <module>\n     43 \n     44 if is_auto_gptq_available():\n---> 45     from\
    \ auto_gptq import exllama_set_max_input_length\n     46     from auto_gptq.modeling._utils\
    \ import autogptq_post_init\n     47     from auto_gptq.quantization import GPTQ\n\
    \n/usr/local/lib/python3.10/dist-packages/auto_gptq/__init__.py in <module>\n\
    \      2 from .modeling import BaseQuantizeConfig\n      3 from .modeling import\
    \ AutoGPTQForCausalLM\n----> 4 from .utils.peft_utils import get_gptq_peft_model\n\
    \      5 from .utils.exllama_utils import exllama_set_max_input_length\n\n/usr/local/lib/python3.10/dist-packages/auto_gptq/utils/peft_utils.py\
    \ in <module>\n     18 from ..nn_modules.qlinear.qlinear_cuda import QuantLinear\
    \ as QuantLinearCuda\n     19 from ..nn_modules.qlinear.qlinear_cuda_old import\
    \ QuantLinear as QuantLinearCudaOld\n---> 20 from ..nn_modules.qlinear.qlinear_exllama\
    \ import QuantLinear as QuantLinearExllama\n     21 from ..nn_modules.qlinear.qlinear_qigen\
    \ import QuantLinear as QuantLinearQigen\n     22 from ..nn_modules.qlinear.qlinear_triton\
    \ import QuantLinear as QuantLinearTriton\n\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py\
    \ in <module>\n     12 \n     13 try:\n---> 14     from exllama_kernels import\
    \ make_q4, q4_matmul\n     15 except ImportError:\n     16     logger.error('exllama_kernels\
    \ not installed.')\n\nImportError: libcudart.so.12: cannot open shared object\
    \ file: No such file or directory\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
    \nI was using this code :\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path\
    \ = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n# To use a different branch, change\
    \ revision\n# For example: revision=\"gptq-4bit-32g-actorder_True\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
    \                                             device_map=\"auto\",\n         \
    \                                    trust_remote_code=False,\n              \
    \                               revision=\"gptq-8bit-32g-actorder_True\")\n#\n\
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
    -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
  created_at: 2023-11-07 08:06:55+00:00
  edited: false
  hidden: false
  id: 6549f01ff6a1a302357175b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/550287c9dfdd697100d6dee19070a1d6.svg
      fullname: Shivji Agnihotri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShivjiAgnihotri
      type: user
    createdAt: '2023-11-07T08:08:54.000Z'
    data:
      edited: false
      editors:
      - ShivjiAgnihotri
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6374019980430603
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/550287c9dfdd697100d6dee19070a1d6.svg
          fullname: Shivji Agnihotri
          isHf: false
          isPro: false
          name: ShivjiAgnihotri
          type: user
        html: '<p>I am Running this on Colab and using Pytorch version 2.1</p>

          '
        raw: I am Running this on Colab and using Pytorch version 2.1
        updatedAt: '2023-11-07T08:08:54.027Z'
      numEdits: 0
      reactions: []
    id: 6549f096a46359771494a6d0
    type: comment
  author: ShivjiAgnihotri
  content: I am Running this on Colab and using Pytorch version 2.1
  created_at: 2023-11-07 08:08:54+00:00
  edited: false
  hidden: false
  id: 6549f096a46359771494a6d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/550287c9dfdd697100d6dee19070a1d6.svg
      fullname: Shivji Agnihotri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShivjiAgnihotri
      type: user
    createdAt: '2023-11-07T08:11:31.000Z'
    data:
      edited: false
      editors:
      - ShivjiAgnihotri
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6016047596931458
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/550287c9dfdd697100d6dee19070a1d6.svg
          fullname: Shivji Agnihotri
          isHf: false
          isPro: false
          name: ShivjiAgnihotri
          type: user
        html: '<p>Even this pipeline is not working and throwing the same error</p>

          <h1 id="load-model-directly">Load model directly</h1>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("TheBloke/Llama-2-13B-chat-GPTQ")<br>model
          = AutoModelForCausalLM.from_pretrained("TheBloke/Llama-2-13B-chat-GPTQ")</p>

          '
        raw: 'Even this pipeline is not working and throwing the same error



          # Load model directly

          from transformers import AutoTokenizer, AutoModelForCausalLM


          tokenizer = AutoTokenizer.from_pretrained("TheBloke/Llama-2-13B-chat-GPTQ")

          model = AutoModelForCausalLM.from_pretrained("TheBloke/Llama-2-13B-chat-GPTQ")'
        updatedAt: '2023-11-07T08:11:31.267Z'
      numEdits: 0
      reactions: []
    id: 6549f1336c87164d305b4d24
    type: comment
  author: ShivjiAgnihotri
  content: 'Even this pipeline is not working and throwing the same error



    # Load model directly

    from transformers import AutoTokenizer, AutoModelForCausalLM


    tokenizer = AutoTokenizer.from_pretrained("TheBloke/Llama-2-13B-chat-GPTQ")

    model = AutoModelForCausalLM.from_pretrained("TheBloke/Llama-2-13B-chat-GPTQ")'
  created_at: 2023-11-07 08:11:31+00:00
  edited: false
  hidden: false
  id: 6549f1336c87164d305b4d24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/660ae6ae72b34714bcf77841fd7dc536.svg
      fullname: Rajendra Baskota
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rajendrabaskota
      type: user
    createdAt: '2023-11-07T09:45:55.000Z'
    data:
      edited: false
      editors:
      - rajendrabaskota
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8144903182983398
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/660ae6ae72b34714bcf77841fd7dc536.svg
          fullname: Rajendra Baskota
          isHf: false
          isPro: false
          name: rajendrabaskota
          type: user
        html: '<p>I''m having the same issue. It was running few days back but now
          it throws the following error: </p>

          <p>ImportError: /opt/conda/lib/python3.10/site-packages/exllama_kernels.cpython-310-x86_64-linux-gnu.so:
          undefined symbol: _ZN3c104cuda9SetDeviceEi</p>

          '
        raw: "I'm having the same issue. It was running few days back but now it throws\
          \ the following error: \n\nImportError: /opt/conda/lib/python3.10/site-packages/exllama_kernels.cpython-310-x86_64-linux-gnu.so:\
          \ undefined symbol: _ZN3c104cuda9SetDeviceEi\n"
        updatedAt: '2023-11-07T09:45:55.235Z'
      numEdits: 0
      reactions: []
    id: 654a07538fde27109bda6b84
    type: comment
  author: rajendrabaskota
  content: "I'm having the same issue. It was running few days back but now it throws\
    \ the following error: \n\nImportError: /opt/conda/lib/python3.10/site-packages/exllama_kernels.cpython-310-x86_64-linux-gnu.so:\
    \ undefined symbol: _ZN3c104cuda9SetDeviceEi\n"
  created_at: 2023-11-07 09:45:55+00:00
  edited: false
  hidden: false
  id: 654a07538fde27109bda6b84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/660ae6ae72b34714bcf77841fd7dc536.svg
      fullname: Rajendra Baskota
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rajendrabaskota
      type: user
    createdAt: '2023-11-07T09:48:27.000Z'
    data:
      edited: false
      editors:
      - rajendrabaskota
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3394450545310974
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/660ae6ae72b34714bcf77841fd7dc536.svg
          fullname: Rajendra Baskota
          isHf: false
          isPro: false
          name: rajendrabaskota
          type: user
        html: '<p>ImportError                               Traceback (most recent
          call last)<br>Cell In[3], line 7<br>      4 model_name_or_path = "TheBloke/Llama-2-13B-chat-GPTQ"<br>      5
          # To use a different branch, change revision<br>      6 # For example: revision="main"<br>----&gt;
          7 model = AutoModelForCausalLM.from_pretrained(model_name_or_path,<br>      8                                              device_map="auto",<br>      9                                              trust_remote_code=False,<br>     10                                              revision="main")<br>     12
          tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)<br>     13
          # tokenizer.pad_token = tokenizer.eos_token</p>

          <p>File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563,
          in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,
          *model_args, **kwargs)<br>    561 elif type(config) in cls._model_mapping.keys():<br>    562     model_class
          = _get_model_class(config, cls._model_mapping)<br>--&gt; 563     return
          model_class.from_pretrained(<br>    564         pretrained_model_name_or_path,
          *model_args, config=config, **hub_kwargs, **kwargs<br>    565     )<br>    566
          raise ValueError(<br>    567     f"Unrecognized configuration class {config.<strong>class</strong>}
          for this kind of AutoModel: {cls.<strong>name</strong>}.\n"<br>    568     f"Model
          type should be one of {'', ''.join(c.<strong>name</strong> for c in cls._model_mapping.keys())}."<br>    569
          )</p>

          <p>File /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2577,
          in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config,
          cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token,
          revision, use_safetensors, *model_args, **kwargs)<br>   2572     raise ImportError(<br>   2573         "Loading
          GPTQ quantized model requires optimum library : <code>pip install optimum</code>
          and auto-gptq library ''pip install auto-gptq''"<br>   2574     )<br>   2575
          else:<br>   2576     # Need to protect the import<br>-&gt; 2577     from
          optimum.gptq import GPTQQuantizer<br>   2578 if quantization_method_from_config
          == QuantizationMethod.GPTQ:<br>   2579     quantization_config = GPTQConfig.from_dict(config.quantization_config)</p>

          <p>File /opt/conda/lib/python3.10/site-packages/optimum/gptq/<strong>init</strong>.py:15<br>      1
          # coding=utf-8<br>      2 # Copyright 2023 HuggingFace Inc. team.<br>      3
          #<br>   (...)<br>     13 # See the License for the specific language governing
          permissions and<br>     14 # limitations under the License.<br>---&gt; 15
          from .quantizer import GPTQQuantizer, load_quantized_model</p>

          <p>File /opt/conda/lib/python3.10/site-packages/optimum/gptq/quantizer.py:45<br>     42     from
          accelerate.hooks import remove_hook_from_module<br>     44 if is_auto_gptq_available():<br>---&gt;
          45     from auto_gptq import exllama_set_max_input_length<br>     46     from
          auto_gptq.modeling._utils import autogptq_post_init<br>     47     from
          auto_gptq.quantization import GPTQ</p>

          <p>File /opt/conda/lib/python3.10/site-packages/auto_gptq/<strong>init</strong>.py:4<br>      2
          from .modeling import BaseQuantizeConfig<br>      3 from .modeling import
          AutoGPTQForCausalLM<br>----&gt; 4 from .utils.peft_utils import get_gptq_peft_model<br>      5
          from .utils.exllama_utils import exllama_set_max_input_length</p>

          <p>File /opt/conda/lib/python3.10/site-packages/auto_gptq/utils/peft_utils.py:20<br>     18
          from ..nn_modules.qlinear.qlinear_cuda import QuantLinear as QuantLinearCuda<br>     19
          from ..nn_modules.qlinear.qlinear_cuda_old import QuantLinear as QuantLinearCudaOld<br>---&gt;
          20 from ..nn_modules.qlinear.qlinear_exllama import QuantLinear as QuantLinearExllama<br>     21
          from ..nn_modules.qlinear.qlinear_qigen import QuantLinear as QuantLinearQigen<br>     22
          from ..nn_modules.qlinear.qlinear_triton import QuantLinear as QuantLinearTriton</p>

          <p>File /opt/conda/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py:14<br>     11
          logger = getLogger(<strong>name</strong>)<br>     13 try:<br>---&gt; 14     from
          exllama_kernels import make_q4, q4_matmul<br>     15 except ImportError:<br>     16     logger.error(''exllama_kernels
          not installed.'')</p>

          <p>ImportError: /opt/conda/lib/python3.10/site-packages/exllama_kernels.cpython-310-x86_64-linux-gnu.so:
          undefined symbol: _ZN3c104cuda9SetDeviceEi</p>

          '
        raw: "ImportError                               Traceback (most recent call\
          \ last)\nCell In[3], line 7\n      4 model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\
          \n      5 # To use a different branch, change revision\n      6 # For example:\
          \ revision=\"main\"\n----> 7 model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \      8                                              device_map=\"auto\"\
          ,\n      9                                              trust_remote_code=False,\n\
          \     10                                              revision=\"main\"\
          )\n     12 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n     13 # tokenizer.pad_token = tokenizer.eos_token\n\n\
          File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    561 elif type(config) in cls._model_mapping.keys():\n\
          \    562     model_class = _get_model_class(config, cls._model_mapping)\n\
          --> 563     return model_class.from_pretrained(\n    564         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    565     )\n  \
          \  566 raise ValueError(\n    567     f\"Unrecognized configuration class\
          \ {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n\
          \    568     f\"Model type should be one of {', '.join(c.__name__ for c\
          \ in cls._model_mapping.keys())}.\"\n    569 )\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2577,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\n   2572    \
          \ raise ImportError(\n   2573         \"Loading GPTQ quantized model requires\
          \ optimum library : `pip install optimum` and auto-gptq library 'pip install\
          \ auto-gptq'\"\n   2574     )\n   2575 else:\n   2576     # Need to protect\
          \ the import\n-> 2577     from optimum.gptq import GPTQQuantizer\n   2578\
          \ if quantization_method_from_config == QuantizationMethod.GPTQ:\n   2579\
          \     quantization_config = GPTQConfig.from_dict(config.quantization_config)\n\
          \nFile /opt/conda/lib/python3.10/site-packages/optimum/gptq/__init__.py:15\n\
          \      1 # coding=utf-8\n      2 # Copyright 2023 HuggingFace Inc. team.\n\
          \      3 #\n   (...)\n     13 # See the License for the specific language\
          \ governing permissions and\n     14 # limitations under the License.\n\
          ---> 15 from .quantizer import GPTQQuantizer, load_quantized_model\n\nFile\
          \ /opt/conda/lib/python3.10/site-packages/optimum/gptq/quantizer.py:45\n\
          \     42     from accelerate.hooks import remove_hook_from_module\n    \
          \ 44 if is_auto_gptq_available():\n---> 45     from auto_gptq import exllama_set_max_input_length\n\
          \     46     from auto_gptq.modeling._utils import autogptq_post_init\n\
          \     47     from auto_gptq.quantization import GPTQ\n\nFile /opt/conda/lib/python3.10/site-packages/auto_gptq/__init__.py:4\n\
          \      2 from .modeling import BaseQuantizeConfig\n      3 from .modeling\
          \ import AutoGPTQForCausalLM\n----> 4 from .utils.peft_utils import get_gptq_peft_model\n\
          \      5 from .utils.exllama_utils import exllama_set_max_input_length\n\
          \nFile /opt/conda/lib/python3.10/site-packages/auto_gptq/utils/peft_utils.py:20\n\
          \     18 from ..nn_modules.qlinear.qlinear_cuda import QuantLinear as QuantLinearCuda\n\
          \     19 from ..nn_modules.qlinear.qlinear_cuda_old import QuantLinear as\
          \ QuantLinearCudaOld\n---> 20 from ..nn_modules.qlinear.qlinear_exllama\
          \ import QuantLinear as QuantLinearExllama\n     21 from ..nn_modules.qlinear.qlinear_qigen\
          \ import QuantLinear as QuantLinearQigen\n     22 from ..nn_modules.qlinear.qlinear_triton\
          \ import QuantLinear as QuantLinearTriton\n\nFile /opt/conda/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py:14\n\
          \     11 logger = getLogger(__name__)\n     13 try:\n---> 14     from exllama_kernels\
          \ import make_q4, q4_matmul\n     15 except ImportError:\n     16     logger.error('exllama_kernels\
          \ not installed.')\n\nImportError: /opt/conda/lib/python3.10/site-packages/exllama_kernels.cpython-310-x86_64-linux-gnu.so:\
          \ undefined symbol: _ZN3c104cuda9SetDeviceEi"
        updatedAt: '2023-11-07T09:48:27.359Z'
      numEdits: 0
      reactions: []
    id: 654a07eb1718116e3515def0
    type: comment
  author: rajendrabaskota
  content: "ImportError                               Traceback (most recent call\
    \ last)\nCell In[3], line 7\n      4 model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\
    \n      5 # To use a different branch, change revision\n      6 # For example:\
    \ revision=\"main\"\n----> 7 model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
    \      8                                              device_map=\"auto\",\n \
    \     9                                              trust_remote_code=False,\n\
    \     10                                              revision=\"main\")\n   \
    \  12 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
    \     13 # tokenizer.pad_token = tokenizer.eos_token\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    561 elif type(config) in cls._model_mapping.keys():\n    562\
    \     model_class = _get_model_class(config, cls._model_mapping)\n--> 563    \
    \ return model_class.from_pretrained(\n    564         pretrained_model_name_or_path,\
    \ *model_args, config=config, **hub_kwargs, **kwargs\n    565     )\n    566 raise\
    \ ValueError(\n    567     f\"Unrecognized configuration class {config.__class__}\
    \ for this kind of AutoModel: {cls.__name__}.\\n\"\n    568     f\"Model type\
    \ should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\
    \n    569 )\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2577,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config,\
    \ cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token,\
    \ revision, use_safetensors, *model_args, **kwargs)\n   2572     raise ImportError(\n\
    \   2573         \"Loading GPTQ quantized model requires optimum library : `pip\
    \ install optimum` and auto-gptq library 'pip install auto-gptq'\"\n   2574  \
    \   )\n   2575 else:\n   2576     # Need to protect the import\n-> 2577     from\
    \ optimum.gptq import GPTQQuantizer\n   2578 if quantization_method_from_config\
    \ == QuantizationMethod.GPTQ:\n   2579     quantization_config = GPTQConfig.from_dict(config.quantization_config)\n\
    \nFile /opt/conda/lib/python3.10/site-packages/optimum/gptq/__init__.py:15\n \
    \     1 # coding=utf-8\n      2 # Copyright 2023 HuggingFace Inc. team.\n    \
    \  3 #\n   (...)\n     13 # See the License for the specific language governing\
    \ permissions and\n     14 # limitations under the License.\n---> 15 from .quantizer\
    \ import GPTQQuantizer, load_quantized_model\n\nFile /opt/conda/lib/python3.10/site-packages/optimum/gptq/quantizer.py:45\n\
    \     42     from accelerate.hooks import remove_hook_from_module\n     44 if\
    \ is_auto_gptq_available():\n---> 45     from auto_gptq import exllama_set_max_input_length\n\
    \     46     from auto_gptq.modeling._utils import autogptq_post_init\n     47\
    \     from auto_gptq.quantization import GPTQ\n\nFile /opt/conda/lib/python3.10/site-packages/auto_gptq/__init__.py:4\n\
    \      2 from .modeling import BaseQuantizeConfig\n      3 from .modeling import\
    \ AutoGPTQForCausalLM\n----> 4 from .utils.peft_utils import get_gptq_peft_model\n\
    \      5 from .utils.exllama_utils import exllama_set_max_input_length\n\nFile\
    \ /opt/conda/lib/python3.10/site-packages/auto_gptq/utils/peft_utils.py:20\n \
    \    18 from ..nn_modules.qlinear.qlinear_cuda import QuantLinear as QuantLinearCuda\n\
    \     19 from ..nn_modules.qlinear.qlinear_cuda_old import QuantLinear as QuantLinearCudaOld\n\
    ---> 20 from ..nn_modules.qlinear.qlinear_exllama import QuantLinear as QuantLinearExllama\n\
    \     21 from ..nn_modules.qlinear.qlinear_qigen import QuantLinear as QuantLinearQigen\n\
    \     22 from ..nn_modules.qlinear.qlinear_triton import QuantLinear as QuantLinearTriton\n\
    \nFile /opt/conda/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_exllama.py:14\n\
    \     11 logger = getLogger(__name__)\n     13 try:\n---> 14     from exllama_kernels\
    \ import make_q4, q4_matmul\n     15 except ImportError:\n     16     logger.error('exllama_kernels\
    \ not installed.')\n\nImportError: /opt/conda/lib/python3.10/site-packages/exllama_kernels.cpython-310-x86_64-linux-gnu.so:\
    \ undefined symbol: _ZN3c104cuda9SetDeviceEi"
  created_at: 2023-11-07 09:48:27+00:00
  edited: false
  hidden: false
  id: 654a07eb1718116e3515def0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/660ae6ae72b34714bcf77841fd7dc536.svg
      fullname: Rajendra Baskota
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rajendrabaskota
      type: user
    createdAt: '2023-11-07T10:45:43.000Z'
    data:
      edited: false
      editors:
      - rajendrabaskota
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9331640601158142
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/660ae6ae72b34714bcf77841fd7dc536.svg
          fullname: Rajendra Baskota
          isHf: false
          isPro: false
          name: rajendrabaskota
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> can you please\
          \ help us with this!</p>\n"
        raw: '@TheBloke can you please help us with this!'
        updatedAt: '2023-11-07T10:45:43.185Z'
      numEdits: 0
      reactions: []
    id: 654a15574666b903b79e4adb
    type: comment
  author: rajendrabaskota
  content: '@TheBloke can you please help us with this!'
  created_at: 2023-11-07 10:45:43+00:00
  edited: false
  hidden: false
  id: 654a15574666b903b79e4adb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/550287c9dfdd697100d6dee19070a1d6.svg
      fullname: Shivji Agnihotri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShivjiAgnihotri
      type: user
    createdAt: '2023-11-09T19:24:44.000Z'
    data:
      edited: false
      editors:
      - ShivjiAgnihotri
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7828752994537354
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/550287c9dfdd697100d6dee19070a1d6.svg
          fullname: Shivji Agnihotri
          isHf: false
          isPro: false
          name: ShivjiAgnihotri
          type: user
        html: '<p>I have found a solution for this I think. While fine-tuning on GPU
          especially when we fine-tune a gpt-q qunatized models, then we need to disable
          the Exllama. </p>

          <p>I came across this video on YouTube. This might help you :- <a rel="nofollow"
          href="https://youtu.be/T7haqIbHKm0?si=mPxt8NvUqggvWMli">https://youtu.be/T7haqIbHKm0?si=mPxt8NvUqggvWMli</a></p>

          <p>Watch from 14:13</p>

          '
        raw: "I have found a solution for this I think. While fine-tuning on GPU especially\
          \ when we fine-tune a gpt-q qunatized models, then we need to disable the\
          \ Exllama. \n\nI came across this video on YouTube. This might help you\
          \ :- https://youtu.be/T7haqIbHKm0?si=mPxt8NvUqggvWMli\n\nWatch from 14:13\n\
          \n"
        updatedAt: '2023-11-09T19:24:44.049Z'
      numEdits: 0
      reactions: []
    id: 654d31fc93023cd3d832a6c6
    type: comment
  author: ShivjiAgnihotri
  content: "I have found a solution for this I think. While fine-tuning on GPU especially\
    \ when we fine-tune a gpt-q qunatized models, then we need to disable the Exllama.\
    \ \n\nI came across this video on YouTube. This might help you :- https://youtu.be/T7haqIbHKm0?si=mPxt8NvUqggvWMli\n\
    \nWatch from 14:13\n\n"
  created_at: 2023-11-09 19:24:44+00:00
  edited: false
  hidden: false
  id: 654d31fc93023cd3d832a6c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/550287c9dfdd697100d6dee19070a1d6.svg
      fullname: Shivji Agnihotri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShivjiAgnihotri
      type: user
    createdAt: '2023-11-10T03:51:14.000Z'
    data:
      edited: false
      editors:
      - ShivjiAgnihotri
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4476070702075958
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/550287c9dfdd697100d6dee19070a1d6.svg
          fullname: Shivji Agnihotri
          isHf: false
          isPro: false
          name: ShivjiAgnihotri
          type: user
        html: '<p>----- The Bug is Fixed NOW: ----</p>

          <p>Now the error issue does not persists. Just use: --------&gt; pip install
          auto-gptq</p>

          <p>Keep in mind to use this version of torch. ----------&gt; 2.1.0+cu118</p>

          <p><code>In case if getting these warnings WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA
          extension not installed. WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA
          extension not installed.</code><br>then use this: ----------&gt;                 !sudo
          apt install -q nvidia-cuda-toolkit</p>

          <p>Reference:<br><a rel="nofollow" href="https://pypi.org/project/auto-gptq/">https://pypi.org/project/auto-gptq/</a>   ----------&gt;
          auto-gptq has been updated on 9th Nov<br><a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/issues/398">https://github.com/PanQiWei/AutoGPTQ/issues/398</a>        ---------&gt;
          This thread talks about using an older version of auto-gpt i.e 0.4.2 BUT
          NOW THAT''S NOT NEEDED</p>

          '
        raw: '----- The Bug is Fixed NOW: ----


          Now the error issue does not persists. Just use: --------> pip install auto-gptq


          Keep in mind to use this version of torch. ----------> 2.1.0+cu118


          ````In case if getting these warnings

          WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.

          WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not
          installed.````

          then use this: ---------->                 !sudo apt install -q nvidia-cuda-toolkit


          Reference:

          https://pypi.org/project/auto-gptq/   ----------> auto-gptq has been updated
          on 9th Nov

          https://github.com/PanQiWei/AutoGPTQ/issues/398        ---------> This thread
          talks about using an older version of auto-gpt i.e 0.4.2 BUT NOW THAT''S
          NOT NEEDED'
        updatedAt: '2023-11-10T03:51:14.647Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - rajendrabaskota
        - ditchtech
        - jamescurtis
    id: 654da8b2a312584f5dea2b23
    type: comment
  author: ShivjiAgnihotri
  content: '----- The Bug is Fixed NOW: ----


    Now the error issue does not persists. Just use: --------> pip install auto-gptq


    Keep in mind to use this version of torch. ----------> 2.1.0+cu118


    ````In case if getting these warnings

    WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.

    WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.````

    then use this: ---------->                 !sudo apt install -q nvidia-cuda-toolkit


    Reference:

    https://pypi.org/project/auto-gptq/   ----------> auto-gptq has been updated on
    9th Nov

    https://github.com/PanQiWei/AutoGPTQ/issues/398        ---------> This thread
    talks about using an older version of auto-gpt i.e 0.4.2 BUT NOW THAT''S NOT NEEDED'
  created_at: 2023-11-10 03:51:14+00:00
  edited: false
  hidden: false
  id: 654da8b2a312584f5dea2b23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-11-10T12:40:47.000Z'
    data:
      edited: false
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9859396815299988
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: '<p>Myself, i still have a CUDA version issue to deal with, after some
          other upgrades to get past the other recent issue floating around. Others
          might as well.  My drivers are ''too old'' according to some of the libraries.   (
          i think it was pytorch, haven''t spent the time to go back and review since
          my CPU only box now works again, which is the important one )</p>

          '
        raw: 'Myself, i still have a CUDA version issue to deal with, after some other
          upgrades to get past the other recent issue floating around. Others might
          as well.  My drivers are ''too old'' according to some of the libraries.   (
          i think it was pytorch, haven''t spent the time to go back and review since
          my CPU only box now works again, which is the important one )

          '
        updatedAt: '2023-11-10T12:40:47.238Z'
      numEdits: 0
      reactions: []
    id: 654e24cf298f66afe8950f62
    type: comment
  author: Nurb432
  content: 'Myself, i still have a CUDA version issue to deal with, after some other
    upgrades to get past the other recent issue floating around. Others might as well.  My
    drivers are ''too old'' according to some of the libraries.   ( i think it was
    pytorch, haven''t spent the time to go back and review since my CPU only box now
    works again, which is the important one )

    '
  created_at: 2023-11-10 12:40:47+00:00
  edited: false
  hidden: false
  id: 654e24cf298f66afe8950f62
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 47
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Issues with CUDA and exllama_kernels
