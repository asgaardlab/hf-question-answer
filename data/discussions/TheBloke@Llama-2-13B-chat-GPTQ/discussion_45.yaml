!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DivyanshTiwari7
conflicting_files: null
created_at: 2023-10-11 05:23:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/330e7c2c9a4b7243c8ba77fbc60f4d97.svg
      fullname: Divyansh Tiwari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DivyanshTiwari7
      type: user
    createdAt: '2023-10-11T06:23:05.000Z'
    data:
      edited: false
      editors:
      - DivyanshTiwari7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9366952180862427
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/330e7c2c9a4b7243c8ba77fbc60f4d97.svg
          fullname: Divyansh Tiwari
          isHf: false
          isPro: false
          name: DivyanshTiwari7
          type: user
        html: "<p>I am using this model in text-generation-webui of ooba. But it is\
          \ hallucinating alot and even not following the given prompt properly.<br>My\
          \ use-case is chatPDF where i am providing chunks of pdfs as a knowledge\
          \ source in the prompt along with the question i need answer to. But even\
          \ if answer is present in chunks, Llama2 sometimes is not able to generate\
          \ correct response. It sometimes gives out of context answers as well from\
          \ its own knowledge base instead of sticking to the prompt.</p>\n<p> Can\
          \ you anyone please explain the reason of such issues and its fix if possible.\
          \ Any discussion on this would be highly appreciated.<br>Thank you</p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n"
        raw: "I am using this model in text-generation-webui of ooba. But it is hallucinating\
          \ alot and even not following the given prompt properly.\r\nMy use-case\
          \ is chatPDF where i am providing chunks of pdfs as a knowledge source in\
          \ the prompt along with the question i need answer to. But even if answer\
          \ is present in chunks, Llama2 sometimes is not able to generate correct\
          \ response. It sometimes gives out of context answers as well from its own\
          \ knowledge base instead of sticking to the prompt.\r\n\r\n Can you anyone\
          \ please explain the reason of such issues and its fix if possible. Any\
          \ discussion on this would be highly appreciated.\r\nThank you\r\n\r\n@TheBloke "
        updatedAt: '2023-10-11T06:23:05.709Z'
      numEdits: 0
      reactions: []
    id: 65263f495c25bbf19c9425f1
    type: comment
  author: DivyanshTiwari7
  content: "I am using this model in text-generation-webui of ooba. But it is hallucinating\
    \ alot and even not following the given prompt properly.\r\nMy use-case is chatPDF\
    \ where i am providing chunks of pdfs as a knowledge source in the prompt along\
    \ with the question i need answer to. But even if answer is present in chunks,\
    \ Llama2 sometimes is not able to generate correct response. It sometimes gives\
    \ out of context answers as well from its own knowledge base instead of sticking\
    \ to the prompt.\r\n\r\n Can you anyone please explain the reason of such issues\
    \ and its fix if possible. Any discussion on this would be highly appreciated.\r\
    \nThank you\r\n\r\n@TheBloke "
  created_at: 2023-10-11 05:23:05+00:00
  edited: false
  hidden: false
  id: 65263f495c25bbf19c9425f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-11T15:04:45.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7873426675796509
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>I mean it is a quantized 13b model but you might be using the wrong
          prompt format? You need to choose the llama 2 prompt format.</p>

          '
        raw: I mean it is a quantized 13b model but you might be using the wrong prompt
          format? You need to choose the llama 2 prompt format.
        updatedAt: '2023-10-11T15:04:45.869Z'
      numEdits: 0
      reactions: []
    id: 6526b98de0e15abc60a4dd20
    type: comment
  author: YaTharThShaRma999
  content: I mean it is a quantized 13b model but you might be using the wrong prompt
    format? You need to choose the llama 2 prompt format.
  created_at: 2023-10-11 14:04:45+00:00
  edited: false
  hidden: false
  id: 6526b98de0e15abc60a4dd20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/330e7c2c9a4b7243c8ba77fbc60f4d97.svg
      fullname: Divyansh Tiwari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DivyanshTiwari7
      type: user
    createdAt: '2023-10-11T16:15:12.000Z'
    data:
      edited: false
      editors:
      - DivyanshTiwari7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9171925187110901
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/330e7c2c9a4b7243c8ba77fbc60f4d97.svg
          fullname: Divyansh Tiwari
          isHf: false
          isPro: false
          name: DivyanshTiwari7
          type: user
        html: '<p>Thanks for replying @johnwick123forevr , in text-generation-webui
          I am using openai base to call Llama 2 model which sends prompt exactly
          in GPT format but changes it on backend in the following format for Llama
          2:</p>

          <p>''''''<br>You are a helpful AI assistant. Whatever conversation you have
          with the customer needs to be carried out in a polite and professional manner.<br>Generate
          answers ONLY based on the facts given in the list of sources below. When
          you generate an answer from a source, you need to use that source completely
          and not omit a single point. If there isn''t enough information below, say
          you don''t know and do not generate answers on your own. If asking a clarifying
          question to the user would help, ask the question.<br>For tabular information
          return it as an html table. Do not return markdown format. If the question
          is not in English, answer in the language used in the question.<br>For answers
          that do not have a table, generate the answer in a point wise manner.<br>For
          every answer you give, you HAVE to mention the section number from where
          you have generated the answer. For example you need to write the reference
          in the following format - (Reference Section: 10.4.2)</p>

          <p>Sources:<br>{ Source knowledge chunk here }<br>Below is an instruction
          that describes a task. Write a response that appropriately completes the
          request.</p>

          <h3 id="instruction">Instruction:</h3>

          <p>{user query here}</p>

          <p>''''''</p>

          <p>But Llama model does not stick to prompt always and hallucinate. Sometimes
          it does not give answer even if it is present in the source knowledge chunks.</p>

          '
        raw: 'Thanks for replying @johnwick123forevr , in text-generation-webui I
          am using openai base to call Llama 2 model which sends prompt exactly in
          GPT format but changes it on backend in the following format for Llama 2:


          ''''''

          You are a helpful AI assistant. Whatever conversation you have with the
          customer needs to be carried out in a polite and professional manner.

          Generate answers ONLY based on the facts given in the list of sources below.
          When you generate an answer from a source, you need to use that source completely
          and not omit a single point. If there isn''t enough information below, say
          you don''t know and do not generate answers on your own. If asking a clarifying
          question to the user would help, ask the question.

          For tabular information return it as an html table. Do not return markdown
          format. If the question is not in English, answer in the language used in
          the question.

          For answers that do not have a table, generate the answer in a point wise
          manner.

          For every answer you give, you HAVE to mention the section number from where
          you have generated the answer. For example you need to write the reference
          in the following format - (Reference Section: 10.4.2)



          Sources:

          { Source knowledge chunk here }

          Below is an instruction that describes a task. Write a response that appropriately
          completes the request.


          ### Instruction:

          {user query here}


          ''''''


          But Llama model does not stick to prompt always and hallucinate. Sometimes
          it does not give answer even if it is present in the source knowledge chunks.



          '
        updatedAt: '2023-10-11T16:15:12.096Z'
      numEdits: 0
      reactions: []
    id: 6526ca10427ecd343ecc0e3b
    type: comment
  author: DivyanshTiwari7
  content: 'Thanks for replying @johnwick123forevr , in text-generation-webui I am
    using openai base to call Llama 2 model which sends prompt exactly in GPT format
    but changes it on backend in the following format for Llama 2:


    ''''''

    You are a helpful AI assistant. Whatever conversation you have with the customer
    needs to be carried out in a polite and professional manner.

    Generate answers ONLY based on the facts given in the list of sources below. When
    you generate an answer from a source, you need to use that source completely and
    not omit a single point. If there isn''t enough information below, say you don''t
    know and do not generate answers on your own. If asking a clarifying question
    to the user would help, ask the question.

    For tabular information return it as an html table. Do not return markdown format.
    If the question is not in English, answer in the language used in the question.

    For answers that do not have a table, generate the answer in a point wise manner.

    For every answer you give, you HAVE to mention the section number from where you
    have generated the answer. For example you need to write the reference in the
    following format - (Reference Section: 10.4.2)



    Sources:

    { Source knowledge chunk here }

    Below is an instruction that describes a task. Write a response that appropriately
    completes the request.


    ### Instruction:

    {user query here}


    ''''''


    But Llama model does not stick to prompt always and hallucinate. Sometimes it
    does not give answer even if it is present in the source knowledge chunks.



    '
  created_at: 2023-10-11 15:15:12+00:00
  edited: false
  hidden: false
  id: 6526ca10427ecd343ecc0e3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-11T16:38:30.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9135972857475281
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>Hm. That might be the problem.</p>

          <p>Llama2 chat needs a format like this<br>[INST] &lt;&gt;<br>You are a
          helpful, respectful and honest assistant. Always answer as helpfully as
          possible, while being safe.  Your answers should not include any harmful,
          unethical, racist, sexist, toxic, dangerous, or illegal content. Please
          ensure that your responses are socially unbiased and positive in nature.
          If a question does not make any sense, or is not factually coherent, explain
          why instead of answering something not correct. If you don''t know the answer
          to a question, please don''t share false information.<br>&lt;&gt;<br>{prompt}[/INST]</p>

          '
        raw: 'Hm. That might be the problem.


          Llama2 chat needs a format like this

          [INST] <<SYS>>

          You are a helpful, respectful and honest assistant. Always answer as helpfully
          as possible, while being safe.  Your answers should not include any harmful,
          unethical, racist, sexist, toxic, dangerous, or illegal content. Please
          ensure that your responses are socially unbiased and positive in nature.
          If a question does not make any sense, or is not factually coherent, explain
          why instead of answering something not correct. If you don''t know the answer
          to a question, please don''t share false information.

          <</SYS>>

          {prompt}[/INST]'
        updatedAt: '2023-10-11T16:38:30.111Z'
      numEdits: 0
      reactions: []
    id: 6526cf869b9bec533f882d7b
    type: comment
  author: YaTharThShaRma999
  content: 'Hm. That might be the problem.


    Llama2 chat needs a format like this

    [INST] <<SYS>>

    You are a helpful, respectful and honest assistant. Always answer as helpfully
    as possible, while being safe.  Your answers should not include any harmful, unethical,
    racist, sexist, toxic, dangerous, or illegal content. Please ensure that your
    responses are socially unbiased and positive in nature. If a question does not
    make any sense, or is not factually coherent, explain why instead of answering
    something not correct. If you don''t know the answer to a question, please don''t
    share false information.

    <</SYS>>

    {prompt}[/INST]'
  created_at: 2023-10-11 15:38:30+00:00
  edited: false
  hidden: false
  id: 6526cf869b9bec533f882d7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/330e7c2c9a4b7243c8ba77fbc60f4d97.svg
      fullname: Divyansh Tiwari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DivyanshTiwari7
      type: user
    createdAt: '2023-10-11T16:41:47.000Z'
    data:
      edited: false
      editors:
      - DivyanshTiwari7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9622056484222412
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/330e7c2c9a4b7243c8ba77fbc60f4d97.svg
          fullname: Divyansh Tiwari
          isHf: false
          isPro: false
          name: DivyanshTiwari7
          type: user
        html: '<p>Yes I tried this format but it was exceeding token limit as we send
          some chunks too along with prompt. I will try this again with shorter prompt.
          thanks @johnwick123forevr </p>

          '
        raw: 'Yes I tried this format but it was exceeding token limit as we send
          some chunks too along with prompt. I will try this again with shorter prompt.
          thanks @johnwick123forevr '
        updatedAt: '2023-10-11T16:41:47.371Z'
      numEdits: 0
      reactions: []
    id: 6526d04bba9a8279c13cf148
    type: comment
  author: DivyanshTiwari7
  content: 'Yes I tried this format but it was exceeding token limit as we send some
    chunks too along with prompt. I will try this again with shorter prompt. thanks
    @johnwick123forevr '
  created_at: 2023-10-11 15:41:47+00:00
  edited: false
  hidden: false
  id: 6526d04bba9a8279c13cf148
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-11T18:58:41.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9007551670074463
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>Hmm then just remove the whole you are helpful part. And just put
          you are an ai.</p>

          '
        raw: Hmm then just remove the whole you are helpful part. And just put you
          are an ai.
        updatedAt: '2023-10-11T18:58:41.355Z'
      numEdits: 0
      reactions: []
    id: 6526f0612f3eeddf4c950ea2
    type: comment
  author: YaTharThShaRma999
  content: Hmm then just remove the whole you are helpful part. And just put you are
    an ai.
  created_at: 2023-10-11 17:58:41+00:00
  edited: false
  hidden: false
  id: 6526f0612f3eeddf4c950ea2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/330e7c2c9a4b7243c8ba77fbc60f4d97.svg
      fullname: Divyansh Tiwari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DivyanshTiwari7
      type: user
    createdAt: '2023-10-13T06:24:51.000Z'
    data:
      edited: false
      editors:
      - DivyanshTiwari7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8926032185554504
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/330e7c2c9a4b7243c8ba77fbc60f4d97.svg
          fullname: Divyansh Tiwari
          isHf: false
          isPro: false
          name: DivyanshTiwari7
          type: user
        html: '<p>@johnwick123forevr Thanks. It made the result better but still hallucination
          of Llama 2 13b persist. It gives different responses for same query and
          not always correct. Do you think only prompting can help here or we need
          to look for alternatives like Fine tuning etc ??</p>

          '
        raw: '@johnwick123forevr Thanks. It made the result better but still hallucination
          of Llama 2 13b persist. It gives different responses for same query and
          not always correct. Do you think only prompting can help here or we need
          to look for alternatives like Fine tuning etc ??'
        updatedAt: '2023-10-13T06:24:51.145Z'
      numEdits: 0
      reactions: []
    id: 6528e2b39b25d7fe4072be6e
    type: comment
  author: DivyanshTiwari7
  content: '@johnwick123forevr Thanks. It made the result better but still hallucination
    of Llama 2 13b persist. It gives different responses for same query and not always
    correct. Do you think only prompting can help here or we need to look for alternatives
    like Fine tuning etc ??'
  created_at: 2023-10-13 05:24:51+00:00
  edited: false
  hidden: false
  id: 6528e2b39b25d7fe4072be6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed5682a382a509a20eafcb3b666dfe03.svg
      fullname: Denis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: frenge
      type: user
    createdAt: '2023-10-16T08:36:13.000Z'
    data:
      edited: false
      editors:
      - frenge
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7402046918869019
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed5682a382a509a20eafcb3b666dfe03.svg
          fullname: Denis
          isHf: false
          isPro: false
          name: frenge
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;DivyanshTiwari7&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/DivyanshTiwari7\"\
          >@<span class=\"underline\">DivyanshTiwari7</span></a></span>\n\n\t</span></span>\
          \ \" It gives different responses for same query\". Try to set top_k=1 in\
          \ model.generate or pipeline.</p>\n"
        raw: '@DivyanshTiwari7 " It gives different responses for same query". Try
          to set top_k=1 in model.generate or pipeline.'
        updatedAt: '2023-10-16T08:36:13.444Z'
      numEdits: 0
      reactions: []
    id: 652cf5fd1a3250bbfe83e089
    type: comment
  author: frenge
  content: '@DivyanshTiwari7 " It gives different responses for same query". Try to
    set top_k=1 in model.generate or pipeline.'
  created_at: 2023-10-16 07:36:13+00:00
  edited: false
  hidden: false
  id: 652cf5fd1a3250bbfe83e089
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 45
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Hallucination issue in Llama-2-13B-chat-GPTQ
