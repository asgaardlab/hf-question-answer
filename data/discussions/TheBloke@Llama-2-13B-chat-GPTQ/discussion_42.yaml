!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dayton
conflicting_files: null
created_at: 2023-09-21 18:15:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
      fullname: Dayton Turner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dayton
      type: user
    createdAt: '2023-09-21T19:15:39.000Z'
    data:
      edited: false
      editors:
      - dayton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8147079348564148
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
          fullname: Dayton Turner
          isHf: false
          isPro: false
          name: dayton
          type: user
        html: '<p>Hi there - I presume I must be doing something wrong, so hoping
          you can help me figure out what I''ve overlooked.</p>

          <p>I''m running this model with the latest TGI docker container, with the
          following params:</p>

          <pre><code>docker run --gpus all --shm-size 1g -p 8080:80 -v /data:/data
          -e HUGGING_FACE_HUB_TOKEN=$token --pull always ghcr.io/huggingface/text-generation-inference:latest
          --model-id TheBloke/Llama-2-13B-chat-GPTQ--quantize gptq

          </code></pre>

          <p>I can see its pulled the model to disk, and the config json shows bits:4
          in quantization_config</p>

          <p>My expectation is it would have taken roughly 10GB VRAM, but when I load
          it, nvidia-smi shows TGI has consumed 46.74GiB out of my total 48GB vram
          (A6000)</p>

          <p>Have I missed something here? </p>

          '
        raw: "Hi there - I presume I must be doing something wrong, so hoping you\
          \ can help me figure out what I've overlooked.\r\n\r\nI'm running this model\
          \ with the latest TGI docker container, with the following params:\r\n\r\
          \n```\r\ndocker run --gpus all --shm-size 1g -p 8080:80 -v /data:/data -e\
          \ HUGGING_FACE_HUB_TOKEN=$token --pull always ghcr.io/huggingface/text-generation-inference:latest\
          \ --model-id TheBloke/Llama-2-13B-chat-GPTQ--quantize gptq\r\n```\r\n\r\n\
          I can see its pulled the model to disk, and the config json shows bits:4\
          \ in quantization_config\r\n\r\nMy expectation is it would have taken roughly\
          \ 10GB VRAM, but when I load it, nvidia-smi shows TGI has consumed 46.74GiB\
          \ out of my total 48GB vram (A6000)\r\n\r\nHave I missed something here? "
        updatedAt: '2023-09-21T19:15:39.337Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - eternalAnurag
    id: 650c965ba459fa1a44196762
    type: comment
  author: dayton
  content: "Hi there - I presume I must be doing something wrong, so hoping you can\
    \ help me figure out what I've overlooked.\r\n\r\nI'm running this model with\
    \ the latest TGI docker container, with the following params:\r\n\r\n```\r\ndocker\
    \ run --gpus all --shm-size 1g -p 8080:80 -v /data:/data -e HUGGING_FACE_HUB_TOKEN=$token\
    \ --pull always ghcr.io/huggingface/text-generation-inference:latest --model-id\
    \ TheBloke/Llama-2-13B-chat-GPTQ--quantize gptq\r\n```\r\n\r\nI can see its pulled\
    \ the model to disk, and the config json shows bits:4 in quantization_config\r\
    \n\r\nMy expectation is it would have taken roughly 10GB VRAM, but when I load\
    \ it, nvidia-smi shows TGI has consumed 46.74GiB out of my total 48GB vram (A6000)\r\
    \n\r\nHave I missed something here? "
  created_at: 2023-09-21 18:15:39+00:00
  edited: false
  hidden: false
  id: 650c965ba459fa1a44196762
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-21T19:33:29.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49358606338500977
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Missing a space :)<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/rhwWMtfZGTZ8V53voZvpi.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/rhwWMtfZGTZ8V53voZvpi.png"></a></p>

          '
        raw: "Missing a space :) \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/rhwWMtfZGTZ8V53voZvpi.png)\n"
        updatedAt: '2023-09-21T19:33:29.318Z'
      numEdits: 0
      reactions: []
    id: 650c9a89a853f6b3f8a6a93c
    type: comment
  author: TheBloke
  content: "Missing a space :) \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/rhwWMtfZGTZ8V53voZvpi.png)\n"
  created_at: 2023-09-21 18:33:29+00:00
  edited: false
  hidden: false
  id: 650c9a89a853f6b3f8a6a93c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
      fullname: Dayton Turner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dayton
      type: user
    createdAt: '2023-09-21T19:59:16.000Z'
    data:
      edited: false
      editors:
      - dayton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9535561800003052
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
          fullname: Dayton Turner
          isHf: false
          isPro: false
          name: dayton
          type: user
        html: '<p>oops - that was actually an issue from copy/pasting the model name
          into my example - when running it, there is a proper space.  I''ve noticed
          that when TGI loads it actually does initially consume about 10-12 GB of
          vram, but then within 1-2 seconds of TGI warming the model, it immediately
          jumps up to 46GB.  It also happens when i tested against TheBloke/Phind-CodeLlama-34B-v2-GPTQ
          </p>

          <p>There isn''t some special way to load custom branches for the other quant
          methods that I''m overlooking here?</p>

          '
        raw: "oops - that was actually an issue from copy/pasting the model name into\
          \ my example - when running it, there is a proper space.  I've noticed that\
          \ when TGI loads it actually does initially consume about 10-12 GB of vram,\
          \ but then within 1-2 seconds of TGI warming the model, it immediately jumps\
          \ up to 46GB.  It also happens when i tested against TheBloke/Phind-CodeLlama-34B-v2-GPTQ\
          \ \n\nThere isn't some special way to load custom branches for the other\
          \ quant methods that I'm overlooking here?"
        updatedAt: '2023-09-21T19:59:16.954Z'
      numEdits: 0
      reactions: []
    id: 650ca094e31a6f18f0f6f3c1
    type: comment
  author: dayton
  content: "oops - that was actually an issue from copy/pasting the model name into\
    \ my example - when running it, there is a proper space.  I've noticed that when\
    \ TGI loads it actually does initially consume about 10-12 GB of vram, but then\
    \ within 1-2 seconds of TGI warming the model, it immediately jumps up to 46GB.\
    \  It also happens when i tested against TheBloke/Phind-CodeLlama-34B-v2-GPTQ\
    \ \n\nThere isn't some special way to load custom branches for the other quant\
    \ methods that I'm overlooking here?"
  created_at: 2023-09-21 18:59:16+00:00
  edited: false
  hidden: false
  id: 650ca094e31a6f18f0f6f3c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-21T20:00:42.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9076095223426819
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh, then that''s just normal TGI then. It uses all the VRAM it can
          for caching.  Just test it, should be fine.</p>

          '
        raw: Oh, then that's just normal TGI then. It uses all the VRAM it can for
          caching.  Just test it, should be fine.
        updatedAt: '2023-09-21T20:00:42.031Z'
      numEdits: 0
      reactions: []
    id: 650ca0eacbd0c7d550bcf99e
    type: comment
  author: TheBloke
  content: Oh, then that's just normal TGI then. It uses all the VRAM it can for caching.  Just
    test it, should be fine.
  created_at: 2023-09-21 19:00:42+00:00
  edited: false
  hidden: false
  id: 650ca0eacbd0c7d550bcf99e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
      fullname: Dayton Turner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dayton
      type: user
    createdAt: '2023-09-21T20:05:30.000Z'
    data:
      edited: false
      editors:
      - dayton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9829506278038025
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
          fullname: Dayton Turner
          isHf: false
          isPro: false
          name: dayton
          type: user
        html: '<p>OH - I wasn''t aware it did this.  Is that configurable?  I''ve
          posted this separately on the TGI github, so if you don''t know, I''ll just
          wait for their response.  But, I was hoping to take advantage of this 48GB
          card by loading two quantized models in two separate instances of TGI, so
          that I could provide access to two models that would load into the same
          card (in this case, the Llama-2-13B-chat model, as well as the Phind-CodeLlama
          model.  I (perhaps incorrectly?) presumed that running two instances with
          properly quantized models would permit this</p>

          '
        raw: OH - I wasn't aware it did this.  Is that configurable?  I've posted
          this separately on the TGI github, so if you don't know, I'll just wait
          for their response.  But, I was hoping to take advantage of this 48GB card
          by loading two quantized models in two separate instances of TGI, so that
          I could provide access to two models that would load into the same card
          (in this case, the Llama-2-13B-chat model, as well as the Phind-CodeLlama
          model.  I (perhaps incorrectly?) presumed that running two instances with
          properly quantized models would permit this
        updatedAt: '2023-09-21T20:05:30.894Z'
      numEdits: 0
      reactions: []
    id: 650ca20a36ac7eba06e2cced
    type: comment
  author: dayton
  content: OH - I wasn't aware it did this.  Is that configurable?  I've posted this
    separately on the TGI github, so if you don't know, I'll just wait for their response.  But,
    I was hoping to take advantage of this 48GB card by loading two quantized models
    in two separate instances of TGI, so that I could provide access to two models
    that would load into the same card (in this case, the Llama-2-13B-chat model,
    as well as the Phind-CodeLlama model.  I (perhaps incorrectly?) presumed that
    running two instances with properly quantized models would permit this
  created_at: 2023-09-21 19:05:30+00:00
  edited: false
  hidden: false
  id: 650ca20a36ac7eba06e2cced
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
      fullname: Dayton Turner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dayton
      type: user
    createdAt: '2023-09-21T20:08:40.000Z'
    data:
      edited: false
      editors:
      - dayton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8806436061859131
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
          fullname: Dayton Turner
          isHf: false
          isPro: false
          name: dayton
          type: user
        html: '<p>Just found this now that I have more to search on, in the TGI github:</p>

          <pre><code> --cuda-memory-fraction is the way to control total RAM usage
          if you want to stack multiple deployments on the same machine.

          </code></pre>

          '
        raw: "Just found this now that I have more to search on, in the TGI github:\n\
          \n```\n --cuda-memory-fraction is the way to control total RAM usage if\
          \ you want to stack multiple deployments on the same machine.\n```"
        updatedAt: '2023-09-21T20:08:40.484Z'
      numEdits: 0
      reactions: []
    id: 650ca2c836ac7eba06e2f7ab
    type: comment
  author: dayton
  content: "Just found this now that I have more to search on, in the TGI github:\n\
    \n```\n --cuda-memory-fraction is the way to control total RAM usage if you want\
    \ to stack multiple deployments on the same machine.\n```"
  created_at: 2023-09-21 19:08:40+00:00
  edited: false
  hidden: false
  id: 650ca2c836ac7eba06e2f7ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
      fullname: Dayton Turner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dayton
      type: user
    createdAt: '2023-09-21T20:10:05.000Z'
    data:
      edited: false
      editors:
      - dayton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9650375843048096
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
          fullname: Dayton Turner
          isHf: false
          isPro: false
          name: dayton
          type: user
        html: '<p>But I suppose a still-relevant question - if I wanted to use an
          8bit quant instead of 4bit quant of your Llama-2-13B-chat-GPTQ model, how
          would i do that with TGI?  I cant seem to find an obvious answer to how
          I would use a different branch from your repo with TGI</p>

          '
        raw: But I suppose a still-relevant question - if I wanted to use an 8bit
          quant instead of 4bit quant of your Llama-2-13B-chat-GPTQ model, how would
          i do that with TGI?  I cant seem to find an obvious answer to how I would
          use a different branch from your repo with TGI
        updatedAt: '2023-09-21T20:10:05.497Z'
      numEdits: 0
      reactions: []
    id: 650ca31de31a6f18f0f74659
    type: comment
  author: dayton
  content: But I suppose a still-relevant question - if I wanted to use an 8bit quant
    instead of 4bit quant of your Llama-2-13B-chat-GPTQ model, how would i do that
    with TGI?  I cant seem to find an obvious answer to how I would use a different
    branch from your repo with TGI
  created_at: 2023-09-21 19:10:05+00:00
  edited: false
  hidden: false
  id: 650ca31de31a6f18f0f74659
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-21T20:10:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5035725831985474
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It''s the revision parameter, eg <code>--revision gptq-8bit-128g-actorder_True</code>
          or whatever branch name</p>

          '
        raw: It's the revision parameter, eg `--revision gptq-8bit-128g-actorder_True`
          or whatever branch name
        updatedAt: '2023-09-21T20:10:51.858Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - eternalAnurag
    id: 650ca34bdceec9cd4aec1c4c
    type: comment
  author: TheBloke
  content: It's the revision parameter, eg `--revision gptq-8bit-128g-actorder_True`
    or whatever branch name
  created_at: 2023-09-21 19:10:51+00:00
  edited: false
  hidden: false
  id: 650ca34bdceec9cd4aec1c4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
      fullname: Dayton Turner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dayton
      type: user
    createdAt: '2023-09-21T20:14:47.000Z'
    data:
      edited: false
      editors:
      - dayton
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8915016651153564
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
          fullname: Dayton Turner
          isHf: false
          isPro: false
          name: dayton
          type: user
        html: '<p>Got it - I thought revision only accepted a commit hash.  Thanks
          again!</p>

          '
        raw: Got it - I thought revision only accepted a commit hash.  Thanks again!
        updatedAt: '2023-09-21T20:14:47.855Z'
      numEdits: 0
      reactions: []
      relatedEventId: 650ca437d439bbbbad03ffe3
    id: 650ca437d439bbbbad03ffdf
    type: comment
  author: dayton
  content: Got it - I thought revision only accepted a commit hash.  Thanks again!
  created_at: 2023-09-21 19:14:47+00:00
  edited: false
  hidden: false
  id: 650ca437d439bbbbad03ffdf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/c69eb555cbc1722809c8198535acdf2a.svg
      fullname: Dayton Turner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dayton
      type: user
    createdAt: '2023-09-21T20:14:47.000Z'
    data:
      status: closed
    id: 650ca437d439bbbbad03ffe3
    type: status-change
  author: dayton
  created_at: 2023-09-21 19:14:47+00:00
  id: 650ca437d439bbbbad03ffe3
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 42
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: closed
target_branch: null
title: Quantized version being loaded in TGI and consuming way too much memory?
