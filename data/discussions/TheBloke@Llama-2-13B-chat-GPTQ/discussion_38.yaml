!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ZaydJamadar
conflicting_files: null
created_at: 2023-08-28 12:07:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e81b56ef3b78acbb931a2c4f96509625.svg
      fullname: Zayd Jamadar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZaydJamadar
      type: user
    createdAt: '2023-08-28T13:07:15.000Z'
    data:
      edited: false
      editors:
      - ZaydJamadar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8596217036247253
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e81b56ef3b78acbb931a2c4f96509625.svg
          fullname: Zayd Jamadar
          isHf: false
          isPro: false
          name: ZaydJamadar
          type: user
        html: '<p>Now that HuggingFace TGI server can be deployed on AWS SageMaker
          as a deep learning container, it requires a HuggingFaceModel class LLM.
          How can I convert this GPTQ quantised LLM to that class?</p>

          '
        raw: Now that HuggingFace TGI server can be deployed on AWS SageMaker as a
          deep learning container, it requires a HuggingFaceModel class LLM. How can
          I convert this GPTQ quantised LLM to that class?
        updatedAt: '2023-08-28T13:07:15.138Z'
      numEdits: 0
      reactions: []
    id: 64ec9c03a91f5a573e4d1f40
    type: comment
  author: ZaydJamadar
  content: Now that HuggingFace TGI server can be deployed on AWS SageMaker as a deep
    learning container, it requires a HuggingFaceModel class LLM. How can I convert
    this GPTQ quantised LLM to that class?
  created_at: 2023-08-28 12:07:15+00:00
  edited: false
  hidden: false
  id: 64ec9c03a91f5a573e4d1f40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a547d91d93ff00db921f2d296ef3b5b7.svg
      fullname: fabian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fa8ian
      type: user
    createdAt: '2023-09-08T15:13:14.000Z'
    data:
      edited: true
      editors:
      - fa8ian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49746212363243103
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a547d91d93ff00db921f2d296ef3b5b7.svg
          fullname: fabian
          isHf: false
          isPro: false
          name: fa8ian
          type: user
        html: "<p>Hi there,</p>\n<p>It is possible to deploy the model using TGI and\
          \ sagemaker. You just have to adapt the configuration.</p>\n<p>Here is an\
          \ example of how to deploy the main branch to sagemaker:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> sagemaker.huggingface\
          \ <span class=\"hljs-keyword\">import</span> get_huggingface_llm_image_uri\n\
          \nllm_image = get_huggingface_llm_image_uri(<span class=\"hljs-string\"\
          >\"huggingface\"</span>, version=<span class=\"hljs-string\">\"0.9.3\"</span>,\
          \ session=sess)\n</code></pre>\n<p>If you don't know how to set up a session,\
          \ just check out this resource: <a href=\"https://huggingface.co/docs/sagemaker/inference#installation-and-setup\"\
          >https://huggingface.co/docs/sagemaker/inference#installation-and-setup</a></p>\n\
          <h1 id=\"sagemaker-config\">sagemaker config</h1>\n<pre><code class=\"language-python\"\
          >instance_type = <span class=\"hljs-string\">\"ml.g5.2xlarge\"</span>\n\
          number_of_gpu = <span class=\"hljs-number\">1</span>\nhealth_check_timeout\
          \ = <span class=\"hljs-number\">300</span>\nquantize=<span class=\"hljs-string\"\
          >\"gptq\"</span>\nnum_shard=<span class=\"hljs-number\">1</span>\nbits=\
          \ <span class=\"hljs-number\">4</span>\ngroup_size= <span class=\"hljs-number\"\
          >128</span>\nrevision = <span class=\"hljs-string\">'main'</span> <span\
          \ class=\"hljs-comment\"># branch of repo, related with HF_MODEL_ID </span>\n\
          </code></pre>\n<h1 id=\"define-model-and-endpoint-configuration-parameter\"\
          >Define Model and Endpoint configuration parameter</h1>\n<pre><code class=\"\
          language-python\">config = {\n    <span class=\"hljs-string\">'HF_MODEL_ID'</span>:<span\
          \ class=\"hljs-string\">'TheBloke/Llama-2-13B-chat-GPTQ'</span>,\n    <span\
          \ class=\"hljs-string\">'SM_NUM_GPUS'</span>: json.dumps(number_of_gpu),\n\
          \    <span class=\"hljs-string\">'QUANTIZE'</span>:  quantize,\n    <span\
          \ class=\"hljs-string\">'NUM_SHARD'</span>:json.dumps(num_shard),\n    <span\
          \ class=\"hljs-string\">'GPTQ_BITS'</span>: json.dumps(bits),\n    <span\
          \ class=\"hljs-string\">'GPTQ_GROUPSIZE'</span>: json.dumps(group_size),\n\
          \    <span class=\"hljs-string\">'REVISION'</span>: revision\n}\n</code></pre>\n\
          <h1 id=\"create-huggingfacemodel-with-the-image-uri\">create HuggingFaceModel\
          \ with the image uri</h1>\n<pre><code class=\"language-python\">\nllm_model\
          \ = HuggingFaceModel(role=role, image_uri=llm_image, env=hub, sagemaker_session=sess)\n\
          </code></pre>\n<h1 id=\"deploy-model-to-an-endpoint\">Deploy model to an\
          \ endpoint</h1>\n<h1 id=\"httpssagemakerreadthedocsioenstableapiinferencemodelhtmlsagemakermodelmodeldeploy\"\
          ><a rel=\"nofollow\" href=\"https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\"\
          >https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy</a></h1>\n\
          <pre><code class=\"language-python\">llm = llm_model.deploy(\n    initial_instance_count=<span\
          \ class=\"hljs-number\">1</span>,\n    instance_type=instance_type,\n  \
          \  container_startup_health_check_timeout=health_check_timeout,  <span class=\"\
          hljs-comment\"># 10 minutes to be able to load the model</span>\n)\n</code></pre>\n"
        raw: "\nHi there,\n\nIt is possible to deploy the model using TGI and sagemaker.\
          \ You just have to adapt the configuration.\n\nHere is an example of how\
          \ to deploy the main branch to sagemaker:\n```python\nfrom sagemaker.huggingface\
          \ import get_huggingface_llm_image_uri\n\nllm_image = get_huggingface_llm_image_uri(\"\
          huggingface\", version=\"0.9.3\", session=sess)\n```\nIf you don't know\
          \ how to set up a session, just check out this resource: https://huggingface.co/docs/sagemaker/inference#installation-and-setup\n\
          \n# sagemaker config\n```python\ninstance_type = \"ml.g5.2xlarge\"\nnumber_of_gpu\
          \ = 1\nhealth_check_timeout = 300\nquantize=\"gptq\"\nnum_shard=1\nbits=\
          \ 4\ngroup_size= 128\nrevision = 'main' # branch of repo, related with HF_MODEL_ID\
          \ \n```\n# Define Model and Endpoint configuration parameter\n```python\n\
          config = {\n    'HF_MODEL_ID':'TheBloke/Llama-2-13B-chat-GPTQ',\n    'SM_NUM_GPUS':\
          \ json.dumps(number_of_gpu),\n    'QUANTIZE':  quantize,\n    'NUM_SHARD':json.dumps(num_shard),\n\
          \    'GPTQ_BITS': json.dumps(bits),\n    'GPTQ_GROUPSIZE': json.dumps(group_size),\n\
          \    'REVISION': revision\n}\n```\n\n# create HuggingFaceModel with the\
          \ image uri\n\n```python\n\nllm_model = HuggingFaceModel(role=role, image_uri=llm_image,\
          \ env=hub, sagemaker_session=sess)\n\n```\n# Deploy model to an endpoint\n\
          # https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n\
          ```python\nllm = llm_model.deploy(\n    initial_instance_count=1,\n    instance_type=instance_type,\n\
          \    container_startup_health_check_timeout=health_check_timeout,  # 10\
          \ minutes to be able to load the model\n)\n```\n"
        updatedAt: '2023-09-08T15:42:47.717Z'
      numEdits: 5
      reactions: []
    id: 64fb3a0a010f41e4352c80bf
    type: comment
  author: fa8ian
  content: "\nHi there,\n\nIt is possible to deploy the model using TGI and sagemaker.\
    \ You just have to adapt the configuration.\n\nHere is an example of how to deploy\
    \ the main branch to sagemaker:\n```python\nfrom sagemaker.huggingface import\
    \ get_huggingface_llm_image_uri\n\nllm_image = get_huggingface_llm_image_uri(\"\
    huggingface\", version=\"0.9.3\", session=sess)\n```\nIf you don't know how to\
    \ set up a session, just check out this resource: https://huggingface.co/docs/sagemaker/inference#installation-and-setup\n\
    \n# sagemaker config\n```python\ninstance_type = \"ml.g5.2xlarge\"\nnumber_of_gpu\
    \ = 1\nhealth_check_timeout = 300\nquantize=\"gptq\"\nnum_shard=1\nbits= 4\ngroup_size=\
    \ 128\nrevision = 'main' # branch of repo, related with HF_MODEL_ID \n```\n# Define\
    \ Model and Endpoint configuration parameter\n```python\nconfig = {\n    'HF_MODEL_ID':'TheBloke/Llama-2-13B-chat-GPTQ',\n\
    \    'SM_NUM_GPUS': json.dumps(number_of_gpu),\n    'QUANTIZE':  quantize,\n \
    \   'NUM_SHARD':json.dumps(num_shard),\n    'GPTQ_BITS': json.dumps(bits),\n \
    \   'GPTQ_GROUPSIZE': json.dumps(group_size),\n    'REVISION': revision\n}\n```\n\
    \n# create HuggingFaceModel with the image uri\n\n```python\n\nllm_model = HuggingFaceModel(role=role,\
    \ image_uri=llm_image, env=hub, sagemaker_session=sess)\n\n```\n# Deploy model\
    \ to an endpoint\n# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n\
    ```python\nllm = llm_model.deploy(\n    initial_instance_count=1,\n    instance_type=instance_type,\n\
    \    container_startup_health_check_timeout=health_check_timeout,  # 10 minutes\
    \ to be able to load the model\n)\n```\n"
  created_at: 2023-09-08 14:13:14+00:00
  edited: true
  hidden: false
  id: 64fb3a0a010f41e4352c80bf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 38
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Deploying this on Text Generation Inference (TGI) server on AWS SageMaker
