!!python/object:huggingface_hub.community.DiscussionWithDetails
author: amitj
conflicting_files: null
created_at: 2023-07-20 16:49:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/23ae3018512739242870d785f35fdc27.svg
      fullname: Amit Jain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: amitj
      type: user
    createdAt: '2023-07-20T17:49:05.000Z'
    data:
      edited: false
      editors:
      - amitj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.18830890953540802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/23ae3018512739242870d785f35fdc27.svg
          fullname: Amit Jain
          isHf: false
          isPro: false
          name: amitj
          type: user
        html: "<p>I keep getting an error while loading <code>gptq_model-8bit-128g.safetensors</code>\
          \ with revision <code>gptq-8bit-128g-actorder_False</code>, without <code>revision</code>\
          \ the 4bit model from the main branch loads fine. Have updated autogptq\
          \ -&gt; 0.3.0</p>\n<pre><code>\u2502 \u2771  65 \u2502   \u2502   \u2502\
          \   model = AutoGPTQForCausalLM.from_quantized(                        \
          \            \u2502\n\u2502    66 \u2502   \u2502   \u2502   \u2502   model_id,\
          \                                                                  \u2502\
          \n\u2502    67 \u2502   \u2502   \u2502   \u2502   revision=revision,  \
          \                                                       \u2502\n\u2502 \
          \   68 \u2502   \u2502   \u2502   \u2502   model_basename=model_basename,\
          \                                             \u2502\n\u2502           \
          \                                                                      \
          \                 \u2502\n\u2502 /home/a/.local/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:94\
          \ in              \u2502\n\u2502 from_quantized                        \
          \                                                           \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502    91 \u2502   \u2502   \u2502\
          \   for key in signature(quant_func).parameters                        \
          \            \u2502\n\u2502    92 \u2502   \u2502   \u2502   if key in kwargs\
          \                                                               \u2502\n\
          \u2502    93 \u2502   \u2502   }                                       \
          \                                           \u2502\n\u2502 \u2771  94 \u2502\
          \   \u2502   return quant_func(                                        \
          \                         \u2502\n\u2502    95 \u2502   \u2502   \u2502\
          \   model_name_or_path=model_name_or_path,                             \
          \            \u2502\n\u2502    96 \u2502   \u2502   \u2502   save_dir=save_dir,\
          \                                                             \u2502\n\u2502\
          \    97 \u2502   \u2502   \u2502   device_map=device_map,              \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502 /home/a/.local/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:714\
          \ in            \u2502\n\u2502 from_quantized                          \
          \                                                         \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502   711 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   break                                            \
          \                      \u2502\n\u2502   712 \u2502   \u2502            \
          \                                                                      \
          \    \u2502\n\u2502   713 \u2502   \u2502   if resolved_archive_file is\
          \ None: # Could not find a model file to use             \u2502\n\u2502\
          \ \u2771 714 \u2502   \u2502   \u2502   raise FileNotFoundError(f\"Could\
          \ not find model in {model_name_or_path}\")       \u2502\n\u2502   715 \u2502\
          \   \u2502                                                             \
          \                         \u2502\n\u2502   716 \u2502   \u2502   model_save_name\
          \ = resolved_archive_file                                            \u2502\
          \n\u2502   717                                                         \
          \                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nFileNotFoundError:\
          \ Could not find model in TheBloke/Llama-2-13B-chat-GPTQ\n</code></pre>\n"
        raw: "I keep getting an error while loading `gptq_model-8bit-128g.safetensors`\
          \ with revision `gptq-8bit-128g-actorder_False`, without `revision` the\
          \ 4bit model from the main branch loads fine. Have updated autogptq -> 0.3.0\r\
          \n\r\n```\r\n\u2502 \u2771  65 \u2502   \u2502   \u2502   model = AutoGPTQForCausalLM.from_quantized(\
          \                                    \u2502\r\n\u2502    66 \u2502   \u2502\
          \   \u2502   \u2502   model_id,                                        \
          \                          \u2502\r\n\u2502    67 \u2502   \u2502   \u2502\
          \   \u2502   revision=revision,                                        \
          \                 \u2502\r\n\u2502    68 \u2502   \u2502   \u2502   \u2502\
          \   model_basename=model_basename,                                     \
          \        \u2502\r\n\u2502                                              \
          \                                                    \u2502\r\n\u2502 /home/a/.local/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:94\
          \ in              \u2502\r\n\u2502 from_quantized                      \
          \                                                             \u2502\r\n\
          \u2502                                                                 \
          \                                 \u2502\r\n\u2502    91 \u2502   \u2502\
          \   \u2502   for key in signature(quant_func).parameters               \
          \                     \u2502\r\n\u2502    92 \u2502   \u2502   \u2502  \
          \ if key in kwargs                                                     \
          \          \u2502\r\n\u2502    93 \u2502   \u2502   }                  \
          \                                                                \u2502\r\
          \n\u2502 \u2771  94 \u2502   \u2502   return quant_func(               \
          \                                                  \u2502\r\n\u2502    95\
          \ \u2502   \u2502   \u2502   model_name_or_path=model_name_or_path,    \
          \                                     \u2502\r\n\u2502    96 \u2502   \u2502\
          \   \u2502   save_dir=save_dir,                                        \
          \                     \u2502\r\n\u2502    97 \u2502   \u2502   \u2502  \
          \ device_map=device_map,                                               \
          \          \u2502\r\n\u2502                                            \
          \                                                      \u2502\r\n\u2502\
          \ /home/a/.local/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:714\
          \ in            \u2502\r\n\u2502 from_quantized                        \
          \                                                           \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502   711 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   break                                            \
          \                      \u2502\r\n\u2502   712 \u2502   \u2502          \
          \                                                                      \
          \      \u2502\r\n\u2502   713 \u2502   \u2502   if resolved_archive_file\
          \ is None: # Could not find a model file to use             \u2502\r\n\u2502\
          \ \u2771 714 \u2502   \u2502   \u2502   raise FileNotFoundError(f\"Could\
          \ not find model in {model_name_or_path}\")       \u2502\r\n\u2502   715\
          \ \u2502   \u2502                                                      \
          \                                \u2502\r\n\u2502   716 \u2502   \u2502\
          \   model_save_name = resolved_archive_file                            \
          \                \u2502\r\n\u2502   717                                \
          \                                                            \u2502\r\n\u2570\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256F\r\nFileNotFoundError: Could not find model in TheBloke/Llama-2-13B-chat-GPTQ\r\
          \n```"
        updatedAt: '2023-07-20T17:49:05.604Z'
      numEdits: 0
      reactions: []
    id: 64b97391d4463c3d217ebaca
    type: comment
  author: amitj
  content: "I keep getting an error while loading `gptq_model-8bit-128g.safetensors`\
    \ with revision `gptq-8bit-128g-actorder_False`, without `revision` the 4bit model\
    \ from the main branch loads fine. Have updated autogptq -> 0.3.0\r\n\r\n```\r\
    \n\u2502 \u2771  65 \u2502   \u2502   \u2502   model = AutoGPTQForCausalLM.from_quantized(\
    \                                    \u2502\r\n\u2502    66 \u2502   \u2502  \
    \ \u2502   \u2502   model_id,                                                \
    \                  \u2502\r\n\u2502    67 \u2502   \u2502   \u2502   \u2502  \
    \ revision=revision,                                                         \u2502\
    \r\n\u2502    68 \u2502   \u2502   \u2502   \u2502   model_basename=model_basename,\
    \                                             \u2502\r\n\u2502               \
    \                                                                            \
    \       \u2502\r\n\u2502 /home/a/.local/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:94\
    \ in              \u2502\r\n\u2502 from_quantized                            \
    \                                                       \u2502\r\n\u2502     \
    \                                                                            \
    \                 \u2502\r\n\u2502    91 \u2502   \u2502   \u2502   for key in\
    \ signature(quant_func).parameters                                    \u2502\r\
    \n\u2502    92 \u2502   \u2502   \u2502   if key in kwargs                   \
    \                                            \u2502\r\n\u2502    93 \u2502   \u2502\
    \   }                                                                        \
    \          \u2502\r\n\u2502 \u2771  94 \u2502   \u2502   return quant_func(  \
    \                                                               \u2502\r\n\u2502\
    \    95 \u2502   \u2502   \u2502   model_name_or_path=model_name_or_path,    \
    \                                     \u2502\r\n\u2502    96 \u2502   \u2502 \
    \  \u2502   save_dir=save_dir,                                               \
    \              \u2502\r\n\u2502    97 \u2502   \u2502   \u2502   device_map=device_map,\
    \                                                         \u2502\r\n\u2502   \
    \                                                                            \
    \                   \u2502\r\n\u2502 /home/a/.local/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:714\
    \ in            \u2502\r\n\u2502 from_quantized                              \
    \                                                     \u2502\r\n\u2502       \
    \                                                                            \
    \               \u2502\r\n\u2502   711 \u2502   \u2502   \u2502   \u2502   \u2502\
    \   break                                                                  \u2502\
    \r\n\u2502   712 \u2502   \u2502                                             \
    \                                         \u2502\r\n\u2502   713 \u2502   \u2502\
    \   if resolved_archive_file is None: # Could not find a model file to use   \
    \          \u2502\r\n\u2502 \u2771 714 \u2502   \u2502   \u2502   raise FileNotFoundError(f\"\
    Could not find model in {model_name_or_path}\")       \u2502\r\n\u2502   715 \u2502\
    \   \u2502                                                                   \
    \                   \u2502\r\n\u2502   716 \u2502   \u2502   model_save_name =\
    \ resolved_archive_file                                            \u2502\r\n\u2502\
    \   717                                                                      \
    \                      \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u256F\r\nFileNotFoundError: Could not find model in TheBloke/Llama-2-13B-chat-GPTQ\r\
    \n```"
  created_at: 2023-07-20 16:49:05+00:00
  edited: false
  hidden: false
  id: 64b97391d4463c3d217ebaca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-20T19:10:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.760725200176239
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Did you update the basename correctly for the file in the new branch?   The
          <code>model_basename</code> is set to the name of the file <em>without</em>
          <code>.safetensors</code>.  So in this example it should be <code>gptq_model-8bit-128g</code></p>

          '
        raw: Did you update the basename correctly for the file in the new branch?   The
          `model_basename` is set to the name of the file *without* `.safetensors`.  So
          in this example it should be `gptq_model-8bit-128g`
        updatedAt: '2023-07-20T19:10:15.337Z'
      numEdits: 0
      reactions: []
    id: 64b9869782975e7c6019b746
    type: comment
  author: TheBloke
  content: Did you update the basename correctly for the file in the new branch?   The
    `model_basename` is set to the name of the file *without* `.safetensors`.  So
    in this example it should be `gptq_model-8bit-128g`
  created_at: 2023-07-20 18:10:15+00:00
  edited: false
  hidden: false
  id: 64b9869782975e7c6019b746
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/23ae3018512739242870d785f35fdc27.svg
      fullname: Amit Jain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: amitj
      type: user
    createdAt: '2023-07-21T05:43:53.000Z'
    data:
      edited: false
      editors:
      - amitj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9265271425247192
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/23ae3018512739242870d785f35fdc27.svg
          fullname: Amit Jain
          isHf: false
          isPro: false
          name: amitj
          type: user
        html: '<p>Yes I am removing the .safetensors extension. The behavior is as
          if the <code>revision</code> branch is not honored.</p>

          '
        raw: Yes I am removing the .safetensors extension. The behavior is as if the
          `revision` branch is not honored.
        updatedAt: '2023-07-21T05:43:53.270Z'
      numEdits: 0
      reactions: []
    id: 64ba1b19842aa47891ccc717
    type: comment
  author: amitj
  content: Yes I am removing the .safetensors extension. The behavior is as if the
    `revision` branch is not honored.
  created_at: 2023-07-21 04:43:53+00:00
  edited: false
  hidden: false
  id: 64ba1b19842aa47891ccc717
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-21T09:04:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah damn you''re right, it''s not using <code>revision</code> for
          some reason. It''s an AutoGPTQ bug but I can''t immediately see what''s
          wrong. I will keep investigating</p>

          '
        raw: Yeah damn you're right, it's not using `revision` for some reason. It's
          an AutoGPTQ bug but I can't immediately see what's wrong. I will keep investigating
        updatedAt: '2023-07-21T09:04:20.328Z'
      numEdits: 0
      reactions: []
    id: 64ba4a147f7c3e3e4dda1142
    type: comment
  author: TheBloke
  content: Yeah damn you're right, it's not using `revision` for some reason. It's
    an AutoGPTQ bug but I can't immediately see what's wrong. I will keep investigating
  created_at: 2023-07-21 08:04:20+00:00
  edited: false
  hidden: false
  id: 64ba4a147f7c3e3e4dda1142
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5811acbb9129a0e1df4554f798911080.svg
      fullname: Tino
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TiZott
      type: user
    createdAt: '2023-07-28T16:45:11.000Z'
    data:
      edited: false
      editors:
      - TiZott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6929946541786194
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5811acbb9129a0e1df4554f798911080.svg
          fullname: Tino
          isHf: false
          isPro: false
          name: TiZott
          type: user
        html: '<p>I could not load that revision either. My fix may be related:<br><a
          rel="nofollow" href="https://github.com/TheBloke/AutoGPTQ/blob/45576f0933f5e9ef7c1617006d5db359e1669155/auto_gptq/modeling/_base.py#L666C95-L666C95">https://github.com/TheBloke/AutoGPTQ/blob/45576f0933f5e9ef7c1617006d5db359e1669155/auto_gptq/modeling/_base.py#L666C95-L666C95</a><br>That
          kwargs got popped empty, so it defaults to 4bit. If i change that into cached_file_kwargs
          it still keeps warning about the safetensors, but inferences just fine.</p>

          '
        raw: 'I could not load that revision either. My fix may be related:

          https://github.com/TheBloke/AutoGPTQ/blob/45576f0933f5e9ef7c1617006d5db359e1669155/auto_gptq/modeling/_base.py#L666C95-L666C95

          That kwargs got popped empty, so it defaults to 4bit. If i change that into
          cached_file_kwargs it still keeps warning about the safetensors, but inferences
          just fine.'
        updatedAt: '2023-07-28T16:45:11.824Z'
      numEdits: 0
      reactions: []
    id: 64c3f097062585c47fe0e7f6
    type: comment
  author: TiZott
  content: 'I could not load that revision either. My fix may be related:

    https://github.com/TheBloke/AutoGPTQ/blob/45576f0933f5e9ef7c1617006d5db359e1669155/auto_gptq/modeling/_base.py#L666C95-L666C95

    That kwargs got popped empty, so it defaults to 4bit. If i change that into cached_file_kwargs
    it still keeps warning about the safetensors, but inferences just fine.'
  created_at: 2023-07-28 15:45:11+00:00
  edited: false
  hidden: false
  id: 64c3f097062585c47fe0e7f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-28T16:50:13.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9787077307701111
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The bug with revision was fixed in 0.3.2, please update and it will
          work fine</p>

          <p>The warning about the safetensors metadata is also fine, and won''t appear
          for future GPTQs I make. That was also fixed in 0.3.2 (now metadata is saved
          into each GPTQ to prevent that warning)</p>

          '
        raw: 'The bug with revision was fixed in 0.3.2, please update and it will
          work fine


          The warning about the safetensors metadata is also fine, and won''t appear
          for future GPTQs I make. That was also fixed in 0.3.2 (now metadata is saved
          into each GPTQ to prevent that warning)'
        updatedAt: '2023-07-28T16:50:13.572Z'
      numEdits: 0
      reactions: []
    id: 64c3f1c577a6473ac3cede02
    type: comment
  author: TheBloke
  content: 'The bug with revision was fixed in 0.3.2, please update and it will work
    fine


    The warning about the safetensors metadata is also fine, and won''t appear for
    future GPTQs I make. That was also fixed in 0.3.2 (now metadata is saved into
    each GPTQ to prevent that warning)'
  created_at: 2023-07-28 15:50:13+00:00
  edited: false
  hidden: false
  id: 64c3f1c577a6473ac3cede02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5811acbb9129a0e1df4554f798911080.svg
      fullname: Tino
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TiZott
      type: user
    createdAt: '2023-07-28T16:59:03.000Z'
    data:
      edited: false
      editors:
      - TiZott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5130701065063477
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5811acbb9129a0e1df4554f798911080.svg
          fullname: Tino
          isHf: false
          isPro: false
          name: TiZott
          type: user
        html: '<p>Oh i see. Thanks for the quick answer. I just noticed that pip downloaded
          0.3.1 because of this error:<br>Discarding <a rel="nofollow" href="https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz">https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz</a>
          (from <a rel="nofollow" href="https://pypi.org/simple/auto-gptq/">https://pypi.org/simple/auto-gptq/</a>)
          (requires-python:&gt;=3.8.0): Requested auto-gptq from <a rel="nofollow"
          href="https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz">https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz</a>
          has inconsistent version: expected ''0.3.2'', but metadata has ''0.3.2+cu117''</p>

          '
        raw: 'Oh i see. Thanks for the quick answer. I just noticed that pip downloaded
          0.3.1 because of this error:

          Discarding https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz
          (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0): Requested
          auto-gptq from https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz
          has inconsistent version: expected ''0.3.2'', but metadata has ''0.3.2+cu117'''
        updatedAt: '2023-07-28T16:59:03.102Z'
      numEdits: 0
      reactions: []
    id: 64c3f3d7e2e5c94bd01daa00
    type: comment
  author: TiZott
  content: 'Oh i see. Thanks for the quick answer. I just noticed that pip downloaded
    0.3.1 because of this error:

    Discarding https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz
    (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0): Requested
    auto-gptq from https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz
    has inconsistent version: expected ''0.3.2'', but metadata has ''0.3.2+cu117'''
  created_at: 2023-07-28 15:59:03+00:00
  edited: false
  hidden: false
  id: 64c3f3d7e2e5c94bd01daa00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-28T17:09:55.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9363778233528137
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah that''s a bug in AutoGPTQ at the moment, should be fixed this
          weekend.  The revision issue was fixed in 0.3.1 and then 0.3.2 was another
          change, so 0.3.1 should work fine with revision too</p>

          '
        raw: Yeah that's a bug in AutoGPTQ at the moment, should be fixed this weekend.  The
          revision issue was fixed in 0.3.1 and then 0.3.2 was another change, so
          0.3.1 should work fine with revision too
        updatedAt: '2023-07-28T17:09:55.397Z'
      numEdits: 0
      reactions: []
    id: 64c3f663a3f7a8107d9358bd
    type: comment
  author: TheBloke
  content: Yeah that's a bug in AutoGPTQ at the moment, should be fixed this weekend.  The
    revision issue was fixed in 0.3.1 and then 0.3.2 was another change, so 0.3.1
    should work fine with revision too
  created_at: 2023-07-28 16:09:55+00:00
  edited: false
  hidden: false
  id: 64c3f663a3f7a8107d9358bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5811acbb9129a0e1df4554f798911080.svg
      fullname: Tino
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TiZott
      type: user
    createdAt: '2023-07-28T17:12:44.000Z'
    data:
      edited: false
      editors:
      - TiZott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8677524924278259
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5811acbb9129a0e1df4554f798911080.svg
          fullname: Tino
          isHf: false
          isPro: false
          name: TiZott
          type: user
        html: '<p>Alright, thanks again. Great work btw! )</p>

          '
        raw: Alright, thanks again. Great work btw! )
        updatedAt: '2023-07-28T17:12:44.685Z'
      numEdits: 0
      reactions: []
    id: 64c3f70c77a6473ac3cf8b93
    type: comment
  author: TiZott
  content: Alright, thanks again. Great work btw! )
  created_at: 2023-07-28 16:12:44+00:00
  edited: false
  hidden: false
  id: 64c3f70c77a6473ac3cf8b93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c0320ef100e11eabd0bb303a2eac6076.svg
      fullname: 'Tahsin Elahi Navin '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tnavin
      type: user
    createdAt: '2023-09-29T06:13:23.000Z'
    data:
      edited: false
      editors:
      - tnavin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6269985437393188
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c0320ef100e11eabd0bb303a2eac6076.svg
          fullname: 'Tahsin Elahi Navin '
          isHf: false
          isPro: false
          name: tnavin
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ , has this been fixed? I'm also getting the same error. </p>\n<p>\"\"\"\
          <br>from transformers import AutoTokenizer, pipeline, logging<br>from auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig<br>from huggingface_hub\
          \ import snapshot_download</p>\n<p>model_name = \"TheBloke/Llama-2-13B-chat-GPTQ\"\
          <br>local_folder = \"/home/n/resume-parser/llama2/13b\"</p>\n<p>snapshot_download(repo_id=model_name,\
          \ local_dir=local_folder, local_dir_use_symlinks=False)</p>\n<p>model_basename\
          \ = \"gptq_model-4bit-128g\"</p>\n<p>use_triton = False</p>\n<p>tokenizer\
          \ = AutoTokenizer.from_pretrained(local_folder, use_fast=True)</p>\n<p>model\
          \ = AutoGPTQForCausalLM.from_quantized(local_folder,<br>model_basename=model_basename,<br>use_safetensors=True,<br>trust_remote_code=True,<br>device=\"\
          cuda:0\",<br>use_triton=use_triton,<br>quantize_config=None)</p>\n<p>input_ids\
          \ = tokenizer(\"Llamas are\", return_tensors='pt').input_ids.cuda()<br>output\
          \ = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)<br>print(tokenizer.decode(output[0]))</p>\n\
          <p>\"\"\"</p>\n<p>ERROR:</p>\n<p>Exllama kernel is not installed, reset\
          \ disable_exllama to True. This may because you installed auto_gptq using\
          \ a pre-build wheel on Windows, in which exllama_kernels are not compiled.\
          \ To use exllama_kernels to further speedup inference, you can re-install\
          \ auto_gptq from source.<br>CUDA kernels for auto_gptq are not installed,\
          \ this will result in very slow inference speed. This may because:</p>\n\
          <ol>\n<li>You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0\
          \ when install auto_gptq from source.</li>\n<li>You are using pytorch without\
          \ CUDA support.</li>\n<li>CUDA and nvcc are not installed in your device.<br>Traceback\
          \ (most recent call last):<br>  File \"/home/n/resume-parser/main.py\",\
          \ line 16, in <br> model = AutoGPTQForCausalLM.from_quantized(local_folder,<br>\
          \  File \"/home/n/anaconda3/envs/resume-parser/lib/python3.10/site-packages/auto_gptq/modeling/auto.py\"\
          , line 108, in from_quantized<br> return quant_func(<br>  File \"/home/n/anaconda3/envs/resume-parser/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          , line 791, in from_quantized<br> raise FileNotFoundError(f\"Could not find\
          \ model in {model_name_or_path}\")<br>FileNotFoundError: Could not find\
          \ model in /home/n/resume-parser/llama2/13b</li>\n</ol>\n<p>I am using CUDA-11.7\
          \ and Python 3.10</p>\n"
        raw: "Hello @TheBloke , has this been fixed? I'm also getting the same error.\
          \ \n\n\"\"\" \nfrom transformers import AutoTokenizer, pipeline, logging\n\
          from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom huggingface_hub\
          \ import snapshot_download\n\nmodel_name = \"TheBloke/Llama-2-13B-chat-GPTQ\"\
          \nlocal_folder = \"/home/n/resume-parser/llama2/13b\"\n\nsnapshot_download(repo_id=model_name,\
          \ local_dir=local_folder, local_dir_use_symlinks=False)\n\nmodel_basename\
          \ = \"gptq_model-4bit-128g\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(local_folder,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(local_folder,\n\
          model_basename=model_basename,\nuse_safetensors=True,\ntrust_remote_code=True,\n\
          device=\"cuda:0\",\nuse_triton=use_triton,\nquantize_config=None)\n\ninput_ids\
          \ = tokenizer(\"Llamas are\", return_tensors='pt').input_ids.cuda()\noutput\
          \ = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n\n\"\"\"\n\n\nERROR:\n\nExllama kernel\
          \ is not installed, reset disable_exllama to True. This may because you\
          \ installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels\
          \ are not compiled. To use exllama_kernels to further speedup inference,\
          \ you can re-install auto_gptq from source.\nCUDA kernels for auto_gptq\
          \ are not installed, this will result in very slow inference speed. This\
          \ may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0\
          \ when install auto_gptq from source.\n2. You are using pytorch without\
          \ CUDA support.\n3. CUDA and nvcc are not installed in your device.\nTraceback\
          \ (most recent call last):\n  File \"/home/n/resume-parser/main.py\", line\
          \ 16, in <module>\n    model = AutoGPTQForCausalLM.from_quantized(local_folder,\n\
          \  File \"/home/n/anaconda3/envs/resume-parser/lib/python3.10/site-packages/auto_gptq/modeling/auto.py\"\
          , line 108, in from_quantized\n    return quant_func(\n  File \"/home/n/anaconda3/envs/resume-parser/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          , line 791, in from_quantized\n    raise FileNotFoundError(f\"Could not\
          \ find model in {model_name_or_path}\")\nFileNotFoundError: Could not find\
          \ model in /home/n/resume-parser/llama2/13b\n\n\nI am using CUDA-11.7 and\
          \ Python 3.10\n"
        updatedAt: '2023-09-29T06:13:23.393Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F614"
        users:
        - FabDefo
        - voxmenthe
    id: 65166b039a03ccd89938e8c5
    type: comment
  author: tnavin
  content: "Hello @TheBloke , has this been fixed? I'm also getting the same error.\
    \ \n\n\"\"\" \nfrom transformers import AutoTokenizer, pipeline, logging\nfrom\
    \ auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom huggingface_hub\
    \ import snapshot_download\n\nmodel_name = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n\
    local_folder = \"/home/n/resume-parser/llama2/13b\"\n\nsnapshot_download(repo_id=model_name,\
    \ local_dir=local_folder, local_dir_use_symlinks=False)\n\nmodel_basename = \"\
    gptq_model-4bit-128g\"\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(local_folder,\
    \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(local_folder,\n\
    model_basename=model_basename,\nuse_safetensors=True,\ntrust_remote_code=True,\n\
    device=\"cuda:0\",\nuse_triton=use_triton,\nquantize_config=None)\n\ninput_ids\
    \ = tokenizer(\"Llamas are\", return_tensors='pt').input_ids.cuda()\noutput =\
    \ model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\
    \n\"\"\"\n\n\nERROR:\n\nExllama kernel is not installed, reset disable_exllama\
    \ to True. This may because you installed auto_gptq using a pre-build wheel on\
    \ Windows, in which exllama_kernels are not compiled. To use exllama_kernels to\
    \ further speedup inference, you can re-install auto_gptq from source.\nCUDA kernels\
    \ for auto_gptq are not installed, this will result in very slow inference speed.\
    \ This may because:\n1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0\
    \ when install auto_gptq from source.\n2. You are using pytorch without CUDA support.\n\
    3. CUDA and nvcc are not installed in your device.\nTraceback (most recent call\
    \ last):\n  File \"/home/n/resume-parser/main.py\", line 16, in <module>\n   \
    \ model = AutoGPTQForCausalLM.from_quantized(local_folder,\n  File \"/home/n/anaconda3/envs/resume-parser/lib/python3.10/site-packages/auto_gptq/modeling/auto.py\"\
    , line 108, in from_quantized\n    return quant_func(\n  File \"/home/n/anaconda3/envs/resume-parser/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
    , line 791, in from_quantized\n    raise FileNotFoundError(f\"Could not find model\
    \ in {model_name_or_path}\")\nFileNotFoundError: Could not find model in /home/n/resume-parser/llama2/13b\n\
    \n\nI am using CUDA-11.7 and Python 3.10\n"
  created_at: 2023-09-29 05:13:23+00:00
  edited: false
  hidden: false
  id: 65166b039a03ccd89938e8c5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/Llama-2-13B-chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Error loading model from a different branch with revision
