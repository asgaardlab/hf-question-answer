!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Whitepaper
conflicting_files: null
created_at: 2023-03-22 15:03:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9a7d05af8bdbd53b38f4b805fd7c77f.svg
      fullname: Michael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Whitepaper
      type: user
    createdAt: '2023-03-22T16:03:09.000Z'
    data:
      edited: false
      editors:
      - Whitepaper
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9a7d05af8bdbd53b38f4b805fd7c77f.svg
          fullname: Michael
          isHf: false
          isPro: false
          name: Whitepaper
          type: user
        html: '<p>Hello. Can u retrain model with actual changes with tloen/alpaca-lora?</p>

          '
        raw: Hello. Can u retrain model with actual changes with tloen/alpaca-lora?
        updatedAt: '2023-03-22T16:03:09.377Z'
      numEdits: 0
      reactions: []
    id: 641b26bdaf42e9b7dd42e174
    type: comment
  author: Whitepaper
  content: Hello. Can u retrain model with actual changes with tloen/alpaca-lora?
  created_at: 2023-03-22 15:03:09+00:00
  edited: false
  hidden: false
  id: 641b26bdaf42e9b7dd42e174
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659971187637-60d3b57ad7b174177faabd6e.jpeg?w=200&h=200&f=face
      fullname: chansung park
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: chansung
      type: user
    createdAt: '2023-03-25T13:10:48.000Z'
    data:
      edited: false
      editors:
      - chansung
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659971187637-60d3b57ad7b174177faabd6e.jpeg?w=200&h=200&f=face
          fullname: chansung park
          isHf: false
          isPro: true
          name: chansung
          type: user
        html: '<p>It was trained on a initial version of cleaned dataset from tloen/alpaca-lora.
          I will refresh once the more of clean ups are done</p>

          '
        raw: It was trained on a initial version of cleaned dataset from tloen/alpaca-lora.
          I will refresh once the more of clean ups are done
        updatedAt: '2023-03-25T13:10:48.048Z'
      numEdits: 0
      reactions: []
    id: 641ef2d8dd15d15f8ed2e0b3
    type: comment
  author: chansung
  content: It was trained on a initial version of cleaned dataset from tloen/alpaca-lora.
    I will refresh once the more of clean ups are done
  created_at: 2023-03-25 12:10:48+00:00
  edited: false
  hidden: false
  id: 641ef2d8dd15d15f8ed2e0b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-03-28T12:45:11.000Z'
    data:
      edited: true
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<p>Awesome, from trying multiple versions I can tell that the cleaned
          data has a huge effect. The original Alpaca (not cleaned) would say "As
          a large language model blah blah consider the many factors blah blah", but
          the cleaned one does it a lot less. The final cleaned set should be even
          better good.</p>

          <p>I''ve been running the 30B model at 4bit (<a rel="nofollow" href="https://github.com/johnsmith0031/alpaca_lora_4bit">https://github.com/johnsmith0031/alpaca_lora_4bit</a>)
          and it''s fantastic to have it on my desktop.</p>

          <p>There are some interesting data sets out there (in order of quality)</p>

          <ul>

          <li>oasst 20k (pre-released from open assistant, see discord)</li>

          <li><a href="https://huggingface.co/datasets/stanfordnlp/SHP">https://huggingface.co/datasets/stanfordnlp/SHP</a>
          300k</li>

          <li><a rel="nofollow" href="https://github.com/hendrycks/ethics">https://github.com/hendrycks/ethics</a></li>

          <li><a href="https://huggingface.co/datasets/Anthropic/hh-rlhf">https://huggingface.co/datasets/Anthropic/hh-rlhf</a></li>

          <li><a rel="nofollow" href="https://github.com/allenai/natural-instructions">https://github.com/allenai/natural-instructions</a>
          64k</li>

          <li><a rel="nofollow" href="https://github.com/orhonovich/unnatural-instructions">https://github.com/orhonovich/unnatural-instructions</a></li>

          <li><a rel="nofollow" href="https://laion.ai/blog/oig-dataset/">https://laion.ai/blog/oig-dataset/</a></li>

          <li><a href="https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences">https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences</a></li>

          </ul>

          <p>Now I''m not suggesting you used them, I''m just sharing them out of
          interest. But the fine tuning approach from Anthropic seems to be </p>

          <ol>

          <li>fine tune on a large low quality instruction dataset</li>

          <li>fine tune on a small high quality instruction dataset</li>

          </ol>

          <p>But here we are skipping straight to a high quality dataset, and it works
          OK.</p>

          '
        raw: "Awesome, from trying multiple versions I can tell that the cleaned data\
          \ has a huge effect. The original Alpaca (not cleaned) would say \"As a\
          \ large language model blah blah consider the many factors blah blah\",\
          \ but the cleaned one does it a lot less. The final cleaned set should be\
          \ even better good.\n\nI've been running the 30B model at 4bit (https://github.com/johnsmith0031/alpaca_lora_4bit)\
          \ and it's fantastic to have it on my desktop.\n\nThere are some interesting\
          \ data sets out there (in order of quality)\n- oasst 20k (pre-released from\
          \ open assistant, see discord)\n- https://huggingface.co/datasets/stanfordnlp/SHP\
          \ 300k\n- https://github.com/hendrycks/ethics\n- https://huggingface.co/datasets/Anthropic/hh-rlhf\n\
          - https://github.com/allenai/natural-instructions 64k\n- https://github.com/orhonovich/unnatural-instructions\n\
          - https://laion.ai/blog/oig-dataset/\n- https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences\n\
          \nNow I'm not suggesting you used them, I'm just sharing them out of interest.\
          \ But the fine tuning approach from Anthropic seems to be \n1) fine tune\
          \ on a large low quality instruction dataset\n2) fine tune on a small high\
          \ quality instruction dataset\n\nBut here we are skipping straight to a\
          \ high quality dataset, and it works OK."
        updatedAt: '2023-03-28T12:45:46.265Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - zhaoqf123
        - chansung
        - digitous
    id: 6422e1570b06c1a2b1f43bc9
    type: comment
  author: wassname
  content: "Awesome, from trying multiple versions I can tell that the cleaned data\
    \ has a huge effect. The original Alpaca (not cleaned) would say \"As a large\
    \ language model blah blah consider the many factors blah blah\", but the cleaned\
    \ one does it a lot less. The final cleaned set should be even better good.\n\n\
    I've been running the 30B model at 4bit (https://github.com/johnsmith0031/alpaca_lora_4bit)\
    \ and it's fantastic to have it on my desktop.\n\nThere are some interesting data\
    \ sets out there (in order of quality)\n- oasst 20k (pre-released from open assistant,\
    \ see discord)\n- https://huggingface.co/datasets/stanfordnlp/SHP 300k\n- https://github.com/hendrycks/ethics\n\
    - https://huggingface.co/datasets/Anthropic/hh-rlhf\n- https://github.com/allenai/natural-instructions\
    \ 64k\n- https://github.com/orhonovich/unnatural-instructions\n- https://laion.ai/blog/oig-dataset/\n\
    - https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences\n\n\
    Now I'm not suggesting you used them, I'm just sharing them out of interest. But\
    \ the fine tuning approach from Anthropic seems to be \n1) fine tune on a large\
    \ low quality instruction dataset\n2) fine tune on a small high quality instruction\
    \ dataset\n\nBut here we are skipping straight to a high quality dataset, and\
    \ it works OK."
  created_at: 2023-03-28 11:45:11+00:00
  edited: true
  hidden: false
  id: 6422e1570b06c1a2b1f43bc9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659971187637-60d3b57ad7b174177faabd6e.jpeg?w=200&h=200&f=face
      fullname: chansung park
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: chansung
      type: user
    createdAt: '2023-04-06T06:33:20.000Z'
    data:
      edited: false
      editors:
      - chansung
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659971187637-60d3b57ad7b174177faabd6e.jpeg?w=200&h=200&f=face
          fullname: chansung park
          isHf: false
          isPro: true
          name: chansung
          type: user
        html: '<p>The model is refreshed once again just today</p>

          '
        raw: The model is refreshed once again just today
        updatedAt: '2023-04-06T06:33:20.435Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - wassname
        - P4l1ndr0m
    id: 642e67b0bc17f2419d383d85
    type: comment
  author: chansung
  content: The model is refreshed once again just today
  created_at: 2023-04-06 05:33:20+00:00
  edited: false
  hidden: false
  id: 642e67b0bc17f2419d383d85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-04-07T09:36:43.000Z'
    data:
      edited: false
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<p>Thanks, there seems to be a quite a difference! It''s quite fun
          to play with</p>

          '
        raw: Thanks, there seems to be a quite a difference! It's quite fun to play
          with
        updatedAt: '2023-04-07T09:36:43.078Z'
      numEdits: 0
      reactions: []
    id: 642fe42bd08b9afcdeb054f4
    type: comment
  author: wassname
  content: Thanks, there seems to be a quite a difference! It's quite fun to play
    with
  created_at: 2023-04-07 08:36:43+00:00
  edited: false
  hidden: false
  id: 642fe42bd08b9afcdeb054f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0309f91429d4eca200572b9d260d9949.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Bloodofthedragon
      type: user
    createdAt: '2023-06-07T00:14:01.000Z'
    data:
      edited: false
      editors:
      - Bloodofthedragon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8359792828559875
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0309f91429d4eca200572b9d260d9949.svg
          fullname: None
          isHf: false
          isPro: false
          name: Bloodofthedragon
          type: user
        html: '<p>I want to fine tune a alpaca30b with your finetuned config on my
          custom dataset. I tried this:<br>model = LlamaForCausalLM.from_pretrained(<br>        base_model,<br>        load_in_8bit=True,<br>        device_map=device_map,<br>    )<br>    model
          = PeftModel.from_pretrained(model, "chansung/alpaca-lora-30b")<br>    tokenizer
          = LlamaTokenizer.from_pretrained(base_model)</p>

          <p>My scripts is just finetune.py from alpacalora, with that peft addition.</p>

          <p>Training runs, but during inference it doesnt seem to have any of your
          fine tuning present. Can you offer any help. Thanks for your work.</p>

          '
        raw: "I want to fine tune a alpaca30b with your finetuned config on my custom\
          \ dataset. I tried this:\nmodel = LlamaForCausalLM.from_pretrained(\n  \
          \      base_model,\n        load_in_8bit=True,\n        device_map=device_map,\n\
          \    )\n    model = PeftModel.from_pretrained(model, \"chansung/alpaca-lora-30b\"\
          )\n    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n\nMy scripts\
          \ is just finetune.py from alpacalora, with that peft addition.\n\nTraining\
          \ runs, but during inference it doesnt seem to have any of your fine tuning\
          \ present. Can you offer any help. Thanks for your work."
        updatedAt: '2023-06-07T00:14:01.515Z'
      numEdits: 0
      reactions: []
    id: 647fcbc9cbb8294ed80ce96a
    type: comment
  author: Bloodofthedragon
  content: "I want to fine tune a alpaca30b with your finetuned config on my custom\
    \ dataset. I tried this:\nmodel = LlamaForCausalLM.from_pretrained(\n        base_model,\n\
    \        load_in_8bit=True,\n        device_map=device_map,\n    )\n    model\
    \ = PeftModel.from_pretrained(model, \"chansung/alpaca-lora-30b\")\n    tokenizer\
    \ = LlamaTokenizer.from_pretrained(base_model)\n\nMy scripts is just finetune.py\
    \ from alpacalora, with that peft addition.\n\nTraining runs, but during inference\
    \ it doesnt seem to have any of your fine tuning present. Can you offer any help.\
    \ Thanks for your work."
  created_at: 2023-06-06 23:14:01+00:00
  edited: false
  hidden: false
  id: 647fcbc9cbb8294ed80ce96a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: chansung/alpaca-lora-30b
repo_type: model
status: open
target_branch: null
title: Refresh model
