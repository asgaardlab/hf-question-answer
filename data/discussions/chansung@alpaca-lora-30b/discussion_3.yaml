!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jlopez-dl
conflicting_files: null
created_at: 2023-04-13 05:02:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304d96941387c7f1178dd1c/FMg7fCKU9bhe_vMAv4tR-.jpeg?w=200&h=200&f=face
      fullname: Jose Lopez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jlopez-dl
      type: user
    createdAt: '2023-04-13T06:02:57.000Z'
    data:
      edited: true
      editors:
      - jlopez-dl
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304d96941387c7f1178dd1c/FMg7fCKU9bhe_vMAv4tR-.jpeg?w=200&h=200&f=face
          fullname: Jose Lopez
          isHf: false
          isPro: false
          name: jlopez-dl
          type: user
        html: '<p>Hi,<br>I have a 4xA100 (80GB) but I don''t manage to fine-tune it.
          I''m suffering some OOM.</p>

          <p>Could you please share your command?</p>

          <p>I''m using this one.</p>

          <p>PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:256" torchrun --nnodes=1 --nproc_per_node=4
          --master_port=3333 <br>    finetune.py <br>    --base_model "decapoda-research/llama-30b-hf"
          <br>    --data_path ''./alpaca_data.json'' <br>    --output_dir ''./lora-alpaca-30b-multi-gpu''
          <br>    --batch_size 128 <br>    --micro_batch_size 4 <br>    --num_epochs
          10 <br>    --cutoff_len 256 <br>    --val_set_size 0 <br>    --lora_r 8
          <br>    --lora_alpha 16 <br>    --lora_dropout 0.05 <br>    --lora_target_modules
          [''q_proj'',''k_proj'',''v_proj'',''o_proj''] <br>    --group_by_length</p>

          '
        raw: "Hi, \nI have a 4xA100 (80GB) but I don't manage to fine-tune it. I'm\
          \ suffering some OOM.\n\nCould you please share your command?\n\nI'm using\
          \ this one.\n\nPYTORCH_CUDA_ALLOC_CONF=\"max_split_size_mb:256\" torchrun\
          \ --nnodes=1 --nproc_per_node=4 --master_port=3333 \\\n    finetune.py \\\
          \n    --base_model \"decapoda-research/llama-30b-hf\" \\\n    --data_path\
          \ './alpaca_data.json' \\\n    --output_dir './lora-alpaca-30b-multi-gpu'\
          \ \\\n    --batch_size 128 \\\n    --micro_batch_size 4 \\\n    --num_epochs\
          \ 10 \\\n    --cutoff_len 256 \\\n    --val_set_size 0 \\\n    --lora_r\
          \ 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --lora_target_modules\
          \ ['q_proj','k_proj','v_proj','o_proj'] \\\n    --group_by_length"
        updatedAt: '2023-04-13T06:03:23.709Z'
      numEdits: 1
      reactions: []
    id: 64379b1170cb1a21df85030f
    type: comment
  author: jlopez-dl
  content: "Hi, \nI have a 4xA100 (80GB) but I don't manage to fine-tune it. I'm suffering\
    \ some OOM.\n\nCould you please share your command?\n\nI'm using this one.\n\n\
    PYTORCH_CUDA_ALLOC_CONF=\"max_split_size_mb:256\" torchrun --nnodes=1 --nproc_per_node=4\
    \ --master_port=3333 \\\n    finetune.py \\\n    --base_model \"decapoda-research/llama-30b-hf\"\
    \ \\\n    --data_path './alpaca_data.json' \\\n    --output_dir './lora-alpaca-30b-multi-gpu'\
    \ \\\n    --batch_size 128 \\\n    --micro_batch_size 4 \\\n    --num_epochs 10\
    \ \\\n    --cutoff_len 256 \\\n    --val_set_size 0 \\\n    --lora_r 8 \\\n  \
    \  --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --lora_target_modules ['q_proj','k_proj','v_proj','o_proj']\
    \ \\\n    --group_by_length"
  created_at: 2023-04-13 05:02:57+00:00
  edited: true
  hidden: false
  id: 64379b1170cb1a21df85030f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: chansung/alpaca-lora-30b
repo_type: model
status: open
target_branch: null
title: Command for finetuning in a multi gpu machine
