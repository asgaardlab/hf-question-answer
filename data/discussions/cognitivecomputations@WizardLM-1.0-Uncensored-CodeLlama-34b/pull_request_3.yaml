!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheBloke
conflicting_files: []
created_at: 2023-09-05 18:34:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-05T19:34:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11044929921627045
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: ''
        raw: ''
        updatedAt: '2023-09-05T19:34:38.633Z'
      numEdits: 0
      reactions: []
    id: 64f782cef54762997475a1dc
    type: comment
  author: TheBloke
  content: ''
  created_at: 2023-09-05 18:34:38+00:00
  edited: false
  hidden: false
  id: 64f782cef54762997475a1dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-05T19:34:39.000Z'
    data:
      oid: 1a41a8c0a56a9894057366eec405db87443a9fa0
      parents:
      - 3e8df2cf4a4ee1c0b2d079cb7be70024d425ea8c
      subject: Shouldn't CodeLlama 34B have 16K context and rope_theta 1M?
    id: 64f782cf0000000000000000
    type: commit
  author: TheBloke
  created_at: 2023-09-05 18:34:39+00:00
  id: 64f782cf0000000000000000
  oid: 1a41a8c0a56a9894057366eec405db87443a9fa0
  summary: Shouldn't CodeLlama 34B have 16K context and rope_theta 1M?
  type: commit
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-09-05T20:03:55.000Z'
    data:
      status: merged
    id: 64f789abdcd7b028c159d7fd
    type: status-change
  author: ehartford
  created_at: 2023-09-05 19:03:55+00:00
  id: 64f789abdcd7b028c159d7fd
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: 3fe88e0724f25a00aa071a1557972e4fe5668950
num: 3
repo_id: cognitivecomputations/WizardLM-1.0-Uncensored-CodeLlama-34b
repo_type: model
status: merged
target_branch: refs/heads/main
title: Shouldn't CodeLlama 34B have 16K context and rope_theta 1M?
