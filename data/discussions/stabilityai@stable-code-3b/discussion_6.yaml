!!python/object:huggingface_hub.community.DiscussionWithDetails
author: davideuler
conflicting_files: null
created_at: 2024-01-20 10:54:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c03297c68f90b9cdb7adc1f21c724dce.svg
      fullname: David Euler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: davideuler
      type: user
    createdAt: '2024-01-20T10:54:36.000Z'
    data:
      edited: false
      editors:
      - davideuler
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4782770276069641
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c03297c68f90b9cdb7adc1f21c724dce.svg
          fullname: David Euler
          isHf: false
          isPro: false
          name: davideuler
          type: user
        html: '<p>How could I inference with the gguf models?  When I run the gguf
          models with llama.cpp executable, it shows:</p>

          <pre><code>gguf_init_from_file: invalid magic characters ''vers''

          llama_model_load: error loading model: llama_model_loader: failed to load
          model from stable-code-3b-Q6_K.gguf


          llama_load_model_from_file: failed to load model

          llama_init_from_gpt_params: error: failed to load model ''stable-code-3b-Q6_K.gguf''

          main: error: unable to load model

          </code></pre>

          '
        raw: "How could I inference with the gguf models?  When I run the gguf models\
          \ with llama.cpp executable, it shows:\r\n```\r\ngguf_init_from_file: invalid\
          \ magic characters 'vers'\r\nllama_model_load: error loading model: llama_model_loader:\
          \ failed to load model from stable-code-3b-Q6_K.gguf\r\n\r\nllama_load_model_from_file:\
          \ failed to load model\r\nllama_init_from_gpt_params: error: failed to load\
          \ model 'stable-code-3b-Q6_K.gguf'\r\nmain: error: unable to load model\r\
          \n```"
        updatedAt: '2024-01-20T10:54:36.023Z'
      numEdits: 0
      reactions: []
    id: 65aba66c9aba49e1d0764986
    type: comment
  author: davideuler
  content: "How could I inference with the gguf models?  When I run the gguf models\
    \ with llama.cpp executable, it shows:\r\n```\r\ngguf_init_from_file: invalid\
    \ magic characters 'vers'\r\nllama_model_load: error loading model: llama_model_loader:\
    \ failed to load model from stable-code-3b-Q6_K.gguf\r\n\r\nllama_load_model_from_file:\
    \ failed to load model\r\nllama_init_from_gpt_params: error: failed to load model\
    \ 'stable-code-3b-Q6_K.gguf'\r\nmain: error: unable to load model\r\n```"
  created_at: 2024-01-20 10:54:36+00:00
  edited: false
  hidden: false
  id: 65aba66c9aba49e1d0764986
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/afb0d21c0fdfc5e4c572007e8237b954.svg
      fullname: Shahil Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: skfrost19
      type: user
    createdAt: '2024-01-22T17:42:08.000Z'
    data:
      edited: false
      editors:
      - skfrost19
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5453620553016663
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/afb0d21c0fdfc5e4c572007e8237b954.svg
          fullname: Shahil Kumar
          isHf: false
          isPro: false
          name: skfrost19
          type: user
        html: "<p>Can use LangChain for the same , Here is a sample code in python\
          \ :-</p>\n<pre><code>from langchain.callbacks.manager import CallbackManager\n\
          from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\
          from langchain.chains import LLMChain\nfrom langchain.llms import LlamaCpp\n\
          from langchain.prompts import PromptTemplate\nimport time\nfrom googletrans\
          \ import Translator\n\nn_gpu_layers = 14  # Change this value based on your\
          \ model and your GPU VRAM pool.\nn_batch = 50  # Should be between 1 and\
          \ n_ctx, consider the amount of VRAM in your GPU.\n\n# Callbacks support\
          \ token-wise streaming\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\
          \n# Make sure the model path is correct for your system!\nllm = LlamaCpp(\n\
          \    model_path=\"models/stable-code-3b.gguf\",\n    n_gpu_layers=n_gpu_layers,\n\
          \    n_batch=n_batch,\n    max_new_tokens=512,\n    callback_manager=callback_manager,\n\
          \    verbose=True,  # Verbose is required to pass to the callback manager\n\
          )\n\n# Prompt Template\ntemplate = \"\"\"\n    Write a python code for:\n\
          \    {_prompt_} &lt;|endoftext|&gt;\n    \"\"\"\n\nprompt = PromptTemplate(input_variables=[\"\
          tweet\"], template=template)\n\nres = llm(prompt.format(tweet=\"How to connect\
          \ to a sql database\"))\nprint(res)\n</code></pre>\n"
        raw: "Can use LangChain for the same , Here is a sample code in python :-\n\
          \n```\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout\
          \ import StreamingStdOutCallbackHandler\nfrom langchain.chains import LLMChain\n\
          from langchain.llms import LlamaCpp\nfrom langchain.prompts import PromptTemplate\n\
          import time\nfrom googletrans import Translator\n\nn_gpu_layers = 14  #\
          \ Change this value based on your model and your GPU VRAM pool.\nn_batch\
          \ = 50  # Should be between 1 and n_ctx, consider the amount of VRAM in\
          \ your GPU.\n\n# Callbacks support token-wise streaming\ncallback_manager\
          \ = CallbackManager([StreamingStdOutCallbackHandler()])\n\n# Make sure the\
          \ model path is correct for your system!\nllm = LlamaCpp(\n    model_path=\"\
          models/stable-code-3b.gguf\",\n    n_gpu_layers=n_gpu_layers,\n    n_batch=n_batch,\n\
          \    max_new_tokens=512,\n    callback_manager=callback_manager,\n    verbose=True,\
          \  # Verbose is required to pass to the callback manager\n)\n\n# Prompt\
          \ Template\ntemplate = \"\"\"\n    Write a python code for:\n    {_prompt_}\
          \ <|endoftext|>\n    \"\"\"\n\nprompt = PromptTemplate(input_variables=[\"\
          tweet\"], template=template)\n\nres = llm(prompt.format(tweet=\"How to connect\
          \ to a sql database\"))\nprint(res)\n```"
        updatedAt: '2024-01-22T17:42:08.302Z'
      numEdits: 0
      reactions: []
    id: 65aea8f01f418c7449e3d920
    type: comment
  author: skfrost19
  content: "Can use LangChain for the same , Here is a sample code in python :-\n\n\
    ```\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout\
    \ import StreamingStdOutCallbackHandler\nfrom langchain.chains import LLMChain\n\
    from langchain.llms import LlamaCpp\nfrom langchain.prompts import PromptTemplate\n\
    import time\nfrom googletrans import Translator\n\nn_gpu_layers = 14  # Change\
    \ this value based on your model and your GPU VRAM pool.\nn_batch = 50  # Should\
    \ be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n\n# Callbacks\
    \ support token-wise streaming\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\
    \n# Make sure the model path is correct for your system!\nllm = LlamaCpp(\n  \
    \  model_path=\"models/stable-code-3b.gguf\",\n    n_gpu_layers=n_gpu_layers,\n\
    \    n_batch=n_batch,\n    max_new_tokens=512,\n    callback_manager=callback_manager,\n\
    \    verbose=True,  # Verbose is required to pass to the callback manager\n)\n\
    \n# Prompt Template\ntemplate = \"\"\"\n    Write a python code for:\n    {_prompt_}\
    \ <|endoftext|>\n    \"\"\"\n\nprompt = PromptTemplate(input_variables=[\"tweet\"\
    ], template=template)\n\nres = llm(prompt.format(tweet=\"How to connect to a sql\
    \ database\"))\nprint(res)\n```"
  created_at: 2024-01-22 17:42:08+00:00
  edited: false
  hidden: false
  id: 65aea8f01f418c7449e3d920
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e421a77e2096893ffeb2576e27fa1073.svg
      fullname: allways2008
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: allways
      type: user
    createdAt: '2024-01-24T03:34:59.000Z'
    data:
      edited: false
      editors:
      - allways
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9952895641326904
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e421a77e2096893ffeb2576e27fa1073.svg
          fullname: allways2008
          isHf: false
          isPro: false
          name: allways
          type: user
        html: '<p>A Chinese boy and his father are playing football in the sunshine</p>

          '
        raw: A Chinese boy and his father are playing football in the sunshine
        updatedAt: '2024-01-24T03:34:59.220Z'
      numEdits: 0
      reactions: []
    id: 65b085639d4b4d793083aead
    type: comment
  author: allways
  content: A Chinese boy and his father are playing football in the sunshine
  created_at: 2024-01-24 03:34:59+00:00
  edited: false
  hidden: false
  id: 65b085639d4b4d793083aead
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2024-01-24T14:29:46.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8562087416648865
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<blockquote>

          <p>How could I inference with the gguf models?  When I run the gguf models
          with llama.cpp executable, it shows:</p>

          <pre><code>gguf_init_from_file: invalid magic characters ''vers''

          llama_model_load: error loading model: llama_model_loader: failed to load
          model from stable-code-3b-Q6_K.gguf


          llama_load_model_from_file: failed to load model

          llama_init_from_gpt_params: error: failed to load model ''stable-code-3b-Q6_K.gguf''

          main: error: unable to load model

          </code></pre>

          </blockquote>

          <p>Huggingface has made an own inference c++ engine, it abuses the gguf
          format instead of changing the name. No one I know ever used that engine
          but it''s there, causing incompatible files to be uploaded.<br>I''ver seen
          those type of gguf files a few times, it''s extremely confusing to people.</p>

          <p>If you want a GGUF file, then download it either from "TheBloke", he''s
          converting and hosting them all here on Huggingface and those will all come
          with explanation and they should work.<br>Or you download the repository
          and use llama.cpp convert.py and quantize to create your own gguf.<br>If
          you intend to work a lot with a particular model that''s recommended, as
          you can quickly regenerate any quantization or improvement you need.</p>

          '
        raw: "> How could I inference with the gguf models?  When I run the gguf models\
          \ with llama.cpp executable, it shows:\n> ```\n> gguf_init_from_file: invalid\
          \ magic characters 'vers'\n> llama_model_load: error loading model: llama_model_loader:\
          \ failed to load model from stable-code-3b-Q6_K.gguf\n> \n> llama_load_model_from_file:\
          \ failed to load model\n> llama_init_from_gpt_params: error: failed to load\
          \ model 'stable-code-3b-Q6_K.gguf'\n> main: error: unable to load model\n\
          > ```\n\nHuggingface has made an own inference c++ engine, it abuses the\
          \ gguf format instead of changing the name. No one I know ever used that\
          \ engine but it's there, causing incompatible files to be uploaded.\nI'ver\
          \ seen those type of gguf files a few times, it's extremely confusing to\
          \ people.\n\nIf you want a GGUF file, then download it either from \"TheBloke\"\
          , he's converting and hosting them all here on Huggingface and those will\
          \ all come with explanation and they should work.\nOr you download the repository\
          \ and use llama.cpp convert.py and quantize to create your own gguf.\nIf\
          \ you intend to work a lot with a particular model that's recommended, as\
          \ you can quickly regenerate any quantization or improvement you need."
        updatedAt: '2024-01-24T14:29:46.627Z'
      numEdits: 0
      reactions: []
    id: 65b11edad84a1f119fb14cd6
    type: comment
  author: cmp-nct
  content: "> How could I inference with the gguf models?  When I run the gguf models\
    \ with llama.cpp executable, it shows:\n> ```\n> gguf_init_from_file: invalid\
    \ magic characters 'vers'\n> llama_model_load: error loading model: llama_model_loader:\
    \ failed to load model from stable-code-3b-Q6_K.gguf\n> \n> llama_load_model_from_file:\
    \ failed to load model\n> llama_init_from_gpt_params: error: failed to load model\
    \ 'stable-code-3b-Q6_K.gguf'\n> main: error: unable to load model\n> ```\n\nHuggingface\
    \ has made an own inference c++ engine, it abuses the gguf format instead of changing\
    \ the name. No one I know ever used that engine but it's there, causing incompatible\
    \ files to be uploaded.\nI'ver seen those type of gguf files a few times, it's\
    \ extremely confusing to people.\n\nIf you want a GGUF file, then download it\
    \ either from \"TheBloke\", he's converting and hosting them all here on Huggingface\
    \ and those will all come with explanation and they should work.\nOr you download\
    \ the repository and use llama.cpp convert.py and quantize to create your own\
    \ gguf.\nIf you intend to work a lot with a particular model that's recommended,\
    \ as you can quickly regenerate any quantization or improvement you need."
  created_at: 2024-01-24 14:29:46+00:00
  edited: false
  hidden: false
  id: 65b11edad84a1f119fb14cd6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: stabilityai/stable-code-3b
repo_type: model
status: open
target_branch: null
title: How could I run inference with the gguf models?
