!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cmp-nct
conflicting_files: null
created_at: 2023-12-05 14:40:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-12-05T14:40:53.000Z'
    data:
      edited: true
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9773468375205994
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>Just wanted to share some feedback. I was testing the new variant
          and compared it to the little DPO trained 14B brother.<br>It produced significantly
          worse results, less well written, less precisely followed on instructions.</p>

          <p>The task was summarization in good language of a structured input based
          on a set of instructions.</p>

          <p>There are likely other tasks where this model will be better but at this
          point I''d not choose it</p>

          '
        raw: 'Just wanted to share some feedback. I was testing the new variant and
          compared it to the little DPO trained 14B brother.

          It produced significantly worse results, less well written, less precisely
          followed on instructions.


          The task was summarization in good language of a structured input based
          on a set of instructions.


          There are likely other tasks where this model will be better but at this
          point I''d not choose it'
        updatedAt: '2023-12-05T14:41:17.250Z'
      numEdits: 1
      reactions: []
    id: 656f36750492a9d6355c4a17
    type: comment
  author: cmp-nct
  content: 'Just wanted to share some feedback. I was testing the new variant and
    compared it to the little DPO trained 14B brother.

    It produced significantly worse results, less well written, less precisely followed
    on instructions.


    The task was summarization in good language of a structured input based on a set
    of instructions.


    There are likely other tasks where this model will be better but at this point
    I''d not choose it'
  created_at: 2023-12-05 14:40:53+00:00
  edited: true
  hidden: false
  id: 656f36750492a9d6355c4a17
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642c48673a6a3fa1ebadc436/mJuX9X61Z5HCuq1Tgti9O.png?w=200&h=200&f=face
      fullname: feather
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sssfeather
      type: user
    createdAt: '2023-12-11T03:22:35.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642c48673a6a3fa1ebadc436/mJuX9X61Z5HCuq1Tgti9O.png?w=200&h=200&f=face
          fullname: feather
          isHf: false
          isPro: false
          name: sssfeather
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-11T03:23:16.778Z'
      numEdits: 1
      reactions: []
    id: 6576807b4fffc3f08b9dbeb3
    type: comment
  author: sssfeather
  content: This comment has been hidden
  created_at: 2023-12-11 03:22:35+00:00
  edited: true
  hidden: true
  id: 6576807b4fffc3f08b9dbeb3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: CausalLM/34B-preview
repo_type: model
status: open
target_branch: null
title: In my use case, significantly worse than the 14-DPO variant
