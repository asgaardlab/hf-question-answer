!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Se-Hun
conflicting_files: null
created_at: 2023-10-12 03:19:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1626828295148-noauth.jpeg?w=200&h=200&f=face
      fullname: sehun hu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Se-Hun
      type: user
    createdAt: '2023-10-12T04:19:51.000Z'
    data:
      edited: false
      editors:
      - Se-Hun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9445837140083313
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1626828295148-noauth.jpeg?w=200&h=200&f=face
          fullname: sehun hu
          isHf: false
          isPro: false
          name: Se-Hun
          type: user
        html: '<p>I found empty output string when long context is passed to this
          model.<br>As my inference testing, i suggest that this problem is occurred
          in case of text longer than 2000 tokens (or about 2040 tokens) is passed.</p>

          <p>Why this problem is occured ? Is it caused by your dataset configurations
          ?</p>

          '
        raw: "I found empty output string when long context is passed to this model.\r\
          \nAs my inference testing, i suggest that this problem is occurred in case\
          \ of text longer than 2000 tokens (or about 2040 tokens) is passed.\r\n\r\
          \nWhy this problem is occured ? Is it caused by your dataset configurations\
          \ ?"
        updatedAt: '2023-10-12T04:19:51.153Z'
      numEdits: 0
      reactions: []
    id: 652773e7d87552be6eaea16a
    type: comment
  author: Se-Hun
  content: "I found empty output string when long context is passed to this model.\r\
    \nAs my inference testing, i suggest that this problem is occurred in case of\
    \ text longer than 2000 tokens (or about 2040 tokens) is passed.\r\n\r\nWhy this\
    \ problem is occured ? Is it caused by your dataset configurations ?"
  created_at: 2023-10-12 03:19:51+00:00
  edited: false
  hidden: false
  id: 652773e7d87552be6eaea16a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1626828295148-noauth.jpeg?w=200&h=200&f=face
      fullname: sehun hu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Se-Hun
      type: user
    createdAt: '2023-10-12T04:20:26.000Z'
    data:
      from: List of datasets
      to: Occured problem at long context
    id: 6527740a34bf5ece73d0dc3f
    type: title-change
  author: Se-Hun
  created_at: 2023-10-12 03:20:26+00:00
  id: 6527740a34bf5ece73d0dc3f
  new_title: Occured problem at long context
  old_title: List of datasets
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1f763ca61fb1f281f7ac24bdee8722f.svg
      fullname: DAEHEEKIM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreaKIM
      type: user
    createdAt: '2023-10-21T07:46:55.000Z'
    data:
      edited: false
      editors:
      - andreaKIM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8822388052940369
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1f763ca61fb1f281f7ac24bdee8722f.svg
          fullname: DAEHEEKIM
          isHf: false
          isPro: false
          name: andreaKIM
          type: user
        html: '<p>Did you check max_position_embedding in config.json? I guess this
          problem occurd by token length. Also, check the tokenizer with your data
          language. Becuase llama''s vocab does not contain much tokens except english
          subwords.</p>

          '
        raw: Did you check max_position_embedding in config.json? I guess this problem
          occurd by token length. Also, check the tokenizer with your data language.
          Becuase llama's vocab does not contain much tokens except english subwords.
        updatedAt: '2023-10-21T07:46:55.749Z'
      numEdits: 0
      reactions: []
    id: 653381efae42162a1ec4abfa
    type: comment
  author: andreaKIM
  content: Did you check max_position_embedding in config.json? I guess this problem
    occurd by token length. Also, check the tokenizer with your data language. Becuase
    llama's vocab does not contain much tokens except english subwords.
  created_at: 2023-10-21 06:46:55+00:00
  edited: false
  hidden: false
  id: 653381efae42162a1ec4abfa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: hyunseoki/ko-en-llama2-13b
repo_type: model
status: open
target_branch: null
title: Occured problem at long context
