!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lemonflourorange
conflicting_files: null
created_at: 2023-08-03 05:47:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bea11e9c4bf19b2a1884eda1a8cbf00.svg
      fullname: Lemon Flour
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lemonflourorange
      type: user
    createdAt: '2023-08-03T06:47:59.000Z'
    data:
      edited: false
      editors:
      - lemonflourorange
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8097692728042603
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bea11e9c4bf19b2a1884eda1a8cbf00.svg
          fullname: Lemon Flour
          isHf: false
          isPro: false
          name: lemonflourorange
          type: user
        html: '<p>The local gradio demo provided by Meta <a rel="nofollow" href="https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md">here</a>
          using the non-HF weights is about 2x faster than the HF weights. Worse,
          bitsandbytes quantization results in 3-4x slower inference when it should
          be faster. Looks like the Transformers implementation still needs some work.</p>

          '
        raw: The local gradio demo provided by Meta [here](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md)
          using the non-HF weights is about 2x faster than the HF weights. Worse,
          bitsandbytes quantization results in 3-4x slower inference when it should
          be faster. Looks like the Transformers implementation still needs some work.
        updatedAt: '2023-08-03T06:47:59.859Z'
      numEdits: 0
      reactions: []
    id: 64cb4d9f76200ec80fee86d7
    type: comment
  author: lemonflourorange
  content: The local gradio demo provided by Meta [here](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md)
    using the non-HF weights is about 2x faster than the HF weights. Worse, bitsandbytes
    quantization results in 3-4x slower inference when it should be faster. Looks
    like the Transformers implementation still needs some work.
  created_at: 2023-08-03 05:47:59+00:00
  edited: false
  hidden: false
  id: 64cb4d9f76200ec80fee86d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1655385361868-61b85ce86eb1f2c5e6233736.jpeg?w=200&h=200&f=face
      fullname: Vaibhav Srivastav
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: reach-vb
      type: user
    createdAt: '2023-08-03T15:58:21.000Z'
    data:
      edited: false
      editors:
      - reach-vb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9403214454650879
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1655385361868-61b85ce86eb1f2c5e6233736.jpeg?w=200&h=200&f=face
          fullname: Vaibhav Srivastav
          isHf: true
          isPro: false
          name: reach-vb
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;lemonflourorange&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lemonflourorange\"\
          >@<span class=\"underline\">lemonflourorange</span></a></span>\n\n\t</span></span>\
          \ - Sorry to hear that. Can you please share the inference code you are\
          \ using?</p>\n"
        raw: Hi @lemonflourorange - Sorry to hear that. Can you please share the inference
          code you are using?
        updatedAt: '2023-08-03T15:58:21.481Z'
      numEdits: 0
      reactions: []
    id: 64cbce9da34a1fab1921879e
    type: comment
  author: reach-vb
  content: Hi @lemonflourorange - Sorry to hear that. Can you please share the inference
    code you are using?
  created_at: 2023-08-03 14:58:21+00:00
  edited: false
  hidden: false
  id: 64cbce9da34a1fab1921879e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-08-03T16:35:05.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8509628772735596
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;lemonflourorange&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lemonflourorange\"\
          >@<span class=\"underline\">lemonflourorange</span></a></span>\n\n\t</span></span>!\
          \ Thanks for opening this issue. Note that the default Meta implementation\
          \ uses fp16 by default, whereas <code>transformers</code> uses fp32. You\
          \ can put the model in fp16 precision by calling:</p>\n<pre><code class=\"\
          language-python\">model.half()\n</code></pre>\n<p>This should give you a\
          \ nice speed-up versus full fp32 precision.</p>\n<p>Note that bitsandbytes\
          \ quantisation is expected to be slower than fp16; 3-4x slower is about\
          \ what we'd expect for dynamic 8-bit quantisation (this will be lower for\
          \ <a href=\"https://huggingface.co/blog/4bit-transformers-bitsandbytes\"\
          >dynamic 4-bit quantisation</a>). See results for Whisper ASR, which tests\
          \ models for a similar model size for the speech recognition task: <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/peft/discussions/477\"\
          >https://github.com/huggingface/peft/discussions/477</a></p>\n"
        raw: 'Hey @lemonflourorange! Thanks for opening this issue. Note that the
          default Meta implementation uses fp16 by default, whereas `transformers`
          uses fp32. You can put the model in fp16 precision by calling:


          ```python

          model.half()

          ```


          This should give you a nice speed-up versus full fp32 precision.


          Note that bitsandbytes quantisation is expected to be slower than fp16;
          3-4x slower is about what we''d expect for dynamic 8-bit quantisation (this
          will be lower for [dynamic 4-bit quantisation](https://huggingface.co/blog/4bit-transformers-bitsandbytes)).
          See results for Whisper ASR, which tests models for a similar model size
          for the speech recognition task: https://github.com/huggingface/peft/discussions/477'
        updatedAt: '2023-08-03T16:48:47.696Z'
      numEdits: 1
      reactions: []
    id: 64cbd7394dcdaead7a141f28
    type: comment
  author: sanchit-gandhi
  content: 'Hey @lemonflourorange! Thanks for opening this issue. Note that the default
    Meta implementation uses fp16 by default, whereas `transformers` uses fp32. You
    can put the model in fp16 precision by calling:


    ```python

    model.half()

    ```


    This should give you a nice speed-up versus full fp32 precision.


    Note that bitsandbytes quantisation is expected to be slower than fp16; 3-4x slower
    is about what we''d expect for dynamic 8-bit quantisation (this will be lower
    for [dynamic 4-bit quantisation](https://huggingface.co/blog/4bit-transformers-bitsandbytes)).
    See results for Whisper ASR, which tests models for a similar model size for the
    speech recognition task: https://github.com/huggingface/peft/discussions/477'
  created_at: 2023-08-03 15:35:05+00:00
  edited: true
  hidden: false
  id: 64cbd7394dcdaead7a141f28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bea11e9c4bf19b2a1884eda1a8cbf00.svg
      fullname: Lemon Flour
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lemonflourorange
      type: user
    createdAt: '2023-08-07T19:39:08.000Z'
    data:
      edited: false
      editors:
      - lemonflourorange
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8819094896316528
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bea11e9c4bf19b2a1884eda1a8cbf00.svg
          fullname: Lemon Flour
          isHf: false
          isPro: false
          name: lemonflourorange
          type: user
        html: '<p>Thanks. Half precision fixes this for me. Still not sure why quantization
          ends up being slower than fp16. I guess quantization only improves inference
          speed in LLMs?</p>

          '
        raw: Thanks. Half precision fixes this for me. Still not sure why quantization
          ends up being slower than fp16. I guess quantization only improves inference
          speed in LLMs?
        updatedAt: '2023-08-07T19:39:08.169Z'
      numEdits: 0
      reactions: []
    id: 64d1485c01931c6016449161
    type: comment
  author: lemonflourorange
  content: Thanks. Half precision fixes this for me. Still not sure why quantization
    ends up being slower than fp16. I guess quantization only improves inference speed
    in LLMs?
  created_at: 2023-08-07 18:39:08+00:00
  edited: false
  hidden: false
  id: 64d1485c01931c6016449161
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-09-06T15:41:57.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8739335536956787
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>Indeed - we have (relatively) small matmuls, and large inputs, which
          causes the 8-bit bnb algorithm to be quite slow for MusicGen. You can try
          using the latest 4-bit algorithm which should be faster: <a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">https://huggingface.co/blog/4bit-transformers-bitsandbytes</a></p>

          '
        raw: 'Indeed - we have (relatively) small matmuls, and large inputs, which
          causes the 8-bit bnb algorithm to be quite slow for MusicGen. You can try
          using the latest 4-bit algorithm which should be faster: https://huggingface.co/blog/4bit-transformers-bitsandbytes'
        updatedAt: '2023-09-06T15:41:57.626Z'
      numEdits: 0
      reactions: []
    id: 64f89dc5b7c50aef83c66eef
    type: comment
  author: sanchit-gandhi
  content: 'Indeed - we have (relatively) small matmuls, and large inputs, which causes
    the 8-bit bnb algorithm to be quite slow for MusicGen. You can try using the latest
    4-bit algorithm which should be faster: https://huggingface.co/blog/4bit-transformers-bitsandbytes'
  created_at: 2023-09-06 14:41:57+00:00
  edited: false
  hidden: false
  id: 64f89dc5b7c50aef83c66eef
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: facebook/musicgen-large
repo_type: model
status: open
target_branch: null
title: Runtime is about 2x slower than with Meta's own audiocraft code
