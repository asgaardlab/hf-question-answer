!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tunan01
conflicting_files: null
created_at: 2023-08-17 07:00:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/53160d6462e249a792a4730469953cf5.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tunan01
      type: user
    createdAt: '2023-08-17T08:00:54.000Z'
    data:
      edited: true
      editors:
      - Tunan01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9060275554656982
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/53160d6462e249a792a4730469953cf5.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Tunan01
          type: user
        html: "<p>Thanks for your work!</p>\n<p>I followed the tutorial you wrote\
          \ on <a rel=\"nofollow\" href=\"https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#-transformers-usage\"\
          >GitHub</a>. I put the code in Colab for inference. </p>\n<p>Using the audiocraft\
          \ API example, I downloaded the <code>state_dict.bin</code> file (6.51GB)\
          \ for the large model, and it worked fine on the T4 and 12GRAM.</p>\n<p>However,\
          \ when using the \U0001F917 Transformers Usage, I downloaded the <code>pytorch_model-00001-of-00002.bin</code>,\
          \ <code>pytorch_model-00002-of-00002.bin</code>, and <code>state_dict.bin</code>,\
          \ but it caused a RAM crash.</p>\n<p>I would like to know the difference\
          \ between these two approaches. As a beginner, I appreciate your help.</p>\n"
        raw: "\n\nThanks for your work!\n\nI followed the tutorial you wrote on [GitHub](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#-transformers-usage).\
          \ I put the code in Colab for inference. \n\nUsing the audiocraft API example,\
          \ I downloaded the `state_dict.bin` file (6.51GB) for the large model, and\
          \ it worked fine on the T4 and 12GRAM.\n\nHowever, when using the \U0001F917\
          \ Transformers Usage, I downloaded the `pytorch_model-00001-of-00002.bin`,\
          \ `pytorch_model-00002-of-00002.bin`, and `state_dict.bin`, but it caused\
          \ a RAM crash.\n\nI would like to know the difference between these two\
          \ approaches. As a beginner, I appreciate your help."
        updatedAt: '2023-08-17T08:02:01.169Z'
      numEdits: 1
      reactions: []
    id: 64ddd3b6fab22723a86dbe85
    type: comment
  author: Tunan01
  content: "\n\nThanks for your work!\n\nI followed the tutorial you wrote on [GitHub](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#-transformers-usage).\
    \ I put the code in Colab for inference. \n\nUsing the audiocraft API example,\
    \ I downloaded the `state_dict.bin` file (6.51GB) for the large model, and it\
    \ worked fine on the T4 and 12GRAM.\n\nHowever, when using the \U0001F917 Transformers\
    \ Usage, I downloaded the `pytorch_model-00001-of-00002.bin`, `pytorch_model-00002-of-00002.bin`,\
    \ and `state_dict.bin`, but it caused a RAM crash.\n\nI would like to know the\
    \ difference between these two approaches. As a beginner, I appreciate your help."
  created_at: 2023-08-17 07:00:54+00:00
  edited: true
  hidden: false
  id: 64ddd3b6fab22723a86dbe85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-08-17T16:59:01.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8059830665588379
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Tunan01&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Tunan01\"\
          >@<span class=\"underline\">Tunan01</span></a></span>\n\n\t</span></span>\
          \ - when the PyTorch weights exceed a given threshold of size (default =\
          \ 10GB) we <strong>shard</strong> (or <strong>split</strong>) them into\
          \ multiple smaller files, see <a href=\"https://huggingface.co/docs/transformers/main/main_classes/model#transformers.PreTrainedModel.push_to_hub.max_shard_size\"\
          >https://huggingface.co/docs/transformers/main/main_classes/model#transformers.PreTrainedModel.push_to_hub.max_shard_size</a></p>\n\
          <p>This makes loading large model weights faster, since we can load multiple\
          \ shards of model weights in parallel. The recommended way of loading model\
          \ weights with Hugging Face Transformers is using the <code>.from_pretrained</code>\
          \ method:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> MusicgenForConditionalGeneration\n\
          \nmodel = MusicgenForConditionalGeneration.from_pretrained(<span class=\"\
          hljs-string\">\"facebook/musicgen-large\"</span>)\n</code></pre>\n<p>=&gt;\
          \ this will take care of loading the sharded weights for you automatically\
          \ and load the state dict into the MusicGen model class</p>\n"
        raw: 'Hey @Tunan01 - when the PyTorch weights exceed a given threshold of
          size (default = 10GB) we **shard** (or **split**) them into multiple smaller
          files, see https://huggingface.co/docs/transformers/main/main_classes/model#transformers.PreTrainedModel.push_to_hub.max_shard_size


          This makes loading large model weights faster, since we can load multiple
          shards of model weights in parallel. The recommended way of loading model
          weights with Hugging Face Transformers is using the `.from_pretrained` method:

          ```python

          from transformers import MusicgenForConditionalGeneration


          model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-large")

          ```

          => this will take care of loading the sharded weights for you automatically
          and load the state dict into the MusicGen model class'
        updatedAt: '2023-08-17T16:59:18.529Z'
      numEdits: 1
      reactions: []
    id: 64de51d51499c62d84b2e0d7
    type: comment
  author: sanchit-gandhi
  content: 'Hey @Tunan01 - when the PyTorch weights exceed a given threshold of size
    (default = 10GB) we **shard** (or **split**) them into multiple smaller files,
    see https://huggingface.co/docs/transformers/main/main_classes/model#transformers.PreTrainedModel.push_to_hub.max_shard_size


    This makes loading large model weights faster, since we can load multiple shards
    of model weights in parallel. The recommended way of loading model weights with
    Hugging Face Transformers is using the `.from_pretrained` method:

    ```python

    from transformers import MusicgenForConditionalGeneration


    model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-large")

    ```

    => this will take care of loading the sharded weights for you automatically and
    load the state dict into the MusicGen model class'
  created_at: 2023-08-17 15:59:01+00:00
  edited: true
  hidden: false
  id: 64de51d51499c62d84b2e0d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/53160d6462e249a792a4730469953cf5.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tunan01
      type: user
    createdAt: '2023-08-18T01:46:08.000Z'
    data:
      edited: false
      editors:
      - Tunan01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8040438294410706
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/53160d6462e249a792a4730469953cf5.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Tunan01
          type: user
        html: '<p>Thank you for your reply!</p>

          <p>But I would like to ask if downloading only <code>state_dict.bin</code>
          (using audiocraft api example) <strong>without</strong> <code>pytorch_model-xxxxx-of-00002.bin</code>
          will not affect the results in a bad way?<br>Or, Will downloading <code>state_dict.bin</code>
          <strong>with</strong> <code>pytorch_model-xxxxx-of-00002.bin</code> (using
          Hugging Face Transformers) improve the results? </p>

          <p>Because these two files(<code>pytorch_model-xxxxx-of-00002.bin</code>)
          are quite large, I''m not sure how important they are to the results.<br>Specifically,
          I would like to know if the weights of musicgen_large model are contained
          solely within <code>state_dict.bin</code>, or if they are distributed across
          both <code>state_dict.bin</code> and <code>pytorch_model-xxxxx-of-00002.bin</code>.</p>

          '
        raw: "Thank you for your reply!\n\nBut I would like to ask if downloading\
          \ only `state_dict.bin` (using audiocraft api example) **without** `pytorch_model-xxxxx-of-00002.bin`\
          \ will not affect the results in a bad way?\nOr, Will downloading `state_dict.bin`\
          \ **with** `pytorch_model-xxxxx-of-00002.bin` (using Hugging Face Transformers)\
          \ improve the results? \n\nBecause these two files(`pytorch_model-xxxxx-of-00002.bin`)\
          \ are quite large, I'm not sure how important they are to the results. \n\
          Specifically, I would like to know if the weights of musicgen_large model\
          \ are contained solely within `state_dict.bin`, or if they are distributed\
          \ across both `state_dict.bin` and `pytorch_model-xxxxx-of-00002.bin`."
        updatedAt: '2023-08-18T01:46:08.496Z'
      numEdits: 0
      reactions: []
    id: 64decd601499c62d84c5b33f
    type: comment
  author: Tunan01
  content: "Thank you for your reply!\n\nBut I would like to ask if downloading only\
    \ `state_dict.bin` (using audiocraft api example) **without** `pytorch_model-xxxxx-of-00002.bin`\
    \ will not affect the results in a bad way?\nOr, Will downloading `state_dict.bin`\
    \ **with** `pytorch_model-xxxxx-of-00002.bin` (using Hugging Face Transformers)\
    \ improve the results? \n\nBecause these two files(`pytorch_model-xxxxx-of-00002.bin`)\
    \ are quite large, I'm not sure how important they are to the results. \nSpecifically,\
    \ I would like to know if the weights of musicgen_large model are contained solely\
    \ within `state_dict.bin`, or if they are distributed across both `state_dict.bin`\
    \ and `pytorch_model-xxxxx-of-00002.bin`."
  created_at: 2023-08-18 00:46:08+00:00
  edited: false
  hidden: false
  id: 64decd601499c62d84c5b33f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-08-18T18:33:41.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.755527913570404
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Tunan01&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Tunan01\"\
          >@<span class=\"underline\">Tunan01</span></a></span>\n\n\t</span></span>\
          \ - if using the Audiocraft repository, you just need <code>state_dict.bin</code>\
          \ (these are all the weights AudioCraft needs to download the MusicGen language\
          \ model. The text encoder model is downloaded under-the-hood using Transformers'\
          \ <code>.from_pretrained</code>). If using <code>transformers</code>, you'll\
          \ need both <code>pytorch_model-xxxxx-of-00002.bin</code> files - these\
          \ contain the text encoder <strong>and</strong> language model, hence the\
          \ larger file size.</p>\n"
        raw: Hey @Tunan01 - if using the Audiocraft repository, you just need `state_dict.bin`
          (these are all the weights AudioCraft needs to download the MusicGen language
          model. The text encoder model is downloaded under-the-hood using Transformers'
          `.from_pretrained`). If using `transformers`, you'll need both `pytorch_model-xxxxx-of-00002.bin`
          files - these contain the text encoder **and** language model, hence the
          larger file size.
        updatedAt: '2023-08-18T18:33:41.169Z'
      numEdits: 0
      reactions: []
    id: 64dfb9858e2084e1d7b49500
    type: comment
  author: sanchit-gandhi
  content: Hey @Tunan01 - if using the Audiocraft repository, you just need `state_dict.bin`
    (these are all the weights AudioCraft needs to download the MusicGen language
    model. The text encoder model is downloaded under-the-hood using Transformers'
    `.from_pretrained`). If using `transformers`, you'll need both `pytorch_model-xxxxx-of-00002.bin`
    files - these contain the text encoder **and** language model, hence the larger
    file size.
  created_at: 2023-08-18 17:33:41+00:00
  edited: false
  hidden: false
  id: 64dfb9858e2084e1d7b49500
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/53160d6462e249a792a4730469953cf5.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tunan01
      type: user
    createdAt: '2023-08-19T04:28:57.000Z'
    data:
      edited: false
      editors:
      - Tunan01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9927978515625
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/53160d6462e249a792a4730469953cf5.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Tunan01
          type: user
        html: "<p>Thanks a lot for your patience! I think I get it. \U0001F917</p>\n"
        raw: "Thanks a lot for your patience! I think I get it. \U0001F917"
        updatedAt: '2023-08-19T04:28:57.021Z'
      numEdits: 0
      reactions: []
    id: 64e04509d646147f304ce4d0
    type: comment
  author: Tunan01
  content: "Thanks a lot for your patience! I think I get it. \U0001F917"
  created_at: 2023-08-19 03:28:57+00:00
  edited: false
  hidden: false
  id: 64e04509d646147f304ce4d0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: facebook/musicgen-large
repo_type: model
status: open
target_branch: null
title: What's the usage of  `pytorch_model-XXXXX-of-00002.bin`?
