!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tginart
conflicting_files: null
created_at: 2023-06-08 18:51:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f9b53970acf3a20d84fb36a287a69ecc.svg
      fullname: Tony Ginart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tginart
      type: user
    createdAt: '2023-06-08T19:51:06.000Z'
    data:
      edited: true
      editors:
      - tginart
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.290180504322052
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f9b53970acf3a20d84fb36a287a69ecc.svg
          fullname: Tony Ginart
          isHf: false
          isPro: false
          name: tginart
          type: user
        html: "<pre><code>---------------------------------------------------------------------------\n\
          KeyError                                  Traceback (most recent call last)\n\
          Cell In[4], line 9\n      6 device = \"cuda\" # for GPU usage or \"cpu\"\
          \ for CPU usage\n      8 tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
          ----&gt; 9 model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\
          \     11 inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"\
          pt\").to(device)\n     12 outputs = model.generate(inputs)\n\nFile /usr/lib/python3/dist-packages/transformers/models/auto/auto_factory.py:434,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    432 hub_kwargs = {name: kwargs.pop(name) for\
          \ name in hub_kwargs_names if name in kwargs}\n    433 if not isinstance(config,\
          \ PretrainedConfig):\n--&gt; 434     config, kwargs = AutoConfig.from_pretrained(\n\
          \    435         pretrained_model_name_or_path,\n    436         return_unused_kwargs=True,\n\
          \    437         trust_remote_code=trust_remote_code,\n    438         **hub_kwargs,\n\
          \    439         **kwargs,\n    440     )\n    441 if hasattr(config, \"\
          auto_map\") and cls.__name__ in config.auto_map:\n    442     if not trust_remote_code:\n\
          \nFile /usr/lib/python3/dist-packages/transformers/models/auto/configuration_auto.py:873,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \    871     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\n    872 elif \"model_type\" in config_dict:\n--&gt; 873   \
          \  config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\n    874 \
          \    return config_class.from_dict(config_dict, **unused_kwargs)\n    875\
          \ else:\n    876     # Fallback: use pattern matching on the string.\n \
          \   877     # We go from longer names to shorter names to catch roberta\
          \ before bert (for instance)\n\nFile /usr/lib/python3/dist-packages/transformers/models/auto/configuration_auto.py:579,\
          \ in _LazyConfigMapping.__getitem__(self, key)\n    577     return self._extra_content[key]\n\
          \    578 if key not in self._mapping:\n--&gt; 579     raise KeyError(key)\n\
          \    580 value = self._mapping[key]\n    581 module_name = model_type_to_module_name(key)\n\
          \nKeyError: 'gpt_bigcode'\n</code></pre>\n"
        raw: "```\n---------------------------------------------------------------------------\n\
          KeyError                                  Traceback (most recent call last)\n\
          Cell In[4], line 9\n      6 device = \"cuda\" # for GPU usage or \"cpu\"\
          \ for CPU usage\n      8 tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
          ----> 9 model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\
          \     11 inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"\
          pt\").to(device)\n     12 outputs = model.generate(inputs)\n\nFile /usr/lib/python3/dist-packages/transformers/models/auto/auto_factory.py:434,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    432 hub_kwargs = {name: kwargs.pop(name) for\
          \ name in hub_kwargs_names if name in kwargs}\n    433 if not isinstance(config,\
          \ PretrainedConfig):\n--> 434     config, kwargs = AutoConfig.from_pretrained(\n\
          \    435         pretrained_model_name_or_path,\n    436         return_unused_kwargs=True,\n\
          \    437         trust_remote_code=trust_remote_code,\n    438         **hub_kwargs,\n\
          \    439         **kwargs,\n    440     )\n    441 if hasattr(config, \"\
          auto_map\") and cls.__name__ in config.auto_map:\n    442     if not trust_remote_code:\n\
          \nFile /usr/lib/python3/dist-packages/transformers/models/auto/configuration_auto.py:873,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \    871     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\n    872 elif \"model_type\" in config_dict:\n--> 873     config_class\
          \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\n    874     return config_class.from_dict(config_dict,\
          \ **unused_kwargs)\n    875 else:\n    876     # Fallback: use pattern matching\
          \ on the string.\n    877     # We go from longer names to shorter names\
          \ to catch roberta before bert (for instance)\n\nFile /usr/lib/python3/dist-packages/transformers/models/auto/configuration_auto.py:579,\
          \ in _LazyConfigMapping.__getitem__(self, key)\n    577     return self._extra_content[key]\n\
          \    578 if key not in self._mapping:\n--> 579     raise KeyError(key)\n\
          \    580 value = self._mapping[key]\n    581 module_name = model_type_to_module_name(key)\n\
          \nKeyError: 'gpt_bigcode'\n```"
        updatedAt: '2023-06-08T19:51:22.457Z'
      numEdits: 1
      reactions: []
    id: 6482312acb42b6d49763ff90
    type: comment
  author: tginart
  content: "```\n---------------------------------------------------------------------------\n\
    KeyError                                  Traceback (most recent call last)\n\
    Cell In[4], line 9\n      6 device = \"cuda\" # for GPU usage or \"cpu\" for CPU\
    \ usage\n      8 tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n---->\
    \ 9 model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n    \
    \ 11 inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"\
    pt\").to(device)\n     12 outputs = model.generate(inputs)\n\nFile /usr/lib/python3/dist-packages/transformers/models/auto/auto_factory.py:434,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    432 hub_kwargs = {name: kwargs.pop(name) for name in hub_kwargs_names\
    \ if name in kwargs}\n    433 if not isinstance(config, PretrainedConfig):\n-->\
    \ 434     config, kwargs = AutoConfig.from_pretrained(\n    435         pretrained_model_name_or_path,\n\
    \    436         return_unused_kwargs=True,\n    437         trust_remote_code=trust_remote_code,\n\
    \    438         **hub_kwargs,\n    439         **kwargs,\n    440     )\n   \
    \ 441 if hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map:\n\
    \    442     if not trust_remote_code:\n\nFile /usr/lib/python3/dist-packages/transformers/models/auto/configuration_auto.py:873,\
    \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
    \    871     return config_class.from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs)\n    872 elif \"model_type\" in config_dict:\n--> 873     config_class\
    \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\n    874     return config_class.from_dict(config_dict,\
    \ **unused_kwargs)\n    875 else:\n    876     # Fallback: use pattern matching\
    \ on the string.\n    877     # We go from longer names to shorter names to catch\
    \ roberta before bert (for instance)\n\nFile /usr/lib/python3/dist-packages/transformers/models/auto/configuration_auto.py:579,\
    \ in _LazyConfigMapping.__getitem__(self, key)\n    577     return self._extra_content[key]\n\
    \    578 if key not in self._mapping:\n--> 579     raise KeyError(key)\n    580\
    \ value = self._mapping[key]\n    581 module_name = model_type_to_module_name(key)\n\
    \nKeyError: 'gpt_bigcode'\n```"
  created_at: 2023-06-08 18:51:06+00:00
  edited: true
  hidden: false
  id: 6482312acb42b6d49763ff90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg?w=200&h=200&f=face
      fullname: Qian Liu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SivilTaram
      type: user
    createdAt: '2023-06-09T01:09:03.000Z'
    data:
      edited: false
      editors:
      - SivilTaram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9083005785942078
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg?w=200&h=200&f=face
          fullname: Qian Liu
          isHf: false
          isPro: false
          name: SivilTaram
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;tginart&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/tginart\">@<span class=\"\
          underline\">tginart</span></a></span>\n\n\t</span></span> May I know your\
          \ transformers' version? You should upgrade it to the latest version to\
          \ make it work with <code>gpt_bigcode</code>.</p>\n"
        raw: '@tginart May I know your transformers'' version? You should upgrade
          it to the latest version to make it work with `gpt_bigcode`.'
        updatedAt: '2023-06-09T01:09:03.582Z'
      numEdits: 0
      reactions: []
    id: 64827bafe4bbb1c2dd2e7ece
    type: comment
  author: SivilTaram
  content: '@tginart May I know your transformers'' version? You should upgrade it
    to the latest version to make it work with `gpt_bigcode`.'
  created_at: 2023-06-09 00:09:03+00:00
  edited: false
  hidden: false
  id: 64827bafe4bbb1c2dd2e7ece
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
      fullname: Loubna Ben Allal
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: loubnabnl
      type: user
    createdAt: '2023-06-12T16:10:05.000Z'
    data:
      edited: false
      editors:
      - loubnabnl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9077730774879456
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg?w=200&h=200&f=face
          fullname: Loubna Ben Allal
          isHf: true
          isPro: false
          name: loubnabnl
          type: user
        html: '<p>Like Qian suggested, you need <code>transformers&gt;=4.28.1</code>
          to be able to load the model</p>

          '
        raw: Like Qian suggested, you need `transformers>=4.28.1` to be able to load
          the model
        updatedAt: '2023-06-12T16:10:05.129Z'
      numEdits: 0
      reactions: []
    id: 6487435d89a7434ac98420ca
    type: comment
  author: loubnabnl
  content: Like Qian suggested, you need `transformers>=4.28.1` to be able to load
    the model
  created_at: 2023-06-12 15:10:05+00:00
  edited: false
  hidden: false
  id: 6487435d89a7434ac98420ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f9b53970acf3a20d84fb36a287a69ecc.svg
      fullname: Tony Ginart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tginart
      type: user
    createdAt: '2023-06-12T18:35:27.000Z'
    data:
      edited: false
      editors:
      - tginart
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46368739008903503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f9b53970acf3a20d84fb36a287a69ecc.svg
          fullname: Tony Ginart
          isHf: false
          isPro: false
          name: tginart
          type: user
        html: '<p>Thank you!</p>

          '
        raw: Thank you!
        updatedAt: '2023-06-12T18:35:27.645Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6487656f340a0f4d721ca961
    id: 6487656f340a0f4d721ca95b
    type: comment
  author: tginart
  content: Thank you!
  created_at: 2023-06-12 17:35:27+00:00
  edited: false
  hidden: false
  id: 6487656f340a0f4d721ca95b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f9b53970acf3a20d84fb36a287a69ecc.svg
      fullname: Tony Ginart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tginart
      type: user
    createdAt: '2023-06-12T18:35:27.000Z'
    data:
      status: closed
    id: 6487656f340a0f4d721ca961
    type: status-change
  author: tginart
  created_at: 2023-06-12 17:35:27+00:00
  id: 6487656f340a0f4d721ca961
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: bigcode/starcoderplus
repo_type: model
status: closed
target_branch: null
title: Bug in loading model
