!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joujiboi
conflicting_files: null
created_at: 2023-12-28 14:52:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671130976305-631608b395b55e2621d033a8.png?w=200&h=200&f=face
      fullname: JawGBoi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joujiboi
      type: user
    createdAt: '2023-12-28T14:52:21.000Z'
    data:
      edited: false
      editors:
      - joujiboi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8029273748397827
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671130976305-631608b395b55e2621d033a8.png?w=200&h=200&f=face
          fullname: JawGBoi
          isHf: false
          isPro: false
          name: joujiboi
          type: user
        html: '<p>How do you run this model?</p>

          '
        raw: How do you run this model?
        updatedAt: '2023-12-28T14:52:21.921Z'
      numEdits: 0
      reactions: []
    id: 658d8ba5c03a6e1461676d40
    type: comment
  author: joujiboi
  content: How do you run this model?
  created_at: 2023-12-28 14:52:21+00:00
  edited: false
  hidden: false
  id: 658d8ba5c03a6e1461676d40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2023-12-28T15:07:35.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9657059907913208
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: '<p>Sorry, you can''t run this model with the current version of transformers
          afaik (you need more than 2 experts). It''s purely experimental for now.</p>

          '
        raw: Sorry, you can't run this model with the current version of transformers
          afaik (you need more than 2 experts). It's purely experimental for now.
        updatedAt: '2023-12-28T15:07:35.485Z'
      numEdits: 0
      reactions: []
    id: 658d8f37fc70ec268f743fe1
    type: comment
  author: mlabonne
  content: Sorry, you can't run this model with the current version of transformers
    afaik (you need more than 2 experts). It's purely experimental for now.
  created_at: 2023-12-28 15:07:35+00:00
  edited: false
  hidden: false
  id: 658d8f37fc70ec268f743fe1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2023-12-28T18:48:29.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5132185816764832
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: "<p>Nevermind, it actually works. Here's the code to run it in 4-bit\
          \ precision:</p>\n<pre><code class=\"language-python\">!pip install -qU\
          \ transformers bitsandbytes accelerate\n\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\
          <span class=\"hljs-keyword\">import</span> transformers\n<span class=\"\
          hljs-keyword\">import</span> torch\n\nmodel = <span class=\"hljs-string\"\
          >\"mlabonne/NeuralMix-2x7b\"</span>\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = transformers.pipeline(\n    <span class=\"hljs-string\">\"text-generation\"\
          </span>,\n    model=model,\n    model_kwargs={<span class=\"hljs-string\"\
          >\"torch_dtype\"</span>: torch.float16, <span class=\"hljs-string\">\"load_in_4bit\"\
          </span>: <span class=\"hljs-literal\">True</span>},\n)\n\nmessages = [{<span\
          \ class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"\
          user\"</span>, <span class=\"hljs-string\">\"content\"</span>: <span class=\"\
          hljs-string\">\"Explain what a Mixture of Experts is in less than 100 words.\"\
          </span>}]\nprompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=<span\
          \ class=\"hljs-literal\">False</span>, add_generation_prompt=<span class=\"\
          hljs-literal\">True</span>)\noutputs = pipeline(prompt, max_new_tokens=<span\
          \ class=\"hljs-number\">256</span>, do_sample=<span class=\"hljs-literal\"\
          >True</span>, temperature=<span class=\"hljs-number\">0.7</span>, top_k=<span\
          \ class=\"hljs-number\">50</span>, top_p=<span class=\"hljs-number\">0.95</span>)\n\
          <span class=\"hljs-built_in\">print</span>(outputs[<span class=\"hljs-number\"\
          >0</span>][<span class=\"hljs-string\">\"generated_text\"</span>])\n</code></pre>\n"
        raw: "Nevermind, it actually works. Here's the code to run it in 4-bit precision:\n\
          \n```python\n!pip install -qU transformers bitsandbytes accelerate\n\nfrom\
          \ transformers import AutoTokenizer\nimport transformers\nimport torch\n\
          \nmodel = \"mlabonne/NeuralMix-2x7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_4bit\": True},\n\
          )\n\nmessages = [{\"role\": \"user\", \"content\": \"Explain what a Mixture\
          \ of Experts is in less than 100 words.\"}]\nprompt = pipeline.tokenizer.apply_chat_template(messages,\
          \ tokenize=False, add_generation_prompt=True)\noutputs = pipeline(prompt,\
          \ max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n\
          print(outputs[0][\"generated_text\"])\n```"
        updatedAt: '2023-12-28T18:48:29.033Z'
      numEdits: 0
      reactions: []
    id: 658dc2fda6567cb93cf5cf11
    type: comment
  author: mlabonne
  content: "Nevermind, it actually works. Here's the code to run it in 4-bit precision:\n\
    \n```python\n!pip install -qU transformers bitsandbytes accelerate\n\nfrom transformers\
    \ import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"mlabonne/NeuralMix-2x7b\"\
    \n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    model_kwargs={\"torch_dtype\"\
    : torch.float16, \"load_in_4bit\": True},\n)\n\nmessages = [{\"role\": \"user\"\
    , \"content\": \"Explain what a Mixture of Experts is in less than 100 words.\"\
    }]\nprompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False,\
    \ add_generation_prompt=True)\noutputs = pipeline(prompt, max_new_tokens=256,\
    \ do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"\
    generated_text\"])\n```"
  created_at: 2023-12-28 18:48:29+00:00
  edited: false
  hidden: false
  id: 658dc2fda6567cb93cf5cf11
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mlabonne/NeuralMix-2x7b
repo_type: model
status: open
target_branch: null
title: how to run?
