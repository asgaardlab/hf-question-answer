!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Fionn2200
conflicting_files: null
created_at: 2023-06-23 15:07:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56bc84db83674e153046893e5631a208.svg
      fullname: Fionn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fionn2200
      type: user
    createdAt: '2023-06-23T16:07:51.000Z'
    data:
      edited: false
      editors:
      - Fionn2200
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9286553859710693
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56bc84db83674e153046893e5631a208.svg
          fullname: Fionn
          isHf: false
          isPro: false
          name: Fionn2200
          type: user
        html: '<p>You offer an intriguing solution for compressing models, but sharing
          the essential quantisation code would be more beneficial. Uploading pre-quantized
          models alone is not as valuable since users are interested in obtaining
          the base model and having the flexibility to fine-tune and quantize it according
          to their specific requirements. This has been expressed on the GitHub repository
          many times  <a rel="nofollow" href="https://github.com/SqueezeAILab/SqueezeLLM/issues/12">#12</a>
          <a rel="nofollow" href="https://github.com/SqueezeAILab/SqueezeLLM/issues/7">#7</a></p>

          '
        raw: You offer an intriguing solution for compressing models, but sharing
          the essential quantisation code would be more beneficial. Uploading pre-quantized
          models alone is not as valuable since users are interested in obtaining
          the base model and having the flexibility to fine-tune and quantize it according
          to their specific requirements. This has been expressed on the GitHub repository
          many times  [#12](https://github.com/SqueezeAILab/SqueezeLLM/issues/12)
          [#7](https://github.com/SqueezeAILab/SqueezeLLM/issues/7)
        updatedAt: '2023-06-23T16:07:51.425Z'
      numEdits: 0
      reactions: []
    id: 6495c357961457314f43e557
    type: comment
  author: Fionn2200
  content: You offer an intriguing solution for compressing models, but sharing the
    essential quantisation code would be more beneficial. Uploading pre-quantized
    models alone is not as valuable since users are interested in obtaining the base
    model and having the flexibility to fine-tune and quantize it according to their
    specific requirements. This has been expressed on the GitHub repository many times  [#12](https://github.com/SqueezeAILab/SqueezeLLM/issues/12)
    [#7](https://github.com/SqueezeAILab/SqueezeLLM/issues/7)
  created_at: 2023-06-23 15:07:51+00:00
  edited: false
  hidden: false
  id: 6495c357961457314f43e557
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: squeeze-ai-lab/sq-llama-30b-w3-s45
repo_type: model
status: open
target_branch: null
title: No quantisation code?
