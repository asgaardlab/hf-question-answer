!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ArbitrationCity
conflicting_files: null
created_at: 2023-07-16 17:42:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/54334315bb84c613ce8a07eb64c80958.svg
      fullname: ARBI Assistant
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArbitrationCity
      type: user
    createdAt: '2023-07-16T18:42:16.000Z'
    data:
      edited: true
      editors:
      - ArbitrationCity
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.80894535779953
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/54334315bb84c613ce8a07eb64c80958.svg
          fullname: ARBI Assistant
          isHf: false
          isPro: false
          name: ArbitrationCity
          type: user
        html: '<p>Hi, I tried running the tts server (v.0.2 and v.0.1) locally using
          an RTX 4090 graphics card, but getting a Pytorch error related to the architecture.  I
          believe the RTX 4090 (Ada Lovelace) needs CUDA 11.8 or later - and a corresponding
          Pytorch version.    Anything I can to do make it work?  </p>

          <p>Error:  I0716 18:25:17.133145 1 libtorch.cc:2076] TRITONBACKEND_ModelInstanceInitialize:
          tts_decoder (GPU device 0)<br>[W cuda_graph_fuser.h:17] Warning: RegisterCudaFuseGraph::registerPass()
          is deprecated. Please use torch::jit::fuser::cuda::setEnabled(). (function
          registerPass)<br>I0716 18:25:17.817603 1 model_lifecycle.cc:693] successfully
          loaded ''tts_decoder'' version 1<br>[INFO:/opt/balacoon_tts/build_server_on_docker/_deps/balacoon_neural-src/src/lib/triton_metrics_service.cc:128]
          0.0.0.0:8002: metrics server[W cuda_graph_fuser.h:17] Warning: RegisterCudaFuseGraph::registerPass()
          is deprecated. Please use torch::jit::fuser::cuda::setEnabled(). (function
          registerPass)<br>[W cuda_graph_fuser.h:17] Warning: RegisterCudaFuseGraph::registerPass()
          is deprecated. Please use torch::jit::fuser::cuda::setEnabled(). (function
          registerPass)<br>terminate called after throwing an instance of ''std::runtime_error''<br>  what():  async
          response status: Internal - PyTorch execute failure: nvrtc: error: invalid
          value for --gpu-architecture (-arch)</p>

          <p>nvrtc compilation failed:</p>

          <p>#define NAN __int_as_float(0x7fffffff)<br>#define POS_INFINITY __int_as_float(0x7f800000)<br>#define
          NEG_INFINITY __int_as_float(0xff800000)</p>

          '
        raw: "Hi, I tried running the tts server (v.0.2 and v.0.1) locally using an\
          \ RTX 4090 graphics card, but getting a Pytorch error related to the architecture.\
          \  I believe the RTX 4090 (Ada Lovelace) needs CUDA 11.8 or later - and\
          \ a corresponding Pytorch version.    Anything I can to do make it work?\
          \  \n\nError:  I0716 18:25:17.133145 1 libtorch.cc:2076] TRITONBACKEND_ModelInstanceInitialize:\
          \ tts_decoder (GPU device 0)\n[W cuda_graph_fuser.h:17] Warning: RegisterCudaFuseGraph::registerPass()\
          \ is deprecated. Please use torch::jit::fuser::cuda::setEnabled(). (function\
          \ registerPass)\nI0716 18:25:17.817603 1 model_lifecycle.cc:693] successfully\
          \ loaded 'tts_decoder' version 1\n[INFO:/opt/balacoon_tts/build_server_on_docker/_deps/balacoon_neural-src/src/lib/triton_metrics_service.cc:128]\
          \ 0.0.0.0:8002: metrics server[W cuda_graph_fuser.h:17] Warning: RegisterCudaFuseGraph::registerPass()\
          \ is deprecated. Please use torch::jit::fuser::cuda::setEnabled(). (function\
          \ registerPass)\n[W cuda_graph_fuser.h:17] Warning: RegisterCudaFuseGraph::registerPass()\
          \ is deprecated. Please use torch::jit::fuser::cuda::setEnabled(). (function\
          \ registerPass)\nterminate called after throwing an instance of 'std::runtime_error'\n\
          \  what():  async response status: Internal - PyTorch execute failure: nvrtc:\
          \ error: invalid value for --gpu-architecture (-arch)\n\nnvrtc compilation\
          \ failed:\n\n#define NAN __int_as_float(0x7fffffff)\n#define POS_INFINITY\
          \ __int_as_float(0x7f800000)\n#define NEG_INFINITY __int_as_float(0xff800000)\n"
        updatedAt: '2023-07-16T18:43:07.166Z'
      numEdits: 1
      reactions: []
    id: 64b43a082fc8324fcb528d63
    type: comment
  author: ArbitrationCity
  content: "Hi, I tried running the tts server (v.0.2 and v.0.1) locally using an\
    \ RTX 4090 graphics card, but getting a Pytorch error related to the architecture.\
    \  I believe the RTX 4090 (Ada Lovelace) needs CUDA 11.8 or later - and a corresponding\
    \ Pytorch version.    Anything I can to do make it work?  \n\nError:  I0716 18:25:17.133145\
    \ 1 libtorch.cc:2076] TRITONBACKEND_ModelInstanceInitialize: tts_decoder (GPU\
    \ device 0)\n[W cuda_graph_fuser.h:17] Warning: RegisterCudaFuseGraph::registerPass()\
    \ is deprecated. Please use torch::jit::fuser::cuda::setEnabled(). (function registerPass)\n\
    I0716 18:25:17.817603 1 model_lifecycle.cc:693] successfully loaded 'tts_decoder'\
    \ version 1\n[INFO:/opt/balacoon_tts/build_server_on_docker/_deps/balacoon_neural-src/src/lib/triton_metrics_service.cc:128]\
    \ 0.0.0.0:8002: metrics server[W cuda_graph_fuser.h:17] Warning: RegisterCudaFuseGraph::registerPass()\
    \ is deprecated. Please use torch::jit::fuser::cuda::setEnabled(). (function registerPass)\n\
    [W cuda_graph_fuser.h:17] Warning: RegisterCudaFuseGraph::registerPass() is deprecated.\
    \ Please use torch::jit::fuser::cuda::setEnabled(). (function registerPass)\n\
    terminate called after throwing an instance of 'std::runtime_error'\n  what():\
    \  async response status: Internal - PyTorch execute failure: nvrtc: error: invalid\
    \ value for --gpu-architecture (-arch)\n\nnvrtc compilation failed:\n\n#define\
    \ NAN __int_as_float(0x7fffffff)\n#define POS_INFINITY __int_as_float(0x7f800000)\n\
    #define NEG_INFINITY __int_as_float(0xff800000)\n"
  created_at: 2023-07-16 17:42:16+00:00
  edited: true
  hidden: false
  id: 64b43a082fc8324fcb528d63
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664496061478-noauth.png?w=200&h=200&f=face
      fullname: Clement Ruhm
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: clementruhm
      type: user
    createdAt: '2023-07-18T12:12:01.000Z'
    data:
      edited: false
      editors:
      - clementruhm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8363164663314819
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664496061478-noauth.png?w=200&h=200&f=face
          fullname: Clement Ruhm
          isHf: false
          isPro: false
          name: clementruhm
          type: user
        html: '<p>Hi, indeed service is based on triton:22.08 (<a rel="nofollow" href="https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_22-08.html">https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_22-08.html</a>),
          which has cuda 11.7.1 in it. As far as I see 4090 needs cuda 11.8. I need
          to rebuild my docker with newer triton. I will do it once there is a bit
          more time on my hands. reach out <a rel="nofollow" href="mailto:clement@balacoon.com">clement@balacoon.com</a>
          or join our slack <a rel="nofollow" href="https://join.slack.com/t/balacoon/shared_invite/zt-1syqpvq75-s7iCBJhZcQrsmrLrAU3fhw">https://join.slack.com/t/balacoon/shared_invite/zt-1syqpvq75-s7iCBJhZcQrsmrLrAU3fhw</a>
          for more prompt communication</p>

          '
        raw: Hi, indeed service is based on triton:22.08 (https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_22-08.html),
          which has cuda 11.7.1 in it. As far as I see 4090 needs cuda 11.8. I need
          to rebuild my docker with newer triton. I will do it once there is a bit
          more time on my hands. reach out clement@balacoon.com or join our slack
          https://join.slack.com/t/balacoon/shared_invite/zt-1syqpvq75-s7iCBJhZcQrsmrLrAU3fhw
          for more prompt communication
        updatedAt: '2023-07-18T12:12:01.277Z'
      numEdits: 0
      reactions: []
    id: 64b68191c67700d8a3276dd6
    type: comment
  author: clementruhm
  content: Hi, indeed service is based on triton:22.08 (https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel_22-08.html),
    which has cuda 11.7.1 in it. As far as I see 4090 needs cuda 11.8. I need to rebuild
    my docker with newer triton. I will do it once there is a bit more time on my
    hands. reach out clement@balacoon.com or join our slack https://join.slack.com/t/balacoon/shared_invite/zt-1syqpvq75-s7iCBJhZcQrsmrLrAU3fhw
    for more prompt communication
  created_at: 2023-07-18 11:12:01+00:00
  edited: false
  hidden: false
  id: 64b68191c67700d8a3276dd6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664496061478-noauth.png?w=200&h=200&f=face
      fullname: Clement Ruhm
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: clementruhm
      type: user
    createdAt: '2023-07-18T12:12:06.000Z'
    data:
      status: closed
    id: 64b6819699f00b423d71743a
    type: status-change
  author: clementruhm
  created_at: 2023-07-18 11:12:06+00:00
  id: 64b6819699f00b423d71743a
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: balacoon/tts
repo_type: model
status: closed
target_branch: null
title: support for RTX 4090 graphics card?
