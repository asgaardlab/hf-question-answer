!!python/object:huggingface_hub.community.DiscussionWithDetails
author: WaltsonNedo
conflicting_files: null
created_at: 2024-01-11 03:15:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a969a1a89338800761865b22b5d6c9f.svg
      fullname: WaltsonNedo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WaltsonNedo
      type: user
    createdAt: '2024-01-11T03:15:24.000Z'
    data:
      edited: false
      editors:
      - WaltsonNedo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8311156034469604
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a969a1a89338800761865b22b5d6c9f.svg
          fullname: WaltsonNedo
          isHf: false
          isPro: false
          name: WaltsonNedo
          type: user
        html: '<p>How do you convert the source code to ONNX format? And what your
          environments?<br>I have tried  ''optimum-cli''  and ''optimum.onnxruntime.ORTOptimizer
          '', in [torch1.12 + optimum1.16.1 + python3.8.5].<br>But the onnx file I
          got has lack of the input name ''token_type_ids'', only got the other two,
          which will occur errors in the load &amp; predict process.</p>

          <p>While the same ''optimum-cli'' way I applied to convert the ''bge-rerank-large''
          model,  only difference is task-type from ''feature-extraction'' to ''text-classification'',
          and the onnx file I got works well.</p>

          <p>Can you show me your converting scripts and working environments? thx
          alot.</p>

          '
        raw: "How do you convert the source code to ONNX format? And what your environments?\r\
          \nI have tried  'optimum-cli'  and 'optimum.onnxruntime.ORTOptimizer ',\
          \ in [torch1.12 + optimum1.16.1 + python3.8.5].\r\nBut the onnx file I got\
          \ has lack of the input name 'token_type_ids', only got the other two, which\
          \ will occur errors in the load & predict process.\r\n\r\nWhile the same\
          \ 'optimum-cli' way I applied to convert the 'bge-rerank-large' model, \
          \ only difference is task-type from 'feature-extraction' to 'text-classification',\
          \ and the onnx file I got works well.\r\n\r\nCan you show me your converting\
          \ scripts and working environments? thx alot."
        updatedAt: '2024-01-11T03:15:24.874Z'
      numEdits: 0
      reactions: []
    id: 659f5d4cd2e705b3fb5c6a33
    type: comment
  author: WaltsonNedo
  content: "How do you convert the source code to ONNX format? And what your environments?\r\
    \nI have tried  'optimum-cli'  and 'optimum.onnxruntime.ORTOptimizer ', in [torch1.12\
    \ + optimum1.16.1 + python3.8.5].\r\nBut the onnx file I got has lack of the input\
    \ name 'token_type_ids', only got the other two, which will occur errors in the\
    \ load & predict process.\r\n\r\nWhile the same 'optimum-cli' way I applied to\
    \ convert the 'bge-rerank-large' model,  only difference is task-type from 'feature-extraction'\
    \ to 'text-classification', and the onnx file I got works well.\r\n\r\nCan you\
    \ show me your converting scripts and working environments? thx alot."
  created_at: 2024-01-11 03:15:24+00:00
  edited: false
  hidden: false
  id: 659f5d4cd2e705b3fb5c6a33
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: weakit-v/bge-base-en-v1.5-onnx
repo_type: model
status: open
target_branch: null
title: converting scripts
