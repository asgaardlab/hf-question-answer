!!python/object:huggingface_hub.community.DiscussionWithDetails
author: goodromka
conflicting_files: null
created_at: 2023-11-10 16:55:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: goodromka
      type: user
    createdAt: '2023-11-10T16:55:11.000Z'
    data:
      edited: false
      editors:
      - goodromka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9447598457336426
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: goodromka
          type: user
        html: '<p>Thank you for providing the gguf version of the model. I followed
          your suggestion and utilized make-ggml.py as instructed earlier. However,
          I encountered an error: FileNotFoundError - the tokenizer.model file could
          not be found. It seems that this file is not present in their directory.
          Can you confirm if you used the same make-ggml.py script as previously recommended?
          If affirmative, could you please specify where you obtained the tokenizer.model
          file?</p>

          '
        raw: 'Thank you for providing the gguf version of the model. I followed your
          suggestion and utilized make-ggml.py as instructed earlier. However, I encountered
          an error: FileNotFoundError - the tokenizer.model file could not be found.
          It seems that this file is not present in their directory. Can you confirm
          if you used the same make-ggml.py script as previously recommended? If affirmative,
          could you please specify where you obtained the tokenizer.model file?'
        updatedAt: '2023-11-10T16:55:11.453Z'
      numEdits: 0
      reactions: []
    id: 654e606f12c9597b20a405c5
    type: comment
  author: goodromka
  content: 'Thank you for providing the gguf version of the model. I followed your
    suggestion and utilized make-ggml.py as instructed earlier. However, I encountered
    an error: FileNotFoundError - the tokenizer.model file could not be found. It
    seems that this file is not present in their directory. Can you confirm if you
    used the same make-ggml.py script as previously recommended? If affirmative, could
    you please specify where you obtained the tokenizer.model file?'
  created_at: 2023-11-10 16:55:11+00:00
  edited: false
  hidden: false
  id: 654e606f12c9597b20a405c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: goodromka
      type: user
    createdAt: '2023-11-13T20:22:16.000Z'
    data:
      edited: false
      editors:
      - goodromka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6248570680618286
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: goodromka
          type: user
        html: '<p>I attempted to perform inference using llama.cpp for your model
          following the instructions provided in the README. However, I encountered
          an error related to the tokenizer. I am uncertain whether I overlooked something
          in the process or if llama.cpp is incompatible with this model type. The
          specific error message is as follows:</p>

          <p>...<br>llama_model_loader: - type  f32:  121 tensors<br>llama_model_loader:
          - type q4_K:  361 tensors<br>llama_model_loader: - type q6_K:   61 tensors<br>ERROR:
          byte not found in vocab: ''<br>''<br>Segmentation fault (core dumped)</p>

          <p>I used next commands:</p>

          <p>huggingface-cli download TheBloke/AquilaChat2-34B-GGUF aquilachat2-34b.Q4_K_M.gguf
          --local-dir . --local-dir-use-symlinks False</p>

          <p>./main -ngl 32 -m aquilachat2-34b.Q4_K_M.gguf --color -c 2048 --temp
          0.7 --repeat_penalty 1.1 -n -1 -p "System: A chat between a curious human
          and an artificial intelligence assistant. The assistant gives helpful, detailed,
          and polite answers to the human''s questions.\nHuman: {prompt}\nAssistant:"</p>

          '
        raw: 'I attempted to perform inference using llama.cpp for your model following
          the instructions provided in the README. However, I encountered an error
          related to the tokenizer. I am uncertain whether I overlooked something
          in the process or if llama.cpp is incompatible with this model type. The
          specific error message is as follows:


          ...

          llama_model_loader: - type  f32:  121 tensors

          llama_model_loader: - type q4_K:  361 tensors

          llama_model_loader: - type q6_K:   61 tensors

          ERROR: byte not found in vocab: ''

          ''

          Segmentation fault (core dumped)


          I used next commands:


          huggingface-cli download TheBloke/AquilaChat2-34B-GGUF aquilachat2-34b.Q4_K_M.gguf
          --local-dir . --local-dir-use-symlinks False


          ./main -ngl 32 -m aquilachat2-34b.Q4_K_M.gguf --color -c 2048 --temp 0.7
          --repeat_penalty 1.1 -n -1 -p "System: A chat between a curious human and
          an artificial intelligence assistant. The assistant gives helpful, detailed,
          and polite answers to the human''s questions.\nHuman: {prompt}\nAssistant:"

          '
        updatedAt: '2023-11-13T20:22:16.212Z'
      numEdits: 0
      reactions: []
    id: 65528578c5fcedbaa324cc75
    type: comment
  author: goodromka
  content: 'I attempted to perform inference using llama.cpp for your model following
    the instructions provided in the README. However, I encountered an error related
    to the tokenizer. I am uncertain whether I overlooked something in the process
    or if llama.cpp is incompatible with this model type. The specific error message
    is as follows:


    ...

    llama_model_loader: - type  f32:  121 tensors

    llama_model_loader: - type q4_K:  361 tensors

    llama_model_loader: - type q6_K:   61 tensors

    ERROR: byte not found in vocab: ''

    ''

    Segmentation fault (core dumped)


    I used next commands:


    huggingface-cli download TheBloke/AquilaChat2-34B-GGUF aquilachat2-34b.Q4_K_M.gguf
    --local-dir . --local-dir-use-symlinks False


    ./main -ngl 32 -m aquilachat2-34b.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty
    1.1 -n -1 -p "System: A chat between a curious human and an artificial intelligence
    assistant. The assistant gives helpful, detailed, and polite answers to the human''s
    questions.\nHuman: {prompt}\nAssistant:"

    '
  created_at: 2023-11-13 20:22:16+00:00
  edited: false
  hidden: false
  id: 65528578c5fcedbaa324cc75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-13T21:49:25.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8927540183067322
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Make sure you''re using the latest version of llama.cpp - earlier
          versions had the bug you describe.</p>

          '
        raw: Make sure you're using the latest version of llama.cpp - earlier versions
          had the bug you describe.
        updatedAt: '2023-11-13T21:49:45.955Z'
      numEdits: 1
      reactions: []
    id: 655299e5929eae1101e95526
    type: comment
  author: TheBloke
  content: Make sure you're using the latest version of llama.cpp - earlier versions
    had the bug you describe.
  created_at: 2023-11-13 21:49:25+00:00
  edited: true
  hidden: false
  id: 655299e5929eae1101e95526
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/AquilaChat2-34B-GGUF
repo_type: model
status: open
target_branch: null
title: FileNotFoundError - the tokenizer.model file could not be found
