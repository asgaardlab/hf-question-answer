!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nicoleds
conflicting_files: null
created_at: 2023-06-21 08:00:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7622c1e940db9d5cfb4ae599c608d5c8.svg
      fullname: Nicole
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicoleds
      type: user
    createdAt: '2023-06-21T09:00:28.000Z'
    data:
      edited: false
      editors:
      - nicoleds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5905028581619263
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7622c1e940db9d5cfb4ae599c608d5c8.svg
          fullname: Nicole
          isHf: false
          isPro: false
          name: nicoleds
          type: user
        html: '<p>Hi,</p>

          <p>What''s the difference between the 2 models pytorch_model-00001-of-00002.bin
          and pytorch_model-00002-of-00002.bin? And which model is used for inference
          when we run the code below:</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("TheBloke/wizardLM-7B-HF")</p>

          <p>model = AutoModelForCausalLM.from_pretrained("TheBloke/wizardLM-7B-HF")</p>

          <p>Thanks</p>

          '
        raw: "Hi,\r\n\r\nWhat's the difference between the 2 models pytorch_model-00001-of-00002.bin\
          \ and pytorch_model-00002-of-00002.bin? And which model is used for inference\
          \ when we run the code below:\r\n\r\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          TheBloke/wizardLM-7B-HF\")\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          TheBloke/wizardLM-7B-HF\")\r\n\r\nThanks"
        updatedAt: '2023-06-21T09:00:28.922Z'
      numEdits: 0
      reactions: []
    id: 6492bc2cfe1c0c0843f72a94
    type: comment
  author: nicoleds
  content: "Hi,\r\n\r\nWhat's the difference between the 2 models pytorch_model-00001-of-00002.bin\
    \ and pytorch_model-00002-of-00002.bin? And which model is used for inference\
    \ when we run the code below:\r\n\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"TheBloke/wizardLM-7B-HF\")\r\
    \n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"TheBloke/wizardLM-7B-HF\"\
    )\r\n\r\nThanks"
  created_at: 2023-06-21 08:00:28+00:00
  edited: false
  hidden: false
  id: 6492bc2cfe1c0c0843f72a94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-21T09:07:10.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9137972593307495
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Both are used. It''s called sharding - larger models get split up
          into multiple smaller files.  It makes it easier for people to download
          (can use multiple threads to grab multiple files at once, which is often
          faster), and it used to help load models on multiple GPUs (one shard per
          GPU.)  That''s not needed any more as models can be split across GPUs on
          a per-layer basis.  But sharding is still commonplace for unquantised models
          like this.</p>

          <p>Expect to see:</p>

          <ul>

          <li>Two files for 7B fp16 models</li>

          <li>Three files for 13B fp16 models</li>

          <li>Seven files for 33B fp16 models</li>

          <li>Fourteen files for 65B fp16 models</li>

          </ul>

          <p>It is possible to specify an arbitrary shard size, so some models have
          smaller shards and therefore many more files.</p>

          <p>There''s a json file called <code>pytorch_model.bin.index.json</code>
          that tells the code what files there are and what layers are in each file.</p>

          <p>TLDR: Always download the whole repo and then don''t worry about how
          many files there are. Transformers sorts it out automatically when you do
          <code>model = AutoModelForCausalLM.from_pretrained()</code>, loading all
          the files that are needed for that model.</p>

          '
        raw: 'Both are used. It''s called sharding - larger models get split up into
          multiple smaller files.  It makes it easier for people to download (can
          use multiple threads to grab multiple files at once, which is often faster),
          and it used to help load models on multiple GPUs (one shard per GPU.)  That''s
          not needed any more as models can be split across GPUs on a per-layer basis.  But
          sharding is still commonplace for unquantised models like this.


          Expect to see:

          - Two files for 7B fp16 models

          - Three files for 13B fp16 models

          - Seven files for 33B fp16 models

          - Fourteen files for 65B fp16 models


          It is possible to specify an arbitrary shard size, so some models have smaller
          shards and therefore many more files.


          There''s a json file called `pytorch_model.bin.index.json` that tells the
          code what files there are and what layers are in each file.


          TLDR: Always download the whole repo and then don''t worry about how many
          files there are. Transformers sorts it out automatically when you do `model
          = AutoModelForCausalLM.from_pretrained()`, loading all the files that are
          needed for that model.'
        updatedAt: '2023-06-21T09:07:10.319Z'
      numEdits: 0
      reactions: []
    id: 6492bdbe73aef30728c1df89
    type: comment
  author: TheBloke
  content: 'Both are used. It''s called sharding - larger models get split up into
    multiple smaller files.  It makes it easier for people to download (can use multiple
    threads to grab multiple files at once, which is often faster), and it used to
    help load models on multiple GPUs (one shard per GPU.)  That''s not needed any
    more as models can be split across GPUs on a per-layer basis.  But sharding is
    still commonplace for unquantised models like this.


    Expect to see:

    - Two files for 7B fp16 models

    - Three files for 13B fp16 models

    - Seven files for 33B fp16 models

    - Fourteen files for 65B fp16 models


    It is possible to specify an arbitrary shard size, so some models have smaller
    shards and therefore many more files.


    There''s a json file called `pytorch_model.bin.index.json` that tells the code
    what files there are and what layers are in each file.


    TLDR: Always download the whole repo and then don''t worry about how many files
    there are. Transformers sorts it out automatically when you do `model = AutoModelForCausalLM.from_pretrained()`,
    loading all the files that are needed for that model.'
  created_at: 2023-06-21 08:07:10+00:00
  edited: false
  hidden: false
  id: 6492bdbe73aef30728c1df89
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/wizardLM-7B-HF
repo_type: model
status: open
target_branch: null
title: Difference between the 2 models pytorch_model-00001-of-00002.bin and pytorch_model-00002-of-00002.bin
