!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gabriead
conflicting_files: null
created_at: 2023-05-05 07:23:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
      fullname: Adrian Gabriel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gabriead
      type: user
    createdAt: '2023-05-05T08:23:57.000Z'
    data:
      edited: true
      editors:
      - gabriead
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
          fullname: Adrian Gabriel
          isHf: false
          isPro: false
          name: gabriead
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ I have used your model \"TheBloke/wizardLM-7B-HF\" and fine-tuned it with\
          \ the train_freeform.py script from the wizardLM Gihub repo on a custom\
          \ dataset. The training works fine but the pytorch_model.bin is only a few\
          \ Kilobytes so something is off with saving model.? I am using \"safe_save_model_for_hf_trainer(..)\"\
          \ in their script for saving.  Did you experience something similar? Any\
          \ suggestions of what I could try</p>\n"
        raw: Hi @TheBloke I have used your model "TheBloke/wizardLM-7B-HF" and fine-tuned
          it with the train_freeform.py script from the wizardLM Gihub repo on a custom
          dataset. The training works fine but the pytorch_model.bin is only a few
          Kilobytes so something is off with saving model.? I am using "safe_save_model_for_hf_trainer(..)"
          in their script for saving.  Did you experience something similar? Any suggestions
          of what I could try
        updatedAt: '2023-05-05T08:26:27.965Z'
      numEdits: 1
      reactions: []
    id: 6454bd1da13edf669cd5ad81
    type: comment
  author: gabriead
  content: Hi @TheBloke I have used your model "TheBloke/wizardLM-7B-HF" and fine-tuned
    it with the train_freeform.py script from the wizardLM Gihub repo on a custom
    dataset. The training works fine but the pytorch_model.bin is only a few Kilobytes
    so something is off with saving model.? I am using "safe_save_model_for_hf_trainer(..)"
    in their script for saving.  Did you experience something similar? Any suggestions
    of what I could try
  created_at: 2023-05-05 07:23:57+00:00
  edited: true
  hidden: false
  id: 6454bd1da13edf669cd5ad81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T08:55:09.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''m not an expert on training/fine-tuning, but  I have tested re-creating
          the WizardLM fine tuning method, using their original dataset and relative
          to Llama 7B.  If you were to follow that method you''d take their original
          dataset and add yours to it, and re-run the whole thing.  But that takes
          ~15 hours on 8 x A6000 GPUs.</p>

          <p>Instead what you most likely want to do is train a LoRA.  What that does
          is ''freezes'' the weights in the already-trained WizardLM model, and then
          trains new weights on top.  The resulting file will be very small - maybe
          50-100MB.  But you can then load WizardLM + your LoRA and get the results
          of both.</p>

          <p>Here''s a short introductory video, with code, on LoRA fine tuning: <a
          rel="nofollow" href="https://youtu.be/Us5ZFp16PaU">https://youtu.be/Us5ZFp16PaU</a></p>

          <p>If you''d like a place to talk about this in more detail, with people
          who have done this already, join us on the Alpaca Lora Discord:  <a rel="nofollow"
          href="https://discord.gg/ZMHkCGy9">https://discord.gg/ZMHkCGy9</a></p>

          '
        raw: 'I''m not an expert on training/fine-tuning, but  I have tested re-creating
          the WizardLM fine tuning method, using their original dataset and relative
          to Llama 7B.  If you were to follow that method you''d take their original
          dataset and add yours to it, and re-run the whole thing.  But that takes
          ~15 hours on 8 x A6000 GPUs.


          Instead what you most likely want to do is train a LoRA.  What that does
          is ''freezes'' the weights in the already-trained WizardLM model, and then
          trains new weights on top.  The resulting file will be very small - maybe
          50-100MB.  But you can then load WizardLM + your LoRA and get the results
          of both.


          Here''s a short introductory video, with code, on LoRA fine tuning: https://youtu.be/Us5ZFp16PaU


          If you''d like a place to talk about this in more detail, with people who
          have done this already, join us on the Alpaca Lora Discord:  https://discord.gg/ZMHkCGy9'
        updatedAt: '2023-05-05T08:55:50.883Z'
      numEdits: 2
      reactions: []
    id: 6454c46df61f10d69dc10166
    type: comment
  author: TheBloke
  content: 'I''m not an expert on training/fine-tuning, but  I have tested re-creating
    the WizardLM fine tuning method, using their original dataset and relative to
    Llama 7B.  If you were to follow that method you''d take their original dataset
    and add yours to it, and re-run the whole thing.  But that takes ~15 hours on
    8 x A6000 GPUs.


    Instead what you most likely want to do is train a LoRA.  What that does is ''freezes''
    the weights in the already-trained WizardLM model, and then trains new weights
    on top.  The resulting file will be very small - maybe 50-100MB.  But you can
    then load WizardLM + your LoRA and get the results of both.


    Here''s a short introductory video, with code, on LoRA fine tuning: https://youtu.be/Us5ZFp16PaU


    If you''d like a place to talk about this in more detail, with people who have
    done this already, join us on the Alpaca Lora Discord:  https://discord.gg/ZMHkCGy9'
  created_at: 2023-05-05 07:55:09+00:00
  edited: true
  hidden: false
  id: 6454c46df61f10d69dc10166
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
      fullname: Adrian Gabriel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gabriead
      type: user
    createdAt: '2023-05-05T09:50:10.000Z'
    data:
      edited: false
      editors:
      - gabriead
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
          fullname: Adrian Gabriel
          isHf: false
          isPro: false
          name: gabriead
          type: user
        html: '<p>Hi Tom,<br>thank''s a lot for your response. Just to better understand
          what I might be overlooking: shouldn''t I be able of taking your model and
          further fine-tune it on my custom data using their training script? What
          am I missing here? Or is it because I didn''t recover their original model
          weights and used them for fine-tuning? Can I use your model as base model
          and use the alpaca training script that includes the LoRa technique?</p>

          '
        raw: 'Hi Tom,

          thank''s a lot for your response. Just to better understand what I might
          be overlooking: shouldn''t I be able of taking your model and further fine-tune
          it on my custom data using their training script? What am I missing here?
          Or is it because I didn''t recover their original model weights and used
          them for fine-tuning? Can I use your model as base model and use the alpaca
          training script that includes the LoRa technique?'
        updatedAt: '2023-05-05T09:50:10.309Z'
      numEdits: 0
      reactions: []
    id: 6454d152d55525a4fee0888b
    type: comment
  author: gabriead
  content: 'Hi Tom,

    thank''s a lot for your response. Just to better understand what I might be overlooking:
    shouldn''t I be able of taking your model and further fine-tune it on my custom
    data using their training script? What am I missing here? Or is it because I didn''t
    recover their original model weights and used them for fine-tuning? Can I use
    your model as base model and use the alpaca training script that includes the
    LoRa technique?'
  created_at: 2023-05-05 08:50:10+00:00
  edited: false
  hidden: false
  id: 6454d152d55525a4fee0888b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T09:53:44.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You certainly can use the original WizardLM code, just expect it
          to take a long time and require multiple big GPUS.</p>

          <p>The scripts provided by WizardLM are for a full training. They load the
          original model (they assume you load base Llama 7B, but you could load WizardLM
          7B instead) and then train the specified dataset on top of it.  It''s arguably
          the best method for training, but it''s also slow and expensive.</p>

          <p>The LoRA method is much quicker and doesn''t require nearly as much hardware.
          This is because it freezes the base weights and then trains new weights
          on top of them.   Whereas a full fine tuning might need 8 x A6000 or 4 x
          A100 GPUs, a LoRA fine tune could be done on a single 3090 or 4090 (and
          other GPUs).  The result may not be quite as good, but it still works and
          it''s much more affordable and accessible.</p>

          <p>Yes, you can use the Alpaca Lora code on top of this model. That will
          produce a LoRA fine tuning that starts with WizardLM 7B and then fine tunes
          your own data on top.  That''s probably the best place to start.</p>

          '
        raw: 'You certainly can use the original WizardLM code, just expect it to
          take a long time and require multiple big GPUS.


          The scripts provided by WizardLM are for a full training. They load the
          original model (they assume you load base Llama 7B, but you could load WizardLM
          7B instead) and then train the specified dataset on top of it.  It''s arguably
          the best method for training, but it''s also slow and expensive.


          The LoRA method is much quicker and doesn''t require nearly as much hardware.
          This is because it freezes the base weights and then trains new weights
          on top of them.   Whereas a full fine tuning might need 8 x A6000 or 4 x
          A100 GPUs, a LoRA fine tune could be done on a single 3090 or 4090 (and
          other GPUs).  The result may not be quite as good, but it still works and
          it''s much more affordable and accessible.


          Yes, you can use the Alpaca Lora code on top of this model. That will produce
          a LoRA fine tuning that starts with WizardLM 7B and then fine tunes your
          own data on top.  That''s probably the best place to start.'
        updatedAt: '2023-05-05T09:53:57.233Z'
      numEdits: 1
      reactions: []
    id: 6454d228fe2f48cb4b60bd5b
    type: comment
  author: TheBloke
  content: 'You certainly can use the original WizardLM code, just expect it to take
    a long time and require multiple big GPUS.


    The scripts provided by WizardLM are for a full training. They load the original
    model (they assume you load base Llama 7B, but you could load WizardLM 7B instead)
    and then train the specified dataset on top of it.  It''s arguably the best method
    for training, but it''s also slow and expensive.


    The LoRA method is much quicker and doesn''t require nearly as much hardware.
    This is because it freezes the base weights and then trains new weights on top
    of them.   Whereas a full fine tuning might need 8 x A6000 or 4 x A100 GPUs, a
    LoRA fine tune could be done on a single 3090 or 4090 (and other GPUs).  The result
    may not be quite as good, but it still works and it''s much more affordable and
    accessible.


    Yes, you can use the Alpaca Lora code on top of this model. That will produce
    a LoRA fine tuning that starts with WizardLM 7B and then fine tunes your own data
    on top.  That''s probably the best place to start.'
  created_at: 2023-05-05 08:53:44+00:00
  edited: true
  hidden: false
  id: 6454d228fe2f48cb4b60bd5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
      fullname: Adrian Gabriel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gabriead
      type: user
    createdAt: '2023-05-05T10:16:06.000Z'
    data:
      edited: false
      editors:
      - gabriead
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
          fullname: Adrian Gabriel
          isHf: false
          isPro: false
          name: gabriead
          type: user
        html: '<p>Thank''s again for your feedback! As I have already used your model
          (wizardLM-7B-HF) and fine tuned it with their  training script I wonder
          if you have an idea why the weights/checkpoints that produces are not usabel?  This
          approach results in a pytorch_model.bin that can not be used for inference
          for some reason. When I load the checkpoint it throws an error (File "/anaconda/envs/llamax/lib/python3.10/site-packages/torch/nn/functional.py",
          line 2199, in embedding return torch.embedding(weight, input, padding_idx,
          scale_grad_by_freq, sparse RuntimeError: ''weight'' must be 2-D)</p>

          '
        raw: 'Thank''s again for your feedback! As I have already used your model
          (wizardLM-7B-HF) and fine tuned it with their  training script I wonder
          if you have an idea why the weights/checkpoints that produces are not usabel?  This
          approach results in a pytorch_model.bin that can not be used for inference
          for some reason. When I load the checkpoint it throws an error (File "/anaconda/envs/llamax/lib/python3.10/site-packages/torch/nn/functional.py",
          line 2199, in embedding return torch.embedding(weight, input, padding_idx,
          scale_grad_by_freq, sparse RuntimeError: ''weight'' must be 2-D)'
        updatedAt: '2023-05-05T10:16:06.702Z'
      numEdits: 0
      reactions: []
    id: 6454d766a473375be56ac921
    type: comment
  author: gabriead
  content: 'Thank''s again for your feedback! As I have already used your model (wizardLM-7B-HF)
    and fine tuned it with their  training script I wonder if you have an idea why
    the weights/checkpoints that produces are not usabel?  This approach results in
    a pytorch_model.bin that can not be used for inference for some reason. When I
    load the checkpoint it throws an error (File "/anaconda/envs/llamax/lib/python3.10/site-packages/torch/nn/functional.py",
    line 2199, in embedding return torch.embedding(weight, input, padding_idx, scale_grad_by_freq,
    sparse RuntimeError: ''weight'' must be 2-D)'
  created_at: 2023-05-05 09:16:06+00:00
  edited: false
  hidden: false
  id: 6454d766a473375be56ac921
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T10:17:34.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I don''t know, but if the file is only a few KB it definitely didn''t
          run properly.  Do you have the output log from when it ran?</p>

          <p>How long did it take? It should have been in the region of 12 - 100 hours,
          depending on your hardware</p>

          '
        raw: 'I don''t know, but if the file is only a few KB it definitely didn''t
          run properly.  Do you have the output log from when it ran?


          How long did it take? It should have been in the region of 12 - 100 hours,
          depending on your hardware'
        updatedAt: '2023-05-05T10:17:34.183Z'
      numEdits: 0
      reactions: []
    id: 6454d7bea473375be56acf79
    type: comment
  author: TheBloke
  content: 'I don''t know, but if the file is only a few KB it definitely didn''t
    run properly.  Do you have the output log from when it ran?


    How long did it take? It should have been in the region of 12 - 100 hours, depending
    on your hardware'
  created_at: 2023-05-05 09:17:34+00:00
  edited: false
  hidden: false
  id: 6454d7bea473375be56acf79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
      fullname: Adrian Gabriel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gabriead
      type: user
    createdAt: '2023-05-05T10:19:23.000Z'
    data:
      edited: false
      editors:
      - gabriead
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
          fullname: Adrian Gabriel
          isHf: false
          isPro: false
          name: gabriead
          type: user
        html: '<p>If I undestand your model card correctly the wizard-7B-HF is the
          result of producing the delta weights right so fine tuning this should produce
          a usable model again?</p>

          '
        raw: If I undestand your model card correctly the wizard-7B-HF is the result
          of producing the delta weights right so fine tuning this should produce
          a usable model again?
        updatedAt: '2023-05-05T10:19:23.381Z'
      numEdits: 0
      reactions: []
    id: 6454d82ba473375be56ad842
    type: comment
  author: gabriead
  content: If I undestand your model card correctly the wizard-7B-HF is the result
    of producing the delta weights right so fine tuning this should produce a usable
    model again?
  created_at: 2023-05-05 09:19:23+00:00
  edited: false
  hidden: false
  id: 6454d82ba473375be56ad842
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T10:21:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes you should be able to run a fine tuning on top of this HF format
          model. I''ve not tried it specifically, but it should work.</p>

          <p>The resulting output files would then be the same size as the best model,
          ie around 13GB</p>

          '
        raw: 'Yes you should be able to run a fine tuning on top of this HF format
          model. I''ve not tried it specifically, but it should work.


          The resulting output files would then be the same size as the best model,
          ie around 13GB'
        updatedAt: '2023-05-05T10:21:49.677Z'
      numEdits: 0
      reactions: []
    id: 6454d8bdfe2f48cb4b6148d1
    type: comment
  author: TheBloke
  content: 'Yes you should be able to run a fine tuning on top of this HF format model.
    I''ve not tried it specifically, but it should work.


    The resulting output files would then be the same size as the best model, ie around
    13GB'
  created_at: 2023-05-05 09:21:49+00:00
  edited: false
  hidden: false
  id: 6454d8bdfe2f48cb4b6148d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
      fullname: Adrian Gabriel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gabriead
      type: user
    createdAt: '2023-05-05T10:24:36.000Z'
    data:
      edited: true
      editors:
      - gabriead
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
          fullname: Adrian Gabriel
          isHf: false
          isPro: false
          name: gabriead
          type: user
        html: "<p>I am using an A-100 and it took 3 hours with the original data.\
          \ For the log below I was only using 1 sample  in their data set and 1 epoch\
          \ to make is fast as possible to show the training process. From my understanding\
          \ the training seems to be running just fine but maybe I am overlooking\
          \ something.<br>Original command on terminal:<br>\"deepspeed train_freeform.py\
          \ \\</p>\n<blockquote>\n<pre><code>--model_name_or_path \"TheBloke/wizardLM-7B-HF\"\
          \ \\\n--data_path alpaca_evol_instruct_1.json \\\n--output_dir finetuned_model\\\
          \n--num_train_epochs 1 \\\n--model_max_length 512 \\\n--per_device_train_batch_size\
          \ 8 \\\n--per_device_eval_batch_size 1 \\\n--gradient_accumulation_steps\
          \ 1 \\\n--evaluation_strategy \"no\" \\\n--save_strategy \"steps\" \\\n\
          --save_steps 800 \\\n--save_total_limit 3 \\\n--learning_rate 2e-5 \\\n\
          --warmup_steps 2 \\\n--logging_steps 2 \\\n--lr_scheduler_type \"cosine\"\
          \ \\\n--report_to \"tensorboard\" \\\n--gradient_checkpointing True \\\n\
          --deepspeed deepspeed_config.json \\\n--fp16 True\"\n</code></pre>\n<p>[2023-05-05\
          \ 08:10:06,103] [WARNING] [runner.py:186:fetch_hostfile] Unable to find\
          \ hostfile, will proceed with training with local resources only.<br>[2023-05-05\
          \ 08:10:06,103] [INFO] [runner.py:550:main] cmd = /anaconda/envs/llamax/bin/python\
          \ -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119\
          \ --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None\
          \ train_freeform.py --model_name_or_path TheBloke/wizardLM-7B-HF --data_path\
          \ alpaca_evol_instruct_1.json --output_dir finetuned_model --num_train_epochs\
          \ 1 --model_max_length 512 --per_device_train_batch_size 8 --per_device_eval_batch_size\
          \ 1 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy\
          \ steps --save_steps 800 --save_total_limit 3 --learning_rate 2e-5 --warmup_steps\
          \ 2 --logging_steps 2 --lr_scheduler_type cosine --report_to tensorboard\
          \ --gradient_checkpointing True --deepspeed deepspeed_config.json --fp16\
          \ True<br>[2023-05-05 08:10:07,676] [INFO] [launch.py:142:main] WORLD INFO\
          \ DICT: {'localhost': [0, 1, 2, 3]}<br>[2023-05-05 08:10:07,676] [INFO]\
          \ [launch.py:148:main] nnodes=1, num_local_procs=4, node_rank=0<br>[2023-05-05\
          \ 08:10:07,676] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(&lt;class\
          \ 'list'&gt;, {'localhost': [0, 1, 2, 3]})<br>[2023-05-05 08:10:07,677]\
          \ [INFO] [launch.py:162:main] dist_world_size=4<br>[2023-05-05 08:10:07,677]\
          \ [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3<br>[2023-05-05\
          \ 08:10:37,866] [INFO] [partition_parameters.py:415:<strong>exit</strong>]\
          \ finished initializing model with 6.74B parameters<br>Loading checkpoint\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2\
          \ [00:12&lt;00:00,  6.26s/it]<br>Loading checkpoint shards: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12&lt;00:00,  6.26s/it]<br>Loading\
          \ checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 2/2 [00:12&lt;00:00,  6.26s/it]<br>Loading checkpoint shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12&lt;00:00, \
          \ 6.26s/it]<br>WARNING:root:Loading data...<br>WARNING:root:Formatting inputs...<br>WARNING:root:Tokenizing\
          \ inputs... This may take some time...<br>WARNING:root:Loading data...<br>WARNING:root:Formatting\
          \ inputs...<br>WARNING:root:Tokenizing inputs... This may take some time...<br>WARNING:root:Loading\
          \ data...<br>WARNING:root:Formatting inputs...<br>WARNING:root:Tokenizing\
          \ inputs... This may take some time...<br>WARNING:root:Loading data...<br>WARNING:root:Formatting\
          \ inputs...<br>WARNING:root:Tokenizing inputs... This may take some time...<br>[2023-05-05\
          \ 08:10:55,895] [WARNING] [cpu_adam.py:85:<strong>init</strong>] FP16 params\
          \ for CPUAdam may not work on AMD CPUs<br>[2023-05-05 08:10:55,896] [WARNING]\
          \ [cpu_adam.py:85:<strong>init</strong>] FP16 params for CPUAdam may not\
          \ work on AMD CPUs<br>[2023-05-05 08:10:55,920] [WARNING] [cpu_adam.py:85:<strong>init</strong>]\
          \ FP16 params for CPUAdam may not work on AMD CPUs<br>[2023-05-05 08:10:55,920]\
          \ [WARNING] [cpu_adam.py:85:<strong>init</strong>] FP16 params for CPUAdam\
          \ may not work on AMD CPUs<br>Using /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...Using /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...</p>\n</blockquote>\n<p>Using /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...<br>Using /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...<br>Detected CUDA files, patching ldflags<br>Emitting\
          \ ninja build file /home/azureuser/.cache/torch_extensions/py310_cu113/cpu_adam/build.ninja...<br>Building\
          \ extension module cpu_adam...<br>Allowing ninja to set a default number\
          \ of workers... (overridable by setting the environment variable MAX_JOBS=N)<br>ninja:\
          \ no work to do.<br>Loading extension module cpu_adam...<br>Time to load\
          \ cpu_adam op: 2.449831962585449 seconds<br>Loading extension module cpu_adam...<br>Loading\
          \ extension module cpu_adam...<br>Loading extension module cpu_adam...<br>Time\
          \ to load cpu_adam op: 2.4798665046691895 seconds<br>Time to load cpu_adam\
          \ op: 2.4798264503479004 seconds<br>Time to load cpu_adam op: 2.4801440238952637\
          \ seconds<br>Using /home/azureuser/.cache/torch_extensions/py310_cu113 as\
          \ PyTorch extensions root...<br>Emitting ninja build file /home/azureuser/.cache/torch_extensions/py310_cu113/utils/build.ninja...<br>Building\
          \ extension module utils...<br>Allowing ninja to set a default number of\
          \ workers... (overridable by setting the environment variable MAX_JOBS=N)<br>ninja:\
          \ no work to do.<br>Loading extension module utils...<br>Time to load utils\
          \ op: 0.08765268325805664 seconds<br>Using /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...<br>Emitting ninja build file /home/azureuser/.cache/torch_extensions/py310_cu113/utils/build.ninja...<br>Building\
          \ extension module utils...<br>Allowing ninja to set a default number of\
          \ workers... (overridable by setting the environment variable MAX_JOBS=N)<br>Using\
          \ /home/azureuser/.cache/torch_extensions/py310_cu113 as PyTorch extensions\
          \ root...<br>Using /home/azureuser/.cache/torch_extensions/py310_cu113 as\
          \ PyTorch extensions root...<br>ninja: no work to do.<br>Loading extension\
          \ module utils...<br>Time to load utils op: 0.059262752532958984 seconds<br>Loading\
          \ extension module utils...<br>Time to load utils op: 0.10180521011352539\
          \ seconds<br>Loading extension module utils...<br>Time to load utils op:\
          \ 0.10201025009155273 seconds<br>Parameter Offload: Total persistent parameters:\
          \ 266240 in 65 params<br>Using /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...<br>No modifications detected for re-loaded\
          \ extension module utils, skipping build step...<br>Loading extension module\
          \ utils...<br>Time to load utils op: 0.0006754398345947266 seconds<br>Using\
          \ /home/azureuser/.cache/torch_extensions/py310_cu113 as PyTorch extensions\
          \ root...<br>No modifications detected for re-loaded extension module utils,\
          \ skipping build step...<br>Loading extension module utils...<br>Time to\
          \ load utils op: 0.0006668567657470703 seconds<br>Using /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...<br>No modifications detected for re-loaded\
          \ extension module utils, skipping build step...<br>Loading extension module\
          \ utils...<br>Time to load utils op: 0.0018012523651123047 seconds<br>Using\
          \ /home/azureuser/.cache/torch_extensions/py310_cu113 as PyTorch extensions\
          \ root...<br>No modifications detected for re-loaded extension module utils,\
          \ skipping build step...<br>Loading extension module utils...<br>Time to\
          \ load utils op: 0.0003707408905029297 seconds<br>{'train_runtime': 18.7022,\
          \ 'train_samples_per_second': 0.053, 'train_steps_per_second': 0.053, 'train_loss':\
          \ 0.024322509765625, 'epoch': 1.0}<br>100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:18&lt;00:00, 18.50s/it]<br>[2023-05-05\
          \ 08:13:25,962] [INFO] [launch.py:350:main] Process 27755 exits successfully.<br>[2023-05-05\
          \ 08:13:27,965] [INFO] [launch.py:350:main] Process 27754 exits successfully.<br>[2023-05-05\
          \ 08:13:27,965] [INFO] [launch.py:350:main] Process 27753 exits successfully.<br>[2023-05-05\
          \ 08:13:28,966] [INFO] [launch.py:350:main] Process 27756 exits successfully.</p>\n"
        raw: "I am using an A-100 and it took 3 hours with the original data. For\
          \ the log below I was only using 1 sample  in their data set and 1 epoch\
          \ to make is fast as possible to show the training process. From my understanding\
          \ the training seems to be running just fine but maybe I am overlooking\
          \ something.\nOriginal command on terminal:\n\"deepspeed train_freeform.py\
          \ \\\n>     --model_name_or_path \"TheBloke/wizardLM-7B-HF\" \\\n>     --data_path\
          \ alpaca_evol_instruct_1.json \\\n>     --output_dir finetuned_model\\\n\
          >     --num_train_epochs 1 \\\n>     --model_max_length 512 \\\n>     --per_device_train_batch_size\
          \ 8 \\\n>     --per_device_eval_batch_size 1 \\\n>     --gradient_accumulation_steps\
          \ 1 \\\n>     --evaluation_strategy \"no\" \\\n>     --save_strategy \"\
          steps\" \\\n>     --save_steps 800 \\\n>     --save_total_limit 3 \\\n>\
          \     --learning_rate 2e-5 \\\n>     --warmup_steps 2 \\\n>     --logging_steps\
          \ 2 \\\n>     --lr_scheduler_type \"cosine\" \\\n>     --report_to \"tensorboard\"\
          \ \\\n>     --gradient_checkpointing True \\\n>     --deepspeed deepspeed_config.json\
          \ \\\n>     --fp16 True\"\n[2023-05-05 08:10:06,103] [WARNING] [runner.py:186:fetch_hostfile]\
          \ Unable to find hostfile, will proceed with training with local resources\
          \ only.\n[2023-05-05 08:10:06,103] [INFO] [runner.py:550:main] cmd = /anaconda/envs/llamax/bin/python\
          \ -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119\
          \ --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None\
          \ train_freeform.py --model_name_or_path TheBloke/wizardLM-7B-HF --data_path\
          \ alpaca_evol_instruct_1.json --output_dir finetuned_model --num_train_epochs\
          \ 1 --model_max_length 512 --per_device_train_batch_size 8 --per_device_eval_batch_size\
          \ 1 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy\
          \ steps --save_steps 800 --save_total_limit 3 --learning_rate 2e-5 --warmup_steps\
          \ 2 --logging_steps 2 --lr_scheduler_type cosine --report_to tensorboard\
          \ --gradient_checkpointing True --deepspeed deepspeed_config.json --fp16\
          \ True\n[2023-05-05 08:10:07,676] [INFO] [launch.py:142:main] WORLD INFO\
          \ DICT: {'localhost': [0, 1, 2, 3]}\n[2023-05-05 08:10:07,676] [INFO] [launch.py:148:main]\
          \ nnodes=1, num_local_procs=4, node_rank=0\n[2023-05-05 08:10:07,676] [INFO]\
          \ [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost':\
          \ [0, 1, 2, 3]})\n[2023-05-05 08:10:07,677] [INFO] [launch.py:162:main]\
          \ dist_world_size=4\n[2023-05-05 08:10:07,677] [INFO] [launch.py:164:main]\
          \ Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n[2023-05-05 08:10:37,866] [INFO]\
          \ [partition_parameters.py:415:__exit__] finished initializing model with\
          \ 6.74B parameters\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 2/2 [00:12<00:00,  6.26s/it]\nLoading checkpoint\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2\
          \ [00:12<00:00,  6.26s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12<00:00,  6.26s/it]\n\
          Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 2/2 [00:12<00:00,  6.26s/it]\nWARNING:root:Loading data...\n\
          WARNING:root:Formatting inputs...\nWARNING:root:Tokenizing inputs... This\
          \ may take some time...\nWARNING:root:Loading data...\nWARNING:root:Formatting\
          \ inputs...\nWARNING:root:Tokenizing inputs... This may take some time...\n\
          WARNING:root:Loading data...\nWARNING:root:Formatting inputs...\nWARNING:root:Tokenizing\
          \ inputs... This may take some time...\nWARNING:root:Loading data...\nWARNING:root:Formatting\
          \ inputs...\nWARNING:root:Tokenizing inputs... This may take some time...\n\
          [2023-05-05 08:10:55,895] [WARNING] [cpu_adam.py:85:__init__] FP16 params\
          \ for CPUAdam may not work on AMD CPUs\n[2023-05-05 08:10:55,896] [WARNING]\
          \ [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD\
          \ CPUs\n[2023-05-05 08:10:55,920] [WARNING] [cpu_adam.py:85:__init__] FP16\
          \ params for CPUAdam may not work on AMD CPUs\n[2023-05-05 08:10:55,920]\
          \ [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work\
          \ on AMD CPUs\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...Using /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...\n\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...\nDetected CUDA files, patching ldflags\n\
          Emitting ninja build file /home/azureuser/.cache/torch_extensions/py310_cu113/cpu_adam/build.ninja...\n\
          Building extension module cpu_adam...\nAllowing ninja to set a default number\
          \ of workers... (overridable by setting the environment variable MAX_JOBS=N)\n\
          ninja: no work to do.\nLoading extension module cpu_adam...\nTime to load\
          \ cpu_adam op: 2.449831962585449 seconds\nLoading extension module cpu_adam...\n\
          Loading extension module cpu_adam...\nLoading extension module cpu_adam...\n\
          Time to load cpu_adam op: 2.4798665046691895 seconds\nTime to load cpu_adam\
          \ op: 2.4798264503479004 seconds\nTime to load cpu_adam op: 2.4801440238952637\
          \ seconds\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113 as\
          \ PyTorch extensions root...\nEmitting ninja build file /home/azureuser/.cache/torch_extensions/py310_cu113/utils/build.ninja...\n\
          Building extension module utils...\nAllowing ninja to set a default number\
          \ of workers... (overridable by setting the environment variable MAX_JOBS=N)\n\
          ninja: no work to do.\nLoading extension module utils...\nTime to load utils\
          \ op: 0.08765268325805664 seconds\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...\nEmitting ninja build file /home/azureuser/.cache/torch_extensions/py310_cu113/utils/build.ninja...\n\
          Building extension module utils...\nAllowing ninja to set a default number\
          \ of workers... (overridable by setting the environment variable MAX_JOBS=N)\n\
          Using /home/azureuser/.cache/torch_extensions/py310_cu113 as PyTorch extensions\
          \ root...\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113 as\
          \ PyTorch extensions root...\nninja: no work to do.\nLoading extension module\
          \ utils...\nTime to load utils op: 0.059262752532958984 seconds\nLoading\
          \ extension module utils...\nTime to load utils op: 0.10180521011352539\
          \ seconds\nLoading extension module utils...\nTime to load utils op: 0.10201025009155273\
          \ seconds\nParameter Offload: Total persistent parameters: 266240 in 65\
          \ params\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113 as PyTorch\
          \ extensions root...\nNo modifications detected for re-loaded extension\
          \ module utils, skipping build step...\nLoading extension module utils...\n\
          Time to load utils op: 0.0006754398345947266 seconds\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...\nNo modifications detected for re-loaded\
          \ extension module utils, skipping build step...\nLoading extension module\
          \ utils...\nTime to load utils op: 0.0006668567657470703 seconds\nUsing\
          \ /home/azureuser/.cache/torch_extensions/py310_cu113 as PyTorch extensions\
          \ root...\nNo modifications detected for re-loaded extension module utils,\
          \ skipping build step...\nLoading extension module utils...\nTime to load\
          \ utils op: 0.0018012523651123047 seconds\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
          \ as PyTorch extensions root...\nNo modifications detected for re-loaded\
          \ extension module utils, skipping build step...\nLoading extension module\
          \ utils...\nTime to load utils op: 0.0003707408905029297 seconds\n{'train_runtime':\
          \ 18.7022, 'train_samples_per_second': 0.053, 'train_steps_per_second':\
          \ 0.053, 'train_loss': 0.024322509765625, 'epoch': 1.0}                \
          \                                              \n100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:18<00:00,\
          \ 18.50s/it]\n[2023-05-05 08:13:25,962] [INFO] [launch.py:350:main] Process\
          \ 27755 exits successfully.\n[2023-05-05 08:13:27,965] [INFO] [launch.py:350:main]\
          \ Process 27754 exits successfully.\n[2023-05-05 08:13:27,965] [INFO] [launch.py:350:main]\
          \ Process 27753 exits successfully.\n[2023-05-05 08:13:28,966] [INFO] [launch.py:350:main]\
          \ Process 27756 exits successfully."
        updatedAt: '2023-05-05T10:26:00.801Z'
      numEdits: 2
      reactions: []
    id: 6454d964d55525a4fee1318f
    type: comment
  author: gabriead
  content: "I am using an A-100 and it took 3 hours with the original data. For the\
    \ log below I was only using 1 sample  in their data set and 1 epoch to make is\
    \ fast as possible to show the training process. From my understanding the training\
    \ seems to be running just fine but maybe I am overlooking something.\nOriginal\
    \ command on terminal:\n\"deepspeed train_freeform.py \\\n>     --model_name_or_path\
    \ \"TheBloke/wizardLM-7B-HF\" \\\n>     --data_path alpaca_evol_instruct_1.json\
    \ \\\n>     --output_dir finetuned_model\\\n>     --num_train_epochs 1 \\\n> \
    \    --model_max_length 512 \\\n>     --per_device_train_batch_size 8 \\\n>  \
    \   --per_device_eval_batch_size 1 \\\n>     --gradient_accumulation_steps 1 \\\
    \n>     --evaluation_strategy \"no\" \\\n>     --save_strategy \"steps\" \\\n\
    >     --save_steps 800 \\\n>     --save_total_limit 3 \\\n>     --learning_rate\
    \ 2e-5 \\\n>     --warmup_steps 2 \\\n>     --logging_steps 2 \\\n>     --lr_scheduler_type\
    \ \"cosine\" \\\n>     --report_to \"tensorboard\" \\\n>     --gradient_checkpointing\
    \ True \\\n>     --deepspeed deepspeed_config.json \\\n>     --fp16 True\"\n[2023-05-05\
    \ 08:10:06,103] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile,\
    \ will proceed with training with local resources only.\n[2023-05-05 08:10:06,103]\
    \ [INFO] [runner.py:550:main] cmd = /anaconda/envs/llamax/bin/python -u -m deepspeed.launcher.launch\
    \ --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500\
    \ --enable_each_rank_log=None train_freeform.py --model_name_or_path TheBloke/wizardLM-7B-HF\
    \ --data_path alpaca_evol_instruct_1.json --output_dir finetuned_model --num_train_epochs\
    \ 1 --model_max_length 512 --per_device_train_batch_size 8 --per_device_eval_batch_size\
    \ 1 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps\
    \ --save_steps 800 --save_total_limit 3 --learning_rate 2e-5 --warmup_steps 2\
    \ --logging_steps 2 --lr_scheduler_type cosine --report_to tensorboard --gradient_checkpointing\
    \ True --deepspeed deepspeed_config.json --fp16 True\n[2023-05-05 08:10:07,676]\
    \ [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n[2023-05-05\
    \ 08:10:07,676] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=4, node_rank=0\n\
    [2023-05-05 08:10:07,676] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class\
    \ 'list'>, {'localhost': [0, 1, 2, 3]})\n[2023-05-05 08:10:07,677] [INFO] [launch.py:162:main]\
    \ dist_world_size=4\n[2023-05-05 08:10:07,677] [INFO] [launch.py:164:main] Setting\
    \ CUDA_VISIBLE_DEVICES=0,1,2,3\n[2023-05-05 08:10:37,866] [INFO] [partition_parameters.py:415:__exit__]\
    \ finished initializing model with 6.74B parameters\nLoading checkpoint shards:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2\
    \ [00:12<00:00,  6.26s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12<00:00,  6.26s/it]\n\
    Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 2/2 [00:12<00:00,  6.26s/it]\nLoading checkpoint shards: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12<00:00,\
    \  6.26s/it]\nWARNING:root:Loading data...\nWARNING:root:Formatting inputs...\n\
    WARNING:root:Tokenizing inputs... This may take some time...\nWARNING:root:Loading\
    \ data...\nWARNING:root:Formatting inputs...\nWARNING:root:Tokenizing inputs...\
    \ This may take some time...\nWARNING:root:Loading data...\nWARNING:root:Formatting\
    \ inputs...\nWARNING:root:Tokenizing inputs... This may take some time...\nWARNING:root:Loading\
    \ data...\nWARNING:root:Formatting inputs...\nWARNING:root:Tokenizing inputs...\
    \ This may take some time...\n[2023-05-05 08:10:55,895] [WARNING] [cpu_adam.py:85:__init__]\
    \ FP16 params for CPUAdam may not work on AMD CPUs\n[2023-05-05 08:10:55,896]\
    \ [WARNING] [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on\
    \ AMD CPUs\n[2023-05-05 08:10:55,920] [WARNING] [cpu_adam.py:85:__init__] FP16\
    \ params for CPUAdam may not work on AMD CPUs\n[2023-05-05 08:10:55,920] [WARNING]\
    \ [cpu_adam.py:85:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n\
    Using /home/azureuser/.cache/torch_extensions/py310_cu113 as PyTorch extensions\
    \ root...Using /home/azureuser/.cache/torch_extensions/py310_cu113 as PyTorch\
    \ extensions root...\n\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
    \ as PyTorch extensions root...\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
    \ as PyTorch extensions root...\nDetected CUDA files, patching ldflags\nEmitting\
    \ ninja build file /home/azureuser/.cache/torch_extensions/py310_cu113/cpu_adam/build.ninja...\n\
    Building extension module cpu_adam...\nAllowing ninja to set a default number\
    \ of workers... (overridable by setting the environment variable MAX_JOBS=N)\n\
    ninja: no work to do.\nLoading extension module cpu_adam...\nTime to load cpu_adam\
    \ op: 2.449831962585449 seconds\nLoading extension module cpu_adam...\nLoading\
    \ extension module cpu_adam...\nLoading extension module cpu_adam...\nTime to\
    \ load cpu_adam op: 2.4798665046691895 seconds\nTime to load cpu_adam op: 2.4798264503479004\
    \ seconds\nTime to load cpu_adam op: 2.4801440238952637 seconds\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
    \ as PyTorch extensions root...\nEmitting ninja build file /home/azureuser/.cache/torch_extensions/py310_cu113/utils/build.ninja...\n\
    Building extension module utils...\nAllowing ninja to set a default number of\
    \ workers... (overridable by setting the environment variable MAX_JOBS=N)\nninja:\
    \ no work to do.\nLoading extension module utils...\nTime to load utils op: 0.08765268325805664\
    \ seconds\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113 as PyTorch\
    \ extensions root...\nEmitting ninja build file /home/azureuser/.cache/torch_extensions/py310_cu113/utils/build.ninja...\n\
    Building extension module utils...\nAllowing ninja to set a default number of\
    \ workers... (overridable by setting the environment variable MAX_JOBS=N)\nUsing\
    \ /home/azureuser/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...\n\
    Using /home/azureuser/.cache/torch_extensions/py310_cu113 as PyTorch extensions\
    \ root...\nninja: no work to do.\nLoading extension module utils...\nTime to load\
    \ utils op: 0.059262752532958984 seconds\nLoading extension module utils...\n\
    Time to load utils op: 0.10180521011352539 seconds\nLoading extension module utils...\n\
    Time to load utils op: 0.10201025009155273 seconds\nParameter Offload: Total persistent\
    \ parameters: 266240 in 65 params\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
    \ as PyTorch extensions root...\nNo modifications detected for re-loaded extension\
    \ module utils, skipping build step...\nLoading extension module utils...\nTime\
    \ to load utils op: 0.0006754398345947266 seconds\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
    \ as PyTorch extensions root...\nNo modifications detected for re-loaded extension\
    \ module utils, skipping build step...\nLoading extension module utils...\nTime\
    \ to load utils op: 0.0006668567657470703 seconds\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
    \ as PyTorch extensions root...\nNo modifications detected for re-loaded extension\
    \ module utils, skipping build step...\nLoading extension module utils...\nTime\
    \ to load utils op: 0.0018012523651123047 seconds\nUsing /home/azureuser/.cache/torch_extensions/py310_cu113\
    \ as PyTorch extensions root...\nNo modifications detected for re-loaded extension\
    \ module utils, skipping build step...\nLoading extension module utils...\nTime\
    \ to load utils op: 0.0003707408905029297 seconds\n{'train_runtime': 18.7022,\
    \ 'train_samples_per_second': 0.053, 'train_steps_per_second': 0.053, 'train_loss':\
    \ 0.024322509765625, 'epoch': 1.0}                                           \
    \                   \n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 1/1 [00:18<00:00, 18.50s/it]\n[2023-05-05 08:13:25,962] [INFO]\
    \ [launch.py:350:main] Process 27755 exits successfully.\n[2023-05-05 08:13:27,965]\
    \ [INFO] [launch.py:350:main] Process 27754 exits successfully.\n[2023-05-05 08:13:27,965]\
    \ [INFO] [launch.py:350:main] Process 27753 exits successfully.\n[2023-05-05 08:13:28,966]\
    \ [INFO] [launch.py:350:main] Process 27756 exits successfully."
  created_at: 2023-05-05 09:24:36+00:00
  edited: true
  hidden: false
  id: 6454d964d55525a4fee1318f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T10:30:43.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Based on the progress bar at the end it only trained one row of
          data, which is why it took 18 seconds instead of 12+ hours for 1 epoch :)  When
          I did a quick test on 4 x A100 80GB the ETA was 35 hours for 3 epochs on
          their original 70k WizardLM dataset</p>

          <p>show me <code>ls -al alpaca_evol_instruct_1.json</code></p>

          '
        raw: 'Based on the progress bar at the end it only trained one row of data,
          which is why it took 18 seconds instead of 12+ hours for 1 epoch :)  When
          I did a quick test on 4 x A100 80GB the ETA was 35 hours for 3 epochs on
          their original 70k WizardLM dataset


          show me `ls -al alpaca_evol_instruct_1.json`'
        updatedAt: '2023-05-05T10:30:43.385Z'
      numEdits: 0
      reactions: []
    id: 6454dad3a473375be56b103b
    type: comment
  author: TheBloke
  content: 'Based on the progress bar at the end it only trained one row of data,
    which is why it took 18 seconds instead of 12+ hours for 1 epoch :)  When I did
    a quick test on 4 x A100 80GB the ETA was 35 hours for 3 epochs on their original
    70k WizardLM dataset


    show me `ls -al alpaca_evol_instruct_1.json`'
  created_at: 2023-05-05 09:30:43+00:00
  edited: false
  hidden: false
  id: 6454dad3a473375be56b103b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
      fullname: Adrian Gabriel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gabriead
      type: user
    createdAt: '2023-05-05T11:00:55.000Z'
    data:
      edited: false
      editors:
      - gabriead
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
          fullname: Adrian Gabriel
          isHf: false
          isPro: false
          name: gabriead
          type: user
        html: "<p>Yeah exactly (there is just one sample in the data set regarding\
          \ the log above) I don't have the original log (when I was training on my\
          \ full dataset) that's why I quickly reproduced it to show the output.<br>For\
          \ the full fledged \"alpaca_evol_instruct_70k.json\" I can confirm what\
          \ you said earlier it will take 35 hours (see log below) using your model.\
          \ So long story short: I think that in theory the training is running correctly.\
          \ The question that remains is why is it not worring for my specific data\
          \ set? I have currently no idea how to debug this.</p>\n<p>This is the log\
          \ using your model and the alpaca_evol_instruct_70k.json.<br>deepspeed train_freeform.py\
          \ \\</p>\n<blockquote>\n<pre><code>--model_name_or_path \"TheBloke/wizardLM-7B-HF\"\
          \ \\\n--data_path alpaca_evol_instruct_70k.json \\\n--output_dir finetuned_model\\\
          \n--num_train_epochs 3 \\\n--model_max_length 512 \\\n--per_device_train_batch_size\
          \ 8 \\\n--per_device_eval_batch_size 1 \\\n--gradient_accumulation_steps\
          \ 1 \\\n--evaluation_strategy \"no\" \\\n--save_strategy \"steps\" \\\n\
          --save_steps 800 \\\n--save_total_limit 3 \\\n--learning_rate 2e-5 \\\n\
          --warmup_steps 2 \\\n--logging_steps 2 \\\n--lr_scheduler_type \"cosine\"\
          \ \\\n--report_to \"tensorboard\" \\\n--gradient_checkpointing True \\\n\
          --deepspeed deepspeed_config.json \\\n--fp16 True\n</code></pre>\n<p>[2023-05-05\
          \ 10:48:06,422] [WARNING] [runner.py:186:fetch_hostfile] Unable to find\
          \ hostfile, will proceed with training with local resources only.<br>[2023-05-05\
          \ 10:48:06,422] [INFO] [runner.py:550:main] cmd = /anaconda/envs/llamax/bin/python\
          \ -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119\
          \ --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None\
          \ train_freeform.py --model_name_or_path TheBloke/wizardLM-7B-HF --data_path\
          \ alpaca_evol_instruct_70k.json --output_dir finetuned_model --num_train_epochs\
          \ 3 --model_max_length 512 --per_device_train_batch_size 8 --per_device_eval_batch_size\
          \ 1 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy\
          \ steps --save_steps 800 --save_total_limit 3 --learning_rate 2e-5 --warmup_steps\
          \ 2 --logging_steps 2 --lr_scheduler_type cosine --report_to tensorboard\
          \ --gradient_checkpointing True --deepspeed deepspeed_config.json --fp16\
          \ True<br>[2023-05-05 10:48:07,879] [INFO] [launch.py:142:main] WORLD INFO\
          \ DICT: {'localhost': [0, 1, 2, 3]}<br>[2023-05-05 10:48:07,879] [INFO]\
          \ [launch.py:148:main] nnodes=1, num_local_procs=4, node_rank=0<br>[2023-05-05\
          \ 10:48:07,879] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(&lt;class\
          \ 'list'&gt;, {'localhost': [0, 1, 2, 3]})<br>[2023-05-05 10:48:07,879]\
          \ [INFO] [launch.py:162:main] dist_world_size=4<br>[2023-05-05 10:48:07,879]\
          \ [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3<br>Downloading\
          \ (\u2026)lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 555/555 [00:00&lt;00:00, 1.32MB/s]<br>Downloading (\u2026\
          )model.bin.index.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.8k/26.8k [00:00&lt;00:00,\
          \ 56.3MB/s]<br>Downloading (\u2026)l-00001-of-00002.bin: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 9.98G/9.98G [00:18&lt;00:00, 548MB/s]<br>Downloading\
          \ (\u2026)l-00002-of-00002.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.50G/3.50G\
          \ [00:06&lt;00:00, 537MB/s]<br>Downloading shards: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 2/2 [00:25&lt;00:00, 12.53s/it]<br>Downloading shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 2/2 [00:25&lt;00:00, 12.52s/it]<br>Downloading shards:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 2/2 [00:25&lt;00:00, 12.52s/it]<br>Downloading\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:25&lt;00:00, 12.54s/it]<br>[2023-05-05\
          \ 10:49:10,190] [INFO] [partition_parameters.py:415:<strong>exit</strong>]\
          \ finished initializing model with 6.74B parameters<br>Loading checkpoint\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2\
          \ [00:12&lt;00:00,  6.48s/it]<br>Loading checkpoint shards: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12&lt;00:00,  6.48s/it]<br>Loading\
          \ checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 2/2 [00:12&lt;00:00,  6.49s/it]<br>Loading checkpoint shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12&lt;00:00, \
          \ 6.49s/it]<br>Downloading (\u2026)neration_config.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 132/132 [00:00&lt;00:00, 227kB/s]<br>Downloading\
          \ (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 727/727 [00:00&lt;00:00, 1.45MB/s]<br>Downloading tokenizer.model:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 500k/500k [00:00&lt;00:00, 91.5MB/s]<br>Downloading\
          \ (\u2026)in/added_tokens.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 21.0/21.0 [00:00&lt;00:00, 57.6kB/s]<br>Downloading (\u2026)cial_tokens_map.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 435/435 [00:00&lt;00:00,\
          \ 857kB/s]<br>WARNING:root:Loading data...<br>WARNING:root:Loading data...<br>WARNING:root:Loading\
          \ data...<br>WARNING:root:Loading data...<br>.......................   \
          \                                                                      \
          \                                                  </p>\n</blockquote>\n\
          <p>No modifications detected for re-loaded extension module utils, skipping\
          \ build step...<br>Loading extension module utils...<br>Time to load utils\
          \ op: 0.0004246234893798828 seconds<br>{'loss': 0.1972, 'learning_rate':\
          \ 0.0, 'epoch': 0.0}<br>{'loss': 0.2099, 'learning_rate': 0.0, 'epoch':\
          \ 0.0}<br>{'loss': 0.2171, 'learning_rate': 0.0, 'epoch': 0.0}<br>{'loss':\
          \ 0.2163, 'learning_rate': 0.0, 'epoch': 0.0}<br>{'loss': 0.2404, 'learning_rate':\
          \ 0.0, 'epoch': 0.0}<br>{'loss': 0.2244, 'learning_rate': 0.0, 'epoch':\
          \ 0.01}<br>{'loss': 0.1907, 'learning_rate': 0.0, 'epoch': 0.01}<br>{'loss':\
          \ 0.2223, 'learning_rate': 0.0, 'epoch': 0.01}<br>{'loss': 0.2312, 'learning_rate':\
          \ 0.0, 'epoch': 0.01}<br>  0%|\u258D                                   \
          \                                                                      \
          \                                                       | 19/6564 [05:01&lt;29:32:06,\
          \ 16.25s/it]</p>\n"
        raw: "Yeah exactly (there is just one sample in the data set regarding the\
          \ log above) I don't have the original log (when I was training on my full\
          \ dataset) that's why I quickly reproduced it to show the output. \nFor\
          \ the full fledged \"alpaca_evol_instruct_70k.json\" I can confirm what\
          \ you said earlier it will take 35 hours (see log below) using your model.\
          \ So long story short: I think that in theory the training is running correctly.\
          \ The question that remains is why is it not worring for my specific data\
          \ set? I have currently no idea how to debug this.\n\nThis is the log using\
          \ your model and the alpaca_evol_instruct_70k.json. \ndeepspeed train_freeform.py\
          \ \\\n>     --model_name_or_path \"TheBloke/wizardLM-7B-HF\" \\\n>     --data_path\
          \ alpaca_evol_instruct_70k.json \\\n>     --output_dir finetuned_model\\\
          \n>     --num_train_epochs 3 \\\n>     --model_max_length 512 \\\n>    \
          \ --per_device_train_batch_size 8 \\\n>     --per_device_eval_batch_size\
          \ 1 \\\n>     --gradient_accumulation_steps 1 \\\n>     --evaluation_strategy\
          \ \"no\" \\\n>     --save_strategy \"steps\" \\\n>     --save_steps 800\
          \ \\\n>     --save_total_limit 3 \\\n>     --learning_rate 2e-5 \\\n>  \
          \   --warmup_steps 2 \\\n>     --logging_steps 2 \\\n>     --lr_scheduler_type\
          \ \"cosine\" \\\n>     --report_to \"tensorboard\" \\\n>     --gradient_checkpointing\
          \ True \\\n>     --deepspeed deepspeed_config.json \\\n>     --fp16 True\n\
          [2023-05-05 10:48:06,422] [WARNING] [runner.py:186:fetch_hostfile] Unable\
          \ to find hostfile, will proceed with training with local resources only.\n\
          [2023-05-05 10:48:06,422] [INFO] [runner.py:550:main] cmd = /anaconda/envs/llamax/bin/python\
          \ -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119\
          \ --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None\
          \ train_freeform.py --model_name_or_path TheBloke/wizardLM-7B-HF --data_path\
          \ alpaca_evol_instruct_70k.json --output_dir finetuned_model --num_train_epochs\
          \ 3 --model_max_length 512 --per_device_train_batch_size 8 --per_device_eval_batch_size\
          \ 1 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy\
          \ steps --save_steps 800 --save_total_limit 3 --learning_rate 2e-5 --warmup_steps\
          \ 2 --logging_steps 2 --lr_scheduler_type cosine --report_to tensorboard\
          \ --gradient_checkpointing True --deepspeed deepspeed_config.json --fp16\
          \ True\n[2023-05-05 10:48:07,879] [INFO] [launch.py:142:main] WORLD INFO\
          \ DICT: {'localhost': [0, 1, 2, 3]}\n[2023-05-05 10:48:07,879] [INFO] [launch.py:148:main]\
          \ nnodes=1, num_local_procs=4, node_rank=0\n[2023-05-05 10:48:07,879] [INFO]\
          \ [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost':\
          \ [0, 1, 2, 3]})\n[2023-05-05 10:48:07,879] [INFO] [launch.py:162:main]\
          \ dist_world_size=4\n[2023-05-05 10:48:07,879] [INFO] [launch.py:164:main]\
          \ Setting CUDA_VISIBLE_DEVICES=0,1,2,3\nDownloading (\u2026)lve/main/config.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 555/555 [00:00<00:00,\
          \ 1.32MB/s]\nDownloading (\u2026)model.bin.index.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 26.8k/26.8k [00:00<00:00, 56.3MB/s]\nDownloading (\u2026)l-00001-of-00002.bin:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 9.98G/9.98G [00:18<00:00, 548MB/s]\n\
          Downloading (\u2026)l-00002-of-00002.bin: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 3.50G/3.50G [00:06<00:00, 537MB/s]\nDownloading shards: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 2/2 [00:25<00:00, 12.53s/it]\nDownloading shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 2/2 [00:25<00:00, 12.52s/it]\nDownloading shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 2/2 [00:25<00:00, 12.52s/it]\nDownloading shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 2/2 [00:25<00:00, 12.54s/it]\n[2023-05-05 10:49:10,190]\
          \ [INFO] [partition_parameters.py:415:__exit__] finished initializing model\
          \ with 6.74B parameters\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12<00:00,  6.48s/it]\nLoading\
          \ checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 2/2 [00:12<00:00,  6.48s/it]\nLoading checkpoint shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12<00:00,  6.49s/it]\n\
          Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588| 2/2 [00:12<00:00,  6.49s/it]\nDownloading (\u2026)neration_config.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 132/132 [00:00<00:00,\
          \ 227kB/s]\nDownloading (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 727/727 [00:00<00:00, 1.45MB/s]\nDownloading\
          \ tokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 500k/500k [00:00<00:00, 91.5MB/s]\n\
          Downloading (\u2026)in/added_tokens.json: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 21.0/21.0 [00:00<00:00, 57.6kB/s]\nDownloading (\u2026)cial_tokens_map.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 435/435 [00:00<00:00,\
          \ 857kB/s]\nWARNING:root:Loading data...\nWARNING:root:Loading data...\n\
          WARNING:root:Loading data...\nWARNING:root:Loading data...\n.......................\
          \                                                                      \
          \                                                     \n\n\nNo modifications\
          \ detected for re-loaded extension module utils, skipping build step...\n\
          Loading extension module utils...\nTime to load utils op: 0.0004246234893798828\
          \ seconds\n{'loss': 0.1972, 'learning_rate': 0.0, 'epoch': 0.0}        \
          \                                                                      \
          \                                                                      \
          \   \n{'loss': 0.2099, 'learning_rate': 0.0, 'epoch': 0.0}             \
          \                                                                      \
          \                                                                    \n\
          {'loss': 0.2171, 'learning_rate': 0.0, 'epoch': 0.0}                   \
          \                                                                      \
          \                                                              \n{'loss':\
          \ 0.2163, 'learning_rate': 0.0, 'epoch': 0.0}                          \
          \                                                                      \
          \                                                       \n{'loss': 0.2404,\
          \ 'learning_rate': 0.0, 'epoch': 0.0}                                  \
          \                                                                      \
          \                                               \n{'loss': 0.2244, 'learning_rate':\
          \ 0.0, 'epoch': 0.01}                                                  \
          \                                                                      \
          \                              \n{'loss': 0.1907, 'learning_rate': 0.0,\
          \ 'epoch': 0.01}                                                       \
          \                                                                      \
          \                         \n{'loss': 0.2223, 'learning_rate': 0.0, 'epoch':\
          \ 0.01}                                                                \
          \                                                                      \
          \                \n{'loss': 0.2312, 'learning_rate': 0.0, 'epoch': 0.01}\
          \                                                                      \
          \                                                                      \
          \          \n  0%|\u258D                                               \
          \                                                                      \
          \                                           | 19/6564 [05:01<29:32:06, 16.25s/it]"
        updatedAt: '2023-05-05T11:00:55.613Z'
      numEdits: 0
      reactions: []
    id: 6454e1e7a473375be56ba448
    type: comment
  author: gabriead
  content: "Yeah exactly (there is just one sample in the data set regarding the log\
    \ above) I don't have the original log (when I was training on my full dataset)\
    \ that's why I quickly reproduced it to show the output. \nFor the full fledged\
    \ \"alpaca_evol_instruct_70k.json\" I can confirm what you said earlier it will\
    \ take 35 hours (see log below) using your model. So long story short: I think\
    \ that in theory the training is running correctly. The question that remains\
    \ is why is it not worring for my specific data set? I have currently no idea\
    \ how to debug this.\n\nThis is the log using your model and the alpaca_evol_instruct_70k.json.\
    \ \ndeepspeed train_freeform.py \\\n>     --model_name_or_path \"TheBloke/wizardLM-7B-HF\"\
    \ \\\n>     --data_path alpaca_evol_instruct_70k.json \\\n>     --output_dir finetuned_model\\\
    \n>     --num_train_epochs 3 \\\n>     --model_max_length 512 \\\n>     --per_device_train_batch_size\
    \ 8 \\\n>     --per_device_eval_batch_size 1 \\\n>     --gradient_accumulation_steps\
    \ 1 \\\n>     --evaluation_strategy \"no\" \\\n>     --save_strategy \"steps\"\
    \ \\\n>     --save_steps 800 \\\n>     --save_total_limit 3 \\\n>     --learning_rate\
    \ 2e-5 \\\n>     --warmup_steps 2 \\\n>     --logging_steps 2 \\\n>     --lr_scheduler_type\
    \ \"cosine\" \\\n>     --report_to \"tensorboard\" \\\n>     --gradient_checkpointing\
    \ True \\\n>     --deepspeed deepspeed_config.json \\\n>     --fp16 True\n[2023-05-05\
    \ 10:48:06,422] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile,\
    \ will proceed with training with local resources only.\n[2023-05-05 10:48:06,422]\
    \ [INFO] [runner.py:550:main] cmd = /anaconda/envs/llamax/bin/python -u -m deepspeed.launcher.launch\
    \ --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500\
    \ --enable_each_rank_log=None train_freeform.py --model_name_or_path TheBloke/wizardLM-7B-HF\
    \ --data_path alpaca_evol_instruct_70k.json --output_dir finetuned_model --num_train_epochs\
    \ 3 --model_max_length 512 --per_device_train_batch_size 8 --per_device_eval_batch_size\
    \ 1 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps\
    \ --save_steps 800 --save_total_limit 3 --learning_rate 2e-5 --warmup_steps 2\
    \ --logging_steps 2 --lr_scheduler_type cosine --report_to tensorboard --gradient_checkpointing\
    \ True --deepspeed deepspeed_config.json --fp16 True\n[2023-05-05 10:48:07,879]\
    \ [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n[2023-05-05\
    \ 10:48:07,879] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=4, node_rank=0\n\
    [2023-05-05 10:48:07,879] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class\
    \ 'list'>, {'localhost': [0, 1, 2, 3]})\n[2023-05-05 10:48:07,879] [INFO] [launch.py:162:main]\
    \ dist_world_size=4\n[2023-05-05 10:48:07,879] [INFO] [launch.py:164:main] Setting\
    \ CUDA_VISIBLE_DEVICES=0,1,2,3\nDownloading (\u2026)lve/main/config.json: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 555/555 [00:00<00:00,\
    \ 1.32MB/s]\nDownloading (\u2026)model.bin.index.json: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 26.8k/26.8k [00:00<00:00, 56.3MB/s]\nDownloading (\u2026\
    )l-00001-of-00002.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.98G/9.98G\
    \ [00:18<00:00, 548MB/s]\nDownloading (\u2026)l-00002-of-00002.bin: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.50G/3.50G [00:06<00:00, 537MB/s]\n\
    Downloading shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:25<00:00, 12.53s/it]\n\
    Downloading shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:25<00:00, 12.52s/it]\n\
    Downloading shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:25<00:00, 12.52s/it]\n\
    Downloading shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:25<00:00, 12.54s/it]\n\
    [2023-05-05 10:49:10,190] [INFO] [partition_parameters.py:415:__exit__] finished\
    \ initializing model with 6.74B parameters\nLoading checkpoint shards: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12<00:00,\
    \  6.48s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 2/2 [00:12<00:00,  6.48s/it]\nLoading checkpoint\
    \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    | 2/2 [00:12<00:00,  6.49s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:12<00:00,  6.49s/it]\n\
    Downloading (\u2026)neration_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 132/132 [00:00<00:00, 227kB/s]\nDownloading\
    \ (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 727/727 [00:00<00:00, 1.45MB/s]\nDownloading tokenizer.model:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 500k/500k [00:00<00:00, 91.5MB/s]\nDownloading\
    \ (\u2026)in/added_tokens.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 21.0/21.0 [00:00<00:00, 57.6kB/s]\nDownloading (\u2026)cial_tokens_map.json:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    | 435/435 [00:00<00:00, 857kB/s]\nWARNING:root:Loading data...\nWARNING:root:Loading\
    \ data...\nWARNING:root:Loading data...\nWARNING:root:Loading data...\n.......................\
    \                                                                            \
    \                                               \n\n\nNo modifications detected\
    \ for re-loaded extension module utils, skipping build step...\nLoading extension\
    \ module utils...\nTime to load utils op: 0.0004246234893798828 seconds\n{'loss':\
    \ 0.1972, 'learning_rate': 0.0, 'epoch': 0.0}                                \
    \                                                                            \
    \                                           \n{'loss': 0.2099, 'learning_rate':\
    \ 0.0, 'epoch': 0.0}                                                         \
    \                                                                            \
    \                  \n{'loss': 0.2171, 'learning_rate': 0.0, 'epoch': 0.0}    \
    \                                                                            \
    \                                                                       \n{'loss':\
    \ 0.2163, 'learning_rate': 0.0, 'epoch': 0.0}                                \
    \                                                                            \
    \                                           \n{'loss': 0.2404, 'learning_rate':\
    \ 0.0, 'epoch': 0.0}                                                         \
    \                                                                            \
    \                  \n{'loss': 0.2244, 'learning_rate': 0.0, 'epoch': 0.01}   \
    \                                                                            \
    \                                                                       \n{'loss':\
    \ 0.1907, 'learning_rate': 0.0, 'epoch': 0.01}                               \
    \                                                                            \
    \                                           \n{'loss': 0.2223, 'learning_rate':\
    \ 0.0, 'epoch': 0.01}                                                        \
    \                                                                            \
    \                  \n{'loss': 0.2312, 'learning_rate': 0.0, 'epoch': 0.01}   \
    \                                                                            \
    \                                                                       \n  0%|\u258D\
    \                                                                            \
    \                                                                            \
    \        | 19/6564 [05:01<29:32:06, 16.25s/it]"
  created_at: 2023-05-05 10:00:55+00:00
  edited: false
  hidden: false
  id: 6454e1e7a473375be56ba448
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
      fullname: Adrian Gabriel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gabriead
      type: user
    createdAt: '2023-05-05T11:03:43.000Z'
    data:
      edited: false
      editors:
      - gabriead
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
          fullname: Adrian Gabriel
          isHf: false
          isPro: false
          name: gabriead
          type: user
        html: '<p>If I use a very small fraction the alpaca_evol_instruct_70k.json
          data set (say five samples) and fine-tune it should produce a pytorch_model.bin
          that I should be able to load again and do inference with correct?</p>

          '
        raw: If I use a very small fraction the alpaca_evol_instruct_70k.json data
          set (say five samples) and fine-tune it should produce a pytorch_model.bin
          that I should be able to load again and do inference with correct?
        updatedAt: '2023-05-05T11:03:43.636Z'
      numEdits: 0
      reactions: []
    id: 6454e28fd55525a4fee1ef63
    type: comment
  author: gabriead
  content: If I use a very small fraction the alpaca_evol_instruct_70k.json data set
    (say five samples) and fine-tune it should produce a pytorch_model.bin that I
    should be able to load again and do inference with correct?
  created_at: 2023-05-05 10:03:43+00:00
  edited: false
  hidden: false
  id: 6454e28fd55525a4fee1ef63
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T11:08:03.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>OK I see.  And yes I think that should work.  Although I've never\
          \ actually gone as far as the full training, I just started it and let it\
          \ run for 30-60 mins to get an idea of how it worked.</p>\n<p> I know a\
          \ couple of people who have completed the full training, such as <span data-props=\"\
          {&quot;user&quot;:&quot;ehartford&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/ehartford\">@<span class=\"underline\">ehartford</span></a></span>\n\
          \n\t</span></span> , and I know they tweaked the code a bit in certain places.\
          \   Maybe the final model saving was one of those.</p>\n<p>Why don't you\
          \ come to the Alpaca Lora Discord as it'll be easier to get support there.\
          \  <a rel=\"nofollow\" href=\"https://discord.gg/eSdptpkm\">https://discord.gg/eSdptpkm</a></p>\n"
        raw: "OK I see.  And yes I think that should work.  Although I've never actually\
          \ gone as far as the full training, I just started it and let it run for\
          \ 30-60 mins to get an idea of how it worked.\n\n I know a couple of people\
          \ who have completed the full training, such as @ehartford , and I know\
          \ they tweaked the code a bit in certain places.   Maybe the final model\
          \ saving was one of those.\n\nWhy don't you come to the Alpaca Lora Discord\
          \ as it'll be easier to get support there.  https://discord.gg/eSdptpkm"
        updatedAt: '2023-05-05T11:08:24.605Z'
      numEdits: 1
      reactions: []
    id: 6454e393f61f10d69dc3a7f7
    type: comment
  author: TheBloke
  content: "OK I see.  And yes I think that should work.  Although I've never actually\
    \ gone as far as the full training, I just started it and let it run for 30-60\
    \ mins to get an idea of how it worked.\n\n I know a couple of people who have\
    \ completed the full training, such as @ehartford , and I know they tweaked the\
    \ code a bit in certain places.   Maybe the final model saving was one of those.\n\
    \nWhy don't you come to the Alpaca Lora Discord as it'll be easier to get support\
    \ there.  https://discord.gg/eSdptpkm"
  created_at: 2023-05-05 10:08:03+00:00
  edited: true
  hidden: false
  id: 6454e393f61f10d69dc3a7f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
      fullname: Adrian Gabriel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gabriead
      type: user
    createdAt: '2023-05-05T11:17:44.000Z'
    data:
      edited: false
      editors:
      - gabriead
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612a16dbb0e033524c184f80ee5c1bf7.svg
          fullname: Adrian Gabriel
          isHf: false
          isPro: false
          name: gabriead
          type: user
        html: '<p>Thank''s a lot for your effort so far! Yes then I will join you
          guys in Discord :-)</p>

          '
        raw: Thank's a lot for your effort so far! Yes then I will join you guys in
          Discord :-)
        updatedAt: '2023-05-05T11:17:44.356Z'
      numEdits: 0
      reactions: []
    id: 6454e5d8d55525a4fee23a63
    type: comment
  author: gabriead
  content: Thank's a lot for your effort so far! Yes then I will join you guys in
    Discord :-)
  created_at: 2023-05-05 10:17:44+00:00
  edited: false
  hidden: false
  id: 6454e5d8d55525a4fee23a63
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-05-16T17:24:14.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>How can we join the Alpaca Lora Discord? Because the link is no
          longer valid.</p>

          '
        raw: How can we join the Alpaca Lora Discord? Because the link is no longer
          valid.
        updatedAt: '2023-05-16T17:24:14.828Z'
      numEdits: 0
      reactions: []
    id: 6463bc3e65d811c4962946b8
    type: comment
  author: CR2022
  content: How can we join the Alpaca Lora Discord? Because the link is no longer
    valid.
  created_at: 2023-05-16 16:24:14+00:00
  edited: false
  hidden: false
  id: 6463bc3e65d811c4962946b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-16T17:25:13.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Here you go <a rel="nofollow" href="https://discord.gg/Rh9e8MfH">https://discord.gg/Rh9e8MfH</a></p>

          '
        raw: Here you go https://discord.gg/Rh9e8MfH
        updatedAt: '2023-05-16T17:25:13.608Z'
      numEdits: 0
      reactions: []
    id: 6463bc7906c6a952bd02658d
    type: comment
  author: TheBloke
  content: Here you go https://discord.gg/Rh9e8MfH
  created_at: 2023-05-16 16:25:13+00:00
  edited: false
  hidden: false
  id: 6463bc7906c6a952bd02658d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/891e1c2074e4bd651af370d2d4ee1e64.svg
      fullname: jay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: waytohou
      type: user
    createdAt: '2023-07-04T11:20:46.000Z'
    data:
      edited: false
      editors:
      - waytohou
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/891e1c2074e4bd651af370d2d4ee1e64.svg
          fullname: jay
          isHf: false
          isPro: false
          name: waytohou
          type: user
        html: "<blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ I have used your model \"TheBloke/wizardLM-7B-HF\" and fine-tuned it with\
          \ the train_freeform.py script from the wizardLM Gihub repo on a custom\
          \ dataset. The training works fine but the pytorch_model.bin is only a few\
          \ Kilobytes so something is off with saving model.? I am using \"safe_save_model_for_hf_trainer(..)\"\
          \ in their script for saving.  Did you experience something similar? Any\
          \ suggestions of what I could try</p>\n</blockquote>\n<p>hi\uFF0Cgabriead,\
          \ i meet the same problems here, the pytorch_model.bin is only a few MB\
          \ that can't be load correctly, it always show the 'weight' must be 2-D\
          \ error, i don't know how to do with it? Did you solve it in the end? if\
          \ you solved it, can you tell me how? </p>\n"
        raw: "> Hi @TheBloke I have used your model \"TheBloke/wizardLM-7B-HF\" and\
          \ fine-tuned it with the train_freeform.py script from the wizardLM Gihub\
          \ repo on a custom dataset. The training works fine but the pytorch_model.bin\
          \ is only a few Kilobytes so something is off with saving model.? I am using\
          \ \"safe_save_model_for_hf_trainer(..)\" in their script for saving.  Did\
          \ you experience something similar? Any suggestions of what I could try\n\
          \nhi\uFF0Cgabriead, i meet the same problems here, the pytorch_model.bin\
          \ is only a few MB that can't be load correctly, it always show the 'weight'\
          \ must be 2-D error, i don't know how to do with it? Did you solve it in\
          \ the end? if you solved it, can you tell me how? "
        updatedAt: '2023-07-04T11:20:46.352Z'
      numEdits: 0
      reactions: []
    id: 64a4008e92f59567ba5c9d14
    type: comment
  author: waytohou
  content: "> Hi @TheBloke I have used your model \"TheBloke/wizardLM-7B-HF\" and\
    \ fine-tuned it with the train_freeform.py script from the wizardLM Gihub repo\
    \ on a custom dataset. The training works fine but the pytorch_model.bin is only\
    \ a few Kilobytes so something is off with saving model.? I am using \"safe_save_model_for_hf_trainer(..)\"\
    \ in their script for saving.  Did you experience something similar? Any suggestions\
    \ of what I could try\n\nhi\uFF0Cgabriead, i meet the same problems here, the\
    \ pytorch_model.bin is only a few MB that can't be load correctly, it always show\
    \ the 'weight' must be 2-D error, i don't know how to do with it? Did you solve\
    \ it in the end? if you solved it, can you tell me how? "
  created_at: 2023-07-04 10:20:46+00:00
  edited: false
  hidden: false
  id: 64a4008e92f59567ba5c9d14
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/wizardLM-7B-HF
repo_type: model
status: open
target_branch: null
title: 'How to save fine-tuned model and load it again? '
