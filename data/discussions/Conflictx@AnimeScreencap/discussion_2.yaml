!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hosioka
conflicting_files: null
created_at: 2022-12-17 19:58:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6354ffb5805be5a8f30b7585/FtK0VbEytjhBJKNsUDdgT.png?w=200&h=200&f=face
      fullname: Hosioka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hosioka
      type: user
    createdAt: '2022-12-17T19:58:54.000Z'
    data:
      edited: false
      editors:
      - Hosioka
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6354ffb5805be5a8f30b7585/FtK0VbEytjhBJKNsUDdgT.png?w=200&h=200&f=face
          fullname: Hosioka
          isHf: false
          isPro: false
          name: Hosioka
          type: user
        html: '<p>Error completing request<br>Arguments: (0, ''1girl, water drop,
          water, AnimeScreenCap, solo, ribbon, frills, skirt, shirt, bangs, glass,
          dress, bow, gradient, shards, buttons, vest, long hair, black hair, looking
          at viewer, short sleeves, red ribbon, parted lips, neck ribbon, puffy sleeves,
          white shirt, grey eyes, white skirt, hair intakes, outstretched arms, puffy
          short sleeves, center frills, white dress, grey background, upper body,
          simple background, gradient background, black eyes, collared shirt, black
          skirt, blue eyes, closed mouth, cowboy shot, purple eyes, frilled shirt,
          pleated skirt, black background, black vest, frilled skirt, purple hair,'',
          ''painting by bad-artist, painting by bad-artist-anime, '', ''None'', ''None'',
          &lt;PIL.Image.Image image mode=RGB size=1024x1536 at 0x1BF84ACA770&gt;,
          {''image'': &lt;PIL.Image.Image image mode=RGBA size=1024x1536 at 0x1BF84ACB970&gt;,
          ''mask'': &lt;PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1024x1536
          at 0x1BF84AC9B10&gt;}, None, None, None, 0, 50, 15, 0, 0, 1, False, False,
          1, 1, 7, 0.6, -1.0, -1.0, 0, 0, 0, False, 1536, 1024, 3, False, 32, 0, '''',
          '''', 0, ''</p><ul>\n<li><code>CFG Scale</code> should be 2 or lower.</li>\n</ul>\n'',
          True, True, '''', '''', True, 50, True, 1, 0, False, 4, 1, ''<p style="margin-bottom:0.75em">Recommended
          settings: Sampling Steps: 80-100, Sampler: Euler a, Denoising strength:
          0.8</p>'', 128, 8, [''left'', ''right'', ''up'', ''down''], 1, 0.05, 128,
          4, 0, [''left'', ''right'', ''up'', ''down''], False, False, False, False,
          '''', ''<p style="margin-bottom:0.75em">Will upscale the image by the selected
          scale factor; use width and height sliders to set tile size</p>'', 64, 0,
          2, 1, '''', 0, '''', True, False, False) {}<br>Traceback (most recent call
          last):<br>  File "D:\A111\stable-diffusion-webui\modules\call_queue.py",
          line 45, in f<br>    res = list(func(*args, **kwargs))<br>  File "D:\A111\stable-diffusion-webui\modules\call_queue.py",
          line 28, in f<br>    res = func(*args, **kwargs)<br>  File "D:\A111\stable-diffusion-webui\modules\img2img.py",
          line 152, in img2img<br>    processed = process_images(p)<br>  File "D:\A111\stable-diffusion-webui\modules\processing.py",
          line 464, in process_images<br>    res = process_images_inner(p)<br>  File
          "D:\A111\stable-diffusion-webui\modules\processing.py", line 557, in process_images_inner<br>    c
          = prompt_parser.get_multicond_learned_conditioning(shared.sd_model, prompts,
          p.steps)<br>  File "D:\A111\stable-diffusion-webui\modules\prompt_parser.py",
          line 203, in get_multicond_learned_conditioning<br>    learned_conditioning
          = get_learned_conditioning(model, prompt_flat_list, steps)<br>  File "D:\A111\stable-diffusion-webui\modules\prompt_parser.py",
          line 138, in get_learned_conditioning<br>    conds = model.get_learned_conditioning(texts)<br>  File
          "D:\A111\stable-diffusion-webui\scripts\v2.py", line 36, in get_learned_conditioning_with_prior<br>    cond
          = ldm.models.diffusion.ddpm.LatentDiffusion.get_learned_conditioning_original(self,
          c)<br>  File "D:\A111\stable-diffusion-webui\repositories\stable-diffusion-stability-ai\ldm\models\diffusion\ddpm.py",
          line 669, in get_learned_conditioning<br>    c = self.cond_stage_model(c)<br>  File
          "D:\A111\stable-diffusion-webui\venv\lib\site-packages\torch\nn\modules\module.py",
          line 1130, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "D:\A111\stable-diffusion-webui\modules\sd_hijack_clip.py", line 219, in
          forward<br>    z1 = self.process_tokens(tokens, multipliers)<br>  File "D:\A111\stable-diffusion-webui\modules\sd_hijack_clip.py",
          line 240, in process_tokens<br>    z = self.encode_with_transformers(tokens)<br>  File
          "D:\A111\stable-diffusion-webui\modules\sd_hijack_clip.py", line 286, in
          encode_with_transformers<br>    outputs = self.wrapped.transformer(input_ids=tokens,
          output_hidden_states=-opts.CLIP_stop_at_last_layers)<br>  File "D:\A111\stable-diffusion-webui\venv\lib\site-packages\torch\nn\modules\module.py",
          line 1148, in _call_impl<br>    result = forward_call(*input, **kwargs)<br>  File
          "D:\A111\stable-diffusion-webui\venv\lib\site-packages\transformers\models\clip\modeling_clip.py",
          line 811, in forward<br>    return self.text_model(<br>  File "D:\A111\stable-diffusion-webui\venv\lib\site-packages\torch\nn\modules\module.py",
          line 1130, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "D:\A111\stable-diffusion-webui\venv\lib\site-packages\transformers\models\clip\modeling_clip.py",
          line 708, in forward<br>    hidden_states = self.embeddings(input_ids=input_ids,
          position_ids=position_ids)<br>  File "D:\A111\stable-diffusion-webui\venv\lib\site-packages\torch\nn\modules\module.py",
          line 1130, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "D:\A111\stable-diffusion-webui\venv\lib\site-packages\transformers\models\clip\modeling_clip.py",
          line 223, in forward<br>    inputs_embeds = self.token_embedding(input_ids)<br>  File
          "D:\A111\stable-diffusion-webui\venv\lib\site-packages\torch\nn\modules\module.py",
          line 1130, in _call_impl<br>    return forward_call(*input, **kwargs)<br>  File
          "D:\A111\stable-diffusion-webui\modules\sd_hijack.py", line 156, in forward<br>    tensor
          = torch.cat([tensor[0:offset + 1], emb[0:emb_len], tensor[offset + 1 + emb_len:]])<br>RuntimeError:
          Sizes of tensors must match except in dimension 0. Expected size 768 but
          got size 1024 for tensor number 1 in the list.<p></p>

          '
        raw: "Error completing request\r\nArguments: (0, '1girl, water drop, water,\
          \ AnimeScreenCap, solo, ribbon, frills, skirt, shirt, bangs, glass, dress,\
          \ bow, gradient, shards, buttons, vest, long hair, black hair, looking at\
          \ viewer, short sleeves, red ribbon, parted lips, neck ribbon, puffy sleeves,\
          \ white shirt, grey eyes, white skirt, hair intakes, outstretched arms,\
          \ puffy short sleeves, center frills, white dress, grey background, upper\
          \ body, simple background, gradient background, black eyes, collared shirt,\
          \ black skirt, blue eyes, closed mouth, cowboy shot, purple eyes, frilled\
          \ shirt, pleated skirt, black background, black vest, frilled skirt, purple\
          \ hair,', 'painting by bad-artist, painting by bad-artist-anime, ', 'None',\
          \ 'None', <PIL.Image.Image image mode=RGB size=1024x1536 at 0x1BF84ACA770>,\
          \ {'image': <PIL.Image.Image image mode=RGBA size=1024x1536 at 0x1BF84ACB970>,\
          \ 'mask': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1024x1536\
          \ at 0x1BF84AC9B10>}, None, None, None, 0, 50, 15, 0, 0, 1, False, False,\
          \ 1, 1, 7, 0.6, -1.0, -1.0, 0, 0, 0, False, 1536, 1024, 3, False, 32, 0,\
          \ '', '', 0, '<ul>\\n<li><code>CFG Scale</code> should be 2 or lower.</li>\\\
          n</ul>\\n', True, True, '', '', True, 50, True, 1, 0, False, 4, 1, '<p style=\"\
          margin-bottom:0.75em\">Recommended settings: Sampling Steps: 80-100, Sampler:\
          \ Euler a, Denoising strength: 0.8</p>', 128, 8, ['left', 'right', 'up',\
          \ 'down'], 1, 0.05, 128, 4, 0, ['left', 'right', 'up', 'down'], False, False,\
          \ False, False, '', '<p style=\"margin-bottom:0.75em\">Will upscale the\
          \ image by the selected scale factor; use width and height sliders to set\
          \ tile size</p>', 64, 0, 2, 1, '', 0, '', True, False, False) {}\r\nTraceback\
          \ (most recent call last):\r\n  File \"D:\\A111\\stable-diffusion-webui\\\
          modules\\call_queue.py\", line 45, in f\r\n    res = list(func(*args, **kwargs))\r\
          \n  File \"D:\\A111\\stable-diffusion-webui\\modules\\call_queue.py\", line\
          \ 28, in f\r\n    res = func(*args, **kwargs)\r\n  File \"D:\\A111\\stable-diffusion-webui\\\
          modules\\img2img.py\", line 152, in img2img\r\n    processed = process_images(p)\r\
          \n  File \"D:\\A111\\stable-diffusion-webui\\modules\\processing.py\", line\
          \ 464, in process_images\r\n    res = process_images_inner(p)\r\n  File\
          \ \"D:\\A111\\stable-diffusion-webui\\modules\\processing.py\", line 557,\
          \ in process_images_inner\r\n    c = prompt_parser.get_multicond_learned_conditioning(shared.sd_model,\
          \ prompts, p.steps)\r\n  File \"D:\\A111\\stable-diffusion-webui\\modules\\\
          prompt_parser.py\", line 203, in get_multicond_learned_conditioning\r\n\
          \    learned_conditioning = get_learned_conditioning(model, prompt_flat_list,\
          \ steps)\r\n  File \"D:\\A111\\stable-diffusion-webui\\modules\\prompt_parser.py\"\
          , line 138, in get_learned_conditioning\r\n    conds = model.get_learned_conditioning(texts)\r\
          \n  File \"D:\\A111\\stable-diffusion-webui\\scripts\\v2.py\", line 36,\
          \ in get_learned_conditioning_with_prior\r\n    cond = ldm.models.diffusion.ddpm.LatentDiffusion.get_learned_conditioning_original(self,\
          \ c)\r\n  File \"D:\\A111\\stable-diffusion-webui\\repositories\\stable-diffusion-stability-ai\\\
          ldm\\models\\diffusion\\ddpm.py\", line 669, in get_learned_conditioning\r\
          \n    c = self.cond_stage_model(c)\r\n  File \"D:\\A111\\stable-diffusion-webui\\\
          venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in\
          \ _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"D:\\\
          A111\\stable-diffusion-webui\\modules\\sd_hijack_clip.py\", line 219, in\
          \ forward\r\n    z1 = self.process_tokens(tokens, multipliers)\r\n  File\
          \ \"D:\\A111\\stable-diffusion-webui\\modules\\sd_hijack_clip.py\", line\
          \ 240, in process_tokens\r\n    z = self.encode_with_transformers(tokens)\r\
          \n  File \"D:\\A111\\stable-diffusion-webui\\modules\\sd_hijack_clip.py\"\
          , line 286, in encode_with_transformers\r\n    outputs = self.wrapped.transformer(input_ids=tokens,\
          \ output_hidden_states=-opts.CLIP_stop_at_last_layers)\r\n  File \"D:\\\
          A111\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1148, in _call_impl\r\n    result = forward_call(*input,\
          \ **kwargs)\r\n  File \"D:\\A111\\stable-diffusion-webui\\venv\\lib\\site-packages\\\
          transformers\\models\\clip\\modeling_clip.py\", line 811, in forward\r\n\
          \    return self.text_model(\r\n  File \"D:\\A111\\stable-diffusion-webui\\\
          venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in\
          \ _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"D:\\\
          A111\\stable-diffusion-webui\\venv\\lib\\site-packages\\transformers\\models\\\
          clip\\modeling_clip.py\", line 708, in forward\r\n    hidden_states = self.embeddings(input_ids=input_ids,\
          \ position_ids=position_ids)\r\n  File \"D:\\A111\\stable-diffusion-webui\\\
          venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in\
          \ _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"D:\\\
          A111\\stable-diffusion-webui\\venv\\lib\\site-packages\\transformers\\models\\\
          clip\\modeling_clip.py\", line 223, in forward\r\n    inputs_embeds = self.token_embedding(input_ids)\r\
          \n  File \"D:\\A111\\stable-diffusion-webui\\venv\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return\
          \ forward_call(*input, **kwargs)\r\n  File \"D:\\A111\\stable-diffusion-webui\\\
          modules\\sd_hijack.py\", line 156, in forward\r\n    tensor = torch.cat([tensor[0:offset\
          \ + 1], emb[0:emb_len], tensor[offset + 1 + emb_len:]])\r\nRuntimeError:\
          \ Sizes of tensors must match except in dimension 0. Expected size 768 but\
          \ got size 1024 for tensor number 1 in the list."
        updatedAt: '2022-12-17T19:58:54.865Z'
      numEdits: 0
      reactions: []
    id: 639e1f7e72706670111f49e7
    type: comment
  author: Hosioka
  content: "Error completing request\r\nArguments: (0, '1girl, water drop, water,\
    \ AnimeScreenCap, solo, ribbon, frills, skirt, shirt, bangs, glass, dress, bow,\
    \ gradient, shards, buttons, vest, long hair, black hair, looking at viewer, short\
    \ sleeves, red ribbon, parted lips, neck ribbon, puffy sleeves, white shirt, grey\
    \ eyes, white skirt, hair intakes, outstretched arms, puffy short sleeves, center\
    \ frills, white dress, grey background, upper body, simple background, gradient\
    \ background, black eyes, collared shirt, black skirt, blue eyes, closed mouth,\
    \ cowboy shot, purple eyes, frilled shirt, pleated skirt, black background, black\
    \ vest, frilled skirt, purple hair,', 'painting by bad-artist, painting by bad-artist-anime,\
    \ ', 'None', 'None', <PIL.Image.Image image mode=RGB size=1024x1536 at 0x1BF84ACA770>,\
    \ {'image': <PIL.Image.Image image mode=RGBA size=1024x1536 at 0x1BF84ACB970>,\
    \ 'mask': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1024x1536 at 0x1BF84AC9B10>},\
    \ None, None, None, 0, 50, 15, 0, 0, 1, False, False, 1, 1, 7, 0.6, -1.0, -1.0,\
    \ 0, 0, 0, False, 1536, 1024, 3, False, 32, 0, '', '', 0, '<ul>\\n<li><code>CFG\
    \ Scale</code> should be 2 or lower.</li>\\n</ul>\\n', True, True, '', '', True,\
    \ 50, True, 1, 0, False, 4, 1, '<p style=\"margin-bottom:0.75em\">Recommended\
    \ settings: Sampling Steps: 80-100, Sampler: Euler a, Denoising strength: 0.8</p>',\
    \ 128, 8, ['left', 'right', 'up', 'down'], 1, 0.05, 128, 4, 0, ['left', 'right',\
    \ 'up', 'down'], False, False, False, False, '', '<p style=\"margin-bottom:0.75em\"\
    >Will upscale the image by the selected scale factor; use width and height sliders\
    \ to set tile size</p>', 64, 0, 2, 1, '', 0, '', True, False, False) {}\r\nTraceback\
    \ (most recent call last):\r\n  File \"D:\\A111\\stable-diffusion-webui\\modules\\\
    call_queue.py\", line 45, in f\r\n    res = list(func(*args, **kwargs))\r\n  File\
    \ \"D:\\A111\\stable-diffusion-webui\\modules\\call_queue.py\", line 28, in f\r\
    \n    res = func(*args, **kwargs)\r\n  File \"D:\\A111\\stable-diffusion-webui\\\
    modules\\img2img.py\", line 152, in img2img\r\n    processed = process_images(p)\r\
    \n  File \"D:\\A111\\stable-diffusion-webui\\modules\\processing.py\", line 464,\
    \ in process_images\r\n    res = process_images_inner(p)\r\n  File \"D:\\A111\\\
    stable-diffusion-webui\\modules\\processing.py\", line 557, in process_images_inner\r\
    \n    c = prompt_parser.get_multicond_learned_conditioning(shared.sd_model, prompts,\
    \ p.steps)\r\n  File \"D:\\A111\\stable-diffusion-webui\\modules\\prompt_parser.py\"\
    , line 203, in get_multicond_learned_conditioning\r\n    learned_conditioning\
    \ = get_learned_conditioning(model, prompt_flat_list, steps)\r\n  File \"D:\\\
    A111\\stable-diffusion-webui\\modules\\prompt_parser.py\", line 138, in get_learned_conditioning\r\
    \n    conds = model.get_learned_conditioning(texts)\r\n  File \"D:\\A111\\stable-diffusion-webui\\\
    scripts\\v2.py\", line 36, in get_learned_conditioning_with_prior\r\n    cond\
    \ = ldm.models.diffusion.ddpm.LatentDiffusion.get_learned_conditioning_original(self,\
    \ c)\r\n  File \"D:\\A111\\stable-diffusion-webui\\repositories\\stable-diffusion-stability-ai\\\
    ldm\\models\\diffusion\\ddpm.py\", line 669, in get_learned_conditioning\r\n \
    \   c = self.cond_stage_model(c)\r\n  File \"D:\\A111\\stable-diffusion-webui\\\
    venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\
    \n    return forward_call(*input, **kwargs)\r\n  File \"D:\\A111\\stable-diffusion-webui\\\
    modules\\sd_hijack_clip.py\", line 219, in forward\r\n    z1 = self.process_tokens(tokens,\
    \ multipliers)\r\n  File \"D:\\A111\\stable-diffusion-webui\\modules\\sd_hijack_clip.py\"\
    , line 240, in process_tokens\r\n    z = self.encode_with_transformers(tokens)\r\
    \n  File \"D:\\A111\\stable-diffusion-webui\\modules\\sd_hijack_clip.py\", line\
    \ 286, in encode_with_transformers\r\n    outputs = self.wrapped.transformer(input_ids=tokens,\
    \ output_hidden_states=-opts.CLIP_stop_at_last_layers)\r\n  File \"D:\\A111\\\
    stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
    , line 1148, in _call_impl\r\n    result = forward_call(*input, **kwargs)\r\n\
    \  File \"D:\\A111\\stable-diffusion-webui\\venv\\lib\\site-packages\\transformers\\\
    models\\clip\\modeling_clip.py\", line 811, in forward\r\n    return self.text_model(\r\
    \n  File \"D:\\A111\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1130, in _call_impl\r\n    return forward_call(*input,\
    \ **kwargs)\r\n  File \"D:\\A111\\stable-diffusion-webui\\venv\\lib\\site-packages\\\
    transformers\\models\\clip\\modeling_clip.py\", line 708, in forward\r\n    hidden_states\
    \ = self.embeddings(input_ids=input_ids, position_ids=position_ids)\r\n  File\
    \ \"D:\\A111\\stable-diffusion-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1130, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\
    \n  File \"D:\\A111\\stable-diffusion-webui\\venv\\lib\\site-packages\\transformers\\\
    models\\clip\\modeling_clip.py\", line 223, in forward\r\n    inputs_embeds =\
    \ self.token_embedding(input_ids)\r\n  File \"D:\\A111\\stable-diffusion-webui\\\
    venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\r\
    \n    return forward_call(*input, **kwargs)\r\n  File \"D:\\A111\\stable-diffusion-webui\\\
    modules\\sd_hijack.py\", line 156, in forward\r\n    tensor = torch.cat([tensor[0:offset\
    \ + 1], emb[0:emb_len], tensor[offset + 1 + emb_len:]])\r\nRuntimeError: Sizes\
    \ of tensors must match except in dimension 0. Expected size 768 but got size\
    \ 1024 for tensor number 1 in the list."
  created_at: 2022-12-17 19:58:54+00:00
  edited: false
  hidden: false
  id: 639e1f7e72706670111f49e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668343610142-6303c53d7373aacccd859bbd.jpeg?w=200&h=200&f=face
      fullname: Dimitri Bracke
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Conflictx
      type: user
    createdAt: '2022-12-19T05:56:40.000Z'
    data:
      edited: false
      editors:
      - Conflictx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668343610142-6303c53d7373aacccd859bbd.jpeg?w=200&h=200&f=face
          fullname: Dimitri Bracke
          isHf: false
          isPro: false
          name: Conflictx
          type: user
        html: '<p>Make sure to check you have the 768-v2.1-ema.ckpt, or any other
          SD2.0 model active when using the embedding. They won''t work with 1.5 or
          1.4</p>

          <blockquote>

          <p>RuntimeError: Sizes of tensors must match except in dimension 0. Expected
          size 768 but got size 1024 for tensor number 1 in the list.</p>

          </blockquote>

          <p>This message is an indication that an wrong model is being used for the
          embedding.</p>

          '
        raw: "Make sure to check you have the 768-v2.1-ema.ckpt, or any other SD2.0\
          \ model active when using the embedding. They won't work with 1.5 or 1.4\n\
          \n> RuntimeError: Sizes of tensors must match except in dimension 0. Expected\
          \ size 768 but got size 1024 for tensor number 1 in the list.\n \nThis message\
          \ is an indication that an wrong model is being used for the embedding."
        updatedAt: '2022-12-19T05:56:40.803Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Hosioka
    id: 639ffd182e13e54dcbc9a9d9
    type: comment
  author: Conflictx
  content: "Make sure to check you have the 768-v2.1-ema.ckpt, or any other SD2.0\
    \ model active when using the embedding. They won't work with 1.5 or 1.4\n\n>\
    \ RuntimeError: Sizes of tensors must match except in dimension 0. Expected size\
    \ 768 but got size 1024 for tensor number 1 in the list.\n \nThis message is an\
    \ indication that an wrong model is being used for the embedding."
  created_at: 2022-12-19 05:56:40+00:00
  edited: false
  hidden: false
  id: 639ffd182e13e54dcbc9a9d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6354ffb5805be5a8f30b7585/FtK0VbEytjhBJKNsUDdgT.png?w=200&h=200&f=face
      fullname: Hosioka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hosioka
      type: user
    createdAt: '2022-12-20T02:56:46.000Z'
    data:
      edited: false
      editors:
      - Hosioka
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6354ffb5805be5a8f30b7585/FtK0VbEytjhBJKNsUDdgT.png?w=200&h=200&f=face
          fullname: Hosioka
          isHf: false
          isPro: false
          name: Hosioka
          type: user
        html: '<p>im using this with AnythingV3 and My own model it just won''t work
          with anime models?</p>

          '
        raw: im using this with AnythingV3 and My own model it just won't work with
          anime models?
        updatedAt: '2022-12-20T02:56:46.554Z'
      numEdits: 0
      reactions: []
    id: 63a1246e45edac9f75092167
    type: comment
  author: Hosioka
  content: im using this with AnythingV3 and My own model it just won't work with
    anime models?
  created_at: 2022-12-20 02:56:46+00:00
  edited: false
  hidden: false
  id: 63a1246e45edac9f75092167
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/086da3482b4fae07862cce34504ec028.svg
      fullname: Joshua Kulusic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kulusic
      type: user
    createdAt: '2022-12-20T09:00:13.000Z'
    data:
      edited: false
      editors:
      - Kulusic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/086da3482b4fae07862cce34504ec028.svg
          fullname: Joshua Kulusic
          isHf: false
          isPro: false
          name: Kulusic
          type: user
        html: '<p>Embeddings work on the model they are associated with.</p>

          <p>"Textual Inversion Embedding by ConflictX For SD 2.x trained on 768x768
          images from anime sources."</p>

          '
        raw: 'Embeddings work on the model they are associated with.


          "Textual Inversion Embedding by ConflictX For SD 2.x trained on 768x768
          images from anime sources."'
        updatedAt: '2022-12-20T09:00:13.491Z'
      numEdits: 0
      reactions: []
    id: 63a1799d5d09b819feeb01ab
    type: comment
  author: Kulusic
  content: 'Embeddings work on the model they are associated with.


    "Textual Inversion Embedding by ConflictX For SD 2.x trained on 768x768 images
    from anime sources."'
  created_at: 2022-12-20 09:00:13+00:00
  edited: false
  hidden: false
  id: 63a1799d5d09b819feeb01ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6354ffb5805be5a8f30b7585/FtK0VbEytjhBJKNsUDdgT.png?w=200&h=200&f=face
      fullname: Hosioka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hosioka
      type: user
    createdAt: '2022-12-20T10:21:53.000Z'
    data:
      edited: false
      editors:
      - Hosioka
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6354ffb5805be5a8f30b7585/FtK0VbEytjhBJKNsUDdgT.png?w=200&h=200&f=face
          fullname: Hosioka
          isHf: false
          isPro: false
          name: Hosioka
          type: user
        html: '<p>I see. Thanks for the answers</p>

          '
        raw: I see. Thanks for the answers
        updatedAt: '2022-12-20T10:21:53.466Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63a18cc1a0e2f15fea663b73
    id: 63a18cc1a0e2f15fea663b72
    type: comment
  author: Hosioka
  content: I see. Thanks for the answers
  created_at: 2022-12-20 10:21:53+00:00
  edited: false
  hidden: false
  id: 63a18cc1a0e2f15fea663b72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6354ffb5805be5a8f30b7585/FtK0VbEytjhBJKNsUDdgT.png?w=200&h=200&f=face
      fullname: Hosioka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hosioka
      type: user
    createdAt: '2022-12-20T10:21:53.000Z'
    data:
      status: closed
    id: 63a18cc1a0e2f15fea663b73
    type: status-change
  author: Hosioka
  created_at: 2022-12-20 10:21:53+00:00
  id: 63a18cc1a0e2f15fea663b73
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Conflictx/AnimeScreencap
repo_type: model
status: closed
target_branch: null
title: Crashes everytime I try to run in my prompt
