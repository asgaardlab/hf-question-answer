!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mazib
conflicting_files: null
created_at: 2022-09-13 23:17:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/db8da1fcceb3c33db794a50deb3752e4.svg
      fullname: Mazi Boustani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mazib
      type: user
    createdAt: '2022-09-14T00:17:28.000Z'
    data:
      edited: false
      editors:
      - mazib
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/db8da1fcceb3c33db794a50deb3752e4.svg
          fullname: Mazi Boustani
          isHf: false
          isPro: false
          name: mazib
          type: user
        html: '<p>Hello HuggingFace community,</p>

          <p>I am trying to test the Bloom model on an AWS with ''NVIDIA A10G '' GPU
          which has 22GB memory.<br>I did run this code:</p>

          <p><code>from transformers import AutoTokenizer, AutoModel</code><br><code>tokenizer
          = AutoTokenizer.from_pretrained("bigscience/bloom")</code><br><code>model
          = AutoModel.from_pretrained("bigscience/bloom")</code></p>

          <p>It automatically downloaded the Bloom model (72 files). But after that,
          I do get a<code>CUDA out of memory</code>.</p>

          <p>Can someone tell me how much of GPU memory is needed to run the Bloom
          model?</p>

          <p>Thanks </p>

          '
        raw: "Hello HuggingFace community,\r\n\r\nI am trying to test the Bloom model\
          \ on an AWS with 'NVIDIA A10G ' GPU which has 22GB memory. \r\nI did run\
          \ this code:\r\n\r\n`from transformers import AutoTokenizer, AutoModel`\r\
          \n`tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\")`\r\n\
          `model = AutoModel.from_pretrained(\"bigscience/bloom\")`\r\n\r\nIt automatically\
          \ downloaded the Bloom model (72 files). But after that, I do get a`CUDA\
          \ out of memory`.\r\n\r\nCan someone tell me how much of GPU memory is needed\
          \ to run the Bloom model?\r\n\r\nThanks \r\n"
        updatedAt: '2022-09-14T00:17:28.018Z'
      numEdits: 0
      reactions: []
    id: 63211d98b754c42716eff459
    type: comment
  author: mazib
  content: "Hello HuggingFace community,\r\n\r\nI am trying to test the Bloom model\
    \ on an AWS with 'NVIDIA A10G ' GPU which has 22GB memory. \r\nI did run this\
    \ code:\r\n\r\n`from transformers import AutoTokenizer, AutoModel`\r\n`tokenizer\
    \ = AutoTokenizer.from_pretrained(\"bigscience/bloom\")`\r\n`model = AutoModel.from_pretrained(\"\
    bigscience/bloom\")`\r\n\r\nIt automatically downloaded the Bloom model (72 files).\
    \ But after that, I do get a`CUDA out of memory`.\r\n\r\nCan someone tell me how\
    \ much of GPU memory is needed to run the Bloom model?\r\n\r\nThanks \r\n"
  created_at: 2022-09-13 23:17:28+00:00
  edited: false
  hidden: false
  id: 63211d98b754c42716eff459
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-09-14T11:32:26.000Z'
    data:
      edited: false
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: '<p>The 72 checkpoints are 329GB in total, so far inference it might
          take about 350GB.</p>

          '
        raw: The 72 checkpoints are 329GB in total, so far inference it might take
          about 350GB.
        updatedAt: '2022-09-14T11:32:26.875Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mazib
    id: 6321bbca909ac44b57247cec
    type: comment
  author: pai4451
  content: The 72 checkpoints are 329GB in total, so far inference it might take about
    350GB.
  created_at: 2022-09-14 10:32:26+00:00
  edited: false
  hidden: false
  id: 6321bbca909ac44b57247cec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640134437444-609baae0fe087f3d04cf0481.jpeg?w=200&h=200&f=face
      fullname: Yozh
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: justheuristic
      type: user
    createdAt: '2022-09-16T23:01:52.000Z'
    data:
      edited: false
      editors:
      - justheuristic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640134437444-609baae0fe087f3d04cf0481.jpeg?w=200&h=200&f=face
          fullname: Yozh
          isHf: false
          isPro: false
          name: justheuristic
          type: user
        html: '<p>Works in ~200GB if you use load_in_8bit feature from <a rel="nofollow"
          href="https://github.com/huggingface/transformers/pull/17901">https://github.com/huggingface/transformers/pull/17901</a></p>

          '
        raw: Works in ~200GB if you use load_in_8bit feature from https://github.com/huggingface/transformers/pull/17901
        updatedAt: '2022-09-16T23:01:52.589Z'
      numEdits: 0
      reactions: []
    id: 632500604923042d9db06ecf
    type: comment
  author: justheuristic
  content: Works in ~200GB if you use load_in_8bit feature from https://github.com/huggingface/transformers/pull/17901
  created_at: 2022-09-16 22:01:52+00:00
  edited: false
  hidden: false
  id: 632500604923042d9db06ecf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2022-09-27T14:53:44.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mazib&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mazib\">@<span class=\"\
          underline\">mazib</span></a></span>\n\n\t</span></span> you will need at\
          \ least 8 x A100 80 GB GPUs for inference in fp16.<br>Or you can use int8\
          \ for inference.</p>\n"
        raw: '@mazib you will need at least 8 x A100 80 GB GPUs for inference in fp16.

          Or you can use int8 for inference.'
        updatedAt: '2022-09-27T14:53:44.915Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MrPro
    id: 63330e783b90c15b922450b1
    type: comment
  author: mayank31398
  content: '@mazib you will need at least 8 x A100 80 GB GPUs for inference in fp16.

    Or you can use int8 for inference.'
  created_at: 2022-09-27 13:53:44+00:00
  edited: false
  hidden: false
  id: 63330e783b90c15b922450b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664449133824-6305f2cf435ec751b725663e.png?w=200&h=200&f=face
      fullname: Pavel Suchmann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bver
      type: user
    createdAt: '2022-09-28T18:36:04.000Z'
    data:
      edited: false
      editors:
      - bver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664449133824-6305f2cf435ec751b725663e.png?w=200&h=200&f=face
          fullname: Pavel Suchmann
          isHf: false
          isPro: false
          name: bver
          type: user
        html: '<p>Thanks for this discussion thread.<br>I have some (hopefully related)
          observations here: <a href="https://huggingface.co/bigscience/bloom/discussions/118">https://huggingface.co/bigscience/bloom/discussions/118</a></p>

          '
        raw: "Thanks for this discussion thread. \nI have some (hopefully related)\
          \ observations here: https://huggingface.co/bigscience/bloom/discussions/118"
        updatedAt: '2022-09-28T18:36:04.606Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - ybelkada
        - Bourhano
    id: 633494145feddf1fec4d9bf6
    type: comment
  author: bver
  content: "Thanks for this discussion thread. \nI have some (hopefully related) observations\
    \ here: https://huggingface.co/bigscience/bloom/discussions/118"
  created_at: 2022-09-28 17:36:04+00:00
  edited: false
  hidden: false
  id: 633494145feddf1fec4d9bf6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 109
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: How much GPU memory needed?
