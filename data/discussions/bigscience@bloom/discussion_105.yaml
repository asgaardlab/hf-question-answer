!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nora1008
conflicting_files: null
created_at: 2022-09-04 07:00:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf84591406daec6e7f214784ff470c8c.svg
      fullname: Han Yen Ting
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nora1008
      type: user
    createdAt: '2022-09-04T08:00:11.000Z'
    data:
      edited: false
      editors:
      - nora1008
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf84591406daec6e7f214784ff470c8c.svg
          fullname: Han Yen Ting
          isHf: false
          isPro: false
          name: nora1008
          type: user
        html: '<p>Hi everyone I''m new to NLP I want to know how to use BLOOM(traditional
          Chinese) to fine tuning w/ my own data(.csv) (such as QA)</p>

          <p>my data is collected by myself (e.g. prompt and completion --GPT3 format)</p>

          '
        raw: "Hi everyone I'm new to NLP I want to know how to use BLOOM(traditional\
          \ Chinese) to fine tuning w/ my own data(.csv) (such as QA)\r\n\r\nmy data\
          \ is collected by myself (e.g. prompt and completion --GPT3 format)"
        updatedAt: '2022-09-04T08:00:11.971Z'
      numEdits: 0
      reactions: []
    id: 63145b0b27d020853f610eee
    type: comment
  author: nora1008
  content: "Hi everyone I'm new to NLP I want to know how to use BLOOM(traditional\
    \ Chinese) to fine tuning w/ my own data(.csv) (such as QA)\r\n\r\nmy data is\
    \ collected by myself (e.g. prompt and completion --GPT3 format)"
  created_at: 2022-09-04 07:00:11+00:00
  edited: false
  hidden: false
  id: 63145b0b27d020853f610eee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-09-08T19:47:53.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<p>Hey \U0001F917</p>\n<p>I see two options for fine-tuning:</p>\n\
          <ol>\n<li>Transformers Checkpoint (this repo): You'd probably want to make\
          \ use of the DeepSpeed integration for that, see <a href=\"https://huggingface.co/docs/transformers/main_classes/deepspeed\"\
          >https://huggingface.co/docs/transformers/main_classes/deepspeed</a></li>\n\
          <li>Megatron-Deepspeed Checkpoint (available here: <a href=\"https://huggingface.co/bigscience/bloom-optimizer-states\"\
          >https://huggingface.co/bigscience/bloom-optimizer-states</a>): You can\
          \ fine-tune with the same repository used for pre-training available here:\
          \ <a rel=\"nofollow\" href=\"https://github.com/bigscience-workshop/Megatron-DeepSpeed\"\
          >https://github.com/bigscience-workshop/Megatron-DeepSpeed</a></li>\n</ol>\n"
        raw: "Hey \U0001F917\n\nI see two options for fine-tuning:\n1) Transformers\
          \ Checkpoint (this repo): You'd probably want to make use of the DeepSpeed\
          \ integration for that, see https://huggingface.co/docs/transformers/main_classes/deepspeed\n\
          2) Megatron-Deepspeed Checkpoint (available here: https://huggingface.co/bigscience/bloom-optimizer-states):\
          \ You can fine-tune with the same repository used for pre-training available\
          \ here: https://github.com/bigscience-workshop/Megatron-DeepSpeed"
        updatedAt: '2022-09-08T19:47:53.242Z'
      numEdits: 0
      reactions: []
    id: 631a46e9300a072a8da6f92d
    type: comment
  author: Muennighoff
  content: "Hey \U0001F917\n\nI see two options for fine-tuning:\n1) Transformers\
    \ Checkpoint (this repo): You'd probably want to make use of the DeepSpeed integration\
    \ for that, see https://huggingface.co/docs/transformers/main_classes/deepspeed\n\
    2) Megatron-Deepspeed Checkpoint (available here: https://huggingface.co/bigscience/bloom-optimizer-states):\
    \ You can fine-tune with the same repository used for pre-training available here:\
    \ https://github.com/bigscience-workshop/Megatron-DeepSpeed"
  created_at: 2022-09-08 18:47:53+00:00
  edited: false
  hidden: false
  id: 631a46e9300a072a8da6f92d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-04T08:58:31.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Update: We have finetuned BLOOM to produce <a href="https://huggingface.co/bigscience/bloomz">BLOOMZ</a>
          &amp; our guide is <a rel="nofollow" href="https://github.com/bigscience-workshop/xmtf#bloomz">here</a>.</p>

          '
        raw: 'Update: We have finetuned BLOOM to produce [BLOOMZ](https://huggingface.co/bigscience/bloomz)
          & our guide is [here](https://github.com/bigscience-workshop/xmtf#bloomz).'
        updatedAt: '2022-11-04T08:58:31.085Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - victor
        - nora1008
        - wilfoderek
    id: 6364d43712188d67e652beb9
    type: comment
  author: Muennighoff
  content: 'Update: We have finetuned BLOOM to produce [BLOOMZ](https://huggingface.co/bigscience/bloomz)
    & our guide is [here](https://github.com/bigscience-workshop/xmtf#bloomz).'
  created_at: 2022-11-04 07:58:31+00:00
  edited: false
  hidden: false
  id: 6364d43712188d67e652beb9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6d2fe8117535665d7790e0b700b1fbd.svg
      fullname: Wilfredo Martel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wilfoderek
      type: user
    createdAt: '2022-12-20T18:58:52.000Z'
    data:
      edited: false
      editors:
      - wilfoderek
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6d2fe8117535665d7790e0b700b1fbd.svg
          fullname: Wilfredo Martel
          isHf: false
          isPro: false
          name: wilfoderek
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \ how much gpu ram is used to fine tuning Bloom 560m ?<br>Thank you in advance\
          \ my friend.</p>\n"
        raw: '@Muennighoff how much gpu ram is used to fine tuning Bloom 560m ?

          Thank you in advance my friend.'
        updatedAt: '2022-12-20T18:58:52.305Z'
      numEdits: 0
      reactions: []
    id: 63a205ece36f2e4d5b14886e
    type: comment
  author: wilfoderek
  content: '@Muennighoff how much gpu ram is used to fine tuning Bloom 560m ?

    Thank you in advance my friend.'
  created_at: 2022-12-20 18:58:52+00:00
  edited: false
  hidden: false
  id: 63a205ece36f2e4d5b14886e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-12-21T04:04:31.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<p>Depends if you're willing to fine-tune only a few parameters you\
          \ can maybe even do it in a Colab Notebook with 15GB or so; Here are some\
          \ sources that should help \U0001F917</p>\n<ul>\n<li><a href=\"https://huggingface.co/bigscience/bloom/discussions/152\"\
          >https://huggingface.co/bigscience/bloom/discussions/152</a></li>\n<li><a\
          \ href=\"https://huggingface.co/bigscience/bloom/discussions/46\">https://huggingface.co/bigscience/bloom/discussions/46</a></li>\n\
          </ul>\n"
        raw: "Depends if you're willing to fine-tune only a few parameters you can\
          \ maybe even do it in a Colab Notebook with 15GB or so; Here are some sources\
          \ that should help \U0001F917\n- https://huggingface.co/bigscience/bloom/discussions/152\n\
          - https://huggingface.co/bigscience/bloom/discussions/46"
        updatedAt: '2022-12-21T04:04:31.538Z'
      numEdits: 0
      reactions: []
    id: 63a285cf3c003e409326ef05
    type: comment
  author: Muennighoff
  content: "Depends if you're willing to fine-tune only a few parameters you can maybe\
    \ even do it in a Colab Notebook with 15GB or so; Here are some sources that should\
    \ help \U0001F917\n- https://huggingface.co/bigscience/bloom/discussions/152\n\
    - https://huggingface.co/bigscience/bloom/discussions/46"
  created_at: 2022-12-21 04:04:31+00:00
  edited: false
  hidden: false
  id: 63a285cf3c003e409326ef05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6d2fe8117535665d7790e0b700b1fbd.svg
      fullname: Wilfredo Martel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wilfoderek
      type: user
    createdAt: '2023-03-22T02:35:18.000Z'
    data:
      edited: false
      editors:
      - wilfoderek
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6d2fe8117535665d7790e0b700b1fbd.svg
          fullname: Wilfredo Martel
          isHf: false
          isPro: false
          name: wilfoderek
          type: user
        html: '<p>Do you think is possible to do the same modification you did in
          Bloom but in Alpaca 7b for semantic similarity?</p>

          '
        raw: Do you think is possible to do the same modification you did in Bloom
          but in Alpaca 7b for semantic similarity?
        updatedAt: '2023-03-22T02:35:18.268Z'
      numEdits: 0
      reactions: []
    id: 641a69664097fa34bd35e334
    type: comment
  author: wilfoderek
  content: Do you think is possible to do the same modification you did in Bloom but
    in Alpaca 7b for semantic similarity?
  created_at: 2023-03-22 01:35:18+00:00
  edited: false
  hidden: false
  id: 641a69664097fa34bd35e334
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcc766f42a2d082dc259f46c40253c9b.svg
      fullname: Abdul Basit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Niazi
      type: user
    createdAt: '2023-11-06T10:38:46.000Z'
    data:
      edited: false
      editors:
      - Niazi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9648475646972656
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcc766f42a2d082dc259f46c40253c9b.svg
          fullname: Abdul Basit
          isHf: false
          isPro: false
          name: Niazi
          type: user
        html: '<p>I''m currently working with a low-resource language that is a component
          of the ROOTS dataset, which Bloom is trained on. However, upon examining
          the vocabulary and attempting to tokenize it, I encountered a situation
          where there were no tokenizations available for the language.</p>

          <p>Is it feasible to inject this language''s vocabulary into Bloom''s tokenizer?</p>

          '
        raw: 'I''m currently working with a low-resource language that is a component
          of the ROOTS dataset, which Bloom is trained on. However, upon examining
          the vocabulary and attempting to tokenize it, I encountered a situation
          where there were no tokenizations available for the language.


          Is it feasible to inject this language''s vocabulary into Bloom''s tokenizer?'
        updatedAt: '2023-11-06T10:38:46.782Z'
      numEdits: 0
      reactions: []
    id: 6548c236dbce6bd2be438b2a
    type: comment
  author: Niazi
  content: 'I''m currently working with a low-resource language that is a component
    of the ROOTS dataset, which Bloom is trained on. However, upon examining the vocabulary
    and attempting to tokenize it, I encountered a situation where there were no tokenizations
    available for the language.


    Is it feasible to inject this language''s vocabulary into Bloom''s tokenizer?'
  created_at: 2023-11-06 10:38:46+00:00
  edited: false
  hidden: false
  id: 6548c236dbce6bd2be438b2a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 105
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: how to fine tuning
