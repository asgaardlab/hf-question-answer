!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Muennighoff
conflicting_files: null
created_at: 2022-07-16 07:52:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-07-16T08:52:53.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<ol>\n<li>Lingala is mentioned in the language codes (<code>ln</code>),\
          \ but not in the <code>Distribution of Niger Congo and Indic languages.</code>\
          \ part</li>\n<li>What is our definition for a training language? The training\
          \ data has 341B tokens, so at 0.00002%, there are ~68200 tokens of Chi Tumbuka\
          \ in there.  In English 1 token ~ 0.75 words (a)) and 1 sentence ~ 20 words\
          \ (b)). If it's ~the same in Chi Tumbuka, the model has seen 2500 sentences\
          \ of that language, as we did ~1 epoch. Do we have statistics for how much\
          \ German or Japanese accidentally landed in the training data? I'd guess\
          \ it's more than Chi Tumbuka, so where do we draw the line between a \"\
          training language\"? We don't have any evaluation datasets for Chi Tumbuka\
          \ afaik, so if we don't get on one of its 3M speakers we'll never know if\
          \ it knows it at all. \U0001F47B</li>\n</ol>\n<p>a) <a rel=\"nofollow\"\
          \ href=\"https://openai.com/api/pricing/#faq-token\">https://openai.com/api/pricing/#faq-token</a><br>b)\
          \ <a rel=\"nofollow\" href=\"https://www.quora.com/What-is-the-average-number-of-words-per-sentence-in-common-writings-such-as-news-articles-and-college-essays\"\
          >https://www.quora.com/What-is-the-average-number-of-words-per-sentence-in-common-writings-such-as-news-articles-and-college-essays</a></p>\n\
          <p>cc <span data-props=\"{&quot;user&quot;:&quot;lintang&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lintang\">@<span class=\"\
          underline\">lintang</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;cakiki&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/cakiki\">@<span class=\"underline\">cakiki</span></a></span>\n\
          \n\t</span></span> </p>\n"
        raw: "1) Lingala is mentioned in the language codes (`ln`), but not in the\
          \ `Distribution of Niger Congo and Indic languages.` part\r\n2) What is\
          \ our definition for a training language? The training data has 341B tokens,\
          \ so at 0.00002%, there are ~68200 tokens of Chi Tumbuka in there.  In English\
          \ 1 token ~ 0.75 words (a)) and 1 sentence ~ 20 words (b)). If it's ~the\
          \ same in Chi Tumbuka, the model has seen 2500 sentences of that language,\
          \ as we did ~1 epoch. Do we have statistics for how much German or Japanese\
          \ accidentally landed in the training data? I'd guess it's more than Chi\
          \ Tumbuka, so where do we draw the line between a \"training language\"\
          ? We don't have any evaluation datasets for Chi Tumbuka afaik, so if we\
          \ don't get on one of its 3M speakers we'll never know if it knows it at\
          \ all. \U0001F47B\r\n\r\na) https://openai.com/api/pricing/#faq-token\r\n\
          b) https://www.quora.com/What-is-the-average-number-of-words-per-sentence-in-common-writings-such-as-news-articles-and-college-essays\r\
          \n\r\ncc @lintang @cakiki \r\n\r\n\r\n"
        updatedAt: '2022-07-16T08:52:53.550Z'
      numEdits: 0
      reactions: []
    id: 62d27c655666d76902d7a2d4
    type: comment
  author: Muennighoff
  content: "1) Lingala is mentioned in the language codes (`ln`), but not in the `Distribution\
    \ of Niger Congo and Indic languages.` part\r\n2) What is our definition for a\
    \ training language? The training data has 341B tokens, so at 0.00002%, there\
    \ are ~68200 tokens of Chi Tumbuka in there.  In English 1 token ~ 0.75 words\
    \ (a)) and 1 sentence ~ 20 words (b)). If it's ~the same in Chi Tumbuka, the model\
    \ has seen 2500 sentences of that language, as we did ~1 epoch. Do we have statistics\
    \ for how much German or Japanese accidentally landed in the training data? I'd\
    \ guess it's more than Chi Tumbuka, so where do we draw the line between a \"\
    training language\"? We don't have any evaluation datasets for Chi Tumbuka afaik,\
    \ so if we don't get on one of its 3M speakers we'll never know if it knows it\
    \ at all. \U0001F47B\r\n\r\na) https://openai.com/api/pricing/#faq-token\r\nb)\
    \ https://www.quora.com/What-is-the-average-number-of-words-per-sentence-in-common-writings-such-as-news-articles-and-college-essays\r\
    \n\r\ncc @lintang @cakiki \r\n\r\n\r\n"
  created_at: 2022-07-16 07:52:53+00:00
  edited: false
  hidden: false
  id: 62d27c655666d76902d7a2d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2022-07-18T19:13:40.000Z'
    data:
      edited: false
      editors:
      - cakiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
          fullname: Christopher Akiki
          isHf: false
          isPro: false
          name: cakiki
          type: user
        html: "<p>Sorry for the late reply Niklas! I somehow was not notified I was\
          \ mentioned.</p>\n<ol>\n<li>We have 1,650,804 bytes of Lingala data; not\
          \ sure why it's not listed. Good catch! I will have a look. For reference:</li>\n\
          </ol>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1658170972850-5e70f6048ce3c604d78fe133.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1658170972850-5e70f6048ce3c604d78fe133.png\"\
          ></a></p>\n<ol start=\"2\">\n<li>Estimating how many languages accidentally\
          \ made their way into the corpus is a very interesting question (This is\
          \ probably mostly affects the catalogue). I think we have to explicitly\
          \ quantify this; I will bring this up in tomorrow's Viz and Analysis meeting.\
          \ Now the question about whether a certain threshold might qualify a language\
          \ into being considered a \"training language\" is both fascinating and\
          \ difficult; I don't have an answer right now.</li>\n</ol>\n<p>cc <span\
          \ data-props=\"{&quot;user&quot;:&quot;yjernite&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/yjernite\">@<span class=\"\
          underline\">yjernite</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;aymm&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/aymm\">@<span class=\"underline\">aymm</span></a></span>\n\
          \n\t</span></span></p>\n"
        raw: 'Sorry for the late reply Niklas! I somehow was not notified I was mentioned.


          1. We have 1,650,804 bytes of Lingala data; not sure why it''s not listed.
          Good catch! I will have a look. For reference:



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1658170972850-5e70f6048ce3c604d78fe133.png)



          2. Estimating how many languages accidentally made their way into the corpus
          is a very interesting question (This is probably mostly affects the catalogue).
          I think we have to explicitly quantify this; I will bring this up in tomorrow''s
          Viz and Analysis meeting. Now the question about whether a certain threshold
          might qualify a language into being considered a "training language" is
          both fascinating and difficult; I don''t have an answer right now.


          cc @yjernite @aymm'
        updatedAt: '2022-07-18T19:13:40.875Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Muennighoff
    id: 62d5b0e4df299556f9eb9976
    type: comment
  author: cakiki
  content: 'Sorry for the late reply Niklas! I somehow was not notified I was mentioned.


    1. We have 1,650,804 bytes of Lingala data; not sure why it''s not listed. Good
    catch! I will have a look. For reference:



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1658170972850-5e70f6048ce3c604d78fe133.png)



    2. Estimating how many languages accidentally made their way into the corpus is
    a very interesting question (This is probably mostly affects the catalogue). I
    think we have to explicitly quantify this; I will bring this up in tomorrow''s
    Viz and Analysis meeting. Now the question about whether a certain threshold might
    qualify a language into being considered a "training language" is both fascinating
    and difficult; I don''t have an answer right now.


    cc @yjernite @aymm'
  created_at: 2022-07-18 18:13:40+00:00
  edited: false
  hidden: false
  id: 62d5b0e4df299556f9eb9976
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594144055859-5ee3a7cd2a3eae3cbdad1305.jpeg?w=200&h=200&f=face
      fullname: Yacine Jernite
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yjernite
      type: user
    createdAt: '2022-07-18T19:46:20.000Z'
    data:
      edited: false
      editors:
      - yjernite
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594144055859-5ee3a7cd2a3eae3cbdad1305.jpeg?w=200&h=200&f=face
          fullname: Yacine Jernite
          isHf: true
          isPro: false
          name: yjernite
          type: user
        html: '<p>Hi Niklas, thanks for catching the oversight!</p>

          <p>That''s an interesting question, and there are indeed several good definitions
          of what should be considered a training language (or what constitutes a
          language really!).</p>

          <p>In our case, much of the philosophy of how we built the training corpus
          was constructive: identify languages we wanted to represent <em>a priori</em>,
          find participants who had expertise in those languages, select some sources
          of data in the language of interest, and make choices about how each of
          these languages was pre-processed.  We are taking a process-driven perspective
          here, and identifying training languages as the ones for which we followed
          this approach. Conversely, German and Japanese did not receive the same
          level of attention or intentionality, so we are not listing them.</p>

          <p>This brings us to the question of evaluation and reporting the model''s
          performance (and whether they come from the training data or from cross-lingual
          transfer). We do have evaluations that include German at least (I''m not
          sure about Japanese), so we will have to qualify those results by pointing
          to the German text in the training data. We are also missing evaluations
          for some of the languages we intentionally included. We do want to have
          evaluations for all of the Niger-Congo languages eventually, beyond the
          9 that are currently in <a rel="nofollow" href="https://github.com/facebookresearch/flores/blob/main/flores200/README.md">FLORES</a>
          - signaling that there is an LLM out there trained on as much data as was
          available in 2022 for each of these languages (we really looked!) will hopefully
          help make that happen.</p>

          <p>TLDR; in the context of describing the training corpus and collaborative
          approach, listing intentionally selected languages makes more sense even
          if it can conflict with how we think about training languages for evaluation.
          We''ll have to keep analyzing our corpus and document the "language leakage"
          to make sure evaluation results are properly interpreted.</p>

          '
        raw: 'Hi Niklas, thanks for catching the oversight!


          That''s an interesting question, and there are indeed several good definitions
          of what should be considered a training language (or what constitutes a
          language really!).


          In our case, much of the philosophy of how we built the training corpus
          was constructive: identify languages we wanted to represent *a priori*,
          find participants who had expertise in those languages, select some sources
          of data in the language of interest, and make choices about how each of
          these languages was pre-processed.  We are taking a process-driven perspective
          here, and identifying training languages as the ones for which we followed
          this approach. Conversely, German and Japanese did not receive the same
          level of attention or intentionality, so we are not listing them.


          This brings us to the question of evaluation and reporting the model''s
          performance (and whether they come from the training data or from cross-lingual
          transfer). We do have evaluations that include German at least (I''m not
          sure about Japanese), so we will have to qualify those results by pointing
          to the German text in the training data. We are also missing evaluations
          for some of the languages we intentionally included. We do want to have
          evaluations for all of the Niger-Congo languages eventually, beyond the
          9 that are currently in [FLORES](https://github.com/facebookresearch/flores/blob/main/flores200/README.md)
          - signaling that there is an LLM out there trained on as much data as was
          available in 2022 for each of these languages (we really looked!) will hopefully
          help make that happen.


          TLDR; in the context of describing the training corpus and collaborative
          approach, listing intentionally selected languages makes more sense even
          if it can conflict with how we think about training languages for evaluation.
          We''ll have to keep analyzing our corpus and document the "language leakage"
          to make sure evaluation results are properly interpreted.'
        updatedAt: '2022-07-18T19:46:20.580Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Muennighoff
    id: 62d5b88c5c29ac61fecb7070
    type: comment
  author: yjernite
  content: 'Hi Niklas, thanks for catching the oversight!


    That''s an interesting question, and there are indeed several good definitions
    of what should be considered a training language (or what constitutes a language
    really!).


    In our case, much of the philosophy of how we built the training corpus was constructive:
    identify languages we wanted to represent *a priori*, find participants who had
    expertise in those languages, select some sources of data in the language of interest,
    and make choices about how each of these languages was pre-processed.  We are
    taking a process-driven perspective here, and identifying training languages as
    the ones for which we followed this approach. Conversely, German and Japanese
    did not receive the same level of attention or intentionality, so we are not listing
    them.


    This brings us to the question of evaluation and reporting the model''s performance
    (and whether they come from the training data or from cross-lingual transfer).
    We do have evaluations that include German at least (I''m not sure about Japanese),
    so we will have to qualify those results by pointing to the German text in the
    training data. We are also missing evaluations for some of the languages we intentionally
    included. We do want to have evaluations for all of the Niger-Congo languages
    eventually, beyond the 9 that are currently in [FLORES](https://github.com/facebookresearch/flores/blob/main/flores200/README.md)
    - signaling that there is an LLM out there trained on as much data as was available
    in 2022 for each of these languages (we really looked!) will hopefully help make
    that happen.


    TLDR; in the context of describing the training corpus and collaborative approach,
    listing intentionally selected languages makes more sense even if it can conflict
    with how we think about training languages for evaluation. We''ll have to keep
    analyzing our corpus and document the "language leakage" to make sure evaluation
    results are properly interpreted.'
  created_at: 2022-07-18 18:46:20+00:00
  edited: false
  hidden: false
  id: 62d5b88c5c29ac61fecb7070
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-07-22T07:01:28.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<p>Great thoughts, thanks a lot <span data-props=\"{&quot;user&quot;:&quot;yjernite&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yjernite\"\
          >@<span class=\"underline\">yjernite</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;cakiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cakiki\">@<span class=\"\
          underline\">cakiki</span></a></span>\n\n\t</span></span>! Indeed FLORES-200\
          \ has all of our training languages except for code so we can evaluate them\
          \ all on it  \U0001F44D<br>I opened a PR here for Lingala: <a href=\"https://huggingface.co/bigscience/bloom/discussions/60\"\
          >https://huggingface.co/bigscience/bloom/discussions/60</a> \U0001F607</p>\n"
        raw: "Great thoughts, thanks a lot @yjernite @cakiki! Indeed FLORES-200 has\
          \ all of our training languages except for code so we can evaluate them\
          \ all on it  \U0001F44D\nI opened a PR here for Lingala: https://huggingface.co/bigscience/bloom/discussions/60\
          \ \U0001F607"
        updatedAt: '2022-07-22T07:01:28.824Z'
      numEdits: 0
      reactions: []
      relatedEventId: 62da4b48928ae50f4a35805b
    id: 62da4b48928ae50f4a35805a
    type: comment
  author: Muennighoff
  content: "Great thoughts, thanks a lot @yjernite @cakiki! Indeed FLORES-200 has\
    \ all of our training languages except for code so we can evaluate them all on\
    \ it  \U0001F44D\nI opened a PR here for Lingala: https://huggingface.co/bigscience/bloom/discussions/60\
    \ \U0001F607"
  created_at: 2022-07-22 06:01:28+00:00
  edited: false
  hidden: false
  id: 62da4b48928ae50f4a35805a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-07-22T07:01:28.000Z'
    data:
      status: closed
    id: 62da4b48928ae50f4a35805b
    type: status-change
  author: Muennighoff
  created_at: 2022-07-22 06:01:28+00:00
  id: 62da4b48928ae50f4a35805b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 47
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: BLOOM training languages inconsistencies
