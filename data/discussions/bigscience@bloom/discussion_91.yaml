!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rwheel
conflicting_files: null
created_at: 2022-08-22 08:01:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659510146200-62d9166f0f293ca5af576371.jpeg?w=200&h=200&f=face
      fullname: "Ram\xF3n"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rwheel
      type: user
    createdAt: '2022-08-22T09:01:45.000Z'
    data:
      edited: false
      editors:
      - rwheel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659510146200-62d9166f0f293ca5af576371.jpeg?w=200&h=200&f=face
          fullname: "Ram\xF3n"
          isHf: false
          isPro: false
          name: rwheel
          type: user
        html: "<p>I want to stop text generation when a set of special characters\
          \ are found, like \u2018###\u2019, but I can\u2019t achieve it. I know that\
          \ I can implement a piece of code to post-process the generated text and\
          \ extract the expected result, but it would be interesting to stop text\
          \ generation when a criteria is fulfilled to save some words/tokens in the\
          \ task.</p>\n<p>I\u2019m using the parameter eos_token_id to that end, but\
          \ it doesn\u2019t work. I thought that I was doing something wrong, but\
          \ I tried the model Incoder with the same eos_token_id parameter and it\
          \ works!</p>\n<p>Anyone know if this is a specific problem of BLOOM or am\
          \ I doing something wrong?</p>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(\"\
          bigscience/bloom\")\nend_sequence = '###'\n\npayload = {\n    \"inputs\"\
          : f\"{context} \\nHuman: {nl_query} \\nAI: \",\n    \"parameters\":\n  \
          \  {\n        \"top_p\": 0.9,\n        \"temperature\": 0.2,\n        \"\
          max_new_tokens\": 40,\n        \"eos_token_id\": int(tokenizer.convert_tokens_to_ids(end_sequence)),\n\
          \        \"return_full_text\": False\n    }, \n    \"options\": \n    {\
          \  \n          \"use_cache\": True,\n          \"wait_for_model\": True\n\
          \      }\n}\nresponse = requests.post(API_URL, headers=headers, json=payload)\n\
          </code></pre>\n<p>Thanks!</p>\n"
        raw: "I want to stop text generation when a set of special characters are\
          \ found, like \u2018###\u2019, but I can\u2019t achieve it. I know that\
          \ I can implement a piece of code to post-process the generated text and\
          \ extract the expected result, but it would be interesting to stop text\
          \ generation when a criteria is fulfilled to save some words/tokens in the\
          \ task.\r\n\r\nI\u2019m using the parameter eos_token_id to that end, but\
          \ it doesn\u2019t work. I thought that I was doing something wrong, but\
          \ I tried the model Incoder with the same eos_token_id parameter and it\
          \ works!\r\n\r\nAnyone know if this is a specific problem of BLOOM or am\
          \ I doing something wrong?\r\n\r\n```\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          bigscience/bloom\")\r\nend_sequence = '###'\r\n\r\npayload = {\r\n    \"\
          inputs\": f\"{context} \\nHuman: {nl_query} \\nAI: \",\r\n    \"parameters\"\
          :\r\n    {\r\n        \"top_p\": 0.9,\r\n        \"temperature\": 0.2,\r\
          \n        \"max_new_tokens\": 40,\r\n        \"eos_token_id\": int(tokenizer.convert_tokens_to_ids(end_sequence)),\r\
          \n        \"return_full_text\": False\r\n    }, \r\n    \"options\": \r\n\
          \    {  \r\n          \"use_cache\": True,\r\n          \"wait_for_model\"\
          : True\r\n      }\r\n}\r\nresponse = requests.post(API_URL, headers=headers,\
          \ json=payload)\r\n```\r\n\r\nThanks!"
        updatedAt: '2022-08-22T09:01:45.092Z'
      numEdits: 0
      reactions: []
    id: 630345f9d06a313c3ecb782b
    type: comment
  author: rwheel
  content: "I want to stop text generation when a set of special characters are found,\
    \ like \u2018###\u2019, but I can\u2019t achieve it. I know that I can implement\
    \ a piece of code to post-process the generated text and extract the expected\
    \ result, but it would be interesting to stop text generation when a criteria\
    \ is fulfilled to save some words/tokens in the task.\r\n\r\nI\u2019m using the\
    \ parameter eos_token_id to that end, but it doesn\u2019t work. I thought that\
    \ I was doing something wrong, but I tried the model Incoder with the same eos_token_id\
    \ parameter and it works!\r\n\r\nAnyone know if this is a specific problem of\
    \ BLOOM or am I doing something wrong?\r\n\r\n```\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    bigscience/bloom\")\r\nend_sequence = '###'\r\n\r\npayload = {\r\n    \"inputs\"\
    : f\"{context} \\nHuman: {nl_query} \\nAI: \",\r\n    \"parameters\":\r\n    {\r\
    \n        \"top_p\": 0.9,\r\n        \"temperature\": 0.2,\r\n        \"max_new_tokens\"\
    : 40,\r\n        \"eos_token_id\": int(tokenizer.convert_tokens_to_ids(end_sequence)),\r\
    \n        \"return_full_text\": False\r\n    }, \r\n    \"options\": \r\n    {\
    \  \r\n          \"use_cache\": True,\r\n          \"wait_for_model\": True\r\n\
    \      }\r\n}\r\nresponse = requests.post(API_URL, headers=headers, json=payload)\r\
    \n```\r\n\r\nThanks!"
  created_at: 2022-08-22 08:01:45+00:00
  edited: false
  hidden: false
  id: 630345f9d06a313c3ecb782b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-04T08:26:43.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Hey! The problem with BLOOM is it has never really seen an end-of-sequence
          token during text. Our finetuned <a href="https://huggingface.co/bigscience/bloomz">BLOOMZ</a>
          model will automatically stop when it deems it appropriate to do so (Normally
          after fulfilling the user requested task).</p>

          '
        raw: Hey! The problem with BLOOM is it has never really seen an end-of-sequence
          token during text. Our finetuned [BLOOMZ](https://huggingface.co/bigscience/bloomz)
          model will automatically stop when it deems it appropriate to do so (Normally
          after fulfilling the user requested task).
        updatedAt: '2022-11-04T08:26:43.304Z'
      numEdits: 0
      reactions: []
    id: 6364ccc3086df4503e2ea60c
    type: comment
  author: Muennighoff
  content: Hey! The problem with BLOOM is it has never really seen an end-of-sequence
    token during text. Our finetuned [BLOOMZ](https://huggingface.co/bigscience/bloomz)
    model will automatically stop when it deems it appropriate to do so (Normally
    after fulfilling the user requested task).
  created_at: 2022-11-04 07:26:43+00:00
  edited: false
  hidden: false
  id: 6364ccc3086df4503e2ea60c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659510146200-62d9166f0f293ca5af576371.jpeg?w=200&h=200&f=face
      fullname: "Ram\xF3n"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rwheel
      type: user
    createdAt: '2022-11-28T12:06:21.000Z'
    data:
      edited: false
      editors:
      - rwheel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659510146200-62d9166f0f293ca5af576371.jpeg?w=200&h=200&f=face
          fullname: "Ram\xF3n"
          isHf: false
          isPro: false
          name: rwheel
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \ !</p>\n"
        raw: Thanks @Muennighoff !
        updatedAt: '2022-11-28T12:06:21.323Z'
      numEdits: 0
      reactions: []
    id: 6384a43ddf161a84eb668ae9
    type: comment
  author: rwheel
  content: Thanks @Muennighoff !
  created_at: 2022-11-28 12:06:21+00:00
  edited: false
  hidden: false
  id: 6384a43ddf161a84eb668ae9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659510146200-62d9166f0f293ca5af576371.jpeg?w=200&h=200&f=face
      fullname: "Ram\xF3n"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rwheel
      type: user
    createdAt: '2022-11-28T12:06:25.000Z'
    data:
      status: closed
    id: 6384a44116ccd1034bb024ae
    type: status-change
  author: rwheel
  created_at: 2022-11-28 12:06:25+00:00
  id: 6384a44116ccd1034bb024ae
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 91
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: Stopping criteria in text generation
