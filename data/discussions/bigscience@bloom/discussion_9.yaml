!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gsarti
conflicting_files: null
created_at: 2022-06-30 13:45:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg?w=200&h=200&f=face
      fullname: Gabriele Sarti
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gsarti
      type: user
    createdAt: '2022-06-30T14:45:31.000Z'
    data:
      edited: false
      editors:
      - gsarti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg?w=200&h=200&f=face
          fullname: Gabriele Sarti
          isHf: false
          isPro: false
          name: gsarti
          type: user
        html: '<p>I''m trying to use Bloom for generation with the Inference API,
          but if the input is not minimal I only get 1-2 extra tokens. I tried to
          check the detailed parameters in the docs, but there doesn''t seem to be
          a way to control length for text generation models (not seq2seq). Moreover,
          even if I try to pass some parameters to the API request I get a message
          saying "Parameters are not accepted for this specific model".</p>

          <p>Is there a way to enable more lengthy generation using Bloom?</p>

          '
        raw: "I'm trying to use Bloom for generation with the Inference API, but if\
          \ the input is not minimal I only get 1-2 extra tokens. I tried to check\
          \ the detailed parameters in the docs, but there doesn't seem to be a way\
          \ to control length for text generation models (not seq2seq). Moreover,\
          \ even if I try to pass some parameters to the API request I get a message\
          \ saying \"Parameters are not accepted for this specific model\".\r\n\r\n\
          Is there a way to enable more lengthy generation using Bloom?"
        updatedAt: '2022-06-30T14:45:31.651Z'
      numEdits: 0
      reactions:
      - count: 15
        reaction: "\U0001F44D"
        users:
        - Batou
        - Yos9879
        - milkyroad
        - julien-c
        - odegiber
        - orangezulu
        - lhubbard
        - bergr7
        - LincolnStein
        - Hisashi
        - andreaparker
        - alex0603
        - tomjennings
        - tradesurplus
        - Setora
    id: 62bdb70ba71dccf7ed72c3e5
    type: comment
  author: gsarti
  content: "I'm trying to use Bloom for generation with the Inference API, but if\
    \ the input is not minimal I only get 1-2 extra tokens. I tried to check the detailed\
    \ parameters in the docs, but there doesn't seem to be a way to control length\
    \ for text generation models (not seq2seq). Moreover, even if I try to pass some\
    \ parameters to the API request I get a message saying \"Parameters are not accepted\
    \ for this specific model\".\r\n\r\nIs there a way to enable more lengthy generation\
    \ using Bloom?"
  created_at: 2022-06-30 13:45:31+00:00
  edited: false
  hidden: false
  id: 62bdb70ba71dccf7ed72c3e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/95cd48f8ec28fac25f7c0786d7848068.svg
      fullname: Lincoln Stein
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LincolnStein
      type: user
    createdAt: '2022-07-11T01:36:46.000Z'
    data:
      edited: false
      editors:
      - LincolnStein
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/95cd48f8ec28fac25f7c0786d7848068.svg
          fullname: Lincoln Stein
          isHf: false
          isPro: false
          name: LincolnStein
          type: user
        html: "<p>I\u2019m frustrated with this as well. Text generation is useless.</p>\n"
        raw: "I\u2019m frustrated with this as well. Text generation is useless."
        updatedAt: '2022-07-11T01:36:46.287Z'
      numEdits: 0
      reactions: []
    id: 62cb7eae729ccd98a161f9c9
    type: comment
  author: LincolnStein
  content: "I\u2019m frustrated with this as well. Text generation is useless."
  created_at: 2022-07-11 00:36:46+00:00
  edited: false
  hidden: false
  id: 62cb7eae729ccd98a161f9c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-14T23:57:38.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Hi sorry for the very long delay. Currently we hard limit the number
          of tokens one can request in order to prevent users from requesting too
          many tokens and causing the entire system to crash. Though I''m quite surprised
          it generates 1-2 extra tokens ... it should be a lot more.</p>

          <p>I''m closing this discussion due to this being a few months old and quite
          a few things have changed since then. Please feel free to re-open if you
          still see this issue.</p>

          '
        raw: 'Hi sorry for the very long delay. Currently we hard limit the number
          of tokens one can request in order to prevent users from requesting too
          many tokens and causing the entire system to crash. Though I''m quite surprised
          it generates 1-2 extra tokens ... it should be a lot more.


          I''m closing this discussion due to this being a few months old and quite
          a few things have changed since then. Please feel free to re-open if you
          still see this issue.'
        updatedAt: '2022-11-14T23:57:38.997Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6372d5f214acf5038143740f
    id: 6372d5f214acf5038143740e
    type: comment
  author: TimeRobber
  content: 'Hi sorry for the very long delay. Currently we hard limit the number of
    tokens one can request in order to prevent users from requesting too many tokens
    and causing the entire system to crash. Though I''m quite surprised it generates
    1-2 extra tokens ... it should be a lot more.


    I''m closing this discussion due to this being a few months old and quite a few
    things have changed since then. Please feel free to re-open if you still see this
    issue.'
  created_at: 2022-11-14 23:57:38+00:00
  edited: false
  hidden: false
  id: 6372d5f214acf5038143740e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-14T23:57:38.000Z'
    data:
      status: closed
    id: 6372d5f214acf5038143740f
    type: status-change
  author: TimeRobber
  created_at: 2022-11-14 23:57:38+00:00
  id: 6372d5f214acf5038143740f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: Controlling generation length
