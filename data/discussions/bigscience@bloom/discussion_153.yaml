!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ilLancio
conflicting_files: null
created_at: 2022-12-12 20:35:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a60bcf3644989ce5e868882621edc7f2.svg
      fullname: Daniele Lanciotti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ilLancio
      type: user
    createdAt: '2022-12-12T20:35:07.000Z'
    data:
      edited: true
      editors:
      - ilLancio
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a60bcf3644989ce5e868882621edc7f2.svg
          fullname: Daniele Lanciotti
          isHf: false
          isPro: false
          name: ilLancio
          type: user
        html: "<p>The following issues have been observed when using the <code>inferenceApi</code>\
          \ class:</p>\n<h1 id=\"issue-1-incorrect-return_full_text-parameter\">Issue\
          \ 1: Incorrect <code>return_full_text</code> parameter</h1>\n<p>When the\
          \ <code>return_full_text</code> parameter is set to <code>False</code>,\
          \ the full text is still returned in the output.</p>\n<h1 id=\"issue-2-incorrect-handling-of-punctuation-in-generated-text\"\
          >Issue 2: Incorrect handling of punctuation in generated text</h1>\n<p>When\
          \ a punctuation character (only <code>!,.?</code>) is preceded by a space\
          \ in the prompt, the space is not preserved in the <code>generated_text</code>\
          \ output, even if the prompt was generated by the AI itself. This can be\
          \ seen in the following example:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-meta\">&gt;&gt;&gt; </span>inference = InferenceApi(<span\
          \ class=\"hljs-string\">\"bigscience/bloom\"</span>, token = HfFolder.get_token())\n\
          <span class=\"hljs-meta\">&gt;&gt;&gt; </span>generated_text = inference(<span\
          \ class=\"hljs-string\">\"Hello\"</span>) <span class=\"hljs-comment\">#\
          \ Output: \"Hello , world\"</span>\n<span class=\"hljs-meta\">&gt;&gt;&gt;\
          \ </span>inference(generated_text) \n<span class=\"hljs-string\">'Hello,\
          \ world and some other text'</span> <span class=\"hljs-comment\"># Space\
          \ before \",\" is not preserved</span>\n</code></pre>\n<p>It is possible\
          \ that these two issues are related. We hope that a fix will be implemented\
          \ soon.</p>\n"
        raw: "The following issues have been observed when using the `inferenceApi`\
          \ class:\n\n# Issue 1: Incorrect `return_full_text` parameter\nWhen the\
          \ `return_full_text` parameter is set to `False`, the full text is still\
          \ returned in the output.\n\n# Issue 2: Incorrect handling of punctuation\
          \ in generated text\nWhen a punctuation character (only `!,.?`) is preceded\
          \ by a space in the prompt, the space is not preserved in the `generated_text`\
          \ output, even if the prompt was generated by the AI itself. This can be\
          \ seen in the following example:\n\n```python\n>>> inference = InferenceApi(\"\
          bigscience/bloom\", token = HfFolder.get_token())\n>>> generated_text =\
          \ inference(\"Hello\") # Output: \"Hello , world\"\n>>> inference(generated_text)\
          \ \n'Hello, world and some other text' # Space before \",\" is not preserved\n\
          ```\nIt is possible that these two issues are related. We hope that a fix\
          \ will be implemented soon."
        updatedAt: '2022-12-14T18:46:23.804Z'
      numEdits: 3
      reactions: []
    id: 6397907b71eb2455d898e0a4
    type: comment
  author: ilLancio
  content: "The following issues have been observed when using the `inferenceApi`\
    \ class:\n\n# Issue 1: Incorrect `return_full_text` parameter\nWhen the `return_full_text`\
    \ parameter is set to `False`, the full text is still returned in the output.\n\
    \n# Issue 2: Incorrect handling of punctuation in generated text\nWhen a punctuation\
    \ character (only `!,.?`) is preceded by a space in the prompt, the space is not\
    \ preserved in the `generated_text` output, even if the prompt was generated by\
    \ the AI itself. This can be seen in the following example:\n\n```python\n>>>\
    \ inference = InferenceApi(\"bigscience/bloom\", token = HfFolder.get_token())\n\
    >>> generated_text = inference(\"Hello\") # Output: \"Hello , world\"\n>>> inference(generated_text)\
    \ \n'Hello, world and some other text' # Space before \",\" is not preserved\n\
    ```\nIt is possible that these two issues are related. We hope that a fix will\
    \ be implemented soon."
  created_at: 2022-12-12 20:35:07+00:00
  edited: true
  hidden: false
  id: 6397907b71eb2455d898e0a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/a60bcf3644989ce5e868882621edc7f2.svg
      fullname: Daniele Lanciotti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ilLancio
      type: user
    createdAt: '2022-12-14T18:38:39.000Z'
    data:
      from: 'BLOOM Inference API bugs: return_full_text and punctuation'
      to: 'BLOOM Inference API issues: return_full_text and punctuation'
    id: 639a182fa6e1e5cb70b4e3f7
    type: title-change
  author: ilLancio
  created_at: 2022-12-14 18:38:39+00:00
  id: 639a182fa6e1e5cb70b4e3f7
  new_title: 'BLOOM Inference API issues: return_full_text and punctuation'
  old_title: 'BLOOM Inference API bugs: return_full_text and punctuation'
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-12-15T17:01:33.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Hi!</p>

          <blockquote>

          <p>When the return_full_text parameter is set to False, the full text is
          still returned in the output.</p>

          </blockquote>

          <p>Yeah this is somewhat expected, BLOOM being so large has to undergo a
          special deployment mechanism where we essentially recoded some generate
          functions. Right now that deployment has very limited features.</p>

          <blockquote>

          <p>Issue 2: Incorrect handling of punctuation in generated text</p>

          </blockquote>

          <p>Given the code snipped you shared, can you try running <code>inference("Hello,
          world")</code> I'',m suspecting that you''re <code>generated_text</code>
          doesn''t correspond to the comment you shared.</p>

          '
        raw: 'Hi!


          > When the return_full_text parameter is set to False, the full text is
          still returned in the output.


          Yeah this is somewhat expected, BLOOM being so large has to undergo a special
          deployment mechanism where we essentially recoded some generate functions.
          Right now that deployment has very limited features.


          > Issue 2: Incorrect handling of punctuation in generated text


          Given the code snipped you shared, can you try running `inference("Hello,
          world")` I'',m suspecting that you''re `generated_text` doesn''t correspond
          to the comment you shared.'
        updatedAt: '2022-12-15T17:01:33.227Z'
      numEdits: 0
      reactions: []
    id: 639b52ede5aeb2fd0b6d1d08
    type: comment
  author: TimeRobber
  content: 'Hi!


    > When the return_full_text parameter is set to False, the full text is still
    returned in the output.


    Yeah this is somewhat expected, BLOOM being so large has to undergo a special
    deployment mechanism where we essentially recoded some generate functions. Right
    now that deployment has very limited features.


    > Issue 2: Incorrect handling of punctuation in generated text


    Given the code snipped you shared, can you try running `inference("Hello, world")`
    I'',m suspecting that you''re `generated_text` doesn''t correspond to the comment
    you shared.'
  created_at: 2022-12-15 17:01:33+00:00
  edited: false
  hidden: false
  id: 639b52ede5aeb2fd0b6d1d08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a60bcf3644989ce5e868882621edc7f2.svg
      fullname: Daniele Lanciotti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ilLancio
      type: user
    createdAt: '2022-12-18T15:07:48.000Z'
    data:
      edited: true
      editors:
      - ilLancio
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a60bcf3644989ce5e868882621edc7f2.svg
          fullname: Daniele Lanciotti
          isHf: false
          isPro: false
          name: ilLancio
          type: user
        html: '<blockquote>

          <p>Given the code snipped you shared, can you try running <code>inference("Hello,
          world")</code> I'',m suspecting that you''re <code>generated_text</code>
          doesn''t correspond to the comment you shared.</p>

          </blockquote>

          <p>I don''t think I understand what you are trying to point out about the
          second issue.<br>The inputs and outputs I wrote in the example are just
          to let understand what I''m trying to convey more clearly and in a shorter
          form, and are not actual inputs and outputs.<br>Here you can see an actual
          input and output:</p>

          <pre><code class="language-python"><span class="hljs-meta">&gt;&gt;&gt;
          </span>generated_text = inference(<span class="hljs-string">"  ..  ?  !  ,"</span>)[<span
          class="hljs-number">0</span>][<span class="hljs-string">"generated_text"</span>]

          <span class="hljs-meta">&gt;&gt;&gt; </span>generated_text

          <span class="hljs-string">'' .. ? ! ,! ! ! ! ! ! ! ! ! !  ''</span>

          <span class="hljs-meta">&gt;&gt;&gt; </span>inference(generated_text)[<span
          class="hljs-number">0</span>][<span class="hljs-string">"generated_text"</span>]

          <span class="hljs-string">''..?!,!!!!!!!!!! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!''</span>

          </code></pre>

          '
        raw: '> Given the code snipped you shared, can you try running `inference("Hello,
          world")` I'',m suspecting that you''re `generated_text` doesn''t correspond
          to the comment you shared.


          I don''t think I understand what you are trying to point out about the second
          issue.

          The inputs and outputs I wrote in the example are just to let understand
          what I''m trying to convey more clearly and in a shorter form, and are not
          actual inputs and outputs.

          Here you can see an actual input and output:

          ```python

          >>> generated_text = inference("  ..  ?  !  ,")[0]["generated_text"]

          >>> generated_text

          '' .. ? ! ,! ! ! ! ! ! ! ! ! !  ''

          >>> inference(generated_text)[0]["generated_text"]

          ''..?!,!!!!!!!!!! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!''

          ```'
        updatedAt: '2022-12-18T15:08:29.147Z'
      numEdits: 1
      reactions: []
    id: 639f2cc40d679f539434241a
    type: comment
  author: ilLancio
  content: '> Given the code snipped you shared, can you try running `inference("Hello,
    world")` I'',m suspecting that you''re `generated_text` doesn''t correspond to
    the comment you shared.


    I don''t think I understand what you are trying to point out about the second
    issue.

    The inputs and outputs I wrote in the example are just to let understand what
    I''m trying to convey more clearly and in a shorter form, and are not actual inputs
    and outputs.

    Here you can see an actual input and output:

    ```python

    >>> generated_text = inference("  ..  ?  !  ,")[0]["generated_text"]

    >>> generated_text

    '' .. ? ! ,! ! ! ! ! ! ! ! ! !  ''

    >>> inference(generated_text)[0]["generated_text"]

    ''..?!,!!!!!!!!!! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!''

    ```'
  created_at: 2022-12-18 15:07:48+00:00
  edited: true
  hidden: false
  id: 639f2cc40d679f539434241a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-12-19T15:01:44.000Z'
    data:
      edited: true
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p>I couldn't reproduce the first example. Ran the following:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ huggingface_hub <span class=\"hljs-keyword\">import</span> InferenceApi\n\
          \ninference = InferenceApi(<span class=\"hljs-string\">\"bigscience/bloom\"\
          </span>, token =<span class=\"hljs-literal\">True</span>)\ngenerated_text\
          \ = inference(<span class=\"hljs-string\">\"Hello\"</span>)[<span class=\"\
          hljs-number\">0</span>][<span class=\"hljs-string\">\"generated_text\"</span>]\n\
          \n<span class=\"hljs-built_in\">print</span>(generated_text) <span class=\"\
          hljs-comment\"># prints \"Hello, I am a young woman of 28 years old who\
          \ has just arrived in New Braunfels for\"</span>\n\nnew_generated_text =\
          \ inference(generated_text)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">\"generated_text\"</span>]\n\n<span class=\"hljs-built_in\"\
          >print</span>(new_generated_text) <span class=\"hljs-comment\"># prints\
          \ \"Hello, I am a young woman of 28 years old who has just arrived in New\
          \ Braunfels for the first time. I like everything but above all a perfect\
          \ hygiene. I take my time because quality\"</span>\n</code></pre>\n<p>But\
          \ the second one seems to do that:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> huggingface_hub <span class=\"\
          hljs-keyword\">import</span> InferenceApi\n\ninference = InferenceApi(<span\
          \ class=\"hljs-string\">\"bigscience/bloom\"</span>, token =<span class=\"\
          hljs-literal\">True</span>)\ngenerated_text = inference(<span class=\"hljs-string\"\
          >\"  ..  ?  !  ,\"</span>)[<span class=\"hljs-number\">0</span>][<span class=\"\
          hljs-string\">\"generated_text\"</span>]\n\n<span class=\"hljs-built_in\"\
          >print</span>(generated_text) <span class=\"hljs-comment\"># prints \" ..\
          \ ? ! ,! ! ! ! ! ! ! ! ! !  \"</span>\n\nnew_generated_text = inference(generated_text)[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">\"generated_text\"\
          </span>]\n\n<span class=\"hljs-built_in\">print</span>(new_generated_text)\
          \ <span class=\"hljs-comment\"># prints \"..?!,!!!!!!!!!! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!\"\
          </span>\n</code></pre>\n<p>Thanks for reporting. I'm guessing this has something\
          \ to do with the inference API somehow. cc <span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Narsil\"\
          >@<span class=\"underline\">Narsil</span></a></span>\n\n\t</span></span></p>\n"
        raw: 'I couldn''t reproduce the first example. Ran the following:

          ```python

          from huggingface_hub import InferenceApi


          inference = InferenceApi("bigscience/bloom", token =True)

          generated_text = inference("Hello")[0]["generated_text"]


          print(generated_text) # prints "Hello, I am a young woman of 28 years old
          who has just arrived in New Braunfels for"


          new_generated_text = inference(generated_text)[0]["generated_text"]


          print(new_generated_text) # prints "Hello, I am a young woman of 28 years
          old who has just arrived in New Braunfels for the first time. I like everything
          but above all a perfect hygiene. I take my time because quality"

          ```


          But the second one seems to do that:


          ```python

          from huggingface_hub import InferenceApi


          inference = InferenceApi("bigscience/bloom", token =True)

          generated_text = inference("  ..  ?  !  ,")[0]["generated_text"]


          print(generated_text) # prints " .. ? ! ,! ! ! ! ! ! ! ! ! !  "


          new_generated_text = inference(generated_text)[0]["generated_text"]


          print(new_generated_text) # prints "..?!,!!!!!!!!!! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!"

          ```


          Thanks for reporting. I''m guessing this has something to do with the inference
          API somehow. cc @Narsil'
        updatedAt: '2022-12-19T15:02:16.582Z'
      numEdits: 1
      reactions: []
    id: 63a07cd82fabbbb8998f60de
    type: comment
  author: TimeRobber
  content: 'I couldn''t reproduce the first example. Ran the following:

    ```python

    from huggingface_hub import InferenceApi


    inference = InferenceApi("bigscience/bloom", token =True)

    generated_text = inference("Hello")[0]["generated_text"]


    print(generated_text) # prints "Hello, I am a young woman of 28 years old who
    has just arrived in New Braunfels for"


    new_generated_text = inference(generated_text)[0]["generated_text"]


    print(new_generated_text) # prints "Hello, I am a young woman of 28 years old
    who has just arrived in New Braunfels for the first time. I like everything but
    above all a perfect hygiene. I take my time because quality"

    ```


    But the second one seems to do that:


    ```python

    from huggingface_hub import InferenceApi


    inference = InferenceApi("bigscience/bloom", token =True)

    generated_text = inference("  ..  ?  !  ,")[0]["generated_text"]


    print(generated_text) # prints " .. ? ! ,! ! ! ! ! ! ! ! ! !  "


    new_generated_text = inference(generated_text)[0]["generated_text"]


    print(new_generated_text) # prints "..?!,!!!!!!!!!! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!"

    ```


    Thanks for reporting. I''m guessing this has something to do with the inference
    API somehow. cc @Narsil'
  created_at: 2022-12-19 15:01:44+00:00
  edited: true
  hidden: false
  id: 63a07cd82fabbbb8998f60de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-12-20T13:00:36.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Okay just to let you know we''ve found the issue. It''s linked to
          our tokenization mechanism. </p>

          <pre><code class="language-python"><span class="hljs-meta">&gt;&gt;&gt;
          </span>tok = AutoTokenizer.from_pretrained(<span class="hljs-string">"bigscience/bloom"</span>,
          use_fast = <span class="hljs-literal">True</span>)

          <span class="hljs-meta">&gt;&gt;&gt; </span>tok.decode(tok.encode(<span
          class="hljs-string">"Hello , there"</span>))

          <span class="hljs-string">''Hello, there''</span>

          <span class="hljs-meta">&gt;&gt;&gt; </span>tok.decode(tok.encode(<span
          class="hljs-string">"Hello , there"</span>), clean_up_tokenization_spaces=<span
          class="hljs-literal">False</span>)

          <span class="hljs-string">''Hello , there''</span>

          </code></pre>

          <p>We''ll work towards fixing this ASAP!</p>

          '
        raw: "Okay just to let you know we've found the issue. It's linked to our\
          \ tokenization mechanism. \n```python\n>>> tok = AutoTokenizer.from_pretrained(\"\
          bigscience/bloom\", use_fast = True)\n>>> tok.decode(tok.encode(\"Hello\
          \ , there\"))\n'Hello, there'\n>>> tok.decode(tok.encode(\"Hello , there\"\
          ), clean_up_tokenization_spaces=False)\n'Hello , there'\n```\nWe'll work\
          \ towards fixing this ASAP!"
        updatedAt: '2022-12-20T13:00:36.067Z'
      numEdits: 0
      reactions: []
    id: 63a1b1f4e36f2e4d5b0b75bd
    type: comment
  author: TimeRobber
  content: "Okay just to let you know we've found the issue. It's linked to our tokenization\
    \ mechanism. \n```python\n>>> tok = AutoTokenizer.from_pretrained(\"bigscience/bloom\"\
    , use_fast = True)\n>>> tok.decode(tok.encode(\"Hello , there\"))\n'Hello, there'\n\
    >>> tok.decode(tok.encode(\"Hello , there\"), clean_up_tokenization_spaces=False)\n\
    'Hello , there'\n```\nWe'll work towards fixing this ASAP!"
  created_at: 2022-12-20 13:00:36+00:00
  edited: false
  hidden: false
  id: 63a1b1f4e36f2e4d5b0b75bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50af066745748d74fb422d32dbf012a0.svg
      fullname: Gorka Bengochea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gorkanauta
      type: user
    createdAt: '2023-01-12T11:55:53.000Z'
    data:
      edited: false
      editors:
      - gorkanauta
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50af066745748d74fb422d32dbf012a0.svg
          fullname: Gorka Bengochea
          isHf: false
          isPro: false
          name: gorkanauta
          type: user
        html: '<p>How is this issue moving forward? I have the same problem, returning
          full text although specified not to :(</p>

          '
        raw: How is this issue moving forward? I have the same problem, returning
          full text although specified not to :(
        updatedAt: '2023-01-12T11:55:53.106Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Eleiber
        - PrLeung
    id: 63bff54995cbca1d8821248c
    type: comment
  author: gorkanauta
  content: How is this issue moving forward? I have the same problem, returning full
    text although specified not to :(
  created_at: 2023-01-12 11:55:53+00:00
  edited: false
  hidden: false
  id: 63bff54995cbca1d8821248c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2023-01-14T19:35:27.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Sorry we''re coming back from holidays + our HF offsite. <a rel="nofollow"
          href="https://github.com/huggingface/text-generation-inference/pull/13">https://github.com/huggingface/text-generation-inference/pull/13</a>
          should patch BLOOM. It just requires to be deployed which should happen
          ASAP<br> We still want to improve the fix, typically handling <code>return_full_text</code>
          option. If you feel like contributing, please feel free to do so</p>

          '
        raw: "Sorry we're coming back from holidays + our HF offsite. https://github.com/huggingface/text-generation-inference/pull/13\
          \ should patch BLOOM. It just requires to be deployed which should happen\
          \ ASAP \n We still want to improve the fix, typically handling `return_full_text`\
          \ option. If you feel like contributing, please feel free to do so"
        updatedAt: '2023-01-14T19:35:27.461Z'
      numEdits: 0
      reactions: []
    id: 63c303ffaa5c9b84170e35eb
    type: comment
  author: TimeRobber
  content: "Sorry we're coming back from holidays + our HF offsite. https://github.com/huggingface/text-generation-inference/pull/13\
    \ should patch BLOOM. It just requires to be deployed which should happen ASAP\
    \ \n We still want to improve the fix, typically handling `return_full_text` option.\
    \ If you feel like contributing, please feel free to do so"
  created_at: 2023-01-14 19:35:27+00:00
  edited: false
  hidden: false
  id: 63c303ffaa5c9b84170e35eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2023-01-19T23:33:00.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span></p>\n"
        raw: cc @olivierdehaene
        updatedAt: '2023-01-19T23:33:00.243Z'
      numEdits: 0
      reactions: []
    id: 63c9d32c89179f89c72e4d6c
    type: comment
  author: TimeRobber
  content: cc @olivierdehaene
  created_at: 2023-01-19 23:33:00+00:00
  edited: false
  hidden: false
  id: 63c9d32c89179f89c72e4d6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2023-01-26T13:16:10.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p>Closing as <span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>\
          \ has deployed newer version. It fixes the bug from <a href=\"https://huggingface.co/bigscience/bloom/discussions/153#63a07cd82fabbbb8998f60de\"\
          >https://huggingface.co/bigscience/bloom/discussions/153#63a07cd82fabbbb8998f60de</a></p>\n\
          <p>Feel free to re-open if you still see an issue.</p>\n"
        raw: 'Closing as @olivierdehaene has deployed newer version. It fixes the
          bug from https://huggingface.co/bigscience/bloom/discussions/153#63a07cd82fabbbb8998f60de


          Feel free to re-open if you still see an issue.'
        updatedAt: '2023-01-26T13:16:10.198Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63d27d1a1cfb0d40cbf9e01f
    id: 63d27d1a1cfb0d40cbf9e01e
    type: comment
  author: TimeRobber
  content: 'Closing as @olivierdehaene has deployed newer version. It fixes the bug
    from https://huggingface.co/bigscience/bloom/discussions/153#63a07cd82fabbbb8998f60de


    Feel free to re-open if you still see an issue.'
  created_at: 2023-01-26 13:16:10+00:00
  edited: false
  hidden: false
  id: 63d27d1a1cfb0d40cbf9e01e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2023-01-26T13:16:10.000Z'
    data:
      status: closed
    id: 63d27d1a1cfb0d40cbf9e01f
    type: status-change
  author: TimeRobber
  created_at: 2023-01-26 13:16:10+00:00
  id: 63d27d1a1cfb0d40cbf9e01f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 153
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: 'BLOOM Inference API issues: return_full_text and punctuation'
