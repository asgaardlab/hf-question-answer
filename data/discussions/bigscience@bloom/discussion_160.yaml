!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rsoods
conflicting_files: null
created_at: 2022-12-19 20:27:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/908b038acb6392be6e15338b64899ce9.svg
      fullname: Rohit Sood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rsoods
      type: user
    createdAt: '2022-12-19T20:27:01.000Z'
    data:
      edited: false
      editors:
      - rsoods
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/908b038acb6392be6e15338b64899ce9.svg
          fullname: Rohit Sood
          isHf: false
          isPro: false
          name: rsoods
          type: user
        html: '<p>Hello All,</p>

          <p>Has anyone faced this issue, while running the bloom accelerate inference
          script? Is there a prescribed CUDA driver/python/torch setup for running
          the script?</p>

          <p>$ python bloom-inference-scripts/bloom-accelerate-inference.py --name
          bigscience/bloom --batch_size 1 --benchmark</p>

          <p><strong>RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when
          calling <code>cublasLtMatmul( ltHandle, computeDesc.descriptor(), &amp;alpha_val,
          mat1_ptr, Adesc.descriptor(), mat2_ptr, Bdesc.descriptor(), &amp;beta_val,
          result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(), &amp;heuristicResult.algo,
          workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())</code></strong></p>

          <p>I''m trying to run this on a 8x A100-GPU dgx with the following environment
          setup, </p>

          <p>$ pip list<br>Package                  Version</p>

          <hr>

          <p>accelerate               0.15.0<br>certifi                  2022.12.7<br>charset-normalizer       2.1.1<br>filelock                 3.8.2<br>huggingface-hub          0.11.1<br>idna                     3.4<br>numpy                    1.24.0<br>nvidia-cublas-cu11       11.10.3.66<br>nvidia-cuda-nvrtc-cu11   11.7.99<br>nvidia-cuda-runtime-cu11
          11.7.99<br>nvidia-cudnn-cu11        8.5.0.96<br>packaging                22.0<br>pip                      21.2.4<br>psutil                   5.9.4<br>PyYAML                   6.0<br>regex                    2022.10.31<br>requests                 2.28.1<br>setuptools               58.1.0<br>tokenizers               0.13.2<br>torch                    1.13.1<br>tqdm                     4.64.1<br>transformers             4.25.1<br>triton                   1.0.0<br>typing_extensions        4.4.0<br>urllib3                  1.26.13<br>wheel                    0.38.4</p>

          <hr>

          <p> nvidia-smi</p>

          <hr>

          <p>+-----------------------------------------------------------------------------+<br>|
          NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |<br>|-------------------------------+----------------------+----------------------+<br>|
          GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.
          ECC |<br>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute
          M. |<br>|                               |                      |               MIG
          M. |<br>|===============================+======================+======================|<br>|   0  NVIDIA
          A100-SXM...  On   | 00000000:07:00.0 Off |                    0 |<br>| N/A   32C    P0    62W
          / 400W |      3MiB / 81251MiB |      0%      Default |<br>|                               |                      |             Disabled
          |<br>+-------------------------------+----------------------+----------------------+<br>|   1  NVIDIA
          A100-SXM...  On   | 00000000:0F:00.0 Off |                    0 |<br>| N/A   30C    P0    62W
          / 400W |      3MiB / 81251MiB |      0%      Default |<br>|                               |                      |             Disabled
          |<br>+-------------------------------+----------------------+----------------------+<br>|   2  NVIDIA
          A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |<br>| N/A   31C    P0    62W
          / 400W |      3MiB / 81251MiB |      0%      Default |<br>|                               |                      |             Disabled
          |<br>+-------------------------------+----------------------+----------------------+<br>|   3  NVIDIA
          A100-SXM...  On   | 00000000:4E:00.0 Off |                    0 |<br>| N/A   31C    P0    61W
          / 400W |      3MiB / 81251MiB |      0%      Default |<br>|                               |                      |             Disabled
          |<br>+-------------------------------+----------------------+----------------------+<br>|   4  NVIDIA
          A100-SXM...  On   | 00000000:87:00.0 Off |                    0 |<br>| N/A   36C    P0    63W
          / 400W |      3MiB / 81251MiB |      0%      Default |<br>|                               |                      |             Disabled
          |<br>+-------------------------------+----------------------+----------------------+<br>|   5  NVIDIA
          A100-SXM...  On   | 00000000:90:00.0 Off |                    0 |<br>| N/A   35C    P0    63W
          / 400W |      3MiB / 81251MiB |      0%      Default |<br>|                               |                      |             Disabled
          |<br>+-------------------------------+----------------------+----------------------+<br>|   6  NVIDIA
          A100-SXM...  On   | 00000000:B7:00.0 Off |                    0 |<br>| N/A   35C    P0    64W
          / 400W |      3MiB / 81251MiB |      0%      Default |<br>|                               |                      |             Disabled
          |<br>+-------------------------------+----------------------+----------------------+<br>|   7  NVIDIA
          A100-SXM...  On   | 00000000:BD:00.0 Off |                12814 |<br>| N/A   35C    P0    64W
          / 400W |      3MiB / 81251MiB |      0%      Default |<br>|                               |                      |             Disabled
          |<br>+-------------------------------+----------------------+----------------------+</p>

          <p>+-----------------------------------------------------------------------------+<br>|
          Processes:                                                                  |<br>|  GPU   GI   CI        PID   Type   Process
          name                  GPU Memory |<br>|        ID   ID                                                   Usage      |<br>|=============================================================================|<br>|  No
          running processes found                                                 |<br>+-----------------------------------------------------------------------------+</p>

          '
        raw: "Hello All,\r\n\r\nHas anyone faced this issue, while running the bloom\
          \ accelerate inference script? Is there a prescribed CUDA driver/python/torch\
          \ setup for running the script?\r\n\r\n$ python bloom-inference-scripts/bloom-accelerate-inference.py\
          \ --name bigscience/bloom --batch_size 1 --benchmark\r\n\r\n**RuntimeError:\
          \ CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul(\
          \ ltHandle, computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(),\
          \ mat2_ptr, Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(),\
          \ result_ptr, Cdesc.descriptor(), &heuristicResult.algo, workspace.data_ptr(),\
          \ workspaceSize, at::cuda::getCurrentCUDAStream())`**\r\n\r\nI'm trying\
          \ to run this on a 8x A100-GPU dgx with the following environment setup,\
          \ \r\n\r\n$ pip list\r\nPackage                  Version\r\n------------------------\
          \ ----------\r\naccelerate               0.15.0\r\ncertifi             \
          \     2022.12.7\r\ncharset-normalizer       2.1.1\r\nfilelock          \
          \       3.8.2\r\nhuggingface-hub          0.11.1\r\nidna               \
          \      3.4\r\nnumpy                    1.24.0\r\nnvidia-cublas-cu11    \
          \   11.10.3.66\r\nnvidia-cuda-nvrtc-cu11   11.7.99\r\nnvidia-cuda-runtime-cu11\
          \ 11.7.99\r\nnvidia-cudnn-cu11        8.5.0.96\r\npackaging            \
          \    22.0\r\npip                      21.2.4\r\npsutil                 \
          \  5.9.4\r\nPyYAML                   6.0\r\nregex                    2022.10.31\r\
          \nrequests                 2.28.1\r\nsetuptools               58.1.0\r\n\
          tokenizers               0.13.2\r\ntorch                    1.13.1\r\ntqdm\
          \                     4.64.1\r\ntransformers             4.25.1\r\ntriton\
          \                   1.0.0\r\ntyping_extensions        4.4.0\r\nurllib3 \
          \                 1.26.13\r\nwheel                    0.38.4\r\n------------------------\
          \ ----------\r\n nvidia-smi\r\n------------------------ ----------\r\n+-----------------------------------------------------------------------------+\r\
          \n| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4\
          \     |\r\n|-------------------------------+----------------------+----------------------+\r\
          \n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.\
          \ ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util\
          \  Compute M. |\r\n|                               |                   \
          \   |               MIG M. |\r\n|===============================+======================+======================|\r\
          \n|   0  NVIDIA A100-SXM...  On   | 00000000:07:00.0 Off |             \
          \       0 |\r\n| N/A   32C    P0    62W / 400W |      3MiB / 81251MiB |\
          \      0%      Default |\r\n|                               |          \
          \            |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
          \n|   1  NVIDIA A100-SXM...  On   | 00000000:0F:00.0 Off |             \
          \       0 |\r\n| N/A   30C    P0    62W / 400W |      3MiB / 81251MiB |\
          \      0%      Default |\r\n|                               |          \
          \            |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
          \n|   2  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |             \
          \       0 |\r\n| N/A   31C    P0    62W / 400W |      3MiB / 81251MiB |\
          \      0%      Default |\r\n|                               |          \
          \            |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
          \n|   3  NVIDIA A100-SXM...  On   | 00000000:4E:00.0 Off |             \
          \       0 |\r\n| N/A   31C    P0    61W / 400W |      3MiB / 81251MiB |\
          \      0%      Default |\r\n|                               |          \
          \            |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
          \n|   4  NVIDIA A100-SXM...  On   | 00000000:87:00.0 Off |             \
          \       0 |\r\n| N/A   36C    P0    63W / 400W |      3MiB / 81251MiB |\
          \      0%      Default |\r\n|                               |          \
          \            |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
          \n|   5  NVIDIA A100-SXM...  On   | 00000000:90:00.0 Off |             \
          \       0 |\r\n| N/A   35C    P0    63W / 400W |      3MiB / 81251MiB |\
          \      0%      Default |\r\n|                               |          \
          \            |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
          \n|   6  NVIDIA A100-SXM...  On   | 00000000:B7:00.0 Off |             \
          \       0 |\r\n| N/A   35C    P0    64W / 400W |      3MiB / 81251MiB |\
          \      0%      Default |\r\n|                               |          \
          \            |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
          \n|   7  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |             \
          \   12814 |\r\n| N/A   35C    P0    64W / 400W |      3MiB / 81251MiB |\
          \      0%      Default |\r\n|                               |          \
          \            |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
          \n\r\n+-----------------------------------------------------------------------------+\r\
          \n| Processes:                                                         \
          \         |\r\n|  GPU   GI   CI        PID   Type   Process name       \
          \           GPU Memory |\r\n|        ID   ID                           \
          \                        Usage      |\r\n|=============================================================================|\r\
          \n|  No running processes found                                        \
          \         |\r\n+-----------------------------------------------------------------------------+\r\
          \n\r\n"
        updatedAt: '2022-12-19T20:27:01.467Z'
      numEdits: 0
      reactions: []
    id: 63a0c9153c8841cfe2cdf760
    type: comment
  author: rsoods
  content: "Hello All,\r\n\r\nHas anyone faced this issue, while running the bloom\
    \ accelerate inference script? Is there a prescribed CUDA driver/python/torch\
    \ setup for running the script?\r\n\r\n$ python bloom-inference-scripts/bloom-accelerate-inference.py\
    \ --name bigscience/bloom --batch_size 1 --benchmark\r\n\r\n**RuntimeError: CUDA\
    \ error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul( ltHandle,\
    \ computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(), mat2_ptr,\
    \ Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(),\
    \ &heuristicResult.algo, workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())`**\r\
    \n\r\nI'm trying to run this on a 8x A100-GPU dgx with the following environment\
    \ setup, \r\n\r\n$ pip list\r\nPackage                  Version\r\n------------------------\
    \ ----------\r\naccelerate               0.15.0\r\ncertifi                  2022.12.7\r\
    \ncharset-normalizer       2.1.1\r\nfilelock                 3.8.2\r\nhuggingface-hub\
    \          0.11.1\r\nidna                     3.4\r\nnumpy                   \
    \ 1.24.0\r\nnvidia-cublas-cu11       11.10.3.66\r\nnvidia-cuda-nvrtc-cu11   11.7.99\r\
    \nnvidia-cuda-runtime-cu11 11.7.99\r\nnvidia-cudnn-cu11        8.5.0.96\r\npackaging\
    \                22.0\r\npip                      21.2.4\r\npsutil           \
    \        5.9.4\r\nPyYAML                   6.0\r\nregex                    2022.10.31\r\
    \nrequests                 2.28.1\r\nsetuptools               58.1.0\r\ntokenizers\
    \               0.13.2\r\ntorch                    1.13.1\r\ntqdm            \
    \         4.64.1\r\ntransformers             4.25.1\r\ntriton                \
    \   1.0.0\r\ntyping_extensions        4.4.0\r\nurllib3                  1.26.13\r\
    \nwheel                    0.38.4\r\n------------------------ ----------\r\n nvidia-smi\r\
    \n------------------------ ----------\r\n+-----------------------------------------------------------------------------+\r\
    \n| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4  \
    \   |\r\n|-------------------------------+----------------------+----------------------+\r\
    \n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC\
    \ |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute\
    \ M. |\r\n|                               |                      |           \
    \    MIG M. |\r\n|===============================+======================+======================|\r\
    \n|   0  NVIDIA A100-SXM...  On   | 00000000:07:00.0 Off |                   \
    \ 0 |\r\n| N/A   32C    P0    62W / 400W |      3MiB / 81251MiB |      0%    \
    \  Default |\r\n|                               |                      |     \
    \        Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
    \n|   1  NVIDIA A100-SXM...  On   | 00000000:0F:00.0 Off |                   \
    \ 0 |\r\n| N/A   30C    P0    62W / 400W |      3MiB / 81251MiB |      0%    \
    \  Default |\r\n|                               |                      |     \
    \        Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
    \n|   2  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                   \
    \ 0 |\r\n| N/A   31C    P0    62W / 400W |      3MiB / 81251MiB |      0%    \
    \  Default |\r\n|                               |                      |     \
    \        Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
    \n|   3  NVIDIA A100-SXM...  On   | 00000000:4E:00.0 Off |                   \
    \ 0 |\r\n| N/A   31C    P0    61W / 400W |      3MiB / 81251MiB |      0%    \
    \  Default |\r\n|                               |                      |     \
    \        Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
    \n|   4  NVIDIA A100-SXM...  On   | 00000000:87:00.0 Off |                   \
    \ 0 |\r\n| N/A   36C    P0    63W / 400W |      3MiB / 81251MiB |      0%    \
    \  Default |\r\n|                               |                      |     \
    \        Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
    \n|   5  NVIDIA A100-SXM...  On   | 00000000:90:00.0 Off |                   \
    \ 0 |\r\n| N/A   35C    P0    63W / 400W |      3MiB / 81251MiB |      0%    \
    \  Default |\r\n|                               |                      |     \
    \        Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
    \n|   6  NVIDIA A100-SXM...  On   | 00000000:B7:00.0 Off |                   \
    \ 0 |\r\n| N/A   35C    P0    64W / 400W |      3MiB / 81251MiB |      0%    \
    \  Default |\r\n|                               |                      |     \
    \        Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\
    \n|   7  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |                12814\
    \ |\r\n| N/A   35C    P0    64W / 400W |      3MiB / 81251MiB |      0%      Default\
    \ |\r\n|                               |                      |             Disabled\
    \ |\r\n+-------------------------------+----------------------+----------------------+\r\
    \n\r\n+-----------------------------------------------------------------------------+\r\
    \n| Processes:                                                               \
    \   |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU\
    \ Memory |\r\n|        ID   ID                                               \
    \    Usage      |\r\n|=============================================================================|\r\
    \n|  No running processes found                                              \
    \   |\r\n+-----------------------------------------------------------------------------+\r\
    \n\r\n"
  created_at: 2022-12-19 20:27:01+00:00
  edited: false
  hidden: false
  id: 63a0c9153c8841cfe2cdf760
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 160
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: 'CUDA error while running bloom-accelerate-inference.py  | RuntimeError: CUDA
  error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul'
