!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Brendan
conflicting_files: null
created_at: 2022-08-18 21:16:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1635211360068-614a8b805ff360e089b13935.jpeg?w=200&h=200&f=face
      fullname: Brendan King
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Brendan
      type: user
    createdAt: '2022-08-18T22:16:27.000Z'
    data:
      edited: false
      editors:
      - Brendan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1635211360068-614a8b805ff360e089b13935.jpeg?w=200&h=200&f=face
          fullname: Brendan King
          isHf: false
          isPro: false
          name: Brendan
          type: user
        html: '<p>I was interested in getting log-probabilities per token (including
          both prompt and completion tokens), similar to the output given the <code>logprobs</code>
          argument to the OpenAI models. Is this available in the API at all, or could
          it be?</p>

          '
        raw: I was interested in getting log-probabilities per token (including both
          prompt and completion tokens), similar to the output given the `logprobs`
          argument to the OpenAI models. Is this available in the API at all, or could
          it be?
        updatedAt: '2022-08-18T22:16:27.120Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - sleven
        - Tristan
    id: 62feba3bcc803924ee0a348a
    type: comment
  author: Brendan
  content: I was interested in getting log-probabilities per token (including both
    prompt and completion tokens), similar to the output given the `logprobs` argument
    to the OpenAI models. Is this available in the API at all, or could it be?
  created_at: 2022-08-18 21:16:27+00:00
  edited: false
  hidden: false
  id: 62feba3bcc803924ee0a348a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2022-08-23T11:59:31.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Narsil\">@<span class=\"\
          underline\">Narsil</span></a></span>\n\n\t</span></span></p>\n"
        raw: cc @Narsil
        updatedAt: '2022-08-23T11:59:31.329Z'
      numEdits: 0
      reactions: []
    id: 6304c123c2f4f2d4929b3ad8
    type: comment
  author: julien-c
  content: cc @Narsil
  created_at: 2022-08-23 10:59:31+00:00
  edited: false
  hidden: false
  id: 6304c123c2f4f2d4929b3ad8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661270230577-6303aac4a362e7e8b51a8e9f.jpeg?w=200&h=200&f=face
      fullname: Alexander Ljungberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sleven
      type: user
    createdAt: '2022-08-23T16:27:01.000Z'
    data:
      edited: false
      editors:
      - sleven
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661270230577-6303aac4a362e7e8b51a8e9f.jpeg?w=200&h=200&f=face
          fullname: Alexander Ljungberg
          isHf: false
          isPro: false
          name: sleven
          type: user
        html: '<p>+1 for this. To emulate the "best of" option available with GPT-3,
          it would be great to be able to specify different seeds and get the total
          likelihood for each result. Then one could just take the best of 20 requests.</p>

          <p>At a glance, <code>num_return_sequences</code> seems like it might be
          helpful, but in my testing I always get just a single result when calling
          the API even while using this option. Either way, without the probabilities
          it wouldn''t be much use anyhow.</p>

          '
        raw: '+1 for this. To emulate the "best of" option available with GPT-3, it
          would be great to be able to specify different seeds and get the total likelihood
          for each result. Then one could just take the best of 20 requests.


          At a glance, `num_return_sequences` seems like it might be helpful, but
          in my testing I always get just a single result when calling the API even
          while using this option. Either way, without the probabilities it wouldn''t
          be much use anyhow.'
        updatedAt: '2022-08-23T16:27:01.940Z'
      numEdits: 0
      reactions: []
    id: 6304ffd5dae2eb7d0843299b
    type: comment
  author: sleven
  content: '+1 for this. To emulate the "best of" option available with GPT-3, it
    would be great to be able to specify different seeds and get the total likelihood
    for each result. Then one could just take the best of 20 requests.


    At a glance, `num_return_sequences` seems like it might be helpful, but in my
    testing I always get just a single result when calling the API even while using
    this option. Either way, without the probabilities it wouldn''t be much use anyhow.'
  created_at: 2022-08-23 15:27:01+00:00
  edited: false
  hidden: false
  id: 6304ffd5dae2eb7d0843299b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2022-08-29T16:41:47.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sleven&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sleven\">@<span class=\"\
          underline\">sleven</span></a></span>\n\n\t</span></span> what do you mean\
          \ <code>best of</code> option ? Isn't what you are describing being exactly\
          \ what <code>num_beams</code> would be doing (using beam search do find\
          \ the best generation among <code>n</code> ?)</p>\n<p>Looking at their docs\
          \ it's exactly what <code>num_beams</code> does in <code>transformers</code>.\
          \ <a rel=\"nofollow\" href=\"https://beta.openai.com/docs/api-reference/completions/create#completions/create-best_of\"\
          >https://beta.openai.com/docs/api-reference/completions/create#completions/create-best_of</a></p>\n\
          <p>Is that what you are referring to ?</p>\n<p>As for the logprobs, it's\
          \ not yet available for bloom, but we could enable part of it at some point.</p>\n\
          <p>Are there any other parameters from gpt-3 that would be important ?</p>\n"
        raw: '@sleven what do you mean `best of` option ? Isn''t what you are describing
          being exactly what `num_beams` would be doing (using beam search do find
          the best generation among `n` ?)


          Looking at their docs it''s exactly what `num_beams` does in `transformers`.
          https://beta.openai.com/docs/api-reference/completions/create#completions/create-best_of


          Is that what you are referring to ?


          As for the logprobs, it''s not yet available for bloom, but we could enable
          part of it at some point.


          Are there any other parameters from gpt-3 that would be important ?'
        updatedAt: '2022-08-29T16:41:47.251Z'
      numEdits: 0
      reactions: []
    id: 630cec4b3dc31beba6ee03db
    type: comment
  author: Narsil
  content: '@sleven what do you mean `best of` option ? Isn''t what you are describing
    being exactly what `num_beams` would be doing (using beam search do find the best
    generation among `n` ?)


    Looking at their docs it''s exactly what `num_beams` does in `transformers`. https://beta.openai.com/docs/api-reference/completions/create#completions/create-best_of


    Is that what you are referring to ?


    As for the logprobs, it''s not yet available for bloom, but we could enable part
    of it at some point.


    Are there any other parameters from gpt-3 that would be important ?'
  created_at: 2022-08-29 15:41:47+00:00
  edited: false
  hidden: false
  id: 630cec4b3dc31beba6ee03db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661270230577-6303aac4a362e7e8b51a8e9f.jpeg?w=200&h=200&f=face
      fullname: Alexander Ljungberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sleven
      type: user
    createdAt: '2022-08-31T11:53:50.000Z'
    data:
      edited: false
      editors:
      - sleven
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661270230577-6303aac4a362e7e8b51a8e9f.jpeg?w=200&h=200&f=face
          fullname: Alexander Ljungberg
          isHf: false
          isPro: false
          name: sleven
          type: user
        html: '<p>Actually no, beam search is different from "best of". I mean, I
          might be wrong, I don''t have GPT-3''s source code, but I think their "best
          of" feature is the "sample and rank" method from Google''s Meena paper (Adiwardana
          et al).</p>

          <p>Sample and rank generates N fully independent responses using plain random
          sampling with a temperature, then selects the response with the highest
          total log likelihood divided by length ("length-normalized log-likelihood
          scores").</p>

          <p>Beam search is at its core still a greedy algorithm that continuously
          extends a set of B "leading" hypothesis at each step. Beam search is more
          efficient since it throws away "losers" early, but it tends to suffer from
          repetitive and uninteresting results since it can''t go very far down a
          path with an unlikely (but perhaps more interesting) token in it.</p>

          <p>Since sample and rank generates full completions it can sample some more
          unusual tokens and still generate a completion with good average likelihood
          by choosing its other tokens well. So you get more diverse outcomes.</p>

          <p>One could implement sample and rank with the API by requesting X completions
          (no beam search, sampling on, some temperature &gt; 0) with different seeds.
          But in addition to the textual response you''d need the likelihood assigned
          to each token in the response so you can calculate the total log likelihood
          for each of the X completions. (Then pick the top 1 or present the top 2
          to the end user for selection or whatever you want.)</p>

          <blockquote>

          <p>Are there any other parameters from gpt-3 that would be important ?</p>

          </blockquote>

          <p><code>stop</code> for specifying stop tokens would be nice. So you could
          ask to generate exactly one line by giving <code>\n</code> as a stop token,
          for example.</p>

          <p>And although I requested <code>logprobs</code> in the context of getting
          the likelihood per generated token (which would correspond to <code>logprobs=1</code>),
          you can also do <code>logprobs=5</code> which gives you the top k candidates
          for each token. <a rel="nofollow" href="http://gptprompts.wikidot.com/intro:logprobs">Example
          here</a> showing that when prompted with "what is the capital of France",
          GPT-3''s most likely candidates are Paris, par, PAR, etc. This can help
          give a sense of how "confident" the model is about a response beyond the
          simple likelihood of the token. E.g. in this example, if we had a way to
          recognise that Paris, par, PAR are all the same idea, we can conclude the
          model is more confident the answer is Paris than just the likelihood of
          the Paris token alone suggests.</p>

          '
        raw: 'Actually no, beam search is different from "best of". I mean, I might
          be wrong, I don''t have GPT-3''s source code, but I think their "best of"
          feature is the "sample and rank" method from Google''s Meena paper (Adiwardana
          et al).


          Sample and rank generates N fully independent responses using plain random
          sampling with a temperature, then selects the response with the highest
          total log likelihood divided by length ("length-normalized log-likelihood
          scores").


          Beam search is at its core still a greedy algorithm that continuously extends
          a set of B "leading" hypothesis at each step. Beam search is more efficient
          since it throws away "losers" early, but it tends to suffer from repetitive
          and uninteresting results since it can''t go very far down a path with an
          unlikely (but perhaps more interesting) token in it.


          Since sample and rank generates full completions it can sample some more
          unusual tokens and still generate a completion with good average likelihood
          by choosing its other tokens well. So you get more diverse outcomes.


          One could implement sample and rank with the API by requesting X completions
          (no beam search, sampling on, some temperature > 0) with different seeds.
          But in addition to the textual response you''d need the likelihood assigned
          to each token in the response so you can calculate the total log likelihood
          for each of the X completions. (Then pick the top 1 or present the top 2
          to the end user for selection or whatever you want.)


          > Are there any other parameters from gpt-3 that would be important ?


          `stop` for specifying stop tokens would be nice. So you could ask to generate
          exactly one line by giving `\n` as a stop token, for example.


          And although I requested `logprobs` in the context of getting the likelihood
          per generated token (which would correspond to `logprobs=1`), you can also
          do `logprobs=5` which gives you the top k candidates for each token. [Example
          here](http://gptprompts.wikidot.com/intro:logprobs) showing that when prompted
          with "what is the capital of France", GPT-3''s most likely candidates are
          Paris, par, PAR, etc. This can help give a sense of how "confident" the
          model is about a response beyond the simple likelihood of the token. E.g.
          in this example, if we had a way to recognise that Paris, par, PAR are all
          the same idea, we can conclude the model is more confident the answer is
          Paris than just the likelihood of the Paris token alone suggests.'
        updatedAt: '2022-08-31T11:53:50.386Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - pootow
        - Brendan
        - Pwicke
        - Narsil
    id: 630f4bce4dbbbb6f667f634e
    type: comment
  author: sleven
  content: 'Actually no, beam search is different from "best of". I mean, I might
    be wrong, I don''t have GPT-3''s source code, but I think their "best of" feature
    is the "sample and rank" method from Google''s Meena paper (Adiwardana et al).


    Sample and rank generates N fully independent responses using plain random sampling
    with a temperature, then selects the response with the highest total log likelihood
    divided by length ("length-normalized log-likelihood scores").


    Beam search is at its core still a greedy algorithm that continuously extends
    a set of B "leading" hypothesis at each step. Beam search is more efficient since
    it throws away "losers" early, but it tends to suffer from repetitive and uninteresting
    results since it can''t go very far down a path with an unlikely (but perhaps
    more interesting) token in it.


    Since sample and rank generates full completions it can sample some more unusual
    tokens and still generate a completion with good average likelihood by choosing
    its other tokens well. So you get more diverse outcomes.


    One could implement sample and rank with the API by requesting X completions (no
    beam search, sampling on, some temperature > 0) with different seeds. But in addition
    to the textual response you''d need the likelihood assigned to each token in the
    response so you can calculate the total log likelihood for each of the X completions.
    (Then pick the top 1 or present the top 2 to the end user for selection or whatever
    you want.)


    > Are there any other parameters from gpt-3 that would be important ?


    `stop` for specifying stop tokens would be nice. So you could ask to generate
    exactly one line by giving `\n` as a stop token, for example.


    And although I requested `logprobs` in the context of getting the likelihood per
    generated token (which would correspond to `logprobs=1`), you can also do `logprobs=5`
    which gives you the top k candidates for each token. [Example here](http://gptprompts.wikidot.com/intro:logprobs)
    showing that when prompted with "what is the capital of France", GPT-3''s most
    likely candidates are Paris, par, PAR, etc. This can help give a sense of how
    "confident" the model is about a response beyond the simple likelihood of the
    token. E.g. in this example, if we had a way to recognise that Paris, par, PAR
    are all the same idea, we can conclude the model is more confident the answer
    is Paris than just the likelihood of the Paris token alone suggests.'
  created_at: 2022-08-31 10:53:50+00:00
  edited: false
  hidden: false
  id: 630f4bce4dbbbb6f667f634e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662715293681-619e38930b33fae198f1ec41.png?w=200&h=200&f=face
      fullname: Phil Wicke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pwicke
      type: user
    createdAt: '2022-09-09T09:19:30.000Z'
    data:
      edited: true
      editors:
      - Pwicke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662715293681-619e38930b33fae198f1ec41.png?w=200&h=200&f=face
          fullname: Phil Wicke
          isHf: false
          isPro: false
          name: Pwicke
          type: user
        html: '<p>Sorry to hijack this post for a related question. I''m also very
          interested to access the <code>logprobs</code> from BLOOM. I''m not an expert,
          but I''ve read that the <a href="https://huggingface.co/docs/transformers/v4.21.3/en/model_doc/bloom#transformers.BloomForCausalLM"><code>BloomForCausalLM</code></a>
          has the <code>logits</code> return values, which should be the prediction
          scores of the language modeling head (scores for each vocabulary token before
          SoftMax). This is the first time working in-depth with these models, so
          maybe I''m completely on the wrong track and "prediction scores of the language
          modeling head" are not "likelihood per generated token".</p>

          '
        raw: Sorry to hijack this post for a related question. I'm also very interested
          to access the ```logprobs``` from BLOOM. I'm not an expert, but I've read
          that the [```BloomForCausalLM```](https://huggingface.co/docs/transformers/v4.21.3/en/model_doc/bloom#transformers.BloomForCausalLM)
          has the ```logits``` return values, which should be the prediction scores
          of the language modeling head (scores for each vocabulary token before SoftMax).
          This is the first time working in-depth with these models, so maybe I'm
          completely on the wrong track and "prediction scores of the language modeling
          head" are not "likelihood per generated token".
        updatedAt: '2022-09-09T09:19:53.332Z'
      numEdits: 1
      reactions: []
    id: 631b0522bf1351ed2bcb6e93
    type: comment
  author: Pwicke
  content: Sorry to hijack this post for a related question. I'm also very interested
    to access the ```logprobs``` from BLOOM. I'm not an expert, but I've read that
    the [```BloomForCausalLM```](https://huggingface.co/docs/transformers/v4.21.3/en/model_doc/bloom#transformers.BloomForCausalLM)
    has the ```logits``` return values, which should be the prediction scores of the
    language modeling head (scores for each vocabulary token before SoftMax). This
    is the first time working in-depth with these models, so maybe I'm completely
    on the wrong track and "prediction scores of the language modeling head" are not
    "likelihood per generated token".
  created_at: 2022-09-09 08:19:30+00:00
  edited: true
  hidden: false
  id: 631b0522bf1351ed2bcb6e93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662715293681-619e38930b33fae198f1ec41.png?w=200&h=200&f=face
      fullname: Phil Wicke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pwicke
      type: user
    createdAt: '2022-09-09T11:53:05.000Z'
    data:
      edited: true
      editors:
      - Pwicke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662715293681-619e38930b33fae198f1ec41.png?w=200&h=200&f=face
          fullname: Phil Wicke
          isHf: false
          isPro: false
          name: Pwicke
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1662724353576-619e38930b33fae198f1ec41.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1662724353576-619e38930b33fae198f1ec41.png"></a><br>From
          "Natural Language Processing with Transformers" By Lewis Tunstall, Leandro
          von Werra, Thomas Wolf. I''m currently trying to implement this.</p>

          <p><a rel="nofollow" href="https://github.com/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb">https://github.com/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb</a></p>

          '
        raw: '![image.png](https://cdn-uploads.huggingface.co/production/uploads/1662724353576-619e38930b33fae198f1ec41.png)

          From "Natural Language Processing with Transformers" By Lewis Tunstall,
          Leandro von Werra, Thomas Wolf. I''m currently trying to implement this.


          https://github.com/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb'
        updatedAt: '2022-09-09T11:55:00.427Z'
      numEdits: 1
      reactions: []
    id: 631b29219edb6d320a09edbc
    type: comment
  author: Pwicke
  content: '![image.png](https://cdn-uploads.huggingface.co/production/uploads/1662724353576-619e38930b33fae198f1ec41.png)

    From "Natural Language Processing with Transformers" By Lewis Tunstall, Leandro
    von Werra, Thomas Wolf. I''m currently trying to implement this.


    https://github.com/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb'
  created_at: 2022-09-09 10:53:05+00:00
  edited: true
  hidden: false
  id: 631b29219edb6d320a09edbc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662715293681-619e38930b33fae198f1ec41.png?w=200&h=200&f=face
      fullname: Phil Wicke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pwicke
      type: user
    createdAt: '2022-09-09T17:04:16.000Z'
    data:
      edited: true
      editors:
      - julien-c
      - Pwicke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p>This has become an awkward monologue. Anyways, I'm suggesting the\
          \ following:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> BloomTokenizerFast, BloomForCausalLM\n\
          <span class=\"hljs-keyword\">import</span> torch.nn.functional <span class=\"\
          hljs-keyword\">as</span> F\n\ntokenizer = BloomTokenizerFast.from_pretrained(<span\
          \ class=\"hljs-string\">\"bigscience/bloom-560m\"</span>)\nmodel = BloomForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"bigscience/bloom-560m\"</span>)\n\nprompt = prompt\
          \ = <span class=\"hljs-string\">\"The horse raced past the barn fell.\"\
          </span>\nencoded = tokenizer(prompt, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>).to(<span class=\"hljs-string\">\"cpu\"</span>)\ninput_ids\
          \ = encoded[<span class=\"hljs-string\">\"input_ids\"</span>]\noutput =\
          \ model(input_ids=input_ids)\n\n<span class=\"hljs-comment\"># neglecting\
          \ the first token, since we make no prediction about it</span>\nshift_labels\
          \ = input_ids[..., <span class=\"hljs-number\">1</span>:].contiguous()\n\
          shift_logits = output.logits[..., :-<span class=\"hljs-number\">1</span>,\
          \ :].contiguous()\n\n<span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">\"TOKEN : LOGPROB\\n\"</span>)\n<span class=\"hljs-built_in\"\
          >print</span>(tokenizer.decode(input_ids[<span class=\"hljs-number\">0</span>].tolist()[<span\
          \ class=\"hljs-number\">0</span>]),<span class=\"hljs-string\">\" : \"</span>,\
          \ <span class=\"hljs-literal\">None</span>)\n<span class=\"hljs-keyword\"\
          >for</span> label_id,logit <span class=\"hljs-keyword\">in</span> <span\
          \ class=\"hljs-built_in\">zip</span>(shift_labels[<span class=\"hljs-number\"\
          >0</span>].tolist(), shift_logits[<span class=\"hljs-number\">0</span>]):\n\
          \  logprob = F.log_softmax(logit, dim=<span class=\"hljs-number\">0</span>).tolist()[label_id]\n\
          \  <span class=\"hljs-built_in\">print</span>(tokenizer.decode(label_id),<span\
          \ class=\"hljs-string\">\" : \"</span>, logprob)\n</code></pre>\n<p>This\
          \ results in:</p>\n<pre><code>TOKEN : LOGPROB\n\nThe  :  None\n horse  :\
          \  -10.14762020111084\n rac  :  -8.096358299255371\ned  :  -0.07634077966213226\n\
          \ past  :  -3.50999116897583\n the  :  -1.575127363204956\n barn  :  -4.399406433105469\n\
          \ fell  :  -10.955772399902344\n.  :  -4.558294296264648\n</code></pre>\n\
          <p>Please let me know if that makes sense. If it does, I'll write up a function\
          \ for <span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Narsil\">@<span class=\"\
          underline\">Narsil</span></a></span>\n\n\t</span></span></p>\n"
        raw: "This has become an awkward monologue. Anyways, I'm suggesting the following:\n\
          \n```python\nimport torch\nfrom transformers import BloomTokenizerFast,\
          \ BloomForCausalLM\nimport torch.nn.functional as F\n\ntokenizer = BloomTokenizerFast.from_pretrained(\"\
          bigscience/bloom-560m\")\nmodel = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\"\
          )\n\nprompt = prompt = \"The horse raced past the barn fell.\"\nencoded\
          \ = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\ninput_ids = encoded[\"\
          input_ids\"]\noutput = model(input_ids=input_ids)\n\n# neglecting the first\
          \ token, since we make no prediction about it\nshift_labels = input_ids[...,\
          \ 1:].contiguous()\nshift_logits = output.logits[..., :-1, :].contiguous()\n\
          \nprint(\"TOKEN : LOGPROB\\n\")\nprint(tokenizer.decode(input_ids[0].tolist()[0]),\"\
          \ : \", None)\nfor label_id,logit in zip(shift_labels[0].tolist(), shift_logits[0]):\n\
          \  logprob = F.log_softmax(logit, dim=0).tolist()[label_id]\n  print(tokenizer.decode(label_id),\"\
          \ : \", logprob)\n```\nThis results in:\n```\nTOKEN : LOGPROB\n\nThe  :\
          \  None\n horse  :  -10.14762020111084\n rac  :  -8.096358299255371\ned\
          \  :  -0.07634077966213226\n past  :  -3.50999116897583\n the  :  -1.575127363204956\n\
          \ barn  :  -4.399406433105469\n fell  :  -10.955772399902344\n.  :  -4.558294296264648\n\
          ```\nPlease let me know if that makes sense. If it does, I'll write up a\
          \ function for @Narsil"
        updatedAt: '2022-09-15T05:00:24.325Z'
      numEdits: 1
      reactions: []
    id: 631b72104aa47d6afbded14c
    type: comment
  author: Pwicke
  content: "This has become an awkward monologue. Anyways, I'm suggesting the following:\n\
    \n```python\nimport torch\nfrom transformers import BloomTokenizerFast, BloomForCausalLM\n\
    import torch.nn.functional as F\n\ntokenizer = BloomTokenizerFast.from_pretrained(\"\
    bigscience/bloom-560m\")\nmodel = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\"\
    )\n\nprompt = prompt = \"The horse raced past the barn fell.\"\nencoded = tokenizer(prompt,\
    \ return_tensors=\"pt\").to(\"cpu\")\ninput_ids = encoded[\"input_ids\"]\noutput\
    \ = model(input_ids=input_ids)\n\n# neglecting the first token, since we make\
    \ no prediction about it\nshift_labels = input_ids[..., 1:].contiguous()\nshift_logits\
    \ = output.logits[..., :-1, :].contiguous()\n\nprint(\"TOKEN : LOGPROB\\n\")\n\
    print(tokenizer.decode(input_ids[0].tolist()[0]),\" : \", None)\nfor label_id,logit\
    \ in zip(shift_labels[0].tolist(), shift_logits[0]):\n  logprob = F.log_softmax(logit,\
    \ dim=0).tolist()[label_id]\n  print(tokenizer.decode(label_id),\" : \", logprob)\n\
    ```\nThis results in:\n```\nTOKEN : LOGPROB\n\nThe  :  None\n horse  :  -10.14762020111084\n\
    \ rac  :  -8.096358299255371\ned  :  -0.07634077966213226\n past  :  -3.50999116897583\n\
    \ the  :  -1.575127363204956\n barn  :  -4.399406433105469\n fell  :  -10.955772399902344\n\
    .  :  -4.558294296264648\n```\nPlease let me know if that makes sense. If it does,\
    \ I'll write up a function for @Narsil"
  created_at: 2022-09-09 16:04:16+00:00
  edited: true
  hidden: false
  id: 631b72104aa47d6afbded14c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2022-09-14T11:01:24.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<p>Thank you very much for the well crafted answer !<br>Really extremely
          helpful to get me up to speed as to what OpenAI does.</p>

          <p>I don''t think there is support for <code>BestOf</code> within <code>transformers</code>
          in the <code>generate</code> function itself (which we hackishly leverage).<br>Any
          PR here would be super welcome.</p>

          <p>And for the logprobs that''s what I had in mind, it''s sort of supported
          but I think we have multiple sort of logprobs to output so we''d have to
          be careful about what how we return them.</p>

          <p>Just to be super straightforward, there''s a priorization for commercial
          offerings over bloom so if that''s something that interests you I think
          sending an email to <code>api-enterprise@huggingface.co</code>is the best
          bet to fast track this. For the free offering of bloom (the widget of this
          page), we''ll probably integrate back some of this at some point, but it''ll
          take more work as we''re more committed to actually changing our libs to
          make things generic usable by everyone (and right now there''s still quite
          a lot to just enable what you''re currently seeing in a nice open source
          form).</p>

          '
        raw: 'Thank you very much for the well crafted answer !

          Really extremely helpful to get me up to speed as to what OpenAI does.


          I don''t think there is support for `BestOf` within `transformers` in the
          `generate` function itself (which we hackishly leverage).

          Any PR here would be super welcome.


          And for the logprobs that''s what I had in mind, it''s sort of supported
          but I think we have multiple sort of logprobs to output so we''d have to
          be careful about what how we return them.


          Just to be super straightforward, there''s a priorization for commercial
          offerings over bloom so if that''s something that interests you I think
          sending an email to `api-enterprise@huggingface.co`is the best bet to fast
          track this. For the free offering of bloom (the widget of this page), we''ll
          probably integrate back some of this at some point, but it''ll take more
          work as we''re more committed to actually changing our libs to make things
          generic usable by everyone (and right now there''s still quite a lot to
          just enable what you''re currently seeing in a nice open source form).'
        updatedAt: '2022-09-14T11:01:24.215Z'
      numEdits: 0
      reactions: []
    id: 6321b48495d6f717a8c118ce
    type: comment
  author: Narsil
  content: 'Thank you very much for the well crafted answer !

    Really extremely helpful to get me up to speed as to what OpenAI does.


    I don''t think there is support for `BestOf` within `transformers` in the `generate`
    function itself (which we hackishly leverage).

    Any PR here would be super welcome.


    And for the logprobs that''s what I had in mind, it''s sort of supported but I
    think we have multiple sort of logprobs to output so we''d have to be careful
    about what how we return them.


    Just to be super straightforward, there''s a priorization for commercial offerings
    over bloom so if that''s something that interests you I think sending an email
    to `api-enterprise@huggingface.co`is the best bet to fast track this. For the
    free offering of bloom (the widget of this page), we''ll probably integrate back
    some of this at some point, but it''ll take more work as we''re more committed
    to actually changing our libs to make things generic usable by everyone (and right
    now there''s still quite a lot to just enable what you''re currently seeing in
    a nice open source form).'
  created_at: 2022-09-14 10:01:24+00:00
  edited: false
  hidden: false
  id: 6321b48495d6f717a8c118ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662715293681-619e38930b33fae198f1ec41.png?w=200&h=200&f=face
      fullname: Phil Wicke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pwicke
      type: user
    createdAt: '2022-09-14T13:53:13.000Z'
    data:
      edited: true
      editors:
      - julien-c
      - Pwicke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p>No problem. Glad to help! I appreciate the open source of your work.\
          \ Is there a simple way I can contribute this? Probably not enough for a\
          \ pull request, but let me know:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >logprobs_from_prompt</span>(<span class=\"hljs-params\">prompt, tokenizer,\
          \ model</span>):\n      encoded = tokenizer(prompt, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>).to(<span class=\"hljs-string\">\"\
          cpu\"</span>)\n      input_ids = encoded[<span class=\"hljs-string\">\"\
          input_ids\"</span>]\n      output = model(input_ids=input_ids)\n      shift_labels\
          \ = input_ids[..., <span class=\"hljs-number\">1</span>:].contiguous()\n\
          \      shift_logits = output.logits[..., :-<span class=\"hljs-number\">1</span>,\
          \ :].contiguous()\n      log_probs = []\n      log_probs.append((tokenizer.decode(input_ids[<span\
          \ class=\"hljs-number\">0</span>].tolist()[<span class=\"hljs-number\">0</span>]),\
          \ <span class=\"hljs-literal\">None</span>))\n      <span class=\"hljs-keyword\"\
          >for</span> idx, (label_id, logit) <span class=\"hljs-keyword\">in</span>\
          \ <span class=\"hljs-built_in\">enumerate</span>(<span class=\"hljs-built_in\"\
          >zip</span>(shift_labels[<span class=\"hljs-number\">0</span>].tolist(),\
          \ shift_logits[<span class=\"hljs-number\">0</span>])):\n            logprob\
          \ = F.log_softmax(logit, dim=<span class=\"hljs-number\">0</span>).tolist()[label_id]\n\
          \            log_probs.append((tokenizer.decode(label_id), <span class=\"\
          hljs-built_in\">float</span>(logprob)))\n      <span class=\"hljs-keyword\"\
          >return</span> log_probs\n</code></pre>\n"
        raw: "No problem. Glad to help! I appreciate the open source of your work.\
          \ Is there a simple way I can contribute this? Probably not enough for a\
          \ pull request, but let me know:\n\n```python\ndef logprobs_from_prompt(prompt,\
          \ tokenizer, model):\n      encoded = tokenizer(prompt, return_tensors=\"\
          pt\").to(\"cpu\")\n      input_ids = encoded[\"input_ids\"]\n      output\
          \ = model(input_ids=input_ids)\n      shift_labels = input_ids[..., 1:].contiguous()\n\
          \      shift_logits = output.logits[..., :-1, :].contiguous()\n      log_probs\
          \ = []\n      log_probs.append((tokenizer.decode(input_ids[0].tolist()[0]),\
          \ None))\n      for idx, (label_id, logit) in enumerate(zip(shift_labels[0].tolist(),\
          \ shift_logits[0])):\n            logprob = F.log_softmax(logit, dim=0).tolist()[label_id]\n\
          \            log_probs.append((tokenizer.decode(label_id), float(logprob)))\n\
          \      return log_probs\n```"
        updatedAt: '2022-09-15T05:00:14.121Z'
      numEdits: 1
      reactions: []
    id: 6321dcc9b97c618f9a5e3dac
    type: comment
  author: Pwicke
  content: "No problem. Glad to help! I appreciate the open source of your work. Is\
    \ there a simple way I can contribute this? Probably not enough for a pull request,\
    \ but let me know:\n\n```python\ndef logprobs_from_prompt(prompt, tokenizer, model):\n\
    \      encoded = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n      input_ids\
    \ = encoded[\"input_ids\"]\n      output = model(input_ids=input_ids)\n      shift_labels\
    \ = input_ids[..., 1:].contiguous()\n      shift_logits = output.logits[..., :-1,\
    \ :].contiguous()\n      log_probs = []\n      log_probs.append((tokenizer.decode(input_ids[0].tolist()[0]),\
    \ None))\n      for idx, (label_id, logit) in enumerate(zip(shift_labels[0].tolist(),\
    \ shift_logits[0])):\n            logprob = F.log_softmax(logit, dim=0).tolist()[label_id]\n\
    \            log_probs.append((tokenizer.decode(label_id), float(logprob)))\n\
    \      return log_probs\n```"
  created_at: 2022-09-14 12:53:13+00:00
  edited: true
  hidden: false
  id: 6321dcc9b97c618f9a5e3dac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661270230577-6303aac4a362e7e8b51a8e9f.jpeg?w=200&h=200&f=face
      fullname: Alexander Ljungberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sleven
      type: user
    createdAt: '2022-09-14T20:49:45.000Z'
    data:
      edited: true
      editors:
      - sleven
      - julien-c
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661270230577-6303aac4a362e7e8b51a8e9f.jpeg?w=200&h=200&f=face
          fullname: Alexander Ljungberg
          isHf: false
          isPro: false
          name: sleven
          type: user
        html: "<p>Nice work <span data-props=\"{&quot;user&quot;:&quot;Pwicke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Pwicke\"\
          >@<span class=\"underline\">Pwicke</span></a></span>\n\n\t</span></span>.\
          \ The logic looks sound to me. Possibly you can avoid that <code>tolist</code>\
          \ which might be expensive when all you want is a single element. That list\
          \ will be like 250880 elements long, one element per possible token, won't\
          \ it? Maybe just <code>F.log_softmax(logit, dim=0)[label_id].item()</code>\
          \ would work.</p>\n<p>A sniff test that things look right is that <code>ed</code>\
          \ has a high probability after <code>rac</code> in your test data, so that\
          \ indicates there's no off by one error or anything like that.</p>\n<p>Just\
          \ for reference this is what the OpenAI API responds with the <code>logprob</code>\
          \ setting, same prompt:</p>\n<pre><code class=\"language-bash\">curl https://api.openai.com/v1/completions\
          \ \\\n  -H <span class=\"hljs-string\">\"Content-Type: application/json\"\
          </span> \\\n  -H <span class=\"hljs-string\">\"Authorization: Bearer <span\
          \ class=\"hljs-variable\">$OPENAI_API_KEY</span>\"</span> \\\n  -d <span\
          \ class=\"hljs-string\">'{</span>\n<span class=\"hljs-string\">  \"model\"\
          : \"text-davinci-002\",</span>\n<span class=\"hljs-string\">  \"prompt\"\
          : \"The horse raced past the barn fell.\",</span>\n<span class=\"hljs-string\"\
          >  \"temperature\": 0.7,</span>\n<span class=\"hljs-string\">  \"max_tokens\"\
          : 0,</span>\n<span class=\"hljs-string\">  \"top_p\": 1,</span>\n<span class=\"\
          hljs-string\">  \"frequency_penalty\": 0,</span>\n<span class=\"hljs-string\"\
          >  \"presence_penalty\": 0, </span>\n<span class=\"hljs-string\">  \"logprobs\"\
          : 1,  </span>\n<span class=\"hljs-string\">  \"echo\": true</span>\n<span\
          \ class=\"hljs-string\">}'</span> | jq .\n</code></pre>\n<pre><code class=\"\
          language-json\"><span class=\"hljs-punctuation\">{</span>\n  <span class=\"\
          hljs-attr\">\"id\"</span><span class=\"hljs-punctuation\">:</span> <span\
          \ class=\"hljs-string\">\"cmpl-5qTJCj3Q8PO8gQvAQbmBMH1TlyAHf\"</span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"object\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\"\
          >\"text_completion\"</span><span class=\"hljs-punctuation\">,</span>\n \
          \ <span class=\"hljs-attr\">\"created\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">1663187402</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"model\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\"text-davinci-002\"</span><span class=\"\
          hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"choices\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n\
          \    <span class=\"hljs-punctuation\">{</span>\n      <span class=\"hljs-attr\"\
          >\"text\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-string\">\"The horse raced past the barn fell.\"</span><span class=\"\
          hljs-punctuation\">,</span>\n      <span class=\"hljs-attr\">\"index\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">0</span><span\
          \ class=\"hljs-punctuation\">,</span>\n      <span class=\"hljs-attr\">\"\
          logprobs\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-punctuation\">{</span>\n        <span class=\"hljs-attr\">\"tokens\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\"\
          >[</span>\n          <span class=\"hljs-string\">\"The\"</span><span class=\"\
          hljs-punctuation\">,</span>\n          <span class=\"hljs-string\">\" horse\"\
          </span><span class=\"hljs-punctuation\">,</span>\n          <span class=\"\
          hljs-string\">\" raced\"</span><span class=\"hljs-punctuation\">,</span>\n\
          \          <span class=\"hljs-string\">\" past\"</span><span class=\"hljs-punctuation\"\
          >,</span>\n          <span class=\"hljs-string\">\" the\"</span><span class=\"\
          hljs-punctuation\">,</span>\n          <span class=\"hljs-string\">\" barn\"\
          </span><span class=\"hljs-punctuation\">,</span>\n          <span class=\"\
          hljs-string\">\" fell\"</span><span class=\"hljs-punctuation\">,</span>\n\
          \          <span class=\"hljs-string\">\".\"</span>\n        <span class=\"\
          hljs-punctuation\">]</span><span class=\"hljs-punctuation\">,</span>\n \
          \       <span class=\"hljs-attr\">\"token_logprobs\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n\
          \          <span class=\"hljs-literal\"><span class=\"hljs-keyword\">null</span></span><span\
          \ class=\"hljs-punctuation\">,</span>\n          <span class=\"hljs-number\"\
          >-9.926095</span><span class=\"hljs-punctuation\">,</span>\n          <span\
          \ class=\"hljs-number\">-5.8342543</span><span class=\"hljs-punctuation\"\
          >,</span>\n          <span class=\"hljs-number\">-0.5715009</span><span\
          \ class=\"hljs-punctuation\">,</span>\n          <span class=\"hljs-number\"\
          >-0.008218852</span><span class=\"hljs-punctuation\">,</span>\n        \
          \  <span class=\"hljs-number\">-0.0009814043</span><span class=\"hljs-punctuation\"\
          >,</span>\n          <span class=\"hljs-number\">-0.3402846</span><span\
          \ class=\"hljs-punctuation\">,</span>\n          <span class=\"hljs-number\"\
          >-1.3558618</span>\n        <span class=\"hljs-punctuation\">]</span><span\
          \ class=\"hljs-punctuation\">,</span>\n        <span class=\"hljs-attr\"\
          >\"top_logprobs\"</span><span class=\"hljs-punctuation\">:</span> <span\
          \ class=\"hljs-punctuation\">[</span>\n          <span class=\"hljs-literal\"\
          ><span class=\"hljs-keyword\">null</span></span><span class=\"hljs-punctuation\"\
          >,</span>\n          <span class=\"hljs-punctuation\">{</span>\n       \
          \     <span class=\"hljs-attr\">\" first\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">-4.1999307</span>\n          <span\
          \ class=\"hljs-punctuation\">}</span><span class=\"hljs-punctuation\">,</span>\n\
          \          <span class=\"hljs-punctuation\">{</span>\n            <span\
          \ class=\"hljs-attr\">\" is\"</span><span class=\"hljs-punctuation\">:</span>\
          \ <span class=\"hljs-number\">-2.169767</span>\n          <span class=\"\
          hljs-punctuation\">}</span><span class=\"hljs-punctuation\">,</span>\n \
          \         <span class=\"hljs-punctuation\">{</span>\n            <span class=\"\
          hljs-attr\">\" past\"</span><span class=\"hljs-punctuation\">:</span> <span\
          \ class=\"hljs-number\">-0.5715009</span>\n          <span class=\"hljs-punctuation\"\
          >}</span><span class=\"hljs-punctuation\">,</span>\n          <span class=\"\
          hljs-punctuation\">{</span>\n            <span class=\"hljs-attr\">\" the\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >-0.008218852</span>\n          <span class=\"hljs-punctuation\">}</span><span\
          \ class=\"hljs-punctuation\">,</span>\n          <span class=\"hljs-punctuation\"\
          >{</span>\n            <span class=\"hljs-attr\">\" barn\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-number\">-0.0009814043</span>\n\
          \          <span class=\"hljs-punctuation\">}</span><span class=\"hljs-punctuation\"\
          >,</span>\n          <span class=\"hljs-punctuation\">{</span>\n       \
          \     <span class=\"hljs-attr\">\" fell\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">-0.3402846</span>\n          <span\
          \ class=\"hljs-punctuation\">}</span><span class=\"hljs-punctuation\">,</span>\n\
          \          <span class=\"hljs-punctuation\">{</span>\n            <span\
          \ class=\"hljs-attr\">\"\\n\"</span><span class=\"hljs-punctuation\">:</span>\
          \ <span class=\"hljs-number\">-0.9338951</span>\n          <span class=\"\
          hljs-punctuation\">}</span>\n        <span class=\"hljs-punctuation\">]</span><span\
          \ class=\"hljs-punctuation\">,</span>\n        <span class=\"hljs-attr\"\
          >\"text_offset\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-punctuation\">[</span>\n          <span class=\"hljs-number\">0</span><span\
          \ class=\"hljs-punctuation\">,</span>\n          <span class=\"hljs-number\"\
          >3</span><span class=\"hljs-punctuation\">,</span>\n          <span class=\"\
          hljs-number\">9</span><span class=\"hljs-punctuation\">,</span>\n      \
          \    <span class=\"hljs-number\">15</span><span class=\"hljs-punctuation\"\
          >,</span>\n          <span class=\"hljs-number\">20</span><span class=\"\
          hljs-punctuation\">,</span>\n          <span class=\"hljs-number\">24</span><span\
          \ class=\"hljs-punctuation\">,</span>\n          <span class=\"hljs-number\"\
          >29</span><span class=\"hljs-punctuation\">,</span>\n          <span class=\"\
          hljs-number\">34</span>\n        <span class=\"hljs-punctuation\">]</span>\n\
          \      <span class=\"hljs-punctuation\">}</span><span class=\"hljs-punctuation\"\
          >,</span>\n      <span class=\"hljs-attr\">\"finish_reason\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"length\"\
          </span>\n    <span class=\"hljs-punctuation\">}</span>\n  <span class=\"\
          hljs-punctuation\">]</span><span class=\"hljs-punctuation\">,</span>\n \
          \ <span class=\"hljs-attr\">\"usage\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-punctuation\">{</span>\n    <span class=\"\
          hljs-attr\">\"prompt_tokens\"</span><span class=\"hljs-punctuation\">:</span>\
          \ <span class=\"hljs-number\">8</span><span class=\"hljs-punctuation\">,</span>\n\
          \    <span class=\"hljs-attr\">\"total_tokens\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">8</span>\n  <span class=\"hljs-punctuation\"\
          >}</span>\n<span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<p>So\
          \ GPT-3's equivalent output values are:</p>\n<pre><code>          The: null,\n\
          \           horse: -9.926095,\n           raced: -5.8342543,\n         \
          \  past: -0.5715009,\n           the: -0.008218852,\n           barn: -0.0009814043,\n\
          \           fell: -0.3402846,\n          .: -1.3558618\n</code></pre>\n\
          <p>I think this serves as excellent validation that <span data-props=\"\
          {&quot;user&quot;:&quot;Pwicke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Pwicke\">@<span class=\"underline\">Pwicke</span></a></span>\n\
          \n\t</span></span>'s method is sound. The scores and tokenization are different\
          \ (GPT is <em>really</em> confident it's a barn), naturally since it's a\
          \ different model, but if you squint they look the same.</p>\n<p>(Note that\
          \ in addition to the actual token used,  openai sends <code>top_logprobs</code>\
          \ for the top n tokens (1 in this case but they allow up to 5). So in our\
          \ case \"horse\" scored -9.926095 but \"first\" was a more probable choice\
          \ with -4.1999307. That could be grabbed with a simple <code>torch.topk</code>\
          \ call on the logprob vector which gives both indices and values.)</p>\n"
        raw: "Nice work @Pwicke. The logic looks sound to me. Possibly you can avoid\
          \ that `tolist` which might be expensive when all you want is a single element.\
          \ That list will be like 250880 elements long, one element per possible\
          \ token, won't it? Maybe just `F.log_softmax(logit, dim=0)[label_id].item()`\
          \ would work.\n\nA sniff test that things look right is that `ed` has a\
          \ high probability after `rac` in your test data, so that indicates there's\
          \ no off by one error or anything like that.\n\nJust for reference this\
          \ is what the OpenAI API responds with the `logprob` setting, same prompt:\n\
          \n```bash\ncurl https://api.openai.com/v1/completions \\\n  -H \"Content-Type:\
          \ application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
          \ \\\n  -d '{\n  \"model\": \"text-davinci-002\",\n  \"prompt\": \"The horse\
          \ raced past the barn fell.\",\n  \"temperature\": 0.7,\n  \"max_tokens\"\
          : 0,\n  \"top_p\": 1,\n  \"frequency_penalty\": 0,\n  \"presence_penalty\"\
          : 0, \n  \"logprobs\": 1,  \n  \"echo\": true\n}' | jq .\n```\n```json\n\
          {\n  \"id\": \"cmpl-5qTJCj3Q8PO8gQvAQbmBMH1TlyAHf\",\n  \"object\": \"text_completion\"\
          ,\n  \"created\": 1663187402,\n  \"model\": \"text-davinci-002\",\n  \"\
          choices\": [\n    {\n      \"text\": \"The horse raced past the barn fell.\"\
          ,\n      \"index\": 0,\n      \"logprobs\": {\n        \"tokens\": [\n \
          \         \"The\",\n          \" horse\",\n          \" raced\",\n     \
          \     \" past\",\n          \" the\",\n          \" barn\",\n          \"\
          \ fell\",\n          \".\"\n        ],\n        \"token_logprobs\": [\n\
          \          null,\n          -9.926095,\n          -5.8342543,\n        \
          \  -0.5715009,\n          -0.008218852,\n          -0.0009814043,\n    \
          \      -0.3402846,\n          -1.3558618\n        ],\n        \"top_logprobs\"\
          : [\n          null,\n          {\n            \" first\": -4.1999307\n\
          \          },\n          {\n            \" is\": -2.169767\n          },\n\
          \          {\n            \" past\": -0.5715009\n          },\n        \
          \  {\n            \" the\": -0.008218852\n          },\n          {\n  \
          \          \" barn\": -0.0009814043\n          },\n          {\n       \
          \     \" fell\": -0.3402846\n          },\n          {\n            \"\\\
          n\": -0.9338951\n          }\n        ],\n        \"text_offset\": [\n \
          \         0,\n          3,\n          9,\n          15,\n          20,\n\
          \          24,\n          29,\n          34\n        ]\n      },\n     \
          \ \"finish_reason\": \"length\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\"\
          : 8,\n    \"total_tokens\": 8\n  }\n}\n```\nSo GPT-3's equivalent output\
          \ values are:\n\n```\n          The: null,\n           horse: -9.926095,\n\
          \           raced: -5.8342543,\n           past: -0.5715009,\n         \
          \  the: -0.008218852,\n           barn: -0.0009814043,\n           fell:\
          \ -0.3402846,\n          .: -1.3558618\n```\n\nI think this serves as excellent\
          \ validation that @Pwicke's method is sound. The scores and tokenization\
          \ are different (GPT is *really* confident it's a barn), naturally since\
          \ it's a different model, but if you squint they look the same.\n\n(Note\
          \ that in addition to the actual token used,  openai sends `top_logprobs`\
          \ for the top n tokens (1 in this case but they allow up to 5). So in our\
          \ case \"horse\" scored -9.926095 but \"first\" was a more probable choice\
          \ with -4.1999307. That could be grabbed with a simple `torch.topk` call\
          \ on the logprob vector which gives both indices and values.)"
        updatedAt: '2022-09-15T06:48:35.865Z'
      numEdits: 2
      reactions: []
    id: 63223e693007fcbf293061cc
    type: comment
  author: sleven
  content: "Nice work @Pwicke. The logic looks sound to me. Possibly you can avoid\
    \ that `tolist` which might be expensive when all you want is a single element.\
    \ That list will be like 250880 elements long, one element per possible token,\
    \ won't it? Maybe just `F.log_softmax(logit, dim=0)[label_id].item()` would work.\n\
    \nA sniff test that things look right is that `ed` has a high probability after\
    \ `rac` in your test data, so that indicates there's no off by one error or anything\
    \ like that.\n\nJust for reference this is what the OpenAI API responds with the\
    \ `logprob` setting, same prompt:\n\n```bash\ncurl https://api.openai.com/v1/completions\
    \ \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer\
    \ $OPENAI_API_KEY\" \\\n  -d '{\n  \"model\": \"text-davinci-002\",\n  \"prompt\"\
    : \"The horse raced past the barn fell.\",\n  \"temperature\": 0.7,\n  \"max_tokens\"\
    : 0,\n  \"top_p\": 1,\n  \"frequency_penalty\": 0,\n  \"presence_penalty\": 0,\
    \ \n  \"logprobs\": 1,  \n  \"echo\": true\n}' | jq .\n```\n```json\n{\n  \"id\"\
    : \"cmpl-5qTJCj3Q8PO8gQvAQbmBMH1TlyAHf\",\n  \"object\": \"text_completion\",\n\
    \  \"created\": 1663187402,\n  \"model\": \"text-davinci-002\",\n  \"choices\"\
    : [\n    {\n      \"text\": \"The horse raced past the barn fell.\",\n      \"\
    index\": 0,\n      \"logprobs\": {\n        \"tokens\": [\n          \"The\",\n\
    \          \" horse\",\n          \" raced\",\n          \" past\",\n        \
    \  \" the\",\n          \" barn\",\n          \" fell\",\n          \".\"\n  \
    \      ],\n        \"token_logprobs\": [\n          null,\n          -9.926095,\n\
    \          -5.8342543,\n          -0.5715009,\n          -0.008218852,\n     \
    \     -0.0009814043,\n          -0.3402846,\n          -1.3558618\n        ],\n\
    \        \"top_logprobs\": [\n          null,\n          {\n            \" first\"\
    : -4.1999307\n          },\n          {\n            \" is\": -2.169767\n    \
    \      },\n          {\n            \" past\": -0.5715009\n          },\n    \
    \      {\n            \" the\": -0.008218852\n          },\n          {\n    \
    \        \" barn\": -0.0009814043\n          },\n          {\n            \" fell\"\
    : -0.3402846\n          },\n          {\n            \"\\n\": -0.9338951\n   \
    \       }\n        ],\n        \"text_offset\": [\n          0,\n          3,\n\
    \          9,\n          15,\n          20,\n          24,\n          29,\n  \
    \        34\n        ]\n      },\n      \"finish_reason\": \"length\"\n    }\n\
    \  ],\n  \"usage\": {\n    \"prompt_tokens\": 8,\n    \"total_tokens\": 8\n  }\n\
    }\n```\nSo GPT-3's equivalent output values are:\n\n```\n          The: null,\n\
    \           horse: -9.926095,\n           raced: -5.8342543,\n           past:\
    \ -0.5715009,\n           the: -0.008218852,\n           barn: -0.0009814043,\n\
    \           fell: -0.3402846,\n          .: -1.3558618\n```\n\nI think this serves\
    \ as excellent validation that @Pwicke's method is sound. The scores and tokenization\
    \ are different (GPT is *really* confident it's a barn), naturally since it's\
    \ a different model, but if you squint they look the same.\n\n(Note that in addition\
    \ to the actual token used,  openai sends `top_logprobs` for the top n tokens\
    \ (1 in this case but they allow up to 5). So in our case \"horse\" scored -9.926095\
    \ but \"first\" was a more probable choice with -4.1999307. That could be grabbed\
    \ with a simple `torch.topk` call on the logprob vector which gives both indices\
    \ and values.)"
  created_at: 2022-09-14 19:49:45+00:00
  edited: true
  hidden: false
  id: 63223e693007fcbf293061cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2022-09-15T05:00:03.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: '<p>(took the liberty of formatting some of the comments in this thread
          to add syntax highlighting)</p>

          '
        raw: (took the liberty of formatting some of the comments in this thread to
          add syntax highlighting)
        updatedAt: '2022-09-15T05:00:03.511Z'
      numEdits: 0
      reactions: []
    id: 6322b153a418a789a23f3380
    type: comment
  author: julien-c
  content: (took the liberty of formatting some of the comments in this thread to
    add syntax highlighting)
  created_at: 2022-09-15 04:00:03+00:00
  edited: false
  hidden: false
  id: 6322b153a418a789a23f3380
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2022-12-16T17:10:47.000Z'
    data:
      edited: true
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;sleven&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sleven\"\
          >@<span class=\"underline\">sleven</span></a></span>\n\n\t</span></span>\
          \ , <span data-props=\"{&quot;user&quot;:&quot;Brendan&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Brendan\">@<span class=\"\
          underline\">Brendan</span></a></span>\n\n\t</span></span>, <span data-props=\"\
          {&quot;user&quot;:&quot;Pwicke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Pwicke\">@<span class=\"underline\">Pwicke</span></a></span>\n\
          \n\t</span></span> ,</p>\n<p>You can now ask for the log probabilities by\
          \ using the <code>details: true</code> parameter.</p>\n<pre><code class=\"\
          language-shell\">curl https://api-inference.huggingface.co/models/bigscience/bloom\
          \  \\\n        -X POST \\\n        -d '{\"inputs\": \"test\", \"parameters\"\
          :{\"details\":true,\"max_new_tokens\":2}}' \n</code></pre>\n<pre><code class=\"\
          language-json\"><span class=\"hljs-punctuation\">[</span>\n   <span class=\"\
          hljs-punctuation\">{</span>\n      <span class=\"hljs-attr\">\"details\"\
          </span> <span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\"\
          >{</span>\n         <span class=\"hljs-attr\">\"finish_reason\"</span> <span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"length\"\
          </span><span class=\"hljs-punctuation\">,</span>\n         <span class=\"\
          hljs-attr\">\"generated_tokens\"</span> <span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">2</span><span class=\"hljs-punctuation\"\
          >,</span>\n         <span class=\"hljs-attr\">\"tokens\"</span> <span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n\
          \            <span class=\"hljs-punctuation\">[</span>\n               <span\
          \ class=\"hljs-number\">9234</span><span class=\"hljs-punctuation\">,</span>\n\
          \               <span class=\"hljs-string\">\"test\"</span><span class=\"\
          hljs-punctuation\">,</span>\n               <span class=\"hljs-literal\"\
          ><span class=\"hljs-keyword\">null</span></span>\n            <span class=\"\
          hljs-punctuation\">]</span><span class=\"hljs-punctuation\">,</span>\n \
          \           <span class=\"hljs-punctuation\">[</span>\n               <span\
          \ class=\"hljs-number\">17</span><span class=\"hljs-punctuation\">,</span>\n\
          \               <span class=\"hljs-string\">\".\"</span><span class=\"hljs-punctuation\"\
          >,</span>\n               <span class=\"hljs-number\">-1.7421875</span>\n\
          \            <span class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\"\
          >,</span>\n            <span class=\"hljs-punctuation\">[</span>\n     \
          \          <span class=\"hljs-number\">16357</span><span class=\"hljs-punctuation\"\
          >,</span>\n               <span class=\"hljs-string\">\"mark\"</span><span\
          \ class=\"hljs-punctuation\">,</span>\n               <span class=\"hljs-number\"\
          >-2.421875</span>\n            <span class=\"hljs-punctuation\">]</span>\n\
          \         <span class=\"hljs-punctuation\">]</span>\n      <span class=\"\
          hljs-punctuation\">}</span><span class=\"hljs-punctuation\">,</span>\n \
          \     <span class=\"hljs-attr\">\"generated_text\"</span> <span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-string\">\"test.mark\"</span>\n\
          \   <span class=\"hljs-punctuation\">}</span>\n<span class=\"hljs-punctuation\"\
          >]</span>\n</code></pre>\n<p>If you face any problem, feel free to comment\
          \ or open an issue <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference/issues\"\
          >here</a>.</p>\n"
        raw: "Hello @sleven , @Brendan, @Pwicke ,\n\nYou can now ask for the log probabilities\
          \ by using the `details: true` parameter.\n\n```shell\ncurl https://api-inference.huggingface.co/models/bigscience/bloom\
          \  \\\n        -X POST \\\n        -d '{\"inputs\": \"test\", \"parameters\"\
          :{\"details\":true,\"max_new_tokens\":2}}' \n```\n```json\n[\n   {\n   \
          \   \"details\" : {\n         \"finish_reason\" : \"length\",\n        \
          \ \"generated_tokens\" : 2,\n         \"tokens\" : [\n            [\n  \
          \             9234,\n               \"test\",\n               null\n   \
          \         ],\n            [\n               17,\n               \".\",\n\
          \               -1.7421875\n            ],\n            [\n            \
          \   16357,\n               \"mark\",\n               -2.421875\n       \
          \     ]\n         ]\n      },\n      \"generated_text\" : \"test.mark\"\n\
          \   }\n]\n```\n\nIf you face any problem, feel free to comment or open an\
          \ issue [here](https://github.com/huggingface/text-generation-inference/issues)."
        updatedAt: '2022-12-16T17:11:09.415Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Narsil
    id: 639ca6979e1c02384ee62f59
    type: comment
  author: olivierdehaene
  content: "Hello @sleven , @Brendan, @Pwicke ,\n\nYou can now ask for the log probabilities\
    \ by using the `details: true` parameter.\n\n```shell\ncurl https://api-inference.huggingface.co/models/bigscience/bloom\
    \  \\\n        -X POST \\\n        -d '{\"inputs\": \"test\", \"parameters\":{\"\
    details\":true,\"max_new_tokens\":2}}' \n```\n```json\n[\n   {\n      \"details\"\
    \ : {\n         \"finish_reason\" : \"length\",\n         \"generated_tokens\"\
    \ : 2,\n         \"tokens\" : [\n            [\n               9234,\n       \
    \        \"test\",\n               null\n            ],\n            [\n     \
    \          17,\n               \".\",\n               -1.7421875\n           \
    \ ],\n            [\n               16357,\n               \"mark\",\n       \
    \        -2.421875\n            ]\n         ]\n      },\n      \"generated_text\"\
    \ : \"test.mark\"\n   }\n]\n```\n\nIf you face any problem, feel free to comment\
    \ or open an issue [here](https://github.com/huggingface/text-generation-inference/issues)."
  created_at: 2022-12-16 17:10:47+00:00
  edited: true
  hidden: false
  id: 639ca6979e1c02384ee62f59
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 89
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Getting log probabilities from the Inference API?
