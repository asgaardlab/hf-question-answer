!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mishavee
conflicting_files: null
created_at: 2022-10-24 03:14:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-10-24T04:14:56.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>here<br><a rel="nofollow" href="https://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai/">https://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai/</a></p>

          <p>it says gpt3<br>GPT-3 is a very large language model (the largest till
          date) with about 175B parameters.<br>It is trained on about 45TB of text
          data from different datasets.</p>

          <p>it says the above at the bottom of the article. </p>

          <p>how much was Bloom trained on? I thought it was 1.6tb but I think I''m
          wrong. </p>

          '
        raw: "here\r\nhttps://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai/\r\
          \n\r\nit says gpt3 \r\nGPT-3 is a very large language model (the largest\
          \ till date) with about 175B parameters.\r\nIt is trained on about 45TB\
          \ of text data from different datasets.\r\n\r\nit says the above at the\
          \ bottom of the article. \r\n\r\nhow much was Bloom trained on? I thought\
          \ it was 1.6tb but I think I'm wrong. "
        updatedAt: '2022-10-24T04:14:56.613Z'
      numEdits: 0
      reactions: []
    id: 63561140a5546ae4c5cd3a19
    type: comment
  author: mishavee
  content: "here\r\nhttps://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai/\r\
    \n\r\nit says gpt3 \r\nGPT-3 is a very large language model (the largest till\
    \ date) with about 175B parameters.\r\nIt is trained on about 45TB of text data\
    \ from different datasets.\r\n\r\nit says the above at the bottom of the article.\
    \ \r\n\r\nhow much was Bloom trained on? I thought it was 1.6tb but I think I'm\
    \ wrong. "
  created_at: 2022-10-24 03:14:56+00:00
  edited: false
  hidden: false
  id: 63561140a5546ae4c5cd3a19
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2022-10-28T12:48:12.000Z'
    data:
      edited: false
      editors:
      - cakiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
          fullname: Christopher Akiki
          isHf: false
          isPro: false
          name: cakiki
          type: user
        html: '<p>BLOOM was indeed trained on 1.61TB of data. </p>

          <p><a href="https://huggingface.co/bigscience/bloom#training-data">https://huggingface.co/bigscience/bloom#training-data</a></p>

          '
        raw: "BLOOM was indeed trained on 1.61TB of data. \n\nhttps://huggingface.co/bigscience/bloom#training-data"
        updatedAt: '2022-10-28T12:48:12.923Z'
      numEdits: 0
      reactions: []
      relatedEventId: 635bcf8c7a1656011518fe91
    id: 635bcf8c7a1656011518fe90
    type: comment
  author: cakiki
  content: "BLOOM was indeed trained on 1.61TB of data. \n\nhttps://huggingface.co/bigscience/bloom#training-data"
  created_at: 2022-10-28 11:48:12+00:00
  edited: false
  hidden: false
  id: 635bcf8c7a1656011518fe90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2022-10-28T12:48:12.000Z'
    data:
      status: closed
    id: 635bcf8c7a1656011518fe91
    type: status-change
  author: cakiki
  created_at: 2022-10-28 11:48:12+00:00
  id: 635bcf8c7a1656011518fe91
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 133
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: 'How much data was Bloom trained on? '
