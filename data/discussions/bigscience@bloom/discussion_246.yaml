!!python/object:huggingface_hub.community.DiscussionWithDetails
author: matthieumeeus
conflicting_files: null
created_at: 2023-05-02 12:33:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/adc33d83559ad182fa3a7d9fbbb84cb8.svg
      fullname: Meeus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matthieumeeus
      type: user
    createdAt: '2023-05-02T13:33:13.000Z'
    data:
      edited: true
      editors:
      - matthieumeeus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/adc33d83559ad182fa3a7d9fbbb84cb8.svg
          fullname: Meeus
          isHf: false
          isPro: false
          name: matthieumeeus
          type: user
        html: "<p>Hi there! </p>\n<p>High level: starting from BLOOM, we want to create\
          \ a LLM for Dutch and look for tips regarding the hyperparameters/training\
          \ paradigm. </p>\n<p>Context: We\u2019re setting up the code and infrastructure\
          \ to fine-tune all BLOOM models on a Dutch corpus. In general, we don\u2019\
          t wish to particularly optimize for multilingual capabilities and aim for\
          \ a SOTA Dutch LLM. We have collected around 150GB of Dutch data, fitted\
          \ a new tokenizer with a vocabulary size of 40,000 tokens and prepared the\
          \ language model training dataset. We have access to a HPC infrastructure,\
          \ which would allow us to train for 50-100k GPU hours and have set up the\
          \ ZeRO framework using DeepSpeed for parallel training. </p>\n<p>Questions:<br>We\
          \ currently consider two training paradigms. Any general or specific piece\
          \ of advice would be much appreciated. </p>\n<ol>\n<li><p>Continued pretraining.\
          \ To our knowledge, this seems to be the easiest way forward. We currently\
          \ anticipate running the training with all weights unfrozen, for 1 epoch\
          \ and 256 as effective batch size. What could you recommend using as hyperparameters?\
          \ In particular:<br>1.1 Would you recommend freezing a certain set of weights?<br>1.2\
          \ Which learning rate (paradigm) would you propose?<br>1.3 How would these\
          \ decisions scale across different model sizes?</p>\n</li>\n<li><p>Alternatively,\
          \ we would think to use MAD-X adapters just as Yong et al (<a rel=\"nofollow\"\
          \ href=\"https://arxiv.org/pdf/2204.04873.pdf\">https://arxiv.org/pdf/2204.04873.pdf</a>).<br>2.1\
          \ Would you recommend this approach over continued pretraining? And if so,\
          \ where in what capacity would you recommend plugging in the adapters (again\
          \ in function of the BLOOM model size)?</p>\n</li>\n</ol>\n<p>Please let\
          \ us know if you would need any more context to answer our questions. Many\
          \ thanks in advance.</p>\n"
        raw: "Hi there! \n\nHigh level: starting from BLOOM, we want to create a LLM\
          \ for Dutch and look for tips regarding the hyperparameters/training paradigm.\
          \ \n\nContext: We\u2019re setting up the code and infrastructure to fine-tune\
          \ all BLOOM models on a Dutch corpus. In general, we don\u2019t wish to\
          \ particularly optimize for multilingual capabilities and aim for a SOTA\
          \ Dutch LLM. We have collected around 150GB of Dutch data, fitted a new\
          \ tokenizer with a vocabulary size of 40,000 tokens and prepared the language\
          \ model training dataset. We have access to a HPC infrastructure, which\
          \ would allow us to train for 50-100k GPU hours and have set up the ZeRO\
          \ framework using DeepSpeed for parallel training. \n\nQuestions: \nWe currently\
          \ consider two training paradigms. Any general or specific piece of advice\
          \ would be much appreciated. \n1. Continued pretraining. To our knowledge,\
          \ this seems to be the easiest way forward. We currently anticipate running\
          \ the training with all weights unfrozen, for 1 epoch and 256 as effective\
          \ batch size. What could you recommend using as hyperparameters? In particular:\n\
          1.1 Would you recommend freezing a certain set of weights? \n1.2 Which learning\
          \ rate (paradigm) would you propose? \n1.3 How would these decisions scale\
          \ across different model sizes?\n\n2. Alternatively, we would think to use\
          \ MAD-X adapters just as Yong et al (https://arxiv.org/pdf/2204.04873.pdf).\n\
          2.1 Would you recommend this approach over continued pretraining? And if\
          \ so, where in what capacity would you recommend plugging in the adapters\
          \ (again in function of the BLOOM model size)? \n\nPlease let us know if\
          \ you would need any more context to answer our questions. Many thanks in\
          \ advance."
        updatedAt: '2023-05-02T13:51:02.709Z'
      numEdits: 1
      reactions: []
    id: 64511119938967fd069c375d
    type: comment
  author: matthieumeeus
  content: "Hi there! \n\nHigh level: starting from BLOOM, we want to create a LLM\
    \ for Dutch and look for tips regarding the hyperparameters/training paradigm.\
    \ \n\nContext: We\u2019re setting up the code and infrastructure to fine-tune\
    \ all BLOOM models on a Dutch corpus. In general, we don\u2019t wish to particularly\
    \ optimize for multilingual capabilities and aim for a SOTA Dutch LLM. We have\
    \ collected around 150GB of Dutch data, fitted a new tokenizer with a vocabulary\
    \ size of 40,000 tokens and prepared the language model training dataset. We have\
    \ access to a HPC infrastructure, which would allow us to train for 50-100k GPU\
    \ hours and have set up the ZeRO framework using DeepSpeed for parallel training.\
    \ \n\nQuestions: \nWe currently consider two training paradigms. Any general or\
    \ specific piece of advice would be much appreciated. \n1. Continued pretraining.\
    \ To our knowledge, this seems to be the easiest way forward. We currently anticipate\
    \ running the training with all weights unfrozen, for 1 epoch and 256 as effective\
    \ batch size. What could you recommend using as hyperparameters? In particular:\n\
    1.1 Would you recommend freezing a certain set of weights? \n1.2 Which learning\
    \ rate (paradigm) would you propose? \n1.3 How would these decisions scale across\
    \ different model sizes?\n\n2. Alternatively, we would think to use MAD-X adapters\
    \ just as Yong et al (https://arxiv.org/pdf/2204.04873.pdf).\n2.1 Would you recommend\
    \ this approach over continued pretraining? And if so, where in what capacity\
    \ would you recommend plugging in the adapters (again in function of the BLOOM\
    \ model size)? \n\nPlease let us know if you would need any more context to answer\
    \ our questions. Many thanks in advance."
  created_at: 2023-05-02 12:33:13+00:00
  edited: true
  hidden: false
  id: 64511119938967fd069c375d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 246
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Extending BLOOM to Dutch - tips for hyperparameters
