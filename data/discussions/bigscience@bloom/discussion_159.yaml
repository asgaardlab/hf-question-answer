!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lewiswu1209
conflicting_files: null
created_at: 2022-12-18 17:21:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/495da69fc13e4ba3fc8c57ea753127cf.svg
      fullname: lewiswu1209
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lewiswu1209
      type: user
    createdAt: '2022-12-18T17:21:17.000Z'
    data:
      edited: false
      editors:
      - lewiswu1209
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/495da69fc13e4ba3fc8c57ea753127cf.svg
          fullname: lewiswu1209
          isHf: false
          isPro: false
          name: lewiswu1209
          type: user
        html: '<p>How can I make Bloom stop generating when it should?<br>I want bloom
          stop generating when end of the response, do not generate other things until
          max_new_token.<br>Excuse my poor English</p>

          '
        raw: "How can I make Bloom stop generating when it should? \r\nI want bloom\
          \ stop generating when end of the response, do not generate other things\
          \ until max_new_token.\r\nExcuse my poor English"
        updatedAt: '2022-12-18T17:21:17.617Z'
      numEdits: 0
      reactions: []
    id: 639f4c0d9f1f2baab2f3d479
    type: comment
  author: lewiswu1209
  content: "How can I make Bloom stop generating when it should? \r\nI want bloom\
    \ stop generating when end of the response, do not generate other things until\
    \ max_new_token.\r\nExcuse my poor English"
  created_at: 2022-12-18 17:21:17+00:00
  edited: false
  hidden: false
  id: 639f4c0d9f1f2baab2f3d479
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1be4476e3dfb4d3f19ed40e126c1e391.svg
      fullname: Elian vohl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: imwide
      type: user
    createdAt: '2023-02-13T18:13:49.000Z'
    data:
      edited: false
      editors:
      - imwide
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1be4476e3dfb4d3f19ed40e126c1e391.svg
          fullname: Elian vohl
          isHf: false
          isPro: false
          name: imwide
          type: user
        html: '<p>There is a stop parameter. It will stop generating when any of the
          given tokens in the list are generated:</p>

          <p>payload = {<br>            "inputs": promt,<br>            "parameters":
          {<br>                "do_sample": True,<br>                "top_p": X,<br>                "max_new_tokens":
          150,<br>                "temperature": X,<br>                "stop": [''.'',
          ''The next day'']<br>            }<br>response = hfrequests.post("<a rel="nofollow"
          href="https://api-inference.huggingface.co/models/bigscience/bloom&quot;">https://api-inference.huggingface.co/models/bigscience/bloom"</a>,
          headers=headers, json=payload)</p>

          '
        raw: "There is a stop parameter. It will stop generating when any of the given\
          \ tokens in the list are generated:\n\npayload = {\n            \"inputs\"\
          : promt,\n            \"parameters\": {\n                \"do_sample\":\
          \ True,\n                \"top_p\": X,\n                \"max_new_tokens\"\
          : 150,\n                \"temperature\": X,\n                \"stop\": ['.',\
          \ 'The next day']\n            }\nresponse = hfrequests.post(\"https://api-inference.huggingface.co/models/bigscience/bloom\"\
          , headers=headers, json=payload)"
        updatedAt: '2023-02-13T18:13:49.329Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Jiangz
    id: 63ea7ddd2c2fcf93e7825c3f
    type: comment
  author: imwide
  content: "There is a stop parameter. It will stop generating when any of the given\
    \ tokens in the list are generated:\n\npayload = {\n            \"inputs\": promt,\n\
    \            \"parameters\": {\n                \"do_sample\": True,\n       \
    \         \"top_p\": X,\n                \"max_new_tokens\": 150,\n          \
    \      \"temperature\": X,\n                \"stop\": ['.', 'The next day']\n\
    \            }\nresponse = hfrequests.post(\"https://api-inference.huggingface.co/models/bigscience/bloom\"\
    , headers=headers, json=payload)"
  created_at: 2023-02-13 18:13:49+00:00
  edited: false
  hidden: false
  id: 63ea7ddd2c2fcf93e7825c3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/495da69fc13e4ba3fc8c57ea753127cf.svg
      fullname: lewiswu1209
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lewiswu1209
      type: user
    createdAt: '2023-02-14T15:01:02.000Z'
    data:
      edited: false
      editors:
      - lewiswu1209
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/495da69fc13e4ba3fc8c57ea753127cf.svg
          fullname: lewiswu1209
          isHf: false
          isPro: false
          name: lewiswu1209
          type: user
        html: '<blockquote>

          <p>There is a stop parameter. It will stop generating when any of the given
          tokens in the list are generated:</p>

          <p>payload = {<br>            "inputs": promt,<br>            "parameters":
          {<br>                "do_sample": True,<br>                "top_p": X,<br>                "max_new_tokens":
          150,<br>                "temperature": X,<br>                "stop": [''.'',
          ''The next day'']<br>            }<br>response = hfrequests.post("<a rel="nofollow"
          href="https://api-inference.huggingface.co/models/bigscience/bloom&quot;">https://api-inference.huggingface.co/models/bigscience/bloom"</a>,
          headers=headers, json=payload)</p>

          </blockquote>

          <p>I want to let bloom stop when finish the content, I give a () at begin
          and end of example, it will let bloom put ) when finish the content</p>

          '
        raw: "> There is a stop parameter. It will stop generating when any of the\
          \ given tokens in the list are generated:\n> \n> payload = {\n>        \
          \     \"inputs\": promt,\n>             \"parameters\": {\n>           \
          \      \"do_sample\": True,\n>                 \"top_p\": X,\n>        \
          \         \"max_new_tokens\": 150,\n>                 \"temperature\": X,\n\
          >                 \"stop\": ['.', 'The next day']\n>             }\n> response\
          \ = hfrequests.post(\"https://api-inference.huggingface.co/models/bigscience/bloom\"\
          , headers=headers, json=payload)\n\nI want to let bloom stop when finish\
          \ the content, I give a () at begin and end of example, it will let bloom\
          \ put ) when finish the content"
        updatedAt: '2023-02-14T15:01:02.507Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Jiangz
    id: 63eba22e3f6d691c3b2430da
    type: comment
  author: lewiswu1209
  content: "> There is a stop parameter. It will stop generating when any of the given\
    \ tokens in the list are generated:\n> \n> payload = {\n>             \"inputs\"\
    : promt,\n>             \"parameters\": {\n>                 \"do_sample\": True,\n\
    >                 \"top_p\": X,\n>                 \"max_new_tokens\": 150,\n\
    >                 \"temperature\": X,\n>                 \"stop\": ['.', 'The\
    \ next day']\n>             }\n> response = hfrequests.post(\"https://api-inference.huggingface.co/models/bigscience/bloom\"\
    , headers=headers, json=payload)\n\nI want to let bloom stop when finish the content,\
    \ I give a () at begin and end of example, it will let bloom put ) when finish\
    \ the content"
  created_at: 2023-02-14 15:01:02+00:00
  edited: false
  hidden: false
  id: 63eba22e3f6d691c3b2430da
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 159
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: How can I make Bloom stop generating when it should?
