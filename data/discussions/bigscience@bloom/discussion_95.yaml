!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rexer3000
conflicting_files: null
created_at: 2022-08-26 14:47:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/681587409252392809f251fa2ff62d42.svg
      fullname: samuel taliaferro
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rexer3000
      type: user
    createdAt: '2022-08-26T15:47:16.000Z'
    data:
      edited: false
      editors:
      - rexer3000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/681587409252392809f251fa2ff62d42.svg
          fullname: samuel taliaferro
          isHf: false
          isPro: false
          name: rexer3000
          type: user
        html: '<p>I am doing a project where I need to feed Bloom more than 1000 tokens.
          is there a paid API where I can have a higher token limit?</p>

          '
        raw: "I am doing a project where I need to feed Bloom more than 1000 tokens.\
          \ is there a paid API where I can have a higher token limit?\r\n\r\n"
        updatedAt: '2022-08-26T15:47:16.459Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - rexer3000
        - nonoope
        - Jenny51501
        - notmenotme
        - bms
    id: 6308eb047dc1b1a54ccbcb76
    type: comment
  author: rexer3000
  content: "I am doing a project where I need to feed Bloom more than 1000 tokens.\
    \ is there a paid API where I can have a higher token limit?\r\n\r\n"
  created_at: 2022-08-26 14:47:16+00:00
  edited: false
  hidden: false
  id: 6308eb047dc1b1a54ccbcb76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-15T00:18:53.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p>I'm not sure. I guess not. The purpose of the current deployment\
          \ is for people to manually test it and get a feeling of how BLOOM performs.\
          \ cc <span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Narsil\">@<span class=\"\
          underline\">Narsil</span></a></span>\n\n\t</span></span> to confirm</p>\n\
          <p>I think the only thing that's left would be to spawn your own instance\
          \ and run evaluation there? There's some inference codebase we use if you\
          \ want to check it out <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference\"\
          >https://github.com/huggingface/text-generation-inference</a></p>\n"
        raw: 'I''m not sure. I guess not. The purpose of the current deployment is
          for people to manually test it and get a feeling of how BLOOM performs.
          cc @Narsil to confirm


          I think the only thing that''s left would be to spawn your own instance
          and run evaluation there? There''s some inference codebase we use if you
          want to check it out https://github.com/huggingface/text-generation-inference'
        updatedAt: '2022-11-15T00:18:53.348Z'
      numEdits: 0
      reactions: []
    id: 6372daed9a20fe1b5885793a
    type: comment
  author: TimeRobber
  content: 'I''m not sure. I guess not. The purpose of the current deployment is for
    people to manually test it and get a feeling of how BLOOM performs. cc @Narsil
    to confirm


    I think the only thing that''s left would be to spawn your own instance and run
    evaluation there? There''s some inference codebase we use if you want to check
    it out https://github.com/huggingface/text-generation-inference'
  created_at: 2022-11-15 00:18:53+00:00
  edited: false
  hidden: false
  id: 6372daed9a20fe1b5885793a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 95
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: I am doing a project where I need to feed Bloom more than 1000 tokens. is there
  a paid API where I can have a higher token limit?
