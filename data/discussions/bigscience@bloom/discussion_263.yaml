!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Iluvmelons
conflicting_files: null
created_at: 2023-09-28 03:18:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4ed0c237d4ec92c6156999f2232f3c5e.svg
      fullname: Naman Anand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Iluvmelons
      type: user
    createdAt: '2023-09-28T04:18:23.000Z'
    data:
      edited: false
      editors:
      - Iluvmelons
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2900184988975525
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4ed0c237d4ec92c6156999f2232f3c5e.svg
          fullname: Naman Anand
          isHf: false
          isPro: false
          name: Iluvmelons
          type: user
        html: "<pre><code>async function query() {\n    const requestData = {\n  \
          \      inputs: dream\n    };\n\n    const response = await fetch(\n    \
          \    \"https://api-inference.huggingface.co/models/bigscience/bloom\",\n\
          \        {\n            headers: {\n                Authorization: \"Bearer\
          \ {key}\",\n                \"Content-Type\": \"application/json\"\n   \
          \         },\n            method: \"POST\",\n            body: JSON.stringify(requestData),\n\
          \        }\n    );\n\n    if (response.status === 200) {\n        const\
          \ result = await response.json();\n        console.log(result);\n      \
          \  return result;\n    } else {\n        console.error(\"Error:\", response.status,\
          \ response.statusText);\n    }\n}\n</code></pre>\n<p>above is the format\
          \ i am using </p>\n"
        raw: "    async function query() {\r\n        const requestData = {\r\n  \
          \          inputs: dream\r\n        };\r\n    \r\n        const response\
          \ = await fetch(\r\n            \"https://api-inference.huggingface.co/models/bigscience/bloom\"\
          ,\r\n            {\r\n                headers: {\r\n                   \
          \ Authorization: \"Bearer {key}\",\r\n                    \"Content-Type\"\
          : \"application/json\"\r\n                },\r\n                method:\
          \ \"POST\",\r\n                body: JSON.stringify(requestData),\r\n  \
          \          }\r\n        );\r\n    \r\n        if (response.status === 200)\
          \ {\r\n            const result = await response.json();\r\n           \
          \ console.log(result);\r\n            return result;\r\n        } else {\r\
          \n            console.error(\"Error:\", response.status, response.statusText);\r\
          \n        }\r\n    }\r\nabove is the format i am using "
        updatedAt: '2023-09-28T04:18:23.119Z'
      numEdits: 0
      reactions: []
    id: 6514fe8f6e0d30ed7e1234b4
    type: comment
  author: Iluvmelons
  content: "    async function query() {\r\n        const requestData = {\r\n    \
    \        inputs: dream\r\n        };\r\n    \r\n        const response = await\
    \ fetch(\r\n            \"https://api-inference.huggingface.co/models/bigscience/bloom\"\
    ,\r\n            {\r\n                headers: {\r\n                    Authorization:\
    \ \"Bearer {key}\",\r\n                    \"Content-Type\": \"application/json\"\
    \r\n                },\r\n                method: \"POST\",\r\n              \
    \  body: JSON.stringify(requestData),\r\n            }\r\n        );\r\n    \r\
    \n        if (response.status === 200) {\r\n            const result = await response.json();\r\
    \n            console.log(result);\r\n            return result;\r\n        }\
    \ else {\r\n            console.error(\"Error:\", response.status, response.statusText);\r\
    \n        }\r\n    }\r\nabove is the format i am using "
  created_at: 2023-09-28 03:18:23+00:00
  edited: false
  hidden: false
  id: 6514fe8f6e0d30ed7e1234b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-09-28T09:24:10.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.691523015499115
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Iluvmelons&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Iluvmelons\"\
          >@<span class=\"underline\">Iluvmelons</span></a></span>\n\n\t</span></span>,\
          \ do you mean the number of tokens generated by the model?</p>\n<p>It's\
          \ using the Text Generation Inference API, you can read the documentation\
          \ here: <a href=\"https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task\"\
          >https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task</a></p>\n\
          <p>All parameters are available there; the one you're most interested is\
          \ likely <code>max_new_tokens</code> or <code>max_time</code>?</p>\n"
        raw: 'Hey @Iluvmelons, do you mean the number of tokens generated by the model?


          It''s using the Text Generation Inference API, you can read the documentation
          here: https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task


          All parameters are available there; the one you''re most interested is likely
          `max_new_tokens` or `max_time`?'
        updatedAt: '2023-09-28T09:24:10.977Z'
      numEdits: 0
      reactions: []
    id: 6515463a045240ae9e551187
    type: comment
  author: lysandre
  content: 'Hey @Iluvmelons, do you mean the number of tokens generated by the model?


    It''s using the Text Generation Inference API, you can read the documentation
    here: https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task


    All parameters are available there; the one you''re most interested is likely
    `max_new_tokens` or `max_time`?'
  created_at: 2023-09-28 08:24:10+00:00
  edited: false
  hidden: false
  id: 6515463a045240ae9e551187
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 263
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: How do i increase or decrease size of response in API call?
