!!python/object:huggingface_hub.community.DiscussionWithDetails
author: maveriq
conflicting_files: null
created_at: 2022-07-12 13:26:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
      fullname: Haris Jabbar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maveriq
      type: user
    createdAt: '2022-07-12T14:26:50.000Z'
    data:
      edited: false
      editors:
      - maveriq
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
          fullname: Haris Jabbar
          isHf: false
          isPro: false
          name: maveriq
          type: user
        html: '<p>I was wondering what''s the most efficient way to load partial model
          (num_layers&lt;70) for bloom. One naive way could be to specify the number
          of layers in config and then load state dict with strict=False, but this
          would go through all the keys of 70 layers. </p>

          <p>Can there be a more efficient solution?</p>

          '
        raw: "I was wondering what's the most efficient way to load partial model\
          \ (num_layers<70) for bloom. One naive way could be to specify the number\
          \ of layers in config and then load state dict with strict=False, but this\
          \ would go through all the keys of 70 layers. \r\n\r\nCan there be a more\
          \ efficient solution?"
        updatedAt: '2022-07-12T14:26:50.534Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ybelkada
    id: 62cd84aac589e4a9e23ddad2
    type: comment
  author: maveriq
  content: "I was wondering what's the most efficient way to load partial model (num_layers<70)\
    \ for bloom. One naive way could be to specify the number of layers in config\
    \ and then load state dict with strict=False, but this would go through all the\
    \ keys of 70 layers. \r\n\r\nCan there be a more efficient solution?"
  created_at: 2022-07-12 13:26:50+00:00
  edited: false
  hidden: false
  id: 62cd84aac589e4a9e23ddad2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-07-12T14:28:30.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;maveriq&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/maveriq\">@<span class=\"\
          underline\">maveriq</span></a></span>\n\n\t</span></span><br>Thank you for\
          \ your question !!<br>I would go as suggested, but in addition to that manually\
          \ recreate the <code>pytorch_model.bin.index.json</code> file to adjust\
          \ the new mapping + rename the sharded files accordingly. I think this should\
          \ work</p>\n"
        raw: "Hi @maveriq \nThank you for your question !!\nI would go as suggested,\
          \ but in addition to that manually recreate the `pytorch_model.bin.index.json`\
          \ file to adjust the new mapping + rename the sharded files accordingly.\
          \ I think this should work"
        updatedAt: '2022-07-12T14:30:05.577Z'
      numEdits: 1
      reactions: []
    id: 62cd850ef27cc94a75947275
    type: comment
  author: ybelkada
  content: "Hi @maveriq \nThank you for your question !!\nI would go as suggested,\
    \ but in addition to that manually recreate the `pytorch_model.bin.index.json`\
    \ file to adjust the new mapping + rename the sharded files accordingly. I think\
    \ this should work"
  created_at: 2022-07-12 13:28:30+00:00
  edited: true
  hidden: false
  id: 62cd850ef27cc94a75947275
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-07-12T14:32:41.000Z'
    data:
      edited: true
      editors:
      - julien-c
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p>So if let's say you want to load the first layer only your new <code>.index.json</code>\
          \ would look like: </p>\n<pre><code>{\n  \"metadata\": {\n    \"total_size\"\
          : 352494542848\n  },\n  \"weight_map\": {\n    \"h.0.input_layernorm.bias\"\
          : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.input_layernorm.weight\"\
          : \"pytorch_model_00002-of-0001.bin\",\n    \"h.0.mlp.dense_4h_to_h.bias\"\
          : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.mlp.dense_4h_to_h.weight\"\
          : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.mlp.dense_h_to_4h.bias\"\
          : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.mlp.dense_h_to_4h.weight\"\
          : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.post_attention_layernorm.bias\"\
          : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.post_attention_layernorm.weight\"\
          : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.self_attention.dense.bias\"\
          : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.self_attention.dense.weight\"\
          : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.self_attention.query_key_value.bias\"\
          : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.self_attention.query_key_value.weight\"\
          : \"pytorch_model_00002-of-0002.bin\",\n     \"word_embeddings.weight\"\
          : \"pytorch_model_00001-of-0002.bin\",\n    \"word_embeddings_layernorm.bias\"\
          : \"pytorch_model_00001-of-0002.bin\",\n    \"word_embeddings_layernorm.weight\"\
          : \"pytorch_model_00001-of-0002.bin\"\n  }\n}\n</code></pre>\n<p>And you'll\
          \ have to manually rename the <code>pytorch_model_00002-of-0072.bin</code>file\
          \ to <code>pytorch_model_00002-of-0002.bin</code></p>\n"
        raw: "So if let's say you want to load the first layer only your new `.index.json`\
          \ would look like: \n```\n{\n  \"metadata\": {\n    \"total_size\": 352494542848\n\
          \  },\n  \"weight_map\": {\n    \"h.0.input_layernorm.bias\": \"pytorch_model_00002-of-0002.bin\"\
          ,\n    \"h.0.input_layernorm.weight\": \"pytorch_model_00002-of-0001.bin\"\
          ,\n    \"h.0.mlp.dense_4h_to_h.bias\": \"pytorch_model_00002-of-0002.bin\"\
          ,\n    \"h.0.mlp.dense_4h_to_h.weight\": \"pytorch_model_00002-of-0002.bin\"\
          ,\n    \"h.0.mlp.dense_h_to_4h.bias\": \"pytorch_model_00002-of-0002.bin\"\
          ,\n    \"h.0.mlp.dense_h_to_4h.weight\": \"pytorch_model_00002-of-0002.bin\"\
          ,\n    \"h.0.post_attention_layernorm.bias\": \"pytorch_model_00002-of-0002.bin\"\
          ,\n    \"h.0.post_attention_layernorm.weight\": \"pytorch_model_00002-of-0002.bin\"\
          ,\n    \"h.0.self_attention.dense.bias\": \"pytorch_model_00002-of-0002.bin\"\
          ,\n    \"h.0.self_attention.dense.weight\": \"pytorch_model_00002-of-0002.bin\"\
          ,\n    \"h.0.self_attention.query_key_value.bias\": \"pytorch_model_00002-of-0002.bin\"\
          ,\n    \"h.0.self_attention.query_key_value.weight\": \"pytorch_model_00002-of-0002.bin\"\
          ,\n     \"word_embeddings.weight\": \"pytorch_model_00001-of-0002.bin\"\
          ,\n    \"word_embeddings_layernorm.bias\": \"pytorch_model_00001-of-0002.bin\"\
          ,\n    \"word_embeddings_layernorm.weight\": \"pytorch_model_00001-of-0002.bin\"\
          \n  }\n}\n```\nAnd you'll have to manually rename the `pytorch_model_00002-of-0072.bin`file\
          \ to `pytorch_model_00002-of-0002.bin`"
        updatedAt: '2022-07-18T20:01:15.193Z'
      numEdits: 1
      reactions: []
    id: 62cd8609f262ffe83fc99d38
    type: comment
  author: ybelkada
  content: "So if let's say you want to load the first layer only your new `.index.json`\
    \ would look like: \n```\n{\n  \"metadata\": {\n    \"total_size\": 352494542848\n\
    \  },\n  \"weight_map\": {\n    \"h.0.input_layernorm.bias\": \"pytorch_model_00002-of-0002.bin\"\
    ,\n    \"h.0.input_layernorm.weight\": \"pytorch_model_00002-of-0001.bin\",\n\
    \    \"h.0.mlp.dense_4h_to_h.bias\": \"pytorch_model_00002-of-0002.bin\",\n  \
    \  \"h.0.mlp.dense_4h_to_h.weight\": \"pytorch_model_00002-of-0002.bin\",\n  \
    \  \"h.0.mlp.dense_h_to_4h.bias\": \"pytorch_model_00002-of-0002.bin\",\n    \"\
    h.0.mlp.dense_h_to_4h.weight\": \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.post_attention_layernorm.bias\"\
    : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.post_attention_layernorm.weight\"\
    : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.self_attention.dense.bias\"\
    : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.self_attention.dense.weight\"\
    : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.self_attention.query_key_value.bias\"\
    : \"pytorch_model_00002-of-0002.bin\",\n    \"h.0.self_attention.query_key_value.weight\"\
    : \"pytorch_model_00002-of-0002.bin\",\n     \"word_embeddings.weight\": \"pytorch_model_00001-of-0002.bin\"\
    ,\n    \"word_embeddings_layernorm.bias\": \"pytorch_model_00001-of-0002.bin\"\
    ,\n    \"word_embeddings_layernorm.weight\": \"pytorch_model_00001-of-0002.bin\"\
    \n  }\n}\n```\nAnd you'll have to manually rename the `pytorch_model_00002-of-0072.bin`file\
    \ to `pytorch_model_00002-of-0002.bin`"
  created_at: 2022-07-12 13:32:41+00:00
  edited: true
  hidden: false
  id: 62cd8609f262ffe83fc99d38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
      fullname: Haris Jabbar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maveriq
      type: user
    createdAt: '2022-07-12T14:49:15.000Z'
    data:
      edited: false
      editors:
      - maveriq
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
          fullname: Haris Jabbar
          isHf: false
          isPro: false
          name: maveriq
          type: user
        html: '<p>Thank you for your quick reply. Is the file renaming necessary.
          Or can I just point to the correct file with original indices?</p>

          '
        raw: Thank you for your quick reply. Is the file renaming necessary. Or can
          I just point to the correct file with original indices?
        updatedAt: '2022-07-12T14:49:15.472Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ybelkada
    id: 62cd89eb3a2cecfdabea4bb1
    type: comment
  author: maveriq
  content: Thank you for your quick reply. Is the file renaming necessary. Or can
    I just point to the correct file with original indices?
  created_at: 2022-07-12 13:49:15+00:00
  edited: false
  hidden: false
  id: 62cd89eb3a2cecfdabea4bb1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-07-12T14:53:58.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>I think that you are right, you can try as you suggested and let
          me know!</p>

          '
        raw: I think that you are right, you can try as you suggested and let me know!
        updatedAt: '2022-07-12T14:53:58.337Z'
      numEdits: 0
      reactions: []
    id: 62cd8b06248f9e6bc20b2745
    type: comment
  author: ybelkada
  content: I think that you are right, you can try as you suggested and let me know!
  created_at: 2022-07-12 13:53:58+00:00
  edited: false
  hidden: false
  id: 62cd8b06248f9e6bc20b2745
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-07-12T14:58:57.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Also do not forget to add as I forgot to put it above !</p>\n<pre><code>\
          \    \"ln_f.bias\": \"pytorch_model_00072-of-00072.bin\",\n    \"ln_f.weight\"\
          : \"pytorch_model_00072-of-00072.bin\",\n</code></pre>\n"
        raw: "Also do not forget to add as I forgot to put it above !\n\n```\n   \
          \ \"ln_f.bias\": \"pytorch_model_00072-of-00072.bin\",\n    \"ln_f.weight\"\
          : \"pytorch_model_00072-of-00072.bin\",\n```"
        updatedAt: '2022-07-12T14:58:57.579Z'
      numEdits: 0
      reactions: []
    id: 62cd8c31c589e4a9e23e3367
    type: comment
  author: ybelkada
  content: "Also do not forget to add as I forgot to put it above !\n\n```\n    \"\
    ln_f.bias\": \"pytorch_model_00072-of-00072.bin\",\n    \"ln_f.weight\": \"pytorch_model_00072-of-00072.bin\"\
    ,\n```"
  created_at: 2022-07-12 13:58:57+00:00
  edited: false
  hidden: false
  id: 62cd8c31c589e4a9e23e3367
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
      fullname: Haris Jabbar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maveriq
      type: user
    createdAt: '2022-07-12T17:54:40.000Z'
    data:
      edited: false
      editors:
      - maveriq
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
          fullname: Haris Jabbar
          isHf: false
          isPro: false
          name: maveriq
          type: user
        html: "<p>Hi. So I made this function that loads only the layers that you\
          \ pass in, as a list.</p>\n<pre><code>bloom_dir = Path('/mounts/data/huggingface/bloom/')\n\
          \ndef get_json(layers):\n    index = json.load(open(bloom_dir/'pytorch_model.bin.index.json','rt'))\n\
          \n    load_dict = {}\n    load_dict['metadata'] = {'total_size': 352494542848}\n\
          \    load_dict['weight_map'] = {}\n\n    for k,v in index['weight_map'].items():\n\
          \        if k.startswith('h'):\n            l = int(k.split('.')[1])\n \
          \           if l in layers:\n                load_dict['weight_map'][k]=str(base_dir/v)\n\
          \        else:\n            load_dict['weight_map'][k]=str(base_dir/v)\n\
          \n    return load_dict\n</code></pre>\n<p>The model can then be loaded as\
          \ </p>\n<pre><code>json.dump(get_json([4]),open('./tmp/pytorch_model.bin.index.json','wt'))\n\
          model = AutoModel.from_pretrained('./tmp/')\n</code></pre>\n<p>The bloom_dir\
          \ is the path where I cloned the repo using 'git clone <a href=\"https://huggingface.co/bigscience/bloom'\"\
          >https://huggingface.co/bigscience/bloom'</a></p>\n<p>Hope it helps anyone\
          \ who wants to do the same efficiently.</p>\n"
        raw: "Hi. So I made this function that loads only the layers that you pass\
          \ in, as a list.\n\n    bloom_dir = Path('/mounts/data/huggingface/bloom/')\n\
          \n    def get_json(layers):\n        index = json.load(open(bloom_dir/'pytorch_model.bin.index.json','rt'))\n\
          \n        load_dict = {}\n        load_dict['metadata'] = {'total_size':\
          \ 352494542848}\n        load_dict['weight_map'] = {}\n\n        for k,v\
          \ in index['weight_map'].items():\n            if k.startswith('h'):\n \
          \               l = int(k.split('.')[1])\n                if l in layers:\n\
          \                    load_dict['weight_map'][k]=str(base_dir/v)\n      \
          \      else:\n                load_dict['weight_map'][k]=str(base_dir/v)\n\
          \n        return load_dict\n\nThe model can then be loaded as \n\n    json.dump(get_json([4]),open('./tmp/pytorch_model.bin.index.json','wt'))\n\
          \    model = AutoModel.from_pretrained('./tmp/')\n\nThe bloom_dir is the\
          \ path where I cloned the repo using 'git clone https://huggingface.co/bigscience/bloom'\n\
          \nHope it helps anyone who wants to do the same efficiently."
        updatedAt: '2022-07-12T17:54:40.064Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Pwicke
        - seba
    id: 62cdb560c589e4a9e2402078
    type: comment
  author: maveriq
  content: "Hi. So I made this function that loads only the layers that you pass in,\
    \ as a list.\n\n    bloom_dir = Path('/mounts/data/huggingface/bloom/')\n\n  \
    \  def get_json(layers):\n        index = json.load(open(bloom_dir/'pytorch_model.bin.index.json','rt'))\n\
    \n        load_dict = {}\n        load_dict['metadata'] = {'total_size': 352494542848}\n\
    \        load_dict['weight_map'] = {}\n\n        for k,v in index['weight_map'].items():\n\
    \            if k.startswith('h'):\n                l = int(k.split('.')[1])\n\
    \                if l in layers:\n                    load_dict['weight_map'][k]=str(base_dir/v)\n\
    \            else:\n                load_dict['weight_map'][k]=str(base_dir/v)\n\
    \n        return load_dict\n\nThe model can then be loaded as \n\n    json.dump(get_json([4]),open('./tmp/pytorch_model.bin.index.json','wt'))\n\
    \    model = AutoModel.from_pretrained('./tmp/')\n\nThe bloom_dir is the path\
    \ where I cloned the repo using 'git clone https://huggingface.co/bigscience/bloom'\n\
    \nHope it helps anyone who wants to do the same efficiently."
  created_at: 2022-07-12 16:54:40+00:00
  edited: false
  hidden: false
  id: 62cdb560c589e4a9e2402078
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
      fullname: Haris Jabbar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maveriq
      type: user
    createdAt: '2022-07-12T17:54:51.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
          fullname: Haris Jabbar
          isHf: false
          isPro: false
          name: maveriq
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-07-12T17:55:24.575Z'
      numEdits: 0
      reactions: []
    id: 62cdb56baac2c91c95551506
    type: comment
  author: maveriq
  content: This comment has been hidden
  created_at: 2022-07-12 16:54:51+00:00
  edited: true
  hidden: true
  id: 62cdb56baac2c91c95551506
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-15T00:03:17.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Nice! I''ll close this discussion as it seems to be resolved. Feel
          free to re-open if you feel that the snippet doesn''t solve the discussion.</p>

          '
        raw: Nice! I'll close this discussion as it seems to be resolved. Feel free
          to re-open if you feel that the snippet doesn't solve the discussion.
        updatedAt: '2022-11-15T00:03:17.377Z'
      numEdits: 0
      reactions: []
    id: 6372d745759c1ca820452304
    type: comment
  author: TimeRobber
  content: Nice! I'll close this discussion as it seems to be resolved. Feel free
    to re-open if you feel that the snippet doesn't solve the discussion.
  created_at: 2022-11-15 00:03:17+00:00
  edited: false
  hidden: false
  id: 6372d745759c1ca820452304
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-15T00:03:19.000Z'
    data:
      status: closed
    id: 6372d7479a20fe1b588557e8
    type: status-change
  author: TimeRobber
  created_at: 2022-11-15 00:03:19+00:00
  id: 6372d7479a20fe1b588557e8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 34
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: Loading partial model
