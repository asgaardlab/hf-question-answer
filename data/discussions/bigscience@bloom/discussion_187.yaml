!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NicolasExo
conflicting_files: null
created_at: 2023-02-10 08:36:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4673f323abd25f8b24d9e226fa72116e.svg
      fullname: Nicolas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NicolasExo
      type: user
    createdAt: '2023-02-10T08:36:22.000Z'
    data:
      edited: true
      editors:
      - NicolasExo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4673f323abd25f8b24d9e226fa72116e.svg
          fullname: Nicolas
          isHf: false
          isPro: false
          name: NicolasExo
          type: user
        html: '<p>Hello everyone ! I have a question to ask you, dear community.</p>

          <p>How can i train the Bloom AI Model with my own training dataset ?<br>Is
          there any function in Bloom like "BloomSomeClass.train(inputs, outputs,
          params)" ?</p>

          <p>Thank you for your answers in advance !</p>

          '
        raw: "Hello everyone ! I have a question to ask you, dear community.\n\nHow\
          \ can i train the Bloom AI Model with my own training dataset ? \nIs there\
          \ any function in Bloom like \"BloomSomeClass.train(inputs, outputs, params)\"\
          \ ?\n\nThank you for your answers in advance !"
        updatedAt: '2023-02-10T08:39:50.516Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Tykhist
    id: 63e60206b6a40bf941dace2f
    type: comment
  author: NicolasExo
  content: "Hello everyone ! I have a question to ask you, dear community.\n\nHow\
    \ can i train the Bloom AI Model with my own training dataset ? \nIs there any\
    \ function in Bloom like \"BloomSomeClass.train(inputs, outputs, params)\" ?\n\
    \nThank you for your answers in advance !"
  created_at: 2023-02-10 08:36:22+00:00
  edited: true
  hidden: false
  id: 63e60206b6a40bf941dace2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/4673f323abd25f8b24d9e226fa72116e.svg
      fullname: Nicolas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NicolasExo
      type: user
    createdAt: '2023-02-10T09:34:58.000Z'
    data:
      from: Training the Bloom AI Model
      to: Training or Fine-tuning the Bloom AI Model on my own Dataset
    id: 63e60fc25c3664766eb355e5
    type: title-change
  author: NicolasExo
  created_at: 2023-02-10 09:34:58+00:00
  id: 63e60fc25c3664766eb355e5
  new_title: Training or Fine-tuning the Bloom AI Model on my own Dataset
  old_title: Training the Bloom AI Model
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640134437444-609baae0fe087f3d04cf0481.jpeg?w=200&h=200&f=face
      fullname: Yozh
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: justheuristic
      type: user
    createdAt: '2023-02-10T12:36:41.000Z'
    data:
      edited: false
      editors:
      - justheuristic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640134437444-609baae0fe087f3d04cf0481.jpeg?w=200&h=200&f=face
          fullname: Yozh
          isHf: false
          isPro: false
          name: justheuristic
          type: user
        html: '<p>Hi!<br>You fine-tune BLOOM the same way you fine-tune any other
          model on HF.</p>

          <p>Consider the official example for text classification: <a rel="nofollow"
          href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification">https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification</a></p>

          <p>In the readme, you can find <code>  --model_name_or_path bert-base-multilingual-cased
          \</code></p>

          <p>If you replace this line with <code>  --model_name_or_path bigscience/bloom-560m
          \</code>,<br>you will fine-tune the (smallest) bloom model on the dataset
          in question. If you are doing something other than text classification,
          please browse <a rel="nofollow" href="https://github.com/huggingface/transformers/tree/main/examples/pytorch">../examples/pytorch</a>
          to find what works for you. Beware that if you want to train the largest
          bloom (bigscience/bloom), you will need several hundred gigabytes of GPU
          memory.</p>

          <p>If you want to do that in a modest setup, you can try <a rel="nofollow"
          href="https://github.com/bigscience-workshop/petals">https://github.com/bigscience-workshop/petals</a>
          for distributed training.</p>

          '
        raw: "Hi!\nYou fine-tune BLOOM the same way you fine-tune any other model\
          \ on HF.\n\nConsider the official example for text classification: https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\n\
          \nIn the readme, you can find `  --model_name_or_path bert-base-multilingual-cased\
          \ \\`\n\nIf you replace this line with `  --model_name_or_path bigscience/bloom-560m\
          \ \\`, \nyou will fine-tune the (smallest) bloom model on the dataset in\
          \ question. If you are doing something other than text classification, please\
          \ browse [../examples/pytorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch)\
          \ to find what works for you. Beware that if you want to train the largest\
          \ bloom (bigscience/bloom), you will need several hundred gigabytes of GPU\
          \ memory.\n\nIf you want to do that in a modest setup, you can try https://github.com/bigscience-workshop/petals\
          \ for distributed training."
        updatedAt: '2023-02-10T12:36:41.793Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - awacke1
        - gengxiao1216
        - Tykhist
        - shashanku
      - count: 1
        reaction: "\U0001F44D"
        users:
        - kusiko
    id: 63e63a595c3664766ebe1c69
    type: comment
  author: justheuristic
  content: "Hi!\nYou fine-tune BLOOM the same way you fine-tune any other model on\
    \ HF.\n\nConsider the official example for text classification: https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\n\
    \nIn the readme, you can find `  --model_name_or_path bert-base-multilingual-cased\
    \ \\`\n\nIf you replace this line with `  --model_name_or_path bigscience/bloom-560m\
    \ \\`, \nyou will fine-tune the (smallest) bloom model on the dataset in question.\
    \ If you are doing something other than text classification, please browse [../examples/pytorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch)\
    \ to find what works for you. Beware that if you want to train the largest bloom\
    \ (bigscience/bloom), you will need several hundred gigabytes of GPU memory.\n\
    \nIf you want to do that in a modest setup, you can try https://github.com/bigscience-workshop/petals\
    \ for distributed training."
  created_at: 2023-02-10 12:36:41+00:00
  edited: false
  hidden: false
  id: 63e63a595c3664766ebe1c69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4673f323abd25f8b24d9e226fa72116e.svg
      fullname: Nicolas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NicolasExo
      type: user
    createdAt: '2023-02-10T14:21:30.000Z'
    data:
      edited: true
      editors:
      - NicolasExo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4673f323abd25f8b24d9e226fa72116e.svg
          fullname: Nicolas
          isHf: false
          isPro: false
          name: NicolasExo
          type: user
        html: '<p>Justheuristic, thank you very much for your answer !  I am doing
          the text generation for my project and i would like to train the model Bloom
          on my own dataset. In this case should i browse the link ../examples/pytorch
          you have kindly provided in order to find the necessary information about
          it ?</p>

          <p>Thank you very much !</p>

          '
        raw: 'Justheuristic, thank you very much for your answer !  I am doing the
          text generation for my project and i would like to train the model Bloom
          on my own dataset. In this case should i browse the link ../examples/pytorch
          you have kindly provided in order to find the necessary information about
          it ?


          Thank you very much !'
        updatedAt: '2023-02-10T15:48:08.682Z'
      numEdits: 1
      reactions: []
    id: 63e652ea5c3664766ec18822
    type: comment
  author: NicolasExo
  content: 'Justheuristic, thank you very much for your answer !  I am doing the text
    generation for my project and i would like to train the model Bloom on my own
    dataset. In this case should i browse the link ../examples/pytorch you have kindly
    provided in order to find the necessary information about it ?


    Thank you very much !'
  created_at: 2023-02-10 14:21:30+00:00
  edited: true
  hidden: false
  id: 63e652ea5c3664766ec18822
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 187
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Training or Fine-tuning the Bloom AI Model on my own Dataset
