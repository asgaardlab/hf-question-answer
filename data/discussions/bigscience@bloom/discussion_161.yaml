!!python/object:huggingface_hub.community.DiscussionWithDetails
author: damc
conflicting_files: null
created_at: 2022-12-20 18:11:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acb3ad9737c7ec58b61bf0b30f744a2a.svg
      fullname: Damian Czapiewski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: damc
      type: user
    createdAt: '2022-12-20T18:11:31.000Z'
    data:
      edited: false
      editors:
      - damc
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/acb3ad9737c7ec58b61bf0b30f744a2a.svg
          fullname: Damian Czapiewski
          isHf: false
          isPro: false
          name: damc
          type: user
        html: '<p>What does it take to self-host Bloom?</p>

          <p>I would like to have an API endpoint where I can send the request with
          a prompt and get the output. What would it take to accomplish that? How
          much money would that cost presuming that I would generate let''s say 10
          000 tokens a day (about 40 000 characters)?</p>

          '
        raw: "What does it take to self-host Bloom?\r\n\r\nI would like to have an\
          \ API endpoint where I can send the request with a prompt and get the output.\
          \ What would it take to accomplish that? How much money would that cost\
          \ presuming that I would generate let's say 10 000 tokens a day (about 40\
          \ 000 characters)?"
        updatedAt: '2022-12-20T18:11:31.282Z'
      numEdits: 0
      reactions: []
    id: 63a1fad33c003e4093175e06
    type: comment
  author: damc
  content: "What does it take to self-host Bloom?\r\n\r\nI would like to have an API\
    \ endpoint where I can send the request with a prompt and get the output. What\
    \ would it take to accomplish that? How much money would that cost presuming that\
    \ I would generate let's say 10 000 tokens a day (about 40 000 characters)?"
  created_at: 2022-12-20 18:11:31+00:00
  edited: false
  hidden: false
  id: 63a1fad33c003e4093175e06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/acb3ad9737c7ec58b61bf0b30f744a2a.svg
      fullname: Damian Czapiewski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: damc
      type: user
    createdAt: '2022-12-20T18:11:44.000Z'
    data:
      from: What does it take to self-host Bloom?
      to: What does it take to self-host Bloom? How much money would that cost?
    id: 63a1fae0e36f2e4d5b136d36
    type: title-change
  author: damc
  created_at: 2022-12-20 18:11:44+00:00
  id: 63a1fae0e36f2e4d5b136d36
  new_title: What does it take to self-host Bloom? How much money would that cost?
  old_title: What does it take to self-host Bloom?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d22d0f315358abe06ace14327dd09f32.svg
      fullname: Juliano Rodrigo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joojano
      type: user
    createdAt: '2022-12-21T16:25:23.000Z'
    data:
      edited: false
      editors:
      - joojano
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d22d0f315358abe06ace14327dd09f32.svg
          fullname: Juliano Rodrigo
          isHf: false
          isPro: false
          name: joojano
          type: user
        html: '<p>I saw somewhere that the full Bloom model, with 175B parameters,
          will use around 350GB of GPU RAM, so it''ll cost a lot. Let''s see.</p>

          <p>If we use <a rel="nofollow" href="https://www.vultr.com/pricing/#cloud-gpu">Vultr
          pricing</a> as example, and considering that we need around 350GB of GPU
          RAM because we''ll deploy the full Bloom model, we can get 1 instance of
          their 4xNvidia A100 (320GB GPU RAM) that costs USD10.417/hour (or USD 7,000/month),
          plus 1 instance of 1/2 Nvidia A100 (40GB GPU RAM) that costs USD1.302/hour
          (or USD875/month).</p>

          <p>This setup will have around 360GB of GPU, so that should be enough to
          start. If we sum USD7,000 + USD875 = USD7,875 per month, or 10.417+1.302=
          USD11.719 per hour to run this model self hosted with a reasonable response
          time.</p>

          <p>Of course this presumes that you''ll use the full model, if you use less
          parameters (eg. bloom-3b) you can greatly decrease your resouce needs, like
          in <a rel="nofollow" href="https://towardsdatascience.com/lets-bloom-with-bigscience-s-new-ai-model-803b1a0d677">this
          article</a>, that deploys the 3B model into Amazon SageMaker. They''re using
          the m1.g4dn.xlarge, that is USD 0.7364 per hour to run (around USD 530 per
          month).</p>

          <p>You''ll need to check if the full Bloom model is really needed, so you''ll
          need to check other smaller Bloom models to run a cost-effective inference
          API.</p>

          '
        raw: 'I saw somewhere that the full Bloom model, with 175B parameters, will
          use around 350GB of GPU RAM, so it''ll cost a lot. Let''s see.


          If we use [Vultr pricing](https://www.vultr.com/pricing/#cloud-gpu) as example,
          and considering that we need around 350GB of GPU RAM because we''ll deploy
          the full Bloom model, we can get 1 instance of their 4xNvidia A100 (320GB
          GPU RAM) that costs USD10.417/hour (or USD 7,000/month), plus 1 instance
          of 1/2 Nvidia A100 (40GB GPU RAM) that costs USD1.302/hour (or USD875/month).


          This setup will have around 360GB of GPU, so that should be enough to start.
          If we sum USD7,000 + USD875 = USD7,875 per month, or 10.417+1.302= USD11.719
          per hour to run this model self hosted with a reasonable response time.


          Of course this presumes that you''ll use the full model, if you use less
          parameters (eg. bloom-3b) you can greatly decrease your resouce needs, like
          in [this article](https://towardsdatascience.com/lets-bloom-with-bigscience-s-new-ai-model-803b1a0d677),
          that deploys the 3B model into Amazon SageMaker. They''re using the m1.g4dn.xlarge,
          that is USD 0.7364 per hour to run (around USD 530 per month).


          You''ll need to check if the full Bloom model is really needed, so you''ll
          need to check other smaller Bloom models to run a cost-effective inference
          API.'
        updatedAt: '2022-12-21T16:25:23.824Z'
      numEdits: 0
      reactions:
      - count: 12
        reaction: "\U0001F44D"
        users:
        - damc
        - RichardAnd
        - julien-c
        - greco
        - Kagerage
        - amrezo
        - sujithjay
        - IceyPickle
        - ansonlam
        - Sidd07
        - julian-schelb
        - itsamamir
    id: 63a33373b5fc9ab9f63d97f7
    type: comment
  author: joojano
  content: 'I saw somewhere that the full Bloom model, with 175B parameters, will
    use around 350GB of GPU RAM, so it''ll cost a lot. Let''s see.


    If we use [Vultr pricing](https://www.vultr.com/pricing/#cloud-gpu) as example,
    and considering that we need around 350GB of GPU RAM because we''ll deploy the
    full Bloom model, we can get 1 instance of their 4xNvidia A100 (320GB GPU RAM)
    that costs USD10.417/hour (or USD 7,000/month), plus 1 instance of 1/2 Nvidia
    A100 (40GB GPU RAM) that costs USD1.302/hour (or USD875/month).


    This setup will have around 360GB of GPU, so that should be enough to start. If
    we sum USD7,000 + USD875 = USD7,875 per month, or 10.417+1.302= USD11.719 per
    hour to run this model self hosted with a reasonable response time.


    Of course this presumes that you''ll use the full model, if you use less parameters
    (eg. bloom-3b) you can greatly decrease your resouce needs, like in [this article](https://towardsdatascience.com/lets-bloom-with-bigscience-s-new-ai-model-803b1a0d677),
    that deploys the 3B model into Amazon SageMaker. They''re using the m1.g4dn.xlarge,
    that is USD 0.7364 per hour to run (around USD 530 per month).


    You''ll need to check if the full Bloom model is really needed, so you''ll need
    to check other smaller Bloom models to run a cost-effective inference API.'
  created_at: 2022-12-21 16:25:23+00:00
  edited: false
  hidden: false
  id: 63a33373b5fc9ab9f63d97f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acb3ad9737c7ec58b61bf0b30f744a2a.svg
      fullname: Damian Czapiewski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: damc
      type: user
    createdAt: '2022-12-21T18:13:53.000Z'
    data:
      edited: false
      editors:
      - damc
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/acb3ad9737c7ec58b61bf0b30f744a2a.svg
          fullname: Damian Czapiewski
          isHf: false
          isPro: false
          name: damc
          type: user
        html: '<p>Thanks for your great answer.</p>

          '
        raw: Thanks for your great answer.
        updatedAt: '2022-12-21T18:13:53.664Z'
      numEdits: 0
      reactions: []
    id: 63a34ce1b5fc9ab9f640bdbe
    type: comment
  author: damc
  content: Thanks for your great answer.
  created_at: 2022-12-21 18:13:53+00:00
  edited: false
  hidden: false
  id: 63a34ce1b5fc9ab9f640bdbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1687e20278bcc4d2afabde1bedb1ca4a.svg
      fullname: Valentino Leonardo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fusseldieb
      type: user
    createdAt: '2022-12-21T21:34:01.000Z'
    data:
      edited: true
      editors:
      - Fusseldieb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1687e20278bcc4d2afabde1bedb1ca4a.svg
          fullname: Valentino Leonardo
          isHf: false
          isPro: false
          name: Fusseldieb
          type: user
        html: '<p>I''ve read in another thread that there are Tesla M40''s (24GB),
          which are older, however, do have whooping 24GB. Since they are reasonably
          priced at $150-180 "open package", purchasing a load of them should get
          you to 350GB.</p>

          <p>However, the question remains:<br>Would that work at a reasonable speed
          (at least 1 second per token)? Could multiple GPUs work together, if hooked
          up to the same system (think of like a mining rig)?<br>If yes, I could see
          people building this. I could even see myself building this :)<br>But I
          think almost nobody has $3k to just "play around and find out", so a definitive
          answer would be great.</p>

          '
        raw: 'I''ve read in another thread that there are Tesla M40''s (24GB), which
          are older, however, do have whooping 24GB. Since they are reasonably priced
          at $150-180 "open package", purchasing a load of them should get you to
          350GB.


          However, the question remains:

          Would that work at a reasonable speed (at least 1 second per token)? Could
          multiple GPUs work together, if hooked up to the same system (think of like
          a mining rig)?

          If yes, I could see people building this. I could even see myself building
          this :)

          But I think almost nobody has $3k to just "play around and find out", so
          a definitive answer would be great.'
        updatedAt: '2022-12-21T21:40:57.821Z'
      numEdits: 6
      reactions: []
    id: 63a37bc9b5fc9ab9f6465ccd
    type: comment
  author: Fusseldieb
  content: 'I''ve read in another thread that there are Tesla M40''s (24GB), which
    are older, however, do have whooping 24GB. Since they are reasonably priced at
    $150-180 "open package", purchasing a load of them should get you to 350GB.


    However, the question remains:

    Would that work at a reasonable speed (at least 1 second per token)? Could multiple
    GPUs work together, if hooked up to the same system (think of like a mining rig)?

    If yes, I could see people building this. I could even see myself building this
    :)

    But I think almost nobody has $3k to just "play around and find out", so a definitive
    answer would be great.'
  created_at: 2022-12-21 21:34:01+00:00
  edited: true
  hidden: false
  id: 63a37bc9b5fc9ab9f6465ccd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
      fullname: Jan Badertscher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: underlines
      type: user
    createdAt: '2022-12-22T06:27:55.000Z'
    data:
      edited: false
      editors:
      - underlines
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
          fullname: Jan Badertscher
          isHf: false
          isPro: false
          name: underlines
          type: user
        html: '<p>I see so much optimization going on for transformer based models
          like StableDiffusion and LLMs like Bloom.</p>

          <ul>

          <li>Run it 8bit in half precision</li>

          <li>use xformers</li>

          <li>run on DeepSpeed-MII for a 40x cost reduction for Bloom on Azure <a
          rel="nofollow" href="https://github.com/microsoft/DeepSpeed-MII">https://github.com/microsoft/DeepSpeed-MII</a></li>

          </ul>

          <p>But so far I couldn''t deploy it on Azure. No compute units available
          with more than 12GB RAM and I have to manually request higher quota, which
          doesn''t work with my free trial account (containing 200USD)</p>

          '
        raw: 'I see so much optimization going on for transformer based models like
          StableDiffusion and LLMs like Bloom.

          - Run it 8bit in half precision

          - use xformers

          - run on DeepSpeed-MII for a 40x cost reduction for Bloom on Azure https://github.com/microsoft/DeepSpeed-MII


          But so far I couldn''t deploy it on Azure. No compute units available with
          more than 12GB RAM and I have to manually request higher quota, which doesn''t
          work with my free trial account (containing 200USD)'
        updatedAt: '2022-12-22T06:27:55.174Z'
      numEdits: 0
      reactions: []
    id: 63a3f8ebf460e4379b5b0cf2
    type: comment
  author: underlines
  content: 'I see so much optimization going on for transformer based models like
    StableDiffusion and LLMs like Bloom.

    - Run it 8bit in half precision

    - use xformers

    - run on DeepSpeed-MII for a 40x cost reduction for Bloom on Azure https://github.com/microsoft/DeepSpeed-MII


    But so far I couldn''t deploy it on Azure. No compute units available with more
    than 12GB RAM and I have to manually request higher quota, which doesn''t work
    with my free trial account (containing 200USD)'
  created_at: 2022-12-22 06:27:55+00:00
  edited: false
  hidden: false
  id: 63a3f8ebf460e4379b5b0cf2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
      fullname: Alexander Borzunov
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: borzunov
      type: user
    createdAt: '2023-01-18T01:57:16.000Z'
    data:
      edited: false
      editors:
      - borzunov
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
          fullname: Alexander Borzunov
          isHf: false
          isPro: false
          name: borzunov
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;damc&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/damc\">@<span class=\"\
          underline\">damc</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;Fusseldieb&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/Fusseldieb\">@<span class=\"underline\"\
          >Fusseldieb</span></a></span>\n\n\t</span></span> </p>\n<p>Check out Petals:\
          \ <a rel=\"nofollow\" href=\"https://github.com/bigscience-workshop/petals\"\
          >https://github.com/bigscience-workshop/petals</a> It gives the inference\
          \ speed of ~1 sec/token for BLOOM-176B.</p>\n<p>There is also an HTTP/WebSocket\
          \ API endpoint: <a rel=\"nofollow\" href=\"https://github.com/borzunov/chat.petals.ml\"\
          >https://github.com/borzunov/chat.petals.ml</a></p>\n"
        raw: "@damc @Fusseldieb \n\nCheck out Petals: https://github.com/bigscience-workshop/petals\
          \ It gives the inference speed of ~1 sec/token for BLOOM-176B.\n\nThere\
          \ is also an HTTP/WebSocket API endpoint: https://github.com/borzunov/chat.petals.ml"
        updatedAt: '2023-01-18T01:57:16.338Z'
      numEdits: 0
      reactions: []
    id: 63c751fc2f651b676295c21f
    type: comment
  author: borzunov
  content: "@damc @Fusseldieb \n\nCheck out Petals: https://github.com/bigscience-workshop/petals\
    \ It gives the inference speed of ~1 sec/token for BLOOM-176B.\n\nThere is also\
    \ an HTTP/WebSocket API endpoint: https://github.com/borzunov/chat.petals.ml"
  created_at: 2023-01-18 01:57:16+00:00
  edited: false
  hidden: false
  id: 63c751fc2f651b676295c21f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acb3ad9737c7ec58b61bf0b30f744a2a.svg
      fullname: Damian Czapiewski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: damc
      type: user
    createdAt: '2023-01-18T09:47:33.000Z'
    data:
      edited: false
      editors:
      - damc
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/acb3ad9737c7ec58b61bf0b30f744a2a.svg
          fullname: Damian Czapiewski
          isHf: false
          isPro: false
          name: damc
          type: user
        html: '<p>Thank you for sharing, looks interesting.</p>

          '
        raw: Thank you for sharing, looks interesting.
        updatedAt: '2023-01-18T09:47:33.380Z'
      numEdits: 0
      reactions: []
    id: 63c7c03553af58c39913a26c
    type: comment
  author: damc
  content: Thank you for sharing, looks interesting.
  created_at: 2023-01-18 09:47:33+00:00
  edited: false
  hidden: false
  id: 63c7c03553af58c39913a26c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
      fullname: Kiran Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KiranAli
      type: user
    createdAt: '2023-04-15T18:22:50.000Z'
    data:
      edited: false
      editors:
      - KiranAli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7c1bd86b21195a9e351b705ff25dc66.svg
          fullname: Kiran Ali
          isHf: false
          isPro: false
          name: KiranAli
          type: user
        html: '<p>Does sharding help to run it on a low instance profile? Anyone tried
          this blog </p>

          <p><a rel="nofollow" href="https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32">https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32</a></p>

          '
        raw: "Does sharding help to run it on a low instance profile? Anyone tried\
          \ this blog \n\nhttps://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32"
        updatedAt: '2023-04-15T18:22:50.458Z'
      numEdits: 0
      reactions: []
    id: 643aeb7af6ef50c310d846a9
    type: comment
  author: KiranAli
  content: "Does sharding help to run it on a low instance profile? Anyone tried this\
    \ blog \n\nhttps://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32"
  created_at: 2023-04-15 17:22:50+00:00
  edited: false
  hidden: false
  id: 643aeb7af6ef50c310d846a9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 161
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: What does it take to self-host Bloom? How much money would that cost?
