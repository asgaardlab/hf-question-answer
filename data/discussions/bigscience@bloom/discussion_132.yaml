!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mishavee
conflicting_files: null
created_at: 2022-10-23 15:00:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-10-23T16:00:50.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>How does hugging face have so many hosted apis running at once?
          It is literally a hundred thousand or even more models hosted. Do they load
          the models when needed, taking turns using available resources? If I were
          to have only Bloom loaded would there be a significant reduction in the
          time it takes to get a result vs through the hosted api? Also are many requests
          coming in concurrently to the model in the hosted api, slowing it down or
          are requests rare in the hosted api? </p>

          '
        raw: 'How does hugging face have so many hosted apis running at once? It is
          literally a hundred thousand or even more models hosted. Do they load the
          models when needed, taking turns using available resources? If I were to
          have only Bloom loaded would there be a significant reduction in the time
          it takes to get a result vs through the hosted api? Also are many requests
          coming in concurrently to the model in the hosted api, slowing it down or
          are requests rare in the hosted api? '
        updatedAt: '2022-10-23T16:00:50.448Z'
      numEdits: 0
      reactions: []
    id: 635565329c72a7e742f13399
    type: comment
  author: mishavee
  content: 'How does hugging face have so many hosted apis running at once? It is
    literally a hundred thousand or even more models hosted. Do they load the models
    when needed, taking turns using available resources? If I were to have only Bloom
    loaded would there be a significant reduction in the time it takes to get a result
    vs through the hosted api? Also are many requests coming in concurrently to the
    model in the hosted api, slowing it down or are requests rare in the hosted api? '
  created_at: 2022-10-23 15:00:50+00:00
  edited: false
  hidden: false
  id: 635565329c72a7e742f13399
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-10-23T22:54:47.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<blockquote>\n<p>How does hugging face have so many hosted apis running\
          \ at once? It is literally a hundred thousand or even more models hosted.\
          \ Do they load the models when needed, taking turns using available resources?</p>\n\
          </blockquote>\n<p>I don't know specifically. Though let's try to keep discussion\
          \ on this repository BLOOM related. I think it would be better suited somewhere\
          \ else, adressed to the people responsible of infrastructure as Hugging\
          \ Face.</p>\n<blockquote>\n<p>If I were to have only Bloom loaded would\
          \ there be a significant reduction in the time it takes to get a result\
          \ vs through the hosted api?</p>\n</blockquote>\n<p>If you were to deploy\
          \ BLOOM yourself, you'll probably see much lower latency as you could decide\
          \ where you deploy it, probably have a lot more fine grained control in\
          \ terms of what you want (which inference algorithm, remove the limit in\
          \ length etc ...). We didn't so too many specific things, <span data-props=\"\
          {&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Narsil\">@<span class=\"underline\">Narsil</span></a></span>\n\
          \n\t</span></span> has written a great blog about how we deployed it: <a\
          \ href=\"https://huggingface.co/blog/bloom-inference-optimization\">https://huggingface.co/blog/bloom-inference-optimization</a>\
          \ and the code is available here: <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers_bloom_parallel\"\
          >https://github.com/huggingface/transformers_bloom_parallel</a> </p>\n<blockquote>\n\
          <p>Also are many requests coming in concurrently to the model in the hosted\
          \ api, slowing it down or are requests rare in the hosted api?</p>\n</blockquote>\n\
          <p>Yes requests are coming concurrently, I don't think we provide specific\
          \ numbers about how the API is behaving. Is your worry that your requests\
          \ are going to be too slow? We make the best effort so that you get the\
          \ fastest inference, but it's never going to beat you deploying it yourself.</p>\n"
        raw: "> How does hugging face have so many hosted apis running at once? It\
          \ is literally a hundred thousand or even more models hosted. Do they load\
          \ the models when needed, taking turns using available resources?\n\nI don't\
          \ know specifically. Though let's try to keep discussion on this repository\
          \ BLOOM related. I think it would be better suited somewhere else, adressed\
          \ to the people responsible of infrastructure as Hugging Face.\n\n> If I\
          \ were to have only Bloom loaded would there be a significant reduction\
          \ in the time it takes to get a result vs through the hosted api?\n\nIf\
          \ you were to deploy BLOOM yourself, you'll probably see much lower latency\
          \ as you could decide where you deploy it, probably have a lot more fine\
          \ grained control in terms of what you want (which inference algorithm,\
          \ remove the limit in length etc ...). We didn't so too many specific things,\
          \ @Narsil has written a great blog about how we deployed it: https://huggingface.co/blog/bloom-inference-optimization\
          \ and the code is available here: https://github.com/huggingface/transformers_bloom_parallel\
          \ \n\n> Also are many requests coming in concurrently to the model in the\
          \ hosted api, slowing it down or are requests rare in the hosted api?\n\n\
          Yes requests are coming concurrently, I don't think we provide specific\
          \ numbers about how the API is behaving. Is your worry that your requests\
          \ are going to be too slow? We make the best effort so that you get the\
          \ fastest inference, but it's never going to beat you deploying it yourself."
        updatedAt: '2022-10-23T22:54:47.653Z'
      numEdits: 0
      reactions: []
    id: 6355c6376b58fa7cc871f20b
    type: comment
  author: TimeRobber
  content: "> How does hugging face have so many hosted apis running at once? It is\
    \ literally a hundred thousand or even more models hosted. Do they load the models\
    \ when needed, taking turns using available resources?\n\nI don't know specifically.\
    \ Though let's try to keep discussion on this repository BLOOM related. I think\
    \ it would be better suited somewhere else, adressed to the people responsible\
    \ of infrastructure as Hugging Face.\n\n> If I were to have only Bloom loaded\
    \ would there be a significant reduction in the time it takes to get a result\
    \ vs through the hosted api?\n\nIf you were to deploy BLOOM yourself, you'll probably\
    \ see much lower latency as you could decide where you deploy it, probably have\
    \ a lot more fine grained control in terms of what you want (which inference algorithm,\
    \ remove the limit in length etc ...). We didn't so too many specific things,\
    \ @Narsil has written a great blog about how we deployed it: https://huggingface.co/blog/bloom-inference-optimization\
    \ and the code is available here: https://github.com/huggingface/transformers_bloom_parallel\
    \ \n\n> Also are many requests coming in concurrently to the model in the hosted\
    \ api, slowing it down or are requests rare in the hosted api?\n\nYes requests\
    \ are coming concurrently, I don't think we provide specific numbers about how\
    \ the API is behaving. Is your worry that your requests are going to be too slow?\
    \ We make the best effort so that you get the fastest inference, but it's never\
    \ going to beat you deploying it yourself."
  created_at: 2022-10-23 21:54:47+00:00
  edited: false
  hidden: false
  id: 6355c6376b58fa7cc871f20b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2022-10-28T09:31:34.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: "<blockquote>\n<p>Do they load the models when needed, taking turns\
          \ using available resources?</p>\n</blockquote>\n<p>Exactly done that way.\
          \ We use kubernetes, share ressources as much as possible (including GPUs\
          \ whihc comes with it's own share of issues).</p>\n<blockquote>\n<p>If I\
          \ were to have only Bloom loaded would there be a significant reduction\
          \ in the time it takes to get a result vs through the hosted api?</p>\n\
          </blockquote>\n<p>Not necessarily. First of all you would have to recreate\
          \ some of the engineering. <span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\"\
          >@<span class=\"underline\">TimeRobber</span></a></span>\n\n\t</span></span>\
          \ gave you the correct information were to get started, but it might require\
          \ additional work and at least understanding what we have done. That being\
          \ said, once you're in control of the code, you do have a lot more flexibility\
          \ to adapt to your own use case. For instance the API is optimized to run\
          \ on demand on a wide variety of requests and do dynamic batching. This\
          \ might not be useful to you.</p>\n<p>We have gone to great lengths to make\
          \ the API as fast as possible.<br>Bloom is now generously sponsored by <code>AzureML</code>\
          \ which is a huge plus for the community.</p>\n<p>If you're seeing any bad\
          \ latency or overload please mention it here, we did move recently so their\
          \ might be some specific quirks and options we haven't properly tuned yet.</p>\n"
        raw: "> Do they load the models when needed, taking turns using available\
          \ resources?\n\nExactly done that way. We use kubernetes, share ressources\
          \ as much as possible (including GPUs whihc comes with it's own share of\
          \ issues).\n\n> If I were to have only Bloom loaded would there be a significant\
          \ reduction in the time it takes to get a result vs through the hosted api?\n\
          \nNot necessarily. First of all you would have to recreate some of the engineering.\
          \ @TimeRobber gave you the correct information were to get started, but\
          \ it might require additional work and at least understanding what we have\
          \ done. That being said, once you're in control of the code, you do have\
          \ a lot more flexibility to adapt to your own use case. For instance the\
          \ API is optimized to run on demand on a wide variety of requests and do\
          \ dynamic batching. This might not be useful to you.\n\nWe have gone to\
          \ great lengths to make the API as fast as possible. \nBloom is now generously\
          \ sponsored by `AzureML` which is a huge plus for the community.\n\nIf you're\
          \ seeing any bad latency or overload please mention it here, we did move\
          \ recently so their might be some specific quirks and options we haven't\
          \ properly tuned yet."
        updatedAt: '2022-10-28T09:31:34.612Z'
      numEdits: 0
      reactions: []
    id: 635ba176e1e7ef214266880d
    type: comment
  author: Narsil
  content: "> Do they load the models when needed, taking turns using available resources?\n\
    \nExactly done that way. We use kubernetes, share ressources as much as possible\
    \ (including GPUs whihc comes with it's own share of issues).\n\n> If I were to\
    \ have only Bloom loaded would there be a significant reduction in the time it\
    \ takes to get a result vs through the hosted api?\n\nNot necessarily. First of\
    \ all you would have to recreate some of the engineering. @TimeRobber gave you\
    \ the correct information were to get started, but it might require additional\
    \ work and at least understanding what we have done. That being said, once you're\
    \ in control of the code, you do have a lot more flexibility to adapt to your\
    \ own use case. For instance the API is optimized to run on demand on a wide variety\
    \ of requests and do dynamic batching. This might not be useful to you.\n\nWe\
    \ have gone to great lengths to make the API as fast as possible. \nBloom is now\
    \ generously sponsored by `AzureML` which is a huge plus for the community.\n\n\
    If you're seeing any bad latency or overload please mention it here, we did move\
    \ recently so their might be some specific quirks and options we haven't properly\
    \ tuned yet."
  created_at: 2022-10-28 08:31:34+00:00
  edited: false
  hidden: false
  id: 635ba176e1e7ef214266880d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-10-28T16:47:48.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>What exactly do you mean by "adapt to your own case"? Do you mean
          fine tune? How would fine tuning make it work faster?</p>

          '
        raw: What exactly do you mean by "adapt to your own case"? Do you mean fine
          tune? How would fine tuning make it work faster?
        updatedAt: '2022-10-28T16:47:48.053Z'
      numEdits: 0
      reactions: []
    id: 635c07b437c6a2c12e32694e
    type: comment
  author: mishavee
  content: What exactly do you mean by "adapt to your own case"? Do you mean fine
    tune? How would fine tuning make it work faster?
  created_at: 2022-10-28 15:47:48+00:00
  edited: false
  hidden: false
  id: 635c07b437c6a2c12e32694e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-10-28T17:22:03.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>Are you saying that the fact that Bloom needs to be actually loaded
          into the shared resources doesn''t make a request much longer? I understand
          if it is already loaded then there would still be a delay but not as big
          since other people are sending requests but if Bloom hasn''t been loaded
          in you are saying " not necessarily". Is that true for that scenario when
          it hasn''t been loaded?</p>

          '
        raw: Are you saying that the fact that Bloom needs to be actually loaded into
          the shared resources doesn't make a request much longer? I understand if
          it is already loaded then there would still be a delay but not as big since
          other people are sending requests but if Bloom hasn't been loaded in you
          are saying " not necessarily". Is that true for that scenario when it hasn't
          been loaded?
        updatedAt: '2022-10-28T17:22:03.482Z'
      numEdits: 0
      reactions: []
    id: 635c0fbb7a165601151c1a80
    type: comment
  author: mishavee
  content: Are you saying that the fact that Bloom needs to be actually loaded into
    the shared resources doesn't make a request much longer? I understand if it is
    already loaded then there would still be a delay but not as big since other people
    are sending requests but if Bloom hasn't been loaded in you are saying " not necessarily".
    Is that true for that scenario when it hasn't been loaded?
  created_at: 2022-10-28 16:22:03+00:00
  edited: false
  hidden: false
  id: 635c0fbb7a165601151c1a80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2022-10-31T13:45:03.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<blockquote>

          <p>What exactly do you mean by "adapt to your own case"? Do you mean fine
          tune? How would fine tuning make it work faster?</p>

          </blockquote>

          <p>I mean if you are running big batches, or just small requests, or always
          with the same parameters etc.. Then your spec are different from the API
          (which aims to be generalist).<br>When you have a specific enough use case,
          some optimizations can probably be made to get things running faster. For
          instance, using batching improves a lot throughput, but usually it reduces
          latency, so you could try to batch aggressively if you want massive throughput.</p>

          <blockquote>

          <p>Are you saying that the fact that Bloom needs to be actually loaded into
          the shared resources doesn''t make a request much longer? I understand if
          it is already loaded then there would still be a delay but not as big since
          other people are sending requests but if Bloom hasn''t been loaded in you
          are saying " not necessarily". Is that true for that scenario when it hasn''t
          been loaded?</p>

          </blockquote>

          <p>I am not sure I understand exactly.<br>Bloom is really big, so really
          slow to load. It takes ~1mn under optimal conditions for instance. So it''s
          not really realistic to load it "on demand". It''s always up in the API
          for that reason.</p>

          <p>Bloom is loaded and it''s ALONE on the hardware hosting it, so it''s
          not in a shared pool of resources, so I don''t think anything would change
          if you were to host it on dedicated resources.</p>

          <p>Does that answer your question ?</p>

          '
        raw: '> What exactly do you mean by "adapt to your own case"? Do you mean
          fine tune? How would fine tuning make it work faster?


          I mean if you are running big batches, or just small requests, or always
          with the same parameters etc.. Then your spec are different from the API
          (which aims to be generalist).

          When you have a specific enough use case, some optimizations can probably
          be made to get things running faster. For instance, using batching improves
          a lot throughput, but usually it reduces latency, so you could try to batch
          aggressively if you want massive throughput.


          > Are you saying that the fact that Bloom needs to be actually loaded into
          the shared resources doesn''t make a request much longer? I understand if
          it is already loaded then there would still be a delay but not as big since
          other people are sending requests but if Bloom hasn''t been loaded in you
          are saying " not necessarily". Is that true for that scenario when it hasn''t
          been loaded?


          I am not sure I understand exactly.

          Bloom is really big, so really slow to load. It takes ~1mn under optimal
          conditions for instance. So it''s not really realistic to load it "on demand".
          It''s always up in the API for that reason.


          Bloom is loaded and it''s ALONE on the hardware hosting it, so it''s not
          in a shared pool of resources, so I don''t think anything would change if
          you were to host it on dedicated resources.


          Does that answer your question ?'
        updatedAt: '2022-10-31T13:45:03.305Z'
      numEdits: 0
      reactions: []
    id: 635fd15f88e41d249ec9e72b
    type: comment
  author: Narsil
  content: '> What exactly do you mean by "adapt to your own case"? Do you mean fine
    tune? How would fine tuning make it work faster?


    I mean if you are running big batches, or just small requests, or always with
    the same parameters etc.. Then your spec are different from the API (which aims
    to be generalist).

    When you have a specific enough use case, some optimizations can probably be made
    to get things running faster. For instance, using batching improves a lot throughput,
    but usually it reduces latency, so you could try to batch aggressively if you
    want massive throughput.


    > Are you saying that the fact that Bloom needs to be actually loaded into the
    shared resources doesn''t make a request much longer? I understand if it is already
    loaded then there would still be a delay but not as big since other people are
    sending requests but if Bloom hasn''t been loaded in you are saying " not necessarily".
    Is that true for that scenario when it hasn''t been loaded?


    I am not sure I understand exactly.

    Bloom is really big, so really slow to load. It takes ~1mn under optimal conditions
    for instance. So it''s not really realistic to load it "on demand". It''s always
    up in the API for that reason.


    Bloom is loaded and it''s ALONE on the hardware hosting it, so it''s not in a
    shared pool of resources, so I don''t think anything would change if you were
    to host it on dedicated resources.


    Does that answer your question ?'
  created_at: 2022-10-31 12:45:03+00:00
  edited: false
  hidden: false
  id: 635fd15f88e41d249ec9e72b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-10-31T14:40:57.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>partly, ty<br>what is mn? </p>

          <p>what is batching and how do you do it?</p>

          '
        raw: "partly, ty\nwhat is mn? \n\nwhat is batching and how do you do it?"
        updatedAt: '2022-10-31T14:40:57.027Z'
      numEdits: 0
      reactions: []
    id: 635fde793605bd411c15cf25
    type: comment
  author: mishavee
  content: "partly, ty\nwhat is mn? \n\nwhat is batching and how do you do it?"
  created_at: 2022-10-31 13:40:57+00:00
  edited: false
  hidden: false
  id: 635fde793605bd411c15cf25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-10-31T17:57:21.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<blockquote>

          <p>what is mn?</p>

          </blockquote>

          <p>Minute</p>

          <blockquote>

          <p>what is batching and how do you do it?</p>

          </blockquote>

          <p>It''s essentially the ability to pass multiple samples at once, instead
          of processing them individually. It''s a core concept in Deep Learning as
          it increases significantly the speed at which you process entire datasets.</p>

          '
        raw: '> what is mn?


          Minute


          > what is batching and how do you do it?


          It''s essentially the ability to pass multiple samples at once, instead
          of processing them individually. It''s a core concept in Deep Learning as
          it increases significantly the speed at which you process entire datasets.'
        updatedAt: '2022-10-31T17:57:21.516Z'
      numEdits: 0
      reactions: []
    id: 63600c81c579d1b2c66845b7
    type: comment
  author: TimeRobber
  content: '> what is mn?


    Minute


    > what is batching and how do you do it?


    It''s essentially the ability to pass multiple samples at once, instead of processing
    them individually. It''s a core concept in Deep Learning as it increases significantly
    the speed at which you process entire datasets.'
  created_at: 2022-10-31 16:57:21+00:00
  edited: false
  hidden: false
  id: 63600c81c579d1b2c66845b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-10-31T18:05:01.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>ty, </p>

          <p>can you give me an example of passing multiple samples at the same time
          for example with paraphrasing or anything. How is this possible? I thought
          it just continues text from your input. How could it continue text from
          several points?</p>

          '
        raw: "ty, \n\ncan you give me an example of passing multiple samples at the\
          \ same time for example with paraphrasing or anything. How is this possible?\
          \ I thought it just continues text from your input. How could it continue\
          \ text from several points?"
        updatedAt: '2022-10-31T18:05:01.333Z'
      numEdits: 0
      reactions: []
    id: 63600e4dd9e4214b9a679b87
    type: comment
  author: mishavee
  content: "ty, \n\ncan you give me an example of passing multiple samples at the\
    \ same time for example with paraphrasing or anything. How is this possible? I\
    \ thought it just continues text from your input. How could it continue text from\
    \ several points?"
  created_at: 2022-10-31 17:05:01+00:00
  edited: false
  hidden: false
  id: 63600e4dd9e4214b9a679b87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2022-11-01T06:07:38.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<p>At this point, I would suggest you try out lectures and books.<br>For
          instance this: <a rel="nofollow" href="https://www.youtube.com/watch?v=M6adb1j2jPI">https://www.youtube.com/watch?v=M6adb1j2jPI</a></p>

          '
        raw: 'At this point, I would suggest you try out lectures and books.

          For instance this: https://www.youtube.com/watch?v=M6adb1j2jPI'
        updatedAt: '2022-11-01T06:07:38.215Z'
      numEdits: 0
      reactions: []
    id: 6360b7aa9ddc44e710e32121
    type: comment
  author: Narsil
  content: 'At this point, I would suggest you try out lectures and books.

    For instance this: https://www.youtube.com/watch?v=M6adb1j2jPI'
  created_at: 2022-11-01 05:07:38+00:00
  edited: false
  hidden: false
  id: 6360b7aa9ddc44e710e32121
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-11-01T12:35:28.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>I watched. I don''t understand why having the same length sentences
          would optimize the speed?</p>

          '
        raw: I watched. I don't understand why having the same length sentences would
          optimize the speed?
        updatedAt: '2022-11-01T12:35:28.866Z'
      numEdits: 0
      reactions: []
    id: 636112906bd72c97d0fe080d
    type: comment
  author: mishavee
  content: I watched. I don't understand why having the same length sentences would
    optimize the speed?
  created_at: 2022-11-01 11:35:28+00:00
  edited: false
  hidden: false
  id: 636112906bd72c97d0fe080d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2022-11-01T16:30:42.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<blockquote>

          <p>I watched. I don''t understand why having the same length sentences would
          optimize the speed?</p>

          </blockquote>

          <p>Speed doesn''t really mean anything in a webserver context, without specifying
          the speed of what.<br>There''s 2 core concepts.<br>Latency: How fast to
          get your answer once you sent your request. (It''s client speed let''s say).<br>Throughput:
          How fast does the server take to server N requests (It''s a server''s view).</p>

          <p>Both are really important. </p>

          <p>And GPU are really good at throughput (you can add more calculation to
          do at once, and all will be calculated in pretty much at the same speed
          as on a single request).<br>Now the topic is fast, and as I said before,
          just go and check out blogs, books and videos that provide explanation about
          this topic.<br>This is a vast topic and answering is not really feasible.</p>

          <p>Also you can ask questions here: <a rel="nofollow" href="https://discuss.huggingface.co/">https://discuss.huggingface.co/</a><br>Or
          our discord server: <a rel="nofollow" href="https://discord.com/invite/JfAtkvEtRb">https://discord.com/invite/JfAtkvEtRb</a></p>

          '
        raw: "> I watched. I don't understand why having the same length sentences\
          \ would optimize the speed?\n\nSpeed doesn't really mean anything in a webserver\
          \ context, without specifying the speed of what.\nThere's 2 core concepts.\n\
          Latency: How fast to get your answer once you sent your request. (It's client\
          \ speed let's say).\nThroughput: How fast does the server take to server\
          \ N requests (It's a server's view).\n\nBoth are really important. \n\n\
          And GPU are really good at throughput (you can add more calculation to do\
          \ at once, and all will be calculated in pretty much at the same speed as\
          \ on a single request).\nNow the topic is fast, and as I said before, just\
          \ go and check out blogs, books and videos that provide explanation about\
          \ this topic.\nThis is a vast topic and answering is not really feasible.\n\
          \nAlso you can ask questions here: https://discuss.huggingface.co/\nOr our\
          \ discord server: https://discord.com/invite/JfAtkvEtRb"
        updatedAt: '2022-11-01T16:30:42.521Z'
      numEdits: 0
      reactions: []
    id: 636149b23c7147fae7e128cd
    type: comment
  author: Narsil
  content: "> I watched. I don't understand why having the same length sentences would\
    \ optimize the speed?\n\nSpeed doesn't really mean anything in a webserver context,\
    \ without specifying the speed of what.\nThere's 2 core concepts.\nLatency: How\
    \ fast to get your answer once you sent your request. (It's client speed let's\
    \ say).\nThroughput: How fast does the server take to server N requests (It's\
    \ a server's view).\n\nBoth are really important. \n\nAnd GPU are really good\
    \ at throughput (you can add more calculation to do at once, and all will be calculated\
    \ in pretty much at the same speed as on a single request).\nNow the topic is\
    \ fast, and as I said before, just go and check out blogs, books and videos that\
    \ provide explanation about this topic.\nThis is a vast topic and answering is\
    \ not really feasible.\n\nAlso you can ask questions here: https://discuss.huggingface.co/\n\
    Or our discord server: https://discord.com/invite/JfAtkvEtRb"
  created_at: 2022-11-01 15:30:42+00:00
  edited: false
  hidden: false
  id: 636149b23c7147fae7e128cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-11-01T17:44:21.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>please tell me if I finetuned Bloom for paraphrasing like this<br>sentence1
          : ( sentence)<br>sentence2 : ( sentence)</p>

          <p>How could I paraphrase more than one sentence at the same time?</p>

          '
        raw: "please tell me if I finetuned Bloom for paraphrasing like this \nsentence1\
          \ : ( sentence) \nsentence2 : ( sentence)\n\nHow could I paraphrase more\
          \ than one sentence at the same time?"
        updatedAt: '2022-11-01T17:44:21.176Z'
      numEdits: 0
      reactions: []
    id: 63615af53c7147fae7e1db83
    type: comment
  author: mishavee
  content: "please tell me if I finetuned Bloom for paraphrasing like this \nsentence1\
    \ : ( sentence) \nsentence2 : ( sentence)\n\nHow could I paraphrase more than\
    \ one sentence at the same time?"
  created_at: 2022-11-01 16:44:21+00:00
  edited: false
  hidden: false
  id: 63615af53c7147fae7e1db83
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 132
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: 'How does hugging face have so many hosted api s running at once? '
