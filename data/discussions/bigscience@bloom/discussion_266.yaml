!!python/object:huggingface_hub.community.DiscussionWithDetails
author: anuruddhak
conflicting_files: null
created_at: 2023-10-02 23:31:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9caea7073ae0c075adaec6591c81cda7.svg
      fullname: Anu Kualtunga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anuruddhak
      type: user
    createdAt: '2023-10-03T00:31:08.000Z'
    data:
      edited: true
      editors:
      - lysandre
      - anuruddhak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4792585074901581
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: '<p>I am trying to simply use the smallest bloom model to learn how
          to train or specialize the model to generate a new language.</p>

          <p>I run into this error: AttributeError: ''dict'' object has no attribute
          ''full_determinism''.</p>

          <p>I executed pip install transformers so I am hoping it has the latest
          transformer library. Any help is much appreciated!</p>

          <p>Full error message:</p>

          <pre><code>AttributeError Traceback (most recent call last)

          Cell In[25], line 9

          5 t = time.process_time()

          6 #model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1",
          use_cache=True)

          7 #tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-1b1")

          ----&gt; 9 trainer = Trainer(

          10 model=AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1"),

          11 args=training_args,

          12 train_dataset=dataset["train"]

          13 )

          14 t2 = time.process_time()

          15 print("Chekpoint 1 elapsed time "+t2-t1)


          File /opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py:337,
          in Trainer.init(self, model, args, data_collator, train_dataset, eval_dataset,
          tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)

          335 self.args = args

          336 # Seed must be set before instantiating the model when using model

          --&gt; 337 enable_full_determinism(self.args.seed) if self.args.full_determinism
          else set_seed(self.args.seed)

          338 self.hp_name = None

          339 self.deepspeed = None


          AttributeError: ''dict'' object has no attribute ''full_determinism''

          </code></pre>

          <p>Here is my code-------------------------------------:</p>

          <pre><code class="language-py"><span class="hljs-comment">#Load the smallest
          Bloom tokenizer:</span>


          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bigscience/bloom-1b1"</span>)

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Done
          loading"</span>)


          <span class="hljs-comment">#Load the treaining dataset</span>

          dataset = load_dataset(<span class="hljs-string">"text"</span>, data_files={<span
          class="hljs-string">"train"</span>: <span class="hljs-string">"FirstTrainingSet_clean_v1.txt"</span>})

          <span class="hljs-built_in">print</span>(dataset)


          <span class="hljs-comment">#Define the training arguments:</span>


          training_args = {

          <span class="hljs-string">"output_dir"</span>: <span class="hljs-string">"trained_model"</span>,

          <span class="hljs-string">"num_train_epochs"</span>: <span class="hljs-number">10</span>,

          <span class="hljs-string">"per_device_train_batch_size"</span>: <span class="hljs-number">4</span>,

          <span class="hljs-string">"learning_rate"</span>: <span class="hljs-number">1e-5</span>,

          <span class="hljs-string">"full_determinism"</span>:<span class="hljs-string">"False"</span>,

          }


          <span class="hljs-comment">#Train the model:</span>

          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          Trainer, AutoModelForCausalLM

          <span class="hljs-keyword">import</span> time


          t = time.process_time()

          <span class="hljs-comment">#model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1",
          use_cache=True)</span>

          <span class="hljs-comment">#tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-1b1")</span>


          trainer = Trainer(

          model=AutoModelForCausalLM.from_pretrained(<span class="hljs-string">"bigscience/bloom-1b1"</span>),

          args=training_args,

          train_dataset=dataset[<span class="hljs-string">"train"</span>]

          )

          t2 = time.process_time()

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Chekpoint
          1 elapsed time "</span>+t2-t)


          trainer.train()


          t3 = time.process_time()

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Chekpoint
          2 elapsed time [trainer.train()] "</span>+t3-t2)


          <span class="hljs-comment">#save the trained model</span>

          trainer.save_model()

          t4 = time.process_time()

          <span class="hljs-built_in">print</span>(<span class="hljs-string">"Chekpoint
          2 elapsed time [trainer.save_model()] "</span>+t4-t3)

          </code></pre>

          '
        raw: 'I am trying to simply use the smallest bloom model to learn how to train
          or specialize the model to generate a new language.


          I run into this error: AttributeError: ''dict'' object has no attribute
          ''full_determinism''.


          I executed pip install transformers so I am hoping it has the latest transformer
          library. Any help is much appreciated!


          Full error message:

          ```

          AttributeError Traceback (most recent call last)

          Cell In[25], line 9

          5 t = time.process_time()

          6 #model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1",
          use_cache=True)

          7 #tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-1b1")

          ----> 9 trainer = Trainer(

          10 model=AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1"),

          11 args=training_args,

          12 train_dataset=dataset["train"]

          13 )

          14 t2 = time.process_time()

          15 print("Chekpoint 1 elapsed time "+t2-t1)


          File /opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py:337,
          in Trainer.init(self, model, args, data_collator, train_dataset, eval_dataset,
          tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)

          335 self.args = args

          336 # Seed must be set before instantiating the model when using model

          --> 337 enable_full_determinism(self.args.seed) if self.args.full_determinism
          else set_seed(self.args.seed)

          338 self.hp_name = None

          339 self.deepspeed = None


          AttributeError: ''dict'' object has no attribute ''full_determinism''

          ```


          Here is my code-------------------------------------:

          ```py

          #Load the smallest Bloom tokenizer:


          from transformers import AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-1b1")

          print("Done loading")


          #Load the treaining dataset

          dataset = load_dataset("text", data_files={"train": "FirstTrainingSet_clean_v1.txt"})

          print(dataset)


          #Define the training arguments:


          training_args = {

          "output_dir": "trained_model",

          "num_train_epochs": 10,

          "per_device_train_batch_size": 4,

          "learning_rate": 1e-5,

          "full_determinism":"False",

          }


          #Train the model:

          from transformers import Trainer, AutoModelForCausalLM

          import time


          t = time.process_time()

          #model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1", use_cache=True)

          #tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-1b1")


          trainer = Trainer(

          model=AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1"),

          args=training_args,

          train_dataset=dataset["train"]

          )

          t2 = time.process_time()

          print("Chekpoint 1 elapsed time "+t2-t)


          trainer.train()


          t3 = time.process_time()

          print("Chekpoint 2 elapsed time [trainer.train()] "+t3-t2)


          #save the trained model

          trainer.save_model()

          t4 = time.process_time()

          print("Chekpoint 2 elapsed time [trainer.save_model()] "+t4-t3)

          ```'
        updatedAt: '2023-10-03T09:06:18.744Z'
      numEdits: 1
      reactions: []
    id: 651b60cc164539754d8f1c89
    type: comment
  author: anuruddhak
  content: 'I am trying to simply use the smallest bloom model to learn how to train
    or specialize the model to generate a new language.


    I run into this error: AttributeError: ''dict'' object has no attribute ''full_determinism''.


    I executed pip install transformers so I am hoping it has the latest transformer
    library. Any help is much appreciated!


    Full error message:

    ```

    AttributeError Traceback (most recent call last)

    Cell In[25], line 9

    5 t = time.process_time()

    6 #model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1", use_cache=True)

    7 #tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-1b1")

    ----> 9 trainer = Trainer(

    10 model=AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1"),

    11 args=training_args,

    12 train_dataset=dataset["train"]

    13 )

    14 t2 = time.process_time()

    15 print("Chekpoint 1 elapsed time "+t2-t1)


    File /opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py:337, in
    Trainer.init(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer,
    model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)

    335 self.args = args

    336 # Seed must be set before instantiating the model when using model

    --> 337 enable_full_determinism(self.args.seed) if self.args.full_determinism
    else set_seed(self.args.seed)

    338 self.hp_name = None

    339 self.deepspeed = None


    AttributeError: ''dict'' object has no attribute ''full_determinism''

    ```


    Here is my code-------------------------------------:

    ```py

    #Load the smallest Bloom tokenizer:


    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-1b1")

    print("Done loading")


    #Load the treaining dataset

    dataset = load_dataset("text", data_files={"train": "FirstTrainingSet_clean_v1.txt"})

    print(dataset)


    #Define the training arguments:


    training_args = {

    "output_dir": "trained_model",

    "num_train_epochs": 10,

    "per_device_train_batch_size": 4,

    "learning_rate": 1e-5,

    "full_determinism":"False",

    }


    #Train the model:

    from transformers import Trainer, AutoModelForCausalLM

    import time


    t = time.process_time()

    #model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1", use_cache=True)

    #tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-1b1")


    trainer = Trainer(

    model=AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1"),

    args=training_args,

    train_dataset=dataset["train"]

    )

    t2 = time.process_time()

    print("Chekpoint 1 elapsed time "+t2-t)


    trainer.train()


    t3 = time.process_time()

    print("Chekpoint 2 elapsed time [trainer.train()] "+t3-t2)


    #save the trained model

    trainer.save_model()

    t4 = time.process_time()

    print("Chekpoint 2 elapsed time [trainer.save_model()] "+t4-t3)

    ```'
  created_at: 2023-10-02 23:31:08+00:00
  edited: true
  hidden: false
  id: 651b60cc164539754d8f1c89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-03T09:10:03.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6448301672935486
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: '<p>Hello, it seems to me like your args are a <code>dict</code> rather
          than a <code>TrainingArguments</code> instance. Please update as follows:</p>

          <pre><code class="language-diff"><span class="hljs-deletion">- training_args
          = {</span>

          <span class="hljs-deletion">- "output_dir": "trained_model",</span>

          <span class="hljs-deletion">- "num_train_epochs": 10,</span>

          <span class="hljs-deletion">- "per_device_train_batch_size": 4,</span>

          <span class="hljs-deletion">- "learning_rate": 1e-5,</span>

          <span class="hljs-deletion">- "full_determinism":"False",</span>

          }

          <span class="hljs-addition">+ training_args = TrainingArguments(</span>

          <span class="hljs-addition">+     output_dir="trained_model",</span>

          <span class="hljs-addition">+     num_train_epochs=10,</span>

          <span class="hljs-addition">+     per_device_train_batch_size=4,</span>

          <span class="hljs-addition">+     learning_rate=1e-5,</span>

          <span class="hljs-addition">+     full_determinism=False,</span>

          <span class="hljs-addition">+ )</span>

          </code></pre>

          '
        raw: 'Hello, it seems to me like your args are a `dict` rather than a `TrainingArguments`
          instance. Please update as follows:


          ```diff

          - training_args = {

          - "output_dir": "trained_model",

          - "num_train_epochs": 10,

          - "per_device_train_batch_size": 4,

          - "learning_rate": 1e-5,

          - "full_determinism":"False",

          }

          + training_args = TrainingArguments(

          +     output_dir="trained_model",

          +     num_train_epochs=10,

          +     per_device_train_batch_size=4,

          +     learning_rate=1e-5,

          +     full_determinism=False,

          + )

          ```'
        updatedAt: '2023-10-03T09:10:03.705Z'
      numEdits: 0
      reactions: []
    id: 651bda6bb9ae9f37afbf952c
    type: comment
  author: lysandre
  content: 'Hello, it seems to me like your args are a `dict` rather than a `TrainingArguments`
    instance. Please update as follows:


    ```diff

    - training_args = {

    - "output_dir": "trained_model",

    - "num_train_epochs": 10,

    - "per_device_train_batch_size": 4,

    - "learning_rate": 1e-5,

    - "full_determinism":"False",

    }

    + training_args = TrainingArguments(

    +     output_dir="trained_model",

    +     num_train_epochs=10,

    +     per_device_train_batch_size=4,

    +     learning_rate=1e-5,

    +     full_determinism=False,

    + )

    ```'
  created_at: 2023-10-03 08:10:03+00:00
  edited: false
  hidden: false
  id: 651bda6bb9ae9f37afbf952c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9caea7073ae0c075adaec6591c81cda7.svg
      fullname: Anu Kualtunga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anuruddhak
      type: user
    createdAt: '2023-10-08T01:11:58.000Z'
    data:
      edited: false
      editors:
      - anuruddhak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8338025808334351
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9caea7073ae0c075adaec6591c81cda7.svg
          fullname: Anu Kualtunga
          isHf: false
          isPro: false
          name: anuruddhak
          type: user
        html: '<p>Thanks for this advice, I was able to move beyond this step. For
          the benefit of others who might search this, I encountered another error
          related to my hardware Apple Silicon M2 mac. I used the following to get
          around those.</p>

          <p>conda install -c apple tensorflow-deps<br>python -m pip install tensorflow-macos<br>python
          -m pip install tensorflow-metal</p>

          <p>I also ran into an issue with pip time out so I had to use the following:<br>pip
          --default-timeout=5000 install tensorflow</p>

          <p>I found these medium posts to be helpful:<br><a rel="nofollow" href="https://medium.com/@faizififita1/huggingface-installation-on-apple-silicon-2022-m1-pro-max-ultra-m2-9c449b9b4c14">https://medium.com/@faizififita1/huggingface-installation-on-apple-silicon-2022-m1-pro-max-ultra-m2-9c449b9b4c14</a><br><a
          rel="nofollow" href="https://claytonpilat.medium.com/tutorial-tensorflow-on-an-m1-mac-using-jupyter-notebooks-and-miniforge-dbb0ef67bf90">https://claytonpilat.medium.com/tutorial-tensorflow-on-an-m1-mac-using-jupyter-notebooks-and-miniforge-dbb0ef67bf90</a></p>

          '
        raw: 'Thanks for this advice, I was able to move beyond this step. For the
          benefit of others who might search this, I encountered another error related
          to my hardware Apple Silicon M2 mac. I used the following to get around
          those.


          conda install -c apple tensorflow-deps

          python -m pip install tensorflow-macos

          python -m pip install tensorflow-metal


          I also ran into an issue with pip time out so I had to use the following:

          pip --default-timeout=5000 install tensorflow


          I found these medium posts to be helpful:

          https://medium.com/@faizififita1/huggingface-installation-on-apple-silicon-2022-m1-pro-max-ultra-m2-9c449b9b4c14

          https://claytonpilat.medium.com/tutorial-tensorflow-on-an-m1-mac-using-jupyter-notebooks-and-miniforge-dbb0ef67bf90'
        updatedAt: '2023-10-08T01:11:58.070Z'
      numEdits: 0
      reactions: []
    id: 652201ded89bc7773dc4d3ba
    type: comment
  author: anuruddhak
  content: 'Thanks for this advice, I was able to move beyond this step. For the benefit
    of others who might search this, I encountered another error related to my hardware
    Apple Silicon M2 mac. I used the following to get around those.


    conda install -c apple tensorflow-deps

    python -m pip install tensorflow-macos

    python -m pip install tensorflow-metal


    I also ran into an issue with pip time out so I had to use the following:

    pip --default-timeout=5000 install tensorflow


    I found these medium posts to be helpful:

    https://medium.com/@faizififita1/huggingface-installation-on-apple-silicon-2022-m1-pro-max-ultra-m2-9c449b9b4c14

    https://claytonpilat.medium.com/tutorial-tensorflow-on-an-m1-mac-using-jupyter-notebooks-and-miniforge-dbb0ef67bf90'
  created_at: 2023-10-08 00:11:58+00:00
  edited: false
  hidden: false
  id: 652201ded89bc7773dc4d3ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dc8a2e9c490bdcfe7eeda1820fe2ec96.svg
      fullname: Sumanth Meenan Kanneti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sumanthmeenan
      type: user
    createdAt: '2023-12-04T06:19:43.000Z'
    data:
      edited: false
      editors:
      - sumanthmeenan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.39785587787628174
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dc8a2e9c490bdcfe7eeda1820fe2ec96.svg
          fullname: Sumanth Meenan Kanneti
          isHf: false
          isPro: false
          name: sumanthmeenan
          type: user
        html: '<p>Using TrainingArguments I''m getting this error </p>

          <p>ImportError                               Traceback (most recent call
          last)<br> in &lt;cell line: 5&gt;()<br>      3 model_name = model_checkpoint.split("/")[-1]<br>      4<br>----&gt;
          5 training_args = TrainingArguments(<br>      6     output_dir=f"{model_name}-finetuned-squad",<br>      7     evaluation_strategy="epoch",</p>

          <p>4 frames<br>/usr/local/lib/python3.10/dist-packages/transformers/training_args.py
          in _setup_devices(self)<br>   1785         if not is_sagemaker_mp_enabled():<br>   1786             if
          not is_accelerate_available(min_version="0.20.1"):<br>-&gt; 1787                 raise
          ImportError(<br>   1788                     "Using the <code>Trainer</code>
          with <code>PyTorch</code> requires <code>accelerate&gt;=0.20.1</code>: Please
          run <code>pip install transformers[torch]</code> or <code>pip install accelerate
          -U</code>"<br>   1789                 )</p>

          <p>ImportError: Using the <code>Trainer</code> with <code>PyTorch</code>
          requires <code>accelerate&gt;=0.20.1</code>: Please run <code>pip install
          transformers[torch]</code> or <code>pip install accelerate -U</code></p>

          '
        raw: "Using TrainingArguments I'm getting this error \n\nImportError     \
          \                          Traceback (most recent call last)\n<ipython-input-62-d1bef9e6355b>\
          \ in <cell line: 5>()\n      3 model_name = model_checkpoint.split(\"/\"\
          )[-1]\n      4 \n----> 5 training_args = TrainingArguments(\n      6   \
          \  output_dir=f\"{model_name}-finetuned-squad\",\n      7     evaluation_strategy=\"\
          epoch\",\n\n4 frames\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\
          \ in _setup_devices(self)\n   1785         if not is_sagemaker_mp_enabled():\n\
          \   1786             if not is_accelerate_available(min_version=\"0.20.1\"\
          ):\n-> 1787                 raise ImportError(\n   1788                \
          \     \"Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`:\
          \ Please run `pip install transformers[torch]` or `pip install accelerate\
          \ -U`\"\n   1789                 )\n\nImportError: Using the `Trainer` with\
          \ `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]`\
          \ or `pip install accelerate -U`\n"
        updatedAt: '2023-12-04T06:19:43.123Z'
      numEdits: 0
      reactions: []
    id: 656d6f7fb9fa60e33d18e92a
    type: comment
  author: sumanthmeenan
  content: "Using TrainingArguments I'm getting this error \n\nImportError       \
    \                        Traceback (most recent call last)\n<ipython-input-62-d1bef9e6355b>\
    \ in <cell line: 5>()\n      3 model_name = model_checkpoint.split(\"/\")[-1]\n\
    \      4 \n----> 5 training_args = TrainingArguments(\n      6     output_dir=f\"\
    {model_name}-finetuned-squad\",\n      7     evaluation_strategy=\"epoch\",\n\n\
    4 frames\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\
    \ in _setup_devices(self)\n   1785         if not is_sagemaker_mp_enabled():\n\
    \   1786             if not is_accelerate_available(min_version=\"0.20.1\"):\n\
    -> 1787                 raise ImportError(\n   1788                     \"Using\
    \ the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip\
    \ install transformers[torch]` or `pip install accelerate -U`\"\n   1789     \
    \            )\n\nImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`:\
    \ Please run `pip install transformers[torch]` or `pip install accelerate -U`\n"
  created_at: 2023-12-04 06:19:43+00:00
  edited: false
  hidden: false
  id: 656d6f7fb9fa60e33d18e92a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 266
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: 'AttributeError: ''dict'' object has no attribute ''full_determinism'' when
  trying to train the model'
