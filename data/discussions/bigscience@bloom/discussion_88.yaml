!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CedricDB
conflicting_files: null
created_at: 2022-08-18 15:07:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3c3e4fdf16b73f261a66289a7a42595.svg
      fullname: Cedric DB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CedricDB
      type: user
    createdAt: '2022-08-18T16:07:35.000Z'
    data:
      edited: false
      editors:
      - CedricDB
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3c3e4fdf16b73f261a66289a7a42595.svg
          fullname: Cedric DB
          isHf: false
          isPro: false
          name: CedricDB
          type: user
        html: '<p>Is there a way to do it using the system RAM and loading it into
          the GPU in chunks? I''m using the RTX2080 Max-Q (8gb VRAM) and have 64gb
          of ram.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1660838774265-62fb6247a80632fbd477d028.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1660838774265-62fb6247a80632fbd477d028.png"></a></p>

          '
        raw: "Is there a way to do it using the system RAM and loading it into the\
          \ GPU in chunks? I'm using the RTX2080 Max-Q (8gb VRAM) and have 64gb of\
          \ ram.\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1660838774265-62fb6247a80632fbd477d028.png)\r\
          \n"
        updatedAt: '2022-08-18T16:07:35.334Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - tgxn
    id: 62fe63c7cc803924ee07ab74
    type: comment
  author: CedricDB
  content: "Is there a way to do it using the system RAM and loading it into the GPU\
    \ in chunks? I'm using the RTX2080 Max-Q (8gb VRAM) and have 64gb of ram.\r\n\r\
    \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1660838774265-62fb6247a80632fbd477d028.png)\r\
    \n"
  created_at: 2022-08-18 15:07:35+00:00
  edited: false
  hidden: false
  id: 62fe63c7cc803924ee07ab74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640134437444-609baae0fe087f3d04cf0481.jpeg?w=200&h=200&f=face
      fullname: Yozh
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: justheuristic
      type: user
    createdAt: '2022-08-19T01:15:50.000Z'
    data:
      edited: true
      editors:
      - justheuristic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640134437444-609baae0fe087f3d04cf0481.jpeg?w=200&h=200&f=face
          fullname: Yozh
          isHf: false
          isPro: false
          name: justheuristic
          type: user
        html: '<p>You might get away with running that in 8-bit precision.<br>Please
          refer to the transformers+bitsandbytes integration here <a rel="nofollow"
          href="https://github.com/huggingface/transformers/pull/17901">https://github.com/huggingface/transformers/pull/17901</a><br>Specifically,
          this notebook: <a rel="nofollow" href="https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4#scrollTo=W8tQtyjp75O">https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4#scrollTo=W8tQtyjp75O</a></p>

          <p>If not, the next best version is to run parts of the model (embeddings/logits
          and maybe a fraction of model layers) on CPU.<br>We will <em>hopefully</em>
          soon release code for running even larger models (e.g. bloom-176B) on low-memory
          GPUs.</p>

          '
        raw: 'You might get away with running that in 8-bit precision.

          Please refer to the transformers+bitsandbytes integration here https://github.com/huggingface/transformers/pull/17901

          Specifically, this notebook: https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4#scrollTo=W8tQtyjp75O


          If not, the next best version is to run parts of the model (embeddings/logits
          and maybe a fraction of model layers) on CPU.

          We will *hopefully* soon release code for running even larger models (e.g.
          bloom-176B) on low-memory GPUs.'
        updatedAt: '2022-08-19T01:16:55.425Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - stormchaser
        - Ensber
        - gameveloster
    id: 62fee446578749fe22b3329b
    type: comment
  author: justheuristic
  content: 'You might get away with running that in 8-bit precision.

    Please refer to the transformers+bitsandbytes integration here https://github.com/huggingface/transformers/pull/17901

    Specifically, this notebook: https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4#scrollTo=W8tQtyjp75O


    If not, the next best version is to run parts of the model (embeddings/logits
    and maybe a fraction of model layers) on CPU.

    We will *hopefully* soon release code for running even larger models (e.g. bloom-176B)
    on low-memory GPUs.'
  created_at: 2022-08-19 00:15:50+00:00
  edited: true
  hidden: false
  id: 62fee446578749fe22b3329b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3c3e4fdf16b73f261a66289a7a42595.svg
      fullname: Cedric DB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CedricDB
      type: user
    createdAt: '2022-08-20T13:06:04.000Z'
    data:
      edited: false
      editors:
      - CedricDB
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3c3e4fdf16b73f261a66289a7a42595.svg
          fullname: Cedric DB
          isHf: false
          isPro: false
          name: CedricDB
          type: user
        html: '<p>Thanks. I doubt that will work since 7B1 uses about 30gb of RAM
          and it "could reduce the size of the large models by up to 2" which wouldn''t
          make it fit 8gb VRAM, but I''ll try it.</p>

          <p>Side question: if this lower precision is a good way to reduce memory
          usage without losing too much precision, why wasn''t Bloom trained as a
          300B parameter model in 8-bit precision for example?</p>

          '
        raw: 'Thanks. I doubt that will work since 7B1 uses about 30gb of RAM and
          it "could reduce the size of the large models by up to 2" which wouldn''t
          make it fit 8gb VRAM, but I''ll try it.


          Side question: if this lower precision is a good way to reduce memory usage
          without losing too much precision, why wasn''t Bloom trained as a 300B parameter
          model in 8-bit precision for example?'
        updatedAt: '2022-08-20T13:06:04.196Z'
      numEdits: 0
      reactions: []
    id: 6300dc3cc1e149ceaff8a8db
    type: comment
  author: CedricDB
  content: 'Thanks. I doubt that will work since 7B1 uses about 30gb of RAM and it
    "could reduce the size of the large models by up to 2" which wouldn''t make it
    fit 8gb VRAM, but I''ll try it.


    Side question: if this lower precision is a good way to reduce memory usage without
    losing too much precision, why wasn''t Bloom trained as a 300B parameter model
    in 8-bit precision for example?'
  created_at: 2022-08-20 12:06:04+00:00
  edited: false
  hidden: false
  id: 6300dc3cc1e149ceaff8a8db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ff60201c2af69b94f94287/flc8mFRrST35P3DFEOufY.jpeg?w=200&h=200&f=face
      fullname: Boi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: boi-doingthings
      type: user
    createdAt: '2022-08-20T19:03:15.000Z'
    data:
      edited: true
      editors:
      - boi-doingthings
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62ff60201c2af69b94f94287/flc8mFRrST35P3DFEOufY.jpeg?w=200&h=200&f=face
          fullname: Boi
          isHf: false
          isPro: false
          name: boi-doingthings
          type: user
        html: '<p>Too be honest, it is hard to run these big models in that amount
          of VRAM. I have a RTX3070 and similar suffering. :(</p>

          <p>You don''t get to use all the 8 GB, display and other internal resources
          also utilize a part of the memory.</p>

          '
        raw: 'Too be honest, it is hard to run these big models in that amount of
          VRAM. I have a RTX3070 and similar suffering. :(


          You don''t get to use all the 8 GB, display and other internal resources
          also utilize a part of the memory.'
        updatedAt: '2022-08-20T19:04:17.399Z'
      numEdits: 1
      reactions: []
    id: 63012ff3c1e149ceaffacc9a
    type: comment
  author: boi-doingthings
  content: 'Too be honest, it is hard to run these big models in that amount of VRAM.
    I have a RTX3070 and similar suffering. :(


    You don''t get to use all the 8 GB, display and other internal resources also
    utilize a part of the memory.'
  created_at: 2022-08-20 18:03:15+00:00
  edited: true
  hidden: false
  id: 63012ff3c1e149ceaffacc9a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/367e1c9cbb77e454b57832240bf2adf5.svg
      fullname: Gameveloster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gameveloster
      type: user
    createdAt: '2023-01-13T02:15:07.000Z'
    data:
      edited: false
      editors:
      - gameveloster
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/367e1c9cbb77e454b57832240bf2adf5.svg
          fullname: Gameveloster
          isHf: false
          isPro: false
          name: gameveloster
          type: user
        html: '<blockquote>

          <p>You might get away with running that in 8-bit precision.<br>Please refer
          to the transformers+bitsandbytes integration here <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/17901">https://github.com/huggingface/transformers/pull/17901</a><br>Specifically,
          this notebook: <a rel="nofollow" href="https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4#scrollTo=W8tQtyjp75O">https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4#scrollTo=W8tQtyjp75O</a></p>

          <p>If not, the next best version is to run parts of the model (embeddings/logits
          and maybe a fraction of model layers) on CPU.<br>We will <em>hopefully</em>
          soon release code for running even larger models (e.g. bloom-176B) on low-memory
          GPUs.</p>

          </blockquote>

          <p>Any updates regarding running bloom-176B on &lt;=24G GPUs?</p>

          '
        raw: "> You might get away with running that in 8-bit precision.\n> Please\
          \ refer to the transformers+bitsandbytes integration here https://github.com/huggingface/transformers/pull/17901\n\
          > Specifically, this notebook: https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4#scrollTo=W8tQtyjp75O\n\
          > \n> If not, the next best version is to run parts of the model (embeddings/logits\
          \ and maybe a fraction of model layers) on CPU.\n> We will *hopefully* soon\
          \ release code for running even larger models (e.g. bloom-176B) on low-memory\
          \ GPUs.\n\nAny updates regarding running bloom-176B on <=24G GPUs?"
        updatedAt: '2023-01-13T02:15:07.645Z'
      numEdits: 0
      reactions: []
    id: 63c0beabd9e14fd8876373bd
    type: comment
  author: gameveloster
  content: "> You might get away with running that in 8-bit precision.\n> Please refer\
    \ to the transformers+bitsandbytes integration here https://github.com/huggingface/transformers/pull/17901\n\
    > Specifically, this notebook: https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4#scrollTo=W8tQtyjp75O\n\
    > \n> If not, the next best version is to run parts of the model (embeddings/logits\
    \ and maybe a fraction of model layers) on CPU.\n> We will *hopefully* soon release\
    \ code for running even larger models (e.g. bloom-176B) on low-memory GPUs.\n\n\
    Any updates regarding running bloom-176B on <=24G GPUs?"
  created_at: 2023-01-13 02:15:07+00:00
  edited: false
  hidden: false
  id: 63c0beabd9e14fd8876373bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-01-15T09:28:00.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>In this case you may want to be interested in PETALS: <a rel=\"\
          nofollow\" href=\"https://github.com/bigscience-workshop/petals\">https://github.com/bigscience-workshop/petals</a><br>cc\
          \ <span data-props=\"{&quot;user&quot;:&quot;borzunov&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/borzunov\">@<span class=\"\
          underline\">borzunov</span></a></span>\n\n\t</span></span></p>\n"
        raw: 'In this case you may want to be interested in PETALS: https://github.com/bigscience-workshop/petals

          cc @borzunov'
        updatedAt: '2023-01-15T09:28:00.808Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - borzunov
    id: 63c3c720bfd39d7373975ad6
    type: comment
  author: ybelkada
  content: 'In this case you may want to be interested in PETALS: https://github.com/bigscience-workshop/petals

    cc @borzunov'
  created_at: 2023-01-15 09:28:00+00:00
  edited: false
  hidden: false
  id: 63c3c720bfd39d7373975ad6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2023-01-27T07:48:45.000Z'
    data:
      status: closed
    id: 63d381dddaaab164443b53cc
    type: status-change
  author: TimeRobber
  created_at: 2023-01-27 07:48:45+00:00
  id: 63d381dddaaab164443b53cc
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 88
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: Running Bloom-7B1 on an 8gb GPU?
