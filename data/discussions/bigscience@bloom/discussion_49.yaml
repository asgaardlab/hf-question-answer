!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rufimelo
conflicting_files: null
created_at: 2022-07-17 22:02:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
      fullname: Rui Melo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rufimelo
      type: user
    createdAt: '2022-07-17T23:02:27.000Z'
    data:
      edited: false
      editors:
      - rufimelo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
          fullname: Rui Melo
          isHf: false
          isPro: false
          name: rufimelo
          type: user
        html: '<p>In the past few week, I''ve been messing around with SBERTs.</p>

          <p>Is it possible to create embeddings using bloom to perform Semantic Search?</p>

          '
        raw: "In the past few week, I've been messing around with SBERTs.\r\n\r\n\
          Is it possible to create embeddings using bloom to perform Semantic Search?"
        updatedAt: '2022-07-17T23:02:27.096Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - xiyanghu
    id: 62d495035bffe7df01af6e5c
    type: comment
  author: rufimelo
  content: "In the past few week, I've been messing around with SBERTs.\r\n\r\nIs\
    \ it possible to create embeddings using bloom to perform Semantic Search?"
  created_at: 2022-07-17 22:02:27+00:00
  edited: false
  hidden: false
  id: 62d495035bffe7df01af6e5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg?w=200&h=200&f=face
      fullname: Thomas Wolf
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: thomwolf
      type: user
    createdAt: '2022-07-18T18:51:17.000Z'
    data:
      edited: false
      editors:
      - thomwolf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg?w=200&h=200&f=face
          fullname: Thomas Wolf
          isHf: true
          isPro: true
          name: thomwolf
          type: user
        html: '<p>If you download the model and run it yourself yes, but it''s not
          provided through the model widget API</p>

          '
        raw: If you download the model and run it yourself yes, but it's not provided
          through the model widget API
        updatedAt: '2022-07-18T18:51:17.276Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - rufimelo
        - wilfoderek
        - Apps
    id: 62d5aba5a2de3ae5ea7062a3
    type: comment
  author: thomwolf
  content: If you download the model and run it yourself yes, but it's not provided
    through the model widget API
  created_at: 2022-07-18 17:51:17+00:00
  edited: false
  hidden: false
  id: 62d5aba5a2de3ae5ea7062a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-07-18T18:52:43.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Hey! I created an embedding model of BLOOM 1B3 here that may be
          of interest to you: <a href="https://huggingface.co/bigscience-catalogue-lm-data/sgpt-nli-bloom-1b3">https://huggingface.co/bigscience-catalogue-lm-data/sgpt-nli-bloom-1b3</a></p>

          <p>If it is of interest, I can create a similar fine-tuned embedding model
          for this 176B model, but do note that embeddings would be very expensive
          to retrieve. Further storing them would require a lot of space if we don''t
          add a linear layer to reduce their dim.</p>

          <p>Else, you can ofc load the model in HF with AutoModel and produce embeddings,
          but they will not perform well without fine-tuning like is done for the
          model above.</p>

          '
        raw: 'Hey! I created an embedding model of BLOOM 1B3 here that may be of interest
          to you: https://huggingface.co/bigscience-catalogue-lm-data/sgpt-nli-bloom-1b3


          If it is of interest, I can create a similar fine-tuned embedding model
          for this 176B model, but do note that embeddings would be very expensive
          to retrieve. Further storing them would require a lot of space if we don''t
          add a linear layer to reduce their dim.


          Else, you can ofc load the model in HF with AutoModel and produce embeddings,
          but they will not perform well without fine-tuning like is done for the
          model above.'
        updatedAt: '2022-07-18T18:52:43.459Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - rufimelo
        - daosysang
    id: 62d5abfb5c29ac61fecade61
    type: comment
  author: Muennighoff
  content: 'Hey! I created an embedding model of BLOOM 1B3 here that may be of interest
    to you: https://huggingface.co/bigscience-catalogue-lm-data/sgpt-nli-bloom-1b3


    If it is of interest, I can create a similar fine-tuned embedding model for this
    176B model, but do note that embeddings would be very expensive to retrieve. Further
    storing them would require a lot of space if we don''t add a linear layer to reduce
    their dim.


    Else, you can ofc load the model in HF with AutoModel and produce embeddings,
    but they will not perform well without fine-tuning like is done for the model
    above.'
  created_at: 2022-07-18 17:52:43+00:00
  edited: false
  hidden: false
  id: 62d5abfb5c29ac61fecade61
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
      fullname: Rui Melo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rufimelo
      type: user
    createdAt: '2022-07-18T19:39:52.000Z'
    data:
      edited: false
      editors:
      - rufimelo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
          fullname: Rui Melo
          isHf: false
          isPro: false
          name: rufimelo
          type: user
        html: '<p>Thank you for the feedback!</p>

          '
        raw: Thank you for the feedback!
        updatedAt: '2022-07-18T19:39:52.341Z'
      numEdits: 0
      reactions: []
    id: 62d5b708b6ced8fbf398587e
    type: comment
  author: rufimelo
  content: Thank you for the feedback!
  created_at: 2022-07-18 18:39:52+00:00
  edited: false
  hidden: false
  id: 62d5b708b6ced8fbf398587e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
      fullname: Rui Melo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rufimelo
      type: user
    createdAt: '2022-07-18T19:39:57.000Z'
    data:
      status: closed
    id: 62d5b70dc46278c4a995d21d
    type: status-change
  author: rufimelo
  created_at: 2022-07-18 18:39:57+00:00
  id: 62d5b70dc46278c4a995d21d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
      fullname: Rui Melo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rufimelo
      type: user
    createdAt: '2022-07-18T22:51:11.000Z'
    data:
      status: open
    id: 62d5e3df3bf5e059f7d00f21
    type: status-change
  author: rufimelo
  created_at: 2022-07-18 21:51:11+00:00
  id: 62d5e3df3bf5e059f7d00f21
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
      fullname: Rui Melo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rufimelo
      type: user
    createdAt: '2022-07-18T23:35:24.000Z'
    data:
      edited: false
      editors:
      - rufimelo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
          fullname: Rui Melo
          isHf: false
          isPro: false
          name: rufimelo
          type: user
        html: '<p>Versions:</p>

          <p>-transformers: 4.20.1<br>-sentence-transformers: 2.2.2</p>

          <p>I could not load the model using:</p>

          <pre><code>from sentence_transformers import SentenceTransformer

          model = SentenceTransformer("bigscience-catalogue-lm-data/sgpt-nli-bloom-1b3")

          </code></pre>

          <p>,but it ended up working by creating the SentenceTransformer model using
          the word_embedding model.</p>

          <pre><code>from sentence_transformers import SentenceTransformer, models,
          evaluation

          word_embedding_model = models.Transformer(''bigscience-catalogue-lm-data/sgpt-nli-bloom-1b3'',
          max_seq_length=256)

          pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())


          model2 = SentenceTransformer(modules=[word_embedding_model, pooling_model])

          </code></pre>

          <p>I was trying to fine-tune it like I did previously on sentence transformers
          and It did not work.</p>

          <pre><code>#load dataset

          from datasets import load_dataset

          dataset = load_dataset("assin")


          from sentence_transformers import SentenceTransformer, InputExample, losses,
          models, evaluation

          from torch.utils.data import DataLoader

          train_examples = [InputExample(texts=[texts[''premise''], texts[''hypothesis'']],
          label=texts[''relatedness_score'']/5) for texts in dataset[''train'']]


          #Define your train dataset, the dataloader and the train loss

          train_dataloader = DataLoader(train_examples, batch_size=32)

          train_loss = losses.CosineSimilarityLoss(model)


          #Tune the model

          model2.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1,
          warmup_steps=0.1*len(train_dataloader))

          </code></pre>

          <p>Output:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1658186592825-622f26eb163b4ef6fd7a5361.png"><img
          alt="2.png" src="https://cdn-uploads.huggingface.co/production/uploads/1658186592825-622f26eb163b4ef6fd7a5361.png"></a></p>

          <p>Do I need a specific version ? The first example should have loaded correctly
          the model, right?</p>

          <p>Am I fine tuning it the wrong way with this type of model?<br>Thanks
          in advance</p>

          '
        raw: "Versions:\n \n-transformers: 4.20.1\n-sentence-transformers: 2.2.2\n\
          \n\nI could not load the model using:\n\n```\nfrom sentence_transformers\
          \ import SentenceTransformer\nmodel = SentenceTransformer(\"bigscience-catalogue-lm-data/sgpt-nli-bloom-1b3\"\
          )\n```\n\n,but it ended up working by creating the SentenceTransformer model\
          \ using the word_embedding model.\n\n```\nfrom sentence_transformers import\
          \ SentenceTransformer, models, evaluation\nword_embedding_model = models.Transformer('bigscience-catalogue-lm-data/sgpt-nli-bloom-1b3',\
          \ max_seq_length=256)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n\
          \nmodel2 = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\
          ```\n\nI was trying to fine-tune it like I did previously on sentence transformers\
          \ and It did not work.\n```\n#load dataset\nfrom datasets import load_dataset\n\
          dataset = load_dataset(\"assin\")\n\nfrom sentence_transformers import SentenceTransformer,\
          \ InputExample, losses, models, evaluation\nfrom torch.utils.data import\
          \ DataLoader\ntrain_examples = [InputExample(texts=[texts['premise'], texts['hypothesis']],\
          \ label=texts['relatedness_score']/5) for texts in dataset['train']]\n\n\
          #Define your train dataset, the dataloader and the train loss\ntrain_dataloader\
          \ = DataLoader(train_examples, batch_size=32)\ntrain_loss = losses.CosineSimilarityLoss(model)\n\
          \n#Tune the model\nmodel2.fit(train_objectives=[(train_dataloader, train_loss)],\
          \ epochs=1, warmup_steps=0.1*len(train_dataloader))\n\n```\n\n\nOutput:\n\
          ![2.png](https://cdn-uploads.huggingface.co/production/uploads/1658186592825-622f26eb163b4ef6fd7a5361.png)\n\
          \n\n\nDo I need a specific version ? The first example should have loaded\
          \ correctly the model, right?\n\nAm I fine tuning it the wrong way with\
          \ this type of model?\nThanks in advance"
        updatedAt: '2022-07-18T23:35:24.991Z'
      numEdits: 0
      reactions: []
    id: 62d5ee3c8dd45b952f5e3467
    type: comment
  author: rufimelo
  content: "Versions:\n \n-transformers: 4.20.1\n-sentence-transformers: 2.2.2\n\n\
    \nI could not load the model using:\n\n```\nfrom sentence_transformers import\
    \ SentenceTransformer\nmodel = SentenceTransformer(\"bigscience-catalogue-lm-data/sgpt-nli-bloom-1b3\"\
    )\n```\n\n,but it ended up working by creating the SentenceTransformer model using\
    \ the word_embedding model.\n\n```\nfrom sentence_transformers import SentenceTransformer,\
    \ models, evaluation\nword_embedding_model = models.Transformer('bigscience-catalogue-lm-data/sgpt-nli-bloom-1b3',\
    \ max_seq_length=256)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n\
    \nmodel2 = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\
    ```\n\nI was trying to fine-tune it like I did previously on sentence transformers\
    \ and It did not work.\n```\n#load dataset\nfrom datasets import load_dataset\n\
    dataset = load_dataset(\"assin\")\n\nfrom sentence_transformers import SentenceTransformer,\
    \ InputExample, losses, models, evaluation\nfrom torch.utils.data import DataLoader\n\
    train_examples = [InputExample(texts=[texts['premise'], texts['hypothesis']],\
    \ label=texts['relatedness_score']/5) for texts in dataset['train']]\n\n#Define\
    \ your train dataset, the dataloader and the train loss\ntrain_dataloader = DataLoader(train_examples,\
    \ batch_size=32)\ntrain_loss = losses.CosineSimilarityLoss(model)\n\n#Tune the\
    \ model\nmodel2.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1,\
    \ warmup_steps=0.1*len(train_dataloader))\n\n```\n\n\nOutput:\n![2.png](https://cdn-uploads.huggingface.co/production/uploads/1658186592825-622f26eb163b4ef6fd7a5361.png)\n\
    \n\n\nDo I need a specific version ? The first example should have loaded correctly\
    \ the model, right?\n\nAm I fine tuning it the wrong way with this type of model?\n\
    Thanks in advance"
  created_at: 2022-07-18 22:35:24+00:00
  edited: false
  hidden: false
  id: 62d5ee3c8dd45b952f5e3467
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-07-19T13:40:15.000Z'
    data:
      edited: true
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<ol>\n<li><p>To load it you need to either use this branch of SentenceTransformers:\
          \ <a rel=\"nofollow\" href=\"https://github.com/UKPLab/sentence-transformers/pull/1613\"\
          >https://github.com/UKPLab/sentence-transformers/pull/1613</a> or install\
          \ sentence transformers from this repo: <a rel=\"nofollow\" href=\"https://github.com/Muennighoff/sgpt/tree/main/biencoder/nli_msmarco\"\
          >https://github.com/Muennighoff/sgpt/tree/main/biencoder/nli_msmarco</a></p>\n\
          </li>\n<li><p>By loading it via one of the ways above, your problem will\
          \ probably be solved. Note though that the model is already fine-tuned.\
          \ If you want to fine-tune further, you probably also need to use one of\
          \ the two repos above as the pooling of the model is not implemented in\
          \ sentence transformers. If you have a lot of data for your use-case it\
          \ might make more sense to just fine-tune from scratch again.</p>\n</li>\n\
          </ol>\n<p>Hope this helps \U0001F917</p>\n"
        raw: "1) To load it you need to either use this branch of SentenceTransformers:\
          \ https://github.com/UKPLab/sentence-transformers/pull/1613 or install sentence\
          \ transformers from this repo: https://github.com/Muennighoff/sgpt/tree/main/biencoder/nli_msmarco\n\
          \n2) By loading it via one of the ways above, your problem will probably\
          \ be solved. Note though that the model is already fine-tuned. If you want\
          \ to fine-tune further, you probably also need to use one of the two repos\
          \ above as the pooling of the model is not implemented in sentence transformers.\
          \ If you have a lot of data for your use-case it might make more sense to\
          \ just fine-tune from scratch again.\n\nHope this helps \U0001F917"
        updatedAt: '2022-07-20T07:01:09.885Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - rufimelo
        - julien-c
    id: 62d6b43fa543b9b7921573bb
    type: comment
  author: Muennighoff
  content: "1) To load it you need to either use this branch of SentenceTransformers:\
    \ https://github.com/UKPLab/sentence-transformers/pull/1613 or install sentence\
    \ transformers from this repo: https://github.com/Muennighoff/sgpt/tree/main/biencoder/nli_msmarco\n\
    \n2) By loading it via one of the ways above, your problem will probably be solved.\
    \ Note though that the model is already fine-tuned. If you want to fine-tune further,\
    \ you probably also need to use one of the two repos above as the pooling of the\
    \ model is not implemented in sentence transformers. If you have a lot of data\
    \ for your use-case it might make more sense to just fine-tune from scratch again.\n\
    \nHope this helps \U0001F917"
  created_at: 2022-07-19 12:40:15+00:00
  edited: true
  hidden: false
  id: 62d6b43fa543b9b7921573bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
      fullname: Rui Melo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rufimelo
      type: user
    createdAt: '2022-07-19T19:22:02.000Z'
    data:
      edited: false
      editors:
      - rufimelo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
          fullname: Rui Melo
          isHf: false
          isPro: false
          name: rufimelo
          type: user
        html: '<p>Thanks, I believe it works. I just need to have a GPU available
          to train and complete the fine-tuning.</p>

          <p>I just have one other question.<br>On SBERT models, we can perform "Domain
          Adaptation" of the BERT model, before creating a SBERT one.<br>It would
          allow our model to be more familiarized with our specific context.</p>

          <p>On my side, I only have access to raw texts.<br>I was wondering if I
          could perform something similar to this model, with relatively scarce computational
          resources.</p>

          '
        raw: "Thanks, I believe it works. I just need to have a GPU available to train\
          \ and complete the fine-tuning.\n\nI just have one other question.\nOn SBERT\
          \ models, we can perform \"Domain Adaptation\" of the BERT model, before\
          \ creating a SBERT one. \nIt would allow our model to be more familiarized\
          \ with our specific context.\n\nOn my side, I only have access to raw texts.\n\
          I was wondering if I could perform something similar to this model, with\
          \ relatively scarce computational resources."
        updatedAt: '2022-07-19T19:22:02.110Z'
      numEdits: 0
      reactions: []
    id: 62d7045a09062b81b402ff4d
    type: comment
  author: rufimelo
  content: "Thanks, I believe it works. I just need to have a GPU available to train\
    \ and complete the fine-tuning.\n\nI just have one other question.\nOn SBERT models,\
    \ we can perform \"Domain Adaptation\" of the BERT model, before creating a SBERT\
    \ one. \nIt would allow our model to be more familiarized with our specific context.\n\
    \nOn my side, I only have access to raw texts.\nI was wondering if I could perform\
    \ something similar to this model, with relatively scarce computational resources."
  created_at: 2022-07-19 18:22:02+00:00
  edited: false
  hidden: false
  id: 62d7045a09062b81b402ff4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
      fullname: Rui Melo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rufimelo
      type: user
    createdAt: '2022-07-19T23:15:27.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
          fullname: Rui Melo
          isHf: false
          isPro: false
          name: rufimelo
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-07-19T23:16:10.042Z'
      numEdits: 0
      reactions: []
    id: 62d73b0f31d1cf871da7efbf
    type: comment
  author: rufimelo
  content: This comment has been hidden
  created_at: 2022-07-19 22:15:27+00:00
  edited: true
  hidden: true
  id: 62d73b0f31d1cf871da7efbf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
      fullname: Rui Melo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rufimelo
      type: user
    createdAt: '2022-07-19T23:17:21.000Z'
    data:
      edited: false
      editors:
      - rufimelo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
          fullname: Rui Melo
          isHf: false
          isPro: false
          name: rufimelo
          type: user
        html: '<p>Also, in order to fine-tune such a model, what GPU size do I need?</p>

          '
        raw: Also, in order to fine-tune such a model, what GPU size do I need?
        updatedAt: '2022-07-19T23:17:21.407Z'
      numEdits: 0
      reactions: []
    id: 62d73b81bad37ef35400d240
    type: comment
  author: rufimelo
  content: Also, in order to fine-tune such a model, what GPU size do I need?
  created_at: 2022-07-19 22:17:21+00:00
  edited: false
  hidden: false
  id: 62d73b81bad37ef35400d240
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-07-20T07:03:50.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>If you don''t have aligned texts i.e. just raw, you probably want
          to do fine-tune the BERT model not the SentenceTransformer? If you''re texts
          are aligned then for the model above you can probably fine-tune it with
          1x A100 40GB or even 1xV100 36GB. If you have more, training will be faster
          via data parallelism. I''d recommend using GradCache to get a larger batch
          size as implemented in this repo: <a rel="nofollow" href="https://github.com/Muennighoff/sgpt/tree/main/biencoder/nli_msmarco">https://github.com/Muennighoff/sgpt/tree/main/biencoder/nli_msmarco</a></p>

          '
        raw: 'If you don''t have aligned texts i.e. just raw, you probably want to
          do fine-tune the BERT model not the SentenceTransformer? If you''re texts
          are aligned then for the model above you can probably fine-tune it with
          1x A100 40GB or even 1xV100 36GB. If you have more, training will be faster
          via data parallelism. I''d recommend using GradCache to get a larger batch
          size as implemented in this repo: https://github.com/Muennighoff/sgpt/tree/main/biencoder/nli_msmarco'
        updatedAt: '2022-07-20T07:03:50.556Z'
      numEdits: 0
      reactions: []
    id: 62d7a8d68edc52b36338a959
    type: comment
  author: Muennighoff
  content: 'If you don''t have aligned texts i.e. just raw, you probably want to do
    fine-tune the BERT model not the SentenceTransformer? If you''re texts are aligned
    then for the model above you can probably fine-tune it with 1x A100 40GB or even
    1xV100 36GB. If you have more, training will be faster via data parallelism. I''d
    recommend using GradCache to get a larger batch size as implemented in this repo:
    https://github.com/Muennighoff/sgpt/tree/main/biencoder/nli_msmarco'
  created_at: 2022-07-20 06:03:50+00:00
  edited: false
  hidden: false
  id: 62d7a8d68edc52b36338a959
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
      fullname: Rui Melo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rufimelo
      type: user
    createdAt: '2022-07-20T11:25:41.000Z'
    data:
      edited: false
      editors:
      - rufimelo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666860016110-622f26eb163b4ef6fd7a5361.jpeg?w=200&h=200&f=face
          fullname: Rui Melo
          isHf: false
          isPro: false
          name: rufimelo
          type: user
        html: '<p>Yes, it would be the BERT in that case.</p>

          <p>Thanks!</p>

          '
        raw: 'Yes, it would be the BERT in that case.


          Thanks!'
        updatedAt: '2022-07-20T11:25:41.144Z'
      numEdits: 0
      reactions: []
    id: 62d7e63543df7719860b92fc
    type: comment
  author: rufimelo
  content: 'Yes, it would be the BERT in that case.


    Thanks!'
  created_at: 2022-07-20 10:25:41+00:00
  edited: false
  hidden: false
  id: 62d7e63543df7719860b92fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-07-21T08:12:07.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<p>Sure, if you want to fine-tune BERT you can do that easily in sentence-transformers\
          \ with mean pooling. For fine-tuning bloom / other auto-regressive models\
          \ for embeddings, it may be better to use the repo I sent above with weighted\
          \ mean pooling (<a rel=\"nofollow\" href=\"https://github.com/Muennighoff/sgpt/tree/main/biencoder/nli_msmarco\"\
          >https://github.com/Muennighoff/sgpt/tree/main/biencoder/nli_msmarco</a>).</p>\n\
          <p>Closing this issue, feel free to reopen if you still have questions \U0001F47B\
          </p>\n"
        raw: "Sure, if you want to fine-tune BERT you can do that easily in sentence-transformers\
          \ with mean pooling. For fine-tuning bloom / other auto-regressive models\
          \ for embeddings, it may be better to use the repo I sent above with weighted\
          \ mean pooling (https://github.com/Muennighoff/sgpt/tree/main/biencoder/nli_msmarco).\n\
          \nClosing this issue, feel free to reopen if you still have questions \U0001F47B\
          \n"
        updatedAt: '2022-07-21T08:12:07.519Z'
      numEdits: 0
      reactions: []
      relatedEventId: 62d90a5745732b0804773e1c
    id: 62d90a5745732b0804773e1b
    type: comment
  author: Muennighoff
  content: "Sure, if you want to fine-tune BERT you can do that easily in sentence-transformers\
    \ with mean pooling. For fine-tuning bloom / other auto-regressive models for\
    \ embeddings, it may be better to use the repo I sent above with weighted mean\
    \ pooling (https://github.com/Muennighoff/sgpt/tree/main/biencoder/nli_msmarco).\n\
    \nClosing this issue, feel free to reopen if you still have questions \U0001F47B\
    \n"
  created_at: 2022-07-21 07:12:07+00:00
  edited: false
  hidden: false
  id: 62d90a5745732b0804773e1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-07-21T08:12:07.000Z'
    data:
      status: closed
    id: 62d90a5745732b0804773e1c
    type: status-change
  author: Muennighoff
  created_at: 2022-07-21 07:12:07+00:00
  id: 62d90a5745732b0804773e1c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 49
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: Token or Sentence Embeddings
