!!python/object:huggingface_hub.community.DiscussionWithDetails
author: VictorSanh
conflicting_files: null
created_at: 2022-10-04 22:52:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2022-10-04T23:52:08.000Z'
    data:
      edited: false
      editors:
      - VictorSanh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
          fullname: Victor Sanh
          isHf: true
          isPro: true
          name: VictorSanh
          type: user
        html: '<p>Hey!</p>

          <p>The model card says <code>A vocabulary size of 250,680</code>.<br><code>len(tokenizer)</code>
          returns <code>250680</code>.</p>

          <p>However, the config has <code>"vocab_size": 250880</code>.</p>

          <p>Also the docstring of <code>BloomConfig</code> still has <code>vocab_size
          (</code>int<code>, *optional*, defaults to 50257):</code> which is coming
          from GPT2 I believe.</p>

          <p>Is there a reason for these mismatches?</p>

          '
        raw: "Hey!\r\n\r\nThe model card says `A vocabulary size of 250,680`.\r\n\
          `len(tokenizer)` returns `250680`.\r\n\r\nHowever, the config has `\"vocab_size\"\
          : 250880`.\r\n\r\nAlso the docstring of `BloomConfig` still has `vocab_size\
          \ (`int`, *optional*, defaults to 50257):` which is coming from GPT2 I believe.\r\
          \n\r\nIs there a reason for these mismatches?"
        updatedAt: '2022-10-04T23:52:08.443Z'
      numEdits: 0
      reactions: []
    id: 633cc728d5935998f75647ff
    type: comment
  author: VictorSanh
  content: "Hey!\r\n\r\nThe model card says `A vocabulary size of 250,680`.\r\n`len(tokenizer)`\
    \ returns `250680`.\r\n\r\nHowever, the config has `\"vocab_size\": 250880`.\r\
    \n\r\nAlso the docstring of `BloomConfig` still has `vocab_size (`int`, *optional*,\
    \ defaults to 50257):` which is coming from GPT2 I believe.\r\n\r\nIs there a\
    \ reason for these mismatches?"
  created_at: 2022-10-04 22:52:08+00:00
  edited: false
  hidden: false
  id: 633cc728d5935998f75647ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2022-10-04T23:57:59.000Z'
    data:
      edited: true
      editors:
      - VictorSanh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
          fullname: Victor Sanh
          isHf: true
          isPro: true
          name: VictorSanh
          type: user
        html: "<p>And at the same time, the <code>word_embeddings</code> matrix is\
          \ of size <code>Embedding(250880, hidden_size)</code>. I am missing something\
          \ \U0001F605</p>\n"
        raw: "And at the same time, the `word_embeddings` matrix is of size `Embedding(250880,\
          \ hidden_size)`. I am missing something \U0001F605"
        updatedAt: '2022-10-04T23:58:11.099Z'
      numEdits: 1
      reactions: []
    id: 633cc887ec4e4abf308203f1
    type: comment
  author: VictorSanh
  content: "And at the same time, the `word_embeddings` matrix is of size `Embedding(250880,\
    \ hidden_size)`. I am missing something \U0001F605"
  created_at: 2022-10-04 22:57:59+00:00
  edited: true
  hidden: false
  id: 633cc887ec4e4abf308203f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618938489629-60741a2e69a66931a0273f0c.png?w=200&h=200&f=face
      fullname: Lucile Saulnier
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SaulLu
      type: user
    createdAt: '2022-10-05T06:46:16.000Z'
    data:
      edited: false
      editors:
      - SaulLu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618938489629-60741a2e69a66931a0273f0c.png?w=200&h=200&f=face
          fullname: Lucile Saulnier
          isHf: false
          isPro: false
          name: SaulLu
          type: user
        html: '<p>Hello!</p>

          <p>There is indeed an explanation for this difference in numbers. In the
          <code>config.json</code> file, the variable <code>vocab_size</code> is only
          used to define the size of the <code>word_embeddings</code> and <code>lm_head</code>
          matrices. The constraint we have is that the size of these matrices must
          be greater than or equal to the number of tokens known by the tokenizer,
          the difference of 200 corresponding to "dummy" tokens that are not used.
          </p>

          <p>There are several reasons in the development of BLOOM that led to this
          difference. The size of the <code>word_embeddings</code> and <code>lm_head</code>
          matrixes had to be divisible by a certain number (4*128 from memory) so
          that the model could be parallelized with tensor parallelism. Then, the
          tokenizer was produced before all the model design was finished and it was
          safer to leave tokens available if we needed to add special tokens for training
          (for PII for example).</p>

          '
        raw: "Hello!\n\nThere is indeed an explanation for this difference in numbers.\
          \ In the `config.json` file, the variable `vocab_size` is only used to define\
          \ the size of the `word_embeddings` and `lm_head` matrices. The constraint\
          \ we have is that the size of these matrices must be greater than or equal\
          \ to the number of tokens known by the tokenizer, the difference of 200\
          \ corresponding to \"dummy\" tokens that are not used. \n\nThere are several\
          \ reasons in the development of BLOOM that led to this difference. The size\
          \ of the `word_embeddings` and `lm_head` matrixes had to be divisible by\
          \ a certain number (4*128 from memory) so that the model could be parallelized\
          \ with tensor parallelism. Then, the tokenizer was produced before all the\
          \ model design was finished and it was safer to leave tokens available if\
          \ we needed to add special tokens for training (for PII for example)."
        updatedAt: '2022-10-05T06:46:16.303Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F91D"
        users:
        - ybelkada
        - jmurphy97
        - zzhang3
    id: 633d28389addb8530b406c2a
    type: comment
  author: SaulLu
  content: "Hello!\n\nThere is indeed an explanation for this difference in numbers.\
    \ In the `config.json` file, the variable `vocab_size` is only used to define\
    \ the size of the `word_embeddings` and `lm_head` matrices. The constraint we\
    \ have is that the size of these matrices must be greater than or equal to the\
    \ number of tokens known by the tokenizer, the difference of 200 corresponding\
    \ to \"dummy\" tokens that are not used. \n\nThere are several reasons in the\
    \ development of BLOOM that led to this difference. The size of the `word_embeddings`\
    \ and `lm_head` matrixes had to be divisible by a certain number (4*128 from memory)\
    \ so that the model could be parallelized with tensor parallelism. Then, the tokenizer\
    \ was produced before all the model design was finished and it was safer to leave\
    \ tokens available if we needed to add special tokens for training (for PII for\
    \ example)."
  created_at: 2022-10-05 05:46:16+00:00
  edited: false
  hidden: false
  id: 633d28389addb8530b406c2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-10-05T09:01:00.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;VictorSanh&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/VictorSanh\"\
          >@<span class=\"underline\">VictorSanh</span></a></span>\n\n\t</span></span>\
          \ &amp; <span data-props=\"{&quot;user&quot;:&quot;SaulLu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/SaulLu\">@<span class=\"\
          underline\">SaulLu</span></a></span>\n\n\t</span></span> for the explanation!<br>I\
          \ agree that the docstring of <code>BloomConfig</code> is slightly confusing,\
          \ I propose to address this issue in <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/pull/19336/files\"\
          >https://github.com/huggingface/transformers/pull/19336/files</a> !</p>\n"
        raw: 'Thanks @VictorSanh & @SaulLu for the explanation!

          I agree that the docstring of `BloomConfig` is slightly confusing, I propose
          to address this issue in https://github.com/huggingface/transformers/pull/19336/files
          !'
        updatedAt: '2022-10-05T09:01:00.157Z'
      numEdits: 0
      reactions: []
    id: 633d47cc8bbe861bdcd52f29
    type: comment
  author: ybelkada
  content: 'Thanks @VictorSanh & @SaulLu for the explanation!

    I agree that the docstring of `BloomConfig` is slightly confusing, I propose to
    address this issue in https://github.com/huggingface/transformers/pull/19336/files
    !'
  created_at: 2022-10-05 08:01:00+00:00
  edited: false
  hidden: false
  id: 633d47cc8bbe861bdcd52f29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2022-10-05T13:27:33.000Z'
    data:
      edited: false
      editors:
      - VictorSanh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
          fullname: Victor Sanh
          isHf: true
          isPro: true
          name: VictorSanh
          type: user
        html: "<p>Thank you for the explanation and the PR <span data-props=\"{&quot;user&quot;:&quot;SaulLu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SaulLu\"\
          >@<span class=\"underline\">SaulLu</span></a></span>\n\n\t</span></span>\
          \ &amp; <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> !<br>I understand\
          \ now, closing this.</p>\n"
        raw: 'Thank you for the explanation and the PR @SaulLu & @ybelkada !

          I understand now, closing this.'
        updatedAt: '2022-10-05T13:27:33.599Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ybelkada
      relatedEventId: 633d8645c6e26ad2575afdfa
    id: 633d8645c6e26ad2575afdf9
    type: comment
  author: VictorSanh
  content: 'Thank you for the explanation and the PR @SaulLu & @ybelkada !

    I understand now, closing this.'
  created_at: 2022-10-05 12:27:33+00:00
  edited: false
  hidden: false
  id: 633d8645c6e26ad2575afdf9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619623771844-5ecea265968f6028e0559fa5.jpeg?w=200&h=200&f=face
      fullname: Victor Sanh
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: VictorSanh
      type: user
    createdAt: '2022-10-05T13:27:33.000Z'
    data:
      status: closed
    id: 633d8645c6e26ad2575afdfa
    type: status-change
  author: VictorSanh
  created_at: 2022-10-05 12:27:33+00:00
  id: 633d8645c6e26ad2575afdfa
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 120
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: '`vocab_size` mismatch?'
