!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tombriss
conflicting_files: null
created_at: 2022-07-26 15:49:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/662c040b6413d6a1e0ea5594b398487f.svg
      fullname: Thomas Brisson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tombriss
      type: user
    createdAt: '2022-07-26T16:49:55.000Z'
    data:
      edited: false
      editors:
      - Tombriss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/662c040b6413d6a1e0ea5594b398487f.svg
          fullname: Thomas Brisson
          isHf: false
          isPro: false
          name: Tombriss
          type: user
        html: '<p>Mathematically, putting two dense layers that follow each other
          without an activation layer in between is equivalent to a single one with
          fewer parameters (just affine composition). So I am surprised to see these
          linear layers "dense_h_to_4h" and "dense_4h_to_h" following each other in
          the model (at least, that''s what pytorch shows). </p>

          <p>What do I miss ? Is that equivalent during inference but not during training
          or something like that ? </p>

          <p>Thanks !</p>

          '
        raw: "Mathematically, putting two dense layers that follow each other without\
          \ an activation layer in between is equivalent to a single one with fewer\
          \ parameters (just affine composition). So I am surprised to see these linear\
          \ layers \"dense_h_to_4h\" and \"dense_4h_to_h\" following each other in\
          \ the model (at least, that's what pytorch shows). \r\n\r\nWhat do I miss\
          \ ? Is that equivalent during inference but not during training or something\
          \ like that ? \r\n\r\nThanks !"
        updatedAt: '2022-07-26T16:49:55.516Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - eliolio
    id: 62e01b3368d80187d20bac79
    type: comment
  author: Tombriss
  content: "Mathematically, putting two dense layers that follow each other without\
    \ an activation layer in between is equivalent to a single one with fewer parameters\
    \ (just affine composition). So I am surprised to see these linear layers \"dense_h_to_4h\"\
    \ and \"dense_4h_to_h\" following each other in the model (at least, that's what\
    \ pytorch shows). \r\n\r\nWhat do I miss ? Is that equivalent during inference\
    \ but not during training or something like that ? \r\n\r\nThanks !"
  created_at: 2022-07-26 15:49:55+00:00
  edited: false
  hidden: false
  id: 62e01b3368d80187d20bac79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-07-26T21:55:27.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Tombriss&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Tombriss\"\
          >@<span class=\"underline\">Tombriss</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Thanks for your message !<br>In the current implementation we\
          \ do have a custom GeLU function between those 2 layers, please check this\
          \ line:   <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/ee67e7ad4fd7a766891b68f708cf03e30f609976/src/transformers/models/bloom/modeling_bloom.py#L360\"\
          >https://github.com/huggingface/transformers/blob/ee67e7ad4fd7a766891b68f708cf03e30f609976/src/transformers/models/bloom/modeling_bloom.py#L360</a></p>\n"
        raw: "Hi @Tombriss \n\nThanks for your message ! \nIn the current implementation\
          \ we do have a custom GeLU function between those 2 layers, please check\
          \ this line:   https://github.com/huggingface/transformers/blob/ee67e7ad4fd7a766891b68f708cf03e30f609976/src/transformers/models/bloom/modeling_bloom.py#L360"
        updatedAt: '2022-07-26T21:55:27.293Z'
      numEdits: 0
      reactions: []
    id: 62e062cfde3f908aa6e97168
    type: comment
  author: ybelkada
  content: "Hi @Tombriss \n\nThanks for your message ! \nIn the current implementation\
    \ we do have a custom GeLU function between those 2 layers, please check this\
    \ line:   https://github.com/huggingface/transformers/blob/ee67e7ad4fd7a766891b68f708cf03e30f609976/src/transformers/models/bloom/modeling_bloom.py#L360"
  created_at: 2022-07-26 20:55:27+00:00
  edited: false
  hidden: false
  id: 62e062cfde3f908aa6e97168
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/662c040b6413d6a1e0ea5594b398487f.svg
      fullname: Thomas Brisson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tombriss
      type: user
    createdAt: '2022-07-27T07:19:36.000Z'
    data:
      edited: false
      editors:
      - Tombriss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/662c040b6413d6a1e0ea5594b398487f.svg
          fullname: Thomas Brisson
          isHf: false
          isPro: false
          name: Tombriss
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>,\
          \ thanks a lot for your quick answer. Indeed, I should have inspected the\
          \ code before asking any question, sorry for that :)<br>I was just puzzled\
          \ when seeing pytorch modules printing :<br><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/1658906256569-62973e82586fa904a84368db.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1658906256569-62973e82586fa904a84368db.png\"\
          ></a></p>\n<p>It's now clear :)</p>\n"
        raw: 'Hi @ybelkada, thanks a lot for your quick answer. Indeed, I should have
          inspected the code before asking any question, sorry for that :)

          I was just puzzled when seeing pytorch modules printing :

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1658906256569-62973e82586fa904a84368db.png)


          It''s now clear :)'
        updatedAt: '2022-07-27T07:19:36.015Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ybelkada
    id: 62e0e708f0a7a9902e5dd5ef
    type: comment
  author: Tombriss
  content: 'Hi @ybelkada, thanks a lot for your quick answer. Indeed, I should have
    inspected the code before asking any question, sorry for that :)

    I was just puzzled when seeing pytorch modules printing :

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1658906256569-62973e82586fa904a84368db.png)


    It''s now clear :)'
  created_at: 2022-07-27 06:19:36+00:00
  edited: false
  hidden: false
  id: 62e0e708f0a7a9902e5dd5ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-07-27T08:13:56.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>No worries! I agree this is a bit confusing for users. I have made
          a Pull Request here: <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/18312">https://github.com/huggingface/transformers/pull/18312</a>
          that should solve this problem</p>

          '
        raw: 'No worries! I agree this is a bit confusing for users. I have made a
          Pull Request here: https://github.com/huggingface/transformers/pull/18312
          that should solve this problem'
        updatedAt: '2022-07-27T08:13:56.575Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F917"
        users:
        - cakiki
        - muhtasham
        - eliolio
        - Tombriss
        - julien-c
    id: 62e0f3c44db2175cd270c0cb
    type: comment
  author: ybelkada
  content: 'No worries! I agree this is a bit confusing for users. I have made a Pull
    Request here: https://github.com/huggingface/transformers/pull/18312 that should
    solve this problem'
  created_at: 2022-07-27 07:13:56+00:00
  edited: false
  hidden: false
  id: 62e0f3c44db2175cd270c0cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2022-07-28T12:31:38.000Z'
    data:
      edited: false
      editors:
      - cakiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
          fullname: Christopher Akiki
          isHf: false
          isPro: false
          name: cakiki
          type: user
        html: "<p>Closing as <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ 's PR was merged :-)</p>\n"
        raw: Closing as @ybelkada 's PR was merged :-)
        updatedAt: '2022-07-28T12:31:38.572Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - Tombriss
        - muhtasham
        - julien-c
        - ybelkada
      relatedEventId: 62e281aa5c0c84966afbb72c
    id: 62e281aa5c0c84966afbb72b
    type: comment
  author: cakiki
  content: Closing as @ybelkada 's PR was merged :-)
  created_at: 2022-07-28 11:31:38+00:00
  edited: false
  hidden: false
  id: 62e281aa5c0c84966afbb72b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2022-07-28T12:31:38.000Z'
    data:
      status: closed
    id: 62e281aa5c0c84966afbb72c
    type: status-change
  author: cakiki
  created_at: 2022-07-28 11:31:38+00:00
  id: 62e281aa5c0c84966afbb72c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 64
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: Why are there some "dense_h_to_4h" and "dense_4h_to_h" layers without any activation
  layers in between ?
