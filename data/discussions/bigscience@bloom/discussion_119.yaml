!!python/object:huggingface_hub.community.DiscussionWithDetails
author: byeai
conflicting_files: null
created_at: 2022-09-30 12:14:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5df3f38f53724524d29267a6de32d1de.svg
      fullname: Mark Berg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: byeai
      type: user
    createdAt: '2022-09-30T13:14:45.000Z'
    data:
      edited: false
      editors:
      - byeai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5df3f38f53724524d29267a6de32d1de.svg
          fullname: Mark Berg
          isHf: false
          isPro: false
          name: byeai
          type: user
        html: "<p>Hello,</p>\n<p>I am a bit of a hobbyist looking to use the amazing\
          \ tutorial of <span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/arteagac\"\
          >@<span class=\"underline\">arteagac</span></a></span>\n\n\t</span></span>\
          \ to run my own instance of Bloom-176B as a personal project on  an Nvidia\
          \ RTX 3090. Several users in this forum seem to be already doing this as\
          \ far as I've seen using a slight adjustment in the code.</p>\n<p>How long\
          \ does computing a token take using this GPU? The article above mentions\
          \ that it takes about 3 minutes to generate a token with an i5 processor.\
          \ Is there even notable performance to be gained by making use of the GPU\
          \ or is the SSD the deciding bottleneck here? (using a PCIe 4.0 Samsung\
          \ 980 Pro just like in the article)</p>\n<p>Thank you for your help.</p>\n\
          <p>Here's the mentioned article: <a rel=\"nofollow\" href=\"https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32\"\
          >https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32</a></p>\n"
        raw: "Hello,\r\n\r\nI am a bit of a hobbyist looking to use the amazing tutorial\
          \ of @arteagac to run my own instance of Bloom-176B as a personal project\
          \ on  an Nvidia RTX 3090. Several users in this forum seem to be already\
          \ doing this as far as I've seen using a slight adjustment in the code.\r\
          \n\r\nHow long does computing a token take using this GPU? The article above\
          \ mentions that it takes about 3 minutes to generate a token with an i5\
          \ processor. Is there even notable performance to be gained by making use\
          \ of the GPU or is the SSD the deciding bottleneck here? (using a PCIe 4.0\
          \ Samsung 980 Pro just like in the article)\r\n\r\nThank you for your help.\r\
          \n\r\nHere's the mentioned article: https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32\r\
          \n"
        updatedAt: '2022-09-30T13:14:45.528Z'
      numEdits: 0
      reactions: []
    id: 6336ebc5be97f1977e78b8ec
    type: comment
  author: byeai
  content: "Hello,\r\n\r\nI am a bit of a hobbyist looking to use the amazing tutorial\
    \ of @arteagac to run my own instance of Bloom-176B as a personal project on \
    \ an Nvidia RTX 3090. Several users in this forum seem to be already doing this\
    \ as far as I've seen using a slight adjustment in the code.\r\n\r\nHow long does\
    \ computing a token take using this GPU? The article above mentions that it takes\
    \ about 3 minutes to generate a token with an i5 processor. Is there even notable\
    \ performance to be gained by making use of the GPU or is the SSD the deciding\
    \ bottleneck here? (using a PCIe 4.0 Samsung 980 Pro just like in the article)\r\
    \n\r\nThank you for your help.\r\n\r\nHere's the mentioned article: https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32\r\
    \n"
  created_at: 2022-09-30 12:14:45+00:00
  edited: false
  hidden: false
  id: 6336ebc5be97f1977e78b8ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2022-09-30T19:03:17.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;byeai&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/byeai\">@<span class=\"\
          underline\">byeai</span></a></span>\n\n\t</span></span>, you can easily\
          \ run it on your 3090 by simply setting <code>device='cuda:0'</code> in\
          \ the script. However, I think this won't significantly speed up the token\
          \ generation, because as you said, the bottleneck is in reading the model\
          \ blocks from disk.  In a hypothetical scenario where you had enough ram\
          \ (&gt;400 GB) to fit all the model at once, then you will likely see some\
          \ significant improvements in speed using a GPU. In such a case, the only\
          \ bottleneck would be moving the model back and forth between the RAM and\
          \ GPU memory, but that should be manageable.</p>\n"
        raw: Hi @byeai, you can easily run it on your 3090 by simply setting `device='cuda:0'`
          in the script. However, I think this won't significantly speed up the token
          generation, because as you said, the bottleneck is in reading the model
          blocks from disk.  In a hypothetical scenario where you had enough ram (>400
          GB) to fit all the model at once, then you will likely see some significant
          improvements in speed using a GPU. In such a case, the only bottleneck would
          be moving the model back and forth between the RAM and GPU memory, but that
          should be manageable.
        updatedAt: '2022-09-30T19:03:17.953Z'
      numEdits: 0
      reactions: []
    id: 63373d75d64a730a3e91cd7d
    type: comment
  author: arteagac
  content: Hi @byeai, you can easily run it on your 3090 by simply setting `device='cuda:0'`
    in the script. However, I think this won't significantly speed up the token generation,
    because as you said, the bottleneck is in reading the model blocks from disk.  In
    a hypothetical scenario where you had enough ram (>400 GB) to fit all the model
    at once, then you will likely see some significant improvements in speed using
    a GPU. In such a case, the only bottleneck would be moving the model back and
    forth between the RAM and GPU memory, but that should be manageable.
  created_at: 2022-09-30 18:03:17+00:00
  edited: false
  hidden: false
  id: 63373d75d64a730a3e91cd7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/977238ce1df1da7777a525f59bee7f87.svg
      fullname: Chris Livingston
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cliv24
      type: user
    createdAt: '2022-12-22T19:46:15.000Z'
    data:
      edited: false
      editors:
      - cliv24
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/977238ce1df1da7777a525f59bee7f87.svg
          fullname: Chris Livingston
          isHf: false
          isPro: false
          name: cliv24
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/arteagac\">@<span class=\"\
          underline\">arteagac</span></a></span>\n\n\t</span></span>  Apologies for\
          \ the bump.</p>\n<p>So given a system configuration of a single RTX 3090\
          \ plus 512GB of ordinary RAM, would there be any changes to the script (besides\
          \ <code>device='cuda:0'</code>) to ensure that it loads the whole model\
          \ to memory instead of running from disk?</p>\n"
        raw: '@arteagac  Apologies for the bump.


          So given a system configuration of a single RTX 3090 plus 512GB of ordinary
          RAM, would there be any changes to the script (besides `device=''cuda:0''`)
          to ensure that it loads the whole model to memory instead of running from
          disk?'
        updatedAt: '2022-12-22T19:46:15.074Z'
      numEdits: 0
      reactions: []
    id: 63a4b40727f1f64ed72c2fa8
    type: comment
  author: cliv24
  content: '@arteagac  Apologies for the bump.


    So given a system configuration of a single RTX 3090 plus 512GB of ordinary RAM,
    would there be any changes to the script (besides `device=''cuda:0''`) to ensure
    that it loads the whole model to memory instead of running from disk?'
  created_at: 2022-12-22 19:46:15+00:00
  edited: false
  hidden: false
  id: 63a4b40727f1f64ed72c2fa8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2022-12-22T23:34:27.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;cliv24&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cliv24\">@<span class=\"\
          underline\">cliv24</span></a></span>\n\n\t</span></span>, </p>\n<p>If you\
          \ have enough RAM to fit the entire model, then you need some minimal changes\
          \ in the script. First, remove the portion that loads each BLOOM block in\
          \ the <code>forward</code> method. Second, load the entire model at the\
          \ beggining of the script and save it on a list of blocks (for this you\
          \ can re-use the code you removed from the <code>forward</code>method).\
          \ Finally use this list of blocks inside the <code>forward</code> method\
          \ to process each block. Note that in this final step you need to properly\
          \ load and offload each block from the GPU. If you need further assistance,\
          \ please let me know, I would be more than happy to help.</p>\n"
        raw: "Hi @cliv24, \n\nIf you have enough RAM to fit the entire model, then\
          \ you need some minimal changes in the script. First, remove the portion\
          \ that loads each BLOOM block in the `forward` method. Second, load the\
          \ entire model at the beggining of the script and save it on a list of blocks\
          \ (for this you can re-use the code you removed from the `forward`method).\
          \ Finally use this list of blocks inside the `forward` method to process\
          \ each block. Note that in this final step you need to properly load and\
          \ offload each block from the GPU. If you need further assistance, please\
          \ let me know, I would be more than happy to help."
        updatedAt: '2022-12-22T23:34:27.245Z'
      numEdits: 0
      reactions: []
    id: 63a4e98327f1f64ed733c0c0
    type: comment
  author: arteagac
  content: "Hi @cliv24, \n\nIf you have enough RAM to fit the entire model, then you\
    \ need some minimal changes in the script. First, remove the portion that loads\
    \ each BLOOM block in the `forward` method. Second, load the entire model at the\
    \ beggining of the script and save it on a list of blocks (for this you can re-use\
    \ the code you removed from the `forward`method). Finally use this list of blocks\
    \ inside the `forward` method to process each block. Note that in this final step\
    \ you need to properly load and offload each block from the GPU. If you need further\
    \ assistance, please let me know, I would be more than happy to help."
  created_at: 2022-12-22 23:34:27+00:00
  edited: false
  hidden: false
  id: 63a4e98327f1f64ed733c0c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ccdcee95f068039d77ca4d7f2388e6ff.svg
      fullname: tom d
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sdenton4
      type: user
    createdAt: '2022-12-25T22:32:13.000Z'
    data:
      edited: false
      editors:
      - sdenton4
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ccdcee95f068039d77ca4d7f2388e6ff.svg
          fullname: tom d
          isHf: false
          isPro: false
          name: sdenton4
          type: user
        html: '<p>if you''ve got 500gb of ram lying around, you may be able to specify
          most of it as a RAM Disk, move the model weights there, and use the model
          as you normally would... Assuming the IO pipeline is reasonably optimized
          - no overhead of checking file hashes every time you read a file, etc -
          you might get pretty close to full-speed without any extra work. (I haven''t
          tried this for the models in question, but have used this trick effectively
          in many other contexts.)</p>

          '
        raw: if you've got 500gb of ram lying around, you may be able to specify most
          of it as a RAM Disk, move the model weights there, and use the model as
          you normally would... Assuming the IO pipeline is reasonably optimized -
          no overhead of checking file hashes every time you read a file, etc - you
          might get pretty close to full-speed without any extra work. (I haven't
          tried this for the models in question, but have used this trick effectively
          in many other contexts.)
        updatedAt: '2022-12-25T22:32:13.670Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - greboide
        - nightfuryx
    id: 63a8cf6d0046b663fa5a3d16
    type: comment
  author: sdenton4
  content: if you've got 500gb of ram lying around, you may be able to specify most
    of it as a RAM Disk, move the model weights there, and use the model as you normally
    would... Assuming the IO pipeline is reasonably optimized - no overhead of checking
    file hashes every time you read a file, etc - you might get pretty close to full-speed
    without any extra work. (I haven't tried this for the models in question, but
    have used this trick effectively in many other contexts.)
  created_at: 2022-12-25 22:32:13+00:00
  edited: false
  hidden: false
  id: 63a8cf6d0046b663fa5a3d16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1a0c5e59bfef39e4c42b29123adcfa07.svg
      fullname: Tin Tran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nightfuryx
      type: user
    createdAt: '2023-01-14T11:20:09.000Z'
    data:
      edited: false
      editors:
      - nightfuryx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1a0c5e59bfef39e4c42b29123adcfa07.svg
          fullname: Tin Tran
          isHf: false
          isPro: false
          name: nightfuryx
          type: user
        html: "<blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;cliv24&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/cliv24\"\
          >@<span class=\"underline\">cliv24</span></a></span>\n\n\t</span></span>,\
          \ </p>\n<p>If you have enough RAM to fit the entire model, then you need\
          \ some minimal changes in the script. First, remove the portion that loads\
          \ each BLOOM block in the <code>forward</code> method. Second, load the\
          \ entire model at the beggining of the script and save it on a list of blocks\
          \ (for this you can re-use the code you removed from the <code>forward</code>method).\
          \ Finally use this list of blocks inside the <code>forward</code> method\
          \ to process each block. Note that in this final step you need to properly\
          \ load and offload each block from the GPU. If you need further assistance,\
          \ please let me know, I would be more than happy to help.</p>\n</blockquote>\n\
          <p>Hi <span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/arteagac\">@<span class=\"\
          underline\">arteagac</span></a></span>\n\n\t</span></span> would it be possible\
          \ for you to post the modified script of this? for anyone that has the hardware\
          \ to try.</p>\n<p>512GB of ram is around $1800 and I'm tempting to give\
          \ that a try assuming the ram does give significant speed up.</p>\n"
        raw: "> Hi @cliv24, \n> \n> If you have enough RAM to fit the entire model,\
          \ then you need some minimal changes in the script. First, remove the portion\
          \ that loads each BLOOM block in the `forward` method. Second, load the\
          \ entire model at the beggining of the script and save it on a list of blocks\
          \ (for this you can re-use the code you removed from the `forward`method).\
          \ Finally use this list of blocks inside the `forward` method to process\
          \ each block. Note that in this final step you need to properly load and\
          \ offload each block from the GPU. If you need further assistance, please\
          \ let me know, I would be more than happy to help.\n\nHi @arteagac would\
          \ it be possible for you to post the modified script of this? for anyone\
          \ that has the hardware to try.\n\n512GB of ram is around $1800 and I'm\
          \ tempting to give that a try assuming the ram does give significant speed\
          \ up."
        updatedAt: '2023-01-14T11:20:09.947Z'
      numEdits: 0
      reactions: []
    id: 63c28fe9aa5c9b841706fec4
    type: comment
  author: nightfuryx
  content: "> Hi @cliv24, \n> \n> If you have enough RAM to fit the entire model,\
    \ then you need some minimal changes in the script. First, remove the portion\
    \ that loads each BLOOM block in the `forward` method. Second, load the entire\
    \ model at the beggining of the script and save it on a list of blocks (for this\
    \ you can re-use the code you removed from the `forward`method). Finally use this\
    \ list of blocks inside the `forward` method to process each block. Note that\
    \ in this final step you need to properly load and offload each block from the\
    \ GPU. If you need further assistance, please let me know, I would be more than\
    \ happy to help.\n\nHi @arteagac would it be possible for you to post the modified\
    \ script of this? for anyone that has the hardware to try.\n\n512GB of ram is\
    \ around $1800 and I'm tempting to give that a try assuming the ram does give\
    \ significant speed up."
  created_at: 2023-01-14 11:20:09+00:00
  edited: false
  hidden: false
  id: 63c28fe9aa5c9b841706fec4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2023-01-16T19:42:11.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;nightfuryx&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nightfuryx\"\
          >@<span class=\"underline\">nightfuryx</span></a></span>\n\n\t</span></span>,\
          \ Sure, in the link below you can find a modified script that assumes you\
          \ have enough RAM to fit the entire model. Unfortunately, this script is\
          \ untested, as I don't have enough RAM to test it, but if you run into any\
          \ errors, please let me know and I would be happy to help you fix them.</p>\n\
          <p><a rel=\"nofollow\" href=\"https://github.com/arteagac/arteagac.github.io/blob/master/blog/bloom_ram.ipynb\"\
          >https://github.com/arteagac/arteagac.github.io/blob/master/blog/bloom_ram.ipynb</a></p>\n"
        raw: 'Hi @nightfuryx, Sure, in the link below you can find a modified script
          that assumes you have enough RAM to fit the entire model. Unfortunately,
          this script is untested, as I don''t have enough RAM to test it, but if
          you run into any errors, please let me know and I would be happy to help
          you fix them.


          https://github.com/arteagac/arteagac.github.io/blob/master/blog/bloom_ram.ipynb'
        updatedAt: '2023-01-16T19:42:11.244Z'
      numEdits: 0
      reactions: []
    id: 63c5a893445f13c9aeb26e29
    type: comment
  author: arteagac
  content: 'Hi @nightfuryx, Sure, in the link below you can find a modified script
    that assumes you have enough RAM to fit the entire model. Unfortunately, this
    script is untested, as I don''t have enough RAM to test it, but if you run into
    any errors, please let me know and I would be happy to help you fix them.


    https://github.com/arteagac/arteagac.github.io/blob/master/blog/bloom_ram.ipynb'
  created_at: 2023-01-16 19:42:11+00:00
  edited: false
  hidden: false
  id: 63c5a893445f13c9aeb26e29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1a0c5e59bfef39e4c42b29123adcfa07.svg
      fullname: Tin Tran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nightfuryx
      type: user
    createdAt: '2023-01-17T04:27:52.000Z'
    data:
      edited: false
      editors:
      - nightfuryx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1a0c5e59bfef39e4c42b29123adcfa07.svg
          fullname: Tin Tran
          isHf: false
          isPro: false
          name: nightfuryx
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/arteagac\"\
          >@<span class=\"underline\">arteagac</span></a></span>\n\n\t</span></span><br>Thank\
          \ you very much for taking the time to update. I tried a similar approach\
          \ to yours over the weekend however it seems only the original implementation\
          \ would offload the  model correctly. When I do the load multiple blocks\
          \ and then move one by one to the GPU, then the gpu does not release this\
          \ block afterward before loading the next block, so I keep running into\
          \ cuda out of memory and have not found a good solution for that.</p>\n"
        raw: "hi @arteagac \nThank you very much for taking the time to update. I\
          \ tried a similar approach to yours over the weekend however it seems only\
          \ the original implementation would offload the  model correctly. When I\
          \ do the load multiple blocks and then move one by one to the GPU, then\
          \ the gpu does not release this block afterward before loading the next\
          \ block, so I keep running into cuda out of memory and have not found a\
          \ good solution for that."
        updatedAt: '2023-01-17T04:27:52.625Z'
      numEdits: 0
      reactions: []
    id: 63c623c80564f3b643c6064b
    type: comment
  author: nightfuryx
  content: "hi @arteagac \nThank you very much for taking the time to update. I tried\
    \ a similar approach to yours over the weekend however it seems only the original\
    \ implementation would offload the  model correctly. When I do the load multiple\
    \ blocks and then move one by one to the GPU, then the gpu does not release this\
    \ block afterward before loading the next block, so I keep running into cuda out\
    \ of memory and have not found a good solution for that."
  created_at: 2023-01-17 04:27:52+00:00
  edited: false
  hidden: false
  id: 63c623c80564f3b643c6064b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2023-01-23T21:26:24.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;nightfuryx&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nightfuryx\"\
          >@<span class=\"underline\">nightfuryx</span></a></span>\n\n\t</span></span>,\
          \ Yes, I just noticed that is a potential issue. To address this, you simply\
          \ need to find a way to move the block to the GPU, one at a time. I assume\
          \ you can create a copy of the block and send it to the GPU and then overwrite\
          \ the same copy object in the next interation.</p>\n"
        raw: Hi @nightfuryx, Yes, I just noticed that is a potential issue. To address
          this, you simply need to find a way to move the block to the GPU, one at
          a time. I assume you can create a copy of the block and send it to the GPU
          and then overwrite the same copy object in the next interation.
        updatedAt: '2023-01-23T21:26:24.284Z'
      numEdits: 0
      reactions: []
    id: 63cefb80cd4bf28655ab8aad
    type: comment
  author: arteagac
  content: Hi @nightfuryx, Yes, I just noticed that is a potential issue. To address
    this, you simply need to find a way to move the block to the GPU, one at a time.
    I assume you can create a copy of the block and send it to the GPU and then overwrite
    the same copy object in the next interation.
  created_at: 2023-01-23 21:26:24+00:00
  edited: false
  hidden: false
  id: 63cefb80cd4bf28655ab8aad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a45a442cbdec1c5b6b0c306501627b23.svg
      fullname: Li Tsz Fung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelvinli709
      type: user
    createdAt: '2023-01-26T02:04:31.000Z'
    data:
      edited: false
      editors:
      - Kelvinli709
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a45a442cbdec1c5b6b0c306501627b23.svg
          fullname: Li Tsz Fung
          isHf: false
          isPro: false
          name: Kelvinli709
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/arteagac\"\
          >@<span class=\"underline\">arteagac</span></a></span>\n\n\t</span></span>\
          \ I wonder is there any even cheaper way to run the model in fast speed.<br>Like\
          \ using raid0 with 3+ U.2 ssd, it should be easily reach ddr4 ram performant\
          \ or even faster if using more disk.<br>But the problem is GPU, do you think\
          \ the model can run in the weak GPU like 1060?</p>\n"
        raw: "Hi, @arteagac I wonder is there any even cheaper way to run the model\
          \ in fast speed. \nLike using raid0 with 3+ U.2 ssd, it should be easily\
          \ reach ddr4 ram performant or even faster if using more disk.\nBut the\
          \ problem is GPU, do you think the model can run in the weak GPU like 1060?"
        updatedAt: '2023-01-26T02:04:31.793Z'
      numEdits: 0
      reactions: []
    id: 63d1dfaf119416cdbe21e09c
    type: comment
  author: Kelvinli709
  content: "Hi, @arteagac I wonder is there any even cheaper way to run the model\
    \ in fast speed. \nLike using raid0 with 3+ U.2 ssd, it should be easily reach\
    \ ddr4 ram performant or even faster if using more disk.\nBut the problem is GPU,\
    \ do you think the model can run in the weak GPU like 1060?"
  created_at: 2023-01-26 02:04:31+00:00
  edited: false
  hidden: false
  id: 63d1dfaf119416cdbe21e09c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 119
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Locally run instance in an RTX 3090 - Performance?
