!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mishavee
conflicting_files: null
created_at: 2022-11-30 00:03:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-11-30T00:03:31.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>When training bloom how many tokens could the input be? 2048?</p>

          '
        raw: When training bloom how many tokens could the input be? 2048?
        updatedAt: '2022-11-30T00:03:31.490Z'
      numEdits: 0
      reactions: []
    id: 63869dd3e7857b3e19c8fde7
    type: comment
  author: mishavee
  content: When training bloom how many tokens could the input be? 2048?
  created_at: 2022-11-30 00:03:31+00:00
  edited: false
  hidden: false
  id: 63869dd3e7857b3e19c8fde7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a66318123f7a40fd0bd9536a5929211f.svg
      fullname: Abhishek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: A-bhishek-01
      type: user
    createdAt: '2023-01-26T17:08:35.000Z'
    data:
      edited: false
      editors:
      - A-bhishek-01
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a66318123f7a40fd0bd9536a5929211f.svg
          fullname: Abhishek
          isHf: false
          isPro: false
          name: A-bhishek-01
          type: user
        html: '<p>2048<br>reference: <a rel="nofollow" href="https://arxiv.org/pdf/2211.05100.pdf">https://arxiv.org/pdf/2211.05100.pdf</a></p>

          '
        raw: '2048

          reference: https://arxiv.org/pdf/2211.05100.pdf'
        updatedAt: '2023-01-26T17:08:35.731Z'
      numEdits: 0
      reactions: []
    id: 63d2b393c3eb22526cac2469
    type: comment
  author: A-bhishek-01
  content: '2048

    reference: https://arxiv.org/pdf/2211.05100.pdf'
  created_at: 2023-01-26 17:08:35+00:00
  edited: false
  hidden: false
  id: 63d2b393c3eb22526cac2469
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2023-01-27T07:43:04.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Actually it was trained with sequence length 2048 but the model
          supports any length, you can try generating more tokens to infinity (with
          some performance degradation as you increase the length) it''s linked to
          our use of alibi</p>

          '
        raw: Actually it was trained with sequence length 2048 but the model supports
          any length, you can try generating more tokens to infinity (with some performance
          degradation as you increase the length) it's linked to our use of alibi
        updatedAt: '2023-01-27T07:43:04.015Z'
      numEdits: 0
      reactions: []
    id: 63d380883edd11d1e70cd304
    type: comment
  author: TimeRobber
  content: Actually it was trained with sequence length 2048 but the model supports
    any length, you can try generating more tokens to infinity (with some performance
    degradation as you increase the length) it's linked to our use of alibi
  created_at: 2023-01-27 07:43:04+00:00
  edited: false
  hidden: false
  id: 63d380883edd11d1e70cd304
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2023-01-27T07:52:24.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>how much degradation? Are you saying I can put 100000 words in one
          training example?<br>thanks</p>

          '
        raw: "how much degradation? Are you saying I can put 100000 words in one training\
          \ example? \nthanks"
        updatedAt: '2023-01-27T07:52:24.090Z'
      numEdits: 0
      reactions: []
    id: 63d382b85507ec44cd9eca34
    type: comment
  author: mishavee
  content: "how much degradation? Are you saying I can put 100000 words in one training\
    \ example? \nthanks"
  created_at: 2023-01-27 07:52:24+00:00
  edited: false
  hidden: false
  id: 63d382b85507ec44cd9eca34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2023-01-28T10:28:09.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>It''s specific to your setup. </p>

          <p>For more explanation on what ALIBI is: <a rel="nofollow" href="https://arxiv.org/abs/2108.12409">https://arxiv.org/abs/2108.12409</a><br>For
          some plots where you can understand how good it becomes on long sequence,
          we had a preliminary result (on 1B model) in: <a rel="nofollow" href="https://arxiv.org/abs/2210.15424">https://arxiv.org/abs/2210.15424</a>
          (Figure 2)</p>

          <p>That''s from a modeling perspective. From a pure hardware perspective,  longer
          sequence means more memory footprint, so you might get out of memory issues
          when using 100_000 words (depending on your setup).</p>

          '
        raw: "It's specific to your setup. \n\nFor more explanation on what ALIBI\
          \ is: https://arxiv.org/abs/2108.12409\nFor some plots where you can understand\
          \ how good it becomes on long sequence, we had a preliminary result (on\
          \ 1B model) in: https://arxiv.org/abs/2210.15424 (Figure 2)\n\nThat's from\
          \ a modeling perspective. From a pure hardware perspective,  longer sequence\
          \ means more memory footprint, so you might get out of memory issues when\
          \ using 100_000 words (depending on your setup)."
        updatedAt: '2023-01-28T10:28:09.740Z'
      numEdits: 0
      reactions: []
    id: 63d4f8b998e20226041b15f7
    type: comment
  author: TimeRobber
  content: "It's specific to your setup. \n\nFor more explanation on what ALIBI is:\
    \ https://arxiv.org/abs/2108.12409\nFor some plots where you can understand how\
    \ good it becomes on long sequence, we had a preliminary result (on 1B model)\
    \ in: https://arxiv.org/abs/2210.15424 (Figure 2)\n\nThat's from a modeling perspective.\
    \ From a pure hardware perspective,  longer sequence means more memory footprint,\
    \ so you might get out of memory issues when using 100_000 words (depending on\
    \ your setup)."
  created_at: 2023-01-28 10:28:09+00:00
  edited: false
  hidden: false
  id: 63d4f8b998e20226041b15f7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 144
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Batching token length
