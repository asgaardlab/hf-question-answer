!!python/object:huggingface_hub.community.DiscussionWithDetails
author: norrr
conflicting_files: null
created_at: 2022-08-30 11:57:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2736672b537031e0738a2170e39149e1.svg
      fullname: Nor Newman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: norrr
      type: user
    createdAt: '2022-08-30T12:57:17.000Z'
    data:
      edited: false
      editors:
      - norrr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2736672b537031e0738a2170e39149e1.svg
          fullname: Nor Newman
          isHf: false
          isPro: false
          name: norrr
          type: user
        html: "<p><strong>Not sure if there's any need or any kind of bug reporting,\
          \ but here, have a look.</strong></p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1661864157354-630e0495c0eca3037afee655.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1661864157354-630e0495c0eca3037afee655.png\"\
          ></a></p>\n<p><strong>My prompt:</strong><br>\"Q: What is a virtue?\" <br>\
          \         \"A:\"</p>\n<p><strong>My setup:</strong><br>import time</p>\n\
          <p>def infer(prompt,<br>          max_length = 128,<br>          top_k =\
          \ 0,<br>          num_beams = 0,<br>          no_repeat_ngram_size = 2,<br>\
          \          top_p = 0.9,<br>          seed=42,<br>          temperature=0.7,<br>\
          \          greedy_decoding = False,<br>          return_full_text = False):</p>\n\
          <pre><code>top_k = None if top_k == 0 else top_k\ndo_sample = False if num_beams\
          \ &gt; 0 else not greedy_decoding\nnum_beams = None if (greedy_decoding\
          \ or num_beams == 0) else num_beams\nno_repeat_ngram_size = None if num_beams\
          \ is None else no_repeat_ngram_size\ntop_p = None if num_beams else top_p\n\
          early_stopping = None if num_beams is None else num_beams &gt; 0\n\nparams\
          \ = {\n    \"max_new_tokens\": max_length,\n    \"top_k\": top_k,\n    \"\
          top_p\": top_p,\n    \"temperature\": temperature,\n    \"do_sample\": do_sample,\n\
          \    \"seed\": seed,\n    \"early_stopping\":early_stopping,\n    \"no_repeat_ngram_size\"\
          :no_repeat_ngram_size,\n    \"num_beams\":num_beams,\n    \"return_full_text\"\
          :return_full_text\n}\n\ns = time.time()\nresponse = inference(prompt, params=params)\n\
          #print(response)\nproc_time = time.time()-s\n#print(f\"Processing time was\
          \ {proc_time} seconds\")\nreturn response\n</code></pre>\n"
        raw: "**Not sure if there's any need or any kind of bug reporting, but here,\
          \ have a look.**\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1661864157354-630e0495c0eca3037afee655.png)\r\
          \n\r\n**My prompt:**\r\n\"Q: What is a virtue?\" \\\r\n         \"A:\"\r\
          \n\r\n**My setup:**\r\nimport time\r\n\r\ndef infer(prompt,\r\n        \
          \  max_length = 128,\r\n          top_k = 0,\r\n          num_beams = 0,\r\
          \n          no_repeat_ngram_size = 2,\r\n          top_p = 0.9,\r\n    \
          \      seed=42,\r\n          temperature=0.7,\r\n          greedy_decoding\
          \ = False,\r\n          return_full_text = False):\r\n\r\n\r\n    top_k\
          \ = None if top_k == 0 else top_k\r\n    do_sample = False if num_beams\
          \ > 0 else not greedy_decoding\r\n    num_beams = None if (greedy_decoding\
          \ or num_beams == 0) else num_beams\r\n    no_repeat_ngram_size = None if\
          \ num_beams is None else no_repeat_ngram_size\r\n    top_p = None if num_beams\
          \ else top_p\r\n    early_stopping = None if num_beams is None else num_beams\
          \ > 0\r\n\r\n    params = {\r\n        \"max_new_tokens\": max_length,\r\
          \n        \"top_k\": top_k,\r\n        \"top_p\": top_p,\r\n        \"temperature\"\
          : temperature,\r\n        \"do_sample\": do_sample,\r\n        \"seed\"\
          : seed,\r\n        \"early_stopping\":early_stopping,\r\n        \"no_repeat_ngram_size\"\
          :no_repeat_ngram_size,\r\n        \"num_beams\":num_beams,\r\n        \"\
          return_full_text\":return_full_text\r\n    }\r\n\r\n    s = time.time()\r\
          \n    response = inference(prompt, params=params)\r\n    #print(response)\r\
          \n    proc_time = time.time()-s\r\n    #print(f\"Processing time was {proc_time}\
          \ seconds\")\r\n    return response"
        updatedAt: '2022-08-30T12:57:17.150Z'
      numEdits: 0
      reactions: []
    id: 630e092dc0eca3037aff1b4d
    type: comment
  author: norrr
  content: "**Not sure if there's any need or any kind of bug reporting, but here,\
    \ have a look.**\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1661864157354-630e0495c0eca3037afee655.png)\r\
    \n\r\n**My prompt:**\r\n\"Q: What is a virtue?\" \\\r\n         \"A:\"\r\n\r\n\
    **My setup:**\r\nimport time\r\n\r\ndef infer(prompt,\r\n          max_length\
    \ = 128,\r\n          top_k = 0,\r\n          num_beams = 0,\r\n          no_repeat_ngram_size\
    \ = 2,\r\n          top_p = 0.9,\r\n          seed=42,\r\n          temperature=0.7,\r\
    \n          greedy_decoding = False,\r\n          return_full_text = False):\r\
    \n\r\n\r\n    top_k = None if top_k == 0 else top_k\r\n    do_sample = False if\
    \ num_beams > 0 else not greedy_decoding\r\n    num_beams = None if (greedy_decoding\
    \ or num_beams == 0) else num_beams\r\n    no_repeat_ngram_size = None if num_beams\
    \ is None else no_repeat_ngram_size\r\n    top_p = None if num_beams else top_p\r\
    \n    early_stopping = None if num_beams is None else num_beams > 0\r\n\r\n  \
    \  params = {\r\n        \"max_new_tokens\": max_length,\r\n        \"top_k\"\
    : top_k,\r\n        \"top_p\": top_p,\r\n        \"temperature\": temperature,\r\
    \n        \"do_sample\": do_sample,\r\n        \"seed\": seed,\r\n        \"early_stopping\"\
    :early_stopping,\r\n        \"no_repeat_ngram_size\":no_repeat_ngram_size,\r\n\
    \        \"num_beams\":num_beams,\r\n        \"return_full_text\":return_full_text\r\
    \n    }\r\n\r\n    s = time.time()\r\n    response = inference(prompt, params=params)\r\
    \n    #print(response)\r\n    proc_time = time.time()-s\r\n    #print(f\"Processing\
    \ time was {proc_time} seconds\")\r\n    return response"
  created_at: 2022-08-30 11:57:17+00:00
  edited: false
  hidden: false
  id: 630e092dc0eca3037aff1b4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
      fullname: Stella Biderman
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stellaathena
      type: user
    createdAt: '2022-08-31T02:30:53.000Z'
    data:
      edited: false
      editors:
      - stellaathena
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
          fullname: Stella Biderman
          isHf: false
          isPro: false
          name: stellaathena
          type: user
        html: '<p>I don''t see any reason to assume this is a bug. Language models
          are not guaranteed to produce high quality text and frequently get caught
          in loops like this</p>

          '
        raw: I don't see any reason to assume this is a bug. Language models are not
          guaranteed to produce high quality text and frequently get caught in loops
          like this
        updatedAt: '2022-08-31T02:30:53.471Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - cakiki
    id: 630ec7ddc9af0163b969e0fc
    type: comment
  author: stellaathena
  content: I don't see any reason to assume this is a bug. Language models are not
    guaranteed to produce high quality text and frequently get caught in loops like
    this
  created_at: 2022-08-31 01:30:53+00:00
  edited: false
  hidden: false
  id: 630ec7ddc9af0163b969e0fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/2736672b537031e0738a2170e39149e1.svg
      fullname: Nor Newman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: norrr
      type: user
    createdAt: '2022-08-31T08:12:35.000Z'
    data:
      status: closed
    id: 630f17f302ce39336c3f5875
    type: status-change
  author: norrr
  created_at: 2022-08-31 07:12:35+00:00
  id: 630f17f302ce39336c3f5875
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 97
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: Possible bug to look into?
