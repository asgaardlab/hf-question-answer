!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aeva
conflicting_files: null
created_at: 2022-12-14 08:29:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aad9b209326579e2e746367a70ae833.svg
      fullname: Aeva Palecek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aeva
      type: user
    createdAt: '2022-12-14T08:29:37.000Z'
    data:
      edited: false
      editors:
      - aeva
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aad9b209326579e2e746367a70ae833.svg
          fullname: Aeva Palecek
          isHf: false
          isPro: false
          name: aeva
          type: user
        html: '<p>Hello.  I''m interested in running bloom on a local machine to experiment
          with interactive uses.  I gather from reading various posts here that the
          model really must be resident in ram (or vram) to avoid being bottlenecked
          on repeated disk io, as that bottleneck prevents interactive use on most
          consumer hardware because most people don''t have ~400gb of ram (much less
          vram).</p>

          <p>Suppose I were to build a machine that had enough ram for the model to
          be resident for the lifetime of the program, what kind of CPU would be necessary
          to hit performance in the ballpark of a token per second or better?  Or
          is this use case only suitable for expensive cloud deployment and would
          I be better off exploring a lighter but less capable model (and if so, which
          one?)</p>

          '
        raw: "Hello.  I'm interested in running bloom on a local machine to experiment\
          \ with interactive uses.  I gather from reading various posts here that\
          \ the model really must be resident in ram (or vram) to avoid being bottlenecked\
          \ on repeated disk io, as that bottleneck prevents interactive use on most\
          \ consumer hardware because most people don't have ~400gb of ram (much less\
          \ vram).\r\n\r\nSuppose I were to build a machine that had enough ram for\
          \ the model to be resident for the lifetime of the program, what kind of\
          \ CPU would be necessary to hit performance in the ballpark of a token per\
          \ second or better?  Or is this use case only suitable for expensive cloud\
          \ deployment and would I be better off exploring a lighter but less capable\
          \ model (and if so, which one?)"
        updatedAt: '2022-12-14T08:29:37.202Z'
      numEdits: 0
      reactions: []
    id: 639989719fed2f011017ee1b
    type: comment
  author: aeva
  content: "Hello.  I'm interested in running bloom on a local machine to experiment\
    \ with interactive uses.  I gather from reading various posts here that the model\
    \ really must be resident in ram (or vram) to avoid being bottlenecked on repeated\
    \ disk io, as that bottleneck prevents interactive use on most consumer hardware\
    \ because most people don't have ~400gb of ram (much less vram).\r\n\r\nSuppose\
    \ I were to build a machine that had enough ram for the model to be resident for\
    \ the lifetime of the program, what kind of CPU would be necessary to hit performance\
    \ in the ballpark of a token per second or better?  Or is this use case only suitable\
    \ for expensive cloud deployment and would I be better off exploring a lighter\
    \ but less capable model (and if so, which one?)"
  created_at: 2022-12-14 08:29:37+00:00
  edited: false
  hidden: false
  id: 639989719fed2f011017ee1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-12-14T09:24:00.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;aeva&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aeva\">@<span class=\"\
          underline\">aeva</span></a></span>\n\n\t</span></span><br>You might be interested\
          \ in <a href=\"https://huggingface.co/bigscience/bloom/discussions/152\"\
          >https://huggingface.co/bigscience/bloom/discussions/152</a><br>I believe\
          \ you'll be able to run it locally if it works on a gcolab instance cc <span\
          \ data-props=\"{&quot;user&quot;:&quot;borzunov&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/borzunov\">@<span class=\"\
          underline\">borzunov</span></a></span>\n\n\t</span></span></p>\n"
        raw: "Hi @aeva \nYou might be interested in https://huggingface.co/bigscience/bloom/discussions/152\
          \ \nI believe you'll be able to run it locally if it works on a gcolab instance\
          \ cc @borzunov"
        updatedAt: '2022-12-14T09:24:00.294Z'
      numEdits: 0
      reactions: []
    id: 639996301b05640a071428fd
    type: comment
  author: ybelkada
  content: "Hi @aeva \nYou might be interested in https://huggingface.co/bigscience/bloom/discussions/152\
    \ \nI believe you'll be able to run it locally if it works on a gcolab instance\
    \ cc @borzunov"
  created_at: 2022-12-14 09:24:00+00:00
  edited: false
  hidden: false
  id: 639996301b05640a071428fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aad9b209326579e2e746367a70ae833.svg
      fullname: Aeva Palecek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aeva
      type: user
    createdAt: '2022-12-14T09:46:58.000Z'
    data:
      edited: false
      editors:
      - aeva
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aad9b209326579e2e746367a70ae833.svg
          fullname: Aeva Palecek
          isHf: false
          isPro: false
          name: aeva
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> I doubt Petals\
          \ is appropriate for my use case since it's distributed whereas I'm specifically\
          \ aiming to perform text generation tasks quickly on a single machine without\
          \ requiring a network connection.</p>\n"
        raw: '@ybelkada I doubt Petals is appropriate for my use case since it''s
          distributed whereas I''m specifically aiming to perform text generation
          tasks quickly on a single machine without requiring a network connection.'
        updatedAt: '2022-12-14T09:46:58.652Z'
      numEdits: 0
      reactions: []
    id: 63999b92048e3a8a928f2d60
    type: comment
  author: aeva
  content: '@ybelkada I doubt Petals is appropriate for my use case since it''s distributed
    whereas I''m specifically aiming to perform text generation tasks quickly on a
    single machine without requiring a network connection.'
  created_at: 2022-12-14 09:46:58+00:00
  edited: false
  hidden: false
  id: 63999b92048e3a8a928f2d60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aad9b209326579e2e746367a70ae833.svg
      fullname: Aeva Palecek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aeva
      type: user
    createdAt: '2022-12-15T03:11:11.000Z'
    data:
      edited: true
      editors:
      - aeva
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aad9b209326579e2e746367a70ae833.svg
          fullname: Aeva Palecek
          isHf: false
          isPro: false
          name: aeva
          type: user
        html: '<p>I did some rough math, and the thing I was hoping to do is probably
          not going to be viable with normal hardware, much less at a reasonable cost.  In
          the event that someone finds my analysis interesting or useful (or I made
          an egregious error and someone kindly points it out), this is what I worked
          out:</p>

          <p>The only motherboard I found on newegg that supports enough ram for the
          whole model to be resident only supports DDR4 2133, which if my numbers
          are right, gives a theoretical bandwidth of 15.89 GB/s.  The build cost
          for just the motherboard and the ram is $3464 so far.</p>

          <p>If my reading of <a rel="nofollow" href="https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32">https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32</a>
          last night was correct, that that gives a pessimal throughput of 21 seconds
          per token, assuming no other bottlenecks and middling CPU cache perf.</p>

          <p>I don''t know enough about how BLOOM works to estimate what CPU cache
          perf might look like, and I haven''t found perf reports on that, so I''m
          not going to assume that putting in a nice processor would be in any way
          likely to make the numbers any better than that.</p>

          <p>It looks extremely improbable that BLOOM is viable to run at interactive
          speeds on today''s high end consumer hardware in any configuration (based
          on the above for CPU-centric build, and other threads for GPU-centric build),
          and that individuals with interests like mine are probably better served
          by paying for cloud time (or finding a more accessible model with similar
          capabilities, if any exist).</p>

          '
        raw: 'I did some rough math, and the thing I was hoping to do is probably
          not going to be viable with normal hardware, much less at a reasonable cost.  In
          the event that someone finds my analysis interesting or useful (or I made
          an egregious error and someone kindly points it out), this is what I worked
          out:


          The only motherboard I found on newegg that supports enough ram for the
          whole model to be resident only supports DDR4 2133, which if my numbers
          are right, gives a theoretical bandwidth of 15.89 GB/s.  The build cost
          for just the motherboard and the ram is $3464 so far.


          If my reading of https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32
          last night was correct, that that gives a pessimal throughput of 21 seconds
          per token, assuming no other bottlenecks and middling CPU cache perf.


          I don''t know enough about how BLOOM works to estimate what CPU cache perf
          might look like, and I haven''t found perf reports on that, so I''m not
          going to assume that putting in a nice processor would be in any way likely
          to make the numbers any better than that.


          It looks extremely improbable that BLOOM is viable to run at interactive
          speeds on today''s high end consumer hardware in any configuration (based
          on the above for CPU-centric build, and other threads for GPU-centric build),
          and that individuals with interests like mine are probably better served
          by paying for cloud time (or finding a more accessible model with similar
          capabilities, if any exist).'
        updatedAt: '2022-12-15T03:15:23.908Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nameer
    id: 639a904ffb933db41319f391
    type: comment
  author: aeva
  content: 'I did some rough math, and the thing I was hoping to do is probably not
    going to be viable with normal hardware, much less at a reasonable cost.  In the
    event that someone finds my analysis interesting or useful (or I made an egregious
    error and someone kindly points it out), this is what I worked out:


    The only motherboard I found on newegg that supports enough ram for the whole
    model to be resident only supports DDR4 2133, which if my numbers are right, gives
    a theoretical bandwidth of 15.89 GB/s.  The build cost for just the motherboard
    and the ram is $3464 so far.


    If my reading of https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32
    last night was correct, that that gives a pessimal throughput of 21 seconds per
    token, assuming no other bottlenecks and middling CPU cache perf.


    I don''t know enough about how BLOOM works to estimate what CPU cache perf might
    look like, and I haven''t found perf reports on that, so I''m not going to assume
    that putting in a nice processor would be in any way likely to make the numbers
    any better than that.


    It looks extremely improbable that BLOOM is viable to run at interactive speeds
    on today''s high end consumer hardware in any configuration (based on the above
    for CPU-centric build, and other threads for GPU-centric build), and that individuals
    with interests like mine are probably better served by paying for cloud time (or
    finding a more accessible model with similar capabilities, if any exist).'
  created_at: 2022-12-15 03:11:11+00:00
  edited: true
  hidden: false
  id: 639a904ffb933db41319f391
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4101e5d46b7954050645d328f287513e.svg
      fullname: Shawn Blaszak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sblaszak
      type: user
    createdAt: '2022-12-16T00:52:33.000Z'
    data:
      edited: true
      editors:
      - sblaszak
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4101e5d46b7954050645d328f287513e.svg
          fullname: Shawn Blaszak
          isHf: false
          isPro: false
          name: sblaszak
          type: user
        html: '<p>I''m quite new to all this but, from my understanding of what you''ve
          posted, the primary driver of performance (seconds per token) is memory
          bandwidth, correct?  If so, might you be able to get better performance
          by, instead of focusing on RAM, looking into running multiple NVMe SSD drives
          on a low cost, consumer grade, PCIe 5.0 motherboard?  For example, the system
          built in this video has a claimed real-world bandwidth performance of 28GB
          per second: <a rel="nofollow" href="https://www.youtube.com/watch?v=3voNJPuLydw">https://www.youtube.com/watch?v=3voNJPuLydw</a>  A
          quick search of Newegg shows Intel based motherboards with PCIe 5.0 support,
          and 4+ M.2 slots, starting at around $175.  I don''t know if board at that
          low end of the market would actually provide the full PCIe 5.0 bandwidth
          but even significantly higher end consumer motherboards should still be
          significantly less expensive than the one you mention having found.</p>

          <p>Edit: On deeper inspection, it looks like the video in question is using
          some commercial SSD hardware that is, undoubtedly, expensive and likely
          doesn''t produce anywhere near that bandwidth for anything but pure sequential
          reads.</p>

          '
        raw: 'I''m quite new to all this but, from my understanding of what you''ve
          posted, the primary driver of performance (seconds per token) is memory
          bandwidth, correct?  If so, might you be able to get better performance
          by, instead of focusing on RAM, looking into running multiple NVMe SSD drives
          on a low cost, consumer grade, PCIe 5.0 motherboard?  For example, the system
          built in this video has a claimed real-world bandwidth performance of 28GB
          per second: https://www.youtube.com/watch?v=3voNJPuLydw  A quick search
          of Newegg shows Intel based motherboards with PCIe 5.0 support, and 4+ M.2
          slots, starting at around $175.  I don''t know if board at that low end
          of the market would actually provide the full PCIe 5.0 bandwidth but even
          significantly higher end consumer motherboards should still be significantly
          less expensive than the one you mention having found.


          Edit: On deeper inspection, it looks like the video in question is using
          some commercial SSD hardware that is, undoubtedly, expensive and likely
          doesn''t produce anywhere near that bandwidth for anything but pure sequential
          reads.'
        updatedAt: '2022-12-16T01:24:29.480Z'
      numEdits: 1
      reactions: []
    id: 639bc15134967bcf455eb53a
    type: comment
  author: sblaszak
  content: 'I''m quite new to all this but, from my understanding of what you''ve
    posted, the primary driver of performance (seconds per token) is memory bandwidth,
    correct?  If so, might you be able to get better performance by, instead of focusing
    on RAM, looking into running multiple NVMe SSD drives on a low cost, consumer
    grade, PCIe 5.0 motherboard?  For example, the system built in this video has
    a claimed real-world bandwidth performance of 28GB per second: https://www.youtube.com/watch?v=3voNJPuLydw  A
    quick search of Newegg shows Intel based motherboards with PCIe 5.0 support, and
    4+ M.2 slots, starting at around $175.  I don''t know if board at that low end
    of the market would actually provide the full PCIe 5.0 bandwidth but even significantly
    higher end consumer motherboards should still be significantly less expensive
    than the one you mention having found.


    Edit: On deeper inspection, it looks like the video in question is using some
    commercial SSD hardware that is, undoubtedly, expensive and likely doesn''t produce
    anywhere near that bandwidth for anything but pure sequential reads.'
  created_at: 2022-12-16 00:52:33+00:00
  edited: true
  hidden: false
  id: 639bc15134967bcf455eb53a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aad9b209326579e2e746367a70ae833.svg
      fullname: Aeva Palecek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aeva
      type: user
    createdAt: '2022-12-16T01:55:33.000Z'
    data:
      edited: true
      editors:
      - aeva
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aad9b209326579e2e746367a70ae833.svg
          fullname: Aeva Palecek
          isHf: false
          isPro: false
          name: aeva
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sblaszak&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sblaszak\">@<span class=\"\
          underline\">sblaszak</span></a></span>\n\n\t</span></span> even if the fastest\
          \ available NVMe is about 7500 MB/s right now, you raise an interesting\
          \ point about PCIe bandwidth and the availability of motherboards supporting\
          \ multiple NVMe slots. </p>\n<p>According to wikipedia, PCIe 5's bandwidth\
          \ is 63&nbsp;GB/s (and the later revisions have very promising gains there\
          \ too).  I haven't thought this through, but if it's possible to meaningfully\
          \ parallelize the IO across multiple high end NVMes, then maybe this could\
          \ be made to work with a much lower amount of high end ram.  From my notes\
          \ from yesterday, 44.7 GB/s is probably the best possible ram bandwidth\
          \ for normal hardware right now, which I think is what we'd have to beat\
          \ for disk IO to stop being the main bottleneck.  I don't know off hand\
          \ if that 44.7 GB/s is per slot or the bus total, but assuming this can\
          \ be set up so that streaming the new data into ram doesn't saturate the\
          \ bus, a double buffering scheme for BLOOM blocks could then hit 7 seconds\
          \ per token.  This is hand waving a lot obviously, and I don't know what\
          \ kind of CPU would be needed.</p>\n<p>Would RAID0ing 6 NVMes be able to\
          \ hit the same bandwidth as the ram?</p>\n"
        raw: "@sblaszak even if the fastest available NVMe is about 7500 MB/s right\
          \ now, you raise an interesting point about PCIe bandwidth and the availability\
          \ of motherboards supporting multiple NVMe slots. \n\nAccording to wikipedia,\
          \ PCIe 5's bandwidth is 63\_GB/s (and the later revisions have very promising\
          \ gains there too).  I haven't thought this through, but if it's possible\
          \ to meaningfully parallelize the IO across multiple high end NVMes, then\
          \ maybe this could be made to work with a much lower amount of high end\
          \ ram.  From my notes from yesterday, 44.7 GB/s is probably the best possible\
          \ ram bandwidth for normal hardware right now, which I think is what we'd\
          \ have to beat for disk IO to stop being the main bottleneck.  I don't know\
          \ off hand if that 44.7 GB/s is per slot or the bus total, but assuming\
          \ this can be set up so that streaming the new data into ram doesn't saturate\
          \ the bus, a double buffering scheme for BLOOM blocks could then hit 7 seconds\
          \ per token.  This is hand waving a lot obviously, and I don't know what\
          \ kind of CPU would be needed.\n\nWould RAID0ing 6 NVMes be able to hit\
          \ the same bandwidth as the ram?"
        updatedAt: '2022-12-16T02:03:42.294Z'
      numEdits: 1
      reactions: []
    id: 639bd01534967bcf455ff9c4
    type: comment
  author: aeva
  content: "@sblaszak even if the fastest available NVMe is about 7500 MB/s right\
    \ now, you raise an interesting point about PCIe bandwidth and the availability\
    \ of motherboards supporting multiple NVMe slots. \n\nAccording to wikipedia,\
    \ PCIe 5's bandwidth is 63\_GB/s (and the later revisions have very promising\
    \ gains there too).  I haven't thought this through, but if it's possible to meaningfully\
    \ parallelize the IO across multiple high end NVMes, then maybe this could be\
    \ made to work with a much lower amount of high end ram.  From my notes from yesterday,\
    \ 44.7 GB/s is probably the best possible ram bandwidth for normal hardware right\
    \ now, which I think is what we'd have to beat for disk IO to stop being the main\
    \ bottleneck.  I don't know off hand if that 44.7 GB/s is per slot or the bus\
    \ total, but assuming this can be set up so that streaming the new data into ram\
    \ doesn't saturate the bus, a double buffering scheme for BLOOM blocks could then\
    \ hit 7 seconds per token.  This is hand waving a lot obviously, and I don't know\
    \ what kind of CPU would be needed.\n\nWould RAID0ing 6 NVMes be able to hit the\
    \ same bandwidth as the ram?"
  created_at: 2022-12-16 01:55:33+00:00
  edited: true
  hidden: false
  id: 639bd01534967bcf455ff9c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4101e5d46b7954050645d328f287513e.svg
      fullname: Shawn Blaszak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sblaszak
      type: user
    createdAt: '2022-12-16T02:32:41.000Z'
    data:
      edited: false
      editors:
      - sblaszak
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4101e5d46b7954050645d328f287513e.svg
          fullname: Shawn Blaszak
          isHf: false
          isPro: false
          name: sblaszak
          type: user
        html: '<p>As I mentioned in my edit, one of the first things I thought of
          after my initial post is that most of the performance gain from RAID 0 will
          be for sequential reads.  Is that a common usage case for BLOOM?  If it
          uses a lot of random reads, I would think that the expected actual bandwidth
          of such a setup would significantly drop off...</p>

          '
        raw: As I mentioned in my edit, one of the first things I thought of after
          my initial post is that most of the performance gain from RAID 0 will be
          for sequential reads.  Is that a common usage case for BLOOM?  If it uses
          a lot of random reads, I would think that the expected actual bandwidth
          of such a setup would significantly drop off...
        updatedAt: '2022-12-16T02:32:41.501Z'
      numEdits: 0
      reactions: []
    id: 639bd8c9f44c46d8af5e0dd4
    type: comment
  author: sblaszak
  content: As I mentioned in my edit, one of the first things I thought of after my
    initial post is that most of the performance gain from RAID 0 will be for sequential
    reads.  Is that a common usage case for BLOOM?  If it uses a lot of random reads,
    I would think that the expected actual bandwidth of such a setup would significantly
    drop off...
  created_at: 2022-12-16 02:32:41+00:00
  edited: false
  hidden: false
  id: 639bd8c9f44c46d8af5e0dd4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 156
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: What is the best way to run Bloom-176B locally at interactive speeds?
