!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ricardoooo
conflicting_files: null
created_at: 2023-06-19 07:04:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8d7e8406f6cc938001fbd00f4631569.svg
      fullname: RicardoLu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ricardoooo
      type: user
    createdAt: '2023-06-19T08:04:09.000Z'
    data:
      edited: true
      editors:
      - ricardoooo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.92786705493927
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8d7e8406f6cc938001fbd00f4631569.svg
          fullname: RicardoLu
          isHf: false
          isPro: false
          name: ricardoooo
          type: user
        html: '<p>I''m new to transformer, as a application developer, I''m curious
          to the inference performance in real world. I didn''t found any benchmark
          docs about LLM, but I do care about how many tokens it can produce one second
          on a specific hardware platform(such as 8 A100 40G). How to design the strategies
          for this test?<br>I noticed that there is a config parameter named max_new_tokens
          for most text models. But I think not every generation request will generate
          as many tokens as the max_new_tokens set, so should I calculate the generated
          tokens for each input by finding out the eos_token_id in the correspond
          output?</p>

          '
        raw: 'I''m new to transformer, as a application developer, I''m curious to
          the inference performance in real world. I didn''t found any benchmark docs
          about LLM, but I do care about how many tokens it can produce one second
          on a specific hardware platform(such as 8 A100 40G). How to design the strategies
          for this test?

          I noticed that there is a config parameter named max_new_tokens for most
          text models. But I think not every generation request will generate as many
          tokens as the max_new_tokens set, so should I calculate the generated tokens
          for each input by finding out the eos_token_id in the correspond output?'
        updatedAt: '2023-06-19T08:04:56.542Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vivek-acc
    id: 64900bf906bb2b4ff280aaa2
    type: comment
  author: ricardoooo
  content: 'I''m new to transformer, as a application developer, I''m curious to the
    inference performance in real world. I didn''t found any benchmark docs about
    LLM, but I do care about how many tokens it can produce one second on a specific
    hardware platform(such as 8 A100 40G). How to design the strategies for this test?

    I noticed that there is a config parameter named max_new_tokens for most text
    models. But I think not every generation request will generate as many tokens
    as the max_new_tokens set, so should I calculate the generated tokens for each
    input by finding out the eos_token_id in the correspond output?'
  created_at: 2023-06-19 07:04:09+00:00
  edited: true
  hidden: false
  id: 64900bf906bb2b4ff280aaa2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 254
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: How to design a inference performance benchmark for text model?
