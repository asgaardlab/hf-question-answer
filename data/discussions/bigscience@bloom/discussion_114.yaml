!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TornButter
conflicting_files: null
created_at: 2022-09-18 17:10:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0f901f41133295f085a227c8a262418f.svg
      fullname: Will Brooks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TornButter
      type: user
    createdAt: '2022-09-18T18:10:29.000Z'
    data:
      edited: false
      editors:
      - TornButter
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0f901f41133295f085a227c8a262418f.svg
          fullname: Will Brooks
          isHf: false
          isPro: false
          name: TornButter
          type: user
        html: '<p>The following code successfully runs on my CPU, maxing out a few
          cores while 3090''s usage remains at 0%:<br>import torch<br>from transformers
          import BloomTokenizerFast, BloomForCausalLM<br>tokenizer = BloomTokenizerFast.from_pretrained("bigscience/bloom-560m")<br>model
          = BloomForCausalLM.from_pretrained("bigscience/bloom-560m")<br>prompt =
          "Dave picked up the baseball and"<br>result_length = 100<br>inputs = tokenizer(prompt,
          return_tensors="pt")<br>raw = model.generate(inputs["input_ids"],max_length=result_length)[0]<br>print(tokenizer.decode(raw))</p>

          <p>However, I want to use my GPU. I have tried using different models like
          1b7, but the same result. When using accelerate and device_map="auto", torch_dtype="auto",
          the models run on my GPU, but when trying to decode, I get an error that
          says "RuntimeError: Expected all tensors to be on the same device, but found
          at least two devices, cuda:0 and cpu!". When I add device = torch.device("cuda:0")
          and .to(device) to the end of the model line, I get the same runtime error.
          Same with .cuda(). I made sure to configure accelerate to not use my CPU.
          What am I doing wrong?</p>

          '
        raw: "The following code successfully runs on my CPU, maxing out a few cores\
          \ while 3090's usage remains at 0%:\r\nimport torch\r\nfrom transformers\
          \ import BloomTokenizerFast, BloomForCausalLM\r\ntokenizer = BloomTokenizerFast.from_pretrained(\"\
          bigscience/bloom-560m\")\r\nmodel = BloomForCausalLM.from_pretrained(\"\
          bigscience/bloom-560m\")\r\nprompt = \"Dave picked up the baseball and\"\
          \r\nresult_length = 100\r\ninputs = tokenizer(prompt, return_tensors=\"\
          pt\")\r\nraw = model.generate(inputs[\"input_ids\"],max_length=result_length)[0]\r\
          \nprint(tokenizer.decode(raw))\r\n\r\nHowever, I want to use my GPU. I have\
          \ tried using different models like 1b7, but the same result. When using\
          \ accelerate and device_map=\"auto\", torch_dtype=\"auto\", the models run\
          \ on my GPU, but when trying to decode, I get an error that says \"RuntimeError:\
          \ Expected all tensors to be on the same device, but found at least two\
          \ devices, cuda:0 and cpu!\". When I add device = torch.device(\"cuda:0\"\
          ) and .to(device) to the end of the model line, I get the same runtime error.\
          \ Same with .cuda(). I made sure to configure accelerate to not use my CPU.\
          \ What am I doing wrong?"
        updatedAt: '2022-09-18T18:10:29.016Z'
      numEdits: 0
      reactions: []
    id: 63275f15e9c2c1287c047419
    type: comment
  author: TornButter
  content: "The following code successfully runs on my CPU, maxing out a few cores\
    \ while 3090's usage remains at 0%:\r\nimport torch\r\nfrom transformers import\
    \ BloomTokenizerFast, BloomForCausalLM\r\ntokenizer = BloomTokenizerFast.from_pretrained(\"\
    bigscience/bloom-560m\")\r\nmodel = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\"\
    )\r\nprompt = \"Dave picked up the baseball and\"\r\nresult_length = 100\r\ninputs\
    \ = tokenizer(prompt, return_tensors=\"pt\")\r\nraw = model.generate(inputs[\"\
    input_ids\"],max_length=result_length)[0]\r\nprint(tokenizer.decode(raw))\r\n\r\
    \nHowever, I want to use my GPU. I have tried using different models like 1b7,\
    \ but the same result. When using accelerate and device_map=\"auto\", torch_dtype=\"\
    auto\", the models run on my GPU, but when trying to decode, I get an error that\
    \ says \"RuntimeError: Expected all tensors to be on the same device, but found\
    \ at least two devices, cuda:0 and cpu!\". When I add device = torch.device(\"\
    cuda:0\") and .to(device) to the end of the model line, I get the same runtime\
    \ error. Same with .cuda(). I made sure to configure accelerate to not use my\
    \ CPU. What am I doing wrong?"
  created_at: 2022-09-18 17:10:29+00:00
  edited: false
  hidden: false
  id: 63275f15e9c2c1287c047419
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0f901f41133295f085a227c8a262418f.svg
      fullname: Will Brooks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TornButter
      type: user
    createdAt: '2022-09-18T18:29:57.000Z'
    data:
      edited: true
      editors:
      - TornButter
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0f901f41133295f085a227c8a262418f.svg
          fullname: Will Brooks
          isHf: false
          isPro: false
          name: TornButter
          type: user
        html: "<p>I found the solution. I added these 2 lines before raw, then made\
          \ raw run on inputs2</p>\n<p>device = torch.device(\u201Ccuda:0\u201D)<br>inputs2\
          \ = inputs.to(device)</p>\n<p>Here is the complete working code:<br>import\
          \ torch<br>from transformers import BloomTokenizerFast, BloomForCausalLM<br>tokenizer\
          \ = BloomTokenizerFast.from_pretrained(\u201Cbigscience/bloom-560m\u201D\
          )<br>model = BloomForCausalLM.from_pretrained(\u201Cbigscience/bloom-560m\u201D\
          ).cuda()<br>prompt = \u201CDave picked up the baseball and\u201D<br>result_length\
          \ = 100<br>inputs = tokenizer(prompt, return_tensors=\u201Cpt\u201D)<br>device\
          \ = torch.device(\u201Ccuda:0\u201D)<br>inputs2 = inputs.to(device)<br>raw\
          \ = model.generate(inputs2[\u201Cinput_ids\u201D],max_length=result_length)[0]<br>print(tokenizer.decode(raw))</p>\n"
        raw: "I found the solution. I added these 2 lines before raw, then made raw\
          \ run on inputs2\n\ndevice = torch.device(\u201Ccuda:0\u201D)\ninputs2 =\
          \ inputs.to(device)\n\nHere is the complete working code:\nimport torch\n\
          from transformers import BloomTokenizerFast, BloomForCausalLM\ntokenizer\
          \ = BloomTokenizerFast.from_pretrained(\u201Cbigscience/bloom-560m\u201D\
          )\nmodel = BloomForCausalLM.from_pretrained(\u201Cbigscience/bloom-560m\u201D\
          ).cuda()\nprompt = \u201CDave picked up the baseball and\u201D\nresult_length\
          \ = 100\ninputs = tokenizer(prompt, return_tensors=\u201Cpt\u201D)\ndevice\
          \ = torch.device(\u201Ccuda:0\u201D)\ninputs2 = inputs.to(device)\nraw =\
          \ model.generate(inputs2[\u201Cinput_ids\u201D],max_length=result_length)[0]\n\
          print(tokenizer.decode(raw))"
        updatedAt: '2022-09-18T18:39:43.227Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - abdullahalzubaer
        - rmyj
    id: 632763a53476801d8f261322
    type: comment
  author: TornButter
  content: "I found the solution. I added these 2 lines before raw, then made raw\
    \ run on inputs2\n\ndevice = torch.device(\u201Ccuda:0\u201D)\ninputs2 = inputs.to(device)\n\
    \nHere is the complete working code:\nimport torch\nfrom transformers import BloomTokenizerFast,\
    \ BloomForCausalLM\ntokenizer = BloomTokenizerFast.from_pretrained(\u201Cbigscience/bloom-560m\u201D\
    )\nmodel = BloomForCausalLM.from_pretrained(\u201Cbigscience/bloom-560m\u201D\
    ).cuda()\nprompt = \u201CDave picked up the baseball and\u201D\nresult_length\
    \ = 100\ninputs = tokenizer(prompt, return_tensors=\u201Cpt\u201D)\ndevice = torch.device(\u201C\
    cuda:0\u201D)\ninputs2 = inputs.to(device)\nraw = model.generate(inputs2[\u201C\
    input_ids\u201D],max_length=result_length)[0]\nprint(tokenizer.decode(raw))"
  created_at: 2022-09-18 17:29:57+00:00
  edited: true
  hidden: false
  id: 632763a53476801d8f261322
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0f901f41133295f085a227c8a262418f.svg
      fullname: Will Brooks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TornButter
      type: user
    createdAt: '2022-09-18T18:30:05.000Z'
    data:
      status: closed
    id: 632763ade9c2c1287c049901
    type: status-change
  author: TornButter
  created_at: 2022-09-18 17:30:05+00:00
  id: 632763ade9c2c1287c049901
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 114
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: BLOOM models don't run on my GPU
