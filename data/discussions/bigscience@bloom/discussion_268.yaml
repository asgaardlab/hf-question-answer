!!python/object:huggingface_hub.community.DiscussionWithDetails
author: char-1ee
conflicting_files: null
created_at: 2023-10-14 02:21:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8f9fdf0032a173b64dba25e46a76c177.svg
      fullname: Xingjian Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: char-1ee
      type: user
    createdAt: '2023-10-14T03:21:35.000Z'
    data:
      edited: false
      editors:
      - char-1ee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8607273697853088
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8f9fdf0032a173b64dba25e46a76c177.svg
          fullname: Xingjian Li
          isHf: false
          isPro: false
          name: char-1ee
          type: user
        html: '<p>Hi, I noticed that there already exists bloom-int8 and bloom-fp16
          models. Anyone know where can find bloom-int4 model, or how can I quantize
          a 4bit model locally?</p>

          '
        raw: Hi, I noticed that there already exists bloom-int8 and bloom-fp16 models.
          Anyone know where can find bloom-int4 model, or how can I quantize a 4bit
          model locally?
        updatedAt: '2023-10-14T03:21:35.766Z'
      numEdits: 0
      reactions: []
    id: 652a093f60e7067305827818
    type: comment
  author: char-1ee
  content: Hi, I noticed that there already exists bloom-int8 and bloom-fp16 models.
    Anyone know where can find bloom-int4 model, or how can I quantize a 4bit model
    locally?
  created_at: 2023-10-14 02:21:35+00:00
  edited: false
  hidden: false
  id: 652a093f60e7067305827818
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-16T18:57:06.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: tr
        probability: 0.10496710985898972
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'cc @ybelkada '
        updatedAt: '2023-10-16T18:57:06.968Z'
      numEdits: 0
      reactions: []
    id: 652d8782322375297010ba91
    type: comment
  author: lysandre
  content: 'cc @ybelkada '
  created_at: 2023-10-16 17:57:06+00:00
  edited: false
  hidden: false
  id: 652d8782322375297010ba91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-16T19:34:32.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7997086048126221
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;char-1ee&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/char-1ee\"\
          >@<span class=\"underline\">char-1ee</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>If you have enough CPU RAM to load the entire BLOOM model, you\
          \ can easily quantize it on-the-fly in 4bit using bitsandbytes and the latest\
          \ transformers package.</p>\n<pre><code class=\"language-bash\">pip install\
          \ -U bitsandbytes transformers\n</code></pre>\n<p>Simply pass <code>load_in_4bit=True</code>\
          \ when calling <code>from_pretrained</code> and that should do the trick\
          \ to quantize the model in 4bit precision. </p>\n<p>Let me know how that\
          \ goes for you!</p>\n"
        raw: "Hi @char-1ee \n\nIf you have enough CPU RAM to load the entire BLOOM\
          \ model, you can easily quantize it on-the-fly in 4bit using bitsandbytes\
          \ and the latest transformers package.\n\n```bash\npip install -U bitsandbytes\
          \ transformers\n```\n\nSimply pass `load_in_4bit=True` when calling `from_pretrained`\
          \ and that should do the trick to quantize the model in 4bit precision.\
          \ \n\nLet me know how that goes for you!"
        updatedAt: '2023-10-16T19:34:32.322Z'
      numEdits: 0
      reactions: []
    id: 652d9048e647b0ee0aee0933
    type: comment
  author: ybelkada
  content: "Hi @char-1ee \n\nIf you have enough CPU RAM to load the entire BLOOM model,\
    \ you can easily quantize it on-the-fly in 4bit using bitsandbytes and the latest\
    \ transformers package.\n\n```bash\npip install -U bitsandbytes transformers\n\
    ```\n\nSimply pass `load_in_4bit=True` when calling `from_pretrained` and that\
    \ should do the trick to quantize the model in 4bit precision. \n\nLet me know\
    \ how that goes for you!"
  created_at: 2023-10-16 18:34:32+00:00
  edited: false
  hidden: false
  id: 652d9048e647b0ee0aee0933
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 268
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: How to quantize bloom with 4-bit
