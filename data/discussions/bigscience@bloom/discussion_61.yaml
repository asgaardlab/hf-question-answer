!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TonoTheHero
conflicting_files: null
created_at: 2022-07-23 05:37:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d2ce882ed4b566ca4068221c04cf7b1.svg
      fullname: Toni Korkiakoski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TonoTheHero
      type: user
    createdAt: '2022-07-23T06:37:12.000Z'
    data:
      edited: false
      editors:
      - TonoTheHero
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d2ce882ed4b566ca4068221c04cf7b1.svg
          fullname: Toni Korkiakoski
          isHf: false
          isPro: false
          name: TonoTheHero
          type: user
        html: '<p>Hi! I''m curious as to why this isn''t possible!<br>Just keep the
          stuff on an SSD and split the work 400/8 on a consumer GPU?</p>

          '
        raw: "Hi! I'm curious as to why this isn't possible!\r\nJust keep the stuff\
          \ on an SSD and split the work 400/8 on a consumer GPU?\r\n\r\n\r\n"
        updatedAt: '2022-07-23T06:37:12.721Z'
      numEdits: 0
      reactions: []
    id: 62db971878a44abeb6e694a0
    type: comment
  author: TonoTheHero
  content: "Hi! I'm curious as to why this isn't possible!\r\nJust keep the stuff\
    \ on an SSD and split the work 400/8 on a consumer GPU?\r\n\r\n\r\n"
  created_at: 2022-07-23 05:37:12+00:00
  edited: false
  hidden: false
  id: 62db971878a44abeb6e694a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
      fullname: Ian Beaver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IanBeaver
      type: user
    createdAt: '2022-07-26T16:38:23.000Z'
    data:
      edited: false
      editors:
      - IanBeaver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
          fullname: Ian Beaver
          isHf: false
          isPro: false
          name: IanBeaver
          type: user
        html: '<p>If you are not concerned about inference times you don''t even need
          GPUs, it runs fine on CPUs given you have enough system RAM.  Much discussion
          has been had on performance on different hardware configurations.  See:</p>

          <p><a href="https://huggingface.co/bigscience/bloom/discussions/45">https://huggingface.co/bigscience/bloom/discussions/45</a><br><a
          href="https://huggingface.co/bigscience/bloom/discussions/59">https://huggingface.co/bigscience/bloom/discussions/59</a><br><a
          href="https://huggingface.co/bigscience/bloom/discussions/58">https://huggingface.co/bigscience/bloom/discussions/58</a></p>

          '
        raw: 'If you are not concerned about inference times you don''t even need
          GPUs, it runs fine on CPUs given you have enough system RAM.  Much discussion
          has been had on performance on different hardware configurations.  See:


          https://huggingface.co/bigscience/bloom/discussions/45

          https://huggingface.co/bigscience/bloom/discussions/59

          https://huggingface.co/bigscience/bloom/discussions/58'
        updatedAt: '2022-07-26T16:38:23.969Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - TonoTheHero
        - muhtasham
        - lvwerra
    id: 62e0187f27c229627107e9da
    type: comment
  author: IanBeaver
  content: 'If you are not concerned about inference times you don''t even need GPUs,
    it runs fine on CPUs given you have enough system RAM.  Much discussion has been
    had on performance on different hardware configurations.  See:


    https://huggingface.co/bigscience/bloom/discussions/45

    https://huggingface.co/bigscience/bloom/discussions/59

    https://huggingface.co/bigscience/bloom/discussions/58'
  created_at: 2022-07-26 15:38:23+00:00
  edited: false
  hidden: false
  id: 62e0187f27c229627107e9da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2022-08-09T17:20:04.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: "<p>In case it helps, I wrote a blog post that shows how to run BLOOM\
          \ (the largest 176B version) on a desktop computer, even if you don\u2019\
          t have a GPU. In my computer (i5 11gen, 16GB RAM, 1TB SSD Samsung 980 pro),\
          \ the generation takes 3 minutes per token using only the CPU, which is\
          \ a little slow but manageable. See the blog post link below.</p>\n<p><a\
          \ rel=\"nofollow\" href=\"https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32\"\
          >https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32</a></p>\n"
        raw: "In case it helps, I wrote a blog post that shows how to run BLOOM (the\
          \ largest 176B version) on a desktop computer, even if you don\u2019t have\
          \ a GPU. In my computer (i5 11gen, 16GB RAM, 1TB SSD Samsung 980 pro), the\
          \ generation takes 3 minutes per token using only the CPU, which is a little\
          \ slow but manageable. See the blog post link below.\n\nhttps://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32"
        updatedAt: '2022-08-09T17:20:04.576Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F917"
        users:
        - ahnafsamin
        - muhtasham
        - victor
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - julien-c
        - muhtasham
    id: 62f29744b0d0488c7fa48425
    type: comment
  author: arteagac
  content: "In case it helps, I wrote a blog post that shows how to run BLOOM (the\
    \ largest 176B version) on a desktop computer, even if you don\u2019t have a GPU.\
    \ In my computer (i5 11gen, 16GB RAM, 1TB SSD Samsung 980 pro), the generation\
    \ takes 3 minutes per token using only the CPU, which is a little slow but manageable.\
    \ See the blog post link below.\n\nhttps://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32"
  created_at: 2022-08-09 16:20:04+00:00
  edited: false
  hidden: false
  id: 62f29744b0d0488c7fa48425
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2022-08-10T07:39:07.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p>really cool article <span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/arteagac\"\
          >@<span class=\"underline\">arteagac</span></a></span>\n\n\t</span></span>\
          \ thanks for sharing! &lt;3</p>\n"
        raw: really cool article @arteagac thanks for sharing! <3
        updatedAt: '2022-08-10T07:39:07.817Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - muhtasham
        - arteagac
    id: 62f3609b4ab27d7c436a43c0
    type: comment
  author: julien-c
  content: really cool article @arteagac thanks for sharing! <3
  created_at: 2022-08-10 06:39:07+00:00
  edited: false
  hidden: false
  id: 62f3609b4ab27d7c436a43c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656797308493-62c0b5203e7b8a5067d2c6d4.jpeg?w=200&h=200&f=face
      fullname: Ahnaf Mozib Samin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ahnafsamin
      type: user
    createdAt: '2022-08-13T19:42:52.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656797308493-62c0b5203e7b8a5067d2c6d4.jpeg?w=200&h=200&f=face
          fullname: Ahnaf Mozib Samin
          isHf: false
          isPro: false
          name: ahnafsamin
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-08-13T20:00:43.493Z'
      numEdits: 0
      reactions: []
    id: 62f7febc04de855c35e2c4b2
    type: comment
  author: ahnafsamin
  content: This comment has been hidden
  created_at: 2022-08-13 18:42:52+00:00
  edited: true
  hidden: true
  id: 62f7febc04de855c35e2c4b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e30c430989ccd1602a74889ffcdd03d8.svg
      fullname: daniel jose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cdani
      type: user
    createdAt: '2022-08-15T21:53:21.000Z'
    data:
      edited: false
      editors:
      - cdani
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e30c430989ccd1602a74889ffcdd03d8.svg
          fullname: daniel jose
          isHf: false
          isPro: false
          name: cdani
          type: user
        html: "<blockquote>\n<p>In case it helps, I wrote a blog post that shows how\
          \ to run BLOOM (the largest 176B version) on a desktop computer, even if\
          \ you don\u2019t have a GPU. In my computer (i5 11gen, 16GB RAM, 1TB SSD\
          \ Samsung 980 pro), the generation takes 3 minutes per token using only\
          \ the CPU, which is a little slow but manageable. See the blog post link\
          \ below.</p>\n<p><a rel=\"nofollow\" href=\"https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32\"\
          >https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32</a></p>\n\
          </blockquote>\n<p>Hello!<br>I follow the guide but the tokenizer.json downloaded\
          \ file is invalid, in fact not even a json file. I found it somewhere in\
          \ internet but now when I do<br>final_lnorm.load_state_dict(get_state_dict(shard_num=72,\
          \ prefix=\"ln_f.\"))<br>it says<br>File \"/home/usuari/anaconda3/lib/python3.9/site-packages/torch/serialization.py\"\
          , line 920, in _legacy_load<br>    magic_number = pickle_module.load(f,\
          \ **pickle_load_args)<br>_pickle.UnpicklingError: invalid load key, 'v'.</p>\n\
          <p>Do you know how to solve it?<br>Thanks!</p>\n"
        raw: "> In case it helps, I wrote a blog post that shows how to run BLOOM\
          \ (the largest 176B version) on a desktop computer, even if you don\u2019\
          t have a GPU. In my computer (i5 11gen, 16GB RAM, 1TB SSD Samsung 980 pro),\
          \ the generation takes 3 minutes per token using only the CPU, which is\
          \ a little slow but manageable. See the blog post link below.\n> \n> https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32\n\
          \nHello!\nI follow the guide but the tokenizer.json downloaded file is invalid,\
          \ in fact not even a json file. I found it somewhere in internet but now\
          \ when I do\nfinal_lnorm.load_state_dict(get_state_dict(shard_num=72, prefix=\"\
          ln_f.\"))\nit says\nFile \"/home/usuari/anaconda3/lib/python3.9/site-packages/torch/serialization.py\"\
          , line 920, in _legacy_load\n    magic_number = pickle_module.load(f, **pickle_load_args)\n\
          _pickle.UnpicklingError: invalid load key, 'v'.\n\nDo you know how to solve\
          \ it?\nThanks!"
        updatedAt: '2022-08-15T21:53:21.014Z'
      numEdits: 0
      reactions: []
    id: 62fac051363251ee40a6b05a
    type: comment
  author: cdani
  content: "> In case it helps, I wrote a blog post that shows how to run BLOOM (the\
    \ largest 176B version) on a desktop computer, even if you don\u2019t have a GPU.\
    \ In my computer (i5 11gen, 16GB RAM, 1TB SSD Samsung 980 pro), the generation\
    \ takes 3 minutes per token using only the CPU, which is a little slow but manageable.\
    \ See the blog post link below.\n> \n> https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32\n\
    \nHello!\nI follow the guide but the tokenizer.json downloaded file is invalid,\
    \ in fact not even a json file. I found it somewhere in internet but now when\
    \ I do\nfinal_lnorm.load_state_dict(get_state_dict(shard_num=72, prefix=\"ln_f.\"\
    ))\nit says\nFile \"/home/usuari/anaconda3/lib/python3.9/site-packages/torch/serialization.py\"\
    , line 920, in _legacy_load\n    magic_number = pickle_module.load(f, **pickle_load_args)\n\
    _pickle.UnpicklingError: invalid load key, 'v'.\n\nDo you know how to solve it?\n\
    Thanks!"
  created_at: 2022-08-15 20:53:21+00:00
  edited: false
  hidden: false
  id: 62fac051363251ee40a6b05a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2022-08-17T15:30:07.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;cdani&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cdani\">@<span class=\"\
          underline\">cdani</span></a></span>\n\n\t</span></span> , I suspect the\
          \ files were not  properly downloaded and some files might be pointers instead\
          \ of the actual files. The easiest way to fix this is to download the entire\
          \ repo from scratch using git lfs as follows:</p>\n<pre><code>git lfs install\n\
          git clone https://huggingface.co/bigscience/bloom\n</code></pre>\n<p>This\
          \ will download the entire repo (including some repo history). When the\
          \ download is complete, make sure the size of the files matches the size\
          \ in the web repository.</p>\n"
        raw: 'Hi @cdani , I suspect the files were not  properly downloaded and some
          files might be pointers instead of the actual files. The easiest way to
          fix this is to download the entire repo from scratch using git lfs as follows:


          ```

          git lfs install

          git clone https://huggingface.co/bigscience/bloom

          ```

          This will download the entire repo (including some repo history). When the
          download is complete, make sure the size of the files matches the size in
          the web repository.'
        updatedAt: '2022-08-17T15:30:07.916Z'
      numEdits: 0
      reactions: []
    id: 62fd097f65ba08da9cd2c71a
    type: comment
  author: arteagac
  content: 'Hi @cdani , I suspect the files were not  properly downloaded and some
    files might be pointers instead of the actual files. The easiest way to fix this
    is to download the entire repo from scratch using git lfs as follows:


    ```

    git lfs install

    git clone https://huggingface.co/bigscience/bloom

    ```

    This will download the entire repo (including some repo history). When the download
    is complete, make sure the size of the files matches the size in the web repository.'
  created_at: 2022-08-17 14:30:07+00:00
  edited: false
  hidden: false
  id: 62fd097f65ba08da9cd2c71a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-15T00:13:42.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Closing due to lack of activity. Feel free to re-open if you feel
          that the discussion is not finished yet.</p>

          '
        raw: Closing due to lack of activity. Feel free to re-open if you feel that
          the discussion is not finished yet.
        updatedAt: '2022-11-15T00:13:42.255Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6372d9b6112fb535baf190ba
    id: 6372d9b6112fb535baf190b9
    type: comment
  author: TimeRobber
  content: Closing due to lack of activity. Feel free to re-open if you feel that
    the discussion is not finished yet.
  created_at: 2022-11-15 00:13:42+00:00
  edited: false
  hidden: false
  id: 6372d9b6112fb535baf190b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-15T00:13:42.000Z'
    data:
      status: closed
    id: 6372d9b6112fb535baf190ba
    type: status-change
  author: TimeRobber
  created_at: 2022-11-15 00:13:42+00:00
  id: 6372d9b6112fb535baf190ba
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 61
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: Why can't the model be run (really slowly) on consumer hardware?
