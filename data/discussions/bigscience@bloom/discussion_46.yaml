!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NXBY
conflicting_files: null
created_at: 2022-07-15 15:41:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
      fullname: Boyu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NXBY
      type: user
    createdAt: '2022-07-15T16:41:34.000Z'
    data:
      edited: false
      editors:
      - NXBY
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
          fullname: Boyu
          isHf: false
          isPro: false
          name: NXBY
          type: user
        html: '<p>Hi everyone, is it possible to fine-tune the BLOOM model? If yes,
          how to do that? Thanks!</p>

          '
        raw: Hi everyone, is it possible to fine-tune the BLOOM model? If yes, how
          to do that? Thanks!
        updatedAt: '2022-07-15T16:41:34.458Z'
      numEdits: 0
      reactions:
      - count: 23
        reaction: "\U0001F44D"
        users:
        - xiyanghu
        - Riona
        - Mandorlo
        - ZZZhang
        - rufimelo
        - jilijeanlouis
        - cyberandy
        - Oasis-X
        - izilotti
        - Long415
        - initmethod
        - jamesoneill12
        - raihan2345
        - sank1238879
        - napulen
        - thangnguyen1991
        - johnAtTrigram
        - shoaibahmed
        - shashanku
        - saibo
        - christangttt
        - khiemdaica2002
        - opelumen
      - count: 2
        reaction: "\U0001F91D"
        users:
        - AnaRhisT
        - Oasis-X
      - count: 1
        reaction: "\U0001F917"
        users:
        - HuaJuan
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - youssefasrar2966
    id: 62d198be1e36881a57f30bef
    type: comment
  author: NXBY
  content: Hi everyone, is it possible to fine-tune the BLOOM model? If yes, how to
    do that? Thanks!
  created_at: 2022-07-15 15:41:34+00:00
  edited: false
  hidden: false
  id: 62d198be1e36881a57f30bef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed1821b234b28ba3876932ec3402b12e.svg
      fullname: Unkown Boy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AnaRhisT
      type: user
    createdAt: '2022-07-18T08:08:33.000Z'
    data:
      edited: false
      editors:
      - AnaRhisT
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed1821b234b28ba3876932ec3402b12e.svg
          fullname: Unkown Boy
          isHf: false
          isPro: false
          name: AnaRhisT
          type: user
        html: '<p>Hey all,<br>I''m also interesting in understanding how can I fine-tune
          this model to do a specific generation task after giving it many prompts.</p>

          '
        raw: 'Hey all,

          I''m also interesting in understanding how can I fine-tune this model to
          do a specific generation task after giving it many prompts.'
        updatedAt: '2022-07-18T08:08:33.674Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F91D"
        users:
        - NXBY
        - bergr7f
        - Oasis-X
        - StarWorkXc
    id: 62d515010157a52e22bc79a7
    type: comment
  author: AnaRhisT
  content: 'Hey all,

    I''m also interesting in understanding how can I fine-tune this model to do a
    specific generation task after giving it many prompts.'
  created_at: 2022-07-18 07:08:33+00:00
  edited: false
  hidden: false
  id: 62d515010157a52e22bc79a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-07-18T08:13:09.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Hi everyone,<br>If you have enough compute you could fine tune BLOOM
          on any downstream task but you would need enough GPU RAM to store the model
          + gradient (optimizer state) which is quite costly. A common tasks people
          fine-tune auto regressive models is Question Answering. I would say if you
          are interested of doing that you can first try it on one of the BLOOM-small
          models (ideally 1b3 since it is the one of a small fully trained model)<br>Another
          option could be to do "prompt-tuning": <a rel="nofollow" href="https://youtu.be/8HwHGGb1zpQ?t=2455">https://youtu.be/8HwHGGb1zpQ?t=2455</a>
          it could be interesting to apply this method on BLOOM as it wont require
          to store the optimizer state of the whole model</p>

          '
        raw: "Hi everyone, \nIf you have enough compute you could fine tune BLOOM\
          \ on any downstream task but you would need enough GPU RAM to store the\
          \ model + gradient (optimizer state) which is quite costly. A common tasks\
          \ people fine-tune auto regressive models is Question Answering. I would\
          \ say if you are interested of doing that you can first try it on one of\
          \ the BLOOM-small models (ideally 1b3 since it is the one of a small fully\
          \ trained model)\nAnother option could be to do \"prompt-tuning\": https://youtu.be/8HwHGGb1zpQ?t=2455\
          \ it could be interesting to apply this method on BLOOM as it wont require\
          \ to store the optimizer state of the whole model"
        updatedAt: '2022-07-18T08:13:38.591Z'
      numEdits: 1
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - NXBY
        - julien-c
        - wasit
        - izilotti
      - count: 2
        reaction: "\U0001F91D"
        users:
        - AnaRhisT
        - Sanchita
      - count: 1
        reaction: "\U0001F92F"
        users:
        - duongkstn
    id: 62d516156426d36ad939586b
    type: comment
  author: ybelkada
  content: "Hi everyone, \nIf you have enough compute you could fine tune BLOOM on\
    \ any downstream task but you would need enough GPU RAM to store the model + gradient\
    \ (optimizer state) which is quite costly. A common tasks people fine-tune auto\
    \ regressive models is Question Answering. I would say if you are interested of\
    \ doing that you can first try it on one of the BLOOM-small models (ideally 1b3\
    \ since it is the one of a small fully trained model)\nAnother option could be\
    \ to do \"prompt-tuning\": https://youtu.be/8HwHGGb1zpQ?t=2455 it could be interesting\
    \ to apply this method on BLOOM as it wont require to store the optimizer state\
    \ of the whole model"
  created_at: 2022-07-18 07:13:09+00:00
  edited: true
  hidden: false
  id: 62d516156426d36ad939586b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed1821b234b28ba3876932ec3402b12e.svg
      fullname: Unkown Boy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AnaRhisT
      type: user
    createdAt: '2022-07-18T08:24:06.000Z'
    data:
      edited: false
      editors:
      - AnaRhisT
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed1821b234b28ba3876932ec3402b12e.svg
          fullname: Unkown Boy
          isHf: false
          isPro: false
          name: AnaRhisT
          type: user
        html: '<blockquote>

          <p>Hi everyone,<br>If you have enough compute you could fine tune BLOOM
          on any downstream task but you would need enough GPU RAM to store the model
          + gradient (optimizer state) which is quite costly. A common tasks people
          fine-tune auto regressive models is Question Answering. I would say if you
          are interested of doing that you can first try it on one of the BLOOM-small
          models (ideally 1b3 since it is the one of a small fully trained model)<br>Another
          option could be to do "prompt-tuning": <a rel="nofollow" href="https://youtu.be/8HwHGGb1zpQ?t=2455">https://youtu.be/8HwHGGb1zpQ?t=2455</a>
          it could be interesting to apply this method on BLOOM as it wont require
          to store the optimizer state of the whole model</p>

          </blockquote>

          <p>Thanks for your reply,<br>I''m not able to find any code on how it should
          be done.<br>Can you please refer me to the code that can do the fine-tuning?</p>

          '
        raw: "> Hi everyone, \n> If you have enough compute you could fine tune BLOOM\
          \ on any downstream task but you would need enough GPU RAM to store the\
          \ model + gradient (optimizer state) which is quite costly. A common tasks\
          \ people fine-tune auto regressive models is Question Answering. I would\
          \ say if you are interested of doing that you can first try it on one of\
          \ the BLOOM-small models (ideally 1b3 since it is the one of a small fully\
          \ trained model)\n> Another option could be to do \"prompt-tuning\": https://youtu.be/8HwHGGb1zpQ?t=2455\
          \ it could be interesting to apply this method on BLOOM as it wont require\
          \ to store the optimizer state of the whole model\n\nThanks for your reply,\n\
          I'm not able to find any code on how it should be done.\nCan you please\
          \ refer me to the code that can do the fine-tuning?"
        updatedAt: '2022-07-18T08:24:06.244Z'
      numEdits: 0
      reactions:
      - count: 8
        reaction: "\U0001F44D"
        users:
        - NXBY
        - mrm8488
        - heyaa
        - Long415
        - initmethod
        - thangnguyen1991
        - johnAtTrigram
        - opelumen
    id: 62d518a6af7e703f014409fb
    type: comment
  author: AnaRhisT
  content: "> Hi everyone, \n> If you have enough compute you could fine tune BLOOM\
    \ on any downstream task but you would need enough GPU RAM to store the model\
    \ + gradient (optimizer state) which is quite costly. A common tasks people fine-tune\
    \ auto regressive models is Question Answering. I would say if you are interested\
    \ of doing that you can first try it on one of the BLOOM-small models (ideally\
    \ 1b3 since it is the one of a small fully trained model)\n> Another option could\
    \ be to do \"prompt-tuning\": https://youtu.be/8HwHGGb1zpQ?t=2455 it could be\
    \ interesting to apply this method on BLOOM as it wont require to store the optimizer\
    \ state of the whole model\n\nThanks for your reply,\nI'm not able to find any\
    \ code on how it should be done.\nCan you please refer me to the code that can\
    \ do the fine-tuning?"
  created_at: 2022-07-18 07:24:06+00:00
  edited: false
  hidden: false
  id: 62d518a6af7e703f014409fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
      fullname: Boyu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NXBY
      type: user
    createdAt: '2022-07-18T08:33:41.000Z'
    data:
      edited: true
      editors:
      - NXBY
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
          fullname: Boyu
          isHf: false
          isPro: false
          name: NXBY
          type: user
        html: '<blockquote>

          <p>Hi everyone,<br>If you have enough compute you could fine tune BLOOM
          on any downstream task but you would need enough GPU RAM to store the model
          + gradient (optimizer state) which is quite costly. A common tasks people
          fine-tune auto regressive models is Question Answering. I would say if you
          are interested of doing that you can first try it on one of the BLOOM-small
          models (ideally 1b3 since it is the one of a small fully trained model)<br>Another
          option could be to do "prompt-tuning": <a rel="nofollow" href="https://youtu.be/8HwHGGb1zpQ?t=2455">https://youtu.be/8HwHGGb1zpQ?t=2455</a>
          it could be interesting to apply this method on BLOOM as it wont require
          to store the optimizer state of the whole model</p>

          </blockquote>

          <p>Hi ybelkada, thank you for your reply. And I just watched the video you
          shared with us, thanks for the video!</p>

          <p>I have the same question as AnaRhisT: is there any code we can use to
          do model tuning?<br>And is there any code to do prompt tuning as mentioned
          in the video?</p>

          <p>Thank you very much!</p>

          '
        raw: "> Hi everyone, \n> If you have enough compute you could fine tune BLOOM\
          \ on any downstream task but you would need enough GPU RAM to store the\
          \ model + gradient (optimizer state) which is quite costly. A common tasks\
          \ people fine-tune auto regressive models is Question Answering. I would\
          \ say if you are interested of doing that you can first try it on one of\
          \ the BLOOM-small models (ideally 1b3 since it is the one of a small fully\
          \ trained model)\n> Another option could be to do \"prompt-tuning\": https://youtu.be/8HwHGGb1zpQ?t=2455\
          \ it could be interesting to apply this method on BLOOM as it wont require\
          \ to store the optimizer state of the whole model\n\nHi ybelkada, thank\
          \ you for your reply. And I just watched the video you shared with us, thanks\
          \ for the video!\n\nI have the same question as AnaRhisT: is there any code\
          \ we can use to do model tuning?\nAnd is there any code to do prompt tuning\
          \ as mentioned in the video?\n\nThank you very much!"
        updatedAt: '2022-07-18T09:08:54.489Z'
      numEdits: 1
      reactions: []
    id: 62d51ae56426d36ad9398a77
    type: comment
  author: NXBY
  content: "> Hi everyone, \n> If you have enough compute you could fine tune BLOOM\
    \ on any downstream task but you would need enough GPU RAM to store the model\
    \ + gradient (optimizer state) which is quite costly. A common tasks people fine-tune\
    \ auto regressive models is Question Answering. I would say if you are interested\
    \ of doing that you can first try it on one of the BLOOM-small models (ideally\
    \ 1b3 since it is the one of a small fully trained model)\n> Another option could\
    \ be to do \"prompt-tuning\": https://youtu.be/8HwHGGb1zpQ?t=2455 it could be\
    \ interesting to apply this method on BLOOM as it wont require to store the optimizer\
    \ state of the whole model\n\nHi ybelkada, thank you for your reply. And I just\
    \ watched the video you shared with us, thanks for the video!\n\nI have the same\
    \ question as AnaRhisT: is there any code we can use to do model tuning?\nAnd\
    \ is there any code to do prompt tuning as mentioned in the video?\n\nThank you\
    \ very much!"
  created_at: 2022-07-18 07:33:41+00:00
  edited: true
  hidden: false
  id: 62d51ae56426d36ad9398a77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
      fullname: Stella Biderman
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stellaathena
      type: user
    createdAt: '2022-07-18T16:41:23.000Z'
    data:
      edited: false
      editors:
      - stellaathena
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
          fullname: Stella Biderman
          isHf: false
          isPro: false
          name: stellaathena
          type: user
        html: '<p>It is highly recommended that you use the same codebase that was
          originally used to train the model rather than the huggingface port. You
          can find that codebase <a rel="nofollow" href="https://github.com/bigscience-workshop/Megatron-DeepSpeed">here</a></p>

          '
        raw: It is highly recommended that you use the same codebase that was originally
          used to train the model rather than the huggingface port. You can find that
          codebase [here](https://github.com/bigscience-workshop/Megatron-DeepSpeed)
        updatedAt: '2022-07-18T16:41:23.985Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - NXBY
        - mrm8488
        - christangttt
    id: 62d58d33cce0d98b7004eb02
    type: comment
  author: stellaathena
  content: It is highly recommended that you use the same codebase that was originally
    used to train the model rather than the huggingface port. You can find that codebase
    [here](https://github.com/bigscience-workshop/Megatron-DeepSpeed)
  created_at: 2022-07-18 15:41:23+00:00
  edited: false
  hidden: false
  id: 62d58d33cce0d98b7004eb02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
      fullname: Boyu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NXBY
      type: user
    createdAt: '2022-07-18T18:30:35.000Z'
    data:
      edited: false
      editors:
      - NXBY
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
          fullname: Boyu
          isHf: false
          isPro: false
          name: NXBY
          type: user
        html: '<blockquote>

          <p>It is highly recommended that you use the same codebase that was originally
          used to train the model rather than the huggingface port. You can find that
          codebase <a rel="nofollow" href="https://github.com/bigscience-workshop/Megatron-DeepSpeed">here</a></p>

          </blockquote>

          <p>Hi stellaathena, thank you for your message!<br>Indeed I should try the
          codebase that has been used originally!<br>I''m quite new to this field,
          in the link you shared I only see the code to pretrain BERT, GPT and T5,
          but no BLOOM.<br>Should I re-use the GPT code? (I saw that they are similar)
          And may I download the BLOOM model from Hugging Face, or I''d better download
          the model from somewhere else?<br>Do you have any code using the original
          codebase to fine-tune BLOOM?</p>

          <p>Thank you!</p>

          '
        raw: "> It is highly recommended that you use the same codebase that was originally\
          \ used to train the model rather than the huggingface port. You can find\
          \ that codebase [here](https://github.com/bigscience-workshop/Megatron-DeepSpeed)\n\
          \nHi stellaathena, thank you for your message! \nIndeed I should try the\
          \ codebase that has been used originally!\nI'm quite new to this field,\
          \ in the link you shared I only see the code to pretrain BERT, GPT and T5,\
          \ but no BLOOM. \nShould I re-use the GPT code? (I saw that they are similar)\
          \ And may I download the BLOOM model from Hugging Face, or I'd better download\
          \ the model from somewhere else?\nDo you have any code using the original\
          \ codebase to fine-tune BLOOM?\n\nThank you!"
        updatedAt: '2022-07-18T18:30:35.824Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AnaRhisT
    id: 62d5a6cb537b00e15b61c6b4
    type: comment
  author: NXBY
  content: "> It is highly recommended that you use the same codebase that was originally\
    \ used to train the model rather than the huggingface port. You can find that\
    \ codebase [here](https://github.com/bigscience-workshop/Megatron-DeepSpeed)\n\
    \nHi stellaathena, thank you for your message! \nIndeed I should try the codebase\
    \ that has been used originally!\nI'm quite new to this field, in the link you\
    \ shared I only see the code to pretrain BERT, GPT and T5, but no BLOOM. \nShould\
    \ I re-use the GPT code? (I saw that they are similar) And may I download the\
    \ BLOOM model from Hugging Face, or I'd better download the model from somewhere\
    \ else?\nDo you have any code using the original codebase to fine-tune BLOOM?\n\
    \nThank you!"
  created_at: 2022-07-18 17:30:35+00:00
  edited: false
  hidden: false
  id: 62d5a6cb537b00e15b61c6b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
      fullname: Stella Biderman
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stellaathena
      type: user
    createdAt: '2022-07-19T12:33:30.000Z'
    data:
      edited: false
      editors:
      - stellaathena
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg?w=200&h=200&f=face
          fullname: Stella Biderman
          isHf: false
          isPro: false
          name: stellaathena
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;NXBY&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/NXBY\">@<span class=\"\
          underline\">NXBY</span></a></span>\n\n\t</span></span> if you are that new\
          \ to this field, finetuning this model is almost certainly not what you\
          \ want to be doing. This model is expensive to finetune and to do inference\
          \ with, and requires highly specialized hardware. You should start off with\
          \ a model like GPT-2 or GPT-Neo, which is several orders of magnitude smaller\
          \ and substantially cheaper to use</p>\n"
        raw: '@NXBY if you are that new to this field, finetuning this model is almost
          certainly not what you want to be doing. This model is expensive to finetune
          and to do inference with, and requires highly specialized hardware. You
          should start off with a model like GPT-2 or GPT-Neo, which is several orders
          of magnitude smaller and substantially cheaper to use'
        updatedAt: '2022-07-19T12:33:30.950Z'
      numEdits: 0
      reactions: []
    id: 62d6a49a6279202d95f9098d
    type: comment
  author: stellaathena
  content: '@NXBY if you are that new to this field, finetuning this model is almost
    certainly not what you want to be doing. This model is expensive to finetune and
    to do inference with, and requires highly specialized hardware. You should start
    off with a model like GPT-2 or GPT-Neo, which is several orders of magnitude smaller
    and substantially cheaper to use'
  created_at: 2022-07-19 11:33:30+00:00
  edited: false
  hidden: false
  id: 62d6a49a6279202d95f9098d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
      fullname: Boyu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NXBY
      type: user
    createdAt: '2022-07-19T13:25:36.000Z'
    data:
      edited: false
      editors:
      - NXBY
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
          fullname: Boyu
          isHf: false
          isPro: false
          name: NXBY
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;NXBY&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/NXBY\"\
          >@<span class=\"underline\">NXBY</span></a></span>\n\n\t</span></span> if\
          \ you are that new to this field, finetuning this model is almost certainly\
          \ not what you want to be doing. This model is expensive to finetune and\
          \ to do inference with, and requires highly specialized hardware. You should\
          \ start off with a model like GPT-2 or GPT-Neo, which is several orders\
          \ of magnitude smaller and substantially cheaper to use</p>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;stellaathena&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/stellaathena\">@<span\
          \ class=\"underline\">stellaathena</span></a></span>\n\n\t</span></span>\
          \ Hi, thank you for your suggestion. I've tried to fine-tune GPT-3 Babbage\
          \ via the OpenAI API, and the fine-tuned model is not performing well for\
          \ my task which is kind of complicated. I'm not sure if a smaller model\
          \ can have a better performance.<br>I'd like to see if there exist any code\
          \ so that I can do fine-tuning on a smaller version of BLOOM, and then use\
          \ the same code to fine-tune a larger version of BLOOM on a server.<br>If\
          \ you could share with us any code it would be great. I'm willing to learn\
          \ the code :-)</p>\n<p>Thank you for your messages :-)</p>\n"
        raw: "> @NXBY if you are that new to this field, finetuning this model is\
          \ almost certainly not what you want to be doing. This model is expensive\
          \ to finetune and to do inference with, and requires highly specialized\
          \ hardware. You should start off with a model like GPT-2 or GPT-Neo, which\
          \ is several orders of magnitude smaller and substantially cheaper to use\n\
          \n@stellaathena Hi, thank you for your suggestion. I've tried to fine-tune\
          \ GPT-3 Babbage via the OpenAI API, and the fine-tuned model is not performing\
          \ well for my task which is kind of complicated. I'm not sure if a smaller\
          \ model can have a better performance. \nI'd like to see if there exist\
          \ any code so that I can do fine-tuning on a smaller version of BLOOM, and\
          \ then use the same code to fine-tune a larger version of BLOOM on a server.\
          \ \nIf you could share with us any code it would be great. I'm willing to\
          \ learn the code :-)\n\nThank you for your messages :-)"
        updatedAt: '2022-07-19T13:25:36.958Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - AnaRhisT
        - shobhittiwarii
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - AnaRhisT
    id: 62d6b0d0a543b9b79215559a
    type: comment
  author: NXBY
  content: "> @NXBY if you are that new to this field, finetuning this model is almost\
    \ certainly not what you want to be doing. This model is expensive to finetune\
    \ and to do inference with, and requires highly specialized hardware. You should\
    \ start off with a model like GPT-2 or GPT-Neo, which is several orders of magnitude\
    \ smaller and substantially cheaper to use\n\n@stellaathena Hi, thank you for\
    \ your suggestion. I've tried to fine-tune GPT-3 Babbage via the OpenAI API, and\
    \ the fine-tuned model is not performing well for my task which is kind of complicated.\
    \ I'm not sure if a smaller model can have a better performance. \nI'd like to\
    \ see if there exist any code so that I can do fine-tuning on a smaller version\
    \ of BLOOM, and then use the same code to fine-tune a larger version of BLOOM\
    \ on a server. \nIf you could share with us any code it would be great. I'm willing\
    \ to learn the code :-)\n\nThank you for your messages :-)"
  created_at: 2022-07-19 12:25:36+00:00
  edited: false
  hidden: false
  id: 62d6b0d0a543b9b79215559a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed1821b234b28ba3876932ec3402b12e.svg
      fullname: Unkown Boy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AnaRhisT
      type: user
    createdAt: '2022-07-19T15:12:55.000Z'
    data:
      edited: false
      editors:
      - AnaRhisT
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed1821b234b28ba3876932ec3402b12e.svg
          fullname: Unkown Boy
          isHf: false
          isPro: false
          name: AnaRhisT
          type: user
        html: '<p>Exactly what NXBY said, I''d love to see as well a code that can
          do fine-tuning on a smaller version of BLOOM. Then we can easily transfer
          it to the larger version.<br>Side note: About the hardware stuff, please
          don''t worry about it, I understand I need at least 8x A100 (80GPU versions)
          for the largest model - which isn''t a problem as well.</p>

          '
        raw: "Exactly what NXBY said, I'd love to see as well a code that can do fine-tuning\
          \ on a smaller version of BLOOM. Then we can easily transfer it to the larger\
          \ version. \nSide note: About the hardware stuff, please don't worry about\
          \ it, I understand I need at least 8x A100 (80GPU versions) for the largest\
          \ model - which isn't a problem as well."
        updatedAt: '2022-07-19T15:12:55.969Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - benmkor
        - jasoneden
        - jamesoneill12
        - shobhittiwarii
      - count: 2
        reaction: "\U0001F91D"
        users:
        - NXBY
        - jasoneden
    id: 62d6c9f72bfde6335478bff2
    type: comment
  author: AnaRhisT
  content: "Exactly what NXBY said, I'd love to see as well a code that can do fine-tuning\
    \ on a smaller version of BLOOM. Then we can easily transfer it to the larger\
    \ version. \nSide note: About the hardware stuff, please don't worry about it,\
    \ I understand I need at least 8x A100 (80GPU versions) for the largest model\
    \ - which isn't a problem as well."
  created_at: 2022-07-19 14:12:55+00:00
  edited: false
  hidden: false
  id: 62d6c9f72bfde6335478bff2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7bfddd584a307de10c064a81a5477a0.svg
      fullname: Viktoria D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FluxWave
      type: user
    createdAt: '2022-09-16T01:17:38.000Z'
    data:
      edited: false
      editors:
      - FluxWave
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7bfddd584a307de10c064a81a5477a0.svg
          fullname: Viktoria D
          isHf: false
          isPro: false
          name: FluxWave
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;AnaRhisT&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/AnaRhisT\"\
          >@<span class=\"underline\">AnaRhisT</span></a></span>\n\n\t</span></span>,\
          \ have you had any luck?</p>\n"
        raw: Hey @AnaRhisT, have you had any luck?
        updatedAt: '2022-09-16T01:17:38.078Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jasoneden
    id: 6323ceb2d444e1d9088b6fc1
    type: comment
  author: FluxWave
  content: Hey @AnaRhisT, have you had any luck?
  created_at: 2022-09-16 00:17:38+00:00
  edited: false
  hidden: false
  id: 6323ceb2d444e1d9088b6fc1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-03T17:36:45.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: '<p>Same question here - I''m happy to hack around using the GPT code,
          etc. but if someone has already solved for customizing a small BLOOM model
          on something like SQUAD or similar, it would save me (and probably a bunch
          of others) a ton of time.</p>

          '
        raw: Same question here - I'm happy to hack around using the GPT code, etc.
          but if someone has already solved for customizing a small BLOOM model on
          something like SQUAD or similar, it would save me (and probably a bunch
          of others) a ton of time.
        updatedAt: '2022-10-03T17:36:45.186Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - NXBY
    id: 633b1dad1fd49ee0b64d1c69
    type: comment
  author: jasoneden
  content: Same question here - I'm happy to hack around using the GPT code, etc.
    but if someone has already solved for customizing a small BLOOM model on something
    like SQUAD or similar, it would save me (and probably a bunch of others) a ton
    of time.
  created_at: 2022-10-03 16:36:45+00:00
  edited: false
  hidden: false
  id: 633b1dad1fd49ee0b64d1c69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-10-03T19:20:18.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Thank you for your messages! (And sorry to respond late here, just
          noticed the notifications)<br>I have found a script for Named Entity Recognition
          on <code>transformers</code> that has been used for BLOOM - <a rel="nofollow"
          href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification">https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification</a>
          (check this PR: <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/18632">https://github.com/huggingface/transformers/pull/18632</a>)<br>I
          believe we can use this codebase as a starting point, in which task do you
          plan to fine tune the model?</p>

          '
        raw: "Thank you for your messages! (And sorry to respond late here, just noticed\
          \ the notifications) \nI have found a script for Named Entity Recognition\
          \ on `transformers` that has been used for BLOOM - https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification\
          \ (check this PR: https://github.com/huggingface/transformers/pull/18632)\
          \ \nI believe we can use this codebase as a starting point, in which task\
          \ do you plan to fine tune the model?"
        updatedAt: '2022-10-03T19:20:18.582Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - NXBY
        - wcopeland
        - jamesoneill12
    id: 633b35f21fd49ee0b64e29d2
    type: comment
  author: ybelkada
  content: "Thank you for your messages! (And sorry to respond late here, just noticed\
    \ the notifications) \nI have found a script for Named Entity Recognition on `transformers`\
    \ that has been used for BLOOM - https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification\
    \ (check this PR: https://github.com/huggingface/transformers/pull/18632) \nI\
    \ believe we can use this codebase as a starting point, in which task do you plan\
    \ to fine tune the model?"
  created_at: 2022-10-03 18:20:18+00:00
  edited: false
  hidden: false
  id: 633b35f21fd49ee0b64e29d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-03T19:23:27.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: '<p>Thanks ybelkada! I think at least a few of us are looking to tune
          BLOOM for question answering.</p>

          '
        raw: Thanks ybelkada! I think at least a few of us are looking to tune BLOOM
          for question answering.
        updatedAt: '2022-10-03T19:23:27.030Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - jilijeanlouis
        - thangnguyen1991
    id: 633b36af5df91da9ceaf6916
    type: comment
  author: jasoneden
  content: Thanks ybelkada! I think at least a few of us are looking to tune BLOOM
    for question answering.
  created_at: 2022-10-03 18:23:27+00:00
  edited: false
  hidden: false
  id: 633b36af5df91da9ceaf6916
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
      fullname: Boyu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NXBY
      type: user
    createdAt: '2022-10-03T20:25:54.000Z'
    data:
      edited: false
      editors:
      - NXBY
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
          fullname: Boyu
          isHf: false
          isPro: false
          name: NXBY
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;jasoneden&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jasoneden\"\
          >@<span class=\"underline\">jasoneden</span></a></span>\n\n\t</span></span>\
          \ for your message and <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ for your reply! I'm going to dive into the code you shared with us as\
          \ well :-)</p>\n"
        raw: Thank you @jasoneden for your message and @ybelkada for your reply! I'm
          going to dive into the code you shared with us as well :-)
        updatedAt: '2022-10-03T20:25:54.632Z'
      numEdits: 0
      reactions: []
    id: 633b45522eb4ac1a0de245c7
    type: comment
  author: NXBY
  content: Thank you @jasoneden for your message and @ybelkada for your reply! I'm
    going to dive into the code you shared with us as well :-)
  created_at: 2022-10-03 19:25:54+00:00
  edited: false
  hidden: false
  id: 633b45522eb4ac1a0de245c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-10-04T15:56:34.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;jasoneden&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jasoneden\"\
          >@<span class=\"underline\">jasoneden</span></a></span>\n\n\t</span></span>\
          \ &amp; <span data-props=\"{&quot;user&quot;:&quot;NXBY&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/NXBY\">@<span class=\"\
          underline\">NXBY</span></a></span>\n\n\t</span></span> !<br>We have just\
          \ merged a PR that adds <code>BloomForQuestionAnswering</code>, here: <a\
          \ rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/pull/19310\"\
          >https://github.com/huggingface/transformers/pull/19310</a><br>Can you try\
          \ this model out with the provided trained script for question answering?\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering\"\
          >https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering</a>\
          \ / Though you need the latest version of <code>transformers</code> (by\
          \ doing for example <code>pip install git+https://github.com/huggingface/transformers.git@main</code>)\
          \ and of course let me know if you face into any issue! ;)</p>\n"
        raw: "Hi @jasoneden & @NXBY ! \nWe have just merged a PR that adds `BloomForQuestionAnswering`,\
          \ here: https://github.com/huggingface/transformers/pull/19310\nCan you\
          \ try this model out with the provided trained script for question answering?\
          \ https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering\
          \ / Though you need the latest version of `transformers` (by doing for example\
          \ `pip install git+https://github.com/huggingface/transformers.git@main`)\
          \ and of course let me know if you face into any issue! ;)"
        updatedAt: '2022-10-04T15:56:34.922Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - NXBY
        - jasoneden
        - thangnguyen1991
      - count: 2
        reaction: "\U0001F44D"
        users:
        - evegarcianz
        - shir0ha
    id: 633c57b2ccce04161f82e6c2
    type: comment
  author: ybelkada
  content: "Hi @jasoneden & @NXBY ! \nWe have just merged a PR that adds `BloomForQuestionAnswering`,\
    \ here: https://github.com/huggingface/transformers/pull/19310\nCan you try this\
    \ model out with the provided trained script for question answering? https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering\
    \ / Though you need the latest version of `transformers` (by doing for example\
    \ `pip install git+https://github.com/huggingface/transformers.git@main`) and\
    \ of course let me know if you face into any issue! ;)"
  created_at: 2022-10-04 14:56:34+00:00
  edited: false
  hidden: false
  id: 633c57b2ccce04161f82e6c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-04T16:19:09.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: '<p>Wow, that''s fantastic! Cannnot wait to give it a whirl. Thank you
          so much!</p>

          '
        raw: Wow, that's fantastic! Cannnot wait to give it a whirl. Thank you so
          much!
        updatedAt: '2022-10-04T16:19:09.352Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
    id: 633c5cfdc0fb6fd232f34b6a
    type: comment
  author: jasoneden
  content: Wow, that's fantastic! Cannnot wait to give it a whirl. Thank you so much!
  created_at: 2022-10-04 15:19:09+00:00
  edited: false
  hidden: false
  id: 633c5cfdc0fb6fd232f34b6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-05T01:18:56.000Z'
    data:
      edited: true
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ - Gave this a go just a bit ago. Installed the latest version of transformers\
          \ (testing in a colab notebook so have to do that every time anyway) and\
          \ cloned the repo, then ran the script with some configurations I had practiced\
          \ with on the T5 sequence to sequence tuner (the one listed in the link\
          \ you gave, which I had previously successfully ran...) I'm not sure if\
          \ I'm doing something wrong, or if there's something else in transformers\
          \ that needs to be updated, but I'm getting the error that says BLOOM is\
          \ not one of the models that is supported by this. Here's the code I used:</p>\n\
          <p><a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1aReC_7AQ5t1HcA_lAm2UrJknNx3O14tU#\"\
          >https://colab.research.google.com/drive/1aReC_7AQ5t1HcA_lAm2UrJknNx3O14tU#</a></p>\n\
          <p>The specific error I've seen before:<br>\"ValueError: Unrecognized configuration\
          \ class &lt;class 'transformers.models.bloom.configuration_bloom.BloomConfig'&gt;\
          \ for this kind of AutoModel: AutoModelForSeq2SeqLM.<br>Model type should\
          \ be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig,\
          \ EncoderDecoderConfig, FSMTConfig, LEDConfig, LongT5Config, M2M100Config,\
          \ MarianConfig, MBartConfig, MT5Config, MvpConfig, PegasusConfig, PegasusXConfig,\
          \ PLBartConfig, ProphetNetConfig, T5Config, XLMProphetNetConfig.\"</p>\n\
          <p>Am I using the wrong script, or do I just need to modify something to\
          \ bypass this check?</p>\n<p>(And by the way - I know the colab is underpowered\
          \ to actually do the model training. I'm just looking at this as an easy\
          \ way to POC that the procedure works, and also easy way to share the code...)</p>\n"
        raw: "Hey @ybelkada - Gave this a go just a bit ago. Installed the latest\
          \ version of transformers (testing in a colab notebook so have to do that\
          \ every time anyway) and cloned the repo, then ran the script with some\
          \ configurations I had practiced with on the T5 sequence to sequence tuner\
          \ (the one listed in the link you gave, which I had previously successfully\
          \ ran...) I'm not sure if I'm doing something wrong, or if there's something\
          \ else in transformers that needs to be updated, but I'm getting the error\
          \ that says BLOOM is not one of the models that is supported by this. Here's\
          \ the code I used:\n\nhttps://colab.research.google.com/drive/1aReC_7AQ5t1HcA_lAm2UrJknNx3O14tU#\n\
          \nThe specific error I've seen before: \n\"ValueError: Unrecognized configuration\
          \ class <class 'transformers.models.bloom.configuration_bloom.BloomConfig'>\
          \ for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should\
          \ be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig,\
          \ EncoderDecoderConfig, FSMTConfig, LEDConfig, LongT5Config, M2M100Config,\
          \ MarianConfig, MBartConfig, MT5Config, MvpConfig, PegasusConfig, PegasusXConfig,\
          \ PLBartConfig, ProphetNetConfig, T5Config, XLMProphetNetConfig.\"\n\nAm\
          \ I using the wrong script, or do I just need to modify something to bypass\
          \ this check?\n\n(And by the way - I know the colab is underpowered to actually\
          \ do the model training. I'm just looking at this as an easy way to POC\
          \ that the procedure works, and also easy way to share the code...)"
        updatedAt: '2022-10-05T01:24:57.118Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - ybelkada
        - NXBY
        - TurboPascal
    id: 633cdb805d7b7741bef573d5
    type: comment
  author: jasoneden
  content: "Hey @ybelkada - Gave this a go just a bit ago. Installed the latest version\
    \ of transformers (testing in a colab notebook so have to do that every time anyway)\
    \ and cloned the repo, then ran the script with some configurations I had practiced\
    \ with on the T5 sequence to sequence tuner (the one listed in the link you gave,\
    \ which I had previously successfully ran...) I'm not sure if I'm doing something\
    \ wrong, or if there's something else in transformers that needs to be updated,\
    \ but I'm getting the error that says BLOOM is not one of the models that is supported\
    \ by this. Here's the code I used:\n\nhttps://colab.research.google.com/drive/1aReC_7AQ5t1HcA_lAm2UrJknNx3O14tU#\n\
    \nThe specific error I've seen before: \n\"ValueError: Unrecognized configuration\
    \ class <class 'transformers.models.bloom.configuration_bloom.BloomConfig'> for\
    \ this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of\
    \ BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig,\
    \ FSMTConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig,\
    \ MT5Config, MvpConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig,\
    \ T5Config, XLMProphetNetConfig.\"\n\nAm I using the wrong script, or do I just\
    \ need to modify something to bypass this check?\n\n(And by the way - I know the\
    \ colab is underpowered to actually do the model training. I'm just looking at\
    \ this as an easy way to POC that the procedure works, and also easy way to share\
    \ the code...)"
  created_at: 2022-10-05 00:18:56+00:00
  edited: true
  hidden: false
  id: 633cdb805d7b7741bef573d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-10-05T08:49:49.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Thanks for your message !<br>In my opinion you are probably using\
          \ the wrong code, since the one you shared me uses <code>run_seq2seq_qa.py</code>\
          \ that supports Seq2Seq models, so Encoder-Decoder models such as T5. Fortunately\
          \ there is another script called <code>run_qa.py</code> that supports <code>AutoModelForQuestionAnswering</code>\
          \ models.<br>I tried your colab notebook by running the script <code>run_qa.py</code>\
          \ and seems to work fine ! Below is the command I ran:</p>\n<pre><code>!python\
          \ run_qa.py \\\n  --model_name_or_path bigscience/bloom-560m \\\n  --dataset_name\
          \ squad_v2 \\\n  --do_train \\\n  --per_device_train_batch_size 12 \\\n\
          \  --learning_rate 3e-5 \\\n  --num_train_epochs 2 \\\n  --max_seq_length\
          \ 384 \\\n  --doc_stride 128 \\\n  --output_dir /tmp/debug_seq2seq_squad/\
          \ \\\n  --eval_accumulation_steps 1 \\\n  --version_2_with_negative \\\n\
          \  --overwrite_output_dir\n</code></pre>\n<p>However, I had to slightly\
          \ change the <code>run_qa.py</code> and I had a question that I believe\
          \ you can answer. It seems that for Encoder-Decoder models and Encoder-based\
          \ models, we label the impossible answers with the index of the CLS_token.\
          \ I know for BLOOM and other auto-regressive decoder-based models this token\
          \ does not exist, so the line 404: <code>cls_index = input_ids.index(tokenizer.cls_token_id)</code>\
          \ throws an error. I had to modify it with  <code>cls_index = input_ids[-1]</code>\
          \ but I think this is wrong. We should probably pre-pend the tokenized input\
          \ with a special token, for example the <code>&lt;bos&gt;</code> token and\
          \ use this token instead as the token that we use for impossible answer.\
          \ I am not sure here so would like to hear from you, how would you approach\
          \ the problem here?</p>\n"
        raw: "Thanks for your message !\nIn my opinion you are probably using the\
          \ wrong code, since the one you shared me uses `run_seq2seq_qa.py` that\
          \ supports Seq2Seq models, so Encoder-Decoder models such as T5. Fortunately\
          \ there is another script called `run_qa.py` that supports `AutoModelForQuestionAnswering`\
          \ models.\nI tried your colab notebook by running the script `run_qa.py`\
          \ and seems to work fine ! Below is the command I ran:\n```\n!python run_qa.py\
          \ \\\n  --model_name_or_path bigscience/bloom-560m \\\n  --dataset_name\
          \ squad_v2 \\\n  --do_train \\\n  --per_device_train_batch_size 12 \\\n\
          \  --learning_rate 3e-5 \\\n  --num_train_epochs 2 \\\n  --max_seq_length\
          \ 384 \\\n  --doc_stride 128 \\\n  --output_dir /tmp/debug_seq2seq_squad/\
          \ \\\n  --eval_accumulation_steps 1 \\\n  --version_2_with_negative \\\n\
          \  --overwrite_output_dir\n```\nHowever, I had to slightly change the `run_qa.py`\
          \ and I had a question that I believe you can answer. It seems that for\
          \ Encoder-Decoder models and Encoder-based models, we label the impossible\
          \ answers with the index of the CLS_token. I know for BLOOM and other auto-regressive\
          \ decoder-based models this token does not exist, so the line 404: `cls_index\
          \ = input_ids.index(tokenizer.cls_token_id)` throws an error. I had to modify\
          \ it with  `cls_index = input_ids[-1]` but I think this is wrong. We should\
          \ probably pre-pend the tokenized input with a special token, for example\
          \ the `<bos>` token and use this token instead as the token that we use\
          \ for impossible answer. I am not sure here so would like to hear from you,\
          \ how would you approach the problem here?"
        updatedAt: '2022-10-05T08:49:49.766Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - NXBY
    id: 633d452d48ab6a0add2b61bd
    type: comment
  author: ybelkada
  content: "Thanks for your message !\nIn my opinion you are probably using the wrong\
    \ code, since the one you shared me uses `run_seq2seq_qa.py` that supports Seq2Seq\
    \ models, so Encoder-Decoder models such as T5. Fortunately there is another script\
    \ called `run_qa.py` that supports `AutoModelForQuestionAnswering` models.\nI\
    \ tried your colab notebook by running the script `run_qa.py` and seems to work\
    \ fine ! Below is the command I ran:\n```\n!python run_qa.py \\\n  --model_name_or_path\
    \ bigscience/bloom-560m \\\n  --dataset_name squad_v2 \\\n  --do_train \\\n  --per_device_train_batch_size\
    \ 12 \\\n  --learning_rate 3e-5 \\\n  --num_train_epochs 2 \\\n  --max_seq_length\
    \ 384 \\\n  --doc_stride 128 \\\n  --output_dir /tmp/debug_seq2seq_squad/ \\\n\
    \  --eval_accumulation_steps 1 \\\n  --version_2_with_negative \\\n  --overwrite_output_dir\n\
    ```\nHowever, I had to slightly change the `run_qa.py` and I had a question that\
    \ I believe you can answer. It seems that for Encoder-Decoder models and Encoder-based\
    \ models, we label the impossible answers with the index of the CLS_token. I know\
    \ for BLOOM and other auto-regressive decoder-based models this token does not\
    \ exist, so the line 404: `cls_index = input_ids.index(tokenizer.cls_token_id)`\
    \ throws an error. I had to modify it with  `cls_index = input_ids[-1]` but I\
    \ think this is wrong. We should probably pre-pend the tokenized input with a\
    \ special token, for example the `<bos>` token and use this token instead as the\
    \ token that we use for impossible answer. I am not sure here so would like to\
    \ hear from you, how would you approach the problem here?"
  created_at: 2022-10-05 07:49:49+00:00
  edited: false
  hidden: false
  id: 633d452d48ab6a0add2b61bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-05T13:55:20.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: '<p>Ah, great catch! Yeah, I suspected the seq2seq.py might be the wrong
          script and I hadn''t put together yet that the auto models should work now
          (doh!), so thanks for that pointer and confirming the code at least runs!
          </p>

          <p>I find it interesting that there isn''t (yet) a script specifically for
          decoder-only models, and I''m not surprised there was a catch when you ran
          the script at first. I think your idea of using the bos token may be the
          right answer. I may see if I can get that to work, and if so, what the outcomes
          are. Will keep you updated!</p>

          '
        raw: "Ah, great catch! Yeah, I suspected the seq2seq.py might be the wrong\
          \ script and I hadn't put together yet that the auto models should work\
          \ now (doh!), so thanks for that pointer and confirming the code at least\
          \ runs! \n\nI find it interesting that there isn't (yet) a script specifically\
          \ for decoder-only models, and I'm not surprised there was a catch when\
          \ you ran the script at first. I think your idea of using the bos token\
          \ may be the right answer. I may see if I can get that to work, and if so,\
          \ what the outcomes are. Will keep you updated!"
        updatedAt: '2022-10-05T13:55:20.247Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
        - TurboPascal
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ybelkada
    id: 633d8cc85ebbadfdabc12ecc
    type: comment
  author: jasoneden
  content: "Ah, great catch! Yeah, I suspected the seq2seq.py might be the wrong script\
    \ and I hadn't put together yet that the auto models should work now (doh!), so\
    \ thanks for that pointer and confirming the code at least runs! \n\nI find it\
    \ interesting that there isn't (yet) a script specifically for decoder-only models,\
    \ and I'm not surprised there was a catch when you ran the script at first. I\
    \ think your idea of using the bos token may be the right answer. I may see if\
    \ I can get that to work, and if so, what the outcomes are. Will keep you updated!"
  created_at: 2022-10-05 12:55:20+00:00
  edited: false
  hidden: false
  id: 633d8cc85ebbadfdabc12ecc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-10-05T14:06:50.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Perfect thank you very much! Can''t wait to see the first BLOOM
          model finetuned on Question Answering !</p>

          '
        raw: Perfect thank you very much! Can't wait to see the first BLOOM model
          finetuned on Question Answering !
        updatedAt: '2022-10-05T14:06:50.534Z'
      numEdits: 0
      reactions: []
    id: 633d8f7a0d5af7f9c13e88e6
    type: comment
  author: ybelkada
  content: Perfect thank you very much! Can't wait to see the first BLOOM model finetuned
    on Question Answering !
  created_at: 2022-10-05 13:06:50+00:00
  edited: false
  hidden: false
  id: 633d8f7a0d5af7f9c13e88e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
      fullname: Boyu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NXBY
      type: user
    createdAt: '2022-10-05T19:46:26.000Z'
    data:
      edited: false
      editors:
      - NXBY
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
          fullname: Boyu
          isHf: false
          isPro: false
          name: NXBY
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;jasoneden&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jasoneden\">@<span class=\"\
          underline\">jasoneden</span></a></span>\n\n\t</span></span> , thank you\
          \ for your messages! I tried to pre-pend the input with a special token\
          \ \"\xA4\" and the code you shared is running without error messages! (It's\
          \ been 15 minutes that the code is running. I'll see if any error message\
          \ appears later).</p>\n"
        raw: "Hi @ybelkada and @jasoneden , thank you for your messages! I tried to\
          \ pre-pend the input with a special token \"\xA4\" and the code you shared\
          \ is running without error messages! (It's been 15 minutes that the code\
          \ is running. I'll see if any error message appears later)."
        updatedAt: '2022-10-05T19:46:26.320Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - ybelkada
        - Laaleh
    id: 633ddf12fb435ad233422520
    type: comment
  author: NXBY
  content: "Hi @ybelkada and @jasoneden , thank you for your messages! I tried to\
    \ pre-pend the input with a special token \"\xA4\" and the code you shared is\
    \ running without error messages! (It's been 15 minutes that the code is running.\
    \ I'll see if any error message appears later)."
  created_at: 2022-10-05 18:46:26+00:00
  edited: false
  hidden: false
  id: 633ddf12fb435ad233422520
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-05T20:08:32.000Z'
    data:
      edited: true
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: "<p>I was able to get it running as well. Tried a few things - the bos_token\
          \ didn't work (expecting an int) nor did bos_token_id - no surprise. I tried\
          \ a few other things, and eventually landed on assigning an out of range\
          \ int (sys.maxsize, then subtract one...) and that seemed to work for training,\
          \ etc. With the 560m model, my free Colab quickly ran out of RAM during\
          \ training though, so I haven't actually gotten to see it work. Trying to\
          \ see if I can get this to run locally or if I'll have to pull out my credit\
          \ card and pony up for a bigger runtime.</p>\n<p>Since the presumed answer\
          \ for \"no answer\" would be the same - i.e. \"I don't know...\" - I'm wondering\
          \ if the IDs need to be unique, or if just assigning a number that's not\
          \ already taken is going to suffice. If any int value will do (within reason),\
          \ maybe that's good enough? Of course, I think I'm logically just representing\
          \ the same solution that <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ came up with in the first place, but am looking forward to giving it a\
          \ full training. It's too bad they took the 350m model out of the repository,\
          \ but maybe that one would have been too large as well.</p>\n"
        raw: 'I was able to get it running as well. Tried a few things - the bos_token
          didn''t work (expecting an int) nor did bos_token_id - no surprise. I tried
          a few other things, and eventually landed on assigning an out of range int
          (sys.maxsize, then subtract one...) and that seemed to work for training,
          etc. With the 560m model, my free Colab quickly ran out of RAM during training
          though, so I haven''t actually gotten to see it work. Trying to see if I
          can get this to run locally or if I''ll have to pull out my credit card
          and pony up for a bigger runtime.


          Since the presumed answer for "no answer" would be the same - i.e. "I don''t
          know..." - I''m wondering if the IDs need to be unique, or if just assigning
          a number that''s not already taken is going to suffice. If any int value
          will do (within reason), maybe that''s good enough? Of course, I think I''m
          logically just representing the same solution that @ybelkada came up with
          in the first place, but am looking forward to giving it a full training.
          It''s too bad they took the 350m model out of the repository, but maybe
          that one would have been too large as well.'
        updatedAt: '2022-10-05T20:09:23.942Z'
      numEdits: 3
      reactions: []
    id: 633de440e7f8b20a2dcc466a
    type: comment
  author: jasoneden
  content: 'I was able to get it running as well. Tried a few things - the bos_token
    didn''t work (expecting an int) nor did bos_token_id - no surprise. I tried a
    few other things, and eventually landed on assigning an out of range int (sys.maxsize,
    then subtract one...) and that seemed to work for training, etc. With the 560m
    model, my free Colab quickly ran out of RAM during training though, so I haven''t
    actually gotten to see it work. Trying to see if I can get this to run locally
    or if I''ll have to pull out my credit card and pony up for a bigger runtime.


    Since the presumed answer for "no answer" would be the same - i.e. "I don''t know..."
    - I''m wondering if the IDs need to be unique, or if just assigning a number that''s
    not already taken is going to suffice. If any int value will do (within reason),
    maybe that''s good enough? Of course, I think I''m logically just representing
    the same solution that @ybelkada came up with in the first place, but am looking
    forward to giving it a full training. It''s too bad they took the 350m model out
    of the repository, but maybe that one would have been too large as well.'
  created_at: 2022-10-05 19:08:32+00:00
  edited: true
  hidden: false
  id: 633de440e7f8b20a2dcc466a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-10-05T20:38:35.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;NXBY&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/NXBY\">@<span class=\"\
          underline\">NXBY</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;jasoneden&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/jasoneden\">@<span class=\"underline\">jasoneden</span></a></span>\n\
          \n\t</span></span><br>Thanks a lot for your comments and insights !!<br><span\
          \ data-props=\"{&quot;user&quot;:&quot;NXBY&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/NXBY\">@<span class=\"underline\">NXBY</span></a></span>\n\
          \n\t</span></span> great to hear that the code is running!! Would love to\
          \ see the Q&amp;A results as soon as they are ready !!<br><span data-props=\"\
          {&quot;user&quot;:&quot;jasoneden&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/jasoneden\">@<span class=\"underline\">jasoneden</span></a></span>\n\
          \n\t</span></span>, I think you could try to run the colab by adding the\
          \ <code>--fp16</code> flag, it should reduce the size of the model  (thus\
          \ optimizer states too) by 2, I believe you will be able to fit the Colab\
          \ notebook then ;)<br>Regarding the pre-pending of the token, I think assigning\
          \ an integer is fine, as it is something that is expected, and also usually\
          \ the integers <code>0</code>, <code>1</code>, <code>2</code> corresponds\
          \ to the <code>unk</code>, <code>bos</code> and <code>eos</code> token,\
          \ see here for eg: <a href=\"https://huggingface.co/bigscience/bloom-7b1/blob/main/config.json#L9\"\
          >https://huggingface.co/bigscience/bloom-7b1/blob/main/config.json#L9</a>\
          \ so maybe you could give it a try with that ;) Overall I would second your\
          \ intuition here</p>\n"
        raw: "@NXBY @jasoneden \nThanks a lot for your comments and insights !!\n\
          @NXBY great to hear that the code is running!! Would love to see the Q&A\
          \ results as soon as they are ready !!\n@jasoneden, I think you could try\
          \ to run the colab by adding the `--fp16` flag, it should reduce the size\
          \ of the model  (thus optimizer states too) by 2, I believe you will be\
          \ able to fit the Colab notebook then ;) \nRegarding the pre-pending of\
          \ the token, I think assigning an integer is fine, as it is something that\
          \ is expected, and also usually the integers `0`, `1`, `2` corresponds to\
          \ the `unk`, `bos` and `eos` token, see here for eg: https://huggingface.co/bigscience/bloom-7b1/blob/main/config.json#L9\
          \ so maybe you could give it a try with that ;) Overall I would second your\
          \ intuition here"
        updatedAt: '2022-10-05T20:38:35.010Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jasoneden
    id: 633deb4b55a63a501a4deb2f
    type: comment
  author: ybelkada
  content: "@NXBY @jasoneden \nThanks a lot for your comments and insights !!\n@NXBY\
    \ great to hear that the code is running!! Would love to see the Q&A results as\
    \ soon as they are ready !!\n@jasoneden, I think you could try to run the colab\
    \ by adding the `--fp16` flag, it should reduce the size of the model  (thus optimizer\
    \ states too) by 2, I believe you will be able to fit the Colab notebook then\
    \ ;) \nRegarding the pre-pending of the token, I think assigning an integer is\
    \ fine, as it is something that is expected, and also usually the integers `0`,\
    \ `1`, `2` corresponds to the `unk`, `bos` and `eos` token, see here for eg: https://huggingface.co/bigscience/bloom-7b1/blob/main/config.json#L9\
    \ so maybe you could give it a try with that ;) Overall I would second your intuition\
    \ here"
  created_at: 2022-10-05 19:38:35+00:00
  edited: false
  hidden: false
  id: 633deb4b55a63a501a4deb2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-05T20:51:42.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: '<p>Yeah, I actually got it to work by assigning a static value of 0
          (which made sense) but I worried a little about overwriting an expected
          value of some sort with that approach, which is why I switched to the sys.maxsize
          minus one value, which also seemed to work. If you''re curious...</p>

          <p>cls_index = 9223372036854775806</p>

          <p>Thanks for the tip on the --fp16 flag! I''ll give that a whirl.</p>

          '
        raw: 'Yeah, I actually got it to work by assigning a static value of 0 (which
          made sense) but I worried a little about overwriting an expected value of
          some sort with that approach, which is why I switched to the sys.maxsize
          minus one value, which also seemed to work. If you''re curious...


          cls_index = 9223372036854775806


          Thanks for the tip on the --fp16 flag! I''ll give that a whirl.'
        updatedAt: '2022-10-05T20:51:42.268Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ybelkada
    id: 633dee5e797cf1b03055aadb
    type: comment
  author: jasoneden
  content: 'Yeah, I actually got it to work by assigning a static value of 0 (which
    made sense) but I worried a little about overwriting an expected value of some
    sort with that approach, which is why I switched to the sys.maxsize minus one
    value, which also seemed to work. If you''re curious...


    cls_index = 9223372036854775806


    Thanks for the tip on the --fp16 flag! I''ll give that a whirl.'
  created_at: 2022-10-05 19:51:42+00:00
  edited: false
  hidden: false
  id: 633dee5e797cf1b03055aadb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
      fullname: Boyu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NXBY
      type: user
    createdAt: '2022-10-05T21:35:16.000Z'
    data:
      edited: false
      editors:
      - NXBY
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
          fullname: Boyu
          isHf: false
          isPro: false
          name: NXBY
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ , thank you for the advice! But even with the --fp16 flag, the disk space\
          \ of the free version of Google Colab is still not enough. I wonder if we\
          \ really have to save all the check points? Thanks!</p>\n"
        raw: Hi @ybelkada , thank you for the advice! But even with the --fp16 flag,
          the disk space of the free version of Google Colab is still not enough.
          I wonder if we really have to save all the check points? Thanks!
        updatedAt: '2022-10-05T21:35:16.633Z'
      numEdits: 0
      reactions: []
    id: 633df894e7f8b20a2dcd245a
    type: comment
  author: NXBY
  content: Hi @ybelkada , thank you for the advice! But even with the --fp16 flag,
    the disk space of the free version of Google Colab is still not enough. I wonder
    if we really have to save all the check points? Thanks!
  created_at: 2022-10-05 20:35:16+00:00
  edited: false
  hidden: false
  id: 633df894e7f8b20a2dcd245a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-10-07T08:37:34.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;NXBY&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/NXBY\">@<span class=\"\
          underline\">NXBY</span></a></span>\n\n\t</span></span> !<br>I don't think\
          \ it is necessary to save all the checkpoints indeed, maybe you can slightly\
          \ tweak the trainer: <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L2055\"\
          >https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L2055</a>\
          \ or here: <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L2100\"\
          >https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L2100</a>\
          \ to save it only once, I see that you can also change it here: <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer_callback.py#L140\"\
          >https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer_callback.py#L140</a>\
          \ maybe? Let me know if you need more help!</p>\n"
        raw: "Hey @NXBY ! \nI don't think it is necessary to save all the checkpoints\
          \ indeed, maybe you can slightly tweak the trainer: https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L2055\
          \ or here: https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L2100\
          \ to save it only once, I see that you can also change it here: https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer_callback.py#L140\
          \ maybe? Let me know if you need more help!"
        updatedAt: '2022-10-07T08:39:56.842Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - NXBY
    id: 633fe54ef81b9d10135fe832
    type: comment
  author: ybelkada
  content: "Hey @NXBY ! \nI don't think it is necessary to save all the checkpoints\
    \ indeed, maybe you can slightly tweak the trainer: https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L2055\
    \ or here: https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L2100\
    \ to save it only once, I see that you can also change it here: https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer_callback.py#L140\
    \ maybe? Let me know if you need more help!"
  created_at: 2022-10-07 07:37:34+00:00
  edited: true
  hidden: false
  id: 633fe54ef81b9d10135fe832
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
      fullname: Boyu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NXBY
      type: user
    createdAt: '2022-10-10T16:42:27.000Z'
    data:
      edited: false
      editors:
      - NXBY
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea15e53f765687578629b79a7e730561.svg
          fullname: Boyu
          isHf: false
          isPro: false
          name: NXBY
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ , thank you for your message and sorry for my late reply! The code is\
          \ long and I took some time to read it, and here I still have one question:\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer_callback.py#L140\"\
          >here in line 140 of traner_callback.py file you shared last time</a>, we\
          \ see that the default value of the parameter \"should_save\" is set to\
          \ False.<br>So I imagine that it has been converted to True somewhere in\
          \ the code that we run.<br>Do you think that in <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L1689\"\
          >this for loop of trainer.py</a>, we can do something in order that we set\
          \ the parameter \"should_save\" to True only in the last epoch of the loop\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L1833\"\
          >here</a> for example? Or perhaps what I think is wrong.<br>Could you please\
          \ share your ideas?<br>Thank you very much!</p>\n"
        raw: "Hi @ybelkada , thank you for your message and sorry for my late reply!\
          \ The code is long and I took some time to read it, and here I still have\
          \ one question: [here in line 140 of traner_callback.py file you shared\
          \ last time](https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer_callback.py#L140),\
          \ we see that the default value of the parameter \"should_save\" is set\
          \ to False. \nSo I imagine that it has been converted to True somewhere\
          \ in the code that we run. \nDo you think that in [this for loop of trainer.py](https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L1689),\
          \ we can do something in order that we set the parameter \"should_save\"\
          \ to True only in the last epoch of the loop [here](https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L1833)\
          \ for example? Or perhaps what I think is wrong. \nCould you please share\
          \ your ideas? \nThank you very much!"
        updatedAt: '2022-10-10T16:42:27.405Z'
      numEdits: 0
      reactions: []
    id: 63444b736e988c6a792b22e7
    type: comment
  author: NXBY
  content: "Hi @ybelkada , thank you for your message and sorry for my late reply!\
    \ The code is long and I took some time to read it, and here I still have one\
    \ question: [here in line 140 of traner_callback.py file you shared last time](https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer_callback.py#L140),\
    \ we see that the default value of the parameter \"should_save\" is set to False.\
    \ \nSo I imagine that it has been converted to True somewhere in the code that\
    \ we run. \nDo you think that in [this for loop of trainer.py](https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L1689),\
    \ we can do something in order that we set the parameter \"should_save\" to True\
    \ only in the last epoch of the loop [here](https://github.com/huggingface/transformers/blob/ae3e3bc60a5f0834d952dfead4b28b1ce506125d/src/transformers/trainer.py#L1833)\
    \ for example? Or perhaps what I think is wrong. \nCould you please share your\
    \ ideas? \nThank you very much!"
  created_at: 2022-10-10 15:42:27+00:00
  edited: false
  hidden: false
  id: 63444b736e988c6a792b22e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-22T18:07:39.000Z'
    data:
      edited: true
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: '<p><em><strong>Edited</strong></em></p>

          <p>Hey all - Just updating the thread here. I was unable to get the training
          to completion on Colab (free version) but was able to tune the scripts a
          bit. As stated above, I set the cls_index value to a large number, and also
          figured out I needed to lower the batch size. On a single GPU, changing
          </p>

          <p>--per_device_train_batch_size 4 \</p>

          <p>...from the default value of 12 seemed to work on the Colab GPUs. The
          real issue is environment timeout though. The 560M parameter model is just
          too large for the training on the Squad data even with the minimal parameters
          set (I believe - I''d love for someone to show me I''m wrong...) So I have
          now switched to a Vertex AI workbench instance with a V100 GPU. Again, even
          there I had to lower the batch size, but the training is running significantly
          faster now. I believe this is a path to success - albeit one that will run
          up some cloud charges - and will report back once I have the model "fully
          trained." I have no illusions it will be a good model with these parameters,
          but as a proof-of-concept just having the steps in place that could be replicated
          on larger model / hardware / etc. has value I think.</p>

          <p>EDIT: Had a newb error first time I tried training the model - forgot
          to change my output directory and ran out of disk space on my boot disk.
          If you''re using Vertex like I am, make sure you prepend the output directory
          to something like this:</p>

          <p>--output_dir /home/jupyter/tmp/debug_bloom_squad/  </p>

          <p>...instead of just /tmp. </p>

          <p>Also, upped the batch size on Vertex from 4 to 6, no out of memory errors.
          Once I''ve got a model built, I may go back and toy with this to see how
          large I can make it without running into the out of memory error on the
          GPU.</p>

          <p>"Experience enables you to recognize a mistake when you make it again."
          - Franklin P. Jones</p>

          '
        raw: "***Edited***\n\nHey all - Just updating the thread here. I was unable\
          \ to get the training to completion on Colab (free version) but was able\
          \ to tune the scripts a bit. As stated above, I set the cls_index value\
          \ to a large number, and also figured out I needed to lower the batch size.\
          \ On a single GPU, changing \n\n--per_device_train_batch_size 4 \\\n\n...from\
          \ the default value of 12 seemed to work on the Colab GPUs. The real issue\
          \ is environment timeout though. The 560M parameter model is just too large\
          \ for the training on the Squad data even with the minimal parameters set\
          \ (I believe - I'd love for someone to show me I'm wrong...) So I have now\
          \ switched to a Vertex AI workbench instance with a V100 GPU. Again, even\
          \ there I had to lower the batch size, but the training is running significantly\
          \ faster now. I believe this is a path to success - albeit one that will\
          \ run up some cloud charges - and will report back once I have the model\
          \ \"fully trained.\" I have no illusions it will be a good model with these\
          \ parameters, but as a proof-of-concept just having the steps in place that\
          \ could be replicated on larger model / hardware / etc. has value I think.\n\
          \nEDIT: Had a newb error first time I tried training the model - forgot\
          \ to change my output directory and ran out of disk space on my boot disk.\
          \ If you're using Vertex like I am, make sure you prepend the output directory\
          \ to something like this:\n\n--output_dir /home/jupyter/tmp/debug_bloom_squad/\
          \  \n\n...instead of just /tmp. \n\nAlso, upped the batch size on Vertex\
          \ from 4 to 6, no out of memory errors. Once I've got a model built, I may\
          \ go back and toy with this to see how large I can make it without running\
          \ into the out of memory error on the GPU.\n\n\"Experience enables you to\
          \ recognize a mistake when you make it again.\" - Franklin P. Jones"
        updatedAt: '2022-10-22T18:29:41.057Z'
      numEdits: 5
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ybelkada
      - count: 1
        reaction: "\U0001F44D"
        users:
        - NXBY
    id: 6354316beca5a823e3b95128
    type: comment
  author: jasoneden
  content: "***Edited***\n\nHey all - Just updating the thread here. I was unable\
    \ to get the training to completion on Colab (free version) but was able to tune\
    \ the scripts a bit. As stated above, I set the cls_index value to a large number,\
    \ and also figured out I needed to lower the batch size. On a single GPU, changing\
    \ \n\n--per_device_train_batch_size 4 \\\n\n...from the default value of 12 seemed\
    \ to work on the Colab GPUs. The real issue is environment timeout though. The\
    \ 560M parameter model is just too large for the training on the Squad data even\
    \ with the minimal parameters set (I believe - I'd love for someone to show me\
    \ I'm wrong...) So I have now switched to a Vertex AI workbench instance with\
    \ a V100 GPU. Again, even there I had to lower the batch size, but the training\
    \ is running significantly faster now. I believe this is a path to success - albeit\
    \ one that will run up some cloud charges - and will report back once I have the\
    \ model \"fully trained.\" I have no illusions it will be a good model with these\
    \ parameters, but as a proof-of-concept just having the steps in place that could\
    \ be replicated on larger model / hardware / etc. has value I think.\n\nEDIT:\
    \ Had a newb error first time I tried training the model - forgot to change my\
    \ output directory and ran out of disk space on my boot disk. If you're using\
    \ Vertex like I am, make sure you prepend the output directory to something like\
    \ this:\n\n--output_dir /home/jupyter/tmp/debug_bloom_squad/  \n\n...instead of\
    \ just /tmp. \n\nAlso, upped the batch size on Vertex from 4 to 6, no out of memory\
    \ errors. Once I've got a model built, I may go back and toy with this to see\
    \ how large I can make it without running into the out of memory error on the\
    \ GPU.\n\n\"Experience enables you to recognize a mistake when you make it again.\"\
    \ - Franklin P. Jones"
  created_at: 2022-10-22 17:07:39+00:00
  edited: true
  hidden: false
  id: 6354316beca5a823e3b95128
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-23T00:31:00.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: '<p>OK - model is trained! Haven''t tested it yet, but I''m calling
          this milestone complete.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1666484786491-630cdf8e3dc31beba6ed4c9d.png"><img
          alt="Screen Shot 2022-10-22 at 5.35.53 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/1666484786491-630cdf8e3dc31beba6ed4c9d.png"></a></p>

          <p>The /tmp directory ended up burning about 600 GB of disk space for the
          model and settings I was using, so make sure you have enough disk space
          configured. (Another Achilles'' heel of Colab I ran into.)</p>

          <p>Thank you for all the support on this! Now to see if it actually works.....</p>

          '
        raw: 'OK - model is trained! Haven''t tested it yet, but I''m calling this
          milestone complete.


          ![Screen Shot 2022-10-22 at 5.35.53 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1666484786491-630cdf8e3dc31beba6ed4c9d.png)


          The /tmp directory ended up burning about 600 GB of disk space for the model
          and settings I was using, so make sure you have enough disk space configured.
          (Another Achilles'' heel of Colab I ran into.)


          Thank you for all the support on this! Now to see if it actually works.....'
        updatedAt: '2022-10-23T00:31:00.595Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
    id: 63548b4412edd0ed5dc54c25
    type: comment
  author: jasoneden
  content: 'OK - model is trained! Haven''t tested it yet, but I''m calling this milestone
    complete.


    ![Screen Shot 2022-10-22 at 5.35.53 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1666484786491-630cdf8e3dc31beba6ed4c9d.png)


    The /tmp directory ended up burning about 600 GB of disk space for the model and
    settings I was using, so make sure you have enough disk space configured. (Another
    Achilles'' heel of Colab I ran into.)


    Thank you for all the support on this! Now to see if it actually works.....'
  created_at: 2022-10-22 23:31:00+00:00
  edited: false
  hidden: false
  id: 63548b4412edd0ed5dc54c25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-10-23T12:29:57.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Wow that''s so cool!!<br>You can also upload your model on the Hub
          so that we can try it out!! Can''t wait to see the results!!</p>

          '
        raw: "Wow that's so cool!! \nYou can also upload your model on the Hub so\
          \ that we can try it out!! Can't wait to see the results!!"
        updatedAt: '2022-10-23T12:29:57.113Z'
      numEdits: 0
      reactions: []
    id: 635533c59c72a7e742efcdec
    type: comment
  author: ybelkada
  content: "Wow that's so cool!! \nYou can also upload your model on the Hub so that\
    \ we can try it out!! Can't wait to see the results!!"
  created_at: 2022-10-23 11:29:57+00:00
  edited: false
  hidden: false
  id: 635533c59c72a7e742efcdec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-23T21:25:14.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: '<p>Yes, that is the plan! I will try to do that ASAP and let everyone
          know when it''s live. It''s not going to be too impressive, I don''t suspect,
          but it may serve as a starting point!</p>

          '
        raw: Yes, that is the plan! I will try to do that ASAP and let everyone know
          when it's live. It's not going to be too impressive, I don't suspect, but
          it may serve as a starting point!
        updatedAt: '2022-10-23T21:25:14.930Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ybelkada
      - count: 1
        reaction: "\U0001F44D"
        users:
        - NXBY
    id: 6355b13aa5546ae4c5cad026
    type: comment
  author: jasoneden
  content: Yes, that is the plan! I will try to do that ASAP and let everyone know
    when it's live. It's not going to be too impressive, I don't suspect, but it may
    serve as a starting point!
  created_at: 2022-10-23 20:25:14+00:00
  edited: false
  hidden: false
  id: 6355b13aa5546ae4c5cad026
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-25T15:53:09.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: '<p>Hey folks, I''m kicking the tires on my model and running into output
          I don''t understand. See the screenshot below:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1666713004075-630cdf8e3dc31beba6ed4c9d.png"><img
          alt="Screen Shot 2022-10-25 at 10.48.55 AM.png" src="https://cdn-uploads.huggingface.co/production/uploads/1666713004075-630cdf8e3dc31beba6ed4c9d.png"></a></p>

          <p>General issues:</p>

          <ol>

          <li>Output is not integer</li>

          <li>Start tokens are larger values than end tokens</li>

          </ol>

          <p>I''ve walked through the question answering guide on colab and there''s
          nothing really on how to deal with outputs like these. Any pointers?</p>

          '
        raw: 'Hey folks, I''m kicking the tires on my model and running into output
          I don''t understand. See the screenshot below:



          ![Screen Shot 2022-10-25 at 10.48.55 AM.png](https://cdn-uploads.huggingface.co/production/uploads/1666713004075-630cdf8e3dc31beba6ed4c9d.png)


          General issues:


          1) Output is not integer

          2) Start tokens are larger values than end tokens


          I''ve walked through the question answering guide on colab and there''s
          nothing really on how to deal with outputs like these. Any pointers?'
        updatedAt: '2022-10-25T15:53:09.777Z'
      numEdits: 0
      reactions: []
    id: 6358066570c0738527a4ae0b
    type: comment
  author: jasoneden
  content: 'Hey folks, I''m kicking the tires on my model and running into output
    I don''t understand. See the screenshot below:



    ![Screen Shot 2022-10-25 at 10.48.55 AM.png](https://cdn-uploads.huggingface.co/production/uploads/1666713004075-630cdf8e3dc31beba6ed4c9d.png)


    General issues:


    1) Output is not integer

    2) Start tokens are larger values than end tokens


    I''ve walked through the question answering guide on colab and there''s nothing
    really on how to deal with outputs like these. Any pointers?'
  created_at: 2022-10-25 14:53:09+00:00
  edited: false
  hidden: false
  id: 6358066570c0738527a4ae0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-25T19:28:25.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: '<p>I have uploaded the model (correctly, I think... please advise if
          not) for anyone that wants to give it a whirl:</p>

          <p><a href="https://huggingface.co/jasoneden/bloom560m-squad-helloworld/tree/main">https://huggingface.co/jasoneden/bloom560m-squad-helloworld/tree/main</a>
          </p>

          <p>Once I figure out how to upload from from my Vertex AI Notebook, I will
          upload all of the checkpoint folders as well.</p>

          '
        raw: "I have uploaded the model (correctly, I think... please advise if not)\
          \ for anyone that wants to give it a whirl:\n\nhttps://huggingface.co/jasoneden/bloom560m-squad-helloworld/tree/main\
          \ \n\nOnce I figure out how to upload from from my Vertex AI Notebook, I\
          \ will upload all of the checkpoint folders as well."
        updatedAt: '2022-10-25T19:28:25.681Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
        - julien-c
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ybelkada
    id: 635838d91d66b442317bbd49
    type: comment
  author: jasoneden
  content: "I have uploaded the model (correctly, I think... please advise if not)\
    \ for anyone that wants to give it a whirl:\n\nhttps://huggingface.co/jasoneden/bloom560m-squad-helloworld/tree/main\
    \ \n\nOnce I figure out how to upload from from my Vertex AI Notebook, I will\
    \ upload all of the checkpoint folders as well."
  created_at: 2022-10-25 18:28:25+00:00
  edited: false
  hidden: false
  id: 635838d91d66b442317bbd49
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-10-26T09:31:58.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jasoneden&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jasoneden\">@<span class=\"\
          underline\">jasoneden</span></a></span>\n\n\t</span></span><br>Your model\
          \ ROCKS!!<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1666776695988-62441d1d9fdefb55a0b7d12c.png\"\
          ><img alt=\"Screenshot 2022-10-26 at 11.31.00.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1666776695988-62441d1d9fdefb55a0b7d12c.png\"\
          ></a><br>And it works as expected on the Hub! I really appreciate your great\
          \ effort for training the model !!</p>\n"
        raw: "@jasoneden\nYour model ROCKS!! \n![Screenshot 2022-10-26 at 11.31.00.png](https://cdn-uploads.huggingface.co/production/uploads/1666776695988-62441d1d9fdefb55a0b7d12c.png)\n\
          And it works as expected on the Hub! I really appreciate your great effort\
          \ for training the model !!"
        updatedAt: '2022-10-26T09:37:38.594Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F917"
        users:
        - jasoneden
        - julien-c
    id: 6358fe8e82cdad8336b08991
    type: comment
  author: ybelkada
  content: "@jasoneden\nYour model ROCKS!! \n![Screenshot 2022-10-26 at 11.31.00.png](https://cdn-uploads.huggingface.co/production/uploads/1666776695988-62441d1d9fdefb55a0b7d12c.png)\n\
    And it works as expected on the Hub! I really appreciate your great effort for\
    \ training the model !!"
  created_at: 2022-10-26 08:31:58+00:00
  edited: true
  hidden: false
  id: 6358fe8e82cdad8336b08991
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-10-26T09:34:45.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>I think that you can use your model directly using  <code>pipeline</code>,
          see: <a href="https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline">https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline</a>  -
          it is how the inference API works under the hood</p>

          '
        raw: 'I think that you can use your model directly using  `pipeline`, see:
          https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline  -
          it is how the inference API works under the hood'
        updatedAt: '2022-10-26T09:34:45.920Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - julien-c
    id: 6358ff3590a313828b9d1a1d
    type: comment
  author: ybelkada
  content: 'I think that you can use your model directly using  `pipeline`, see: https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline  -
    it is how the inference API works under the hood'
  created_at: 2022-10-26 08:34:45+00:00
  edited: false
  hidden: false
  id: 6358ff3590a313828b9d1a1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-10-26T09:43:03.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Also this link could be a great pointer: <a href="https://huggingface.co/docs/transformers/v4.23.1/en/task_summary#extractive-question-answering">https://huggingface.co/docs/transformers/v4.23.1/en/task_summary#extractive-question-answering</a></p>

          '
        raw: 'Also this link could be a great pointer: https://huggingface.co/docs/transformers/v4.23.1/en/task_summary#extractive-question-answering'
        updatedAt: '2022-10-26T09:43:03.967Z'
      numEdits: 0
      reactions: []
    id: 63590127fb69fbb050a63691
    type: comment
  author: ybelkada
  content: 'Also this link could be a great pointer: https://huggingface.co/docs/transformers/v4.23.1/en/task_summary#extractive-question-answering'
  created_at: 2022-10-26 08:43:03+00:00
  edited: false
  hidden: false
  id: 63590127fb69fbb050a63691
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-10-27T14:27:00.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: '<p>Hmm.</p>

          <p>First off, thanks ybelkada for the pointers. I guess I''m a little surprised:
          I thought that the end result might be a generative rather than extractive
          model. In hindsight, maybe I shouldn''t be. This actually begs a couple
          of questions:</p>

          <ol>

          <li><p>Did it actually matter what data I used to train BLOOM, or since
          we''re making it an extractive system, could I have used a small set of
          question / answer pairs and gotten similar results?</p>

          </li>

          <li><p>Is there a way to train BLOOM such that it results in a generative
          (and preferably, closed generative) question answering model?</p>

          </li>

          </ol>

          '
        raw: 'Hmm.


          First off, thanks ybelkada for the pointers. I guess I''m a little surprised:
          I thought that the end result might be a generative rather than extractive
          model. In hindsight, maybe I shouldn''t be. This actually begs a couple
          of questions:


          1) Did it actually matter what data I used to train BLOOM, or since we''re
          making it an extractive system, could I have used a small set of question
          / answer pairs and gotten similar results?


          2) Is there a way to train BLOOM such that it results in a generative (and
          preferably, closed generative) question answering model?'
        updatedAt: '2022-10-27T14:27:00.700Z'
      numEdits: 0
      reactions: []
    id: 635a95341f1698d824b091bc
    type: comment
  author: jasoneden
  content: 'Hmm.


    First off, thanks ybelkada for the pointers. I guess I''m a little surprised:
    I thought that the end result might be a generative rather than extractive model.
    In hindsight, maybe I shouldn''t be. This actually begs a couple of questions:


    1) Did it actually matter what data I used to train BLOOM, or since we''re making
    it an extractive system, could I have used a small set of question / answer pairs
    and gotten similar results?


    2) Is there a way to train BLOOM such that it results in a generative (and preferably,
    closed generative) question answering model?'
  created_at: 2022-10-27 13:27:00+00:00
  edited: false
  hidden: false
  id: 635a95341f1698d824b091bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666122903077-noauth.png?w=200&h=200&f=face
      fullname: Connor Harmelink
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Charm3link
      type: user
    createdAt: '2022-10-27T20:55:31.000Z'
    data:
      edited: false
      editors:
      - Charm3link
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666122903077-noauth.png?w=200&h=200&f=face
          fullname: Connor Harmelink
          isHf: false
          isPro: false
          name: Charm3link
          type: user
        html: '<p>I am curious if it would be possible to train the model for data-to-text
          generation. I attempted to use the Seq2Seq trainer for this, framing the
          problem as a generic text-to-text task, but it seems like the model only
          ever generates more "data". This set up worked for T5, but not for bloom.
          Has anyone else explored this?</p>

          '
        raw: I am curious if it would be possible to train the model for data-to-text
          generation. I attempted to use the Seq2Seq trainer for this, framing the
          problem as a generic text-to-text task, but it seems like the model only
          ever generates more "data". This set up worked for T5, but not for bloom.
          Has anyone else explored this?
        updatedAt: '2022-10-27T20:55:31.682Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - TurboPascal
        - rhui
    id: 635af04331ec0f678c385e19
    type: comment
  author: Charm3link
  content: I am curious if it would be possible to train the model for data-to-text
    generation. I attempted to use the Seq2Seq trainer for this, framing the problem
    as a generic text-to-text task, but it seems like the model only ever generates
    more "data". This set up worked for T5, but not for bloom. Has anyone else explored
    this?
  created_at: 2022-10-27 19:55:31+00:00
  edited: false
  hidden: false
  id: 635af04331ec0f678c385e19
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7ab43ec4ba2c0219c3cc05e952366d3.svg
      fullname: Luca Pro
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lucapro
      type: user
    createdAt: '2022-11-01T10:11:29.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/e7ab43ec4ba2c0219c3cc05e952366d3.svg
          fullname: Luca Pro
          isHf: false
          isPro: false
          name: Lucapro
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2022-11-01T11:05:49.861Z'
      numEdits: 3
      reactions: []
    id: 6360f0d16bd72c97d0fce9b2
    type: comment
  author: Lucapro
  content: This comment has been hidden
  created_at: 2022-11-01 09:11:29+00:00
  edited: true
  hidden: true
  id: 6360f0d16bd72c97d0fce9b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-01T11:16:49.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<blockquote>

          <p>I am curious if it would be possible to train the model for data-to-text
          generation. I attempted to use the Seq2Seq trainer for this, framing the
          problem as a generic text-to-text task, but it seems like the model only
          ever generates more "data". This set up worked for T5, but not for bloom.
          Has anyone else explored this?</p>

          </blockquote>

          <p>Yeah this is actually possible, we''ve seen this on our side. One of
          the reason I believe T5 works well is that inputs and targets are processed
          seperately (in different stacks), whereas in decoder only model like BLOOM
          has a single stack to fix this issue. One of the key issue is how you merge
          inputs and targets together for the decoder model:</p>

          <ul>

          <li>using a space would make it seems like language modeling task, as it
          keeps predicting future tokens so it doesn''t have a strong delimitation
          between your "data" and your target text.</li>

          <li>using "\n" or more clear patterns that make it seem like you require
          something from the model.</li>

          <li>using a special characters. The BLOOM models have a vocabulary that''s
          larger than what the tokenizer can provide (we have 200 extra tokens). The
          reasons why is there exists a bunch of reason why you''d want to use special
          tokens (bert has a mask token for making single tokens, t5 uses special
          tokens to make pointers for spans etc ...) So one of the idea is to seperate
          your append a special token to your input to signal the model to start generating
          the target.</li>

          </ul>

          <p>Bear in mind those are my hunches.</p>

          '
        raw: "> I am curious if it would be possible to train the model for data-to-text\
          \ generation. I attempted to use the Seq2Seq trainer for this, framing the\
          \ problem as a generic text-to-text task, but it seems like the model only\
          \ ever generates more \"data\". This set up worked for T5, but not for bloom.\
          \ Has anyone else explored this?\n\nYeah this is actually possible, we've\
          \ seen this on our side. One of the reason I believe T5 works well is that\
          \ inputs and targets are processed seperately (in different stacks), whereas\
          \ in decoder only model like BLOOM has a single stack to fix this issue.\
          \ One of the key issue is how you merge inputs and targets together for\
          \ the decoder model:\n - using a space would make it seems like language\
          \ modeling task, as it keeps predicting future tokens so it doesn't have\
          \ a strong delimitation between your \"data\" and your target text.\n -\
          \ using \"\\n\" or more clear patterns that make it seem like you require\
          \ something from the model.\n - using a special characters. The BLOOM models\
          \ have a vocabulary that's larger than what the tokenizer can provide (we\
          \ have 200 extra tokens). The reasons why is there exists a bunch of reason\
          \ why you'd want to use special tokens (bert has a mask token for making\
          \ single tokens, t5 uses special tokens to make pointers for spans etc ...)\
          \ So one of the idea is to seperate your append a special token to your\
          \ input to signal the model to start generating the target.\n\nBear in mind\
          \ those are my hunches."
        updatedAt: '2022-11-01T11:16:49.329Z'
      numEdits: 0
      reactions: []
    id: 63610021a46f0cdd62e39d18
    type: comment
  author: TimeRobber
  content: "> I am curious if it would be possible to train the model for data-to-text\
    \ generation. I attempted to use the Seq2Seq trainer for this, framing the problem\
    \ as a generic text-to-text task, but it seems like the model only ever generates\
    \ more \"data\". This set up worked for T5, but not for bloom. Has anyone else\
    \ explored this?\n\nYeah this is actually possible, we've seen this on our side.\
    \ One of the reason I believe T5 works well is that inputs and targets are processed\
    \ seperately (in different stacks), whereas in decoder only model like BLOOM has\
    \ a single stack to fix this issue. One of the key issue is how you merge inputs\
    \ and targets together for the decoder model:\n - using a space would make it\
    \ seems like language modeling task, as it keeps predicting future tokens so it\
    \ doesn't have a strong delimitation between your \"data\" and your target text.\n\
    \ - using \"\\n\" or more clear patterns that make it seem like you require something\
    \ from the model.\n - using a special characters. The BLOOM models have a vocabulary\
    \ that's larger than what the tokenizer can provide (we have 200 extra tokens).\
    \ The reasons why is there exists a bunch of reason why you'd want to use special\
    \ tokens (bert has a mask token for making single tokens, t5 uses special tokens\
    \ to make pointers for spans etc ...) So one of the idea is to seperate your append\
    \ a special token to your input to signal the model to start generating the target.\n\
    \nBear in mind those are my hunches."
  created_at: 2022-11-01 10:16:49+00:00
  edited: false
  hidden: false
  id: 63610021a46f0cdd62e39d18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-04T08:19:05.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<p>We finetuned BLOOM to produce <a href=\"https://huggingface.co/bigscience/bloomz\"\
          >BLOOMZ</a>, maybe our <a rel=\"nofollow\" href=\"https://github.com/bigscience-workshop/xmtf#bloomz\"\
          >guide</a> can help some people in this thread \U0001F917</p>\n"
        raw: "We finetuned BLOOM to produce [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
          \ maybe our [guide](https://github.com/bigscience-workshop/xmtf#bloomz)\
          \ can help some people in this thread \U0001F917"
        updatedAt: '2022-11-04T08:19:05.805Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - cakiki
        - Charm3link
        - bergr7f
        - jamesoneill12
      - count: 3
        reaction: "\U0001F44D"
        users:
        - NXBY
        - Charm3link
        - Tebmer
    id: 6364caf98b6bd0d9f654f5b7
    type: comment
  author: Muennighoff
  content: "We finetuned BLOOM to produce [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
    \ maybe our [guide](https://github.com/bigscience-workshop/xmtf#bloomz) can help\
    \ some people in this thread \U0001F917"
  created_at: 2022-11-04 07:19:05+00:00
  edited: false
  hidden: false
  id: 6364caf98b6bd0d9f654f5b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
      fullname: Khai Mai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: khaimaitien
      type: user
    createdAt: '2022-11-07T08:39:00.000Z'
    data:
      edited: false
      editors:
      - khaimaitien
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
          fullname: Khai Mai
          isHf: false
          isPro: false
          name: khaimaitien
          type: user
        html: "<blockquote>\n<p>We finetuned BLOOM to produce <a href=\"https://huggingface.co/bigscience/bloomz\"\
          >BLOOMZ</a>, maybe our <a rel=\"nofollow\" href=\"https://github.com/bigscience-workshop/xmtf#bloomz\"\
          >guide</a> can help some people in this thread \U0001F917</p>\n</blockquote>\n\
          <p>Can you tell me the number of GPUs with memory that you used to fine-tune\
          \ Bloomz and also the time for finetuning. By the way, Can I finetune 176B\
          \ model using 16 A100 40 GB without offloading to CPU or Nvme with batch-size\
          \ = 1 ?</p>\n"
        raw: "> We finetuned BLOOM to produce [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
          \ maybe our [guide](https://github.com/bigscience-workshop/xmtf#bloomz)\
          \ can help some people in this thread \U0001F917\n\nCan you tell me the\
          \ number of GPUs with memory that you used to fine-tune Bloomz and also\
          \ the time for finetuning. By the way, Can I finetune 176B model using 16\
          \ A100 40 GB without offloading to CPU or Nvme with batch-size = 1 ?"
        updatedAt: '2022-11-07T08:39:00.067Z'
      numEdits: 0
      reactions: []
    id: 6368c42450a665fa20d19328
    type: comment
  author: khaimaitien
  content: "> We finetuned BLOOM to produce [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
    \ maybe our [guide](https://github.com/bigscience-workshop/xmtf#bloomz) can help\
    \ some people in this thread \U0001F917\n\nCan you tell me the number of GPUs\
    \ with memory that you used to fine-tune Bloomz and also the time for finetuning.\
    \ By the way, Can I finetune 176B model using 16 A100 40 GB without offloading\
    \ to CPU or Nvme with batch-size = 1 ?"
  created_at: 2022-11-07 08:39:00+00:00
  edited: false
  hidden: false
  id: 6368c42450a665fa20d19328
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-07T08:54:25.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<blockquote>\n<p>Can you tell me the number of GPUs with memory that\
          \ you used to fine-tune Bloomz and also the time for finetuning. By the\
          \ way, Can I finetune 176B model using 16 A100 40 GB without offloading\
          \ to CPU or Nvme with batch-size = 1 ?</p>\n</blockquote>\n<p>Sure, all\
          \ GPU numbers are in the model cards of each bloomz model, e.g. for the\
          \ big one: <a href=\"https://huggingface.co/bigscience/bloomz#hardware\"\
          >https://huggingface.co/bigscience/bloomz#hardware</a> (also some comments\
          \ <a href=\"https://huggingface.co/bigscience/bloomz/discussions/14#6366182ed0ee6e2662b16392\"\
          >here</a>).</p>\n<p>The time can be found in the <a href=\"https://huggingface.co/bigscience/bloomz/blob/main/logs/logs/xp3capmixnewcodelonglossseq/main_log.txt\"\
          >logs</a>. It will depend a lot on your hardware &amp; the model layout\
          \ though. For us, it took 142s / 4.2M tokens. So for the 2.1B tokens trained,\
          \ roughly 20 hours.</p>\n<pre><code>[default7]: iteration      409/    3100\
          \ | consumed samples:       837632 | consumed tokens:   1715470336 | elapsed\
          \ time per iteration (s): 140.78 | learning rate: 2.000E-05 | global batch\
          \ size:  2048 | lm loss: 1.117569E+00 | grad norm: 0.459 | num zeros: 0.0\
          \ | number of skipped iterations:   0 | number of nan iterations:   0 |\
          \ samples per second: 14.547 | TFLOPs: 148.50 \n[default7]: iteration  \
          \    410/    3100 | consumed samples:       839680 | consumed tokens:  \
          \ 1719664640 | elapsed time per iteration (s): 141.97 | learning rate: 2.000E-05\
          \ | global batch size:  2048 | lm loss: 1.119031E+00 | grad norm: 0.552\
          \ | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:\
          \   0 | samples per second: 14.425 | TFLOPs: 147.26 |\n</code></pre>\n<p>I\
          \ think the 176B is not easily possible with 16 A100 40 GB. For the 7B1\
          \ it should be enough, probably with pipeline parallel = 4 &amp; data parallel\
          \ = 4 &amp; tp = 1.</p>\n"
        raw: "> Can you tell me the number of GPUs with memory that you used to fine-tune\
          \ Bloomz and also the time for finetuning. By the way, Can I finetune 176B\
          \ model using 16 A100 40 GB without offloading to CPU or Nvme with batch-size\
          \ = 1 ?\n\nSure, all GPU numbers are in the model cards of each bloomz model,\
          \ e.g. for the big one: https://huggingface.co/bigscience/bloomz#hardware\
          \ (also some comments [here](https://huggingface.co/bigscience/bloomz/discussions/14#6366182ed0ee6e2662b16392)).\n\
          \nThe time can be found in the [logs](https://huggingface.co/bigscience/bloomz/blob/main/logs/logs/xp3capmixnewcodelonglossseq/main_log.txt).\
          \ It will depend a lot on your hardware & the model layout though. For us,\
          \ it took 142s / 4.2M tokens. So for the 2.1B tokens trained, roughly 20\
          \ hours.\n\n```\n[default7]: iteration      409/    3100 | consumed samples:\
          \       837632 | consumed tokens:   1715470336 | elapsed time per iteration\
          \ (s): 140.78 | learning rate: 2.000E-05 | global batch size:  2048 | lm\
          \ loss: 1.117569E+00 | grad norm: 0.459 | num zeros: 0.0 | number of skipped\
          \ iterations:   0 | number of nan iterations:   0 | samples per second:\
          \ 14.547 | TFLOPs: 148.50 \n[default7]: iteration      410/    3100 | consumed\
          \ samples:       839680 | consumed tokens:   1719664640 | elapsed time per\
          \ iteration (s): 141.97 | learning rate: 2.000E-05 | global batch size:\
          \  2048 | lm loss: 1.119031E+00 | grad norm: 0.552 | num zeros: 0.0 | number\
          \ of skipped iterations:   0 | number of nan iterations:   0 | samples per\
          \ second: 14.425 | TFLOPs: 147.26 |\n```\n\nI think the 176B is not easily\
          \ possible with 16 A100 40 GB. For the 7B1 it should be enough, probably\
          \ with pipeline parallel = 4 & data parallel = 4 & tp = 1."
        updatedAt: '2022-11-07T08:54:25.490Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - khaimaitien
    id: 6368c7c1e943416fc806d623
    type: comment
  author: Muennighoff
  content: "> Can you tell me the number of GPUs with memory that you used to fine-tune\
    \ Bloomz and also the time for finetuning. By the way, Can I finetune 176B model\
    \ using 16 A100 40 GB without offloading to CPU or Nvme with batch-size = 1 ?\n\
    \nSure, all GPU numbers are in the model cards of each bloomz model, e.g. for\
    \ the big one: https://huggingface.co/bigscience/bloomz#hardware (also some comments\
    \ [here](https://huggingface.co/bigscience/bloomz/discussions/14#6366182ed0ee6e2662b16392)).\n\
    \nThe time can be found in the [logs](https://huggingface.co/bigscience/bloomz/blob/main/logs/logs/xp3capmixnewcodelonglossseq/main_log.txt).\
    \ It will depend a lot on your hardware & the model layout though. For us, it\
    \ took 142s / 4.2M tokens. So for the 2.1B tokens trained, roughly 20 hours.\n\
    \n```\n[default7]: iteration      409/    3100 | consumed samples:       837632\
    \ | consumed tokens:   1715470336 | elapsed time per iteration (s): 140.78 | learning\
    \ rate: 2.000E-05 | global batch size:  2048 | lm loss: 1.117569E+00 | grad norm:\
    \ 0.459 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:\
    \   0 | samples per second: 14.547 | TFLOPs: 148.50 \n[default7]: iteration  \
    \    410/    3100 | consumed samples:       839680 | consumed tokens:   1719664640\
    \ | elapsed time per iteration (s): 141.97 | learning rate: 2.000E-05 | global\
    \ batch size:  2048 | lm loss: 1.119031E+00 | grad norm: 0.552 | num zeros: 0.0\
    \ | number of skipped iterations:   0 | number of nan iterations:   0 | samples\
    \ per second: 14.425 | TFLOPs: 147.26 |\n```\n\nI think the 176B is not easily\
    \ possible with 16 A100 40 GB. For the 7B1 it should be enough, probably with\
    \ pipeline parallel = 4 & data parallel = 4 & tp = 1."
  created_at: 2022-11-07 08:54:25+00:00
  edited: false
  hidden: false
  id: 6368c7c1e943416fc806d623
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
      fullname: Khai Mai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: khaimaitien
      type: user
    createdAt: '2022-11-15T09:56:42.000Z'
    data:
      edited: true
      editors:
      - khaimaitien
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
          fullname: Khai Mai
          isHf: false
          isPro: false
          name: khaimaitien
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Can you tell me the number of GPUs with\
          \ memory that you used to fine-tune Bloomz and also the time for finetuning.\
          \ By the way, Can I finetune 176B model using 16 A100 40 GB without offloading\
          \ to CPU or Nvme with batch-size = 1 ?</p>\n</blockquote>\n<p>Sure, all\
          \ GPU numbers are in the model cards of each bloomz model, e.g. for the\
          \ big one: <a href=\"https://huggingface.co/bigscience/bloomz#hardware\"\
          >https://huggingface.co/bigscience/bloomz#hardware</a> (also some comments\
          \ <a href=\"https://huggingface.co/bigscience/bloomz/discussions/14#6366182ed0ee6e2662b16392\"\
          >here</a>).</p>\n<p>The time can be found in the <a href=\"https://huggingface.co/bigscience/bloomz/blob/main/logs/logs/xp3capmixnewcodelonglossseq/main_log.txt\"\
          >logs</a>. It will depend a lot on your hardware &amp; the model layout\
          \ though. For us, it took 142s / 4.2M tokens. So for the 2.1B tokens trained,\
          \ roughly 20 hours.</p>\n<pre><code>[default7]: iteration      409/    3100\
          \ | consumed samples:       837632 | consumed tokens:   1715470336 | elapsed\
          \ time per iteration (s): 140.78 | learning rate: 2.000E-05 | global batch\
          \ size:  2048 | lm loss: 1.117569E+00 | grad norm: 0.459 | num zeros: 0.0\
          \ | number of skipped iterations:   0 | number of nan iterations:   0 |\
          \ samples per second: 14.547 | TFLOPs: 148.50 \n[default7]: iteration  \
          \    410/    3100 | consumed samples:       839680 | consumed tokens:  \
          \ 1719664640 | elapsed time per iteration (s): 141.97 | learning rate: 2.000E-05\
          \ | global batch size:  2048 | lm loss: 1.119031E+00 | grad norm: 0.552\
          \ | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:\
          \   0 | samples per second: 14.425 | TFLOPs: 147.26 |\n</code></pre>\n<p>I\
          \ think the 176B is not easily possible with 16 A100 40 GB. For the 7B1\
          \ it should be enough, probably with pipeline parallel = 4 &amp; data parallel\
          \ = 4 &amp; tp = 1.</p>\n</blockquote>\n<p>Actually I do not have enough\
          \ computing resources to finetune the 176B model totally using GPU. In this\
          \ case, I think I should try Zero-offload or Zero-infinity. Have you released\
          \ the deep-speed checkpoints of Bloom models , I only see the HuggingFace\
          \ checkpoints not the original Deep Speed checkpoints of Bloom. I am trying\
          \ to modify  your code from <a rel=\"nofollow\" href=\"https://github.com/bigscience-workshop/xmtf\"\
          >https://github.com/bigscience-workshop/xmtf</a> to run Zero-offload and\
          \ Zero-infinity. By the way, I see from the training script that: \"# important:\
          \ bf16 must use z0! it implements its own zero stage 1 equivalent\" So I\
          \ we can not use zero-offload (in zero 2) and zero-infinity (in zero 3)\
          \ with BF16 right ?</p>\n"
        raw: "> > Can you tell me the number of GPUs with memory that you used to\
          \ fine-tune Bloomz and also the time for finetuning. By the way, Can I finetune\
          \ 176B model using 16 A100 40 GB without offloading to CPU or Nvme with\
          \ batch-size = 1 ?\n> \n> Sure, all GPU numbers are in the model cards of\
          \ each bloomz model, e.g. for the big one: https://huggingface.co/bigscience/bloomz#hardware\
          \ (also some comments [here](https://huggingface.co/bigscience/bloomz/discussions/14#6366182ed0ee6e2662b16392)).\n\
          > \n> The time can be found in the [logs](https://huggingface.co/bigscience/bloomz/blob/main/logs/logs/xp3capmixnewcodelonglossseq/main_log.txt).\
          \ It will depend a lot on your hardware & the model layout though. For us,\
          \ it took 142s / 4.2M tokens. So for the 2.1B tokens trained, roughly 20\
          \ hours.\n> \n> ```\n> [default7]: iteration      409/    3100 | consumed\
          \ samples:       837632 | consumed tokens:   1715470336 | elapsed time per\
          \ iteration (s): 140.78 | learning rate: 2.000E-05 | global batch size:\
          \  2048 | lm loss: 1.117569E+00 | grad norm: 0.459 | num zeros: 0.0 | number\
          \ of skipped iterations:   0 | number of nan iterations:   0 | samples per\
          \ second: 14.547 | TFLOPs: 148.50 \n> [default7]: iteration      410/  \
          \  3100 | consumed samples:       839680 | consumed tokens:   1719664640\
          \ | elapsed time per iteration (s): 141.97 | learning rate: 2.000E-05 |\
          \ global batch size:  2048 | lm loss: 1.119031E+00 | grad norm: 0.552 |\
          \ num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:\
          \   0 | samples per second: 14.425 | TFLOPs: 147.26 |\n> ```\n> \n> I think\
          \ the 176B is not easily possible with 16 A100 40 GB. For the 7B1 it should\
          \ be enough, probably with pipeline parallel = 4 & data parallel = 4 & tp\
          \ = 1.\n\nActually I do not have enough computing resources to finetune\
          \ the 176B model totally using GPU. In this case, I think I should try Zero-offload\
          \ or Zero-infinity. Have you released the deep-speed checkpoints of Bloom\
          \ models , I only see the HuggingFace checkpoints not the original Deep\
          \ Speed checkpoints of Bloom. I am trying to modify  your code from https://github.com/bigscience-workshop/xmtf\
          \ to run Zero-offload and Zero-infinity. By the way, I see from the training\
          \ script that: \"# important: bf16 must use z0! it implements its own zero\
          \ stage 1 equivalent\" So I we can not use zero-offload (in zero 2) and\
          \ zero-infinity (in zero 3) with BF16 right ?"
        updatedAt: '2022-11-15T09:59:23.777Z'
      numEdits: 1
      reactions: []
    id: 6373625ac3b104ef179a0d3d
    type: comment
  author: khaimaitien
  content: "> > Can you tell me the number of GPUs with memory that you used to fine-tune\
    \ Bloomz and also the time for finetuning. By the way, Can I finetune 176B model\
    \ using 16 A100 40 GB without offloading to CPU or Nvme with batch-size = 1 ?\n\
    > \n> Sure, all GPU numbers are in the model cards of each bloomz model, e.g.\
    \ for the big one: https://huggingface.co/bigscience/bloomz#hardware (also some\
    \ comments [here](https://huggingface.co/bigscience/bloomz/discussions/14#6366182ed0ee6e2662b16392)).\n\
    > \n> The time can be found in the [logs](https://huggingface.co/bigscience/bloomz/blob/main/logs/logs/xp3capmixnewcodelonglossseq/main_log.txt).\
    \ It will depend a lot on your hardware & the model layout though. For us, it\
    \ took 142s / 4.2M tokens. So for the 2.1B tokens trained, roughly 20 hours.\n\
    > \n> ```\n> [default7]: iteration      409/    3100 | consumed samples:     \
    \  837632 | consumed tokens:   1715470336 | elapsed time per iteration (s): 140.78\
    \ | learning rate: 2.000E-05 | global batch size:  2048 | lm loss: 1.117569E+00\
    \ | grad norm: 0.459 | num zeros: 0.0 | number of skipped iterations:   0 | number\
    \ of nan iterations:   0 | samples per second: 14.547 | TFLOPs: 148.50 \n> [default7]:\
    \ iteration      410/    3100 | consumed samples:       839680 | consumed tokens:\
    \   1719664640 | elapsed time per iteration (s): 141.97 | learning rate: 2.000E-05\
    \ | global batch size:  2048 | lm loss: 1.119031E+00 | grad norm: 0.552 | num\
    \ zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations: \
    \  0 | samples per second: 14.425 | TFLOPs: 147.26 |\n> ```\n> \n> I think the\
    \ 176B is not easily possible with 16 A100 40 GB. For the 7B1 it should be enough,\
    \ probably with pipeline parallel = 4 & data parallel = 4 & tp = 1.\n\nActually\
    \ I do not have enough computing resources to finetune the 176B model totally\
    \ using GPU. In this case, I think I should try Zero-offload or Zero-infinity.\
    \ Have you released the deep-speed checkpoints of Bloom models , I only see the\
    \ HuggingFace checkpoints not the original Deep Speed checkpoints of Bloom. I\
    \ am trying to modify  your code from https://github.com/bigscience-workshop/xmtf\
    \ to run Zero-offload and Zero-infinity. By the way, I see from the training script\
    \ that: \"# important: bf16 must use z0! it implements its own zero stage 1 equivalent\"\
    \ So I we can not use zero-offload (in zero 2) and zero-infinity (in zero 3) with\
    \ BF16 right ?"
  created_at: 2022-11-15 09:56:42+00:00
  edited: true
  hidden: false
  id: 6373625ac3b104ef179a0d3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-15T11:34:36.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<blockquote>

          <p>Actually I do not have enough computing resources to finetune the 176B
          model totally using GPU. In this case, I think I should try Zero-offload
          or Zero-infinity. Have you released the deep-speed checkpoints of Bloom
          models , I only see the HuggingFace checkpoints not the original Deep Speed
          checkpoints of Bloom. I am trying to modify your code from <a rel="nofollow"
          href="https://github.com/bigscience-workshop/xmtf">https://github.com/bigscience-workshop/xmtf</a>
          to run Zero-offload and Zero-infinity. By the way, I see from the training
          script that: "# important: bf16 must use z0! it implements its own zero
          stage 1 equivalent" So I we can not use zero-offload (in zero 2) and zero-infinity
          (in zero 3) with BF16 right ?</p>

          </blockquote>

          <p>All Megatron-DeepSpeed checkpoints should be uploaded,<br>e.g.</p>

          <ul>

          <li><a href="https://huggingface.co/bigscience/bloom-optimizer-states">https://huggingface.co/bigscience/bloom-optimizer-states</a></li>

          <li><a href="https://huggingface.co/bigscience/bloomz-optimizer-states">https://huggingface.co/bigscience/bloomz-optimizer-states</a></li>

          <li><a href="https://huggingface.co/bigscience/bloom-7b1-optimizer-states">https://huggingface.co/bigscience/bloom-7b1-optimizer-states</a><br>....</li>

          </ul>

          <p>RE: BF16 + Zero2 &amp; Zero3 best to open an issue on the DeepSpeed Repo:
          <a rel="nofollow" href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></p>

          '
        raw: '> Actually I do not have enough computing resources to finetune the
          176B model totally using GPU. In this case, I think I should try Zero-offload
          or Zero-infinity. Have you released the deep-speed checkpoints of Bloom
          models , I only see the HuggingFace checkpoints not the original Deep Speed
          checkpoints of Bloom. I am trying to modify your code from https://github.com/bigscience-workshop/xmtf
          to run Zero-offload and Zero-infinity. By the way, I see from the training
          script that: "# important: bf16 must use z0! it implements its own zero
          stage 1 equivalent" So I we can not use zero-offload (in zero 2) and zero-infinity
          (in zero 3) with BF16 right ?


          All Megatron-DeepSpeed checkpoints should be uploaded,

          e.g.

          - https://huggingface.co/bigscience/bloom-optimizer-states

          - https://huggingface.co/bigscience/bloomz-optimizer-states

          - https://huggingface.co/bigscience/bloom-7b1-optimizer-states

          ....



          RE: BF16 + Zero2 & Zero3 best to open an issue on the DeepSpeed Repo: https://github.com/microsoft/DeepSpeed'
        updatedAt: '2022-11-15T11:34:36.757Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - khaimaitien
    id: 6373794c3e75422985b6aae0
    type: comment
  author: Muennighoff
  content: '> Actually I do not have enough computing resources to finetune the 176B
    model totally using GPU. In this case, I think I should try Zero-offload or Zero-infinity.
    Have you released the deep-speed checkpoints of Bloom models , I only see the
    HuggingFace checkpoints not the original Deep Speed checkpoints of Bloom. I am
    trying to modify your code from https://github.com/bigscience-workshop/xmtf to
    run Zero-offload and Zero-infinity. By the way, I see from the training script
    that: "# important: bf16 must use z0! it implements its own zero stage 1 equivalent"
    So I we can not use zero-offload (in zero 2) and zero-infinity (in zero 3) with
    BF16 right ?


    All Megatron-DeepSpeed checkpoints should be uploaded,

    e.g.

    - https://huggingface.co/bigscience/bloom-optimizer-states

    - https://huggingface.co/bigscience/bloomz-optimizer-states

    - https://huggingface.co/bigscience/bloom-7b1-optimizer-states

    ....



    RE: BF16 + Zero2 & Zero3 best to open an issue on the DeepSpeed Repo: https://github.com/microsoft/DeepSpeed'
  created_at: 2022-11-15 11:34:36+00:00
  edited: false
  hidden: false
  id: 6373794c3e75422985b6aae0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
      fullname: Khai Mai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: khaimaitien
      type: user
    createdAt: '2022-11-15T15:12:38.000Z'
    data:
      edited: false
      editors:
      - khaimaitien
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
          fullname: Khai Mai
          isHf: false
          isPro: false
          name: khaimaitien
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Actually I do not have enough computing resources to finetune the 176B
          model totally using GPU. In this case, I think I should try Zero-offload
          or Zero-infinity. Have you released the deep-speed checkpoints of Bloom
          models , I only see the HuggingFace checkpoints not the original Deep Speed
          checkpoints of Bloom. I am trying to modify your code from <a rel="nofollow"
          href="https://github.com/bigscience-workshop/xmtf">https://github.com/bigscience-workshop/xmtf</a>
          to run Zero-offload and Zero-infinity. By the way, I see from the training
          script that: "# important: bf16 must use z0! it implements its own zero
          stage 1 equivalent" So I we can not use zero-offload (in zero 2) and zero-infinity
          (in zero 3) with BF16 right ?</p>

          </blockquote>

          <p>All Megatron-DeepSpeed checkpoints should be uploaded,<br>e.g.</p>

          <ul>

          <li><a href="https://huggingface.co/bigscience/bloom-optimizer-states">https://huggingface.co/bigscience/bloom-optimizer-states</a></li>

          <li><a href="https://huggingface.co/bigscience/bloomz-optimizer-states">https://huggingface.co/bigscience/bloomz-optimizer-states</a></li>

          <li><a href="https://huggingface.co/bigscience/bloom-7b1-optimizer-states">https://huggingface.co/bigscience/bloom-7b1-optimizer-states</a><br>....</li>

          </ul>

          <p>RE: BF16 + Zero2 &amp; Zero3 best to open an issue on the DeepSpeed Repo:
          <a rel="nofollow" href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></p>

          </blockquote>

          <p>Thank you so much for figuring out things for me ! By the way, I think
          the use of BF16 in pretraining Bloom model is to avoid being Nan in computation,
          however in the perspective of finetuning 176B model, had you tried using
          FP16 or you just used BF16 from start ?</p>

          '
        raw: "> > Actually I do not have enough computing resources to finetune the\
          \ 176B model totally using GPU. In this case, I think I should try Zero-offload\
          \ or Zero-infinity. Have you released the deep-speed checkpoints of Bloom\
          \ models , I only see the HuggingFace checkpoints not the original Deep\
          \ Speed checkpoints of Bloom. I am trying to modify your code from https://github.com/bigscience-workshop/xmtf\
          \ to run Zero-offload and Zero-infinity. By the way, I see from the training\
          \ script that: \"# important: bf16 must use z0! it implements its own zero\
          \ stage 1 equivalent\" So I we can not use zero-offload (in zero 2) and\
          \ zero-infinity (in zero 3) with BF16 right ?\n> \n> All Megatron-DeepSpeed\
          \ checkpoints should be uploaded,\n> e.g.\n> - https://huggingface.co/bigscience/bloom-optimizer-states\n\
          > - https://huggingface.co/bigscience/bloomz-optimizer-states\n> - https://huggingface.co/bigscience/bloom-7b1-optimizer-states\n\
          > ....\n> \n> \n> RE: BF16 + Zero2 & Zero3 best to open an issue on the\
          \ DeepSpeed Repo: https://github.com/microsoft/DeepSpeed\n\nThank you so\
          \ much for figuring out things for me ! By the way, I think the use of BF16\
          \ in pretraining Bloom model is to avoid being Nan in computation, however\
          \ in the perspective of finetuning 176B model, had you tried using FP16\
          \ or you just used BF16 from start ?"
        updatedAt: '2022-11-15T15:12:38.929Z'
      numEdits: 0
      reactions: []
    id: 6373ac66d398fce0dd48b8fa
    type: comment
  author: khaimaitien
  content: "> > Actually I do not have enough computing resources to finetune the\
    \ 176B model totally using GPU. In this case, I think I should try Zero-offload\
    \ or Zero-infinity. Have you released the deep-speed checkpoints of Bloom models\
    \ , I only see the HuggingFace checkpoints not the original Deep Speed checkpoints\
    \ of Bloom. I am trying to modify your code from https://github.com/bigscience-workshop/xmtf\
    \ to run Zero-offload and Zero-infinity. By the way, I see from the training script\
    \ that: \"# important: bf16 must use z0! it implements its own zero stage 1 equivalent\"\
    \ So I we can not use zero-offload (in zero 2) and zero-infinity (in zero 3) with\
    \ BF16 right ?\n> \n> All Megatron-DeepSpeed checkpoints should be uploaded,\n\
    > e.g.\n> - https://huggingface.co/bigscience/bloom-optimizer-states\n> - https://huggingface.co/bigscience/bloomz-optimizer-states\n\
    > - https://huggingface.co/bigscience/bloom-7b1-optimizer-states\n> ....\n> \n\
    > \n> RE: BF16 + Zero2 & Zero3 best to open an issue on the DeepSpeed Repo: https://github.com/microsoft/DeepSpeed\n\
    \nThank you so much for figuring out things for me ! By the way, I think the use\
    \ of BF16 in pretraining Bloom model is to avoid being Nan in computation, however\
    \ in the perspective of finetuning 176B model, had you tried using FP16 or you\
    \ just used BF16 from start ?"
  created_at: 2022-11-15 15:12:38+00:00
  edited: false
  hidden: false
  id: 6373ac66d398fce0dd48b8fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-15T17:50:31.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<blockquote>

          <p>Thank you so much for figuring out things for me ! By the way, I think
          the use of BF16 in pretraining Bloom model is to avoid being Nan in computation,
          however in the perspective of finetuning 176B model, had you tried using
          FP16 or you just used BF16 from start ?</p>

          </blockquote>

          <p>BF16 is used for pretraining of the 176B because it''s been shown to
          be more stable than FP16. We also wanted to use it for the smaller models
          but could not due to GPU constraints, thus they are pretrained in FP16.</p>

          <p>We also used BF16 for the 176B finetuning (&amp; FP16 for the small models).
          I think it''s not a good idea to switch BF16 -&gt; FP16 or FP16 -&gt; BF16,
          as the numeric ranges are different. You can always switch from FP16/BF16
          to FP32 though.</p>

          '
        raw: '> Thank you so much for figuring out things for me ! By the way, I think
          the use of BF16 in pretraining Bloom model is to avoid being Nan in computation,
          however in the perspective of finetuning 176B model, had you tried using
          FP16 or you just used BF16 from start ?


          BF16 is used for pretraining of the 176B because it''s been shown to be
          more stable than FP16. We also wanted to use it for the smaller models but
          could not due to GPU constraints, thus they are pretrained in FP16.


          We also used BF16 for the 176B finetuning (& FP16 for the small models).
          I think it''s not a good idea to switch BF16 -> FP16 or FP16 -> BF16, as
          the numeric ranges are different. You can always switch from FP16/BF16 to
          FP32 though.'
        updatedAt: '2022-11-15T17:50:31.420Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - khaimaitien
        - xnohat
    id: 6373d1672d4eccfa6f9288fc
    type: comment
  author: Muennighoff
  content: '> Thank you so much for figuring out things for me ! By the way, I think
    the use of BF16 in pretraining Bloom model is to avoid being Nan in computation,
    however in the perspective of finetuning 176B model, had you tried using FP16
    or you just used BF16 from start ?


    BF16 is used for pretraining of the 176B because it''s been shown to be more stable
    than FP16. We also wanted to use it for the smaller models but could not due to
    GPU constraints, thus they are pretrained in FP16.


    We also used BF16 for the 176B finetuning (& FP16 for the small models). I think
    it''s not a good idea to switch BF16 -> FP16 or FP16 -> BF16, as the numeric ranges
    are different. You can always switch from FP16/BF16 to FP32 though.'
  created_at: 2022-11-15 17:50:31+00:00
  edited: false
  hidden: false
  id: 6373d1672d4eccfa6f9288fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
      fullname: Khai Mai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: khaimaitien
      type: user
    createdAt: '2022-11-18T08:55:47.000Z'
    data:
      edited: false
      editors:
      - khaimaitien
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
          fullname: Khai Mai
          isHf: false
          isPro: false
          name: khaimaitien
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Thank you so much for figuring out things for me ! By the way, I think
          the use of BF16 in pretraining Bloom model is to avoid being Nan in computation,
          however in the perspective of finetuning 176B model, had you tried using
          FP16 or you just used BF16 from start ?</p>

          </blockquote>

          <p>BF16 is used for pretraining of the 176B because it''s been shown to
          be more stable than FP16. We also wanted to use it for the smaller models
          but could not due to GPU constraints, thus they are pretrained in FP16.</p>

          <p>We also used BF16 for the 176B finetuning (&amp; FP16 for the small models).
          I think it''s not a good idea to switch BF16 -&gt; FP16 or FP16 -&gt; BF16,
          as the numeric ranges are different. You can always switch from FP16/BF16
          to FP32 though.</p>

          </blockquote>

          <p>Thank you so much for your provided information. By the way, this question
          is not about finetuning but I think you are an expert in Bloom model so
          you may know this:  do you have or know about the conversion script or the
          technique behind quantizing Bloom model to int8, like: microsoft/bloom-deepspeed-inference-int8
          ?</p>

          '
        raw: "> > Thank you so much for figuring out things for me ! By the way, I\
          \ think the use of BF16 in pretraining Bloom model is to avoid being Nan\
          \ in computation, however in the perspective of finetuning 176B model, had\
          \ you tried using FP16 or you just used BF16 from start ?\n> \n> BF16 is\
          \ used for pretraining of the 176B because it's been shown to be more stable\
          \ than FP16. We also wanted to use it for the smaller models but could not\
          \ due to GPU constraints, thus they are pretrained in FP16.\n> \n> We also\
          \ used BF16 for the 176B finetuning (& FP16 for the small models). I think\
          \ it's not a good idea to switch BF16 -> FP16 or FP16 -> BF16, as the numeric\
          \ ranges are different. You can always switch from FP16/BF16 to FP32 though.\n\
          \nThank you so much for your provided information. By the way, this question\
          \ is not about finetuning but I think you are an expert in Bloom model so\
          \ you may know this:  do you have or know about the conversion script or\
          \ the technique behind quantizing Bloom model to int8, like: microsoft/bloom-deepspeed-inference-int8\
          \ ?"
        updatedAt: '2022-11-18T08:55:47.012Z'
      numEdits: 0
      reactions: []
    id: 63774893936772cc89e6f308
    type: comment
  author: khaimaitien
  content: "> > Thank you so much for figuring out things for me ! By the way, I think\
    \ the use of BF16 in pretraining Bloom model is to avoid being Nan in computation,\
    \ however in the perspective of finetuning 176B model, had you tried using FP16\
    \ or you just used BF16 from start ?\n> \n> BF16 is used for pretraining of the\
    \ 176B because it's been shown to be more stable than FP16. We also wanted to\
    \ use it for the smaller models but could not due to GPU constraints, thus they\
    \ are pretrained in FP16.\n> \n> We also used BF16 for the 176B finetuning (&\
    \ FP16 for the small models). I think it's not a good idea to switch BF16 -> FP16\
    \ or FP16 -> BF16, as the numeric ranges are different. You can always switch\
    \ from FP16/BF16 to FP32 though.\n\nThank you so much for your provided information.\
    \ By the way, this question is not about finetuning but I think you are an expert\
    \ in Bloom model so you may know this:  do you have or know about the conversion\
    \ script or the technique behind quantizing Bloom model to int8, like: microsoft/bloom-deepspeed-inference-int8\
    \ ?"
  created_at: 2022-11-18 08:55:47+00:00
  edited: false
  hidden: false
  id: 63774893936772cc89e6f308
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-11-18T10:29:23.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<blockquote>\n<p>Thank you so much for your provided information. By\
          \ the way, this question is not about finetuning but I think you are an\
          \ expert in Bloom model so you may know this:  do you have or know about\
          \ the conversion script or the technique behind quantizing Bloom model to\
          \ int8, like: microsoft/bloom-deepspeed-inference-int8 ?</p>\n</blockquote>\n\
          <p>I think it uses <a href=\"https://huggingface.co/blog/hf-bitsandbytes-integration\"\
          >bitsandbytes</a>, but <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ &amp; <span data-props=\"{&quot;user&quot;:&quot;stas&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/stas\">@<span class=\"\
          underline\">stas</span></a></span>\n\n\t</span></span> are much more knowledgeable\
          \ on that than I am, so they may know more \U0001F917</p>\n"
        raw: "> Thank you so much for your provided information. By the way, this\
          \ question is not about finetuning but I think you are an expert in Bloom\
          \ model so you may know this:  do you have or know about the conversion\
          \ script or the technique behind quantizing Bloom model to int8, like: microsoft/bloom-deepspeed-inference-int8\
          \ ?\n\nI think it uses [bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration),\
          \ but @ybelkada & @stas are much more knowledgeable on that than I am, so\
          \ they may know more \U0001F917"
        updatedAt: '2022-11-18T10:29:23.031Z'
      numEdits: 0
      reactions: []
    id: 63775e8306241efce1e73a33
    type: comment
  author: Muennighoff
  content: "> Thank you so much for your provided information. By the way, this question\
    \ is not about finetuning but I think you are an expert in Bloom model so you\
    \ may know this:  do you have or know about the conversion script or the technique\
    \ behind quantizing Bloom model to int8, like: microsoft/bloom-deepspeed-inference-int8\
    \ ?\n\nI think it uses [bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration),\
    \ but @ybelkada & @stas are much more knowledgeable on that than I am, so they\
    \ may know more \U0001F917"
  created_at: 2022-11-18 10:29:23+00:00
  edited: false
  hidden: false
  id: 63775e8306241efce1e73a33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-11-19T18:23:13.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;khaimaitien&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/khaimaitien\"\
          >@<span class=\"underline\">khaimaitien</span></a></span>\n\n\t</span></span>\
          \ , thanks for reaching out!<br>There are 2 ways to run bloom in int8, let\
          \ me explain you how you would do it using <code>bitsandbytes</code></p>\n\
          <pre><code># pip install bitsandbytes accelerate\nfrom transformers import\
          \ AutoTokenizer, BloomForConditionalGeneration\n\nPATH_TO_BLOOM = XXX\n\n\
          tokenizer = AutoTokenizer.from_pretrained(PATH_TO_BLOOM)\nmodel = BloomForConditionalGeneration.from_pretrained(PATH_TO_BLOOM,\
          \ device_map=\"auto\", load_in_8bit=True, torch_dtype=\"auto\")\n\ninput_text\
          \ = \"Hey my name is\"\ninput_ids = tokenizer(input_text, return_tensors=\"\
          pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n\
          </code></pre>\n<p>You will need at least ~180GB GPU RAM to make it run.\
          \ Let me know if you face into any issue by opening a new issue dedicated\
          \ for int8 inference!</p>\n"
        raw: '@khaimaitien , thanks for reaching out!

          There are 2 ways to run bloom in int8, let me explain you how you would
          do it using `bitsandbytes`


          ```

          # pip install bitsandbytes accelerate

          from transformers import AutoTokenizer, BloomForConditionalGeneration


          PATH_TO_BLOOM = XXX


          tokenizer = AutoTokenizer.from_pretrained(PATH_TO_BLOOM)

          model = BloomForConditionalGeneration.from_pretrained(PATH_TO_BLOOM, device_map="auto",
          load_in_8bit=True, torch_dtype="auto")


          input_text = "Hey my name is"

          input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")


          outputs = model.generate(input_ids)

          print(tokenizer.decode(outputs[0]))

          ```


          You will need at least ~180GB GPU RAM to make it run. Let me know if you
          face into any issue by opening a new issue dedicated for int8 inference!'
        updatedAt: '2022-11-19T18:23:13.941Z'
      numEdits: 0
      reactions: []
    id: 63791f117df2fefdcaf17773
    type: comment
  author: ybelkada
  content: '@khaimaitien , thanks for reaching out!

    There are 2 ways to run bloom in int8, let me explain you how you would do it
    using `bitsandbytes`


    ```

    # pip install bitsandbytes accelerate

    from transformers import AutoTokenizer, BloomForConditionalGeneration


    PATH_TO_BLOOM = XXX


    tokenizer = AutoTokenizer.from_pretrained(PATH_TO_BLOOM)

    model = BloomForConditionalGeneration.from_pretrained(PATH_TO_BLOOM, device_map="auto",
    load_in_8bit=True, torch_dtype="auto")


    input_text = "Hey my name is"

    input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")


    outputs = model.generate(input_ids)

    print(tokenizer.decode(outputs[0]))

    ```


    You will need at least ~180GB GPU RAM to make it run. Let me know if you face
    into any issue by opening a new issue dedicated for int8 inference!'
  created_at: 2022-11-19 18:23:13+00:00
  edited: false
  hidden: false
  id: 63791f117df2fefdcaf17773
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
      fullname: Khai Mai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: khaimaitien
      type: user
    createdAt: '2022-11-21T04:08:51.000Z'
    data:
      edited: false
      editors:
      - khaimaitien
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635baca137c6a2c12e2e2af7/LcL_RhngNv4gDD1oOmoUo.jpeg?w=200&h=200&f=face
          fullname: Khai Mai
          isHf: false
          isPro: false
          name: khaimaitien
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;khaimaitien&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/khaimaitien\"\
          >@<span class=\"underline\">khaimaitien</span></a></span>\n\n\t</span></span>\
          \ , thanks for reaching out!<br>There are 2 ways to run bloom in int8, let\
          \ me explain you how you would do it using <code>bitsandbytes</code></p>\n\
          <pre><code># pip install bitsandbytes accelerate\nfrom transformers import\
          \ AutoTokenizer, BloomForConditionalGeneration\n\nPATH_TO_BLOOM = XXX\n\n\
          tokenizer = AutoTokenizer.from_pretrained(PATH_TO_BLOOM)\nmodel = BloomForConditionalGeneration.from_pretrained(PATH_TO_BLOOM,\
          \ device_map=\"auto\", load_in_8bit=True, torch_dtype=\"auto\")\n\ninput_text\
          \ = \"Hey my name is\"\ninput_ids = tokenizer(input_text, return_tensors=\"\
          pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n\
          </code></pre>\n<p>You will need at least ~180GB GPU RAM to make it run.\
          \ Let me know if you face into any issue by opening a new issue dedicated\
          \ for int8 inference!</p>\n</blockquote>\n<p>Thank you for answering my\
          \ question. Actually I mean do you have the script or know how to convert\
          \ the model to int8 and save the weights then later we can use the int8\
          \ weight in loading the model. For example, I think that: microsoft/bloom-deepspeed-inference-int8\
          \  is converted from bigscience/bloom.</p>\n"
        raw: "> @khaimaitien , thanks for reaching out!\n> There are 2 ways to run\
          \ bloom in int8, let me explain you how you would do it using `bitsandbytes`\n\
          > \n> ```\n> # pip install bitsandbytes accelerate\n> from transformers\
          \ import AutoTokenizer, BloomForConditionalGeneration\n> \n> PATH_TO_BLOOM\
          \ = XXX\n> \n> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_BLOOM)\n\
          > model = BloomForConditionalGeneration.from_pretrained(PATH_TO_BLOOM, device_map=\"\
          auto\", load_in_8bit=True, torch_dtype=\"auto\")\n> \n> input_text = \"\
          Hey my name is\"\n> input_ids = tokenizer(input_text, return_tensors=\"\
          pt\").input_ids.to(\"cuda\")\n> \n> outputs = model.generate(input_ids)\n\
          > print(tokenizer.decode(outputs[0]))\n> ```\n> \n> You will need at least\
          \ ~180GB GPU RAM to make it run. Let me know if you face into any issue\
          \ by opening a new issue dedicated for int8 inference!\n\nThank you for\
          \ answering my question. Actually I mean do you have the script or know\
          \ how to convert the model to int8 and save the weights then later we can\
          \ use the int8 weight in loading the model. For example, I think that: microsoft/bloom-deepspeed-inference-int8\
          \  is converted from bigscience/bloom."
        updatedAt: '2022-11-21T04:08:51.005Z'
      numEdits: 0
      reactions: []
    id: 637af9d3806b18943e4b7460
    type: comment
  author: khaimaitien
  content: "> @khaimaitien , thanks for reaching out!\n> There are 2 ways to run bloom\
    \ in int8, let me explain you how you would do it using `bitsandbytes`\n> \n>\
    \ ```\n> # pip install bitsandbytes accelerate\n> from transformers import AutoTokenizer,\
    \ BloomForConditionalGeneration\n> \n> PATH_TO_BLOOM = XXX\n> \n> tokenizer =\
    \ AutoTokenizer.from_pretrained(PATH_TO_BLOOM)\n> model = BloomForConditionalGeneration.from_pretrained(PATH_TO_BLOOM,\
    \ device_map=\"auto\", load_in_8bit=True, torch_dtype=\"auto\")\n> \n> input_text\
    \ = \"Hey my name is\"\n> input_ids = tokenizer(input_text, return_tensors=\"\
    pt\").input_ids.to(\"cuda\")\n> \n> outputs = model.generate(input_ids)\n> print(tokenizer.decode(outputs[0]))\n\
    > ```\n> \n> You will need at least ~180GB GPU RAM to make it run. Let me know\
    \ if you face into any issue by opening a new issue dedicated for int8 inference!\n\
    \nThank you for answering my question. Actually I mean do you have the script\
    \ or know how to convert the model to int8 and save the weights then later we\
    \ can use the int8 weight in loading the model. For example, I think that: microsoft/bloom-deepspeed-inference-int8\
    \  is converted from bigscience/bloom."
  created_at: 2022-11-21 04:08:51+00:00
  edited: false
  hidden: false
  id: 637af9d3806b18943e4b7460
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-12-02T04:16:12.000Z'
    data:
      edited: true
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: "<p>Hey folks - I just delivered my Capstone presentation for a M.S.\
          \ in Health Data Science program, which was the reason I was exploring BLOOM\
          \ as a QA system. Two potential things of interest:</p>\n<ol>\n<li>I uploaded\
          \ a model tuned on Covid 19 data from the CDC, 100 epochs over a set of\
          \ 10k QA pairs: <a href=\"https://huggingface.co/jasoneden/BLOOM-560-QA-CDC_Covid19-100epochs\"\
          >https://huggingface.co/jasoneden/BLOOM-560-QA-CDC_Covid19-100epochs</a></li>\n\
          </ol>\n<p>In the files section, I have posted the qa train and dev pairs\
          \ I used to build and evaluate the model performance.</p>\n<ol start=\"\
          2\">\n<li>If you want to follow the code journey (which is, full disclosure,\
          \ a spaghetti mess, but all of the basics are in there) I've uploaded everything\
          \ to github: <a rel=\"nofollow\" href=\"https://github.com/jasondeden/capstone\"\
          >https://github.com/jasondeden/capstone</a></li>\n</ol>\n<p>Of particular\
          \ note in the code is the ETL required to take output from Haystack QA production\
          \ and convert it into SQuAD data format, but there's a lot of work that\
          \ went in there that I am happy to share and see others improve on (shouldn't\
          \ be too hard...) :) </p>\n<p>Thanks to <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ x 100,000 for the major assistance. I'm not sure I could have done it\
          \ without your help / intervention.</p>\n"
        raw: "Hey folks - I just delivered my Capstone presentation for a M.S. in\
          \ Health Data Science program, which was the reason I was exploring BLOOM\
          \ as a QA system. Two potential things of interest:\n\n1) I uploaded a model\
          \ tuned on Covid 19 data from the CDC, 100 epochs over a set of 10k QA pairs:\
          \ https://huggingface.co/jasoneden/BLOOM-560-QA-CDC_Covid19-100epochs \n\
          \nIn the files section, I have posted the qa train and dev pairs I used\
          \ to build and evaluate the model performance.\n\n2) If you want to follow\
          \ the code journey (which is, full disclosure, a spaghetti mess, but all\
          \ of the basics are in there) I've uploaded everything to github: https://github.com/jasondeden/capstone\
          \ \n\nOf particular note in the code is the ETL required to take output\
          \ from Haystack QA production and convert it into SQuAD data format, but\
          \ there's a lot of work that went in there that I am happy to share and\
          \ see others improve on (shouldn't be too hard...) :) \n\nThanks to @ybelkada\
          \ x 100,000 for the major assistance. I'm not sure I could have done it\
          \ without your help / intervention."
        updatedAt: '2022-12-02T04:16:54.626Z'
      numEdits: 1
      reactions:
      - count: 5
        reaction: "\u2764\uFE0F"
        users:
        - Muennighoff
        - cakiki
        - Charm3link
        - ybelkada
        - xin1111
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ybelkada
      - count: 1
        reaction: "\U0001F917"
        users:
        - duongkstn
    id: 63897c0cc0b95b2c293bc66e
    type: comment
  author: jasoneden
  content: "Hey folks - I just delivered my Capstone presentation for a M.S. in Health\
    \ Data Science program, which was the reason I was exploring BLOOM as a QA system.\
    \ Two potential things of interest:\n\n1) I uploaded a model tuned on Covid 19\
    \ data from the CDC, 100 epochs over a set of 10k QA pairs: https://huggingface.co/jasoneden/BLOOM-560-QA-CDC_Covid19-100epochs\
    \ \n\nIn the files section, I have posted the qa train and dev pairs I used to\
    \ build and evaluate the model performance.\n\n2) If you want to follow the code\
    \ journey (which is, full disclosure, a spaghetti mess, but all of the basics\
    \ are in there) I've uploaded everything to github: https://github.com/jasondeden/capstone\
    \ \n\nOf particular note in the code is the ETL required to take output from Haystack\
    \ QA production and convert it into SQuAD data format, but there's a lot of work\
    \ that went in there that I am happy to share and see others improve on (shouldn't\
    \ be too hard...) :) \n\nThanks to @ybelkada x 100,000 for the major assistance.\
    \ I'm not sure I could have done it without your help / intervention."
  created_at: 2022-12-02 04:16:12+00:00
  edited: true
  hidden: false
  id: 63897c0cc0b95b2c293bc66e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d2cf5e409d0a71be276fbbfccdcc2ad.svg
      fullname: Rob Boswell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rbos
      type: user
    createdAt: '2022-12-07T23:02:06.000Z'
    data:
      edited: true
      editors:
      - rbos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d2cf5e409d0a71be276fbbfccdcc2ad.svg
          fullname: Rob Boswell
          isHf: false
          isPro: false
          name: rbos
          type: user
        html: "<blockquote>\n<p>I have uploaded the model (correctly, I think... please\
          \ advise if not) for anyone that wants to give it a whirl:</p>\n<p><a href=\"\
          https://huggingface.co/jasoneden/bloom560m-squad-helloworld/tree/main\"\
          >https://huggingface.co/jasoneden/bloom560m-squad-helloworld/tree/main</a>\
          \ </p>\n<p>Once I figure out how to upload from from my Vertex AI Notebook,\
          \ I will upload all of the checkpoint folders as well.</p>\n</blockquote>\n\
          <p>Hi <span data-props=\"{&quot;user&quot;:&quot;jasoneden&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jasoneden\">@<span class=\"\
          underline\">jasoneden</span></a></span>\n\n\t</span></span>, I'm fairly\
          \ new to all this, but just tried out the Question Answering fine-tuned\
          \ version of BLOOM-560M. For some reason, when I ask questions in English,\
          \ it provides answers in Tamil. Is it normal for it to do this? If not,\
          \ do you know what I might be able to do to ensure the answers come out\
          \ in English instead?</p>\n"
        raw: "> I have uploaded the model (correctly, I think... please advise if\
          \ not) for anyone that wants to give it a whirl:\n> \n> https://huggingface.co/jasoneden/bloom560m-squad-helloworld/tree/main\
          \ \n> \n> Once I figure out how to upload from from my Vertex AI Notebook,\
          \ I will upload all of the checkpoint folders as well.\n\nHi @jasoneden,\
          \ I'm fairly new to all this, but just tried out the Question Answering\
          \ fine-tuned version of BLOOM-560M. For some reason, when I ask questions\
          \ in English, it provides answers in Tamil. Is it normal for it to do this?\
          \ If not, do you know what I might be able to do to ensure the answers come\
          \ out in English instead?"
        updatedAt: '2022-12-07T23:02:54.604Z'
      numEdits: 1
      reactions: []
    id: 63911b6e2e1b430e96cad0bf
    type: comment
  author: rbos
  content: "> I have uploaded the model (correctly, I think... please advise if not)\
    \ for anyone that wants to give it a whirl:\n> \n> https://huggingface.co/jasoneden/bloom560m-squad-helloworld/tree/main\
    \ \n> \n> Once I figure out how to upload from from my Vertex AI Notebook, I will\
    \ upload all of the checkpoint folders as well.\n\nHi @jasoneden, I'm fairly new\
    \ to all this, but just tried out the Question Answering fine-tuned version of\
    \ BLOOM-560M. For some reason, when I ask questions in English, it provides answers\
    \ in Tamil. Is it normal for it to do this? If not, do you know what I might be\
    \ able to do to ensure the answers come out in English instead?"
  created_at: 2022-12-07 23:02:06+00:00
  edited: true
  hidden: false
  id: 63911b6e2e1b430e96cad0bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-12-08T18:53:28.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rbos&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rbos\">@<span class=\"\
          underline\">rbos</span></a></span>\n\n\t</span></span>  - I honestly have\
          \ no idea how you're getting it to do that. As far as I know, the model\
          \ should pull something out of the context you provide and supply that as\
          \ an answer, so if you're supplying an English context and you're getting\
          \ Tamil back, you've found something I'd be very interested to figure out\
          \ how it happened. Can you provide a screenshot?</p>\n"
        raw: '@rbos  - I honestly have no idea how you''re getting it to do that.
          As far as I know, the model should pull something out of the context you
          provide and supply that as an answer, so if you''re supplying an English
          context and you''re getting Tamil back, you''ve found something I''d be
          very interested to figure out how it happened. Can you provide a screenshot?'
        updatedAt: '2022-12-08T18:53:28.994Z'
      numEdits: 0
      reactions: []
    id: 639232a873bf5173e36370b3
    type: comment
  author: jasoneden
  content: '@rbos  - I honestly have no idea how you''re getting it to do that. As
    far as I know, the model should pull something out of the context you provide
    and supply that as an answer, so if you''re supplying an English context and you''re
    getting Tamil back, you''ve found something I''d be very interested to figure
    out how it happened. Can you provide a screenshot?'
  created_at: 2022-12-08 18:53:28+00:00
  edited: false
  hidden: false
  id: 639232a873bf5173e36370b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-12-08T22:07:02.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Thanks so much <span data-props=\"{&quot;user&quot;:&quot;jasoneden&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jasoneden\"\
          >@<span class=\"underline\">jasoneden</span></a></span>\n\n\t</span></span>\
          \ for your message, very happy that I was able to help here ;) And congratulations\
          \ for your presentation !! Looking forward to your next contributions</p>\n"
        raw: Thanks so much @jasoneden for your message, very happy that I was able
          to help here ;) And congratulations for your presentation !! Looking forward
          to your next contributions
        updatedAt: '2022-12-08T22:07:02.833Z'
      numEdits: 0
      reactions: []
    id: 63926006526c29d5b500eeff
    type: comment
  author: ybelkada
  content: Thanks so much @jasoneden for your message, very happy that I was able
    to help here ;) And congratulations for your presentation !! Looking forward to
    your next contributions
  created_at: 2022-12-08 22:07:02+00:00
  edited: false
  hidden: false
  id: 63926006526c29d5b500eeff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-12-08T22:08:52.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Out of curiosity, is there any recording of the presentation?</p>

          '
        raw: Out of curiosity, is there any recording of the presentation?
        updatedAt: '2022-12-08T22:08:52.000Z'
      numEdits: 0
      reactions: []
    id: 63926074d906dec271e0b400
    type: comment
  author: ybelkada
  content: Out of curiosity, is there any recording of the presentation?
  created_at: 2022-12-08 22:08:52+00:00
  edited: false
  hidden: false
  id: 63926074d906dec271e0b400
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
      fullname: Alexander Borzunov
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: borzunov
      type: user
    createdAt: '2022-12-09T17:09:55.000Z'
    data:
      edited: true
      editors:
      - borzunov
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
          fullname: Alexander Borzunov
          isHf: false
          isPro: false
          name: borzunov
          type: user
        html: '<p>Hi everyone!</p>

          <p>We''ve made a proof-of-concept of decentralized BLOOM-176B fine-tuning
          and inference from Google Colab. If you''re interested in doing that without
          a GPU cluster, please take a look in this thread: <a href="https://huggingface.co/bigscience/bloom/discussions/152">https://huggingface.co/bigscience/bloom/discussions/152</a></p>

          '
        raw: 'Hi everyone!


          We''ve made a proof-of-concept of decentralized BLOOM-176B fine-tuning and
          inference from Google Colab. If you''re interested in doing that without
          a GPU cluster, please take a look in this thread: https://huggingface.co/bigscience/bloom/discussions/152'
        updatedAt: '2022-12-09T17:26:16.729Z'
      numEdits: 3
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - jasoneden
        - MxKnox
    id: 63936be3b045a31cceedbb54
    type: comment
  author: borzunov
  content: 'Hi everyone!


    We''ve made a proof-of-concept of decentralized BLOOM-176B fine-tuning and inference
    from Google Colab. If you''re interested in doing that without a GPU cluster,
    please take a look in this thread: https://huggingface.co/bigscience/bloom/discussions/152'
  created_at: 2022-12-09 17:09:55+00:00
  edited: true
  hidden: false
  id: 63936be3b045a31cceedbb54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-12-09T19:05:45.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span>  - The presentation\
          \ was not recorded. </p>\n<p>@burzunov - Wow, that is <strong>very</strong>\
          \ cool!</p>\n"
        raw: "@ybelkada  - The presentation was not recorded. \n\n@burzunov - Wow,\
          \ that is **very** cool!"
        updatedAt: '2022-12-09T19:05:45.599Z'
      numEdits: 0
      reactions: []
    id: 6393870999bf5667b7f7a27c
    type: comment
  author: jasoneden
  content: "@ybelkada  - The presentation was not recorded. \n\n@burzunov - Wow, that\
    \ is **very** cool!"
  created_at: 2022-12-09 19:05:45+00:00
  edited: false
  hidden: false
  id: 6393870999bf5667b7f7a27c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d2cf5e409d0a71be276fbbfccdcc2ad.svg
      fullname: Rob Boswell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rbos
      type: user
    createdAt: '2022-12-09T21:34:35.000Z'
    data:
      edited: false
      editors:
      - rbos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d2cf5e409d0a71be276fbbfccdcc2ad.svg
          fullname: Rob Boswell
          isHf: false
          isPro: false
          name: rbos
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;rbos&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rbos\"\
          >@<span class=\"underline\">rbos</span></a></span>\n\n\t</span></span> \
          \ - I honestly have no idea how you're getting it to do that. As far as\
          \ I know, the model should pull something out of the context you provide\
          \ and supply that as an answer, so if you're supplying an English context\
          \ and you're getting Tamil back, you've found something I'd be very interested\
          \ to figure out how it happened. Can you provide a screenshot?</p>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;jasoneden&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jasoneden\">@<span class=\"\
          underline\">jasoneden</span></a></span>\n\n\t</span></span> Yeah, this is\
          \ the output I am getting. I noticed that the Tamil word it gives me means\
          \ \"Early\" in English, and it keeps it repeating it. Even when I have change\
          \ my question, it still gives me the same exact output word in Tamil (again):</p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1670621647086-637b0cf414076b808c6c308f.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1670621647086-637b0cf414076b808c6c308f.png\"\
          ></a></p>\n"
        raw: '> @rbos  - I honestly have no idea how you''re getting it to do that.
          As far as I know, the model should pull something out of the context you
          provide and supply that as an answer, so if you''re supplying an English
          context and you''re getting Tamil back, you''ve found something I''d be
          very interested to figure out how it happened. Can you provide a screenshot?


          @jasoneden Yeah, this is the output I am getting. I noticed that the Tamil
          word it gives me means "Early" in English, and it keeps it repeating it.
          Even when I have change my question, it still gives me the same exact output
          word in Tamil (again):



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1670621647086-637b0cf414076b808c6c308f.png)'
        updatedAt: '2022-12-09T21:34:35.408Z'
      numEdits: 0
      reactions: []
    id: 6393a9ebfa52523d09ec4335
    type: comment
  author: rbos
  content: '> @rbos  - I honestly have no idea how you''re getting it to do that.
    As far as I know, the model should pull something out of the context you provide
    and supply that as an answer, so if you''re supplying an English context and you''re
    getting Tamil back, you''ve found something I''d be very interested to figure
    out how it happened. Can you provide a screenshot?


    @jasoneden Yeah, this is the output I am getting. I noticed that the Tamil word
    it gives me means "Early" in English, and it keeps it repeating it. Even when
    I have change my question, it still gives me the same exact output word in Tamil
    (again):



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1670621647086-637b0cf414076b808c6c308f.png)'
  created_at: 2022-12-09 21:34:35+00:00
  edited: false
  hidden: false
  id: 6393a9ebfa52523d09ec4335
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-12-09T22:22:24.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;jasoneden&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jasoneden\"\
          >@<span class=\"underline\">jasoneden</span></a></span>\n\n\t</span></span>\
          \ Yeah, this is the output I am getting. I noticed that the Tamil word it\
          \ gives me means \"Early\" in English, and it keeps it repeating it. Even\
          \ when I have change my question, it still gives me the same exact output\
          \ word in Tamil (again):</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1670621647086-637b0cf414076b808c6c308f.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1670621647086-637b0cf414076b808c6c308f.png\"\
          ></a></p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;rbos&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rbos\"\
          >@<span class=\"underline\">rbos</span></a></span>\n\n\t</span></span> -\
          \ Ah, I think I know what the problem is. You're trying to use the model\
          \ as a generative, when it has been trained for extractive QA. You actually\
          \ need to supply a context for it to pull the answer from, rather than just\
          \ asking it a question. </p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1670624448443-630cdf8e3dc31beba6ed4c9d.png\"\
          ><img alt=\"Screenshot 2022-12-09 at 4.20.40 PM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1670624448443-630cdf8e3dc31beba6ed4c9d.png\"\
          ></a></p>\n<p>Here's an example of the type of code the model is expecting:</p>\n\
          <p><a rel=\"nofollow\" href=\"https://github.com/jasondeden/capstone/blob/main/model-train-and-final-ETL/QnATests.ipynb\"\
          >https://github.com/jasondeden/capstone/blob/main/model-train-and-final-ETL/QnATests.ipynb</a></p>\n"
        raw: "> @jasoneden Yeah, this is the output I am getting. I noticed that the\
          \ Tamil word it gives me means \"Early\" in English, and it keeps it repeating\
          \ it. Even when I have change my question, it still gives me the same exact\
          \ output word in Tamil (again):\n> \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1670621647086-637b0cf414076b808c6c308f.png)\n\
          \n@rbos - Ah, I think I know what the problem is. You're trying to use the\
          \ model as a generative, when it has been trained for extractive QA. You\
          \ actually need to supply a context for it to pull the answer from, rather\
          \ than just asking it a question. \n\n![Screenshot 2022-12-09 at 4.20.40\
          \ PM.png](https://cdn-uploads.huggingface.co/production/uploads/1670624448443-630cdf8e3dc31beba6ed4c9d.png)\n\
          \nHere's an example of the type of code the model is expecting:\n\nhttps://github.com/jasondeden/capstone/blob/main/model-train-and-final-ETL/QnATests.ipynb"
        updatedAt: '2022-12-09T22:22:24.749Z'
      numEdits: 0
      reactions: []
    id: 6393b52096b5b993798b3ef5
    type: comment
  author: jasoneden
  content: "> @jasoneden Yeah, this is the output I am getting. I noticed that the\
    \ Tamil word it gives me means \"Early\" in English, and it keeps it repeating\
    \ it. Even when I have change my question, it still gives me the same exact output\
    \ word in Tamil (again):\n> \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1670621647086-637b0cf414076b808c6c308f.png)\n\
    \n@rbos - Ah, I think I know what the problem is. You're trying to use the model\
    \ as a generative, when it has been trained for extractive QA. You actually need\
    \ to supply a context for it to pull the answer from, rather than just asking\
    \ it a question. \n\n![Screenshot 2022-12-09 at 4.20.40 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1670624448443-630cdf8e3dc31beba6ed4c9d.png)\n\
    \nHere's an example of the type of code the model is expecting:\n\nhttps://github.com/jasondeden/capstone/blob/main/model-train-and-final-ETL/QnATests.ipynb"
  created_at: 2022-12-09 22:22:24+00:00
  edited: false
  hidden: false
  id: 6393b52096b5b993798b3ef5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-12-09T23:33:30.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rbos&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rbos\">@<span class=\"\
          underline\">rbos</span></a></span>\n\n\t</span></span> try removing trailing\
          \ space in your prompt.</p>\n"
        raw: '@rbos try removing trailing space in your prompt.'
        updatedAt: '2022-12-09T23:33:30.478Z'
      numEdits: 0
      reactions: []
    id: 6393c5cac227661ad977d8ae
    type: comment
  author: TimeRobber
  content: '@rbos try removing trailing space in your prompt.'
  created_at: 2022-12-09 23:33:30+00:00
  edited: false
  hidden: false
  id: 6393c5cac227661ad977d8ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d2cf5e409d0a71be276fbbfccdcc2ad.svg
      fullname: Rob Boswell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rbos
      type: user
    createdAt: '2022-12-10T00:13:41.000Z'
    data:
      edited: true
      editors:
      - rbos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d2cf5e409d0a71be276fbbfccdcc2ad.svg
          fullname: Rob Boswell
          isHf: false
          isPro: false
          name: rbos
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jasoneden&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jasoneden\">@<span class=\"\
          underline\">jasoneden</span></a></span>\n\n\t</span></span> Thanks for the\
          \ explanation and code to look at. Being new to this, what I had originally\
          \ thought was that fine-tuning the BLOOM model would allow someone to use\
          \ the usual generative format to ask questions and obtain answers that would\
          \ still come from the original training set that BLOOM was trained on, but\
          \ with an enhanced ability to perform better specifically on the task of\
          \ Question Answering compared to other types of tasks. Is your goal eventually\
          \ to tweak the code to what I've just mentioned in a generative model approach,\
          \ or is it to extract information from a context sentence/paragraph provided\
          \ to the model?</p>\n<p>My interest in fine-tuning for the Question Answering\
          \ task actually is based on the results in the BLOOM paper, \"BLOOM: A 176B-Parameter\
          \ Open-Access Multilingual Language Model\", where it showed that BLOOMZ\
          \ had better zero-shot performance on several different types of (I think\
          \ non-Question-Answering) tasks because it had been fine-tuned. The same\
          \ thing occurred with DeepMind's Flamingo few-shot learning vision-language\
          \ model when it was fine-tuned (Alayrac et al., 2022): better performance\
          \ for the fine-tuned model over the regular few-shot generative model.</p>\n\
          <p>So, perhaps I misunderstand the goal of fine-tuning a generative model\
          \ (and/or the task of Question Answering itself), but can the BLOOM model\
          \ be fine-tuned and still be used as a generative model - just one that\
          \ performs better on the fine-tuned task?</p>\n"
        raw: '@jasoneden Thanks for the explanation and code to look at. Being new
          to this, what I had originally thought was that fine-tuning the BLOOM model
          would allow someone to use the usual generative format to ask questions
          and obtain answers that would still come from the original training set
          that BLOOM was trained on, but with an enhanced ability to perform better
          specifically on the task of Question Answering compared to other types of
          tasks. Is your goal eventually to tweak the code to what I''ve just mentioned
          in a generative model approach, or is it to extract information from a context
          sentence/paragraph provided to the model?


          My interest in fine-tuning for the Question Answering task actually is based
          on the results in the BLOOM paper, "BLOOM: A 176B-Parameter Open-Access
          Multilingual Language Model", where it showed that BLOOMZ had better zero-shot
          performance on several different types of (I think non-Question-Answering)
          tasks because it had been fine-tuned. The same thing occurred with DeepMind''s
          Flamingo few-shot learning vision-language model when it was fine-tuned
          (Alayrac et al., 2022): better performance for the fine-tuned model over
          the regular few-shot generative model.


          So, perhaps I misunderstand the goal of fine-tuning a generative model (and/or
          the task of Question Answering itself), but can the BLOOM model be fine-tuned
          and still be used as a generative model - just one that performs better
          on the fine-tuned task?'
        updatedAt: '2022-12-10T00:22:42.044Z'
      numEdits: 2
      reactions: []
    id: 6393cf355a266213c52913f9
    type: comment
  author: rbos
  content: '@jasoneden Thanks for the explanation and code to look at. Being new to
    this, what I had originally thought was that fine-tuning the BLOOM model would
    allow someone to use the usual generative format to ask questions and obtain answers
    that would still come from the original training set that BLOOM was trained on,
    but with an enhanced ability to perform better specifically on the task of Question
    Answering compared to other types of tasks. Is your goal eventually to tweak the
    code to what I''ve just mentioned in a generative model approach, or is it to
    extract information from a context sentence/paragraph provided to the model?


    My interest in fine-tuning for the Question Answering task actually is based on
    the results in the BLOOM paper, "BLOOM: A 176B-Parameter Open-Access Multilingual
    Language Model", where it showed that BLOOMZ had better zero-shot performance
    on several different types of (I think non-Question-Answering) tasks because it
    had been fine-tuned. The same thing occurred with DeepMind''s Flamingo few-shot
    learning vision-language model when it was fine-tuned (Alayrac et al., 2022):
    better performance for the fine-tuned model over the regular few-shot generative
    model.


    So, perhaps I misunderstand the goal of fine-tuning a generative model (and/or
    the task of Question Answering itself), but can the BLOOM model be fine-tuned
    and still be used as a generative model - just one that performs better on the
    fine-tuned task?'
  created_at: 2022-12-10 00:13:41+00:00
  edited: true
  hidden: false
  id: 6393cf355a266213c52913f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d2cf5e409d0a71be276fbbfccdcc2ad.svg
      fullname: Rob Boswell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rbos
      type: user
    createdAt: '2022-12-10T00:19:04.000Z'
    data:
      edited: false
      editors:
      - rbos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d2cf5e409d0a71be276fbbfccdcc2ad.svg
          fullname: Rob Boswell
          isHf: false
          isPro: false
          name: rbos
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;rbos&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rbos\"\
          >@<span class=\"underline\">rbos</span></a></span>\n\n\t</span></span> try\
          \ removing trailing space in your prompt.</p>\n</blockquote>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;TimeRobber&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/TimeRobber\">@<span class=\"underline\"\
          >TimeRobber</span></a></span>\n\n\t</span></span> . I tried that too - the\
          \ same problem occurs, unfortunately.</p>\n"
        raw: '> @rbos try removing trailing space in your prompt.


          @TimeRobber . I tried that too - the same problem occurs, unfortunately.'
        updatedAt: '2022-12-10T00:19:04.724Z'
      numEdits: 0
      reactions: []
    id: 6393d078ea0f303ef05566ed
    type: comment
  author: rbos
  content: '> @rbos try removing trailing space in your prompt.


    @TimeRobber . I tried that too - the same problem occurs, unfortunately.'
  created_at: 2022-12-10 00:19:04+00:00
  edited: false
  hidden: false
  id: 6393d078ea0f303ef05566ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2022-12-10T00:37:17.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rbos&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rbos\">@<span class=\"\
          underline\">rbos</span></a></span>\n\n\t</span></span> - From a generative\
          \ QA standpoint, you could tune the standard BLOOM model to about any text\
          \ dataset and provide it prompts like you were trying, and it would come\
          \ up with an answer (probably in the same language you asked it in... lol...\
          \ unless you were specifically asking it to do a translation.) However,\
          \ my project was specific to QA, and specifically related to BLOOM as a\
          \ SQuAD-type modeling approach. If you have a general generative model,\
          \ you could write a program (or use a BERT model, etc.) to turn a question\
          \ into a prompt, feed that prompt to BLOOM, and have it finish the sentence.\
          \ I was trying to see what happens if you take a generative model and try\
          \ to turn it into a specific-use QA tool. Thus, the extractive QA head,\
          \ need for context, etc.</p>\n<p>In my case, if you read through my research\
          \ paper, you'll find out BLOOM performs pretty poorly in this task relative\
          \ to Encoders - which, if you think about it, makes sense. You're asking\
          \ a generative model - something which is designed to \"know the right answer\"\
          \ to do a task it is not built for - i.e. \"find an answer.\" So the BLOOM\
          \ QA-specific model on SQuAD and similar (I trained a model on CDC data\
          \ as well) just doesn't really know what to do. Sometimes it's right, but\
          \ it doesn't know how to be sure that it is, and sometimes it's very, very\
          \ wrong. But in all cases, the extractive QA approach requires the provision\
          \ of context.</p>\n<p>If I were to start over and try to build a generative\
          \ QA model, I wouldn't use the SQuAD approach, and instead would simply\
          \ train standard BLOOM on a set of data files. I would then figure out how\
          \ to write a front-end that does what I described above: takes a question,\
          \ reformats it into a prompt (maybe using a BERT variant?), feed the prompt\
          \ to the custom-trained BLOOM model, and then see how well it did in responding.\
          \ In this context, things like \"exact match\" or EM scores - one of the\
          \ key metrics used to evaluate for SQuAD - doesn't really make sense in\
          \ context, because you might say the same thing three different ways and\
          \ you'd still be right each time. Then again, for a commercial application,\
          \ you're back to the significant risk inherent in generative models: getting\
          \ back an answer that would potentially get a human being fired. :)</p>\n\
          <p>I hope that helps / makes sense. In the end, my efforts to train a BLOOM\
          \ model for QA were a failure in the sense of \"I built a better mousetrap\"\
          \ but a success in the sense of \"that's one more way not to invent a lightbulb.\"\
          \ Looking forward to seeing what others have done and how that moves the\
          \ ball forward for BLOOM!</p>\n"
        raw: '@rbos - From a generative QA standpoint, you could tune the standard
          BLOOM model to about any text dataset and provide it prompts like you were
          trying, and it would come up with an answer (probably in the same language
          you asked it in... lol... unless you were specifically asking it to do a
          translation.) However, my project was specific to QA, and specifically related
          to BLOOM as a SQuAD-type modeling approach. If you have a general generative
          model, you could write a program (or use a BERT model, etc.) to turn a question
          into a prompt, feed that prompt to BLOOM, and have it finish the sentence.
          I was trying to see what happens if you take a generative model and try
          to turn it into a specific-use QA tool. Thus, the extractive QA head, need
          for context, etc.


          In my case, if you read through my research paper, you''ll find out BLOOM
          performs pretty poorly in this task relative to Encoders - which, if you
          think about it, makes sense. You''re asking a generative model - something
          which is designed to "know the right answer" to do a task it is not built
          for - i.e. "find an answer." So the BLOOM QA-specific model on SQuAD and
          similar (I trained a model on CDC data as well) just doesn''t really know
          what to do. Sometimes it''s right, but it doesn''t know how to be sure that
          it is, and sometimes it''s very, very wrong. But in all cases, the extractive
          QA approach requires the provision of context.


          If I were to start over and try to build a generative QA model, I wouldn''t
          use the SQuAD approach, and instead would simply train standard BLOOM on
          a set of data files. I would then figure out how to write a front-end that
          does what I described above: takes a question, reformats it into a prompt
          (maybe using a BERT variant?), feed the prompt to the custom-trained BLOOM
          model, and then see how well it did in responding. In this context, things
          like "exact match" or EM scores - one of the key metrics used to evaluate
          for SQuAD - doesn''t really make sense in context, because you might say
          the same thing three different ways and you''d still be right each time.
          Then again, for a commercial application, you''re back to the significant
          risk inherent in generative models: getting back an answer that would potentially
          get a human being fired. :)


          I hope that helps / makes sense. In the end, my efforts to train a BLOOM
          model for QA were a failure in the sense of "I built a better mousetrap"
          but a success in the sense of "that''s one more way not to invent a lightbulb."
          Looking forward to seeing what others have done and how that moves the ball
          forward for BLOOM!'
        updatedAt: '2022-12-10T00:37:17.748Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ArthurBaia
    id: 6393d4bd98461466b9e40115
    type: comment
  author: jasoneden
  content: '@rbos - From a generative QA standpoint, you could tune the standard BLOOM
    model to about any text dataset and provide it prompts like you were trying, and
    it would come up with an answer (probably in the same language you asked it in...
    lol... unless you were specifically asking it to do a translation.) However, my
    project was specific to QA, and specifically related to BLOOM as a SQuAD-type
    modeling approach. If you have a general generative model, you could write a program
    (or use a BERT model, etc.) to turn a question into a prompt, feed that prompt
    to BLOOM, and have it finish the sentence. I was trying to see what happens if
    you take a generative model and try to turn it into a specific-use QA tool. Thus,
    the extractive QA head, need for context, etc.


    In my case, if you read through my research paper, you''ll find out BLOOM performs
    pretty poorly in this task relative to Encoders - which, if you think about it,
    makes sense. You''re asking a generative model - something which is designed to
    "know the right answer" to do a task it is not built for - i.e. "find an answer."
    So the BLOOM QA-specific model on SQuAD and similar (I trained a model on CDC
    data as well) just doesn''t really know what to do. Sometimes it''s right, but
    it doesn''t know how to be sure that it is, and sometimes it''s very, very wrong.
    But in all cases, the extractive QA approach requires the provision of context.


    If I were to start over and try to build a generative QA model, I wouldn''t use
    the SQuAD approach, and instead would simply train standard BLOOM on a set of
    data files. I would then figure out how to write a front-end that does what I
    described above: takes a question, reformats it into a prompt (maybe using a BERT
    variant?), feed the prompt to the custom-trained BLOOM model, and then see how
    well it did in responding. In this context, things like "exact match" or EM scores
    - one of the key metrics used to evaluate for SQuAD - doesn''t really make sense
    in context, because you might say the same thing three different ways and you''d
    still be right each time. Then again, for a commercial application, you''re back
    to the significant risk inherent in generative models: getting back an answer
    that would potentially get a human being fired. :)


    I hope that helps / makes sense. In the end, my efforts to train a BLOOM model
    for QA were a failure in the sense of "I built a better mousetrap" but a success
    in the sense of "that''s one more way not to invent a lightbulb." Looking forward
    to seeing what others have done and how that moves the ball forward for BLOOM!'
  created_at: 2022-12-10 00:37:17+00:00
  edited: false
  hidden: false
  id: 6393d4bd98461466b9e40115
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d2cf5e409d0a71be276fbbfccdcc2ad.svg
      fullname: Rob Boswell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rbos
      type: user
    createdAt: '2022-12-10T02:48:48.000Z'
    data:
      edited: true
      editors:
      - rbos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d2cf5e409d0a71be276fbbfccdcc2ad.svg
          fullname: Rob Boswell
          isHf: false
          isPro: false
          name: rbos
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jasoneden&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jasoneden\">@<span class=\"\
          underline\">jasoneden</span></a></span>\n\n\t</span></span> That's really\
          \ interesting. Yeah, I\u2019m hoping to keep both the question and answer\
          \ in English, lol.</p>\n<p>For a generative QA approach, would it be best\
          \ to just fine-tune it on any text at all, or would it be better to fine-tune\
          \ it on a huge number of pairs of questions and correct answers without\
          \ any context paragraph (e.g., just so that BLOOM understands more of what\
          \ correctly providing specific correct answers to specific questions is\
          \ like, and then I assume try to replicate this same QA process on the data\
          \ it was originally trained on)?  If it would best to fine-tune it on regular\
          \ text instead, wouldn't that just be teaching the model to do more of what\
          \ it already knows how to do? Or would it somehow become better at answering\
          \ questions in the process?</p>\n<p>For writing a program to turn a question\
          \ into a prompt, would it need to be different than the BLOOM model code\
          \ I showed earlier with <strong>prompt = \u201Csome question here\u201D\
          </strong>, feeding it into the tokenizer, and feeding the tokenizer to the\
          \ fine-tuned model?</p>\n<p>One other thing: do you know why the answers\
          \ that the BLOOM model gives in the output \u2013 at least on the huggingface\
          \ interface api \u2013 will often repeat themselves over and over until\
          \ the maximum_length argument\u2019s number of characters has been reached?\
          \ I\u2019m assuming there must some way to insert a line of code to prevent\
          \ the model from repeating itself. I\u2019ve noticed BLOOM will even sometimes\
          \ present its own new questions for itself to then answer - sometimes not\
          \ even related to the user's original question).</p>\n"
        raw: "@jasoneden That's really interesting. Yeah, I\u2019m hoping to keep\
          \ both the question and answer in English, lol.\n \nFor a generative QA\
          \ approach, would it be best to just fine-tune it on any text at all, or\
          \ would it be better to fine-tune it on a huge number of pairs of questions\
          \ and correct answers without any context paragraph (e.g., just so that\
          \ BLOOM understands more of what correctly providing specific correct answers\
          \ to specific questions is like, and then I assume try to replicate this\
          \ same QA process on the data it was originally trained on)?  If it would\
          \ best to fine-tune it on regular text instead, wouldn't that just be teaching\
          \ the model to do more of what it already knows how to do? Or would it somehow\
          \ become better at answering questions in the process?\n\nFor writing a\
          \ program to turn a question into a prompt, would it need to be different\
          \ than the BLOOM model code I showed earlier with **prompt = \u201Csome\
          \ question here\u201D**, feeding it into the tokenizer, and feeding the\
          \ tokenizer to the fine-tuned model?\n\nOne other thing: do you know why\
          \ the answers that the BLOOM model gives in the output \u2013 at least on\
          \ the huggingface interface api \u2013 will often repeat themselves over\
          \ and over until the maximum_length argument\u2019s number of characters\
          \ has been reached? I\u2019m assuming there must some way to insert a line\
          \ of code to prevent the model from repeating itself. I\u2019ve noticed\
          \ BLOOM will even sometimes present its own new questions for itself to\
          \ then answer - sometimes not even related to the user's original question)."
        updatedAt: '2022-12-22T17:25:16.742Z'
      numEdits: 1
      reactions: []
    id: 6393f3901beeb5c315edc251
    type: comment
  author: rbos
  content: "@jasoneden That's really interesting. Yeah, I\u2019m hoping to keep both\
    \ the question and answer in English, lol.\n \nFor a generative QA approach, would\
    \ it be best to just fine-tune it on any text at all, or would it be better to\
    \ fine-tune it on a huge number of pairs of questions and correct answers without\
    \ any context paragraph (e.g., just so that BLOOM understands more of what correctly\
    \ providing specific correct answers to specific questions is like, and then I\
    \ assume try to replicate this same QA process on the data it was originally trained\
    \ on)?  If it would best to fine-tune it on regular text instead, wouldn't that\
    \ just be teaching the model to do more of what it already knows how to do? Or\
    \ would it somehow become better at answering questions in the process?\n\nFor\
    \ writing a program to turn a question into a prompt, would it need to be different\
    \ than the BLOOM model code I showed earlier with **prompt = \u201Csome question\
    \ here\u201D**, feeding it into the tokenizer, and feeding the tokenizer to the\
    \ fine-tuned model?\n\nOne other thing: do you know why the answers that the BLOOM\
    \ model gives in the output \u2013 at least on the huggingface interface api \u2013\
    \ will often repeat themselves over and over until the maximum_length argument\u2019\
    s number of characters has been reached? I\u2019m assuming there must some way\
    \ to insert a line of code to prevent the model from repeating itself. I\u2019\
    ve noticed BLOOM will even sometimes present its own new questions for itself\
    \ to then answer - sometimes not even related to the user's original question)."
  created_at: 2022-12-10 02:48:48+00:00
  edited: true
  hidden: false
  id: 6393f3901beeb5c315edc251
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d2cf5e409d0a71be276fbbfccdcc2ad.svg
      fullname: Rob Boswell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rbos
      type: user
    createdAt: '2022-12-22T17:27:22.000Z'
    data:
      edited: false
      editors:
      - rbos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d2cf5e409d0a71be276fbbfccdcc2ad.svg
          fullname: Rob Boswell
          isHf: false
          isPro: false
          name: rbos
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;jasoneden&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jasoneden\"\
          >@<span class=\"underline\">jasoneden</span></a></span>\n\n\t</span></span>\
          \ That's really interesting. Yeah, I\u2019m hoping to keep both the question\
          \ and answer in English, lol.</p>\n<p>For a generative QA approach, would\
          \ it be best to just fine-tune it on any text at all, or would it be better\
          \ to fine-tune it on a huge number of pairs of questions and correct answers\
          \ without any context paragraph (e.g., just so that BLOOM understands more\
          \ of what correctly providing specific correct answers to specific questions\
          \ is like, and then I assume try to replicate this same QA process on the\
          \ data it was originally trained on)?  If it would best to fine-tune it\
          \ on regular text instead, wouldn't that just be teaching the model to do\
          \ more of what it already knows how to do? Or would it somehow become better\
          \ at answering questions in the process?</p>\n<p>For writing a program to\
          \ turn a question into a prompt, would it need to be different than the\
          \ BLOOM model code I showed earlier with <strong>prompt = \u201Csome question\
          \ here\u201D</strong>, feeding it into the tokenizer, and feeding the tokenizer\
          \ to the fine-tuned model?</p>\n<p>One other thing: do you know why the\
          \ answers that the BLOOM model gives in the output \u2013 at least on the\
          \ huggingface interface api \u2013 will often repeat themselves over and\
          \ over until the maximum_length argument\u2019s number of characters has\
          \ been reached? I\u2019m assuming there must some way to insert a line of\
          \ code to prevent the model from repeating itself. I\u2019ve noticed BLOOM\
          \ will even sometimes present its own new questions for itself to then answer\
          \ - sometimes not even related to the user's original question).</p>\n</blockquote>\n\
          <p>If anyone else knows the answers to the questions above, your help would\
          \ be much appreciated!</p>\n"
        raw: "> @jasoneden That's really interesting. Yeah, I\u2019m hoping to keep\
          \ both the question and answer in English, lol.\n>  \n> For a generative\
          \ QA approach, would it be best to just fine-tune it on any text at all,\
          \ or would it be better to fine-tune it on a huge number of pairs of questions\
          \ and correct answers without any context paragraph (e.g., just so that\
          \ BLOOM understands more of what correctly providing specific correct answers\
          \ to specific questions is like, and then I assume try to replicate this\
          \ same QA process on the data it was originally trained on)?  If it would\
          \ best to fine-tune it on regular text instead, wouldn't that just be teaching\
          \ the model to do more of what it already knows how to do? Or would it somehow\
          \ become better at answering questions in the process?\n> \n> For writing\
          \ a program to turn a question into a prompt, would it need to be different\
          \ than the BLOOM model code I showed earlier with **prompt = \u201Csome\
          \ question here\u201D**, feeding it into the tokenizer, and feeding the\
          \ tokenizer to the fine-tuned model?\n> \n> One other thing: do you know\
          \ why the answers that the BLOOM model gives in the output \u2013 at least\
          \ on the huggingface interface api \u2013 will often repeat themselves over\
          \ and over until the maximum_length argument\u2019s number of characters\
          \ has been reached? I\u2019m assuming there must some way to insert a line\
          \ of code to prevent the model from repeating itself. I\u2019ve noticed\
          \ BLOOM will even sometimes present its own new questions for itself to\
          \ then answer - sometimes not even related to the user's original question).\n\
          \nIf anyone else knows the answers to the questions above, your help would\
          \ be much appreciated!"
        updatedAt: '2022-12-22T17:27:22.611Z'
      numEdits: 0
      reactions: []
    id: 63a4937a658851481f76d1fd
    type: comment
  author: rbos
  content: "> @jasoneden That's really interesting. Yeah, I\u2019m hoping to keep\
    \ both the question and answer in English, lol.\n>  \n> For a generative QA approach,\
    \ would it be best to just fine-tune it on any text at all, or would it be better\
    \ to fine-tune it on a huge number of pairs of questions and correct answers without\
    \ any context paragraph (e.g., just so that BLOOM understands more of what correctly\
    \ providing specific correct answers to specific questions is like, and then I\
    \ assume try to replicate this same QA process on the data it was originally trained\
    \ on)?  If it would best to fine-tune it on regular text instead, wouldn't that\
    \ just be teaching the model to do more of what it already knows how to do? Or\
    \ would it somehow become better at answering questions in the process?\n> \n\
    > For writing a program to turn a question into a prompt, would it need to be\
    \ different than the BLOOM model code I showed earlier with **prompt = \u201C\
    some question here\u201D**, feeding it into the tokenizer, and feeding the tokenizer\
    \ to the fine-tuned model?\n> \n> One other thing: do you know why the answers\
    \ that the BLOOM model gives in the output \u2013 at least on the huggingface\
    \ interface api \u2013 will often repeat themselves over and over until the maximum_length\
    \ argument\u2019s number of characters has been reached? I\u2019m assuming there\
    \ must some way to insert a line of code to prevent the model from repeating itself.\
    \ I\u2019ve noticed BLOOM will even sometimes present its own new questions for\
    \ itself to then answer - sometimes not even related to the user's original question).\n\
    \nIf anyone else knows the answers to the questions above, your help would be\
    \ much appreciated!"
  created_at: 2022-12-22 17:27:22+00:00
  edited: false
  hidden: false
  id: 63a4937a658851481f76d1fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f338a9912cdde514439c1744b7371eff.svg
      fullname: chris chang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littlexxxxx
      type: user
    createdAt: '2022-12-26T04:41:02.000Z'
    data:
      edited: true
      editors:
      - littlexxxxx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f338a9912cdde514439c1744b7371eff.svg
          fullname: chris chang
          isHf: false
          isPro: false
          name: littlexxxxx
          type: user
        html: "<blockquote>\n<p>We finetuned BLOOM to produce <a href=\"https://huggingface.co/bigscience/bloomz\"\
          >BLOOMZ</a>, maybe our <a rel=\"nofollow\" href=\"https://github.com/bigscience-workshop/xmtf#bloomz\"\
          >guide</a> can help some people in this thread \U0001F917</p>\n</blockquote>\n\
          <p>According to the paper \"Crosslingual Generalization through multitask\
          \ finetuning\", bloomz was trained on bloom which is decoder-only model,\
          \ however in the <a rel=\"nofollow\" href=\"https://github.com/bigscience-workshop/bigscience/blob/master/train/tr13-mtf/tr13-176B-mtf-xp3capmixnewcodelonglossseq.slurm\"\
          >training script</a> of bloomz, finetune_t0.py should be based on t5 which\
          \ is a encoder-decoder model, and I can not find the actual finetune_t0.py\
          \ file right now, am I missing something?</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span></p>\n"
        raw: "> We finetuned BLOOM to produce [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
          \ maybe our [guide](https://github.com/bigscience-workshop/xmtf#bloomz)\
          \ can help some people in this thread \U0001F917\n\nAccording to the paper\
          \ \"Crosslingual Generalization through multitask finetuning\", bloomz was\
          \ trained on bloom which is decoder-only model, however in the [training\
          \ script](https://github.com/bigscience-workshop/bigscience/blob/master/train/tr13-mtf/tr13-176B-mtf-xp3capmixnewcodelonglossseq.slurm)\
          \ of bloomz, finetune_t0.py should be based on t5 which is a encoder-decoder\
          \ model, and I can not find the actual finetune_t0.py file right now, am\
          \ I missing something?\n\n@Muennighoff"
        updatedAt: '2022-12-26T04:41:53.105Z'
      numEdits: 1
      reactions: []
    id: 63a925dea8066873e928b2b3
    type: comment
  author: littlexxxxx
  content: "> We finetuned BLOOM to produce [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
    \ maybe our [guide](https://github.com/bigscience-workshop/xmtf#bloomz) can help\
    \ some people in this thread \U0001F917\n\nAccording to the paper \"Crosslingual\
    \ Generalization through multitask finetuning\", bloomz was trained on bloom which\
    \ is decoder-only model, however in the [training script](https://github.com/bigscience-workshop/bigscience/blob/master/train/tr13-mtf/tr13-176B-mtf-xp3capmixnewcodelonglossseq.slurm)\
    \ of bloomz, finetune_t0.py should be based on t5 which is a encoder-decoder model,\
    \ and I can not find the actual finetune_t0.py file right now, am I missing something?\n\
    \n@Muennighoff"
  created_at: 2022-12-26 04:41:02+00:00
  edited: true
  hidden: false
  id: 63a925dea8066873e928b2b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-12-26T05:08:52.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<blockquote>\n<blockquote>\n<p>We finetuned BLOOM to produce <a href=\"\
          https://huggingface.co/bigscience/bloomz\">BLOOMZ</a>, maybe our <a rel=\"\
          nofollow\" href=\"https://github.com/bigscience-workshop/xmtf#bloomz\">guide</a>\
          \ can help some people in this thread \U0001F917</p>\n</blockquote>\n<p>According\
          \ to the paper \"Crosslingual Generalization through multitask finetuning\"\
          , bloomz was trained on bloom which is decoder-only model, however in the\
          \ <a rel=\"nofollow\" href=\"https://github.com/bigscience-workshop/bigscience/blob/master/train/tr13-mtf/tr13-176B-mtf-xp3capmixnewcodelonglossseq.slurm\"\
          >training script</a> of bloomz, finetune_t0.py should be based on t5 which\
          \ is a encoder-decoder model, and I can not find the actual finetune_t0.py\
          \ file right now, am I missing something?</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span></p>\n\
          </blockquote>\n<p>The naming is bad, but it uses the <code>finetune_t0.py</code>\
          \ file. With t0 we just wanted to refer to the multitask finetuning part.<br>You\
          \ can find it if you switch to the <code>t0loading</code> branch.</p>\n"
        raw: "> > We finetuned BLOOM to produce [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
          \ maybe our [guide](https://github.com/bigscience-workshop/xmtf#bloomz)\
          \ can help some people in this thread \U0001F917\n> \n> According to the\
          \ paper \"Crosslingual Generalization through multitask finetuning\", bloomz\
          \ was trained on bloom which is decoder-only model, however in the [training\
          \ script](https://github.com/bigscience-workshop/bigscience/blob/master/train/tr13-mtf/tr13-176B-mtf-xp3capmixnewcodelonglossseq.slurm)\
          \ of bloomz, finetune_t0.py should be based on t5 which is a encoder-decoder\
          \ model, and I can not find the actual finetune_t0.py file right now, am\
          \ I missing something?\n> \n> @Muennighoff\n\nThe naming is bad, but it\
          \ uses the `finetune_t0.py` file. With t0 we just wanted to refer to the\
          \ multitask finetuning part.\nYou can find it if you switch to the `t0loading`\
          \ branch."
        updatedAt: '2022-12-26T05:08:52.325Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - littlexxxxx
    id: 63a92c645f4ee13aba7a4710
    type: comment
  author: Muennighoff
  content: "> > We finetuned BLOOM to produce [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
    \ maybe our [guide](https://github.com/bigscience-workshop/xmtf#bloomz) can help\
    \ some people in this thread \U0001F917\n> \n> According to the paper \"Crosslingual\
    \ Generalization through multitask finetuning\", bloomz was trained on bloom which\
    \ is decoder-only model, however in the [training script](https://github.com/bigscience-workshop/bigscience/blob/master/train/tr13-mtf/tr13-176B-mtf-xp3capmixnewcodelonglossseq.slurm)\
    \ of bloomz, finetune_t0.py should be based on t5 which is a encoder-decoder model,\
    \ and I can not find the actual finetune_t0.py file right now, am I missing something?\n\
    > \n> @Muennighoff\n\nThe naming is bad, but it uses the `finetune_t0.py` file.\
    \ With t0 we just wanted to refer to the multitask finetuning part.\nYou can find\
    \ it if you switch to the `t0loading` branch."
  created_at: 2022-12-26 05:08:52+00:00
  edited: false
  hidden: false
  id: 63a92c645f4ee13aba7a4710
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2599319be81df1934dd14a123df569e0.svg
      fullname: Rafay Mahmood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RafayPunch
      type: user
    createdAt: '2023-01-19T07:27:45.000Z'
    data:
      edited: false
      editors:
      - RafayPunch
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2599319be81df1934dd14a123df569e0.svg
          fullname: Rafay Mahmood
          isHf: false
          isPro: false
          name: RafayPunch
          type: user
        html: '<p>Hey. I have been exploring BLOOM and its API closely. I have learned
          how the parameters effect the response. After I am getting the responses
          I usually process it and remove garbage token model has produced.<br>Is
          there any way that BLOOM can stop producing more token if the context of
          sentence is completed.<br>GPT-3 basically stops itself when the context
          of sentence is completed or the prompt is answered.<br>But BLOOM produce
          extra tokens just to complete the max_token length provided.<br>Is there
          any way BLOOM can stop producing tokens once context is complete just like
          GPT-3</p>

          <p>For Example: What is Machine Learning?<br>BLOOM will answer it fine with
          first 2 or 3 sentences but produce random text(garbage - it may be some
          code - regular expression formulas)</p>

          '
        raw: "Hey. I have been exploring BLOOM and its API closely. I have learned\
          \ how the parameters effect the response. After I am getting the responses\
          \ I usually process it and remove garbage token model has produced.\nIs\
          \ there any way that BLOOM can stop producing more token if the context\
          \ of sentence is completed. \nGPT-3 basically stops itself when the context\
          \ of sentence is completed or the prompt is answered.\nBut BLOOM produce\
          \ extra tokens just to complete the max_token length provided.\nIs there\
          \ any way BLOOM can stop producing tokens once context is complete just\
          \ like GPT-3\n\nFor Example: What is Machine Learning? \nBLOOM will answer\
          \ it fine with first 2 or 3 sentences but produce random text(garbage -\
          \ it may be some code - regular expression formulas)"
        updatedAt: '2023-01-19T07:27:45.715Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - hoke
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hoke
      - count: 1
        reaction: "\U0001F614"
        users:
        - hoke
      - count: 1
        reaction: "\U0001F92F"
        users:
        - hoke
      - count: 1
        reaction: "\U0001F917"
        users:
        - hoke
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - hoke
    id: 63c8f0f18afd58b44091a8be
    type: comment
  author: RafayPunch
  content: "Hey. I have been exploring BLOOM and its API closely. I have learned how\
    \ the parameters effect the response. After I am getting the responses I usually\
    \ process it and remove garbage token model has produced.\nIs there any way that\
    \ BLOOM can stop producing more token if the context of sentence is completed.\
    \ \nGPT-3 basically stops itself when the context of sentence is completed or\
    \ the prompt is answered.\nBut BLOOM produce extra tokens just to complete the\
    \ max_token length provided.\nIs there any way BLOOM can stop producing tokens\
    \ once context is complete just like GPT-3\n\nFor Example: What is Machine Learning?\
    \ \nBLOOM will answer it fine with first 2 or 3 sentences but produce random text(garbage\
    \ - it may be some code - regular expression formulas)"
  created_at: 2023-01-19 07:27:45+00:00
  edited: false
  hidden: false
  id: 63c8f0f18afd58b44091a8be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-01-19T09:08:53.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<blockquote>

          <p>Hey. I have been exploring BLOOM and its API closely. I have learned
          how the parameters effect the response. After I am getting the responses
          I usually process it and remove garbage token model has produced.<br>Is
          there any way that BLOOM can stop producing more token if the context of
          sentence is completed.<br>GPT-3 basically stops itself when the context
          of sentence is completed or the prompt is answered.<br>But BLOOM produce
          extra tokens just to complete the max_token length provided.<br>Is there
          any way BLOOM can stop producing tokens once context is complete just like
          GPT-3</p>

          <p>For Example: What is Machine Learning?<br>BLOOM will answer it fine with
          first 2 or 3 sentences but produce random text(garbage - it may be some
          code - regular expression formulas)</p>

          </blockquote>

          <p>You may want to try <a href="https://huggingface.co/bigscience/bloomz">BLOOMZ</a>,
          which stops by itself when it deems the question to be answered.<br>Questions
          like <code>What is Machine Learning?</code> should work quite well.</p>

          '
        raw: "> Hey. I have been exploring BLOOM and its API closely. I have learned\
          \ how the parameters effect the response. After I am getting the responses\
          \ I usually process it and remove garbage token model has produced.\n> Is\
          \ there any way that BLOOM can stop producing more token if the context\
          \ of sentence is completed. \n> GPT-3 basically stops itself when the context\
          \ of sentence is completed or the prompt is answered.\n> But BLOOM produce\
          \ extra tokens just to complete the max_token length provided.\n> Is there\
          \ any way BLOOM can stop producing tokens once context is complete just\
          \ like GPT-3\n> \n> For Example: What is Machine Learning? \n> BLOOM will\
          \ answer it fine with first 2 or 3 sentences but produce random text(garbage\
          \ - it may be some code - regular expression formulas)\n\nYou may want to\
          \ try [BLOOMZ](https://huggingface.co/bigscience/bloomz), which stops by\
          \ itself when it deems the question to be answered.\nQuestions like `What\
          \ is Machine Learning?` should work quite well."
        updatedAt: '2023-01-19T09:08:53.501Z'
      numEdits: 0
      reactions: []
    id: 63c908a58afd58b44094157a
    type: comment
  author: Muennighoff
  content: "> Hey. I have been exploring BLOOM and its API closely. I have learned\
    \ how the parameters effect the response. After I am getting the responses I usually\
    \ process it and remove garbage token model has produced.\n> Is there any way\
    \ that BLOOM can stop producing more token if the context of sentence is completed.\
    \ \n> GPT-3 basically stops itself when the context of sentence is completed or\
    \ the prompt is answered.\n> But BLOOM produce extra tokens just to complete the\
    \ max_token length provided.\n> Is there any way BLOOM can stop producing tokens\
    \ once context is complete just like GPT-3\n> \n> For Example: What is Machine\
    \ Learning? \n> BLOOM will answer it fine with first 2 or 3 sentences but produce\
    \ random text(garbage - it may be some code - regular expression formulas)\n\n\
    You may want to try [BLOOMZ](https://huggingface.co/bigscience/bloomz), which\
    \ stops by itself when it deems the question to be answered.\nQuestions like `What\
    \ is Machine Learning?` should work quite well."
  created_at: 2023-01-19 09:08:53+00:00
  edited: false
  hidden: false
  id: 63c908a58afd58b44094157a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2599319be81df1934dd14a123df569e0.svg
      fullname: Rafay Mahmood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RafayPunch
      type: user
    createdAt: '2023-01-19T10:33:40.000Z'
    data:
      edited: false
      editors:
      - RafayPunch
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2599319be81df1934dd14a123df569e0.svg
          fullname: Rafay Mahmood
          isHf: false
          isPro: false
          name: RafayPunch
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Hey. I have been exploring BLOOM and\
          \ its API closely. I have learned how the parameters effect the response.\
          \ After I am getting the responses I usually process it and remove garbage\
          \ token model has produced.<br>Is there any way that BLOOM can stop producing\
          \ more token if the context of sentence is completed.<br>GPT-3 basically\
          \ stops itself when the context of sentence is completed or the prompt is\
          \ answered.<br>But BLOOM produce extra tokens just to complete the max_token\
          \ length provided.<br>Is there any way BLOOM can stop producing tokens once\
          \ context is complete just like GPT-3</p>\n<p>For Example: What is Machine\
          \ Learning?<br>BLOOM will answer it fine with first 2 or 3 sentences but\
          \ produce random text(garbage - it may be some code - regular expression\
          \ formulas)</p>\n</blockquote>\n<p>You may want to try <a href=\"https://huggingface.co/bigscience/bloomz\"\
          >BLOOMZ</a>, which stops by itself when it deems the question to be answered.<br>Questions\
          \ like <code>What is Machine Learning?</code> should work quite well.</p>\n\
          </blockquote>\n<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span><br>I\
          \ cannot really find Inference API to BLOOMZ? Can you provide a link to\
          \ the page to checkout the API. Other then that I do have an idea to make\
          \ it work using transformers.</p>\n"
        raw: "> > Hey. I have been exploring BLOOM and its API closely. I have learned\
          \ how the parameters effect the response. After I am getting the responses\
          \ I usually process it and remove garbage token model has produced.\n> >\
          \ Is there any way that BLOOM can stop producing more token if the context\
          \ of sentence is completed. \n> > GPT-3 basically stops itself when the\
          \ context of sentence is completed or the prompt is answered.\n> > But BLOOM\
          \ produce extra tokens just to complete the max_token length provided.\n\
          > > Is there any way BLOOM can stop producing tokens once context is complete\
          \ just like GPT-3\n> > \n> > For Example: What is Machine Learning? \n>\
          \ > BLOOM will answer it fine with first 2 or 3 sentences but produce random\
          \ text(garbage - it may be some code - regular expression formulas)\n> \n\
          > You may want to try [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
          \ which stops by itself when it deems the question to be answered.\n> Questions\
          \ like `What is Machine Learning?` should work quite well.\n\nThanks @Muennighoff\
          \ \nI cannot really find Inference API to BLOOMZ? Can you provide a link\
          \ to the page to checkout the API. Other then that I do have an idea to\
          \ make it work using transformers."
        updatedAt: '2023-01-19T10:33:40.951Z'
      numEdits: 0
      reactions: []
    id: 63c91c841e3e1bf4a2c2afe6
    type: comment
  author: RafayPunch
  content: "> > Hey. I have been exploring BLOOM and its API closely. I have learned\
    \ how the parameters effect the response. After I am getting the responses I usually\
    \ process it and remove garbage token model has produced.\n> > Is there any way\
    \ that BLOOM can stop producing more token if the context of sentence is completed.\
    \ \n> > GPT-3 basically stops itself when the context of sentence is completed\
    \ or the prompt is answered.\n> > But BLOOM produce extra tokens just to complete\
    \ the max_token length provided.\n> > Is there any way BLOOM can stop producing\
    \ tokens once context is complete just like GPT-3\n> > \n> > For Example: What\
    \ is Machine Learning? \n> > BLOOM will answer it fine with first 2 or 3 sentences\
    \ but produce random text(garbage - it may be some code - regular expression formulas)\n\
    > \n> You may want to try [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
    \ which stops by itself when it deems the question to be answered.\n> Questions\
    \ like `What is Machine Learning?` should work quite well.\n\nThanks @Muennighoff\
    \ \nI cannot really find Inference API to BLOOMZ? Can you provide a link to the\
    \ page to checkout the API. Other then that I do have an idea to make it work\
    \ using transformers."
  created_at: 2023-01-19 10:33:40+00:00
  edited: false
  hidden: false
  id: 63c91c841e3e1bf4a2c2afe6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-01-19T13:43:36.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p>Hey. I have been exploring\
          \ BLOOM and its API closely. I have learned how the parameters effect the\
          \ response. After I am getting the responses I usually process it and remove\
          \ garbage token model has produced.<br>Is there any way that BLOOM can stop\
          \ producing more token if the context of sentence is completed.<br>GPT-3\
          \ basically stops itself when the context of sentence is completed or the\
          \ prompt is answered.<br>But BLOOM produce extra tokens just to complete\
          \ the max_token length provided.<br>Is there any way BLOOM can stop producing\
          \ tokens once context is complete just like GPT-3</p>\n<p>For Example: What\
          \ is Machine Learning?<br>BLOOM will answer it fine with first 2 or 3 sentences\
          \ but produce random text(garbage - it may be some code - regular expression\
          \ formulas)</p>\n</blockquote>\n<p>You may want to try <a href=\"https://huggingface.co/bigscience/bloomz\"\
          >BLOOMZ</a>, which stops by itself when it deems the question to be answered.<br>Questions\
          \ like <code>What is Machine Learning?</code> should work quite well.</p>\n\
          </blockquote>\n<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span><br>I\
          \ cannot really find Inference API to BLOOMZ? Can you provide a link to\
          \ the page to checkout the API. Other then that I do have an idea to make\
          \ it work using transformers.</p>\n</blockquote>\n<p>The widget on the hub\
          \ is turned off, but you can try it using this colab: <a href=\"https://huggingface.co/bigscience/bloomz/discussions/28\"\
          >https://huggingface.co/bigscience/bloomz/discussions/28</a><br>The model\
          \ is also available here, but with some prompting already done: <a rel=\"\
          nofollow\" href=\"http://chat.petals.ml/\">http://chat.petals.ml/</a></p>\n"
        raw: "> > > Hey. I have been exploring BLOOM and its API closely. I have learned\
          \ how the parameters effect the response. After I am getting the responses\
          \ I usually process it and remove garbage token model has produced.\n> >\
          \ > Is there any way that BLOOM can stop producing more token if the context\
          \ of sentence is completed. \n> > > GPT-3 basically stops itself when the\
          \ context of sentence is completed or the prompt is answered.\n> > > But\
          \ BLOOM produce extra tokens just to complete the max_token length provided.\n\
          > > > Is there any way BLOOM can stop producing tokens once context is complete\
          \ just like GPT-3\n> > > \n> > > For Example: What is Machine Learning?\
          \ \n> > > BLOOM will answer it fine with first 2 or 3 sentences but produce\
          \ random text(garbage - it may be some code - regular expression formulas)\n\
          > > \n> > You may want to try [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
          \ which stops by itself when it deems the question to be answered.\n> >\
          \ Questions like `What is Machine Learning?` should work quite well.\n>\
          \ \n> Thanks @Muennighoff \n> I cannot really find Inference API to BLOOMZ?\
          \ Can you provide a link to the page to checkout the API. Other then that\
          \ I do have an idea to make it work using transformers.\n\nThe widget on\
          \ the hub is turned off, but you can try it using this colab: https://huggingface.co/bigscience/bloomz/discussions/28\n\
          The model is also available here, but with some prompting already done:\
          \ http://chat.petals.ml/"
        updatedAt: '2023-01-19T13:43:36.969Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - RafayPunch
        - borzunov
    id: 63c949081e3e1bf4a2c6f6e4
    type: comment
  author: Muennighoff
  content: "> > > Hey. I have been exploring BLOOM and its API closely. I have learned\
    \ how the parameters effect the response. After I am getting the responses I usually\
    \ process it and remove garbage token model has produced.\n> > > Is there any\
    \ way that BLOOM can stop producing more token if the context of sentence is completed.\
    \ \n> > > GPT-3 basically stops itself when the context of sentence is completed\
    \ or the prompt is answered.\n> > > But BLOOM produce extra tokens just to complete\
    \ the max_token length provided.\n> > > Is there any way BLOOM can stop producing\
    \ tokens once context is complete just like GPT-3\n> > > \n> > > For Example:\
    \ What is Machine Learning? \n> > > BLOOM will answer it fine with first 2 or\
    \ 3 sentences but produce random text(garbage - it may be some code - regular\
    \ expression formulas)\n> > \n> > You may want to try [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
    \ which stops by itself when it deems the question to be answered.\n> > Questions\
    \ like `What is Machine Learning?` should work quite well.\n> \n> Thanks @Muennighoff\
    \ \n> I cannot really find Inference API to BLOOMZ? Can you provide a link to\
    \ the page to checkout the API. Other then that I do have an idea to make it work\
    \ using transformers.\n\nThe widget on the hub is turned off, but you can try\
    \ it using this colab: https://huggingface.co/bigscience/bloomz/discussions/28\n\
    The model is also available here, but with some prompting already done: http://chat.petals.ml/"
  created_at: 2023-01-19 13:43:36+00:00
  edited: false
  hidden: false
  id: 63c949081e3e1bf4a2c6f6e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
      fullname: Alexander Borzunov
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: borzunov
      type: user
    createdAt: '2023-01-19T16:03:30.000Z'
    data:
      edited: false
      editors:
      - borzunov
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
          fullname: Alexander Borzunov
          isHf: false
          isPro: false
          name: borzunov
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;RafayPunch&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/RafayPunch\">@<span class=\"\
          underline\">RafayPunch</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;Muennighoff&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/Muennighoff\">@<span class=\"underline\"\
          >Muennighoff</span></a></span>\n\n\t</span></span> Regarding <a rel=\"nofollow\"\
          \ href=\"http://chat.petals.ml\">http://chat.petals.ml</a>, feel free to\
          \ drop default prompts by clicking \"Enable few-shot mode\" at the bottom\
          \ of the page.</p>\n"
        raw: '@RafayPunch @Muennighoff Regarding http://chat.petals.ml, feel free
          to drop default prompts by clicking "Enable few-shot mode" at the bottom
          of the page.'
        updatedAt: '2023-01-19T16:03:30.219Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Muennighoff
    id: 63c969d28afd58b4409e7b28
    type: comment
  author: borzunov
  content: '@RafayPunch @Muennighoff Regarding http://chat.petals.ml, feel free to
    drop default prompts by clicking "Enable few-shot mode" at the bottom of the page.'
  created_at: 2023-01-19 16:03:30+00:00
  edited: false
  hidden: false
  id: 63c969d28afd58b4409e7b28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/38e230aa7e691e62a3a31767f5167320.svg
      fullname: Sandeep Kumar Paleti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sandy317
      type: user
    createdAt: '2023-02-17T06:09:29.000Z'
    data:
      edited: true
      editors:
      - Sandy317
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/38e230aa7e691e62a3a31767f5167320.svg
          fullname: Sandeep Kumar Paleti
          isHf: false
          isPro: false
          name: Sandy317
          type: user
        html: '<p>raise MlflowException("API request to %s failed with exception %s"
          % (url, e))<br>mlflow.exceptions.MlflowException: API request to <a rel="nofollow"
          href="https://https://adb-5844287990719898.18.azuredatabricks.net/api/2.0/mlflow/runs/create">https://https://adb-5844287990719898.18.azuredatabricks.net/api/2.0/mlflow/runs/create</a>
          failed with exception HTTPSConnectionPool(host=''https'', port=443): Max
          retries exceeded with url: //adb-5844287990719898.18.azuredatabricks.net/api/2.0/mlflow/runs/create
          (Caused by NewConnectionError(''&lt;urllib3.connection.HTTPSConnection object
          at 0x7f5d8df50af0&gt;: Failed to establish a new connection: [Errno -2]
          Name or service not known'')).</p>

          <p>How to solve this issue ?</p>

          '
        raw: 'raise MlflowException("API request to %s failed with exception %s" %
          (url, e))

          mlflow.exceptions.MlflowException: API request to https://https://adb-5844287990719898.18.azuredatabricks.net/api/2.0/mlflow/runs/create
          failed with exception HTTPSConnectionPool(host=''https'', port=443): Max
          retries exceeded with url: //adb-5844287990719898.18.azuredatabricks.net/api/2.0/mlflow/runs/create
          (Caused by NewConnectionError(''<urllib3.connection.HTTPSConnection object
          at 0x7f5d8df50af0>: Failed to establish a new connection: [Errno -2] Name
          or service not known'')).


          How to solve this issue ?'
        updatedAt: '2023-02-17T06:26:08.117Z'
      numEdits: 1
      reactions: []
    id: 63ef1a1983dab7ab07c28faa
    type: comment
  author: Sandy317
  content: 'raise MlflowException("API request to %s failed with exception %s" % (url,
    e))

    mlflow.exceptions.MlflowException: API request to https://https://adb-5844287990719898.18.azuredatabricks.net/api/2.0/mlflow/runs/create
    failed with exception HTTPSConnectionPool(host=''https'', port=443): Max retries
    exceeded with url: //adb-5844287990719898.18.azuredatabricks.net/api/2.0/mlflow/runs/create
    (Caused by NewConnectionError(''<urllib3.connection.HTTPSConnection object at
    0x7f5d8df50af0>: Failed to establish a new connection: [Errno -2] Name or service
    not known'')).


    How to solve this issue ?'
  created_at: 2023-02-17 06:09:29+00:00
  edited: true
  hidden: false
  id: 63ef1a1983dab7ab07c28faa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/664019355d85e5fa17be5ffc4fccfbcb.svg
      fullname: Josiah Bryan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: josiahbryan
      type: user
    createdAt: '2023-04-18T02:46:21.000Z'
    data:
      edited: false
      editors:
      - josiahbryan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/664019355d85e5fa17be5ffc4fccfbcb.svg
          fullname: Josiah Bryan
          isHf: false
          isPro: false
          name: josiahbryan
          type: user
        html: "<p>When I tried chat.petals.ml, every thing I tried produced a python\
          \ error, pretty much boiling down to this final line:</p>\n<pre><code> \
          \ File \"/home/borzunov/.local/lib/python3.10/site-packages/hivemind/p2p/p2p_daemon_bindings/utils.py\"\
          , line 72, in raise_if_failed\n    raise ControlFailure(f\"Connect failed.\
          \ msg={response.error.msg}\")\nhivemind.p2p.p2p_daemon_bindings.utils.ControlFailure:\
          \ Connect failed. msg=routing: not found\n</code></pre>\n<p>Thoughts...?</p>\n"
        raw: "When I tried chat.petals.ml, every thing I tried produced a python error,\
          \ pretty much boiling down to this final line:\n\n```\n  File \"/home/borzunov/.local/lib/python3.10/site-packages/hivemind/p2p/p2p_daemon_bindings/utils.py\"\
          , line 72, in raise_if_failed\n    raise ControlFailure(f\"Connect failed.\
          \ msg={response.error.msg}\")\nhivemind.p2p.p2p_daemon_bindings.utils.ControlFailure:\
          \ Connect failed. msg=routing: not found\n```\n\nThoughts...?"
        updatedAt: '2023-04-18T02:46:21.344Z'
      numEdits: 0
      reactions: []
    id: 643e047dd4dcedc3186c785f
    type: comment
  author: josiahbryan
  content: "When I tried chat.petals.ml, every thing I tried produced a python error,\
    \ pretty much boiling down to this final line:\n\n```\n  File \"/home/borzunov/.local/lib/python3.10/site-packages/hivemind/p2p/p2p_daemon_bindings/utils.py\"\
    , line 72, in raise_if_failed\n    raise ControlFailure(f\"Connect failed. msg={response.error.msg}\"\
    )\nhivemind.p2p.p2p_daemon_bindings.utils.ControlFailure: Connect failed. msg=routing:\
    \ not found\n```\n\nThoughts...?"
  created_at: 2023-04-18 01:46:21+00:00
  edited: false
  hidden: false
  id: 643e047dd4dcedc3186c785f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7e776c0ade954dd23d380544e12cb91.svg
      fullname: ramkrish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ramkrish120595
      type: user
    createdAt: '2023-08-04T07:18:01.000Z'
    data:
      edited: false
      editors:
      - ramkrish120595
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7221306562423706
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7e776c0ade954dd23d380544e12cb91.svg
          fullname: ramkrish
          isHf: false
          isPro: false
          name: ramkrish120595
          type: user
        html: '<p>hi ..........I try to implement the  extractive  QA from bloom-560m
          model ........ I need the training script for bloom extractive QA and implement
          steps..................pls help me </p>

          '
        raw: 'hi ..........I try to implement the  extractive  QA from bloom-560m
          model ........ I need the training script for bloom extractive QA and implement
          steps..................pls help me '
        updatedAt: '2023-08-04T07:18:01.065Z'
      numEdits: 0
      reactions: []
    id: 64cca62976200ec80f1a1871
    type: comment
  author: ramkrish120595
  content: 'hi ..........I try to implement the  extractive  QA from bloom-560m model
    ........ I need the training script for bloom extractive QA and implement steps..................pls
    help me '
  created_at: 2023-08-04 06:18:01+00:00
  edited: false
  hidden: false
  id: 64cca62976200ec80f1a1871
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
      fullname: Jason Eden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jasoneden
      type: user
    createdAt: '2023-08-04T14:11:25.000Z'
    data:
      edited: false
      editors:
      - jasoneden
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9541041254997253
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b80ae0ed1a8d0bfc2775b81d01fa66ec.svg
          fullname: Jason Eden
          isHf: false
          isPro: false
          name: jasoneden
          type: user
        html: '<blockquote>

          <p>hi ..........I try to implement the  extractive  QA from bloom-560m model
          ........ I need the training script for bloom extractive QA and implement
          steps..................pls help me</p>

          </blockquote>

          <p>Here''s the work I did for my acadmic project. That''s been a while ago
          now, so I don''t know if the code is still functional, and I will not be
          able to provide support or advice for anything that isn''t working today.
          However, to the degree it''s helpful:</p>

          <p><a rel="nofollow" href="https://github.com/jasondeden/capstone">https://github.com/jasondeden/capstone</a>
          </p>

          <p>Good luck!</p>

          '
        raw: "> hi ..........I try to implement the  extractive  QA from bloom-560m\
          \ model ........ I need the training script for bloom extractive QA and\
          \ implement steps..................pls help me\n\nHere's the work I did\
          \ for my acadmic project. That's been a while ago now, so I don't know if\
          \ the code is still functional, and I will not be able to provide support\
          \ or advice for anything that isn't working today. However, to the degree\
          \ it's helpful:\n\nhttps://github.com/jasondeden/capstone \n\nGood luck!"
        updatedAt: '2023-08-04T14:11:25.583Z'
      numEdits: 0
      reactions: []
    id: 64cd070db7aae805e1605925
    type: comment
  author: jasoneden
  content: "> hi ..........I try to implement the  extractive  QA from bloom-560m\
    \ model ........ I need the training script for bloom extractive QA and implement\
    \ steps..................pls help me\n\nHere's the work I did for my acadmic project.\
    \ That's been a while ago now, so I don't know if the code is still functional,\
    \ and I will not be able to provide support or advice for anything that isn't\
    \ working today. However, to the degree it's helpful:\n\nhttps://github.com/jasondeden/capstone\
    \ \n\nGood luck!"
  created_at: 2023-08-04 13:11:25+00:00
  edited: false
  hidden: false
  id: 64cd070db7aae805e1605925
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7e776c0ade954dd23d380544e12cb91.svg
      fullname: ramkrish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ramkrish120595
      type: user
    createdAt: '2023-08-08T12:48:11.000Z'
    data:
      edited: true
      editors:
      - ramkrish120595
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3739762306213379
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7e776c0ade954dd23d380544e12cb91.svg
          fullname: ramkrish
          isHf: false
          isPro: false
          name: ramkrish120595
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;jasoneden&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jasoneden\"\
          >@<span class=\"underline\">jasoneden</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: "Thanks @jasoneden \n"
        updatedAt: '2023-08-08T12:48:54.220Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jasoneden
    id: 64d2398b3dc82444ac4079ed
    type: comment
  author: ramkrish120595
  content: "Thanks @jasoneden \n"
  created_at: 2023-08-08 11:48:11+00:00
  edited: true
  hidden: false
  id: 64d2398b3dc82444ac4079ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ceff9bd9d0e8e28db2a99e1eb59fce3.svg
      fullname: Yash kakde
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yashqual
      type: user
    createdAt: '2023-09-11T05:39:24.000Z'
    data:
      edited: false
      editors:
      - Yashqual
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9180572032928467
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ceff9bd9d0e8e28db2a99e1eb59fce3.svg
          fullname: Yash kakde
          isHf: false
          isPro: false
          name: Yashqual
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p>Hey. I have been exploring\
          \ BLOOM and its API closely. I have learned how the parameters effect the\
          \ response. After I am getting the responses I usually process it and remove\
          \ garbage token model has produced.<br>Is there any way that BLOOM can stop\
          \ producing more token if the context of sentence is completed.<br>GPT-3\
          \ basically stops itself when the context of sentence is completed or the\
          \ prompt is answered.<br>But BLOOM produce extra tokens just to complete\
          \ the max_token length provided.<br>Is there any way BLOOM can stop producing\
          \ tokens once context is complete just like GPT-3</p>\n<p>For Example: What\
          \ is Machine Learning?<br>BLOOM will answer it fine with first 2 or 3 sentences\
          \ but produce random text(garbage - it may be some code - regular expression\
          \ formulas)</p>\n</blockquote>\n<p>You may want to try <a href=\"https://huggingface.co/bigscience/bloomz\"\
          >BLOOMZ</a>, which stops by itself when it deems the question to be answered.<br>Questions\
          \ like <code>What is Machine Learning?</code> should work quite well.</p>\n\
          </blockquote>\n<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span><br>I\
          \ cannot really find Inference API to BLOOMZ? Can you provide a link to\
          \ the page to checkout the API. Other then that I do have an idea to make\
          \ it work using transformers.</p>\n</blockquote>\n<p>Hey <span data-props=\"\
          {&quot;user&quot;:&quot;RafayPunch&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/RafayPunch\">@<span class=\"underline\"\
          >RafayPunch</span></a></span>\n\n\t</span></span>, you mentioned that you\
          \ know how to tackle this problem using transformers. Ive been stuck on\
          \ the same issue and haven't found a solution for it yet. If you have, could\
          \ you please explain how you did it.</p>\n"
        raw: "> > > Hey. I have been exploring BLOOM and its API closely. I have learned\
          \ how the parameters effect the response. After I am getting the responses\
          \ I usually process it and remove garbage token model has produced.\n> >\
          \ > Is there any way that BLOOM can stop producing more token if the context\
          \ of sentence is completed. \n> > > GPT-3 basically stops itself when the\
          \ context of sentence is completed or the prompt is answered.\n> > > But\
          \ BLOOM produce extra tokens just to complete the max_token length provided.\n\
          > > > Is there any way BLOOM can stop producing tokens once context is complete\
          \ just like GPT-3\n> > > \n> > > For Example: What is Machine Learning?\
          \ \n> > > BLOOM will answer it fine with first 2 or 3 sentences but produce\
          \ random text(garbage - it may be some code - regular expression formulas)\n\
          > > \n> > You may want to try [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
          \ which stops by itself when it deems the question to be answered.\n> >\
          \ Questions like `What is Machine Learning?` should work quite well.\n>\
          \ \n> Thanks @Muennighoff \n> I cannot really find Inference API to BLOOMZ?\
          \ Can you provide a link to the page to checkout the API. Other then that\
          \ I do have an idea to make it work using transformers.\n\nHey @RafayPunch,\
          \ you mentioned that you know how to tackle this problem using transformers.\
          \ Ive been stuck on the same issue and haven't found a solution for it yet.\
          \ If you have, could you please explain how you did it."
        updatedAt: '2023-09-11T05:39:24.354Z'
      numEdits: 0
      reactions: []
    id: 64fea80c35a7fc7d4f6f1f66
    type: comment
  author: Yashqual
  content: "> > > Hey. I have been exploring BLOOM and its API closely. I have learned\
    \ how the parameters effect the response. After I am getting the responses I usually\
    \ process it and remove garbage token model has produced.\n> > > Is there any\
    \ way that BLOOM can stop producing more token if the context of sentence is completed.\
    \ \n> > > GPT-3 basically stops itself when the context of sentence is completed\
    \ or the prompt is answered.\n> > > But BLOOM produce extra tokens just to complete\
    \ the max_token length provided.\n> > > Is there any way BLOOM can stop producing\
    \ tokens once context is complete just like GPT-3\n> > > \n> > > For Example:\
    \ What is Machine Learning? \n> > > BLOOM will answer it fine with first 2 or\
    \ 3 sentences but produce random text(garbage - it may be some code - regular\
    \ expression formulas)\n> > \n> > You may want to try [BLOOMZ](https://huggingface.co/bigscience/bloomz),\
    \ which stops by itself when it deems the question to be answered.\n> > Questions\
    \ like `What is Machine Learning?` should work quite well.\n> \n> Thanks @Muennighoff\
    \ \n> I cannot really find Inference API to BLOOMZ? Can you provide a link to\
    \ the page to checkout the API. Other then that I do have an idea to make it work\
    \ using transformers.\n\nHey @RafayPunch, you mentioned that you know how to tackle\
    \ this problem using transformers. Ive been stuck on the same issue and haven't\
    \ found a solution for it yet. If you have, could you please explain how you did\
    \ it."
  created_at: 2023-09-11 04:39:24+00:00
  edited: false
  hidden: false
  id: 64fea80c35a7fc7d4f6f1f66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1630033724590-612856f6875296178eccf491.jpeg?w=200&h=200&f=face
      fullname: rin2401
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rin2401
      type: user
    createdAt: '2023-10-13T03:26:10.000Z'
    data:
      edited: false
      editors:
      - rin2401
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6909502148628235
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1630033724590-612856f6875296178eccf491.jpeg?w=200&h=200&f=face
          fullname: rin2401
          isHf: false
          isPro: false
          name: rin2401
          type: user
        html: '<p>Where i can view tensorboard of tr13 experiments?<br><a rel="nofollow"
          href="https://github.com/bigscience-workshop/bigscience/blob/master/train/tr13-mtf/tr13-176B-mtf-xp3mt.slurm">https://github.com/bigscience-workshop/bigscience/blob/master/train/tr13-mtf/tr13-176B-mtf-xp3mt.slurm</a></p>

          '
        raw: 'Where i can view tensorboard of tr13 experiments?

          https://github.com/bigscience-workshop/bigscience/blob/master/train/tr13-mtf/tr13-176B-mtf-xp3mt.slurm'
        updatedAt: '2023-10-13T03:26:10.483Z'
      numEdits: 0
      reactions: []
    id: 6528b8d2fbcbed896a5cd0e2
    type: comment
  author: rin2401
  content: 'Where i can view tensorboard of tr13 experiments?

    https://github.com/bigscience-workshop/bigscience/blob/master/train/tr13-mtf/tr13-176B-mtf-xp3mt.slurm'
  created_at: 2023-10-13 02:26:10+00:00
  edited: false
  hidden: false
  id: 6528b8d2fbcbed896a5cd0e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653cb1e63bd61358050e9bc5/PCFSc1A9wVdbjRTK6NsSj.jpeg?w=200&h=200&f=face
      fullname: Rukaiya Hasan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rukaiyaaaah
      type: user
    createdAt: '2023-11-13T15:37:15.000Z'
    data:
      edited: false
      editors:
      - rukaiyaaaah
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9448468089103699
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653cb1e63bd61358050e9bc5/PCFSc1A9wVdbjRTK6NsSj.jpeg?w=200&h=200&f=face
          fullname: Rukaiya Hasan
          isHf: false
          isPro: false
          name: rukaiyaaaah
          type: user
        html: '<p>I want to use bloomz-7b1-mt version of the model and make it more
          ChatGPT like for my language Punjabi. Is there a way I can shred off tokenizers
          and embeddings for languages other than the one I want, since it can be
          done for mt5 which reduced the model size by more than half. Also, are there
          any smaller versions of this model coming soon since I dont have access
          to a cluster of GPUs.</p>

          <p>I have seen multiple tutorials on using the QLORA and PEFT techniques
          to fine-tune many 7B parameter models but they dont seem to work for this
          one here. I want to fine-tune it using a free version on colab and I dont
          want it to take much space, can anyone please help?</p>

          '
        raw: 'I want to use bloomz-7b1-mt version of the model and make it more ChatGPT
          like for my language Punjabi. Is there a way I can shred off tokenizers
          and embeddings for languages other than the one I want, since it can be
          done for mt5 which reduced the model size by more than half. Also, are there
          any smaller versions of this model coming soon since I dont have access
          to a cluster of GPUs.


          I have seen multiple tutorials on using the QLORA and PEFT techniques to
          fine-tune many 7B parameter models but they dont seem to work for this one
          here. I want to fine-tune it using a free version on colab and I dont want
          it to take much space, can anyone please help?


          '
        updatedAt: '2023-11-13T15:37:15.481Z'
      numEdits: 0
      reactions: []
    id: 655242ab7c71b44c30efc86d
    type: comment
  author: rukaiyaaaah
  content: 'I want to use bloomz-7b1-mt version of the model and make it more ChatGPT
    like for my language Punjabi. Is there a way I can shred off tokenizers and embeddings
    for languages other than the one I want, since it can be done for mt5 which reduced
    the model size by more than half. Also, are there any smaller versions of this
    model coming soon since I dont have access to a cluster of GPUs.


    I have seen multiple tutorials on using the QLORA and PEFT techniques to fine-tune
    many 7B parameter models but they dont seem to work for this one here. I want
    to fine-tune it using a free version on colab and I dont want it to take much
    space, can anyone please help?


    '
  created_at: 2023-11-13 15:37:15+00:00
  edited: false
  hidden: false
  id: 655242ab7c71b44c30efc86d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-11-13T17:05:53.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6593834757804871
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>You can try e.g. <a href="https://huggingface.co/bigscience/bloomz-3b">https://huggingface.co/bigscience/bloomz-3b</a>
          or <a href="https://huggingface.co/bigscience/bloomz-3b">https://huggingface.co/bigscience/bloomz-3b</a>
          or <a href="https://huggingface.co/bigscience/bloomz-1b7">https://huggingface.co/bigscience/bloomz-1b7</a>
          which should be similar to bloomz-7b1-mt. For <code>-mt</code>, 7b1 is the
          smallest one though</p>

          '
        raw: You can try e.g. https://huggingface.co/bigscience/bloomz-3b or https://huggingface.co/bigscience/bloomz-3b
          or https://huggingface.co/bigscience/bloomz-1b7 which should be similar
          to bloomz-7b1-mt. For `-mt`, 7b1 is the smallest one though
        updatedAt: '2023-11-13T17:05:53.243Z'
      numEdits: 0
      reactions: []
    id: 65525771f3625761c0fcb0b7
    type: comment
  author: Muennighoff
  content: You can try e.g. https://huggingface.co/bigscience/bloomz-3b or https://huggingface.co/bigscience/bloomz-3b
    or https://huggingface.co/bigscience/bloomz-1b7 which should be similar to bloomz-7b1-mt.
    For `-mt`, 7b1 is the smallest one though
  created_at: 2023-11-13 17:05:53+00:00
  edited: false
  hidden: false
  id: 65525771f3625761c0fcb0b7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 46
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Fine-tune the model?
