!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Siddharth63
conflicting_files: null
created_at: 2022-11-11 09:47:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6f68d0fba68ac649ec62a0a20dfaf05.svg
      fullname: Siddharth Deshpande
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Siddharth63
      type: user
    createdAt: '2022-11-11T09:47:35.000Z'
    data:
      edited: false
      editors:
      - Siddharth63
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6f68d0fba68ac649ec62a0a20dfaf05.svg
          fullname: Siddharth Deshpande
          isHf: false
          isPro: false
          name: Siddharth63
          type: user
        html: '<p>I want to use an already pretrained bloom model and fine-tune (continue
          training) it on my custom biomedical dataset. Has anyone solved it  and
          share a link to the script to do this finetuning?</p>

          '
        raw: I want to use an already pretrained bloom model and fine-tune (continue
          training) it on my custom biomedical dataset. Has anyone solved it  and
          share a link to the script to do this finetuning?
        updatedAt: '2022-11-11T09:47:35.357Z'
      numEdits: 0
      reactions: []
    id: 636e1a37c8bbb1fcb37b85eb
    type: comment
  author: Siddharth63
  content: I want to use an already pretrained bloom model and fine-tune (continue
    training) it on my custom biomedical dataset. Has anyone solved it  and share
    a link to the script to do this finetuning?
  created_at: 2022-11-11 09:47:35+00:00
  edited: false
  hidden: false
  id: 636e1a37c8bbb1fcb37b85eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-14T23:53:33.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Siddharth63&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Siddharth63\"\
          >@<span class=\"underline\">Siddharth63</span></a></span>\n\n\t</span></span>\
          \ ! If you want to use Megatron-DeepSpeed, we were able to do it (typically\
          \ that's how we built <a href=\"https://huggingface.co/bigscience/bloomz\"\
          >BLOOMZ</a>, there's a README in the GH repo <a rel=\"nofollow\" href=\"\
          https://github.com/bigscience-workshop/xmtf\">https://github.com/bigscience-workshop/xmtf</a>).\
          \ Otherwise I'd suggest looking at this: <a href=\"https://huggingface.co/bigscience/bloom/discussions/46\"\
          >https://huggingface.co/bigscience/bloom/discussions/46</a></p>\n<p>Closing\
          \ as this seems to be a duplicate of <a href=\"https://huggingface.co/bigscience/bloom/discussions/46\"\
          >https://huggingface.co/bigscience/bloom/discussions/46</a>. Feel free to\
          \ re-open if you think I mistakenly closed it.</p>\n"
        raw: 'Hi @Siddharth63 ! If you want to use Megatron-DeepSpeed, we were able
          to do it (typically that''s how we built [BLOOMZ](https://huggingface.co/bigscience/bloomz),
          there''s a README in the GH repo https://github.com/bigscience-workshop/xmtf).
          Otherwise I''d suggest looking at this: https://huggingface.co/bigscience/bloom/discussions/46


          Closing as this seems to be a duplicate of https://huggingface.co/bigscience/bloom/discussions/46.
          Feel free to re-open if you think I mistakenly closed it.'
        updatedAt: '2022-11-14T23:53:33.206Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6372d4fdf7e99092f9079deb
    id: 6372d4fdf7e99092f9079dea
    type: comment
  author: TimeRobber
  content: 'Hi @Siddharth63 ! If you want to use Megatron-DeepSpeed, we were able
    to do it (typically that''s how we built [BLOOMZ](https://huggingface.co/bigscience/bloomz),
    there''s a README in the GH repo https://github.com/bigscience-workshop/xmtf).
    Otherwise I''d suggest looking at this: https://huggingface.co/bigscience/bloom/discussions/46


    Closing as this seems to be a duplicate of https://huggingface.co/bigscience/bloom/discussions/46.
    Feel free to re-open if you think I mistakenly closed it.'
  created_at: 2022-11-14 23:53:33+00:00
  edited: false
  hidden: false
  id: 6372d4fdf7e99092f9079dea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-14T23:53:33.000Z'
    data:
      status: closed
    id: 6372d4fdf7e99092f9079deb
    type: status-change
  author: TimeRobber
  created_at: 2022-11-14 23:53:33+00:00
  id: 6372d4fdf7e99092f9079deb
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665398834110-noauth.png?w=200&h=200&f=face
      fullname: Zhang ning
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pe65374
      type: user
    createdAt: '2023-05-23T16:06:57.000Z'
    data:
      edited: false
      editors:
      - pe65374
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665398834110-noauth.png?w=200&h=200&f=face
          fullname: Zhang ning
          isHf: false
          isPro: false
          name: pe65374
          type: user
        html: '<p>I guess Siddharth63 prefer continuous pretraining instead of finetune?
          <a href="https://huggingface.co/bigscience/bloom/discussions/46">https://huggingface.co/bigscience/bloom/discussions/46</a>
          is more likely finetuning discussion thread.</p>

          '
        raw: I guess Siddharth63 prefer continuous pretraining instead of finetune?
          https://huggingface.co/bigscience/bloom/discussions/46 is more likely finetuning
          discussion thread.
        updatedAt: '2023-05-23T16:06:57.431Z'
      numEdits: 0
      reactions: []
    id: 646ce4a1e0c5e395734b6052
    type: comment
  author: pe65374
  content: I guess Siddharth63 prefer continuous pretraining instead of finetune?
    https://huggingface.co/bigscience/bloom/discussions/46 is more likely finetuning
    discussion thread.
  created_at: 2023-05-23 15:06:57+00:00
  edited: false
  hidden: false
  id: 646ce4a1e0c5e395734b6052
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 139
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: 'Continuous training pre-trained Bloom on custom biomedical dataset '
