!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pai4451
conflicting_files: null
created_at: 2022-09-09 03:42:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-09-09T04:42:25.000Z'
    data:
      edited: true
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: '<p>I want to know why the <a href="https://huggingface.co/bigscience/bloom">hosted
          inference API</a> for BLOOM with the interactive playground on HuggingFace
          is so fast. On my hardware and just like many other people reported in the
          inference benchmarks, the inference speed is slow with HuggingFace <code>accelerate</code>.
          The throughput on 8x A100 with the HuggingFace framework in this <a rel="nofollow"
          href="https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/bloom-inference-scripts">link</a>
          is about four tokens per second under batch size 1 (230ms per token). But
          I feel like the HuggingFace interactive playground is way faster than this.
          </p>

          <p>Any tips for doing the inference faster as the Huggingface hosted API?
          Is the hosted inference API a quantized version of BLOOM (for example, the
          int8 version) or the runtime is powered by a different framework such as
          Microsoft <code>DeepSpeed</code>?</p>

          '
        raw: "I want to know why the [hosted inference API](https://huggingface.co/bigscience/bloom)\
          \ for BLOOM with the interactive playground on HuggingFace is so fast. On\
          \ my hardware and just like many other people reported in the inference\
          \ benchmarks, the inference speed is slow with HuggingFace `accelerate`.\
          \ The throughput on 8x A100 with the HuggingFace framework in this [link](https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/bloom-inference-scripts)\
          \ is about four tokens per second under batch size 1 (230ms per token).\
          \ But I feel like the HuggingFace interactive playground is way faster than\
          \ this. \n\nAny tips for doing the inference faster as the Huggingface hosted\
          \ API? Is the hosted inference API a quantized version of BLOOM (for example,\
          \ the int8 version) or the runtime is powered by a different framework such\
          \ as Microsoft `DeepSpeed`?"
        updatedAt: '2022-09-15T02:21:05.235Z'
      numEdits: 5
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sleven
    id: 631ac431223d36c57074cac2
    type: comment
  author: pai4451
  content: "I want to know why the [hosted inference API](https://huggingface.co/bigscience/bloom)\
    \ for BLOOM with the interactive playground on HuggingFace is so fast. On my hardware\
    \ and just like many other people reported in the inference benchmarks, the inference\
    \ speed is slow with HuggingFace `accelerate`. The throughput on 8x A100 with\
    \ the HuggingFace framework in this [link](https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/bloom-inference-scripts)\
    \ is about four tokens per second under batch size 1 (230ms per token). But I\
    \ feel like the HuggingFace interactive playground is way faster than this. \n\
    \nAny tips for doing the inference faster as the Huggingface hosted API? Is the\
    \ hosted inference API a quantized version of BLOOM (for example, the int8 version)\
    \ or the runtime is powered by a different framework such as Microsoft `DeepSpeed`?"
  created_at: 2022-09-09 03:42:25+00:00
  edited: true
  hidden: false
  id: 631ac431223d36c57074cac2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2022-09-09T08:35:48.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p>There <em>might</em> be some secret sauce, cc <span data-props=\"\
          {&quot;user&quot;:&quot;narsil&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/narsil\">@<span class=\"underline\">narsil</span></a></span>\n\
          \n\t</span></span> @nouamane and others =)</p>\n"
        raw: There _might_ be some secret sauce, cc @narsil @nouamane and others =)
        updatedAt: '2022-09-09T08:35:48.232Z'
      numEdits: 0
      reactions: []
    id: 631afae4e73836cc3559ada7
    type: comment
  author: julien-c
  content: There _might_ be some secret sauce, cc @narsil @nouamane and others =)
  created_at: 2022-09-09 07:35:48+00:00
  edited: false
  hidden: false
  id: 631afae4e73836cc3559ada7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-09-09T14:20:25.000Z'
    data:
      status: closed
    id: 631b4ba9bf1351ed2bcea865
    type: status-change
  author: pai4451
  created_at: 2022-09-09 13:20:25+00:00
  id: 631b4ba9bf1351ed2bcea865
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-09-09T14:21:00.000Z'
    data:
      status: open
    id: 631b4bcc824f2502e35072e6
    type: status-change
  author: pai4451
  created_at: 2022-09-09 13:21:00+00:00
  id: 631b4bcc824f2502e35072e6
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594311341799-5f07383b19cb630495b812cd.jpeg?w=200&h=200&f=face
      fullname: Stas Bekman
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stas
      type: user
    createdAt: '2022-09-09T15:22:08.000Z'
    data:
      edited: false
      editors:
      - stas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594311341799-5f07383b19cb630495b812cd.jpeg?w=200&h=200&f=face
          fullname: Stas Bekman
          isHf: false
          isPro: false
          name: stas
          type: user
        html: '<p>That custom server solution is still being worked on and will be
          released when ready.</p>

          <p>Meanwhile you can use Deepspeed-Inference fused-custom-kernel solution
          which is on par or even faster depending on the setup. </p>

          <p>Please see the benchmarks and demo scripts here:<br><a rel="nofollow"
          href="https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/bloom-inference-scripts">https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/bloom-inference-scripts</a></p>

          <p>An alternative server solution by one of the external contributors is
          being developed here:<br><a rel="nofollow" href="https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/bloom-inference-server">https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/bloom-inference-server</a><br>(this
          is not the code used by HF you''re referring to, but should be about the
          same speed)</p>

          '
        raw: "That custom server solution is still being worked on and will be released\
          \ when ready.\n\nMeanwhile you can use Deepspeed-Inference fused-custom-kernel\
          \ solution which is on par or even faster depending on the setup. \n\nPlease\
          \ see the benchmarks and demo scripts here:\nhttps://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/bloom-inference-scripts\n\
          \nAn alternative server solution by one of the external contributors is\
          \ being developed here:\nhttps://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/bloom-inference-server\n\
          (this is not the code used by HF you're referring to, but should be about\
          \ the same speed)"
        updatedAt: '2022-09-09T15:22:08.894Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - julien-c
        - nouamanetazi
    id: 631b5a204aa47d6afbdd9a02
    type: comment
  author: stas
  content: "That custom server solution is still being worked on and will be released\
    \ when ready.\n\nMeanwhile you can use Deepspeed-Inference fused-custom-kernel\
    \ solution which is on par or even faster depending on the setup. \n\nPlease see\
    \ the benchmarks and demo scripts here:\nhttps://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/bloom-inference-scripts\n\
    \nAn alternative server solution by one of the external contributors is being\
    \ developed here:\nhttps://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/scripts/bloom-inference-server\n\
    (this is not the code used by HF you're referring to, but should be about the\
    \ same speed)"
  created_at: 2022-09-09 14:22:08+00:00
  edited: false
  hidden: false
  id: 631b5a204aa47d6afbdd9a02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2022-09-14T10:40:04.000Z'
    data:
      edited: true
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: "<p>As mentionned by <span data-props=\"{&quot;user&quot;:&quot;stas&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/stas\"\
          >@<span class=\"underline\">stas</span></a></span>\n\n\t</span></span> ,\
          \ we're looking to port back our hacky solutions into cleaner open sourceable\
          \ code. It might take time, some of this are not easy to integrate into\
          \ our libs, or necessitate bigger changes that the hack took time to write.</p>\n\
          <p>If you're willing to run your own thing, we're more than willing to help.</p>\n\
          <p>1/ The first thing is to run the model in TP (Tensor parallelism). Deepspeed\
          \ does that, but we don't use deepspeed (because we were seeing too many\
          \ random crashes at runtime because of some unknown issues in the kernels).<br>You\
          \ need to hack the modeling code to handle <code>ProcessGroup</code> from\
          \ torch distributed, and rewrite the Linear layers to be column linear and\
          \ row linear on both attention and MLP (there are other ways to solve this,\
          \ but this is what we went for).<br>That should lower your latency to around\
          \ 90ms for 1 token (when using past) (less if using only 8A100 (80) we're\
          \ using 16xA100(40)) per token (Sorry I'm counting like this because token/s\
          \ is inverted and so it's harder to count performance increase with inverted\
          \ numbers like fps) </p>\n<p>2/ Easy optimization add <code>torch.jit.script</code>\
          \ around <code>gelu</code>. That's a ~10% speed increase, pretty free.</p>\n\
          <p>3/ Create a custom kernel for the attention + softmax (it's more involved)\
          \ that's another 10% speed increase</p>\n<p>For all these first 3 steps,\
          \ you can check out <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/tree/thomas/add_custom_kernels\"\
          >https://github.com/huggingface/transformers/tree/thomas/add_custom_kernels</a>\
          \ which is public, but there's no doc coming with that :).</p>\n<p>4/ Once\
          \ you get this correct, then you need to figure out a serving option. The\
          \ biggest challenging is serving different requests with different parameters\
          \ in the same batch. Some might require 10 tokens, some 200 tokens, some\
          \ might be using sampling, others greedy and different sampling parameters.\
          \ In order to server all uses cases while still batching (batching is free\
          \ 8A100 is a lot more compute than necessary even for bloom on small batch\
          \ sizes) you need to drop the simple code, and basically rewrite <code>generate</code>\
          \ using the low level <code>LogitsProcessor</code> and <code>StoppingCriteria</code>\
          \ logic. Then you can apply that generation logic differently on all different\
          \ items in the batch (the trickier thing is handling the past correctly).\
          \ This adds some overhead (like ~5%) but it does provide a much better latency\
          \ guarantees on API users that might be using the API differently at the\
          \ same  time. Depending on your use case you might not need that. We use\
          \ redis in pub/sub mode to get a simple webserver to actually distribute\
          \ the requests over the n processes which are handling the various GPUs\
          \ for instance (and answering back the generated content)</p>\n<p>Having\
          \ explained all the steps we went through you might understand better why\
          \ it's not already within our libs as some of this stuff requires to be\
          \ heavily more involved than what <code>transformers</code> aims to be (which\
          \ is simple to use and to hack). So we're really trying to figure out a\
          \ way to keep the simplicity where it should be, while enabling others to\
          \ use all those techniques should they want to.</p>\n<p>Also I'm not even\
          \ mentionning running custom weights to speed up loading times from ~10mn+\
          \ to 45s (which helps tremendously when iterating on such a large model)</p>\n"
        raw: "As mentionned by @stas , we're looking to port back our hacky solutions\
          \ into cleaner open sourceable code. It might take time, some of this are\
          \ not easy to integrate into our libs, or necessitate bigger changes that\
          \ the hack took time to write.\n\nIf you're willing to run your own thing,\
          \ we're more than willing to help.\n\n1/ The first thing is to run the model\
          \ in TP (Tensor parallelism). Deepspeed does that, but we don't use deepspeed\
          \ (because we were seeing too many random crashes at runtime because of\
          \ some unknown issues in the kernels).\nYou need to hack the modeling code\
          \ to handle `ProcessGroup` from torch distributed, and rewrite the Linear\
          \ layers to be column linear and row linear on both attention and MLP (there\
          \ are other ways to solve this, but this is what we went for).\nThat should\
          \ lower your latency to around 90ms for 1 token (when using past) (less\
          \ if using only 8A100 (80) we're using 16xA100(40)) per token (Sorry I'm\
          \ counting like this because token/s is inverted and so it's harder to count\
          \ performance increase with inverted numbers like fps) \n\n2/ Easy optimization\
          \ add `torch.jit.script` around `gelu`. That's a ~10% speed increase, pretty\
          \ free.\n\n3/ Create a custom kernel for the attention + softmax (it's more\
          \ involved) that's another 10% speed increase\n\nFor all these first 3 steps,\
          \ you can check out https://github.com/huggingface/transformers/tree/thomas/add_custom_kernels\
          \ which is public, but there's no doc coming with that :).\n\n4/ Once you\
          \ get this correct, then you need to figure out a serving option. The biggest\
          \ challenging is serving different requests with different parameters in\
          \ the same batch. Some might require 10 tokens, some 200 tokens, some might\
          \ be using sampling, others greedy and different sampling parameters. In\
          \ order to server all uses cases while still batching (batching is free\
          \ 8A100 is a lot more compute than necessary even for bloom on small batch\
          \ sizes) you need to drop the simple code, and basically rewrite `generate`\
          \ using the low level `LogitsProcessor` and `StoppingCriteria` logic. Then\
          \ you can apply that generation logic differently on all different items\
          \ in the batch (the trickier thing is handling the past correctly). This\
          \ adds some overhead (like ~5%) but it does provide a much better latency\
          \ guarantees on API users that might be using the API differently at the\
          \ same  time. Depending on your use case you might not need that. We use\
          \ redis in pub/sub mode to get a simple webserver to actually distribute\
          \ the requests over the n processes which are handling the various GPUs\
          \ for instance (and answering back the generated content)\n\nHaving explained\
          \ all the steps we went through you might understand better why it's not\
          \ already within our libs as some of this stuff requires to be heavily more\
          \ involved than what `transformers` aims to be (which is simple to use and\
          \ to hack). So we're really trying to figure out a way to keep the simplicity\
          \ where it should be, while enabling others to use all those techniques\
          \ should they want to.\n\nAlso I'm not even mentionning running custom weights\
          \ to speed up loading times from ~10mn+ to 45s (which helps tremendously\
          \ when iterating on such a large model)"
        updatedAt: '2022-09-16T09:52:11.576Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - pai4451
        - sleven
        - nouamanetazi
    id: 6321af84a97afe3c4c648bfd
    type: comment
  author: Narsil
  content: "As mentionned by @stas , we're looking to port back our hacky solutions\
    \ into cleaner open sourceable code. It might take time, some of this are not\
    \ easy to integrate into our libs, or necessitate bigger changes that the hack\
    \ took time to write.\n\nIf you're willing to run your own thing, we're more than\
    \ willing to help.\n\n1/ The first thing is to run the model in TP (Tensor parallelism).\
    \ Deepspeed does that, but we don't use deepspeed (because we were seeing too\
    \ many random crashes at runtime because of some unknown issues in the kernels).\n\
    You need to hack the modeling code to handle `ProcessGroup` from torch distributed,\
    \ and rewrite the Linear layers to be column linear and row linear on both attention\
    \ and MLP (there are other ways to solve this, but this is what we went for).\n\
    That should lower your latency to around 90ms for 1 token (when using past) (less\
    \ if using only 8A100 (80) we're using 16xA100(40)) per token (Sorry I'm counting\
    \ like this because token/s is inverted and so it's harder to count performance\
    \ increase with inverted numbers like fps) \n\n2/ Easy optimization add `torch.jit.script`\
    \ around `gelu`. That's a ~10% speed increase, pretty free.\n\n3/ Create a custom\
    \ kernel for the attention + softmax (it's more involved) that's another 10% speed\
    \ increase\n\nFor all these first 3 steps, you can check out https://github.com/huggingface/transformers/tree/thomas/add_custom_kernels\
    \ which is public, but there's no doc coming with that :).\n\n4/ Once you get\
    \ this correct, then you need to figure out a serving option. The biggest challenging\
    \ is serving different requests with different parameters in the same batch. Some\
    \ might require 10 tokens, some 200 tokens, some might be using sampling, others\
    \ greedy and different sampling parameters. In order to server all uses cases\
    \ while still batching (batching is free 8A100 is a lot more compute than necessary\
    \ even for bloom on small batch sizes) you need to drop the simple code, and basically\
    \ rewrite `generate` using the low level `LogitsProcessor` and `StoppingCriteria`\
    \ logic. Then you can apply that generation logic differently on all different\
    \ items in the batch (the trickier thing is handling the past correctly). This\
    \ adds some overhead (like ~5%) but it does provide a much better latency guarantees\
    \ on API users that might be using the API differently at the same  time. Depending\
    \ on your use case you might not need that. We use redis in pub/sub mode to get\
    \ a simple webserver to actually distribute the requests over the n processes\
    \ which are handling the various GPUs for instance (and answering back the generated\
    \ content)\n\nHaving explained all the steps we went through you might understand\
    \ better why it's not already within our libs as some of this stuff requires to\
    \ be heavily more involved than what `transformers` aims to be (which is simple\
    \ to use and to hack). So we're really trying to figure out a way to keep the\
    \ simplicity where it should be, while enabling others to use all those techniques\
    \ should they want to.\n\nAlso I'm not even mentionning running custom weights\
    \ to speed up loading times from ~10mn+ to 45s (which helps tremendously when\
    \ iterating on such a large model)"
  created_at: 2022-09-14 09:40:04+00:00
  edited: true
  hidden: false
  id: 6321af84a97afe3c4c648bfd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-09-14T11:06:17.000Z'
    data:
      edited: false
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Narsil\">@<span class=\"\
          underline\">Narsil</span></a></span>\n\n\t</span></span> Hey, thank you\
          \ for sharing so much detail, and I will try some of them on my own. Also\
          \ excited to wait for the open-source solution someday.</p>\n"
        raw: '@Narsil Hey, thank you for sharing so much detail, and I will try some
          of them on my own. Also excited to wait for the open-source solution someday.'
        updatedAt: '2022-09-14T11:06:17.220Z'
      numEdits: 0
      reactions: []
    id: 6321b5a995d6f717a8c12256
    type: comment
  author: pai4451
  content: '@Narsil Hey, thank you for sharing so much detail, and I will try some
    of them on my own. Also excited to wait for the open-source solution someday.'
  created_at: 2022-09-14 10:06:17+00:00
  edited: false
  hidden: false
  id: 6321b5a995d6f717a8c12256
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-09-14T11:38:19.000Z'
    data:
      edited: false
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: "<p>16xA100(40)<br><span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Narsil\"\
          >@<span class=\"underline\">Narsil</span></a></span>\n\n\t</span></span>\
          \ thanks for your good sharing. I would like to know \"16xA100(40)\" you\
          \ mentioned all in single node? Actually we have 16xA6000(48GB), but unfortunately\
          \ they are 2 nodes. It prevents us using \"transformers + accelerate\".\
          \ I would like to know your machine by nature installed 16 GPUs or you adopted\
          \ some kind of virtual machine technology to group 16 GPUs together. Thanks\
          \ in advance.</p>\n"
        raw: '16xA100(40)

          @Narsil thanks for your good sharing. I would like to know "16xA100(40)"
          you mentioned all in single node? Actually we have 16xA6000(48GB), but unfortunately
          they are 2 nodes. It prevents us using "transformers + accelerate". I would
          like to know your machine by nature installed 16 GPUs or you adopted some
          kind of virtual machine technology to group 16 GPUs together. Thanks in
          advance.'
        updatedAt: '2022-09-14T11:38:19.924Z'
      numEdits: 0
      reactions: []
    id: 6321bd2be4399dd613961288
    type: comment
  author: pohunghuang
  content: '16xA100(40)

    @Narsil thanks for your good sharing. I would like to know "16xA100(40)" you mentioned
    all in single node? Actually we have 16xA6000(48GB), but unfortunately they are
    2 nodes. It prevents us using "transformers + accelerate". I would like to know
    your machine by nature installed 16 GPUs or you adopted some kind of virtual machine
    technology to group 16 GPUs together. Thanks in advance.'
  created_at: 2022-09-14 10:38:19+00:00
  edited: false
  hidden: false
  id: 6321bd2be4399dd613961288
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594311341799-5f07383b19cb630495b812cd.jpeg?w=200&h=200&f=face
      fullname: Stas Bekman
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: stas
      type: user
    createdAt: '2022-09-15T21:41:50.000Z'
    data:
      edited: false
      editors:
      - stas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594311341799-5f07383b19cb630495b812cd.jpeg?w=200&h=200&f=face
          fullname: Stas Bekman
          isHf: false
          isPro: false
          name: stas
          type: user
        html: '<p>FYI, we finally re-organized the demo scripts and you can read about
          those here: <a href="https://huggingface.co/blog/bloom-inference-pytorch-scripts">https://huggingface.co/blog/bloom-inference-pytorch-scripts</a></p>

          <p><a rel="nofollow" href="https://github.com/huggingface/transformers-bloom-inference">https://github.com/huggingface/transformers-bloom-inference</a>
          is now the center-point to gather various fast solutions for BLOOM inference.</p>

          '
        raw: 'FYI, we finally re-organized the demo scripts and you can read about
          those here: https://huggingface.co/blog/bloom-inference-pytorch-scripts


          https://github.com/huggingface/transformers-bloom-inference is now the center-point
          to gather various fast solutions for BLOOM inference.'
        updatedAt: '2022-09-15T21:41:50.588Z'
      numEdits: 0
      reactions: []
    id: 63239c1e155b0e2c44f73645
    type: comment
  author: stas
  content: 'FYI, we finally re-organized the demo scripts and you can read about those
    here: https://huggingface.co/blog/bloom-inference-pytorch-scripts


    https://github.com/huggingface/transformers-bloom-inference is now the center-point
    to gather various fast solutions for BLOOM inference.'
  created_at: 2022-09-15 20:41:50+00:00
  edited: false
  hidden: false
  id: 63239c1e155b0e2c44f73645
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2022-09-16T09:52:48.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<p>| I would like to know your machine by nature installed 16 GPUs<br>We''re
          using an GCP node for now (it was the most convenient at the time of our
          work). GCP provides 16xA100 (40Go) so it''s enough to run bloom.<br>Both
          Azure and AWS provide 8xA100(80Go) which are also enough, but maybe getting
          access is slightly more inconvenient.</p>

          <p>I made a big mistake, bloom is running in TP (Tensor Parallelism) not
          PP (pipeline parallelism, which is accelerate). Sorry for the bad typo.
          :)</p>

          '
        raw: '| I would like to know your machine by nature installed 16 GPUs

          We''re using an GCP node for now (it was the most convenient at the time
          of our work). GCP provides 16xA100 (40Go) so it''s enough to run bloom.

          Both Azure and AWS provide 8xA100(80Go) which are also enough, but maybe
          getting access is slightly more inconvenient.


          I made a big mistake, bloom is running in TP (Tensor Parallelism) not PP
          (pipeline parallelism, which is accelerate). Sorry for the bad typo. :)'
        updatedAt: '2022-09-16T09:52:48.199Z'
      numEdits: 0
      reactions: []
    id: 63244770f49a85e868e04436
    type: comment
  author: Narsil
  content: '| I would like to know your machine by nature installed 16 GPUs

    We''re using an GCP node for now (it was the most convenient at the time of our
    work). GCP provides 16xA100 (40Go) so it''s enough to run bloom.

    Both Azure and AWS provide 8xA100(80Go) which are also enough, but maybe getting
    access is slightly more inconvenient.


    I made a big mistake, bloom is running in TP (Tensor Parallelism) not PP (pipeline
    parallelism, which is accelerate). Sorry for the bad typo. :)'
  created_at: 2022-09-16 08:52:48+00:00
  edited: false
  hidden: false
  id: 63244770f49a85e868e04436
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-11-04T06:37:49.000Z'
    data:
      edited: false
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Narsil\">@<span class=\"\
          underline\">Narsil</span></a></span>\n\n\t</span></span>  We noticed an\
          \ excellent blog sharing from you (<a href=\"https://huggingface.co/blog/bloom-inference-optimization\"\
          >https://huggingface.co/blog/bloom-inference-optimization</a>).<br>We now\
          \ implemented whole thing with architecture of \"bloom-176b + deepspeed\
          \ + ds-mii + torchserve on A6000 x 16\". Besides unpredictable crashes,\
          \ we also faced some trouble because ds-mii not supporting multi-node (even\
          \ we has done some hack work to let it work on multi-node, but fail over\
          \ after process crash is still unresolved.).<br>From your branch, we knew\
          \ the way to replace DeepSpeed tensor-parallelism (<a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/tree/thomas/dirty_bloom_tp\"\
          >https://github.com/huggingface/transformers/tree/thomas/dirty_bloom_tp</a>),\
          \ it's very inspiring, and from the segment \"Webserver part\" , we also\
          \ noticed there also ways to replace DS-MII (\"...we opted to communicate\
          \ raw strings over a Redis pub/sub to distribute the requests to all processes\
          \ at once...\"), but unfortunately it seems no source code link shared in\
          \ this blog.<br>Would you like also kindly share the source about web server\
          \ part? It would help us a lot. thanks in advance.</p>\n"
        raw: "@Narsil  We noticed an excellent blog sharing from you (https://huggingface.co/blog/bloom-inference-optimization).\
          \ \nWe now implemented whole thing with architecture of \"bloom-176b + deepspeed\
          \ + ds-mii + torchserve on A6000 x 16\". Besides unpredictable crashes,\
          \ we also faced some trouble because ds-mii not supporting multi-node (even\
          \ we has done some hack work to let it work on multi-node, but fail over\
          \ after process crash is still unresolved.). \nFrom your branch, we knew\
          \ the way to replace DeepSpeed tensor-parallelism (https://github.com/huggingface/transformers/tree/thomas/dirty_bloom_tp),\
          \ it's very inspiring, and from the segment \"Webserver part\" , we also\
          \ noticed there also ways to replace DS-MII (\"...we opted to communicate\
          \ raw strings over a Redis pub/sub to distribute the requests to all processes\
          \ at once...\"), but unfortunately it seems no source code link shared in\
          \ this blog. \nWould you like also kindly share the source about web server\
          \ part? It would help us a lot. thanks in advance."
        updatedAt: '2022-11-04T06:37:49.223Z'
      numEdits: 0
      reactions: []
    id: 6364b33df2984121734038b8
    type: comment
  author: pohunghuang
  content: "@Narsil  We noticed an excellent blog sharing from you (https://huggingface.co/blog/bloom-inference-optimization).\
    \ \nWe now implemented whole thing with architecture of \"bloom-176b + deepspeed\
    \ + ds-mii + torchserve on A6000 x 16\". Besides unpredictable crashes, we also\
    \ faced some trouble because ds-mii not supporting multi-node (even we has done\
    \ some hack work to let it work on multi-node, but fail over after process crash\
    \ is still unresolved.). \nFrom your branch, we knew the way to replace DeepSpeed\
    \ tensor-parallelism (https://github.com/huggingface/transformers/tree/thomas/dirty_bloom_tp),\
    \ it's very inspiring, and from the segment \"Webserver part\" , we also noticed\
    \ there also ways to replace DS-MII (\"...we opted to communicate raw strings\
    \ over a Redis pub/sub to distribute the requests to all processes at once...\"\
    ), but unfortunately it seems no source code link shared in this blog. \nWould\
    \ you like also kindly share the source about web server part? It would help us\
    \ a lot. thanks in advance."
  created_at: 2022-11-04 05:37:49+00:00
  edited: false
  hidden: false
  id: 6364b33df2984121734038b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2022-11-04T07:02:18.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<p>The simpler version.<br><a rel="nofollow" href="https://github.com/huggingface/transformers_bloom_parallel/">https://github.com/huggingface/transformers_bloom_parallel/</a></p>

          <p>The more complex version which has some cooler features.<br><a rel="nofollow"
          href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a></p>

          '
        raw: 'The simpler version.

          https://github.com/huggingface/transformers_bloom_parallel/


          The more complex version which has some cooler features.

          https://github.com/huggingface/text-generation-inference'
        updatedAt: '2022-11-04T07:02:18.264Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F917"
        users:
        - pohunghuang
        - julien-c
        - rk-swype
      - count: 2
        reaction: "\U0001F44D"
        users:
        - pohunghuang
        - julien-c
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - pohunghuang
        - julien-c
    id: 6364b8fa086df4503e2ddec0
    type: comment
  author: Narsil
  content: 'The simpler version.

    https://github.com/huggingface/transformers_bloom_parallel/


    The more complex version which has some cooler features.

    https://github.com/huggingface/text-generation-inference'
  created_at: 2022-11-04 06:02:18+00:00
  edited: false
  hidden: false
  id: 6364b8fa086df4503e2ddec0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-11-08T07:27:05.000Z'
    data:
      edited: false
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Narsil\">@<span class=\"\
          underline\">Narsil</span></a></span>\n\n\t</span></span>  Sorry for bothering,\
          \ do you have prebuilt wheel file for \"safetensors\". I got some SSL problem\
          \ while \"python setup.py develop\" for this, and difficult to workaround\
          \ it. It's also very appreciate if you could let me know any alternative\
          \ way to skip this part.</p>\n"
        raw: '@Narsil  Sorry for bothering, do you have prebuilt wheel file for "safetensors".
          I got some SSL problem while "python setup.py develop" for this, and difficult
          to workaround it. It''s also very appreciate if you could let me know any
          alternative way to skip this part.'
        updatedAt: '2022-11-08T07:27:05.207Z'
      numEdits: 0
      reactions: []
    id: 636a04c98d250800ca6ddbc4
    type: comment
  author: pohunghuang
  content: '@Narsil  Sorry for bothering, do you have prebuilt wheel file for "safetensors".
    I got some SSL problem while "python setup.py develop" for this, and difficult
    to workaround it. It''s also very appreciate if you could let me know any alternative
    way to skip this part.'
  created_at: 2022-11-08 07:27:05+00:00
  edited: false
  hidden: false
  id: 636a04c98d250800ca6ddbc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2022-11-08T09:47:37.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<p><code>pip install safetensors</code> should work on most platform.</p>

          <p>Please report here if not <a rel="nofollow" href="https://github.com/huggingface/safetensors/issues">https://github.com/huggingface/safetensors/issues</a></p>

          '
        raw: '`pip install safetensors` should work on most platform.


          Please report here if not https://github.com/huggingface/safetensors/issues'
        updatedAt: '2022-11-08T09:47:37.023Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - pohunghuang
        - julien-c
    id: 636a25b9f2f9ec4289c51d24
    type: comment
  author: Narsil
  content: '`pip install safetensors` should work on most platform.


    Please report here if not https://github.com/huggingface/safetensors/issues'
  created_at: 2022-11-08 09:47:37+00:00
  edited: false
  hidden: false
  id: 636a25b9f2f9ec4289c51d24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-11-09T02:57:44.000Z'
    data:
      edited: false
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: "<blockquote>\n<p><code>pip install safetensors</code> should work on\
          \ most platform.</p>\n<p>Please report here if not <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/safetensors/issues\">https://github.com/huggingface/safetensors/issues</a></p>\n\
          </blockquote>\n<p>Yes, \"pip install\" works, thank you very much. <span\
          \ data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/Narsil\">@<span class=\"underline\"\
          >Narsil</span></a></span>\n\n\t</span></span></p>\n"
        raw: "> `pip install safetensors` should work on most platform.\n> \n> Please\
          \ report here if not https://github.com/huggingface/safetensors/issues\n\
          \nYes, \"pip install\" works, thank you very much. @Narsil"
        updatedAt: '2022-11-09T02:57:44.731Z'
      numEdits: 0
      reactions: []
    id: 636b1728328133bdb3a5b226
    type: comment
  author: pohunghuang
  content: "> `pip install safetensors` should work on most platform.\n> \n> Please\
    \ report here if not https://github.com/huggingface/safetensors/issues\n\nYes,\
    \ \"pip install\" works, thank you very much. @Narsil"
  created_at: 2022-11-09 02:57:44+00:00
  edited: false
  hidden: false
  id: 636b1728328133bdb3a5b226
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-11-15T10:57:39.000Z'
    data:
      edited: false
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: "<blockquote>\n<p>The simpler version.<br><a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers_bloom_parallel/\">https://github.com/huggingface/transformers_bloom_parallel/</a></p>\n\
          <p>The more complex version which has some cooler features.<br><a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/text-generation-inference\"\
          >https://github.com/huggingface/text-generation-inference</a></p>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Narsil\">@<span class=\"\
          underline\">Narsil</span></a></span>\n\n\t</span></span> first, thanks for\
          \ your selfless sharing of these source code. We have installed text-generation-inference\
          \ solution on our K8S clusters and explore some of the features. but one\
          \ point I am wondering....<br>It's observed \"streaming response mode\"\
          \ provided in huggingface hosted BLOOM inference API(<a href=\"https://huggingface.co/bigscience/bloom\"\
          >https://huggingface.co/bigscience/bloom</a>), but why I can't see related\
          \ implementation inside of <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference/blob/main/router/src/server.rs\"\
          >\"text-generation-inference/router</a><br>Would you like kindly give me\
          \ some hint of this?</p>\n"
        raw: "> The simpler version.\n> https://github.com/huggingface/transformers_bloom_parallel/\n\
          > \n> The more complex version which has some cooler features.\n> https://github.com/huggingface/text-generation-inference\n\
          \n@Narsil first, thanks for your selfless sharing of these source code.\
          \ We have installed text-generation-inference solution on our K8S clusters\
          \ and explore some of the features. but one point I am wondering....\nIt's\
          \ observed \"streaming response mode\" provided in huggingface hosted BLOOM\
          \ inference API(https://huggingface.co/bigscience/bloom), but why I can't\
          \ see related implementation inside of [\"text-generation-inference/router](https://github.com/huggingface/text-generation-inference/blob/main/router/src/server.rs)\
          \ \nWould you like kindly give me some hint of this?"
        updatedAt: '2022-11-15T10:57:39.765Z'
      numEdits: 0
      reactions: []
    id: 637370a33e75422985b66558
    type: comment
  author: pohunghuang
  content: "> The simpler version.\n> https://github.com/huggingface/transformers_bloom_parallel/\n\
    > \n> The more complex version which has some cooler features.\n> https://github.com/huggingface/text-generation-inference\n\
    \n@Narsil first, thanks for your selfless sharing of these source code. We have\
    \ installed text-generation-inference solution on our K8S clusters and explore\
    \ some of the features. but one point I am wondering....\nIt's observed \"streaming\
    \ response mode\" provided in huggingface hosted BLOOM inference API(https://huggingface.co/bigscience/bloom),\
    \ but why I can't see related implementation inside of [\"text-generation-inference/router](https://github.com/huggingface/text-generation-inference/blob/main/router/src/server.rs)\
    \ \nWould you like kindly give me some hint of this?"
  created_at: 2022-11-15 10:57:39+00:00
  edited: false
  hidden: false
  id: 637370a33e75422985b66558
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2022-11-15T12:00:21.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<blockquote>

          <p>streaming response mode</p>

          </blockquote>

          <p>Not sure what you are referring to ?</p>

          '
        raw: '> streaming response mode


          Not sure what you are referring to ?'
        updatedAt: '2022-11-15T12:00:21.232Z'
      numEdits: 0
      reactions: []
    id: 63737f552d4eccfa6f8f6eeb
    type: comment
  author: Narsil
  content: '> streaming response mode


    Not sure what you are referring to ?'
  created_at: 2022-11-15 12:00:21+00:00
  edited: false
  hidden: false
  id: 63737f552d4eccfa6f8f6eeb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-11-16T04:29:31.000Z'
    data:
      edited: false
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: '<blockquote>

          <blockquote>

          <p>streaming response mode</p>

          </blockquote>

          <p>Not sure what you are referring to ?</p>

          </blockquote>

          <p>It means "output word by word"</p>

          '
        raw: "> > streaming response mode\n> \n> Not sure what you are referring to\
          \ ?\n\nIt means \"output word by word\""
        updatedAt: '2022-11-16T04:29:31.025Z'
      numEdits: 0
      reactions: []
    id: 6374672bd398fce0dd4fb72f
    type: comment
  author: pohunghuang
  content: "> > streaming response mode\n> \n> Not sure what you are referring to\
    \ ?\n\nIt means \"output word by word\""
  created_at: 2022-11-16 04:29:31+00:00
  edited: false
  hidden: false
  id: 6374672bd398fce0dd4fb72f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2022-11-16T08:58:20.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<p>Oh, the widget is simply displayed a being progressive, the actual
          request is made in bulk.</p>

          <p>We considered making streaming responses, but so far we haven''t added
          yet in the public demo and it''s unlikely to come soon (too much engineering
          for the benefit currently).</p>

          '
        raw: 'Oh, the widget is simply displayed a being progressive, the actual request
          is made in bulk.


          We considered making streaming responses, but so far we haven''t added yet
          in the public demo and it''s unlikely to come soon (too much engineering
          for the benefit currently).'
        updatedAt: '2022-11-16T08:58:20.915Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - pohunghuang
        - pai4451
    id: 6374a62c0856ac905bfc9a5b
    type: comment
  author: Narsil
  content: 'Oh, the widget is simply displayed a being progressive, the actual request
    is made in bulk.


    We considered making streaming responses, but so far we haven''t added yet in
    the public demo and it''s unlikely to come soon (too much engineering for the
    benefit currently).'
  created_at: 2022-11-16 08:58:20+00:00
  edited: false
  hidden: false
  id: 6374a62c0856ac905bfc9a5b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 107
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Speed of the hosted inference API for interactive playground
