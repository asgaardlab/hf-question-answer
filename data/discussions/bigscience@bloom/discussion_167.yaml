!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LuvIsBadToTheBone
conflicting_files: null
created_at: 2023-01-10 17:19:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671480241983-noauth.png?w=200&h=200&f=face
      fullname: toni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LuvIsBadToTheBone
      type: user
    createdAt: '2023-01-10T17:19:08.000Z'
    data:
      edited: false
      editors:
      - LuvIsBadToTheBone
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671480241983-noauth.png?w=200&h=200&f=face
          fullname: toni
          isHf: false
          isPro: false
          name: LuvIsBadToTheBone
          type: user
        html: '<p>After the clone attempt failed i tried:</p>

          <p>from transformers import AutoTokenizer, AutoModel</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom")</p>

          <p>model = AutoModel.from_pretrained("bigscience/bloom")</p>

          <p>This eats all Ram + Swap  to 100% after the download has finished then
          get killed by ZSH<br>idk what to do anymore to get bloom running :(</p>

          '
        raw: "After the clone attempt failed i tried:\r\n\r\nfrom transformers import\
          \ AutoTokenizer, AutoModel\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          bigscience/bloom\")\r\n\r\nmodel = AutoModel.from_pretrained(\"bigscience/bloom\"\
          )\r\n\r\nThis eats all Ram + Swap  to 100% after the download has finished\
          \ then get killed by ZSH\r\nidk what to do anymore to get bloom running\
          \ :("
        updatedAt: '2023-01-10T17:19:08.498Z'
      numEdits: 0
      reactions: []
    id: 63bd9e0c9a15a3e941960455
    type: comment
  author: LuvIsBadToTheBone
  content: "After the clone attempt failed i tried:\r\n\r\nfrom transformers import\
    \ AutoTokenizer, AutoModel\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    bigscience/bloom\")\r\n\r\nmodel = AutoModel.from_pretrained(\"bigscience/bloom\"\
    )\r\n\r\nThis eats all Ram + Swap  to 100% after the download has finished then\
    \ get killed by ZSH\r\nidk what to do anymore to get bloom running :("
  created_at: 2023-01-10 17:19:08+00:00
  edited: false
  hidden: false
  id: 63bd9e0c9a15a3e941960455
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
      fullname: Alexander Borzunov
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: borzunov
      type: user
    createdAt: '2023-01-18T01:54:43.000Z'
    data:
      edited: false
      editors:
      - borzunov
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1620133776745-noauth.jpeg?w=200&h=200&f=face
          fullname: Alexander Borzunov
          isHf: false
          isPro: false
          name: borzunov
          type: user
        html: '<p>You can try out Petals: <a rel="nofollow" href="https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing">https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing</a></p>

          <p>Without Petals, you need 176+ GB GPU memory or RAM to run BLOOM at a
          decent speed.</p>

          '
        raw: 'You can try out Petals: https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing


          Without Petals, you need 176+ GB GPU memory or RAM to run BLOOM at a decent
          speed.'
        updatedAt: '2023-01-18T01:54:43.491Z'
      numEdits: 0
      reactions: []
    id: 63c751632f651b676295b614
    type: comment
  author: borzunov
  content: 'You can try out Petals: https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing


    Without Petals, you need 176+ GB GPU memory or RAM to run BLOOM at a decent speed.'
  created_at: 2023-01-18 01:54:43+00:00
  edited: false
  hidden: false
  id: 63c751632f651b676295b614
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671480241983-noauth.png?w=200&h=200&f=face
      fullname: toni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LuvIsBadToTheBone
      type: user
    createdAt: '2023-01-18T10:15:45.000Z'
    data:
      edited: false
      editors:
      - LuvIsBadToTheBone
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671480241983-noauth.png?w=200&h=200&f=face
          fullname: toni
          isHf: false
          isPro: false
          name: LuvIsBadToTheBone
          type: user
        html: '<p>Well, even i try with 374GB Swap, ZSH still kills it becus it occupies
          all memory with the above script.</p>

          '
        raw: Well, even i try with 374GB Swap, ZSH still kills it becus it occupies
          all memory with the above script.
        updatedAt: '2023-01-18T10:15:45.035Z'
      numEdits: 0
      reactions: []
    id: 63c7c6d1b17c7c49d9defb7d
    type: comment
  author: LuvIsBadToTheBone
  content: Well, even i try with 374GB Swap, ZSH still kills it becus it occupies
    all memory with the above script.
  created_at: 2023-01-18 10:15:45+00:00
  edited: false
  hidden: false
  id: 63c7c6d1b17c7c49d9defb7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-01-19T11:46:32.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>We should maybe add a git tag  (let's term it as \"pytorch_only\"\
          ) pointing before the safetensors commit: <a href=\"/bigscience/bloom/commit/4d8e28c67403974b0f17a4ac5992e4ba0b0dbb6f\"\
          >4d8e28c67403974b0f17a4ac5992e4ba0b0dbb6f</a> but not sure if this will\
          \ help - cc <span data-props=\"{&quot;user&quot;:&quot;julien-c&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/julien-c\"\
          >@<span class=\"underline\">julien-c</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\">@<span class=\"\
          underline\">TimeRobber</span></a></span>\n\n\t</span></span>  (maybe the\
          \ safetensors weights will be still downloaded to the cache?)<br>Then you'll\
          \ be able to load the model with:</p>\n<pre><code>from transformers import\
          \ AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          bigscience/bloom\")\n\nmodel = AutoModel.from_pretrained(\"bigscience/bloom\"\
          , revision=\"pytorch_only\")\n</code></pre>\n"
        raw: 'We should maybe add a git tag  (let''s term it as "pytorch_only") pointing
          before the safetensors commit: 4d8e28c67403974b0f17a4ac5992e4ba0b0dbb6f
          but not sure if this will help - cc @julien-c @TimeRobber  (maybe the safetensors
          weights will be still downloaded to the cache?)

          Then you''ll be able to load the model with:

          ```

          from transformers import AutoTokenizer, AutoModel


          tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom")


          model = AutoModel.from_pretrained("bigscience/bloom", revision="pytorch_only")

          ```'
        updatedAt: '2023-01-19T11:49:12.027Z'
      numEdits: 6
      reactions: []
    id: 63c92d981e3e1bf4a2c44a54
    type: comment
  author: ybelkada
  content: 'We should maybe add a git tag  (let''s term it as "pytorch_only") pointing
    before the safetensors commit: 4d8e28c67403974b0f17a4ac5992e4ba0b0dbb6f but not
    sure if this will help - cc @julien-c @TimeRobber  (maybe the safetensors weights
    will be still downloaded to the cache?)

    Then you''ll be able to load the model with:

    ```

    from transformers import AutoTokenizer, AutoModel


    tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom")


    model = AutoModel.from_pretrained("bigscience/bloom", revision="pytorch_only")

    ```'
  created_at: 2023-01-19 11:46:32+00:00
  edited: true
  hidden: false
  id: 63c92d981e3e1bf4a2c44a54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2023-01-19T19:00:39.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Hum you can use <code>huggingface_hub</code> to download specific
          files (which I think <code>from_pretrained</code> already does). I think
          the issue is that the <code>from_pretrained</code> also loads in memory,
          so I think you need to just set meta as the device or offload it to disk
          using accelerate.</p>

          '
        raw: Hum you can use `huggingface_hub` to download specific files (which I
          think `from_pretrained` already does). I think the issue is that the `from_pretrained`
          also loads in memory, so I think you need to just set meta as the device
          or offload it to disk using accelerate.
        updatedAt: '2023-01-19T19:00:39.357Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ybelkada
    id: 63c993573674f6ffd596f9b7
    type: comment
  author: TimeRobber
  content: Hum you can use `huggingface_hub` to download specific files (which I think
    `from_pretrained` already does). I think the issue is that the `from_pretrained`
    also loads in memory, so I think you need to just set meta as the device or offload
    it to disk using accelerate.
  created_at: 2023-01-19 19:00:39+00:00
  edited: false
  hidden: false
  id: 63c993573674f6ffd596f9b7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 167
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Eats up all RAM + 163GB Swap
