!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ioulaum
conflicting_files: null
created_at: 2023-01-09 22:09:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ef1e81a89536b4ba66ce926b5e00b63.svg
      fullname: Khurrum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Ioulaum
      type: user
    createdAt: '2023-01-09T22:09:14.000Z'
    data:
      edited: true
      editors:
      - Ioulaum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ef1e81a89536b4ba66ce926b5e00b63.svg
          fullname: Khurrum
          isHf: false
          isPro: true
          name: Ioulaum
          type: user
        html: '<p>Outside of its refinement using human feedback, it''s quite interesting
          just how much context it can maintain between requests.</p>

          <p>It doesn''t look like it''s looking at the actual text that came before,
          ''cus that can easily exceed the 3K words or something window that it can
          handle by a lot.</p>

          <p>I feel that there has to be some kind of compressed (neural net specific)
          representation of what it has learned so far, that is ideally easy to store
          in a file - because you can''t be sure when a user will continue a past
          discussion where you need to still maintain context - so you obviously can''t
          keep state in memory.</p>

          <p>Kinda makes me think of Neo learning Kung Fu in the Matrix... Human learning
          is beyond words, and you''d need some representation of everything down
          to muscle memory (sorta), to transfer that knowledge of Kung Fu in one go.</p>

          <p>I think they haven''t publicly shared the key advancements that are allowing
          ChatGPT to perform as well as it does.</p>

          '
        raw: 'Outside of its refinement using human feedback, it''s quite interesting
          just how much context it can maintain between requests.


          It doesn''t look like it''s looking at the actual text that came before,
          ''cus that can easily exceed the 3K words or something window that it can
          handle by a lot.


          I feel that there has to be some kind of compressed (neural net specific)
          representation of what it has learned so far, that is ideally easy to store
          in a file - because you can''t be sure when a user will continue a past
          discussion where you need to still maintain context - so you obviously can''t
          keep state in memory.


          Kinda makes me think of Neo learning Kung Fu in the Matrix... Human learning
          is beyond words, and you''d need some representation of everything down
          to muscle memory (sorta), to transfer that knowledge of Kung Fu in one go.


          I think they haven''t publicly shared the key advancements that are allowing
          ChatGPT to perform as well as it does.'
        updatedAt: '2023-01-09T22:10:01.193Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - taniki
    id: 63bc908ab8c61b8aa49cddf6
    type: comment
  author: Ioulaum
  content: 'Outside of its refinement using human feedback, it''s quite interesting
    just how much context it can maintain between requests.


    It doesn''t look like it''s looking at the actual text that came before, ''cus
    that can easily exceed the 3K words or something window that it can handle by
    a lot.


    I feel that there has to be some kind of compressed (neural net specific) representation
    of what it has learned so far, that is ideally easy to store in a file - because
    you can''t be sure when a user will continue a past discussion where you need
    to still maintain context - so you obviously can''t keep state in memory.


    Kinda makes me think of Neo learning Kung Fu in the Matrix... Human learning is
    beyond words, and you''d need some representation of everything down to muscle
    memory (sorta), to transfer that knowledge of Kung Fu in one go.


    I think they haven''t publicly shared the key advancements that are allowing ChatGPT
    to perform as well as it does.'
  created_at: 2023-01-09 22:09:14+00:00
  edited: true
  hidden: false
  id: 63bc908ab8c61b8aa49cddf6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fe84e044996af0a66b5dab97e529a8bf.svg
      fullname: P D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tholos
      type: user
    createdAt: '2023-01-10T00:49:55.000Z'
    data:
      edited: false
      editors:
      - Tholos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fe84e044996af0a66b5dab97e529a8bf.svg
          fullname: P D
          isHf: false
          isPro: false
          name: Tholos
          type: user
        html: '<p>They feed the whole input from current session, just refresh the
          window and you will see it will "forget" everything.</p>

          '
        raw: They feed the whole input from current session, just refresh the window
          and you will see it will "forget" everything.
        updatedAt: '2023-01-10T00:49:55.684Z'
      numEdits: 0
      reactions: []
    id: 63bcb6330718d5f0c1f4f433
    type: comment
  author: Tholos
  content: They feed the whole input from current session, just refresh the window
    and you will see it will "forget" everything.
  created_at: 2023-01-10 00:49:55+00:00
  edited: false
  hidden: false
  id: 63bcb6330718d5f0c1f4f433
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/188065bacb142c1c2c48de1047128cda.svg
      fullname: glauber prado
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: greboide
      type: user
    createdAt: '2023-01-10T06:12:10.000Z'
    data:
      edited: false
      editors:
      - greboide
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/188065bacb142c1c2c48de1047128cda.svg
          fullname: glauber prado
          isHf: false
          isPro: false
          name: greboide
          type: user
        html: '<p>there is gpt_index, perhaps they are using something similar to
          bypass current limitations, have you ever tried counting tokens and asking
          questions related to the beggining of conversation after passing some threshold?</p>

          '
        raw: there is gpt_index, perhaps they are using something similar to bypass
          current limitations, have you ever tried counting tokens and asking questions
          related to the beggining of conversation after passing some threshold?
        updatedAt: '2023-01-10T06:12:10.400Z'
      numEdits: 0
      reactions: []
    id: 63bd01ba0718d5f0c1fa092d
    type: comment
  author: greboide
  content: there is gpt_index, perhaps they are using something similar to bypass
    current limitations, have you ever tried counting tokens and asking questions
    related to the beggining of conversation after passing some threshold?
  created_at: 2023-01-10 06:12:10+00:00
  edited: false
  hidden: false
  id: 63bd01ba0718d5f0c1fa092d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ef1e81a89536b4ba66ce926b5e00b63.svg
      fullname: Khurrum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Ioulaum
      type: user
    createdAt: '2023-01-10T10:54:32.000Z'
    data:
      edited: true
      editors:
      - Ioulaum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ef1e81a89536b4ba66ce926b5e00b63.svg
          fullname: Khurrum
          isHf: false
          isPro: true
          name: Ioulaum
          type: user
        html: '<p>I''ve tried it with up to 20K words in the current context, and
          it still remembered everything fine.<br>I''ve also tried it with other kinds
          of data that would probably have been closer to 100K words if it was words,
          and it still had a good grasp of it.</p>

          <p>But on the other hand, the limit for GPT-3 is 4096 tokens, and I think
          that applies to GPT 3.5 as well. So some workaround is obviously needed.</p>

          <p>gpt_index does sound interesting. The goal seems to be to make a large
          database accessible and then use GPT-3 to answer questions in natural language,
          based on a queried subset of relevant information. That way, GPT-3 wouldn''t
          need to be trained with additional data, and could answer questions related
          to your knowledge base.</p>

          '
        raw: 'I''ve tried it with up to 20K words in the current context, and it still
          remembered everything fine.

          I''ve also tried it with other kinds of data that would probably have been
          closer to 100K words if it was words, and it still had a good grasp of it.


          But on the other hand, the limit for GPT-3 is 4096 tokens, and I think that
          applies to GPT 3.5 as well. So some workaround is obviously needed.


          gpt_index does sound interesting. The goal seems to be to make a large database
          accessible and then use GPT-3 to answer questions in natural language, based
          on a queried subset of relevant information. That way, GPT-3 wouldn''t need
          to be trained with additional data, and could answer questions related to
          your knowledge base.'
        updatedAt: '2023-01-10T16:47:14.646Z'
      numEdits: 3
      reactions: []
    id: 63bd43e8065f9bd92cdd00ae
    type: comment
  author: Ioulaum
  content: 'I''ve tried it with up to 20K words in the current context, and it still
    remembered everything fine.

    I''ve also tried it with other kinds of data that would probably have been closer
    to 100K words if it was words, and it still had a good grasp of it.


    But on the other hand, the limit for GPT-3 is 4096 tokens, and I think that applies
    to GPT 3.5 as well. So some workaround is obviously needed.


    gpt_index does sound interesting. The goal seems to be to make a large database
    accessible and then use GPT-3 to answer questions in natural language, based on
    a queried subset of relevant information. That way, GPT-3 wouldn''t need to be
    trained with additional data, and could answer questions related to your knowledge
    base.'
  created_at: 2023-01-10 10:54:32+00:00
  edited: true
  hidden: false
  id: 63bd43e8065f9bd92cdd00ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/188065bacb142c1c2c48de1047128cda.svg
      fullname: glauber prado
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: greboide
      type: user
    createdAt: '2023-01-11T20:21:47.000Z'
    data:
      edited: false
      editors:
      - greboide
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/188065bacb142c1c2c48de1047128cda.svg
          fullname: glauber prado
          isHf: false
          isPro: false
          name: greboide
          type: user
        html: '<p>I never tried it myself with so much tokens, but if it works it
          is impressive indeed</p>

          '
        raw: I never tried it myself with so much tokens, but if it works it is impressive
          indeed
        updatedAt: '2023-01-11T20:21:47.919Z'
      numEdits: 0
      reactions: []
    id: 63bf1a5b82f7306d074de67c
    type: comment
  author: greboide
  content: I never tried it myself with so much tokens, but if it works it is impressive
    indeed
  created_at: 2023-01-11 20:21:47+00:00
  edited: false
  hidden: false
  id: 63bf1a5b82f7306d074de67c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7181f2c37709fda8559eac3da6b8dfc3.svg
      fullname: Luis Bereciartu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 2dluis
      type: user
    createdAt: '2023-01-24T02:55:03.000Z'
    data:
      edited: false
      editors:
      - 2dluis
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7181f2c37709fda8559eac3da6b8dfc3.svg
          fullname: Luis Bereciartu
          isHf: false
          isPro: false
          name: 2dluis
          type: user
        html: '<p>I have a chat that is over 46K words and whenever I ask it to remember/recall
          it says it cannot but what I do is just write something along the lines
          of "read this entire chat and.." and it has no problem recounting any part
          of the chat or summarizing anything about the chat.</p>

          '
        raw: I have a chat that is over 46K words and whenever I ask it to remember/recall
          it says it cannot but what I do is just write something along the lines
          of "read this entire chat and.." and it has no problem recounting any part
          of the chat or summarizing anything about the chat.
        updatedAt: '2023-01-24T02:55:03.827Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - gaa23
        - Ioulaum
    id: 63cf488714ef989d7a03b611
    type: comment
  author: 2dluis
  content: I have a chat that is over 46K words and whenever I ask it to remember/recall
    it says it cannot but what I do is just write something along the lines of "read
    this entire chat and.." and it has no problem recounting any part of the chat
    or summarizing anything about the chat.
  created_at: 2023-01-24 02:55:03+00:00
  edited: false
  hidden: false
  id: 63cf488714ef989d7a03b611
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7cf5723614f92bde23662d983bc9e81.svg
      fullname: Charles Anifowose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CharlesAnifowose
      type: user
    createdAt: '2023-01-24T11:52:14.000Z'
    data:
      edited: false
      editors:
      - CharlesAnifowose
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7cf5723614f92bde23662d983bc9e81.svg
          fullname: Charles Anifowose
          isHf: false
          isPro: false
          name: CharlesAnifowose
          type: user
        html: '<p>You could get around the input token limitation with a bit of engineering?</p>

          <ul>

          <li>Store conversation so far as a compressed knowledge graph (setup for
          semantic search)</li>

          <li>When you receive a <strong>prompt</strong>, generate search terms for
          the knowledge graph and identify relevant parts</li>

          <li>If too many relevant parts of the conversation are identified, can constrain
          it using (1) ranking by relevance, (2) compression of text, (3) random sampling
          [can weigh the sampling by rank]</li>

          <li>Feed this compressed representation along as a header to the <strong>prompt</strong>.</li>

          </ul>

          <p>I have no idea if ChatGPT is doing any of these things, but it''s one
          possible approach that I am assessing as I continue using the tool and re-evaluating
          my mental model of what it can do.</p>

          '
        raw: 'You could get around the input token limitation with a bit of engineering?


          - Store conversation so far as a compressed knowledge graph (setup for semantic
          search)

          - When you receive a **prompt**, generate search terms for the knowledge
          graph and identify relevant parts

          - If too many relevant parts of the conversation are identified, can constrain
          it using (1) ranking by relevance, (2) compression of text, (3) random sampling
          [can weigh the sampling by rank]

          - Feed this compressed representation along as a header to the **prompt**.


          I have no idea if ChatGPT is doing any of these things, but it''s one possible
          approach that I am assessing as I continue using the tool and re-evaluating
          my mental model of what it can do.'
        updatedAt: '2023-01-24T11:52:14.986Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - gaa23
    id: 63cfc66e44794ae289f0ceea
    type: comment
  author: CharlesAnifowose
  content: 'You could get around the input token limitation with a bit of engineering?


    - Store conversation so far as a compressed knowledge graph (setup for semantic
    search)

    - When you receive a **prompt**, generate search terms for the knowledge graph
    and identify relevant parts

    - If too many relevant parts of the conversation are identified, can constrain
    it using (1) ranking by relevance, (2) compression of text, (3) random sampling
    [can weigh the sampling by rank]

    - Feed this compressed representation along as a header to the **prompt**.


    I have no idea if ChatGPT is doing any of these things, but it''s one possible
    approach that I am assessing as I continue using the tool and re-evaluating my
    mental model of what it can do.'
  created_at: 2023-01-24 11:52:14+00:00
  edited: false
  hidden: false
  id: 63cfc66e44794ae289f0ceea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677317271354-63f9d3f26f55c54f43de63b4.jpeg?w=200&h=200&f=face
      fullname: TDY-RD-1
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TDY-RD-1
      type: user
    createdAt: '2023-02-25T09:44:27.000Z'
    data:
      edited: false
      editors:
      - TDY-RD-1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677317271354-63f9d3f26f55c54f43de63b4.jpeg?w=200&h=200&f=face
          fullname: TDY-RD-1
          isHf: false
          isPro: false
          name: TDY-RD-1
          type: user
        html: '<p>We have had interesting findings on context retention in a recent
          trial (20,310 lines; 1,186,413 positions in Notepad++). How does that size
          compare to typical exploratory single conversation lengths?</p>

          '
        raw: We have had interesting findings on context retention in a recent trial
          (20,310 lines; 1,186,413 positions in Notepad++). How does that size compare
          to typical exploratory single conversation lengths?
        updatedAt: '2023-02-25T09:44:27.341Z'
      numEdits: 0
      reactions: []
    id: 63f9d87b122a67a9d90b6c57
    type: comment
  author: TDY-RD-1
  content: We have had interesting findings on context retention in a recent trial
    (20,310 lines; 1,186,413 positions in Notepad++). How does that size compare to
    typical exploratory single conversation lengths?
  created_at: 2023-02-25 09:44:27+00:00
  edited: false
  hidden: false
  id: 63f9d87b122a67a9d90b6c57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ef1e81a89536b4ba66ce926b5e00b63.svg
      fullname: Khurrum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Ioulaum
      type: user
    createdAt: '2023-02-25T11:40:45.000Z'
    data:
      edited: true
      editors:
      - Ioulaum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ef1e81a89536b4ba66ce926b5e00b63.svg
          fullname: Khurrum
          isHf: false
          isPro: true
          name: Ioulaum
          type: user
        html: '<blockquote>

          <p>You could get around the input token limitation with a bit of engineering?</p>

          <ul>

          <li>Store conversation so far as a compressed knowledge graph (setup for
          semantic search)</li>

          <li>When you receive a <strong>prompt</strong>, generate search terms for
          the knowledge graph and identify relevant parts</li>

          <li>If too many relevant parts of the conversation are identified, can constrain
          it using (1) ranking by relevance, (2) compression of text, (3) random sampling
          [can weigh the sampling by rank]</li>

          <li>Feed this compressed representation along as a header to the <strong>prompt</strong>.</li>

          </ul>

          <p>I have no idea if ChatGPT is doing any of these things, but it''s one
          possible approach that I am assessing as I continue using the tool and re-evaluating
          my mental model of what it can do.</p>

          </blockquote>

          <p>I think knowledge graph + summarization is part of it, probably along
          with other data structures. I also think it does more than a single pass
          when the situation is complicated, breaking things down and processing them
          further, before it starts up on the current generation (possibly using different
          / faster models for specific sub-tasks).</p>

          '
        raw: "> You could get around the input token limitation with a bit of engineering?\n\
          > \n> - Store conversation so far as a compressed knowledge graph (setup\
          \ for semantic search)\n> - When you receive a **prompt**, generate search\
          \ terms for the knowledge graph and identify relevant parts\n> - If too\
          \ many relevant parts of the conversation are identified, can constrain\
          \ it using (1) ranking by relevance, (2) compression of text, (3) random\
          \ sampling [can weigh the sampling by rank]\n> - Feed this compressed representation\
          \ along as a header to the **prompt**.\n> \n> I have no idea if ChatGPT\
          \ is doing any of these things, but it's one possible approach that I am\
          \ assessing as I continue using the tool and re-evaluating my mental model\
          \ of what it can do.\n\nI think knowledge graph + summarization is part\
          \ of it, probably along with other data structures. I also think it does\
          \ more than a single pass when the situation is complicated, breaking things\
          \ down and processing them further, before it starts up on the current generation\
          \ (possibly using different / faster models for specific sub-tasks)."
        updatedAt: '2023-02-25T11:41:21.131Z'
      numEdits: 1
      reactions: []
    id: 63f9f3bda81c4e4468ba3b56
    type: comment
  author: Ioulaum
  content: "> You could get around the input token limitation with a bit of engineering?\n\
    > \n> - Store conversation so far as a compressed knowledge graph (setup for semantic\
    \ search)\n> - When you receive a **prompt**, generate search terms for the knowledge\
    \ graph and identify relevant parts\n> - If too many relevant parts of the conversation\
    \ are identified, can constrain it using (1) ranking by relevance, (2) compression\
    \ of text, (3) random sampling [can weigh the sampling by rank]\n> - Feed this\
    \ compressed representation along as a header to the **prompt**.\n> \n> I have\
    \ no idea if ChatGPT is doing any of these things, but it's one possible approach\
    \ that I am assessing as I continue using the tool and re-evaluating my mental\
    \ model of what it can do.\n\nI think knowledge graph + summarization is part\
    \ of it, probably along with other data structures. I also think it does more\
    \ than a single pass when the situation is complicated, breaking things down and\
    \ processing them further, before it starts up on the current generation (possibly\
    \ using different / faster models for specific sub-tasks)."
  created_at: 2023-02-25 11:40:45+00:00
  edited: true
  hidden: false
  id: 63f9f3bda81c4e4468ba3b56
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 165
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Looking at what makes ChatGPT special...
