!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Muennighoff
conflicting_files: null
created_at: 2022-09-22 14:49:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-09-22T15:49:42.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<p>Shouldn't the dropouts in the config be 0.1, as the model was pre-trained\
          \ with dropout <span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\"\
          >@<span class=\"underline\">TimeRobber</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> ?</p>\n"
        raw: Shouldn't the dropouts in the config be 0.1, as the model was pre-trained
          with dropout @TimeRobber @ybelkada ?
        updatedAt: '2022-09-22T15:49:42.930Z'
      numEdits: 0
      reactions: []
    id: 632c841616faa31b24db8064
    type: comment
  author: Muennighoff
  content: Shouldn't the dropouts in the config be 0.1, as the model was pre-trained
    with dropout @TimeRobber @ybelkada ?
  created_at: 2022-09-22 14:49:42+00:00
  edited: false
  hidden: false
  id: 632c841616faa31b24db8064
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-09-27T15:52:24.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>I don''t know about this. I think this depends on what we want those
          configs to reflect:</p>

          <ul>

          <li>training procedure? In that sense yes we did use dropout 0.1 so we can
          update those</li>

          <li>best training procedure? My strong intuition is that we shouldn''t have
          used dropout. Palm didn''t set it for example.</li>

          <li>best config for finetuning? I think in this case we''ve seen that dropout
          has substantial impact on downstream tasks: <a rel="nofollow" href="https://arxiv.org/abs/2204.05832">https://arxiv.org/abs/2204.05832</a></li>

          </ul>

          '
        raw: "I don't know about this. I think this depends on what we want those\
          \ configs to reflect:\n - training procedure? In that sense yes we did use\
          \ dropout 0.1 so we can update those\n - best training procedure? My strong\
          \ intuition is that we shouldn't have used dropout. Palm didn't set it for\
          \ example.\n - best config for finetuning? I think in this case we've seen\
          \ that dropout has substantial impact on downstream tasks: https://arxiv.org/abs/2204.05832"
        updatedAt: '2022-09-27T15:52:24.789Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ybelkada
    id: 63331c381a41600d6745de15
    type: comment
  author: TimeRobber
  content: "I don't know about this. I think this depends on what we want those configs\
    \ to reflect:\n - training procedure? In that sense yes we did use dropout 0.1\
    \ so we can update those\n - best training procedure? My strong intuition is that\
    \ we shouldn't have used dropout. Palm didn't set it for example.\n - best config\
    \ for finetuning? I think in this case we've seen that dropout has substantial\
    \ impact on downstream tasks: https://arxiv.org/abs/2204.05832"
  created_at: 2022-09-27 14:52:24+00:00
  edited: false
  hidden: false
  id: 63331c381a41600d6745de15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2022-09-27T15:56:02.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>I think either 1) or 3), so we should change the config, no?<br>2)
          could be the default parameters in transformers, but not for a model on
          the hub imo when it was trained differently</p>

          '
        raw: 'I think either 1) or 3), so we should change the config, no?

          2) could be the default parameters in transformers, but not for a model
          on the hub imo when it was trained differently'
        updatedAt: '2022-09-27T15:56:02.105Z'
      numEdits: 0
      reactions: []
    id: 63331d12a90c64c6997b9fea
    type: comment
  author: Muennighoff
  content: 'I think either 1) or 3), so we should change the config, no?

    2) could be the default parameters in transformers, but not for a model on the
    hub imo when it was trained differently'
  created_at: 2022-09-27 14:56:02+00:00
  edited: false
  hidden: false
  id: 63331d12a90c64c6997b9fea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-09-28T08:12:24.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p>No strong opinion, but I feel this should already be answered somewhere.\
          \ cc <span data-props=\"{&quot;user&quot;:&quot;patrickvonplaten&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/patrickvonplaten\"\
          >@<span class=\"underline\">patrickvonplaten</span></a></span>\n\n\t</span></span></p>\n"
        raw: No strong opinion, but I feel this should already be answered somewhere.
          cc @patrickvonplaten
        updatedAt: '2022-09-28T08:12:24.770Z'
      numEdits: 0
      reactions: []
    id: 633401e8f68a3fb7efa5dfe2
    type: comment
  author: TimeRobber
  content: No strong opinion, but I feel this should already be answered somewhere.
    cc @patrickvonplaten
  created_at: 2022-09-28 07:12:24+00:00
  edited: false
  hidden: false
  id: 633401e8f68a3fb7efa5dfe2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-09-29T09:10:51.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>I second what <span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\"\
          >@<span class=\"underline\">TimeRobber</span></a></span>\n\n\t</span></span>\
          \ said, I don't have any strong opinion on that. But would be nice if we\
          \ can update it with the parameter used for training, ie, 0.1 to make the\
          \ config file reflect the parameters used for the training</p>\n"
        raw: I second what @TimeRobber said, I don't have any strong opinion on that.
          But would be nice if we can update it with the parameter used for training,
          ie, 0.1 to make the config file reflect the parameters used for the training
        updatedAt: '2022-09-29T14:06:38.511Z'
      numEdits: 1
      reactions: []
    id: 6335611ba01bd734f7231d67
    type: comment
  author: ybelkada
  content: I second what @TimeRobber said, I don't have any strong opinion on that.
    But would be nice if we can update it with the parameter used for training, ie,
    0.1 to make the config file reflect the parameters used for the training
  created_at: 2022-09-29 08:10:51+00:00
  edited: true
  hidden: false
  id: 6335611ba01bd734f7231d67
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 116
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Dropout
