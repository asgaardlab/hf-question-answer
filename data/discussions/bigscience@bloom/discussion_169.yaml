!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gameveloster
conflicting_files: null
created_at: 2023-01-13 02:13:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/367e1c9cbb77e454b57832240bf2adf5.svg
      fullname: Gameveloster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gameveloster
      type: user
    createdAt: '2023-01-13T02:13:50.000Z'
    data:
      edited: false
      editors:
      - gameveloster
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/367e1c9cbb77e454b57832240bf2adf5.svg
          fullname: Gameveloster
          isHf: false
          isPro: false
          name: gameveloster
          type: user
        html: '<p>bloom-176b requirea about 300G of VRAM<br>bloom-7b1 requires about
          30G of VRAM </p>

          <p>Can anyone create a version that uses 48G of VRAM, so it can be fine
          tuned on two 3090s?<br>Or 86G of VRAM, so it can be fine tuned on four 3090s?</p>

          <p>Do you expect noticable performance increase going from bloom-7b1 to
          a bloom-14b?</p>

          '
        raw: "bloom-176b requirea about 300G of VRAM\r\nbloom-7b1 requires about 30G\
          \ of VRAM \r\n\r\nCan anyone create a version that uses 48G of VRAM, so\
          \ it can be fine tuned on two 3090s?\r\nOr 86G of VRAM, so it can be fine\
          \ tuned on four 3090s?\r\n\r\nDo you expect noticable performance increase\
          \ going from bloom-7b1 to a bloom-14b?"
        updatedAt: '2023-01-13T02:13:50.705Z'
      numEdits: 0
      reactions: []
    id: 63c0be5e7f52541dfc7d4de5
    type: comment
  author: gameveloster
  content: "bloom-176b requirea about 300G of VRAM\r\nbloom-7b1 requires about 30G\
    \ of VRAM \r\n\r\nCan anyone create a version that uses 48G of VRAM, so it can\
    \ be fine tuned on two 3090s?\r\nOr 86G of VRAM, so it can be fine tuned on four\
    \ 3090s?\r\n\r\nDo you expect noticable performance increase going from bloom-7b1\
    \ to a bloom-14b?"
  created_at: 2023-01-13 02:13:50+00:00
  edited: false
  hidden: false
  id: 63c0be5e7f52541dfc7d4de5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640134437444-609baae0fe087f3d04cf0481.jpeg?w=200&h=200&f=face
      fullname: Yozh
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: justheuristic
      type: user
    createdAt: '2023-01-19T13:45:19.000Z'
    data:
      edited: true
      editors:
      - justheuristic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640134437444-609baae0fe087f3d04cf0481.jpeg?w=200&h=200&f=face
          fullname: Yozh
          isHf: false
          isPro: false
          name: justheuristic
          type: user
        html: '<p>I cannot know for sure, but, to the best of my knowledge, it is
          unlikely that the bigscience will train another model just like BLOOM, since
          the compute resources used for training the original bloom have been released.
          Then again, some other training projects have related models in the range
          you have specified.</p>

          <p>If you need local compute, but you''re okay with english-only models,
          there are several 20B variants (<a href="https://huggingface.co/google/flan-t5-xxl">flan-T5</a>
          or  <a href="https://huggingface.co/EleutherAI/gpt-neox-20b">gpt-neox</a>
          and many other awesome projects).</p>

          <p>If you want to inference/train this specific version of bloom with your
          3090s, there''s <a rel="nofollow" href="https://github.com/bigscience-workshop/petals">https://github.com/bigscience-workshop/petals</a></p>

          '
        raw: 'I cannot know for sure, but, to the best of my knowledge, it is unlikely
          that the bigscience will train another model just like BLOOM, since the
          compute resources used for training the original bloom have been released.
          Then again, some other training projects have related models in the range
          you have specified.


          If you need local compute, but you''re okay with english-only models, there
          are several 20B variants ([flan-T5](https://huggingface.co/google/flan-t5-xxl)
          or  [gpt-neox](https://huggingface.co/EleutherAI/gpt-neox-20b) and many
          other awesome projects).


          If you want to inference/train this specific version of bloom with your
          3090s, there''s https://github.com/bigscience-workshop/petals'
        updatedAt: '2023-01-19T13:48:02.713Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - gameveloster
    id: 63c9496f4c74c79fff652ba8
    type: comment
  author: justheuristic
  content: 'I cannot know for sure, but, to the best of my knowledge, it is unlikely
    that the bigscience will train another model just like BLOOM, since the compute
    resources used for training the original bloom have been released. Then again,
    some other training projects have related models in the range you have specified.


    If you need local compute, but you''re okay with english-only models, there are
    several 20B variants ([flan-T5](https://huggingface.co/google/flan-t5-xxl) or  [gpt-neox](https://huggingface.co/EleutherAI/gpt-neox-20b)
    and many other awesome projects).


    If you want to inference/train this specific version of bloom with your 3090s,
    there''s https://github.com/bigscience-workshop/petals'
  created_at: 2023-01-19 13:45:19+00:00
  edited: true
  hidden: false
  id: 63c9496f4c74c79fff652ba8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 169
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Something between BLOOM-176B and BLOOM-7B1?
