!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Philimipp
conflicting_files: null
created_at: 2022-08-17 07:05:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a424f212ca849d336b873c311526664.svg
      fullname: Limbrzk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Philimipp
      type: user
    createdAt: '2022-08-17T08:05:41.000Z'
    data:
      edited: true
      editors:
      - Philimipp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a424f212ca849d336b873c311526664.svg
          fullname: Limbrzk
          isHf: false
          isPro: false
          name: Philimipp
          type: user
        html: "<p>I'm following this <a rel=\"nofollow\" href=\"https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32\"\
          >this wonderful article</a> by <span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/arteagac\"\
          >@<span class=\"underline\">arteagac</span></a></span>\n\n\t</span></span>,\
          \ trying to run Bloom-176B on my own machine by loading it into memory block\
          \ by block. The author claims in an answer to a comment that the approximate\
          \ duration to generate a token from a prompt is 3 minutes, using off-the-shelf\
          \ hardware.</p>\n<p>My own experiments yield other results, however. On\
          \ my machine it takes around 4 minutes to even evaluate a single block (the\
          \ call to <code>block(...)</code>), hence the loop over all ~70 blocks would\
          \ require hours to terminate in order to generate a single token. May I\
          \ kindly ask you to tell me what your own experience is? I'm trying to find\
          \ errors on my part which are causing this huge slowdown.</p>\n<p>While\
          \ I'm using a Threadripper 3960x, a 3090 and 64GB of RAM, none of this is\
          \ used as all I'm getting is single-core eval when I run the code in the\
          \ article.</p>\n"
        raw: 'I''m following this [this wonderful article](https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32)
          by @arteagac, trying to run Bloom-176B on my own machine by loading it into
          memory block by block. The author claims in an answer to a comment that
          the approximate duration to generate a token from a prompt is 3 minutes,
          using off-the-shelf hardware.


          My own experiments yield other results, however. On my machine it takes
          around 4 minutes to even evaluate a single block (the call to `block(...)`),
          hence the loop over all ~70 blocks would require hours to terminate in order
          to generate a single token. May I kindly ask you to tell me what your own
          experience is? I''m trying to find errors on my part which are causing this
          huge slowdown.


          While I''m using a Threadripper 3960x, a 3090 and 64GB of RAM, none of this
          is used as all I''m getting is single-core eval when I run the code in the
          article.


          '
        updatedAt: '2022-08-17T08:06:31.016Z'
      numEdits: 1
      reactions: []
    id: 62fca15503f866462203045a
    type: comment
  author: Philimipp
  content: 'I''m following this [this wonderful article](https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32)
    by @arteagac, trying to run Bloom-176B on my own machine by loading it into memory
    block by block. The author claims in an answer to a comment that the approximate
    duration to generate a token from a prompt is 3 minutes, using off-the-shelf hardware.


    My own experiments yield other results, however. On my machine it takes around
    4 minutes to even evaluate a single block (the call to `block(...)`), hence the
    loop over all ~70 blocks would require hours to terminate in order to generate
    a single token. May I kindly ask you to tell me what your own experience is? I''m
    trying to find errors on my part which are causing this huge slowdown.


    While I''m using a Threadripper 3960x, a 3090 and 64GB of RAM, none of this is
    used as all I''m getting is single-core eval when I run the code in the article.


    '
  created_at: 2022-08-17 07:05:41+00:00
  edited: true
  hidden: false
  id: 62fca15503f866462203045a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2022-08-17T15:20:49.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: '<p>Hi. I''m the author of the blog post. I had a similar experience
          when I used an AMD processor. The inference was significantly slower, and
          I think it has to do with PyTorch''s limited performance on AMD processors
          (see this discussion: <a rel="nofollow" href="https://www.reddit.com/r/MachineLearning/comments/iap6yo">https://www.reddit.com/r/MachineLearning/comments/iap6yo</a>
          ). I addressed it to some degree by compiling PyTorch from scratch using
          OpenBLAS. However, even with the from-scratch compilation, the modest intel
          i5 12 gen was faster than the AMD 5900X. To use the GPU, you can change
          <code>device = ''cpu''</code> to <code>device = ''cuda:0''</code> in the
          code.</p>

          <p>In addition, make sure you have a fast SSD, as it might be the main bottleneck
          when running the code in the blog post.</p>

          '
        raw: 'Hi. I''m the author of the blog post. I had a similar experience when
          I used an AMD processor. The inference was significantly slower, and I think
          it has to do with PyTorch''s limited performance on AMD processors (see
          this discussion: https://www.reddit.com/r/MachineLearning/comments/iap6yo
          ). I addressed it to some degree by compiling PyTorch from scratch using
          OpenBLAS. However, even with the from-scratch compilation, the modest intel
          i5 12 gen was faster than the AMD 5900X. To use the GPU, you can change
          `device = ''cpu''` to `device = ''cuda:0''` in the code.


          In addition, make sure you have a fast SSD, as it might be the main bottleneck
          when running the code in the blog post.'
        updatedAt: '2022-08-17T15:20:49.092Z'
      numEdits: 0
      reactions: []
    id: 62fd0751cad078c7972c70c7
    type: comment
  author: arteagac
  content: 'Hi. I''m the author of the blog post. I had a similar experience when
    I used an AMD processor. The inference was significantly slower, and I think it
    has to do with PyTorch''s limited performance on AMD processors (see this discussion:
    https://www.reddit.com/r/MachineLearning/comments/iap6yo ). I addressed it to
    some degree by compiling PyTorch from scratch using OpenBLAS. However, even with
    the from-scratch compilation, the modest intel i5 12 gen was faster than the AMD
    5900X. To use the GPU, you can change `device = ''cpu''` to `device = ''cuda:0''`
    in the code.


    In addition, make sure you have a fast SSD, as it might be the main bottleneck
    when running the code in the blog post.'
  created_at: 2022-08-17 14:20:49+00:00
  edited: false
  hidden: false
  id: 62fd0751cad078c7972c70c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a424f212ca849d336b873c311526664.svg
      fullname: Limbrzk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Philimipp
      type: user
    createdAt: '2022-08-17T19:14:30.000Z'
    data:
      edited: true
      editors:
      - Philimipp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a424f212ca849d336b873c311526664.svg
          fullname: Limbrzk
          isHf: false
          isPro: false
          name: Philimipp
          type: user
        html: '<p>Hi Cristian, thanks for the reply. I wasn''t aware of the disparity
          between AMD and Intel CPUs, very curious. I''ll have to do some reading
          and we''ll see whether compiling Pytorch for my particular instruction set
          or for my 3090 will help. Will keep you posted.</p>

          <p>Also, I do have a SSD and I''ve benchmarked the loading of a block into
          memory, which takes 1 to 2 seconds, roughly corresponding to the order of
          magnitude I''d expect from 7500MB/s peak read speed vs. 4GB chunks of memory.
          It''s really the evaluation of the block which takes those aforementioned
          4 minutes.</p>

          '
        raw: 'Hi Cristian, thanks for the reply. I wasn''t aware of the disparity
          between AMD and Intel CPUs, very curious. I''ll have to do some reading
          and we''ll see whether compiling Pytorch for my particular instruction set
          or for my 3090 will help. Will keep you posted.


          Also, I do have a SSD and I''ve benchmarked the loading of a block into
          memory, which takes 1 to 2 seconds, roughly corresponding to the order of
          magnitude I''d expect from 7500MB/s peak read speed vs. 4GB chunks of memory.
          It''s really the evaluation of the block which takes those aforementioned
          4 minutes.'
        updatedAt: '2022-08-17T19:21:41.500Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - arteagac
    id: 62fd3e16dfc51b409bd1a84f
    type: comment
  author: Philimipp
  content: 'Hi Cristian, thanks for the reply. I wasn''t aware of the disparity between
    AMD and Intel CPUs, very curious. I''ll have to do some reading and we''ll see
    whether compiling Pytorch for my particular instruction set or for my 3090 will
    help. Will keep you posted.


    Also, I do have a SSD and I''ve benchmarked the loading of a block into memory,
    which takes 1 to 2 seconds, roughly corresponding to the order of magnitude I''d
    expect from 7500MB/s peak read speed vs. 4GB chunks of memory. It''s really the
    evaluation of the block which takes those aforementioned 4 minutes.'
  created_at: 2022-08-17 18:14:30+00:00
  edited: true
  hidden: false
  id: 62fd3e16dfc51b409bd1a84f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fbedb09af1d16bc0b15f57/RSp12KhS2FD8W7awCgEJD.jpeg?w=200&h=200&f=face
      fullname: Maxence Pastor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maxencepastor
      type: user
    createdAt: '2022-08-18T09:37:10.000Z'
    data:
      edited: false
      editors:
      - maxencepastor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fbedb09af1d16bc0b15f57/RSp12KhS2FD8W7awCgEJD.jpeg?w=200&h=200&f=face
          fullname: Maxence Pastor
          isHf: false
          isPro: false
          name: maxencepastor
          type: user
        html: '<p>Hi both, I can confirm, having tried to deploy and use the model
          on significant AWS infrastructure, that the AMD processors are not giving
          good performance. The difference is day and night (seconds vs long minutes)
          between AMD and Intel processors.</p>

          '
        raw: Hi both, I can confirm, having tried to deploy and use the model on significant
          AWS infrastructure, that the AMD processors are not giving good performance.
          The difference is day and night (seconds vs long minutes) between AMD and
          Intel processors.
        updatedAt: '2022-08-18T09:37:10.762Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Philimipp
        - arteagac
    id: 62fe08462ab4812db7018111
    type: comment
  author: maxencepastor
  content: Hi both, I can confirm, having tried to deploy and use the model on significant
    AWS infrastructure, that the AMD processors are not giving good performance. The
    difference is day and night (seconds vs long minutes) between AMD and Intel processors.
  created_at: 2022-08-18 08:37:10+00:00
  edited: false
  hidden: false
  id: 62fe08462ab4812db7018111
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a424f212ca849d336b873c311526664.svg
      fullname: Limbrzk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Philimipp
      type: user
    createdAt: '2022-08-18T11:18:16.000Z'
    data:
      edited: true
      editors:
      - Philimipp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a424f212ca849d336b873c311526664.svg
          fullname: Limbrzk
          isHf: false
          isPro: false
          name: Philimipp
          type: user
        html: '<p>Good news. Running on GPU really is performant and reduces the evaluation
          time per block to 2ms. So really the only timesink is loading blocks into
          VRAM at ca. 3s per block. That results in roughly 3.5 minutes per token.</p>

          <p>I''ll see I whether I can compile pytorch to properly perform on my AMD
          CPU.</p>

          '
        raw: 'Good news. Running on GPU really is performant and reduces the evaluation
          time per block to 2ms. So really the only timesink is loading blocks into
          VRAM at ca. 3s per block. That results in roughly 3.5 minutes per token.


          I''ll see I whether I can compile pytorch to properly perform on my AMD
          CPU.'
        updatedAt: '2022-08-18T11:18:55.311Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - arteagac
    id: 62fe1ff827c98b09b502aa66
    type: comment
  author: Philimipp
  content: 'Good news. Running on GPU really is performant and reduces the evaluation
    time per block to 2ms. So really the only timesink is loading blocks into VRAM
    at ca. 3s per block. That results in roughly 3.5 minutes per token.


    I''ll see I whether I can compile pytorch to properly perform on my AMD CPU.'
  created_at: 2022-08-18 10:18:16+00:00
  edited: true
  hidden: false
  id: 62fe1ff827c98b09b502aa66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661355296466-62f62c3ed278a8f3e781efa7.png?w=200&h=200&f=face
      fullname: Hannes Planatscher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: derduff
      type: user
    createdAt: '2022-08-23T10:23:27.000Z'
    data:
      edited: false
      editors:
      - derduff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661355296466-62f62c3ed278a8f3e781efa7.png?w=200&h=200&f=face
          fullname: Hannes Planatscher
          isHf: false
          isPro: false
          name: derduff
          type: user
        html: '<p>Hi, could you share the code you have used to load the blocks to
          GPU and cycle through the blocks?</p>

          '
        raw: Hi, could you share the code you have used to load the blocks to GPU
          and cycle through the blocks?
        updatedAt: '2022-08-23T10:23:27.054Z'
      numEdits: 0
      reactions: []
    id: 6304aa9fdae2eb7d083fa1e4
    type: comment
  author: derduff
  content: Hi, could you share the code you have used to load the blocks to GPU and
    cycle through the blocks?
  created_at: 2022-08-23 09:23:27+00:00
  edited: false
  hidden: false
  id: 6304aa9fdae2eb7d083fa1e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a424f212ca849d336b873c311526664.svg
      fullname: Limbrzk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Philimipp
      type: user
    createdAt: '2022-08-23T12:35:37.000Z'
    data:
      edited: true
      editors:
      - Philimipp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a424f212ca849d336b873c311526664.svg
          fullname: Limbrzk
          isHf: false
          isPro: false
          name: Philimipp
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;derduff&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/derduff\">@<span class=\"\
          underline\">derduff</span></a></span>\n\n\t</span></span> I use the author's\
          \ exact code, with the exception of setting <code>device = \"cuda:0\"</code>.</p>\n\
          <p>You'll have to ensure you install pytorch with cuda support. I'm working\
          \ in a conda env with the following pytorch and cuda toolkit: <code>pytorch=1.12.1=py3.10_cuda11.6_cudnn8.3.2_0</code>\
          \ and <code>cudatoolkit=11.6.0=hecad31d_10</code>.</p>\n<p>For a quick check\
          \ if all is well, try this</p>\n<pre><code>import torch\nprint(f\"CUDA available:\
          \ {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\"\
          )\nprint(f\"CUDA arch list: {torch.cuda.get_arch_list()}\")\nprint(f\"CUDNN\
          \ available: {torch.backends.cudnn.is_available()}\")\nprint(f\"CUDNN version:\
          \ {torch.backends.cudnn.version()}\")\n</code></pre>\n"
        raw: '@derduff I use the author''s exact code, with the exception of setting
          `device = "cuda:0"`.


          You''ll have to ensure you install pytorch with cuda support. I''m working
          in a conda env with the following pytorch and cuda toolkit: `pytorch=1.12.1=py3.10_cuda11.6_cudnn8.3.2_0`
          and `cudatoolkit=11.6.0=hecad31d_10`.


          For a quick check if all is well, try this


          ```

          import torch

          print(f"CUDA available: {torch.cuda.is_available()}")

          print(f"CUDA version: {torch.version.cuda}")

          print(f"CUDA arch list: {torch.cuda.get_arch_list()}")

          print(f"CUDNN available: {torch.backends.cudnn.is_available()}")

          print(f"CUDNN version: {torch.backends.cudnn.version()}")

          ```'
        updatedAt: '2022-08-23T12:37:03.323Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - derduff
    id: 6304c999bad6ce7fc0276add
    type: comment
  author: Philimipp
  content: '@derduff I use the author''s exact code, with the exception of setting
    `device = "cuda:0"`.


    You''ll have to ensure you install pytorch with cuda support. I''m working in
    a conda env with the following pytorch and cuda toolkit: `pytorch=1.12.1=py3.10_cuda11.6_cudnn8.3.2_0`
    and `cudatoolkit=11.6.0=hecad31d_10`.


    For a quick check if all is well, try this


    ```

    import torch

    print(f"CUDA available: {torch.cuda.is_available()}")

    print(f"CUDA version: {torch.version.cuda}")

    print(f"CUDA arch list: {torch.cuda.get_arch_list()}")

    print(f"CUDNN available: {torch.backends.cudnn.is_available()}")

    print(f"CUDNN version: {torch.backends.cudnn.version()}")

    ```'
  created_at: 2022-08-23 11:35:37+00:00
  edited: true
  hidden: false
  id: 6304c999bad6ce7fc0276add
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fc2bb20f7b66bb2f26db775d5e52c532.svg
      fullname: Sasha White
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MusashiGarami
      type: user
    createdAt: '2022-08-24T13:00:57.000Z'
    data:
      edited: false
      editors:
      - MusashiGarami
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fc2bb20f7b66bb2f26db775d5e52c532.svg
          fullname: Sasha White
          isHf: false
          isPro: false
          name: MusashiGarami
          type: user
        html: "<p>I don't want to hi-jack this thread... BUT I notice the creator\
          \ of that article <span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/arteagac\"\
          >@<span class=\"underline\">arteagac</span></a></span>\n\n\t</span></span>,\
          \ is here... </p>\n<p>I used<br>git lfs install<br>export GIT_LFS_SKIP_SMUDGE=1<br>git\
          \ clone <a href=\"https://huggingface.co/bigscience/bloom\">https://huggingface.co/bigscience/bloom</a><br>cd\
          \ bloom<br>git lfs fetch origin <a href=\"/bigscience/bloom/commit/2a3d62e\"\
          >2a3d62e</a></p>\n<p>To retrieve the checkpoint, and it was about 350GB,\
          \ (folder says 330GB... So feels right.. BUT my .BIN files for each shard\
          \ is only 1 KB?)</p>\n"
        raw: "I don't want to hi-jack this thread... BUT I notice the creator of that\
          \ article @arteagac, is here... \n\nI used\ngit lfs install\nexport GIT_LFS_SKIP_SMUDGE=1\n\
          git clone https://huggingface.co/bigscience/bloom\ncd bloom\ngit lfs fetch\
          \ origin 2a3d62e\n\nTo retrieve the checkpoint, and it was about 350GB,\
          \ (folder says 330GB... So feels right.. BUT my .BIN files for each shard\
          \ is only 1 KB?)"
        updatedAt: '2022-08-24T13:00:57.819Z'
      numEdits: 0
      reactions: []
    id: 63062109cfbde33ef7d73fa9
    type: comment
  author: MusashiGarami
  content: "I don't want to hi-jack this thread... BUT I notice the creator of that\
    \ article @arteagac, is here... \n\nI used\ngit lfs install\nexport GIT_LFS_SKIP_SMUDGE=1\n\
    git clone https://huggingface.co/bigscience/bloom\ncd bloom\ngit lfs fetch origin\
    \ 2a3d62e\n\nTo retrieve the checkpoint, and it was about 350GB, (folder says\
    \ 330GB... So feels right.. BUT my .BIN files for each shard is only 1 KB?)"
  created_at: 2022-08-24 12:00:57+00:00
  edited: false
  hidden: false
  id: 63062109cfbde33ef7d73fa9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2022-08-24T13:48:41.000Z'
    data:
      edited: false
      editors:
      - cakiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
          fullname: Christopher Akiki
          isHf: false
          isPro: false
          name: cakiki
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;MusashiGarami&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MusashiGarami\"\
          >@<span class=\"underline\">MusashiGarami</span></a></span>\n\n\t</span></span>\
          \ <code>GIT_LFS_SKIP_SMUDGE=1</code> only fetches the LFS pointers to the\
          \ files. You want to clone without that set to get the actual files.</p>\n"
        raw: '@MusashiGarami `GIT_LFS_SKIP_SMUDGE=1` only fetches the LFS pointers
          to the files. You want to clone without that set to get the actual files.'
        updatedAt: '2022-08-24T13:48:41.702Z'
      numEdits: 0
      reactions: []
    id: 63062c39d70693fdf1ca67f2
    type: comment
  author: cakiki
  content: '@MusashiGarami `GIT_LFS_SKIP_SMUDGE=1` only fetches the LFS pointers to
    the files. You want to clone without that set to get the actual files.'
  created_at: 2022-08-24 12:48:41+00:00
  edited: false
  hidden: false
  id: 63062c39d70693fdf1ca67f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fc2bb20f7b66bb2f26db775d5e52c532.svg
      fullname: Sasha White
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MusashiGarami
      type: user
    createdAt: '2022-08-24T14:04:17.000Z'
    data:
      edited: false
      editors:
      - MusashiGarami
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fc2bb20f7b66bb2f26db775d5e52c532.svg
          fullname: Sasha White
          isHf: false
          isPro: false
          name: MusashiGarami
          type: user
        html: '<p>Oh... how big would the DL be without that line?</p>

          '
        raw: Oh... how big would the DL be without that line?
        updatedAt: '2022-08-24T14:04:17.124Z'
      numEdits: 0
      reactions: []
    id: 63062fe1cfbde33ef7d7f512
    type: comment
  author: MusashiGarami
  content: Oh... how big would the DL be without that line?
  created_at: 2022-08-24 13:04:17+00:00
  edited: false
  hidden: false
  id: 63062fe1cfbde33ef7d7f512
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a585cbfcd1f335b70c101458f7c0b002.svg
      fullname: abubakar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stormchaser
      type: user
    createdAt: '2022-08-25T09:41:19.000Z'
    data:
      edited: false
      editors:
      - stormchaser
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a585cbfcd1f335b70c101458f7c0b002.svg
          fullname: abubakar
          isHf: false
          isPro: false
          name: stormchaser
          type: user
        html: '<p>Hi, my device=cpu is doing just fine, but taking 8~ minutes per
          token generation, on laptop with core i7 11800H, 40GB ram, RTX 3070.<br>Since
          I want more speed when I change device=cuda:0, I get the following error:<br>RuntimeError:
          CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 8.00 GiB total capacity;
          5.08 GiB already allocated; 0 bytes free; 6.87 GiB reserved in total by
          PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb
          to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>help needed, what to do here? really want this running on gpu. thanks.<br>PS:
          my bloom directory size is 681GB in the end, no where near 350 as others
          are getting.</p>

          '
        raw: "Hi, my device=cpu is doing just fine, but taking 8~ minutes per token\
          \ generation, on laptop with core i7 11800H, 40GB ram, RTX 3070. \nSince\
          \ I want more speed when I change device=cuda:0, I get the following error:\n\
          RuntimeError: CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 8.00\
          \ GiB total capacity; 5.08 GiB already allocated; 0 bytes free; 6.87 GiB\
          \ reserved in total by PyTorch) If reserved memory is >> allocated memory\
          \ try setting max_split_size_mb to avoid fragmentation.  See documentation\
          \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\nhelp needed, what\
          \ to do here? really want this running on gpu. thanks.\nPS: my bloom directory\
          \ size is 681GB in the end, no where near 350 as others are getting."
        updatedAt: '2022-08-25T09:41:19.200Z'
      numEdits: 0
      reactions: []
    id: 630743bffd79b417f1bae8a6
    type: comment
  author: stormchaser
  content: "Hi, my device=cpu is doing just fine, but taking 8~ minutes per token\
    \ generation, on laptop with core i7 11800H, 40GB ram, RTX 3070. \nSince I want\
    \ more speed when I change device=cuda:0, I get the following error:\nRuntimeError:\
    \ CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 8.00 GiB total capacity;\
    \ 5.08 GiB already allocated; 0 bytes free; 6.87 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
    \nhelp needed, what to do here? really want this running on gpu. thanks.\nPS:\
    \ my bloom directory size is 681GB in the end, no where near 350 as others are\
    \ getting."
  created_at: 2022-08-25 08:41:19+00:00
  edited: false
  hidden: false
  id: 630743bffd79b417f1bae8a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fbedb09af1d16bc0b15f57/RSp12KhS2FD8W7awCgEJD.jpeg?w=200&h=200&f=face
      fullname: Maxence Pastor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maxencepastor
      type: user
    createdAt: '2022-08-25T10:32:00.000Z'
    data:
      edited: false
      editors:
      - maxencepastor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fbedb09af1d16bc0b15f57/RSp12KhS2FD8W7awCgEJD.jpeg?w=200&h=200&f=face
          fullname: Maxence Pastor
          isHf: false
          isPro: false
          name: maxencepastor
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;stormchaser&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/stormchaser\"\
          >@<span class=\"underline\">stormchaser</span></a></span>\n\n\t</span></span>\
          \ If it's anything like what I've experienced, make sure that you do not\
          \ call the model when you want to do inferences. Let me explain:<br>When\
          \ you call model= blablabla, it will download it if it's not there and put\
          \ it in the memory. But what you want to use for inferences is Pipe. And\
          \ pipe does the same thing. So you end up with your memory being loaded\
          \ twice with the same thing. </p>\n<p>So just make sure to only call the\
          \ pipe and the tokenizer.<br>As for the GPU side of things, I haven't tried\
          \ it myself yet, as I have access to machines that are good enough to do\
          \ the job on CPU and RAM.</p>\n"
        raw: "@stormchaser If it's anything like what I've experienced, make sure\
          \ that you do not call the model when you want to do inferences. Let me\
          \ explain: \nWhen you call model= blablabla, it will download it if it's\
          \ not there and put it in the memory. But what you want to use for inferences\
          \ is Pipe. And pipe does the same thing. So you end up with your memory\
          \ being loaded twice with the same thing. \n\nSo just make sure to only\
          \ call the pipe and the tokenizer. \nAs for the GPU side of things, I haven't\
          \ tried it myself yet, as I have access to machines that are good enough\
          \ to do the job on CPU and RAM."
        updatedAt: '2022-08-25T10:32:00.263Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - stormchaser
    id: 63074fa02a3936b94c9e4fe3
    type: comment
  author: maxencepastor
  content: "@stormchaser If it's anything like what I've experienced, make sure that\
    \ you do not call the model when you want to do inferences. Let me explain: \n\
    When you call model= blablabla, it will download it if it's not there and put\
    \ it in the memory. But what you want to use for inferences is Pipe. And pipe\
    \ does the same thing. So you end up with your memory being loaded twice with\
    \ the same thing. \n\nSo just make sure to only call the pipe and the tokenizer.\
    \ \nAs for the GPU side of things, I haven't tried it myself yet, as I have access\
    \ to machines that are good enough to do the job on CPU and RAM."
  created_at: 2022-08-25 09:32:00+00:00
  edited: false
  hidden: false
  id: 63074fa02a3936b94c9e4fe3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a585cbfcd1f335b70c101458f7c0b002.svg
      fullname: abubakar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: stormchaser
      type: user
    createdAt: '2022-08-25T10:58:12.000Z'
    data:
      edited: false
      editors:
      - stormchaser
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a585cbfcd1f335b70c101458f7c0b002.svg
          fullname: abubakar
          isHf: false
          isPro: false
          name: stormchaser
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;stormchaser&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/stormchaser\"\
          >@<span class=\"underline\">stormchaser</span></a></span>\n\n\t</span></span>\
          \ If it's anything like what I've experienced, make sure that you do not\
          \ call the model when you want to do inferences. Let me explain:<br>When\
          \ you call model= blablabla, it will download it if it's not there and put\
          \ it in the memory. But what you want to use for inferences is Pipe. And\
          \ pipe does the same thing. So you end up with your memory being loaded\
          \ twice with the same thing. </p>\n<p>So just make sure to only call the\
          \ pipe and the tokenizer.<br>As for the GPU side of things, I haven't tried\
          \ it myself yet, as I have access to machines that are good enough to do\
          \ the job on CPU and RAM.</p>\n</blockquote>\n<p>the problem is i am not\
          \ a python dev, and nor am in the AI field as such, i just make boring database\
          \ apps lol, and I am so excited to run this on my machine which is reasonably\
          \ good machine. What you are saying will have to be done either by the person\
          \ who wrote the blog post ie,. <span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/arteagac\"\
          >@<span class=\"underline\">arteagac</span></a></span>\n\n\t</span></span>,\
          \ or someone here who understands your point and make the change in the\
          \ script we are running.</p>\n"
        raw: "> @stormchaser If it's anything like what I've experienced, make sure\
          \ that you do not call the model when you want to do inferences. Let me\
          \ explain: \n> When you call model= blablabla, it will download it if it's\
          \ not there and put it in the memory. But what you want to use for inferences\
          \ is Pipe. And pipe does the same thing. So you end up with your memory\
          \ being loaded twice with the same thing. \n> \n> So just make sure to only\
          \ call the pipe and the tokenizer. \n> As for the GPU side of things, I\
          \ haven't tried it myself yet, as I have access to machines that are good\
          \ enough to do the job on CPU and RAM.\n\nthe problem is i am not a python\
          \ dev, and nor am in the AI field as such, i just make boring database apps\
          \ lol, and I am so excited to run this on my machine which is reasonably\
          \ good machine. What you are saying will have to be done either by the person\
          \ who wrote the blog post ie,. @arteagac, or someone here who understands\
          \ your point and make the change in the script we are running."
        updatedAt: '2022-08-25T10:58:12.118Z'
      numEdits: 0
      reactions: []
    id: 630755c4cb09c0a9042a2465
    type: comment
  author: stormchaser
  content: "> @stormchaser If it's anything like what I've experienced, make sure\
    \ that you do not call the model when you want to do inferences. Let me explain:\
    \ \n> When you call model= blablabla, it will download it if it's not there and\
    \ put it in the memory. But what you want to use for inferences is Pipe. And pipe\
    \ does the same thing. So you end up with your memory being loaded twice with\
    \ the same thing. \n> \n> So just make sure to only call the pipe and the tokenizer.\
    \ \n> As for the GPU side of things, I haven't tried it myself yet, as I have\
    \ access to machines that are good enough to do the job on CPU and RAM.\n\nthe\
    \ problem is i am not a python dev, and nor am in the AI field as such, i just\
    \ make boring database apps lol, and I am so excited to run this on my machine\
    \ which is reasonably good machine. What you are saying will have to be done either\
    \ by the person who wrote the blog post ie,. @arteagac, or someone here who understands\
    \ your point and make the change in the script we are running."
  created_at: 2022-08-25 09:58:12+00:00
  edited: false
  hidden: false
  id: 630755c4cb09c0a9042a2465
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661355296466-62f62c3ed278a8f3e781efa7.png?w=200&h=200&f=face
      fullname: Hannes Planatscher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: derduff
      type: user
    createdAt: '2022-08-25T12:49:08.000Z'
    data:
      edited: false
      editors:
      - derduff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661355296466-62f62c3ed278a8f3e781efa7.png?w=200&h=200&f=face
          fullname: Hannes Planatscher
          isHf: false
          isPro: false
          name: derduff
          type: user
        html: '<p>Hi, I am currently designing setup to run this on 2 nodes. I''m
          thinking of loading half the blocks to the GPUs on one machine, the other
          half to the other machine. The idea is to transfer the hidden states over
          LAN (or Infiniband) to the second machine. How big is the hiddenstates array?
          Is it only 14336 floating point numbers or more than that?</p>

          '
        raw: Hi, I am currently designing setup to run this on 2 nodes. I'm thinking
          of loading half the blocks to the GPUs on one machine, the other half to
          the other machine. The idea is to transfer the hidden states over LAN (or
          Infiniband) to the second machine. How big is the hiddenstates array? Is
          it only 14336 floating point numbers or more than that?
        updatedAt: '2022-08-25T12:49:08.349Z'
      numEdits: 0
      reactions: []
    id: 63076fc4dc6b7663aa9565b7
    type: comment
  author: derduff
  content: Hi, I am currently designing setup to run this on 2 nodes. I'm thinking
    of loading half the blocks to the GPUs on one machine, the other half to the other
    machine. The idea is to transfer the hidden states over LAN (or Infiniband) to
    the second machine. How big is the hiddenstates array? Is it only 14336 floating
    point numbers or more than that?
  created_at: 2022-08-25 11:49:08+00:00
  edited: false
  hidden: false
  id: 63076fc4dc6b7663aa9565b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2022-08-25T16:43:17.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: "<blockquote>\n<p>Hi, my device=cpu is doing just fine, but taking 8~\
          \ minutes per token generation, on laptop with core i7 11800H, 40GB ram,\
          \ RTX 3070.<br>Since I want more speed when I change device=cuda:0, I get\
          \ the following error:<br>RuntimeError: CUDA out of memory. Tried to allocate\
          \ 6.70 GiB (GPU 0; 8.00 GiB total capacity; 5.08 GiB already allocated;\
          \ 0 bytes free; 6.87 GiB reserved in total by PyTorch) If reserved memory\
          \ is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>\n\
          </blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;stormchaser&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/stormchaser\"\
          >@<span class=\"underline\">stormchaser</span></a></span>\n\n\t</span></span>,\
          \ I suspect 8GB of GPU memory is not enough.  You could try running the\
          \ word embeddings layer and language model layer (the largest ones) on the\
          \ CPU and only use the GPU to run the Bloom blocks. </p>\n<blockquote>\n\
          <p>help needed, what to do here? really want this running on gpu. thanks.<br>PS:\
          \ my bloom directory size is 681GB in the end, no where near 350 as others\
          \ are getting.</p>\n</blockquote>\n"
        raw: "> Hi, my device=cpu is doing just fine, but taking 8~ minutes per token\
          \ generation, on laptop with core i7 11800H, 40GB ram, RTX 3070. \n> Since\
          \ I want more speed when I change device=cuda:0, I get the following error:\n\
          > RuntimeError: CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 8.00\
          \ GiB total capacity; 5.08 GiB already allocated; 0 bytes free; 6.87 GiB\
          \ reserved in total by PyTorch) If reserved memory is >> allocated memory\
          \ try setting max_split_size_mb to avoid fragmentation.  See documentation\
          \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\nHi @stormchaser,\
          \ I suspect 8GB of GPU memory is not enough.  You could try running the\
          \ word embeddings layer and language model layer (the largest ones) on the\
          \ CPU and only use the GPU to run the Bloom blocks. \n> \n> help needed,\
          \ what to do here? really want this running on gpu. thanks.\n> PS: my bloom\
          \ directory size is 681GB in the end, no where near 350 as others are getting."
        updatedAt: '2022-08-25T16:43:17.052Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - stormchaser
    id: 6307a6a56bf7bb2fee5ddfa2
    type: comment
  author: arteagac
  content: "> Hi, my device=cpu is doing just fine, but taking 8~ minutes per token\
    \ generation, on laptop with core i7 11800H, 40GB ram, RTX 3070. \n> Since I want\
    \ more speed when I change device=cuda:0, I get the following error:\n> RuntimeError:\
    \ CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 8.00 GiB total capacity;\
    \ 5.08 GiB already allocated; 0 bytes free; 6.87 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
    \nHi @stormchaser, I suspect 8GB of GPU memory is not enough.  You could try running\
    \ the word embeddings layer and language model layer (the largest ones) on the\
    \ CPU and only use the GPU to run the Bloom blocks. \n> \n> help needed, what\
    \ to do here? really want this running on gpu. thanks.\n> PS: my bloom directory\
    \ size is 681GB in the end, no where near 350 as others are getting."
  created_at: 2022-08-25 15:43:17+00:00
  edited: false
  hidden: false
  id: 6307a6a56bf7bb2fee5ddfa2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2022-08-25T16:47:07.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: "<blockquote>\n<p>Hi, I am currently designing setup to run this on\
          \ 2 nodes. I'm thinking of loading half the blocks to the GPUs on one machine,\
          \ the other half to the other machine. The idea is to transfer the hidden\
          \ states over LAN (or Infiniband) to the second machine. How big is the\
          \ hiddenstates array? Is it only 14336 floating point numbers or more than\
          \ that?</p>\n</blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;derduff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/derduff\"\
          >@<span class=\"underline\">derduff</span></a></span>\n\n\t</span></span>\
          \ . The shape of the hidden states is  (n_input_tokens, hidden_size). If\
          \ you have a sentence with say 10 tokens, your hidden size would be (10,\
          \ 14336). You simply need to compute how much this would be in MB for the\
          \ bfloat16 dtype.</p>\n"
        raw: '> Hi, I am currently designing setup to run this on 2 nodes. I''m thinking
          of loading half the blocks to the GPUs on one machine, the other half to
          the other machine. The idea is to transfer the hidden states over LAN (or
          Infiniband) to the second machine. How big is the hiddenstates array? Is
          it only 14336 floating point numbers or more than that?


          Hi @derduff . The shape of the hidden states is  (n_input_tokens, hidden_size).
          If you have a sentence with say 10 tokens, your hidden size would be (10,
          14336). You simply need to compute how much this would be in MB for the
          bfloat16 dtype.'
        updatedAt: '2022-08-25T16:47:07.627Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - derduff
    id: 6307a78b161cfe1383a1e8a0
    type: comment
  author: arteagac
  content: '> Hi, I am currently designing setup to run this on 2 nodes. I''m thinking
    of loading half the blocks to the GPUs on one machine, the other half to the other
    machine. The idea is to transfer the hidden states over LAN (or Infiniband) to
    the second machine. How big is the hiddenstates array? Is it only 14336 floating
    point numbers or more than that?


    Hi @derduff . The shape of the hidden states is  (n_input_tokens, hidden_size).
    If you have a sentence with say 10 tokens, your hidden size would be (10, 14336).
    You simply need to compute how much this would be in MB for the bfloat16 dtype.'
  created_at: 2022-08-25 15:47:07+00:00
  edited: false
  hidden: false
  id: 6307a78b161cfe1383a1e8a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2022-08-25T16:48:47.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: "<blockquote>\n<p>I don't want to hi-jack this thread... BUT I notice\
          \ the creator of that article <span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/arteagac\"\
          >@<span class=\"underline\">arteagac</span></a></span>\n\n\t</span></span>,\
          \ is here... </p>\n<p>I used<br>git lfs install<br>export GIT_LFS_SKIP_SMUDGE=1<br>git\
          \ clone <a href=\"https://huggingface.co/bigscience/bloom\">https://huggingface.co/bigscience/bloom</a><br>cd\
          \ bloom<br>git lfs fetch origin <a href=\"/bigscience/bloom/commit/2a3d62e\"\
          >2a3d62e</a></p>\n<p>To retrieve the checkpoint, and it was about 350GB,\
          \ (folder says 330GB... So feels right.. BUT my .BIN files for each shard\
          \ is only 1 KB?)</p>\n</blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;MusashiGarami&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MusashiGarami\"\
          >@<span class=\"underline\">MusashiGarami</span></a></span>\n\n\t</span></span>,<br>I\
          \ edited the instructions. You need to additionally run <code>git lfs checkout</code>.\
          \ Unfortunately, this downloads several linked git files, which makes the\
          \ download almost 700GB. I haven't figured out a way to alternatively download\
          \ only the 330GB that correspond to BLOOM.</p>\n"
        raw: "> I don't want to hi-jack this thread... BUT I notice the creator of\
          \ that article @arteagac, is here... \n> \n> I used\n> git lfs install\n\
          > export GIT_LFS_SKIP_SMUDGE=1\n> git clone https://huggingface.co/bigscience/bloom\n\
          > cd bloom\n> git lfs fetch origin 2a3d62e\n> \n> To retrieve the checkpoint,\
          \ and it was about 350GB, (folder says 330GB... So feels right.. BUT my\
          \ .BIN files for each shard is only 1 KB?)\n\nHi @MusashiGarami,\nI edited\
          \ the instructions. You need to additionally run `git lfs checkout`. Unfortunately,\
          \ this downloads several linked git files, which makes the download almost\
          \ 700GB. I haven't figured out a way to alternatively download only the\
          \ 330GB that correspond to BLOOM."
        updatedAt: '2022-08-25T16:48:47.933Z'
      numEdits: 0
      reactions: []
    id: 6307a7ef161cfe1383a1ed5b
    type: comment
  author: arteagac
  content: "> I don't want to hi-jack this thread... BUT I notice the creator of that\
    \ article @arteagac, is here... \n> \n> I used\n> git lfs install\n> export GIT_LFS_SKIP_SMUDGE=1\n\
    > git clone https://huggingface.co/bigscience/bloom\n> cd bloom\n> git lfs fetch\
    \ origin 2a3d62e\n> \n> To retrieve the checkpoint, and it was about 350GB, (folder\
    \ says 330GB... So feels right.. BUT my .BIN files for each shard is only 1 KB?)\n\
    \nHi @MusashiGarami,\nI edited the instructions. You need to additionally run\
    \ `git lfs checkout`. Unfortunately, this downloads several linked git files,\
    \ which makes the download almost 700GB. I haven't figured out a way to alternatively\
    \ download only the 330GB that correspond to BLOOM."
  created_at: 2022-08-25 15:48:47+00:00
  edited: false
  hidden: false
  id: 6307a7ef161cfe1383a1ed5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2022-08-25T16:55:39.000Z'
    data:
      edited: true
      editors:
      - cakiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
          fullname: Christopher Akiki
          isHf: false
          isPro: false
          name: cakiki
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/arteagac\">@<span class=\"\
          underline\">arteagac</span></a></span>\n\n\t</span></span> You can use the\
          \ <code>huggingface_hub</code> library to filter and download select files:</p>\n\
          <ul>\n<li><a href=\"https://huggingface.co/docs/huggingface_hub/v0.9.1/en/package_reference/hf_api#huggingface_hub.HfApi.list_repo_files\"\
          >https://huggingface.co/docs/huggingface_hub/v0.9.1/en/package_reference/hf_api#huggingface_hub.HfApi.list_repo_files</a></li>\n\
          <li><a href=\"https://huggingface.co/docs/huggingface_hub/v0.9.1/en/package_reference/file_download#huggingface_hub.hf_hub_download\"\
          >https://huggingface.co/docs/huggingface_hub/v0.9.1/en/package_reference/file_download#huggingface_hub.hf_hub_download</a></li>\n\
          </ul>\n"
        raw: '@arteagac You can use the `huggingface_hub` library to filter and download
          select files:

          - https://huggingface.co/docs/huggingface_hub/v0.9.1/en/package_reference/hf_api#huggingface_hub.HfApi.list_repo_files

          - https://huggingface.co/docs/huggingface_hub/v0.9.1/en/package_reference/file_download#huggingface_hub.hf_hub_download'
        updatedAt: '2022-08-26T09:55:36.153Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - stormchaser
    id: 6307a98b4b2a17f13f5b45c0
    type: comment
  author: cakiki
  content: '@arteagac You can use the `huggingface_hub` library to filter and download
    select files:

    - https://huggingface.co/docs/huggingface_hub/v0.9.1/en/package_reference/hf_api#huggingface_hub.HfApi.list_repo_files

    - https://huggingface.co/docs/huggingface_hub/v0.9.1/en/package_reference/file_download#huggingface_hub.hf_hub_download'
  created_at: 2022-08-25 15:55:39+00:00
  edited: true
  hidden: false
  id: 6307a98b4b2a17f13f5b45c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2022-08-25T17:01:31.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;cakiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cakiki\">@<span class=\"\
          underline\">cakiki</span></a></span>\n\n\t</span></span>. This sounds great.\
          \ If it is fine with you, would you mind sharing a specific script to achieve\
          \ this for BLOOM? I think this would benefit several people who are trying\
          \ to download only the specific BLOOM checkpoint that weighs 330GB.</p>\n"
        raw: Hi @cakiki. This sounds great. If it is fine with you, would you mind
          sharing a specific script to achieve this for BLOOM? I think this would
          benefit several people who are trying to download only the specific BLOOM
          checkpoint that weighs 330GB.
        updatedAt: '2022-08-25T17:01:31.157Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - stormchaser
        - MusashiGarami
    id: 6307aaeba670ed10f9cf9456
    type: comment
  author: arteagac
  content: Hi @cakiki. This sounds great. If it is fine with you, would you mind sharing
    a specific script to achieve this for BLOOM? I think this would benefit several
    people who are trying to download only the specific BLOOM checkpoint that weighs
    330GB.
  created_at: 2022-08-25 16:01:31+00:00
  edited: false
  hidden: false
  id: 6307aaeba670ed10f9cf9456
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a424f212ca849d336b873c311526664.svg
      fullname: Limbrzk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Philimipp
      type: user
    createdAt: '2022-08-26T11:43:31.000Z'
    data:
      edited: false
      editors:
      - Philimipp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a424f212ca849d336b873c311526664.svg
          fullname: Limbrzk
          isHf: false
          isPro: false
          name: Philimipp
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;stormchaser&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/stormchaser\"\
          >@<span class=\"underline\">stormchaser</span></a></span>\n\n\t</span></span>\
          \ Your directory is double the size because the binaries are downloaded\
          \ by <code>git lfs pull</code> into your <em>.git</em> directory (in a specific\
          \ binary format). Only after you run <code>git lfs checkout</code> the <em>real</em>\
          \ binary files (e.g. <em>pytorch_model_00054-of-00072.bin</em>) are constructed\
          \ from the contents of the <em>.git</em> directory. After <code>git lfs\
          \ checkout</code> has finished and after you've verified that everything\
          \ works you can just delete the <code>.git</code> directory and free those\
          \ extra 300GB.</p>\n<p>Also, may I kindly ask those folks who have questions\
          \ unrelated to this topic to simply create a new thread? There's no shame\
          \ in asking, even if you think it's a \"noob\" question or if you're not\
          \ a programmer or ml professional. Just ask away, I'll gladly share what\
          \ I've learned - but not inside this mess of a thread of spaghetti messages.</p>\n"
        raw: '@stormchaser Your directory is double the size because the binaries
          are downloaded by `git lfs pull` into your *.git* directory (in a specific
          binary format). Only after you run `git lfs checkout` the *real* binary
          files (e.g. *pytorch_model_00054-of-00072.bin*) are constructed from the
          contents of the *.git* directory. After `git lfs checkout` has finished
          and after you''ve verified that everything works you can just delete the
          `.git` directory and free those extra 300GB.


          Also, may I kindly ask those folks who have questions unrelated to this
          topic to simply create a new thread? There''s no shame in asking, even if
          you think it''s a "noob" question or if you''re not a programmer or ml professional.
          Just ask away, I''ll gladly share what I''ve learned - but not inside this
          mess of a thread of spaghetti messages.'
        updatedAt: '2022-08-26T11:43:31.347Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - stormchaser
    id: 6308b1e36fb2ea4413f331c6
    type: comment
  author: Philimipp
  content: '@stormchaser Your directory is double the size because the binaries are
    downloaded by `git lfs pull` into your *.git* directory (in a specific binary
    format). Only after you run `git lfs checkout` the *real* binary files (e.g. *pytorch_model_00054-of-00072.bin*)
    are constructed from the contents of the *.git* directory. After `git lfs checkout`
    has finished and after you''ve verified that everything works you can just delete
    the `.git` directory and free those extra 300GB.


    Also, may I kindly ask those folks who have questions unrelated to this topic
    to simply create a new thread? There''s no shame in asking, even if you think
    it''s a "noob" question or if you''re not a programmer or ml professional. Just
    ask away, I''ll gladly share what I''ve learned - but not inside this mess of
    a thread of spaghetti messages.'
  created_at: 2022-08-26 10:43:31+00:00
  edited: false
  hidden: false
  id: 6308b1e36fb2ea4413f331c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ef6c2fa6ba45b8dab31072a927c0310.svg
      fullname: WillianZ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: willianz
      type: user
    createdAt: '2022-08-31T14:17:55.000Z'
    data:
      edited: true
      editors:
      - willianz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ef6c2fa6ba45b8dab31072a927c0310.svg
          fullname: WillianZ
          isHf: false
          isPro: false
          name: willianz
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/arteagac\"\
          >@<span class=\"underline\">arteagac</span></a></span>\n\n\t</span></span>.\
          \ I'm trying to infer bloom on my apple silicon Mac (20c 128G), however\
          \ model runs  extremely slow on CPU (60s/layer, seemingly not properly parallelized)\
          \ nor <code>mps</code> backend working properly (outputs identical token\
          \ for various inputs, 0.1s/layer though).</p>\n<p>I'm trying to mitigate\
          \ this by adopting tensorflow-metal (by Apple) which presumably be more\
          \ polished on macOS, however couldn't find an easy way to convert bloom\
          \ pt back to tensorflow checkpoints.<br>I tried to mildly modify both <code>transformers/convert_pytorch_checkpoint_to_tf2.py</code>\
          \ and <code>transformers/commands/pt_to_tf.py</code> and take a chance either\
          \ of them would work. But no luck.<br>The closest I can get to is to force\
          \ <code>pt_to_tf.py</code> to load the whole model then save it , however\
          \ an SIGKILL (most likely OOM kill) kicked in.</p>\n<p>Do you have, by any\
          \ chance, an idea how to properly convert the bin file back to tensorflow\
          \ ckpt  layer by layer on the fly like what you did on loading then inferencing\
          \ them?</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;Philimipp&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Philimipp\"\
          >@<span class=\"underline\">Philimipp</span></a></span>\n\n\t</span></span>\
          \ Sorry if I deviated from the original topic as well.<br>I'm not sure if\
          \ we are suffering from similar problem as I do think m1 share similar architecture\
          \ to AMD processors in terms of CPU cache architectures and lack of AVX512\
          \ support ... and, maybe, not sure if related, or even true claim, negative\
          \ optimization on non-intel CPU to OpenMP framework by Intel .</p>\n"
        raw: "Hi @arteagac. I'm trying to infer bloom on my apple silicon Mac (20c\
          \ 128G), however model runs  extremely slow on CPU (60s/layer, seemingly\
          \ not properly parallelized) nor `mps` backend working properly (outputs\
          \ identical token for various inputs, 0.1s/layer though).\n\nI'm trying\
          \ to mitigate this by adopting tensorflow-metal (by Apple) which presumably\
          \ be more polished on macOS, however couldn't find an easy way to convert\
          \ bloom pt back to tensorflow checkpoints.\nI tried to mildly modify both\
          \ `transformers/convert_pytorch_checkpoint_to_tf2.py` and `transformers/commands/pt_to_tf.py`\
          \ and take a chance either of them would work. But no luck.\nThe closest\
          \ I can get to is to force `pt_to_tf.py` to load the whole model then save\
          \ it , however an SIGKILL (most likely OOM kill) kicked in.\n\nDo you have,\
          \ by any chance, an idea how to properly convert the bin file back to tensorflow\
          \ ckpt  layer by layer on the fly like what you did on loading then inferencing\
          \ them?\n\n@Philimipp Sorry if I deviated from the original topic as well.\
          \ \nI'm not sure if we are suffering from similar problem as I do think\
          \ m1 share similar architecture to AMD processors in terms of CPU cache\
          \ architectures and lack of AVX512 support ... and, maybe, not sure if related,\
          \ or even true claim, negative optimization on non-intel CPU to OpenMP framework\
          \ by Intel ."
        updatedAt: '2022-08-31T14:41:00.374Z'
      numEdits: 9
      reactions: []
    id: 630f6d93c3962881af73e593
    type: comment
  author: willianz
  content: "Hi @arteagac. I'm trying to infer bloom on my apple silicon Mac (20c 128G),\
    \ however model runs  extremely slow on CPU (60s/layer, seemingly not properly\
    \ parallelized) nor `mps` backend working properly (outputs identical token for\
    \ various inputs, 0.1s/layer though).\n\nI'm trying to mitigate this by adopting\
    \ tensorflow-metal (by Apple) which presumably be more polished on macOS, however\
    \ couldn't find an easy way to convert bloom pt back to tensorflow checkpoints.\n\
    I tried to mildly modify both `transformers/convert_pytorch_checkpoint_to_tf2.py`\
    \ and `transformers/commands/pt_to_tf.py` and take a chance either of them would\
    \ work. But no luck.\nThe closest I can get to is to force `pt_to_tf.py` to load\
    \ the whole model then save it , however an SIGKILL (most likely OOM kill) kicked\
    \ in.\n\nDo you have, by any chance, an idea how to properly convert the bin file\
    \ back to tensorflow ckpt  layer by layer on the fly like what you did on loading\
    \ then inferencing them?\n\n@Philimipp Sorry if I deviated from the original topic\
    \ as well. \nI'm not sure if we are suffering from similar problem as I do think\
    \ m1 share similar architecture to AMD processors in terms of CPU cache architectures\
    \ and lack of AVX512 support ... and, maybe, not sure if related, or even true\
    \ claim, negative optimization on non-intel CPU to OpenMP framework by Intel ."
  created_at: 2022-08-31 13:17:55+00:00
  edited: true
  hidden: false
  id: 630f6d93c3962881af73e593
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2022-08-31T16:03:50.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;willianz&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/willianz\"\
          >@<span class=\"underline\">willianz</span></a></span>\n\n\t</span></span>.\
          \ Converting the checkpoints on the fly might be time consuming. Perhaps\
          \ a better option is to convert the entire model to  TF's checkpoints. This\
          \ should be doable by creating a script that takes each of the 72 PyTorch\
          \ bin files and converts them to TF checkpoints, one by one to avoid OOM.\
          \  However, even if you have the model in TF's format, you may need to write\
          \ your own BLOOM Tensorflow source code, because as far as I know, the current\
          \ BLOOM source code in the Transformers repo has only a PyTorch version.\
          \ Have you alternatively tried running an optimized version of PyTorch for\
          \ MAC (e.g., <a rel=\"nofollow\" href=\"https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\"\
          >https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/</a>)?</p>\n"
        raw: Hi @willianz. Converting the checkpoints on the fly might be time consuming.
          Perhaps a better option is to convert the entire model to  TF's checkpoints.
          This should be doable by creating a script that takes each of the 72 PyTorch
          bin files and converts them to TF checkpoints, one by one to avoid OOM.  However,
          even if you have the model in TF's format, you may need to write your own
          BLOOM Tensorflow source code, because as far as I know, the current BLOOM
          source code in the Transformers repo has only a PyTorch version. Have you
          alternatively tried running an optimized version of PyTorch for MAC (e.g.,
          https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)?
        updatedAt: '2022-08-31T16:03:50.683Z'
      numEdits: 0
      reactions: []
    id: 630f8666c3962881af75137a
    type: comment
  author: arteagac
  content: Hi @willianz. Converting the checkpoints on the fly might be time consuming.
    Perhaps a better option is to convert the entire model to  TF's checkpoints. This
    should be doable by creating a script that takes each of the 72 PyTorch bin files
    and converts them to TF checkpoints, one by one to avoid OOM.  However, even if
    you have the model in TF's format, you may need to write your own BLOOM Tensorflow
    source code, because as far as I know, the current BLOOM source code in the Transformers
    repo has only a PyTorch version. Have you alternatively tried running an optimized
    version of PyTorch for MAC (e.g., https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)?
  created_at: 2022-08-31 15:03:50+00:00
  edited: false
  hidden: false
  id: 630f8666c3962881af75137a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ef6c2fa6ba45b8dab31072a927c0310.svg
      fullname: WillianZ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: willianz
      type: user
    createdAt: '2022-08-31T17:17:48.000Z'
    data:
      edited: true
      editors:
      - willianz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ef6c2fa6ba45b8dab31072a927c0310.svg
          fullname: WillianZ
          isHf: false
          isPro: false
          name: willianz
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/arteagac\">@<span class=\"\
          underline\">arteagac</span></a></span>\n\n\t</span></span> </p>\n<blockquote>\n\
          <p>Hi <span data-props=\"{&quot;user&quot;:&quot;willianz&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/willianz\">@<span class=\"\
          underline\">willianz</span></a></span>\n\n\t</span></span>. Converting the\
          \ checkpoints on the fly might be time consuming. Perhaps a better option\
          \ is to convert the entire model to  TF's checkpoints. This should be doable\
          \ by creating a script that takes each of the 72 PyTorch bin files and converts\
          \ them to TF checkpoints, one by one to avoid OOM.  However, even if you\
          \ have the model in TF's format, you may need to write your own BLOOM Tensorflow\
          \ source code, because as far as I know, the current BLOOM source code in\
          \ the Transformers repo has only a PyTorch version. Have you alternatively\
          \ tried running an optimized version of PyTorch for MAC (e.g., <a rel=\"\
          nofollow\" href=\"https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\"\
          >https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/</a>)?</p>\n\
          </blockquote>\n<p>Sorry for the confusing wording on <code>on the fly</code>,\
          \ actually I meant, as you mentioned, in comparison to what <code>pt_to_tf.py</code>\
          \ did, to load the whole model then resave, convert them layer by layer.</p>\n\
          <blockquote>\n<p>you may need to write your own BLOOM Tensorflow source\
          \ code</p>\n</blockquote>\n<p>From my shallow understanding, it should be\
          \ very much identical to what you've written on the Medium Blog Post, the\
          \ only difference would be instead of using pretrained PyTorch Module (completely\
          \ describable with TorchScript), I should be using TF Graph or TF Model\
          \ with trivial loading code for those converted checkpoints.<br>The thing\
          \ I am not sure is that from a quick glance on the <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/convert_bloom_original_checkpoint_to_pytorch.py\"\
          >Bloom Convert</a> script. There seems to be some custom Op conversion logics\
          \ included for torch module construction, I am not sure if that is also\
          \ needed for tf on back convert.</p>\n<blockquote>\n<p>Have you alternatively\
          \ tried running an optimized version of PyTorch for MAC</p>\n</blockquote>\n\
          <p>Yes I did, actually this is exactly the 0.1s/layer \"mps\" backend I\
          \ was talking about. =)<br>Actually, I have submitted an <a rel=\"nofollow\"\
          \ href=\"https://github.com/pytorch/pytorch/issues/84169\">issue</a> most\
          \ likely to be the same one with smaller model size on this.</p>\n"
        raw: "@arteagac \n> Hi @willianz. Converting the checkpoints on the fly might\
          \ be time consuming. Perhaps a better option is to convert the entire model\
          \ to  TF's checkpoints. This should be doable by creating a script that\
          \ takes each of the 72 PyTorch bin files and converts them to TF checkpoints,\
          \ one by one to avoid OOM.  However, even if you have the model in TF's\
          \ format, you may need to write your own BLOOM Tensorflow source code, because\
          \ as far as I know, the current BLOOM source code in the Transformers repo\
          \ has only a PyTorch version. Have you alternatively tried running an optimized\
          \ version of PyTorch for MAC (e.g., https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)?\n\
          \nSorry for the confusing wording on `on the fly`, actually I meant, as\
          \ you mentioned, in comparison to what `pt_to_tf.py` did, to load the whole\
          \ model then resave, convert them layer by layer.\n\n> you may need to write\
          \ your own BLOOM Tensorflow source code\n\nFrom my shallow understanding,\
          \ it should be very much identical to what you've written on the Medium\
          \ Blog Post, the only difference would be instead of using pretrained PyTorch\
          \ Module (completely describable with TorchScript), I should be using TF\
          \ Graph or TF Model with trivial loading code for those converted checkpoints.\n\
          The thing I am not sure is that from a quick glance on the [Bloom Convert](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/convert_bloom_original_checkpoint_to_pytorch.py)\
          \ script. There seems to be some custom Op conversion logics included for\
          \ torch module construction, I am not sure if that is also needed for tf\
          \ on back convert.\n\n> Have you alternatively tried running an optimized\
          \ version of PyTorch for MAC\n\nYes I did, actually this is exactly the\
          \ 0.1s/layer \"mps\" backend I was talking about. =) \nActually, I have\
          \ submitted an [issue](https://github.com/pytorch/pytorch/issues/84169)\
          \ most likely to be the same one with smaller model size on this."
        updatedAt: '2022-09-01T13:46:56.257Z'
      numEdits: 4
      reactions: []
    id: 630f97bce52a259b85638733
    type: comment
  author: willianz
  content: "@arteagac \n> Hi @willianz. Converting the checkpoints on the fly might\
    \ be time consuming. Perhaps a better option is to convert the entire model to\
    \  TF's checkpoints. This should be doable by creating a script that takes each\
    \ of the 72 PyTorch bin files and converts them to TF checkpoints, one by one\
    \ to avoid OOM.  However, even if you have the model in TF's format, you may need\
    \ to write your own BLOOM Tensorflow source code, because as far as I know, the\
    \ current BLOOM source code in the Transformers repo has only a PyTorch version.\
    \ Have you alternatively tried running an optimized version of PyTorch for MAC\
    \ (e.g., https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)?\n\
    \nSorry for the confusing wording on `on the fly`, actually I meant, as you mentioned,\
    \ in comparison to what `pt_to_tf.py` did, to load the whole model then resave,\
    \ convert them layer by layer.\n\n> you may need to write your own BLOOM Tensorflow\
    \ source code\n\nFrom my shallow understanding, it should be very much identical\
    \ to what you've written on the Medium Blog Post, the only difference would be\
    \ instead of using pretrained PyTorch Module (completely describable with TorchScript),\
    \ I should be using TF Graph or TF Model with trivial loading code for those converted\
    \ checkpoints.\nThe thing I am not sure is that from a quick glance on the [Bloom\
    \ Convert](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/convert_bloom_original_checkpoint_to_pytorch.py)\
    \ script. There seems to be some custom Op conversion logics included for torch\
    \ module construction, I am not sure if that is also needed for tf on back convert.\n\
    \n> Have you alternatively tried running an optimized version of PyTorch for MAC\n\
    \nYes I did, actually this is exactly the 0.1s/layer \"mps\" backend I was talking\
    \ about. =) \nActually, I have submitted an [issue](https://github.com/pytorch/pytorch/issues/84169)\
    \ most likely to be the same one with smaller model size on this."
  created_at: 2022-08-31 16:17:48+00:00
  edited: true
  hidden: false
  id: 630f97bce52a259b85638733
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674490197803-63ceae72257cc48a04f94ed3.jpeg?w=200&h=200&f=face
      fullname: "Marek \u0141abuzek"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marek-Labuzek-at-Capg
      type: user
    createdAt: '2023-01-23T16:28:04.000Z'
    data:
      edited: true
      editors:
      - Marek-Labuzek-at-Capg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674490197803-63ceae72257cc48a04f94ed3.jpeg?w=200&h=200&f=face
          fullname: "Marek \u0141abuzek"
          isHf: false
          isPro: false
          name: Marek-Labuzek-at-Capg
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/arteagac\">@<span class=\"\
          underline\">arteagac</span></a></span>\n\n\t</span></span> Many thanks for\
          \ this work, it's exciting to run 'essence of the internet' :) on your local\
          \ env!<br>For the benefit of others that may still arrive here, I wanted\
          \ to note that there were a few corrections needed in the code to make it\
          \ running as expected, e.g. reproduce example with SQL command from the\
          \ article.<br>Most importantly I needed to copy from BloomModel the function\
          \ _prepare_attn_mask to create causal mask and pass it to block's forward\
          \ - instead of original attention mask, still used to create alibi. And\
          \ talking about alibi, it was the other change to make the code running\
          \ at all - pass attention_mask to build_alibi_tensor function as first argument.</p>\n\
          <p>I was running the code on the server without GPU but with 8 cores and\
          \ the sheer calculations were very fast, around a second, burning all CPU\
          \ cores nicely :) Unfortunately disk was horribly slow, loading a block\
          \ lasted minutes, resulting with a few hours to get one token.<br>Then I've\
          \ moved to a machine with 2 GPUs (24GB each) and 64GB RAM (CPU) and a decent\
          \ HDD. First set back was that CPU calculations didn't run multithreaded,\
          \ despite decent Intel (i9 with 24 cores) on board. I've pushed all calculations\
          \ to GPU and then it started to run in the reasonable limits. I've made\
          \ a few efforts to speed things up, e.g. keeping everything beside blocks\
          \ in GPU (embeddings, lnorm and head) and as much as possible blocks in\
          \ CPU RAM - pushing them to GPU when needed. I confirm that it's possible\
          \ to get on token generated below 5 minutes :)</p>\n"
        raw: "@arteagac Many thanks for this work, it's exciting to run 'essence of\
          \ the internet' :) on your local env! \nFor the benefit of others that may\
          \ still arrive here, I wanted to note that there were a few corrections\
          \ needed in the code to make it running as expected, e.g. reproduce example\
          \ with SQL command from the article.\nMost importantly I needed to copy\
          \ from BloomModel the function _prepare_attn_mask to create causal mask\
          \ and pass it to block's forward - instead of original attention mask, still\
          \ used to create alibi. And talking about alibi, it was the other change\
          \ to make the code running at all - pass attention_mask to build_alibi_tensor\
          \ function as first argument.\n\nI was running the code on the server without\
          \ GPU but with 8 cores and the sheer calculations were very fast, around\
          \ a second, burning all CPU cores nicely :) Unfortunately disk was horribly\
          \ slow, loading a block lasted minutes, resulting with a few hours to get\
          \ one token. \nThen I've moved to a machine with 2 GPUs (24GB each) and\
          \ 64GB RAM (CPU) and a decent HDD. First set back was that CPU calculations\
          \ didn't run multithreaded, despite decent Intel (i9 with 24 cores) on board.\
          \ I've pushed all calculations to GPU and then it started to run in the\
          \ reasonable limits. I've made a few efforts to speed things up, e.g. keeping\
          \ everything beside blocks in GPU (embeddings, lnorm and head) and as much\
          \ as possible blocks in CPU RAM - pushing them to GPU when needed. I confirm\
          \ that it's possible to get on token generated below 5 minutes :)"
        updatedAt: '2023-01-23T16:29:35.320Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - stormchaser
    id: 63ceb5941378c94f84c42296
    type: comment
  author: Marek-Labuzek-at-Capg
  content: "@arteagac Many thanks for this work, it's exciting to run 'essence of\
    \ the internet' :) on your local env! \nFor the benefit of others that may still\
    \ arrive here, I wanted to note that there were a few corrections needed in the\
    \ code to make it running as expected, e.g. reproduce example with SQL command\
    \ from the article.\nMost importantly I needed to copy from BloomModel the function\
    \ _prepare_attn_mask to create causal mask and pass it to block's forward - instead\
    \ of original attention mask, still used to create alibi. And talking about alibi,\
    \ it was the other change to make the code running at all - pass attention_mask\
    \ to build_alibi_tensor function as first argument.\n\nI was running the code\
    \ on the server without GPU but with 8 cores and the sheer calculations were very\
    \ fast, around a second, burning all CPU cores nicely :) Unfortunately disk was\
    \ horribly slow, loading a block lasted minutes, resulting with a few hours to\
    \ get one token. \nThen I've moved to a machine with 2 GPUs (24GB each) and 64GB\
    \ RAM (CPU) and a decent HDD. First set back was that CPU calculations didn't\
    \ run multithreaded, despite decent Intel (i9 with 24 cores) on board. I've pushed\
    \ all calculations to GPU and then it started to run in the reasonable limits.\
    \ I've made a few efforts to speed things up, e.g. keeping everything beside blocks\
    \ in GPU (embeddings, lnorm and head) and as much as possible blocks in CPU RAM\
    \ - pushing them to GPU when needed. I confirm that it's possible to get on token\
    \ generated below 5 minutes :)"
  created_at: 2023-01-23 16:28:04+00:00
  edited: true
  hidden: false
  id: 63ceb5941378c94f84c42296
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
      fullname: Cristian Arteaga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arteagac
      type: user
    createdAt: '2023-01-23T21:23:40.000Z'
    data:
      edited: false
      editors:
      - arteagac
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624505795996-60d3fc6d07da9c17c7270922.jpeg?w=200&h=200&f=face
          fullname: Cristian Arteaga
          isHf: false
          isPro: false
          name: arteagac
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Marek-Labuzek-at-Capg&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Marek-Labuzek-at-Capg\"\
          >@<span class=\"underline\">Marek-Labuzek-at-Capg</span></a></span>\n\n\t\
          </span></span>, I am glad you found the blog post useful. Perhaps you needed\
          \ to make some corrections because of the version of  the<code>transformers</code>\
          \ library you use.  Are you using<code>transformers==4.20.0</code> as specified\
          \ in the blog post? In any case, I think you figured out the updates necessary\
          \ to make it work with newer versions of transformers, and I am glad it\
          \ worked out for you.<br>In terms of speed, using a very fast hard drive\
          \ will definetely help. I think using a GPU helps, but the speed gains are\
          \ not significant, as the bottleneck is in the reading from disk. In a hypothetical\
          \ scenario where you had enough RAM to fit the entire model at once, then\
          \ the GPU would significantly reduce processing times.</p>\n"
        raw: 'Hi @Marek-Labuzek-at-Capg, I am glad you found the blog post useful.
          Perhaps you needed to make some corrections because of the version of  the`transformers`
          library you use.  Are you using`transformers==4.20.0` as specified in the
          blog post? In any case, I think you figured out the updates necessary to
          make it work with newer versions of transformers, and I am glad it worked
          out for you.

          In terms of speed, using a very fast hard drive will definetely help. I
          think using a GPU helps, but the speed gains are not significant, as the
          bottleneck is in the reading from disk. In a hypothetical scenario where
          you had enough RAM to fit the entire model at once, then the GPU would significantly
          reduce processing times.'
        updatedAt: '2023-01-23T21:23:40.673Z'
      numEdits: 0
      reactions: []
    id: 63cefadc14ef989d7afd185b
    type: comment
  author: arteagac
  content: 'Hi @Marek-Labuzek-at-Capg, I am glad you found the blog post useful. Perhaps
    you needed to make some corrections because of the version of  the`transformers`
    library you use.  Are you using`transformers==4.20.0` as specified in the blog
    post? In any case, I think you figured out the updates necessary to make it work
    with newer versions of transformers, and I am glad it worked out for you.

    In terms of speed, using a very fast hard drive will definetely help. I think
    using a GPU helps, but the speed gains are not significant, as the bottleneck
    is in the reading from disk. In a hypothetical scenario where you had enough RAM
    to fit the entire model at once, then the GPU would significantly reduce processing
    times.'
  created_at: 2023-01-23 21:23:40+00:00
  edited: false
  hidden: false
  id: 63cefadc14ef989d7afd185b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674490197803-63ceae72257cc48a04f94ed3.jpeg?w=200&h=200&f=face
      fullname: "Marek \u0141abuzek"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marek-Labuzek-at-Capg
      type: user
    createdAt: '2023-01-24T17:33:18.000Z'
    data:
      edited: false
      editors:
      - Marek-Labuzek-at-Capg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674490197803-63ceae72257cc48a04f94ed3.jpeg?w=200&h=200&f=face
          fullname: "Marek \u0141abuzek"
          isHf: false
          isPro: false
          name: Marek-Labuzek-at-Capg
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;arteagac&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/arteagac\">@<span class=\"\
          underline\">arteagac</span></a></span>\n\n\t</span></span> you are right,\
          \ that must be the reason, I'm on transformers 4.24.0, and pytroch 1.12.1\
          \ which may also make a difference.</p>\n"
        raw: '@arteagac you are right, that must be the reason, I''m on transformers
          4.24.0, and pytroch 1.12.1 which may also make a difference.'
        updatedAt: '2023-01-24T17:33:18.559Z'
      numEdits: 0
      reactions: []
    id: 63d0165e6ffd32ed009a8ffb
    type: comment
  author: Marek-Labuzek-at-Capg
  content: '@arteagac you are right, that must be the reason, I''m on transformers
    4.24.0, and pytroch 1.12.1 which may also make a difference.'
  created_at: 2023-01-24 17:33:18+00:00
  edited: false
  hidden: false
  id: 63d0165e6ffd32ed009a8ffb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f04f23fb26c7d5fa51aa9667c0e73ba3.svg
      fullname: Weilian Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hexeon
      type: user
    createdAt: '2023-02-02T23:46:11.000Z'
    data:
      edited: false
      editors:
      - Hexeon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f04f23fb26c7d5fa51aa9667c0e73ba3.svg
          fullname: Weilian Zhang
          isHf: false
          isPro: false
          name: Hexeon
          type: user
        html: '<p>Hi everyone,</p>

          <p>I am quite a newbie with no prior experience in machine learning. I have
          followed the instructions entailed in the original article by Cristian Arteaga.
          What started my curiosity originally was that I was very impressed by ChatGPT
          but it was not always available, so I thought I might try to run something
          similar to it but truly opensource on my own computer, even if it weren''t
          as good, and maybe I could tweak it when I got more familiar with it. I
          found Arteaga''s article thru Googling and it was the first and only article
          I''ve found online with enough detailed instructions to get someone like
          me started. Anyway after following every step and some Googling to fill
          in the gaps, I was able to duplicate the result on a machine equipped with
          4 x Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz and 512GB of RAM with no GPU,
          running Ubuntu 22.04 LTS, with the following observations:</p>

          <ol>

          <li>performance was about 6.8 minutes per token;</li>

          <li>CPU usage seemed to oscillate between using anywhere from 1 to 8 cores,
          based on %CPU reported by top, even though the machine has 80 cores;</li>

          <li>Memory usage seemed to oscillate between 6GB and 10GB;</li>

          <li>Virtual memory usage was around 25GB.</li>

          </ol>

          <p>I have some questions too:</p>

          <ol>

          <li>Since the machine has a large amount of RAM, could this be utilized
          to enhance the performance?</li>

          <li>Same with the utilization of CPUs, it seems that it''s only using a
          small percentage of the cores.</li>

          <li>How can I get it to run interactively, in a chatbot fashion?</li>

          <li>I''ve noticed that there are newer versions than commit ID <a href="/bigscience/bloom/commit/2a3d62e">2a3d62e</a>
          used in the original article. Do later commits require more hard drive space
          and RAM to run?</li>

          </ol>

          <p>Thanks!</p>

          '
        raw: 'Hi everyone,


          I am quite a newbie with no prior experience in machine learning. I have
          followed the instructions entailed in the original article by Cristian Arteaga.
          What started my curiosity originally was that I was very impressed by ChatGPT
          but it was not always available, so I thought I might try to run something
          similar to it but truly opensource on my own computer, even if it weren''t
          as good, and maybe I could tweak it when I got more familiar with it. I
          found Arteaga''s article thru Googling and it was the first and only article
          I''ve found online with enough detailed instructions to get someone like
          me started. Anyway after following every step and some Googling to fill
          in the gaps, I was able to duplicate the result on a machine equipped with
          4 x Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz and 512GB of RAM with no GPU,
          running Ubuntu 22.04 LTS, with the following observations:


          1) performance was about 6.8 minutes per token;

          2) CPU usage seemed to oscillate between using anywhere from 1 to 8 cores,
          based on %CPU reported by top, even though the machine has 80 cores;

          3) Memory usage seemed to oscillate between 6GB and 10GB;

          4) Virtual memory usage was around 25GB.


          I have some questions too:


          1) Since the machine has a large amount of RAM, could this be utilized to
          enhance the performance?

          2) Same with the utilization of CPUs, it seems that it''s only using a small
          percentage of the cores.

          3) How can I get it to run interactively, in a chatbot fashion?

          4) I''ve noticed that there are newer versions than commit ID 2a3d62e used
          in the original article. Do later commits require more hard drive space
          and RAM to run?


          Thanks!'
        updatedAt: '2023-02-02T23:46:11.634Z'
      numEdits: 0
      reactions: []
    id: 63dc4b438438e03c87df14dd
    type: comment
  author: Hexeon
  content: 'Hi everyone,


    I am quite a newbie with no prior experience in machine learning. I have followed
    the instructions entailed in the original article by Cristian Arteaga. What started
    my curiosity originally was that I was very impressed by ChatGPT but it was not
    always available, so I thought I might try to run something similar to it but
    truly opensource on my own computer, even if it weren''t as good, and maybe I
    could tweak it when I got more familiar with it. I found Arteaga''s article thru
    Googling and it was the first and only article I''ve found online with enough
    detailed instructions to get someone like me started. Anyway after following every
    step and some Googling to fill in the gaps, I was able to duplicate the result
    on a machine equipped with 4 x Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz and 512GB
    of RAM with no GPU, running Ubuntu 22.04 LTS, with the following observations:


    1) performance was about 6.8 minutes per token;

    2) CPU usage seemed to oscillate between using anywhere from 1 to 8 cores, based
    on %CPU reported by top, even though the machine has 80 cores;

    3) Memory usage seemed to oscillate between 6GB and 10GB;

    4) Virtual memory usage was around 25GB.


    I have some questions too:


    1) Since the machine has a large amount of RAM, could this be utilized to enhance
    the performance?

    2) Same with the utilization of CPUs, it seems that it''s only using a small percentage
    of the cores.

    3) How can I get it to run interactively, in a chatbot fashion?

    4) I''ve noticed that there are newer versions than commit ID 2a3d62e used in
    the original article. Do later commits require more hard drive space and RAM to
    run?


    Thanks!'
  created_at: 2023-02-02 23:46:11+00:00
  edited: false
  hidden: false
  id: 63dc4b438438e03c87df14dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674490197803-63ceae72257cc48a04f94ed3.jpeg?w=200&h=200&f=face
      fullname: "Marek \u0141abuzek"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marek-Labuzek-at-Capg
      type: user
    createdAt: '2023-02-06T10:36:12.000Z'
    data:
      edited: true
      editors:
      - Marek-Labuzek-at-Capg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674490197803-63ceae72257cc48a04f94ed3.jpeg?w=200&h=200&f=face
          fullname: "Marek \u0141abuzek"
          isHf: false
          isPro: false
          name: Marek-Labuzek-at-Capg
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Hexeon&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Hexeon\">@<span class=\"\
          underline\">Hexeon</span></a></span>\n\n\t</span></span><br>Nice to hear\
          \ that somebody is following the same path :) </p>\n<ol>\n<li>As mentioned\
          \ above, I've cached as much data as it was possible to fit into CPU &amp;\
          \ GPU memory and got some boost - with time per token going down from ca.\
          \ 4 minutes down below 3 minutes - still not satisfying :) but always some\
          \ gain...</li>\n<li>Try checking and setting number of threads pytorch is\
          \ using: torch.set/get_num_threads, torch.set/get_num_interop_threads<br>ADDITIONALLY:\
          \ But I also observe on one of my servers (Intel based, with mkl installed\
          \ - but I still need to check that all is ok there) that only few threads\
          \ are used, below what is available (20 threads), while on the other, I\
          \ can see that all 24 are used (for nodes forward calculations).</li>\n\
          <li>Bloom is generative model like GPT3. chatGPT was created by finetuning\
          \ GPT3 with RLHR approach (check articles by OpenAI or Huggingface). There\
          \ is BLOOMZ model, check:<br><a href=\"https://huggingface.co/bigscience/bloomz\"\
          >https://huggingface.co/bigscience/bloomz</a></li>\n<li>github is saying\
          \ that the commit changed only documentation :)</li>\n</ol>\n"
        raw: "Hi @Hexeon \nNice to hear that somebody is following the same path :)\
          \ \n1. As mentioned above, I've cached as much data as it was possible to\
          \ fit into CPU & GPU memory and got some boost - with time per token going\
          \ down from ca. 4 minutes down below 3 minutes - still not satisfying :)\
          \ but always some gain...\n2. Try checking and setting number of threads\
          \ pytorch is using: torch.set/get_num_threads, torch.set/get_num_interop_threads\n\
          ADDITIONALLY: But I also observe on one of my servers (Intel based, with\
          \ mkl installed - but I still need to check that all is ok there) that only\
          \ few threads are used, below what is available (20 threads), while on the\
          \ other, I can see that all 24 are used (for nodes forward calculations).\n\
          3. Bloom is generative model like GPT3. chatGPT was created by finetuning\
          \ GPT3 with RLHR approach (check articles by OpenAI or Huggingface). There\
          \ is BLOOMZ model, check:\nhttps://huggingface.co/bigscience/bloomz\n4.\
          \ github is saying that the commit changed only documentation :)"
        updatedAt: '2023-02-06T11:41:19.259Z'
      numEdits: 2
      reactions: []
    id: 63e0d81cc09e55e1186e6c5e
    type: comment
  author: Marek-Labuzek-at-Capg
  content: "Hi @Hexeon \nNice to hear that somebody is following the same path :)\
    \ \n1. As mentioned above, I've cached as much data as it was possible to fit\
    \ into CPU & GPU memory and got some boost - with time per token going down from\
    \ ca. 4 minutes down below 3 minutes - still not satisfying :) but always some\
    \ gain...\n2. Try checking and setting number of threads pytorch is using: torch.set/get_num_threads,\
    \ torch.set/get_num_interop_threads\nADDITIONALLY: But I also observe on one of\
    \ my servers (Intel based, with mkl installed - but I still need to check that\
    \ all is ok there) that only few threads are used, below what is available (20\
    \ threads), while on the other, I can see that all 24 are used (for nodes forward\
    \ calculations).\n3. Bloom is generative model like GPT3. chatGPT was created\
    \ by finetuning GPT3 with RLHR approach (check articles by OpenAI or Huggingface).\
    \ There is BLOOMZ model, check:\nhttps://huggingface.co/bigscience/bloomz\n4.\
    \ github is saying that the commit changed only documentation :)"
  created_at: 2023-02-06 10:36:12+00:00
  edited: true
  hidden: false
  id: 63e0d81cc09e55e1186e6c5e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c87e3f20a8ec3d9b281540b37dee46e.svg
      fullname: H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: EsaAI
      type: user
    createdAt: '2023-02-10T10:06:32.000Z'
    data:
      edited: true
      editors:
      - EsaAI
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c87e3f20a8ec3d9b281540b37dee46e.svg
          fullname: H
          isHf: false
          isPro: false
          name: EsaAI
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Marek-Labuzek-at-Capg&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Marek-Labuzek-at-Capg\"\
          >@<span class=\"underline\">Marek-Labuzek-at-Capg</span></a></span>\n\n\t\
          </span></span>,<br>your results are interesting :) I run the script from\
          \ Arteaga and get a similar problem on my computer (Intel i9 13900k, 64\
          \ GB RAM, GeForce RTX 2080 Super, Samsung 980 PRO M.2 NVMe).<br><span data-props=\"\
          {&quot;user&quot;:&quot;Arteaga&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Arteaga\">@<span class=\"underline\">Arteaga</span></a></span>\n\
          \n\t</span></span>: many thanks for you work. You helped a lot to understand\
          \ the topic.</p>\n<p>When I run the script on Ubuntu 22.04 with Kernel 6.1.11-060111-generic\
          \ it takes 33 minutes for one token. I run the same script on fresh installed\
          \ Win10 with newest driver and it's a little faster with 25 minutes for\
          \ one token. In both cases just one core out of 32 is used when execute\
          \ \"block(....)\" which takes the longest time.<br>I also checked torch.get_num_threads\
          \ and torch.get_num_interop_threads. It is set to 24 and 16 by default.</p>\n\
          <p>Is it possible that pytorch don't support this CPU and that's the reason\
          \ why the script can't run multithreaded?</p>\n"
        raw: 'Hi @Marek-Labuzek-at-Capg,

          your results are interesting :) I run the script from Arteaga and get a
          similar problem on my computer (Intel i9 13900k, 64 GB RAM, GeForce RTX
          2080 Super, Samsung 980 PRO M.2 NVMe).

          @Arteaga: many thanks for you work. You helped a lot to understand the topic.


          When I run the script on Ubuntu 22.04 with Kernel 6.1.11-060111-generic
          it takes 33 minutes for one token. I run the same script on fresh installed
          Win10 with newest driver and it''s a little faster with 25 minutes for one
          token. In both cases just one core out of 32 is used when execute "block(....)"
          which takes the longest time.

          I also checked torch.get_num_threads and torch.get_num_interop_threads.
          It is set to 24 and 16 by default.


          Is it possible that pytorch don''t support this CPU and that''s the reason
          why the script can''t run multithreaded?'
        updatedAt: '2023-02-10T13:16:57.734Z'
      numEdits: 7
      reactions: []
    id: 63e6172863037c7d960493c6
    type: comment
  author: EsaAI
  content: 'Hi @Marek-Labuzek-at-Capg,

    your results are interesting :) I run the script from Arteaga and get a similar
    problem on my computer (Intel i9 13900k, 64 GB RAM, GeForce RTX 2080 Super, Samsung
    980 PRO M.2 NVMe).

    @Arteaga: many thanks for you work. You helped a lot to understand the topic.


    When I run the script on Ubuntu 22.04 with Kernel 6.1.11-060111-generic it takes
    33 minutes for one token. I run the same script on fresh installed Win10 with
    newest driver and it''s a little faster with 25 minutes for one token. In both
    cases just one core out of 32 is used when execute "block(....)" which takes the
    longest time.

    I also checked torch.get_num_threads and torch.get_num_interop_threads. It is
    set to 24 and 16 by default.


    Is it possible that pytorch don''t support this CPU and that''s the reason why
    the script can''t run multithreaded?'
  created_at: 2023-02-10 10:06:32+00:00
  edited: true
  hidden: false
  id: 63e6172863037c7d960493c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674490197803-63ceae72257cc48a04f94ed3.jpeg?w=200&h=200&f=face
      fullname: "Marek \u0141abuzek"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marek-Labuzek-at-Capg
      type: user
    createdAt: '2023-02-10T16:55:49.000Z'
    data:
      edited: false
      editors:
      - Marek-Labuzek-at-Capg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674490197803-63ceae72257cc48a04f94ed3.jpeg?w=200&h=200&f=face
          fullname: "Marek \u0141abuzek"
          isHf: false
          isPro: false
          name: Marek-Labuzek-at-Capg
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;EsaAI&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/EsaAI\">@<span class=\"\
          underline\">EsaAI</span></a></span>\n\n\t</span></span> ,<br>As you have\
          \ GPU on board you can move calculation to it. If you set variable device='cuda:0'\
          \ (or whatever it is on you set-up) then it should work out of the box.</p>\n"
        raw: 'Hi @EsaAI ,

          As you have GPU on board you can move calculation to it. If you set variable
          device=''cuda:0'' (or whatever it is on you set-up) then it should work
          out of the box.'
        updatedAt: '2023-02-10T16:55:49.123Z'
      numEdits: 0
      reactions: []
    id: 63e6771563037c7d96159381
    type: comment
  author: Marek-Labuzek-at-Capg
  content: 'Hi @EsaAI ,

    As you have GPU on board you can move calculation to it. If you set variable device=''cuda:0''
    (or whatever it is on you set-up) then it should work out of the box.'
  created_at: 2023-02-10 16:55:49+00:00
  edited: false
  hidden: false
  id: 63e6771563037c7d96159381
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674490197803-63ceae72257cc48a04f94ed3.jpeg?w=200&h=200&f=face
      fullname: "Marek \u0141abuzek"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marek-Labuzek-at-Capg
      type: user
    createdAt: '2023-02-24T13:55:06.000Z'
    data:
      edited: false
      editors:
      - Marek-Labuzek-at-Capg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674490197803-63ceae72257cc48a04f94ed3.jpeg?w=200&h=200&f=face
          fullname: "Marek \u0141abuzek"
          isHf: false
          isPro: false
          name: Marek-Labuzek-at-Capg
          type: user
        html: '<p>I observe modelling leaking memory, both in the setup from the article
          - loading blocks one by one, and same when running on a machine with big
          enough (CPU) memory to load whole model.<br>I''ve added used mem dumps when
          each block is evaluated and it constantly goes up over time - on self_attention.forwad
          and mlp.<br>The effect is that generating bigger number of blocks needs
          bigger memory..<br>Anybody observing the same? Any fixes / ideas?</p>

          '
        raw: "I observe modelling leaking memory, both in the setup from the article\
          \ - loading blocks one by one, and same when running on a machine with big\
          \ enough (CPU) memory to load whole model.\nI've added used mem dumps when\
          \ each block is evaluated and it constantly goes up over time - on self_attention.forwad\
          \ and mlp. \nThe effect is that generating bigger number of blocks needs\
          \ bigger memory..\nAnybody observing the same? Any fixes / ideas?"
        updatedAt: '2023-02-24T13:55:06.959Z'
      numEdits: 0
      reactions: []
    id: 63f8c1ba19cec76f5cd6ccff
    type: comment
  author: Marek-Labuzek-at-Capg
  content: "I observe modelling leaking memory, both in the setup from the article\
    \ - loading blocks one by one, and same when running on a machine with big enough\
    \ (CPU) memory to load whole model.\nI've added used mem dumps when each block\
    \ is evaluated and it constantly goes up over time - on self_attention.forwad\
    \ and mlp. \nThe effect is that generating bigger number of blocks needs bigger\
    \ memory..\nAnybody observing the same? Any fixes / ideas?"
  created_at: 2023-02-24 13:55:06+00:00
  edited: false
  hidden: false
  id: 63f8c1ba19cec76f5cd6ccff
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 87
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Can Bloom-176B really be evaluated on normal hardware at a rate of 3 minutes
  per token?
