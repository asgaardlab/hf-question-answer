!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gsmoon97
conflicting_files: null
created_at: 2023-02-15 09:28:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6f7c51292f5621386eb5b196b82cf7a6.svg
      fullname: Moon Geonsik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsmoon97
      type: user
    createdAt: '2023-02-15T09:28:55.000Z'
    data:
      edited: true
      editors:
      - gsmoon97
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6f7c51292f5621386eb5b196b82cf7a6.svg
          fullname: Moon Geonsik
          isHf: false
          isPro: false
          name: gsmoon97
          type: user
        html: '<p>I am planning to run inference on the 176B model, but I could not
          find much information, regarding the minimum requirements.<br>If anyone
          has experience with this, could you please provide some insights on what
          is the minimum set-up required to run inference on the 176B model?</p>

          <p>Below are my specifications.</p>

          <ul>

          <li>4 x A100 40GB GPUs</li>

          <li>Can allocate up to 10TB of free disk space</li>

          </ul>

          <p>Thank you.</p>

          '
        raw: 'I am planning to run inference on the 176B model, but I could not find
          much information, regarding the minimum requirements.

          If anyone has experience with this, could you please provide some insights
          on what is the minimum set-up required to run inference on the 176B model?


          Below are my specifications.

          - 4 x A100 40GB GPUs

          - Can allocate up to 10TB of free disk space


          Thank you.'
        updatedAt: '2023-02-16T00:34:30.950Z'
      numEdits: 1
      reactions: []
    id: 63eca5d754bba67815164b1a
    type: comment
  author: gsmoon97
  content: 'I am planning to run inference on the 176B model, but I could not find
    much information, regarding the minimum requirements.

    If anyone has experience with this, could you please provide some insights on
    what is the minimum set-up required to run inference on the 176B model?


    Below are my specifications.

    - 4 x A100 40GB GPUs

    - Can allocate up to 10TB of free disk space


    Thank you.'
  created_at: 2023-02-15 09:28:55+00:00
  edited: true
  hidden: false
  id: 63eca5d754bba67815164b1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0dd4440f78520022bc12e10fd5249429.svg
      fullname: Jani Niemitalo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Linore
      type: user
    createdAt: '2023-02-15T23:09:23.000Z'
    data:
      edited: false
      editors:
      - Linore
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0dd4440f78520022bc12e10fd5249429.svg
          fullname: Jani Niemitalo
          isHf: false
          isPro: false
          name: Linore
          type: user
        html: '<p>If you even looked at the files section, you would see that even
          the smallest possible size of the 176B model is over 300GB of disk space.</p>

          '
        raw: If you even looked at the files section, you would see that even the
          smallest possible size of the 176B model is over 300GB of disk space.
        updatedAt: '2023-02-15T23:09:23.429Z'
      numEdits: 0
      reactions: []
    id: 63ed6623f3827af9bb5226d9
    type: comment
  author: Linore
  content: If you even looked at the files section, you would see that even the smallest
    possible size of the 176B model is over 300GB of disk space.
  created_at: 2023-02-15 23:09:23+00:00
  edited: false
  hidden: false
  id: 63ed6623f3827af9bb5226d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6f7c51292f5621386eb5b196b82cf7a6.svg
      fullname: Moon Geonsik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsmoon97
      type: user
    createdAt: '2023-02-16T00:34:03.000Z'
    data:
      edited: false
      editors:
      - gsmoon97
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6f7c51292f5621386eb5b196b82cf7a6.svg
          fullname: Moon Geonsik
          isHf: false
          isPro: false
          name: gsmoon97
          type: user
        html: '<blockquote>

          <p>If you even looked at the files section, you would see that even the
          smallest possible size of the 176B model is over 300GB of disk space.</p>

          </blockquote>

          <p>Thanks for the input! I should have mentioned that I can allocate up
          to 10TB more space. Changed the original post accordingly.</p>

          '
        raw: '> If you even looked at the files section, you would see that even the
          smallest possible size of the 176B model is over 300GB of disk space.


          Thanks for the input! I should have mentioned that I can allocate up to
          10TB more space. Changed the original post accordingly.'
        updatedAt: '2023-02-16T00:34:03.998Z'
      numEdits: 0
      reactions: []
    id: 63ed79fb39a391155460054b
    type: comment
  author: gsmoon97
  content: '> If you even looked at the files section, you would see that even the
    smallest possible size of the 176B model is over 300GB of disk space.


    Thanks for the input! I should have mentioned that I can allocate up to 10TB more
    space. Changed the original post accordingly.'
  created_at: 2023-02-16 00:34:03+00:00
  edited: false
  hidden: false
  id: 63ed79fb39a391155460054b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd71cbf8edcfaa4c88ca334dfb33ea2d.svg
      fullname: Zach
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mrdrifter
      type: user
    createdAt: '2023-02-16T02:01:38.000Z'
    data:
      edited: false
      editors:
      - Mrdrifter
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd71cbf8edcfaa4c88ca334dfb33ea2d.svg
          fullname: Zach
          isHf: false
          isPro: false
          name: Mrdrifter
          type: user
        html: '<p>What is the largest possible size the model could be?</p>

          '
        raw: What is the largest possible size the model could be?
        updatedAt: '2023-02-16T02:01:38.443Z'
      numEdits: 0
      reactions: []
    id: 63ed8e8239a39115546129b4
    type: comment
  author: Mrdrifter
  content: What is the largest possible size the model could be?
  created_at: 2023-02-16 02:01:38+00:00
  edited: false
  hidden: false
  id: 63ed8e8239a39115546129b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1a0c5e59bfef39e4c42b29123adcfa07.svg
      fullname: Tin Tran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nightfuryx
      type: user
    createdAt: '2023-02-16T07:18:04.000Z'
    data:
      edited: false
      editors:
      - nightfuryx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1a0c5e59bfef39e4c42b29123adcfa07.svg
          fullname: Tin Tran
          isHf: false
          isPro: false
          name: nightfuryx
          type: user
        html: '<p>You need to be able to fully load the model into the GPUs, the disk
          space is not relevant.</p>

          <p>So 400GB of GPU Vram....at minimum</p>

          '
        raw: 'You need to be able to fully load the model into the GPUs, the disk
          space is not relevant.


          So 400GB of GPU Vram....at minimum'
        updatedAt: '2023-02-16T07:18:04.999Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - gsmoon97
        - TingchenFu
    id: 63edd8ac8d11d147d125c271
    type: comment
  author: nightfuryx
  content: 'You need to be able to fully load the model into the GPUs, the disk space
    is not relevant.


    So 400GB of GPU Vram....at minimum'
  created_at: 2023-02-16 07:18:04+00:00
  edited: false
  hidden: false
  id: 63edd8ac8d11d147d125c271
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-02-16T18:29:21.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>If you load your model in 8bit you can half the GPU memory requirement
          (200GB needed instead of 400) . Install <code>bitsandbytes</code> and just
          add <code>load_in_8bit=True</code> when calling <code>from_pretrained</code></p>

          '
        raw: If you load your model in 8bit you can half the GPU memory requirement
          (200GB needed instead of 400) . Install `bitsandbytes` and just add `load_in_8bit=True`
          when calling `from_pretrained`
        updatedAt: '2023-02-16T18:29:45.630Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - gsmoon97
        - Magnator
        - TingchenFu
    id: 63ee7601049599a8df92e0a2
    type: comment
  author: ybelkada
  content: If you load your model in 8bit you can half the GPU memory requirement
    (200GB needed instead of 400) . Install `bitsandbytes` and just add `load_in_8bit=True`
    when calling `from_pretrained`
  created_at: 2023-02-16 18:29:21+00:00
  edited: true
  hidden: false
  id: 63ee7601049599a8df92e0a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6f7c51292f5621386eb5b196b82cf7a6.svg
      fullname: Moon Geonsik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsmoon97
      type: user
    createdAt: '2023-02-17T09:13:35.000Z'
    data:
      edited: false
      editors:
      - gsmoon97
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6f7c51292f5621386eb5b196b82cf7a6.svg
          fullname: Moon Geonsik
          isHf: false
          isPro: false
          name: gsmoon97
          type: user
        html: '<blockquote>

          <p>If you load your model in 8bit you can half the GPU memory requirement
          (200GB needed instead of 400) . Install <code>bitsandbytes</code> and just
          add <code>load_in_8bit=True</code> when calling <code>from_pretrained</code></p>

          </blockquote>

          <p>Thank you for the suggestion! May I also ask if there would be any significant
          effects on the model performance, if I load the model in 8bit?</p>

          '
        raw: '> If you load your model in 8bit you can half the GPU memory requirement
          (200GB needed instead of 400) . Install `bitsandbytes` and just add `load_in_8bit=True`
          when calling `from_pretrained`


          Thank you for the suggestion! May I also ask if there would be any significant
          effects on the model performance, if I load the model in 8bit?'
        updatedAt: '2023-02-17T09:13:35.432Z'
      numEdits: 0
      reactions: []
    id: 63ef453fa2f69d2950319184
    type: comment
  author: gsmoon97
  content: '> If you load your model in 8bit you can half the GPU memory requirement
    (200GB needed instead of 400) . Install `bitsandbytes` and just add `load_in_8bit=True`
    when calling `from_pretrained`


    Thank you for the suggestion! May I also ask if there would be any significant
    effects on the model performance, if I load the model in 8bit?'
  created_at: 2023-02-17 09:13:35+00:00
  edited: false
  hidden: false
  id: 63ef453fa2f69d2950319184
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-02-17T11:56:46.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>You should not observe any performance degradation, check out the
          paper: <a rel="nofollow" href="https://arxiv.org/abs/2208.07339">https://arxiv.org/abs/2208.07339</a>
          or the blogpost about the integration: <a href="https://huggingface.co/blog/hf-bitsandbytes-integration">https://huggingface.co/blog/hf-bitsandbytes-integration</a></p>

          '
        raw: 'You should not observe any performance degradation, check out the paper:
          https://arxiv.org/abs/2208.07339 or the blogpost about the integration:
          https://huggingface.co/blog/hf-bitsandbytes-integration'
        updatedAt: '2023-02-17T11:56:46.582Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - gsmoon97
        - Maxi
        - JanekFalke
        - TingchenFu
    id: 63ef6b7e8da865b7fcde476c
    type: comment
  author: ybelkada
  content: 'You should not observe any performance degradation, check out the paper:
    https://arxiv.org/abs/2208.07339 or the blogpost about the integration: https://huggingface.co/blog/hf-bitsandbytes-integration'
  created_at: 2023-02-17 11:56:46+00:00
  edited: false
  hidden: false
  id: 63ef6b7e8da865b7fcde476c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 195
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Minimum requirements for running inference on 176B model
