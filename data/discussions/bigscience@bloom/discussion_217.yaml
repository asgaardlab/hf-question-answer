!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Imran1
conflicting_files: null
created_at: 2023-03-23 10:22:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/oTEr0Ns7Kmez7CzvcQEtL.jpeg?w=200&h=200&f=face
      fullname: Imran ullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Imran1
      type: user
    createdAt: '2023-03-23T11:22:30.000Z'
    data:
      edited: false
      editors:
      - Imran1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/oTEr0Ns7Kmez7CzvcQEtL.jpeg?w=200&h=200&f=face
          fullname: Imran ullah
          isHf: false
          isPro: false
          name: Imran1
          type: user
        html: "<p>Here is code </p>\n<p>batch = tokenizer(\" \u0685\u0648\u06A9 \u062F\
          \ \u0632\u0693\u0647 \", return_tensors='pt')</p>\n<p>max_length = 200<br>temperature\
          \ = 0.5<br>top_k = 10<br>top_p = 0.95<br>do_sample = True<br>with torch.cuda.amp.autocast():<br>\
          \    # Pass the additional parameters to the model.generate() function<br>\
          \    output_tokens = model.generate(input_ids=batch[\"input_ids\"],<br>\
          \                                   attention_mask=batch['attention_mask'],<br>\
          \                                   max_length=max_length,<br>         \
          \                          temperature=temperature,<br>                \
          \                   top_k=top_k,<br>                                   top_p=top_p,repetition_penalty=1.03,<br>\
          \                                   #penalty_alpha=0.6,<br>            \
          \                       #do_sample=do_sample<br>                       \
          \            )</p>\n<pre><code>print(\"\\n\\n\", tokenizer.decode(output_tokens[0],\
          \ skip_special_tokens=True))\n</code></pre>\n<p><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/ppKTlu2sjmfV1XIKu3mSf.png\"\
          ><img alt=\"Screenshot_20230323-151456.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/ppKTlu2sjmfV1XIKu3mSf.png\"\
          ></a></p>\n<p>The out put is repeating. </p>\n"
        raw: "Here is code \r\n\r\nbatch = tokenizer(\" \u0685\u0648\u06A9 \u062F\
          \ \u0632\u0693\u0647 \", return_tensors='pt')\r\n\r\nmax_length = 200\r\n\
          temperature = 0.5\r\ntop_k = 10\r\ntop_p = 0.95\r\ndo_sample = True\r\n\
          with torch.cuda.amp.autocast():\r\n    # Pass the additional parameters\
          \ to the model.generate() function\r\n    output_tokens = model.generate(input_ids=batch[\"\
          input_ids\"],\r\n                                   attention_mask=batch['attention_mask'],\r\
          \n                                   max_length=max_length,\r\n        \
          \                           temperature=temperature,\r\n               \
          \                    top_k=top_k,\r\n                                  \
          \ top_p=top_p,repetition_penalty=1.03,\r\n                             \
          \      #penalty_alpha=0.6,\r\n                                   #do_sample=do_sample\r\
          \n                                   )\r\n\r\n    print(\"\\n\\n\", tokenizer.decode(output_tokens[0],\
          \ skip_special_tokens=True))\r\n\r\n\r\n\r\n![Screenshot_20230323-151456.png](https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/ppKTlu2sjmfV1XIKu3mSf.png)\r\
          \n\r\nThe out put is repeating. \r\n"
        updatedAt: '2023-03-23T11:22:30.896Z'
      numEdits: 0
      reactions: []
    id: 641c367645487810d13256aa
    type: comment
  author: Imran1
  content: "Here is code \r\n\r\nbatch = tokenizer(\" \u0685\u0648\u06A9 \u062F \u0632\
    \u0693\u0647 \", return_tensors='pt')\r\n\r\nmax_length = 200\r\ntemperature =\
    \ 0.5\r\ntop_k = 10\r\ntop_p = 0.95\r\ndo_sample = True\r\nwith torch.cuda.amp.autocast():\r\
    \n    # Pass the additional parameters to the model.generate() function\r\n  \
    \  output_tokens = model.generate(input_ids=batch[\"input_ids\"],\r\n        \
    \                           attention_mask=batch['attention_mask'],\r\n      \
    \                             max_length=max_length,\r\n                     \
    \              temperature=temperature,\r\n                                  \
    \ top_k=top_k,\r\n                                   top_p=top_p,repetition_penalty=1.03,\r\
    \n                                   #penalty_alpha=0.6,\r\n                 \
    \                  #do_sample=do_sample\r\n                                  \
    \ )\r\n\r\n    print(\"\\n\\n\", tokenizer.decode(output_tokens[0], skip_special_tokens=True))\r\
    \n\r\n\r\n\r\n![Screenshot_20230323-151456.png](https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/ppKTlu2sjmfV1XIKu3mSf.png)\r\
    \n\r\nThe out put is repeating. \r\n"
  created_at: 2023-03-23 10:22:30+00:00
  edited: false
  hidden: false
  id: 641c367645487810d13256aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2023-03-23T12:03:18.000Z'
    data:
      edited: false
      editors:
      - cakiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
          fullname: Christopher Akiki
          isHf: false
          isPro: false
          name: cakiki
          type: user
        html: '<p>I think <code>top_k</code> is way too small. What happens when you
          use a bigger value?</p>

          '
        raw: I think `top_k` is way too small. What happens when you use a bigger
          value?
        updatedAt: '2023-03-23T12:03:18.479Z'
      numEdits: 0
      reactions: []
    id: 641c4006d1e1671e3f178ed9
    type: comment
  author: cakiki
  content: I think `top_k` is way too small. What happens when you use a bigger value?
  created_at: 2023-03-23 11:03:18+00:00
  edited: false
  hidden: false
  id: 641c4006d1e1671e3f178ed9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/oTEr0Ns7Kmez7CzvcQEtL.jpeg?w=200&h=200&f=face
      fullname: Imran ullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Imran1
      type: user
    createdAt: '2023-03-23T12:24:50.000Z'
    data:
      edited: false
      editors:
      - Imran1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/oTEr0Ns7Kmez7CzvcQEtL.jpeg?w=200&h=200&f=face
          fullname: Imran ullah
          isHf: false
          isPro: false
          name: Imran1
          type: user
        html: '<p>I play with temperature= 0.5 to 1.0<br>Also top_k = 4 to 50, and<br>Top_p=
          0.3 to 0.95<br>But again generate same text</p>

          '
        raw: "I play with temperature= 0.5 to 1.0\nAlso top_k = 4 to 50, and \nTop_p=\
          \ 0.3 to 0.95 \nBut again generate same text"
        updatedAt: '2023-03-23T12:24:50.169Z'
      numEdits: 0
      reactions: []
    id: 641c45120d7b2cc2db0919b4
    type: comment
  author: Imran1
  content: "I play with temperature= 0.5 to 1.0\nAlso top_k = 4 to 50, and \nTop_p=\
    \ 0.3 to 0.95 \nBut again generate same text"
  created_at: 2023-03-23 11:24:50+00:00
  edited: false
  hidden: false
  id: 641c45120d7b2cc2db0919b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/oTEr0Ns7Kmez7CzvcQEtL.jpeg?w=200&h=200&f=face
      fullname: Imran ullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Imran1
      type: user
    createdAt: '2023-03-23T12:26:06.000Z'
    data:
      edited: false
      editors:
      - Imran1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/oTEr0Ns7Kmez7CzvcQEtL.jpeg?w=200&h=200&f=face
          fullname: Imran ullah
          isHf: false
          isPro: false
          name: Imran1
          type: user
        html: '<p>It''s give me the following error when I pass do_sample or penalty_alpha
          parameters. </p>

          <p>RuntimeError: "topk_cpu" not implemented for ''Half''</p>

          '
        raw: "It's give me the following error when I pass do_sample or penalty_alpha\
          \ parameters. \n\n\nRuntimeError: \"topk_cpu\" not implemented for 'Half'"
        updatedAt: '2023-03-23T12:26:06.925Z'
      numEdits: 0
      reactions: []
    id: 641c455e0d7b2cc2db091ca2
    type: comment
  author: Imran1
  content: "It's give me the following error when I pass do_sample or penalty_alpha\
    \ parameters. \n\n\nRuntimeError: \"topk_cpu\" not implemented for 'Half'"
  created_at: 2023-03-23 11:26:06+00:00
  edited: false
  hidden: false
  id: 641c455e0d7b2cc2db091ca2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2023-03-24T08:43:27.000Z'
    data:
      edited: false
      editors:
      - cakiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
          fullname: Christopher Akiki
          isHf: false
          isPro: false
          name: cakiki
          type: user
        html: "<p>Ah, I just noticed the details in your title. <code>top_k</code>\
          \ and <code>top_p</code> and <code>temperature</code> are not used when\
          \ <code>do_sample</code> is False, so you're just generating deterministically\
          \ even when setting values. I don't know how to make it work with PEFT.\
          \ Maybe <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> can help?</p>\n"
        raw: Ah, I just noticed the details in your title. `top_k` and `top_p` and
          `temperature` are not used when `do_sample` is False, so you're just generating
          deterministically even when setting values. I don't know how to make it
          work with PEFT. Maybe @ybelkada can help?
        updatedAt: '2023-03-24T08:43:27.527Z'
      numEdits: 0
      reactions: []
    id: 641d62af02628769d2e47718
    type: comment
  author: cakiki
  content: Ah, I just noticed the details in your title. `top_k` and `top_p` and `temperature`
    are not used when `do_sample` is False, so you're just generating deterministically
    even when setting values. I don't know how to make it work with PEFT. Maybe @ybelkada
    can help?
  created_at: 2023-03-24 07:43:27+00:00
  edited: false
  hidden: false
  id: 641d62af02628769d2e47718
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/oTEr0Ns7Kmez7CzvcQEtL.jpeg?w=200&h=200&f=face
      fullname: Imran ullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Imran1
      type: user
    createdAt: '2023-03-24T09:18:00.000Z'
    data:
      edited: false
      editors:
      - Imran1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/oTEr0Ns7Kmez7CzvcQEtL.jpeg?w=200&h=200&f=face
          fullname: Imran ullah
          isHf: false
          isPro: false
          name: Imran1
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cakiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cakiki\">@<span class=\"\
          underline\">cakiki</span></a></span>\n\n\t</span></span>    You mean use\
          \ top_p and do_sample not use temperature !</p>\n"
        raw: '@cakiki    You mean use top_p and do_sample not use temperature !'
        updatedAt: '2023-03-24T09:18:00.489Z'
      numEdits: 0
      reactions: []
    id: 641d6ac8cbf117cdf93d016b
    type: comment
  author: Imran1
  content: '@cakiki    You mean use top_p and do_sample not use temperature !'
  created_at: 2023-03-24 08:18:00+00:00
  edited: false
  hidden: false
  id: 641d6ac8cbf117cdf93d016b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 217
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: 'The bloom7b model not support contrastive search nor do_sample with peft and
  just repeating the output '
