!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mishavee
conflicting_files: null
created_at: 2022-10-22 17:24:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-10-22T18:24:13.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>Suggest a cloud gpu service to fine tune Bloom. </p>

          '
        raw: 'Suggest a cloud gpu service to fine tune Bloom. '
        updatedAt: '2022-10-22T18:24:13.375Z'
      numEdits: 0
      reactions: []
    id: 6354354d6fd9f07930258bbf
    type: comment
  author: mishavee
  content: 'Suggest a cloud gpu service to fine tune Bloom. '
  created_at: 2022-10-22 17:24:13+00:00
  edited: false
  hidden: false
  id: 6354354d6fd9f07930258bbf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-10-23T09:24:33.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Could you explain more clearly what your issue is? Typically what
          is the problem you''re facing. I mean whatever provider with enough A100
          would work. In our case we use 384 80G A100s to train it, with 48 GPUs per
          replicas. Depending on how you want to finetune, you might need less than
          48 A100s.</p>

          <p>And also you can batch your questions into a single discussion.</p>

          '
        raw: 'Could you explain more clearly what your issue is? Typically what is
          the problem you''re facing. I mean whatever provider with enough A100 would
          work. In our case we use 384 80G A100s to train it, with 48 GPUs per replicas.
          Depending on how you want to finetune, you might need less than 48 A100s.


          And also you can batch your questions into a single discussion.'
        updatedAt: '2022-10-23T09:24:33.693Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Brenow
    id: 63550851a5546ae4c5c62a56
    type: comment
  author: TimeRobber
  content: 'Could you explain more clearly what your issue is? Typically what is the
    problem you''re facing. I mean whatever provider with enough A100 would work.
    In our case we use 384 80G A100s to train it, with 48 GPUs per replicas. Depending
    on how you want to finetune, you might need less than 48 A100s.


    And also you can batch your questions into a single discussion.'
  created_at: 2022-10-23 08:24:33+00:00
  edited: false
  hidden: false
  id: 63550851a5546ae4c5c62a56
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-10-23T22:21:13.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>well I would like to fine tune Bloom for two things. One is paraphrasing
          and two summarization. However the costs of a 100 rental is a lot because
          I assume it would take at least a month to fine tune for each purpose. What
          is the cheapest approach? How long does one epoch take on a reasonable amount
          of gpus. My calculations are something like at least 25k per month per purpose
          from lambda, the cheapest cloud gpu service.</p>

          '
        raw: well I would like to fine tune Bloom for two things. One is paraphrasing
          and two summarization. However the costs of a 100 rental is a lot because
          I assume it would take at least a month to fine tune for each purpose. What
          is the cheapest approach? How long does one epoch take on a reasonable amount
          of gpus. My calculations are something like at least 25k per month per purpose
          from lambda, the cheapest cloud gpu service.
        updatedAt: '2022-10-23T22:21:13.394Z'
      numEdits: 0
      reactions: []
    id: 6355be59b8b79340d4635109
    type: comment
  author: mishavee
  content: well I would like to fine tune Bloom for two things. One is paraphrasing
    and two summarization. However the costs of a 100 rental is a lot because I assume
    it would take at least a month to fine tune for each purpose. What is the cheapest
    approach? How long does one epoch take on a reasonable amount of gpus. My calculations
    are something like at least 25k per month per purpose from lambda, the cheapest
    cloud gpu service.
  created_at: 2022-10-23 21:21:13+00:00
  edited: false
  hidden: false
  id: 6355be59b8b79340d4635109
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-10-23T22:43:33.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Not sure what the cheapest approach is. I do think that you first
          need to figure out how to optimize your software before worrying about the
          hardware. Typically we use Megatron-DeepSpeed to train as it allowed to
          get the fastest training we could get at the time.</p>

          <blockquote>

          <p>How long does one epoch take on a reasonable amount of gpus.</p>

          </blockquote>

          <p>That question isn''t very well formulated, as epoch is usually dataset
          dependent. During pretraining we only do one epoch, but it last 3 months
          on 382 GPUs. I''m guessing your fine tuning setup is probably going to be
          shorter.</p>

          <p>I would usually advise against fine tuning such big models as you could
          just use prompting in order to solve your task. The only fine tuning I would
          really consider is T0-style, where we fine tune on a bunch of tasks.</p>

          '
        raw: 'Not sure what the cheapest approach is. I do think that you first need
          to figure out how to optimize your software before worrying about the hardware.
          Typically we use Megatron-DeepSpeed to train as it allowed to get the fastest
          training we could get at the time.


          > How long does one epoch take on a reasonable amount of gpus.


          That question isn''t very well formulated, as epoch is usually dataset dependent.
          During pretraining we only do one epoch, but it last 3 months on 382 GPUs.
          I''m guessing your fine tuning setup is probably going to be shorter.


          I would usually advise against fine tuning such big models as you could
          just use prompting in order to solve your task. The only fine tuning I would
          really consider is T0-style, where we fine tune on a bunch of tasks.'
        updatedAt: '2022-10-23T22:43:33.207Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Brenow
    id: 6355c39560c1b72f6268c829
    type: comment
  author: TimeRobber
  content: 'Not sure what the cheapest approach is. I do think that you first need
    to figure out how to optimize your software before worrying about the hardware.
    Typically we use Megatron-DeepSpeed to train as it allowed to get the fastest
    training we could get at the time.


    > How long does one epoch take on a reasonable amount of gpus.


    That question isn''t very well formulated, as epoch is usually dataset dependent.
    During pretraining we only do one epoch, but it last 3 months on 382 GPUs. I''m
    guessing your fine tuning setup is probably going to be shorter.


    I would usually advise against fine tuning such big models as you could just use
    prompting in order to solve your task. The only fine tuning I would really consider
    is T0-style, where we fine tune on a bunch of tasks.'
  created_at: 2022-10-23 21:43:33+00:00
  edited: false
  hidden: false
  id: 6355c39560c1b72f6268c829
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
      fullname: Mike Vinitsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mishavee
      type: user
    createdAt: '2022-10-24T01:03:41.000Z'
    data:
      edited: false
      editors:
      - mishavee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/474bab3ba399fa7861ae098f6e4b3901.svg
          fullname: Mike Vinitsky
          isHf: false
          isPro: false
          name: mishavee
          type: user
        html: '<p>ty</p>

          <p>what is T0-style? </p>

          <p>Is prompting just giving it one example and then starting the sequence
          for the next and have Bloom complete it? </p>

          <p>yes you are right it is badly formulated. If the bloom original training
          set was 1.6tb and all my training data is 1gb. Will it be 1600 times faster
          or require 1600 times less gpu power or some combination of both? </p>

          <p>what is pretraining,  does it mean before validation? </p>

          <p>how many words is 2048 tokens in Bloom approximately?</p>

          '
        raw: "ty\n\nwhat is T0-style? \n\nIs prompting just giving it one example\
          \ and then starting the sequence for the next and have Bloom complete it?\
          \ \n\nyes you are right it is badly formulated. If the bloom original training\
          \ set was 1.6tb and all my training data is 1gb. Will it be 1600 times faster\
          \ or require 1600 times less gpu power or some combination of both? \n\n\
          what is pretraining,  does it mean before validation? \n\nhow many words\
          \ is 2048 tokens in Bloom approximately?"
        updatedAt: '2022-10-24T01:03:41.658Z'
      numEdits: 0
      reactions: []
    id: 6355e46d525beaee688fc7dc
    type: comment
  author: mishavee
  content: "ty\n\nwhat is T0-style? \n\nIs prompting just giving it one example and\
    \ then starting the sequence for the next and have Bloom complete it? \n\nyes\
    \ you are right it is badly formulated. If the bloom original training set was\
    \ 1.6tb and all my training data is 1gb. Will it be 1600 times faster or require\
    \ 1600 times less gpu power or some combination of both? \n\nwhat is pretraining,\
    \  does it mean before validation? \n\nhow many words is 2048 tokens in Bloom\
    \ approximately?"
  created_at: 2022-10-24 00:03:41+00:00
  edited: false
  hidden: false
  id: 6355e46d525beaee688fc7dc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 128
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: 'Suggest a cloud gpu service to fine tune Bloom. '
