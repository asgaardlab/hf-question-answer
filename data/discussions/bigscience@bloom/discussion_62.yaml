!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xuyifan
conflicting_files: null
created_at: 2022-07-23 15:44:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed10f5a1d6e977700cd4022b6f96e3ff.svg
      fullname: Xu Yifan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xuyifan
      type: user
    createdAt: '2022-07-23T16:44:58.000Z'
    data:
      edited: false
      editors:
      - xuyifan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed10f5a1d6e977700cd4022b6f96e3ff.svg
          fullname: Xu Yifan
          isHf: false
          isPro: false
          name: xuyifan
          type: user
        html: '<p>I have a machine with 8*A100, and i would like to use bloom-176B
          to generate text, and apply evaluation on different datasets. I use the
          code in bigscience-workshop/bigscience/sortval repo,bigscience/evaluation/generation/generate.py.
          But this code only allows the model to run on a graphics card.<br>Apperently
          single A100 is not enough:<br>RuntimeError: CUDA out of memory. Tried to
          allocate 1.53 GiB (GPU 0; 79.17 GiB total capacity; 77.14 GiB already allocated;
          1.20 GiB free; 77.14 GiB reserved in total by PyTorch) If reserved memory
          is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>How to use bloom-176B to generate or evaluate on Multi-graphics? Should
          I change the code in generate.py,or use other code?</p>

          '
        raw: "I have a machine with 8*A100, and i would like to use bloom-176B to\
          \ generate text, and apply evaluation on different datasets. I use the code\
          \ in bigscience-workshop/bigscience/sortval repo,bigscience/evaluation/generation/generate.py.\
          \ But this code only allows the model to run on a graphics card.\r\nApperently\
          \ single A100 is not enough:\r\nRuntimeError: CUDA out of memory. Tried\
          \ to allocate 1.53 GiB (GPU 0; 79.17 GiB total capacity; 77.14 GiB already\
          \ allocated; 1.20 GiB free; 77.14 GiB reserved in total by PyTorch) If reserved\
          \ memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
          \n\r\nHow to use bloom-176B to generate or evaluate on Multi-graphics? Should\
          \ I change the code in generate.py,or use other code?\r\n"
        updatedAt: '2022-07-23T16:44:58.726Z'
      numEdits: 0
      reactions: []
    id: 62dc258a2a419e78f6cef4bc
    type: comment
  author: xuyifan
  content: "I have a machine with 8*A100, and i would like to use bloom-176B to generate\
    \ text, and apply evaluation on different datasets. I use the code in bigscience-workshop/bigscience/sortval\
    \ repo,bigscience/evaluation/generation/generate.py. But this code only allows\
    \ the model to run on a graphics card.\r\nApperently single A100 is not enough:\r\
    \nRuntimeError: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 79.17 GiB\
    \ total capacity; 77.14 GiB already allocated; 1.20 GiB free; 77.14 GiB reserved\
    \ in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
    \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
    \n\r\nHow to use bloom-176B to generate or evaluate on Multi-graphics? Should\
    \ I change the code in generate.py,or use other code?\r\n"
  created_at: 2022-07-23 15:44:58+00:00
  edited: false
  hidden: false
  id: 62dc258a2a419e78f6cef4bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed10f5a1d6e977700cd4022b6f96e3ff.svg
      fullname: Xu Yifan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xuyifan
      type: user
    createdAt: '2022-07-23T18:20:58.000Z'
    data:
      edited: false
      editors:
      - xuyifan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed10f5a1d6e977700cd4022b6f96e3ff.svg
          fullname: Xu Yifan
          isHf: false
          isPro: false
          name: xuyifan
          type: user
        html: "<p>I have tried use accelerate:</p>\n<pre><code>from transformers import\
          \ AutoTokenizer, AutoModel\nfrom transformers import AutoModelForCausalLM\n\
          import torch\nfrom accelerate import Accelerator\ndef generate_from_text(model,\
          \ text, tokenizer, max_length=200, greedy=False, top_k=0):\n    input_ids\
          \ = tokenizer.encode(text, return_tensors='pt').to(\"cuda:0\")\n    max_length\
          \ = input_ids.size(-1) + max_length\n    \n    greedy_output = model.generate(\n\
          \        input_ids.to('cuda:0'),\n        max_length=max_length,\n     \
          \   do_sample=not greedy,\n        top_k=None if greedy else top_k,\n  \
          \  )\n    return tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n\
          \ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\")\nprint(\"\
          load tokenizer\")\naccelerator = Accelerator()\ndevice = accelerator.device\n\
          model = AutoModelForCausalLM.from_pretrained(\n    arg.checkpoints,\n  \
          \  device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    offload_folder=\"\
          ./offload\",\n)\nmodel = accelerator.prepare(model)\nprint(\"load model\"\
          )\n\n\ntext = 'hello'\noutput = generate_from_text(model, text, tokenizer,\
          \ max_length=500, greedy=True, top_k=1)\nprint(output)\n</code></pre>\n\
          <p>and it shows :</p>\n<p>load tokenizer<br>Traceback (most recent call\
          \ last):<br>  File \"/mnt/yrfs/xuyifan/bloom/bigscience-sorteval/eva.py\"\
          , line 28, in <br>    model = accelerator.prepare(model)<br>  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/accelerate/accelerator.py\"\
          , line 545, in prepare<br>    result = tuple(self._prepare_one(obj, first_pass=True)\
          \ for obj in args)<br>  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/accelerate/accelerator.py\"\
          , line 545, in <br>    result = tuple(self._prepare_one(obj, first_pass=True)\
          \ for obj in args)<br>  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/accelerate/accelerator.py\"\
          , line 441, in _prepare_one<br>    return self.prepare_model(obj)<br>  File\
          \ \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/accelerate/accelerator.py\"\
          , line 565, in prepare_model<br>    model = model.to(self.device)<br>  File\
          \ \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 927, in to<br>    return self._apply(convert)<br>  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 579, in _apply<br>    module._apply(fn)<br>  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 579, in _apply<br>    module._apply(fn)<br>  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 579, in _apply<br>    module._apply(fn)<br>  [Previous line repeated\
          \ 2 more times]<br>  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 602, in _apply<br>    param_applied = fn(param)<br>  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 925, in convert<br>    return t.to(device, dtype if t.is_floating_point()\
          \ or t.is_complex() else None, non_blocking)<br>RuntimeError: CUDA out of\
          \ memory. Tried to allocate 1.53 GiB (GPU 0; 79.17 GiB total capacity; 77.14\
          \ GiB already allocated; 1.20 GiB free; 77.14 GiB reserved in total by PyTorch)\
          \ If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>\n"
        raw: "I have tried use accelerate:\n\n```\nfrom transformers import AutoTokenizer,\
          \ AutoModel\nfrom transformers import AutoModelForCausalLM\nimport torch\n\
          from accelerate import Accelerator\ndef generate_from_text(model, text,\
          \ tokenizer, max_length=200, greedy=False, top_k=0):\n    input_ids = tokenizer.encode(text,\
          \ return_tensors='pt').to(\"cuda:0\")\n    max_length = input_ids.size(-1)\
          \ + max_length\n    \n    greedy_output = model.generate(\n        input_ids.to('cuda:0'),\n\
          \        max_length=max_length,\n        do_sample=not greedy,\n       \
          \ top_k=None if greedy else top_k,\n    )\n    return tokenizer.decode(greedy_output[0],\
          \ skip_special_tokens=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          bigscience/bloom\")\nprint(\"load tokenizer\")\naccelerator = Accelerator()\n\
          device = accelerator.device\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    arg.checkpoints,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n\
          \    offload_folder=\"./offload\",\n)\nmodel = accelerator.prepare(model)\n\
          print(\"load model\")\n\n\ntext = 'hello'\noutput = generate_from_text(model,\
          \ text, tokenizer, max_length=500, greedy=True, top_k=1)\nprint(output)\n\
          ```\n\nand it shows :\n\nload tokenizer\nTraceback (most recent call last):\n\
          \  File \"/mnt/yrfs/xuyifan/bloom/bigscience-sorteval/eva.py\", line 28,\
          \ in <module>\n    model = accelerator.prepare(model)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/accelerate/accelerator.py\"\
          , line 545, in prepare\n    result = tuple(self._prepare_one(obj, first_pass=True)\
          \ for obj in args)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/accelerate/accelerator.py\"\
          , line 545, in <genexpr>\n    result = tuple(self._prepare_one(obj, first_pass=True)\
          \ for obj in args)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/accelerate/accelerator.py\"\
          , line 441, in _prepare_one\n    return self.prepare_model(obj)\n  File\
          \ \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/accelerate/accelerator.py\"\
          , line 565, in prepare_model\n    model = model.to(self.device)\n  File\
          \ \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 927, in to\n    return self._apply(convert)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 579, in _apply\n    module._apply(fn)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 579, in _apply\n    module._apply(fn)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 579, in _apply\n    module._apply(fn)\n  [Previous line repeated\
          \ 2 more times]\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 602, in _apply\n    param_applied = fn(param)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 925, in convert\n    return t.to(device, dtype if t.is_floating_point()\
          \ or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA out of\
          \ memory. Tried to allocate 1.53 GiB (GPU 0; 79.17 GiB total capacity; 77.14\
          \ GiB already allocated; 1.20 GiB free; 77.14 GiB reserved in total by PyTorch)\
          \ If reserved memory is >> allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
        updatedAt: '2022-07-23T18:20:58.054Z'
      numEdits: 0
      reactions: []
    id: 62dc3c0a4e5b927494d5a336
    type: comment
  author: xuyifan
  content: "I have tried use accelerate:\n\n```\nfrom transformers import AutoTokenizer,\
    \ AutoModel\nfrom transformers import AutoModelForCausalLM\nimport torch\nfrom\
    \ accelerate import Accelerator\ndef generate_from_text(model, text, tokenizer,\
    \ max_length=200, greedy=False, top_k=0):\n    input_ids = tokenizer.encode(text,\
    \ return_tensors='pt').to(\"cuda:0\")\n    max_length = input_ids.size(-1) + max_length\n\
    \    \n    greedy_output = model.generate(\n        input_ids.to('cuda:0'),\n\
    \        max_length=max_length,\n        do_sample=not greedy,\n        top_k=None\
    \ if greedy else top_k,\n    )\n    return tokenizer.decode(greedy_output[0],\
    \ skip_special_tokens=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\"\
    )\nprint(\"load tokenizer\")\naccelerator = Accelerator()\ndevice = accelerator.device\n\
    model = AutoModelForCausalLM.from_pretrained(\n    arg.checkpoints,\n    device_map=\"\
    auto\",\n    torch_dtype=torch.bfloat16,\n    offload_folder=\"./offload\",\n\
    )\nmodel = accelerator.prepare(model)\nprint(\"load model\")\n\n\ntext = 'hello'\n\
    output = generate_from_text(model, text, tokenizer, max_length=500, greedy=True,\
    \ top_k=1)\nprint(output)\n```\n\nand it shows :\n\nload tokenizer\nTraceback\
    \ (most recent call last):\n  File \"/mnt/yrfs/xuyifan/bloom/bigscience-sorteval/eva.py\"\
    , line 28, in <module>\n    model = accelerator.prepare(model)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/accelerate/accelerator.py\"\
    , line 545, in prepare\n    result = tuple(self._prepare_one(obj, first_pass=True)\
    \ for obj in args)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/accelerate/accelerator.py\"\
    , line 545, in <genexpr>\n    result = tuple(self._prepare_one(obj, first_pass=True)\
    \ for obj in args)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/accelerate/accelerator.py\"\
    , line 441, in _prepare_one\n    return self.prepare_model(obj)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/accelerate/accelerator.py\"\
    , line 565, in prepare_model\n    model = model.to(self.device)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 927, in to\n    return self._apply(convert)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 579, in _apply\n    module._apply(fn)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 579, in _apply\n    module._apply(fn)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 579, in _apply\n    module._apply(fn)\n  [Previous line repeated 2 more\
    \ times]\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 602, in _apply\n    param_applied = fn(param)\n  File \"/mnt/yrfs/qinkai/miniconda3/envs/bloom12/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 925, in convert\n    return t.to(device, dtype if t.is_floating_point()\
    \ or t.is_complex() else None, non_blocking)\nRuntimeError: CUDA out of memory.\
    \ Tried to allocate 1.53 GiB (GPU 0; 79.17 GiB total capacity; 77.14 GiB already\
    \ allocated; 1.20 GiB free; 77.14 GiB reserved in total by PyTorch) If reserved\
    \ memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
    \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
  created_at: 2022-07-23 17:20:58+00:00
  edited: false
  hidden: false
  id: 62dc3c0a4e5b927494d5a336
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-11-15T00:15:50.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;xuyifan&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/xuyifan\">@<span class=\"\
          underline\">xuyifan</span></a></span>\n\n\t</span></span> ! Sorry for the\
          \ late reply. Why is it that you need <code>Accelerator</code>? I don't\
          \ think you need it.</p>\n"
        raw: Hi @xuyifan ! Sorry for the late reply. Why is it that you need `Accelerator`?
          I don't think you need it.
        updatedAt: '2022-11-15T00:15:50.006Z'
      numEdits: 0
      reactions: []
    id: 6372da36f7e99092f907c868
    type: comment
  author: TimeRobber
  content: Hi @xuyifan ! Sorry for the late reply. Why is it that you need `Accelerator`?
    I don't think you need it.
  created_at: 2022-11-15 00:15:50+00:00
  edited: false
  hidden: false
  id: 6372da36f7e99092f907c868
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 62
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: How to use bloom-176B to generate or evaluate on Multi-graphics?
