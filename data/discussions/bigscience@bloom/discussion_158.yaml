!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 1geek0
conflicting_files: null
created_at: 2022-12-16 12:44:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ac6b3485348619d022f14e80e268443.svg
      fullname: Nilay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 1geek0
      type: user
    createdAt: '2022-12-16T12:44:09.000Z'
    data:
      edited: false
      editors:
      - 1geek0
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ac6b3485348619d022f14e80e268443.svg
          fullname: Nilay
          isHf: false
          isPro: false
          name: 1geek0
          type: user
        html: '<p>The model keeps downloading over and over again</p>

          <p>These are the only lines of code I''ve run:</p>

          <pre><code>from transformers import AutoTokenizer,AutoModelForCausalLM

          tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom")


          model = AutoModelForCausalLM.from_pretrained("bigscience/bloom", device_map="auto",

          torch_dtype="auto")```


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1671194619631-633dbefc67e67df066ee5dbc.png)

          </code></pre>

          '
        raw: "The model keeps downloading over and over again\r\n\r\nThese are the\
          \ only lines of code I've run:\r\n```\r\nfrom transformers import AutoTokenizer,AutoModelForCausalLM\r\
          \ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\")\r\n\r\n\
          model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\", device_map=\"\
          auto\",\r\ntorch_dtype=\"auto\")```\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1671194619631-633dbefc67e67df066ee5dbc.png)\r\
          \n"
        updatedAt: '2022-12-16T12:44:09.103Z'
      numEdits: 0
      reactions: []
    id: 639c6819a41c0ad6baf4ac86
    type: comment
  author: 1geek0
  content: "The model keeps downloading over and over again\r\n\r\nThese are the only\
    \ lines of code I've run:\r\n```\r\nfrom transformers import AutoTokenizer,AutoModelForCausalLM\r\
    \ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\")\r\n\r\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\", device_map=\"auto\"\
    ,\r\ntorch_dtype=\"auto\")```\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1671194619631-633dbefc67e67df066ee5dbc.png)\r\
    \n"
  created_at: 2022-12-16 12:44:09+00:00
  edited: false
  hidden: false
  id: 639c6819a41c0ad6baf4ac86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-12-17T14:43:35.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>This is expected. The model is 350gb so we actually save it in shards,
          seperate smaller files. So you have to download all the shards to get it
          working.</p>

          '
        raw: This is expected. The model is 350gb so we actually save it in shards,
          seperate smaller files. So you have to download all the shards to get it
          working.
        updatedAt: '2022-12-17T14:43:35.705Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - 1geek0
    id: 639dd5976f45b49b2fb157ca
    type: comment
  author: TimeRobber
  content: This is expected. The model is 350gb so we actually save it in shards,
    seperate smaller files. So you have to download all the shards to get it working.
  created_at: 2022-12-17 14:43:35+00:00
  edited: false
  hidden: false
  id: 639dd5976f45b49b2fb157ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ac6b3485348619d022f14e80e268443.svg
      fullname: Nilay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 1geek0
      type: user
    createdAt: '2022-12-17T19:48:27.000Z'
    data:
      edited: false
      editors:
      - 1geek0
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ac6b3485348619d022f14e80e268443.svg
          fullname: Nilay
          isHf: false
          isPro: false
          name: 1geek0
          type: user
        html: '<p>Ah, didn''t read the model size. Thanks</p>

          '
        raw: Ah, didn't read the model size. Thanks
        updatedAt: '2022-12-17T19:48:27.359Z'
      numEdits: 0
      reactions: []
      relatedEventId: 639e1d0b6f45b49b2fb7c85b
    id: 639e1d0b6f45b49b2fb7c85a
    type: comment
  author: 1geek0
  content: Ah, didn't read the model size. Thanks
  created_at: 2022-12-17 19:48:27+00:00
  edited: false
  hidden: false
  id: 639e1d0b6f45b49b2fb7c85a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7ac6b3485348619d022f14e80e268443.svg
      fullname: Nilay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 1geek0
      type: user
    createdAt: '2022-12-17T19:48:27.000Z'
    data:
      status: closed
    id: 639e1d0b6f45b49b2fb7c85b
    type: status-change
  author: 1geek0
  created_at: 2022-12-17 19:48:27+00:00
  id: 639e1d0b6f45b49b2fb7c85b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 158
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: Model keeps downloading
