!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ankit5678
conflicting_files: null
created_at: 2023-01-15 08:11:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a32529f3b0180438e0fc4926372a347.svg
      fullname: Ankit Panigrahy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ankit5678
      type: user
    createdAt: '2023-01-15T08:11:34.000Z'
    data:
      edited: false
      editors:
      - ankit5678
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a32529f3b0180438e0fc4926372a347.svg
          fullname: Ankit Panigrahy
          isHf: false
          isPro: false
          name: ankit5678
          type: user
        html: '<p>how to create summary using bloom , I wanted to compare the output
          of bloom with gpt-3.<br>is it possible?</p>

          '
        raw: "how to create summary using bloom , I wanted to compare the output of\
          \ bloom with gpt-3.\r\nis it possible?"
        updatedAt: '2023-01-15T08:11:34.378Z'
      numEdits: 0
      reactions: []
    id: 63c3b536c7d7f4c63a4ed95c
    type: comment
  author: ankit5678
  content: "how to create summary using bloom , I wanted to compare the output of\
    \ bloom with gpt-3.\r\nis it possible?"
  created_at: 2023-01-15 08:11:34+00:00
  edited: false
  hidden: false
  id: 63c3b536c7d7f4c63a4ed95c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-01-15T09:30:44.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>You may want to pre-pend your prompt with something like: <code>Summarize
          the following text: </code>(or put it after the text). Please also have
          a look at BLOOMZ, which is much better on these tasks than BLOOM: <a href="https://huggingface.co/bigscience/bloomz">https://huggingface.co/bigscience/bloomz</a></p>

          '
        raw: 'You may want to pre-pend your prompt with something like: `Summarize
          the following text: `(or put it after the text). Please also have a look
          at BLOOMZ, which is much better on these tasks than BLOOM: https://huggingface.co/bigscience/bloomz'
        updatedAt: '2023-01-15T09:30:44.864Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - Kato-22
    id: 63c3c7c46e793fba08759400
    type: comment
  author: ybelkada
  content: 'You may want to pre-pend your prompt with something like: `Summarize the
    following text: `(or put it after the text). Please also have a look at BLOOMZ,
    which is much better on these tasks than BLOOM: https://huggingface.co/bigscience/bloomz'
  created_at: 2023-01-15 09:30:44+00:00
  edited: false
  hidden: false
  id: 63c3c7c46e793fba08759400
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673670043032-62129484581b98bb1ad00a60.jpeg?w=200&h=200&f=face
      fullname: Nigel The Maker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NigelTheMaker
      type: user
    createdAt: '2023-01-21T07:59:02.000Z'
    data:
      edited: false
      editors:
      - NigelTheMaker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673670043032-62129484581b98bb1ad00a60.jpeg?w=200&h=200&f=face
          fullname: Nigel The Maker
          isHf: false
          isPro: false
          name: NigelTheMaker
          type: user
        html: '<p>The following is an abstractive summarisation system. It allows
          user to input a large block of text then it summarises that input in to
          a smaller paragraph. The system stops generating when it has finished summarising,
          it does not print extra text after. The system always sticks to the context
          when summarising, does not repeat any text and finishes at a full stop:</p>

          <p>Source text: Peter and  Elizabeth took a taxi to attend the right party
          in the city. While in the party Elizabeth collapsed and was rushed to the
          hospital.</p>

          <p>Summary: Elizabeth was hospitalised after attending a party with Peter.</p>

          <p>Source text: John went buy sweets at the corner shop. At some point on
          the way back he dropped 5 of them and was left with 3.</p>

          <p>Summary: John dropped 5 sweets on the way back from the corner shop and
          was left with 3.</p>

          <h3 id=""></h3>

          <p>If you to learn more about prompting or having questions about use bloom
          come join my discord <a rel="nofollow" href="https://discord.gg/y9newnc9">https://discord.gg/y9newnc9</a>.
          I''ll be releasing chat interface templates which come with built in prompts
          that you can easily customise. You will also have the ability to store and
          emit prompts into the interface  easily</p>

          '
        raw: 'The following is an abstractive summarisation system. It allows user
          to input a large block of text then it summarises that input in to a smaller
          paragraph. The system stops generating when it has finished summarising,
          it does not print extra text after. The system always sticks to the context
          when summarising, does not repeat any text and finishes at a full stop:


          Source text: Peter and  Elizabeth took a taxi to attend the right party
          in the city. While in the party Elizabeth collapsed and was rushed to the
          hospital.


          Summary: Elizabeth was hospitalised after attending a party with Peter.


          Source text: John went buy sweets at the corner shop. At some point on the
          way back he dropped 5 of them and was left with 3.


          Summary: John dropped 5 sweets on the way back from the corner shop and
          was left with 3.


          ###

          If you to learn more about prompting or having questions about use bloom
          come join my discord https://discord.gg/y9newnc9. I''ll be releasing chat
          interface templates which come with built in prompts that you can easily
          customise. You will also have the ability to store and emit prompts into
          the interface  easily'
        updatedAt: '2023-01-21T07:59:02.428Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - greboide
        - crgz
    id: 63cb9b467b5b4fad6e7f9e1b
    type: comment
  author: NigelTheMaker
  content: 'The following is an abstractive summarisation system. It allows user to
    input a large block of text then it summarises that input in to a smaller paragraph.
    The system stops generating when it has finished summarising, it does not print
    extra text after. The system always sticks to the context when summarising, does
    not repeat any text and finishes at a full stop:


    Source text: Peter and  Elizabeth took a taxi to attend the right party in the
    city. While in the party Elizabeth collapsed and was rushed to the hospital.


    Summary: Elizabeth was hospitalised after attending a party with Peter.


    Source text: John went buy sweets at the corner shop. At some point on the way
    back he dropped 5 of them and was left with 3.


    Summary: John dropped 5 sweets on the way back from the corner shop and was left
    with 3.


    ###

    If you to learn more about prompting or having questions about use bloom come
    join my discord https://discord.gg/y9newnc9. I''ll be releasing chat interface
    templates which come with built in prompts that you can easily customise. You
    will also have the ability to store and emit prompts into the interface  easily'
  created_at: 2023-01-21 07:59:02+00:00
  edited: false
  hidden: false
  id: 63cb9b467b5b4fad6e7f9e1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37eb5827544b03886973378c4ded3ad7.svg
      fullname: Saraswathi Chandrasekaran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sarasc
      type: user
    createdAt: '2023-03-23T08:05:42.000Z'
    data:
      edited: false
      editors:
      - sarasc
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37eb5827544b03886973378c4ded3ad7.svg
          fullname: Saraswathi Chandrasekaran
          isHf: false
          isPro: false
          name: sarasc
          type: user
        html: '<blockquote>

          <p>how to create summary using bloom , I wanted to compare the output of
          bloom with gpt-3.<br>is it possible?</p>

          </blockquote>

          <p>Did you get to perform this?</p>

          '
        raw: '> how to create summary using bloom , I wanted to compare the output
          of bloom with gpt-3.

          > is it possible?


          Did you get to perform this?'
        updatedAt: '2023-03-23T08:05:42.244Z'
      numEdits: 0
      reactions: []
    id: 641c085675cd1b2271354e84
    type: comment
  author: sarasc
  content: '> how to create summary using bloom , I wanted to compare the output of
    bloom with gpt-3.

    > is it possible?


    Did you get to perform this?'
  created_at: 2023-03-23 07:05:42+00:00
  edited: false
  hidden: false
  id: 641c085675cd1b2271354e84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e6470332ebc447f8a4ba32a339a0c01c.svg
      fullname: Nimja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nimja
      type: user
    createdAt: '2023-06-01T12:54:30.000Z'
    data:
      edited: true
      editors:
      - Nimja
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e6470332ebc447f8a4ba32a339a0c01c.svg
          fullname: Nimja
          isHf: false
          isPro: false
          name: Nimja
          type: user
        html: '<p>This is the code I used and for long texts, the summary is completely
          useless.</p>

          <p>Chat-GPT (3.5) gave me VERY good answers;</p>

          <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer,
          set_seed

          import torch

          torch.set_default_tensor_type(torch.cuda.FloatTensor)


          language_model = "bigscience/bloomz-1b1"


          model = AutoModelForCausalLM.from_pretrained(language_model, use_cache=True)

          model.cuda()

          tokenizer = AutoTokenizer.from_pretrained(language_model)


          prompt = "Mysteriously long text..."


          summary_instruction = f"\n\nSummarize the previous text in three sentences:\n\n"


          total_prompt = prompt + summary_instruction


          input_ids = tokenizer(prompt + f"\n\n" + summary_instruction, return_tensors="pt").to(0)

          sample = model.generate(**input_ids, max_length=5000,  top_k=1, temperature=0.9,
          repetition_penalty = 2.0)


          result_string = tokenizer.decode(sample[0], truncate_before_pattern=[r"\n\n^#",
          "^''", "\n\n\n"])


          print(result_string[len(total_prompt)::])

          </code></pre>

          '
        raw: 'This is the code I used and for long texts, the summary is completely
          useless.


          Chat-GPT (3.5) gave me VERY good answers;


          ```

          from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed

          import torch

          torch.set_default_tensor_type(torch.cuda.FloatTensor)


          language_model = "bigscience/bloomz-1b1"


          model = AutoModelForCausalLM.from_pretrained(language_model, use_cache=True)

          model.cuda()

          tokenizer = AutoTokenizer.from_pretrained(language_model)


          prompt = "Mysteriously long text..."


          summary_instruction = f"\n\nSummarize the previous text in three sentences:\n\n"


          total_prompt = prompt + summary_instruction


          input_ids = tokenizer(prompt + f"\n\n" + summary_instruction, return_tensors="pt").to(0)

          sample = model.generate(**input_ids, max_length=5000,  top_k=1, temperature=0.9,
          repetition_penalty = 2.0)


          result_string = tokenizer.decode(sample[0], truncate_before_pattern=[r"\n\n^#",
          "^''", "\n\n\n"])


          print(result_string[len(total_prompt)::])

          ```'
        updatedAt: '2023-06-01T12:55:13.072Z'
      numEdits: 1
      reactions: []
    id: 647895061f9756aa89d1ccc8
    type: comment
  author: Nimja
  content: 'This is the code I used and for long texts, the summary is completely
    useless.


    Chat-GPT (3.5) gave me VERY good answers;


    ```

    from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed

    import torch

    torch.set_default_tensor_type(torch.cuda.FloatTensor)


    language_model = "bigscience/bloomz-1b1"


    model = AutoModelForCausalLM.from_pretrained(language_model, use_cache=True)

    model.cuda()

    tokenizer = AutoTokenizer.from_pretrained(language_model)


    prompt = "Mysteriously long text..."


    summary_instruction = f"\n\nSummarize the previous text in three sentences:\n\n"


    total_prompt = prompt + summary_instruction


    input_ids = tokenizer(prompt + f"\n\n" + summary_instruction, return_tensors="pt").to(0)

    sample = model.generate(**input_ids, max_length=5000,  top_k=1, temperature=0.9,
    repetition_penalty = 2.0)


    result_string = tokenizer.decode(sample[0], truncate_before_pattern=[r"\n\n^#",
    "^''", "\n\n\n"])


    print(result_string[len(total_prompt)::])

    ```'
  created_at: 2023-06-01 11:54:30+00:00
  edited: true
  hidden: false
  id: 647895061f9756aa89d1ccc8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e6470332ebc447f8a4ba32a339a0c01c.svg
      fullname: Nimja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nimja
      type: user
    createdAt: '2023-06-01T12:57:50.000Z'
    data:
      edited: false
      editors:
      - Nimja
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e6470332ebc447f8a4ba32a339a0c01c.svg
          fullname: Nimja
          isHf: false
          isPro: false
          name: Nimja
          type: user
        html: '<blockquote>

          <p>The following is an abstractive summarisation system. It allows user
          to input a large block of text then it summarises that input in to a smaller
          paragraph. The system stops generating when it has finished summarising,
          it does not print extra text after. The system always sticks to the context
          when summarising, does not repeat any text and finishes at a full stop:</p>

          <p>Source text: Peter and  Elizabeth took a taxi to attend the right party
          in the city. While in the party Elizabeth collapsed and was rushed to the
          hospital.</p>

          <p>Summary: Elizabeth was hospitalised after attending a party with Peter.</p>

          <p>Source text: John went buy sweets at the corner shop. At some point on
          the way back he dropped 5 of them and was left with 3.</p>

          <p>Summary: John dropped 5 sweets on the way back from the corner shop and
          was left with 3.</p>

          </blockquote>

          <p>Why not share the prompt and explanation here, just like you did here:
          <a href="https://huggingface.co/bigscience/bloom/discussions/183">https://huggingface.co/bigscience/bloom/discussions/183</a></p>

          '
        raw: "> The following is an abstractive summarisation system. It allows user\
          \ to input a large block of text then it summarises that input in to a smaller\
          \ paragraph. The system stops generating when it has finished summarising,\
          \ it does not print extra text after. The system always sticks to the context\
          \ when summarising, does not repeat any text and finishes at a full stop:\n\
          > \n> Source text: Peter and  Elizabeth took a taxi to attend the right\
          \ party in the city. While in the party Elizabeth collapsed and was rushed\
          \ to the hospital.\n> \n> Summary: Elizabeth was hospitalised after attending\
          \ a party with Peter.\n> \n> Source text: John went buy sweets at the corner\
          \ shop. At some point on the way back he dropped 5 of them and was left\
          \ with 3.\n> \n> Summary: John dropped 5 sweets on the way back from the\
          \ corner shop and was left with 3.\n> \n\n\nWhy not share the prompt and\
          \ explanation here, just like you did here: https://huggingface.co/bigscience/bloom/discussions/183"
        updatedAt: '2023-06-01T12:57:50.271Z'
      numEdits: 0
      reactions: []
    id: 647895ce159a889d00251190
    type: comment
  author: Nimja
  content: "> The following is an abstractive summarisation system. It allows user\
    \ to input a large block of text then it summarises that input in to a smaller\
    \ paragraph. The system stops generating when it has finished summarising, it\
    \ does not print extra text after. The system always sticks to the context when\
    \ summarising, does not repeat any text and finishes at a full stop:\n> \n> Source\
    \ text: Peter and  Elizabeth took a taxi to attend the right party in the city.\
    \ While in the party Elizabeth collapsed and was rushed to the hospital.\n> \n\
    > Summary: Elizabeth was hospitalised after attending a party with Peter.\n> \n\
    > Source text: John went buy sweets at the corner shop. At some point on the way\
    \ back he dropped 5 of them and was left with 3.\n> \n> Summary: John dropped\
    \ 5 sweets on the way back from the corner shop and was left with 3.\n> \n\n\n\
    Why not share the prompt and explanation here, just like you did here: https://huggingface.co/bigscience/bloom/discussions/183"
  created_at: 2023-06-01 11:57:50+00:00
  edited: false
  hidden: false
  id: 647895ce159a889d00251190
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 172
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: How to use BLOOM for text summarization ?
