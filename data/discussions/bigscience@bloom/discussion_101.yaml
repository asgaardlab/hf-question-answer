!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Saiyan
conflicting_files: null
created_at: 2022-09-01 05:53:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d38adaaa8bc2ff32c0b8000848b79737.svg
      fullname: Roy Sadaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saiyan
      type: user
    createdAt: '2022-09-01T06:53:26.000Z'
    data:
      edited: false
      editors:
      - Saiyan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d38adaaa8bc2ff32c0b8000848b79737.svg
          fullname: Roy Sadaka
          isHf: false
          isPro: false
          name: Saiyan
          type: user
        html: "<p>Hey,<br>Is it possible with BL\U0001F338\U0001F338M to encode a\
          \ big prompt for few-shot, and then 'append' the added text for final completion?</p>\n\
          <p>For example, say the prompt \"template\" is (toy example, usually the\
          \ prompt will be much bigger):<br>\"<br>English =&gt; French<br>cheese =&gt;\
          \ fromage<br>chair =&gt; chaise<br>&lt;ADDED_TEXT&gt; =&gt;<br>\"<br>so\
          \ is it possible to encode (only one time) all the text <strong>until</strong>\
          \ the &lt;ADDED_TEXT&gt; part, and then use that state with many different<br>&lt;ADDED_TEXT&gt;\
          \ examples to make the final predictions? instead of encoding the whole\
          \ text every time</p>\n<p>Thank you in advance</p>\n"
        raw: "Hey,\r\nIs it possible with BL\U0001F338\U0001F338M to encode a big\
          \ prompt for few-shot, and then 'append' the added text for final completion?\r\
          \n\r\nFor example, say the prompt \"template\" is (toy example, usually\
          \ the prompt will be much bigger):\r\n\"\r\nEnglish => French\r\ncheese\
          \ => fromage\r\nchair => chaise\r\n\\<ADDED_TEXT\\> =>\r\n\"\r\nso is it\
          \ possible to encode (only one time) all the text **until** the \\<ADDED_TEXT\\\
          > part, and then use that state with many different \r\n\\<ADDED_TEXT\\\
          > examples to make the final predictions? instead of encoding the whole\
          \ text every time\r\n\r\nThank you in advance"
        updatedAt: '2022-09-01T06:53:26.400Z'
      numEdits: 0
      reactions: []
    id: 631056e62c0e5e81e84b5236
    type: comment
  author: Saiyan
  content: "Hey,\r\nIs it possible with BL\U0001F338\U0001F338M to encode a big prompt\
    \ for few-shot, and then 'append' the added text for final completion?\r\n\r\n\
    For example, say the prompt \"template\" is (toy example, usually the prompt will\
    \ be much bigger):\r\n\"\r\nEnglish => French\r\ncheese => fromage\r\nchair =>\
    \ chaise\r\n\\<ADDED_TEXT\\> =>\r\n\"\r\nso is it possible to encode (only one\
    \ time) all the text **until** the \\<ADDED_TEXT\\> part, and then use that state\
    \ with many different \r\n\\<ADDED_TEXT\\> examples to make the final predictions?\
    \ instead of encoding the whole text every time\r\n\r\nThank you in advance"
  created_at: 2022-09-01 05:53:26+00:00
  edited: false
  hidden: false
  id: 631056e62c0e5e81e84b5236
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 101
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Is Few-shots performance optimization possible? (keep initial prompt encoded
  state)
