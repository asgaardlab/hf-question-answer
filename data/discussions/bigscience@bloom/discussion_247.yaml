!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hepbc
conflicting_files: null
created_at: 2023-05-12 13:22:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5b9aba9646df6e332c3ed2597d57d62e.svg
      fullname: BC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hepbc
      type: user
    createdAt: '2023-05-12T14:22:48.000Z'
    data:
      edited: false
      editors:
      - hepbc
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5b9aba9646df6e332c3ed2597d57d62e.svg
          fullname: BC
          isHf: false
          isPro: false
          name: hepbc
          type: user
        html: '<p>Hi folks: am using bloom with Langchain, initialized with temp=1e-10.
          I am using a FewShotPromptTemplate with 4 examples. It is doing an ok job
          but truncates the output at a random point. When I initialize with max_tokens,
          it gives the full string but fills up the balance of the output with arbitrary
          characters. Anyone else seen this happen and has a solution? Am using the
          free tier of HuggingFaceHub.</p>

          <p>Thanks! </p>

          '
        raw: "Hi folks: am using bloom with Langchain, initialized with temp=1e-10.\
          \ I am using a FewShotPromptTemplate with 4 examples. It is doing an ok\
          \ job but truncates the output at a random point. When I initialize with\
          \ max_tokens, it gives the full string but fills up the balance of the output\
          \ with arbitrary characters. Anyone else seen this happen and has a solution?\
          \ Am using the free tier of HuggingFaceHub.\r\n\r\nThanks! "
        updatedAt: '2023-05-12T14:22:48.427Z'
      numEdits: 0
      reactions: []
    id: 645e4bb86e92ff2e8e490248
    type: comment
  author: hepbc
  content: "Hi folks: am using bloom with Langchain, initialized with temp=1e-10.\
    \ I am using a FewShotPromptTemplate with 4 examples. It is doing an ok job but\
    \ truncates the output at a random point. When I initialize with max_tokens, it\
    \ gives the full string but fills up the balance of the output with arbitrary\
    \ characters. Anyone else seen this happen and has a solution? Am using the free\
    \ tier of HuggingFaceHub.\r\n\r\nThanks! "
  created_at: 2023-05-12 13:22:48+00:00
  edited: false
  hidden: false
  id: 645e4bb86e92ff2e8e490248
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd351ae83b3a49c828bc6b4b5320844e.svg
      fullname: Alejandro
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: goofyahead
      type: user
    createdAt: '2023-06-06T14:14:32.000Z'
    data:
      edited: false
      editors:
      - goofyahead
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8931789994239807
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd351ae83b3a49c828bc6b4b5320844e.svg
          fullname: Alejandro
          isHf: false
          isPro: false
          name: goofyahead
          type: user
        html: '<p>I do have the same issue, any luck with the parameters and testing?</p>

          '
        raw: I do have the same issue, any luck with the parameters and testing?
        updatedAt: '2023-06-06T14:14:32.569Z'
      numEdits: 0
      reactions: []
    id: 647f3f481a446a624a445026
    type: comment
  author: goofyahead
  content: I do have the same issue, any luck with the parameters and testing?
  created_at: 2023-06-06 13:14:32+00:00
  edited: false
  hidden: false
  id: 647f3f481a446a624a445026
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 247
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Output getting truncated while using Langchain
