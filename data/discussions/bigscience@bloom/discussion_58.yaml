!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jurassicpark
conflicting_files: null
created_at: 2022-07-20 15:40:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef1d4387370689683a472a2fd000d0dd.svg
      fullname: dude
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jurassicpark
      type: user
    createdAt: '2022-07-20T16:40:19.000Z'
    data:
      edited: false
      editors:
      - jurassicpark
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef1d4387370689683a472a2fd000d0dd.svg
          fullname: dude
          isHf: false
          isPro: false
          name: jurassicpark
          type: user
        html: '<p>I was looking and couldn''t find any recommendations for the required
          hardware to run this model in inference on the CPU or GPU.</p>

          <p>I''m going to test it out but some guidance would be pretty helpful.
          </p>

          <p>Does anyone have this data? Particularly, how much RAM for CPU, and amount
          of GPU RAM (I''ve seen some threads saying ~352GB). Also, perhaps what kind
          of inference times can be expected with different setups.</p>

          '
        raw: "I was looking and couldn't find any recommendations for the required\
          \ hardware to run this model in inference on the CPU or GPU.\r\n\r\nI'm\
          \ going to test it out but some guidance would be pretty helpful. \r\n\r\
          \nDoes anyone have this data? Particularly, how much RAM for CPU, and amount\
          \ of GPU RAM (I've seen some threads saying ~352GB). Also, perhaps what\
          \ kind of inference times can be expected with different setups."
        updatedAt: '2022-07-20T16:40:19.868Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - apol
        - schneiderfelipe
        - Kyounga
        - wpmps
        - usholanb
    id: 62d82ff3e2b34bed743d3005
    type: comment
  author: jurassicpark
  content: "I was looking and couldn't find any recommendations for the required hardware\
    \ to run this model in inference on the CPU or GPU.\r\n\r\nI'm going to test it\
    \ out but some guidance would be pretty helpful. \r\n\r\nDoes anyone have this\
    \ data? Particularly, how much RAM for CPU, and amount of GPU RAM (I've seen some\
    \ threads saying ~352GB). Also, perhaps what kind of inference times can be expected\
    \ with different setups."
  created_at: 2022-07-20 15:40:19+00:00
  edited: false
  hidden: false
  id: 62d82ff3e2b34bed743d3005
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef1d4387370689683a472a2fd000d0dd.svg
      fullname: dude
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jurassicpark
      type: user
    createdAt: '2022-07-20T16:53:39.000Z'
    data:
      edited: false
      editors:
      - jurassicpark
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef1d4387370689683a472a2fd000d0dd.svg
          fullname: dude
          isHf: false
          isPro: false
          name: jurassicpark
          type: user
        html: "<p>Copying some data I found from other threads here:</p>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;IanBeaver&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/IanBeaver\">@<span class=\"\
          underline\">IanBeaver</span></a></span>\n\n\t</span></span></p>\n<blockquote>\n\
          <p>It needed around 400GB [disk space] just to fit the all the weights files.\
          \ They list the sizes of the weights and checkpoints under the Training\
          \ section.</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;IanBeaver&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/IanBeaver\"\
          >@<span class=\"underline\">IanBeaver</span></a></span>\n\n\t</span></span></p>\n\
          <blockquote>\n<p>I have successfully loaded it on a single x2iezn.6xlarge\
          \ instance in AWS but using only CPUs the model is very slow. Text generation\
          \ sampling for several sequences can take several minutes to return, but\
          \ the full model is working and it is much cheaper for local evaluation\
          \ than 9 GPUs!</p>\n</blockquote>\n<p>x2iezn.6xlarge specs:</p>\n<ul>\n\
          <li>768gb RAM</li>\n<li>24 vcpus</li>\n<li>$5.004 / hour</li>\n</ul>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;maveriq&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/maveriq\">@<span class=\"underline\"\
          >maveriq</span></a></span>\n\n\t</span></span></p>\n<blockquote>\n<p>As\
          \ a first order estimate, 176B parameters in half precision (16 bits = 2\
          \ bytes) would need 352 GB RAM. But since some modules are 32-bit, it would\
          \ be more. So about nine GPUs with 40-GB RAM, and it doesn't take into account\
          \ the input.</p>\n</blockquote>\n"
        raw: "Copying some data I found from other threads here:\n\n@IanBeaver\n>\
          \ It needed around 400GB [disk space] just to fit the all the weights files.\
          \ They list the sizes of the weights and checkpoints under the Training\
          \ section.\n\n@IanBeaver\n> I have successfully loaded it on a single x2iezn.6xlarge\
          \ instance in AWS but using only CPUs the model is very slow. Text generation\
          \ sampling for several sequences can take several minutes to return, but\
          \ the full model is working and it is much cheaper for local evaluation\
          \ than 9 GPUs!\n\nx2iezn.6xlarge specs:\n - 768gb RAM\n - 24 vcpus\n - $5.004\
          \ / hour\n\n@maveriq\n> As a first order estimate, 176B parameters in half\
          \ precision (16 bits = 2 bytes) would need 352 GB RAM. But since some modules\
          \ are 32-bit, it would be more. So about nine GPUs with 40-GB RAM, and it\
          \ doesn't take into account the input."
        updatedAt: '2022-07-20T16:53:39.894Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - bwv988
        - tombailey
        - apol
        - luzdealba
        - michal-stefanik
        - schneiderfelipe
    id: 62d83313ad693a1a96287409
    type: comment
  author: jurassicpark
  content: "Copying some data I found from other threads here:\n\n@IanBeaver\n> It\
    \ needed around 400GB [disk space] just to fit the all the weights files. They\
    \ list the sizes of the weights and checkpoints under the Training section.\n\n\
    @IanBeaver\n> I have successfully loaded it on a single x2iezn.6xlarge instance\
    \ in AWS but using only CPUs the model is very slow. Text generation sampling\
    \ for several sequences can take several minutes to return, but the full model\
    \ is working and it is much cheaper for local evaluation than 9 GPUs!\n\nx2iezn.6xlarge\
    \ specs:\n - 768gb RAM\n - 24 vcpus\n - $5.004 / hour\n\n@maveriq\n> As a first\
    \ order estimate, 176B parameters in half precision (16 bits = 2 bytes) would\
    \ need 352 GB RAM. But since some modules are 32-bit, it would be more. So about\
    \ nine GPUs with 40-GB RAM, and it doesn't take into account the input."
  created_at: 2022-07-20 15:53:39+00:00
  edited: false
  hidden: false
  id: 62d83313ad693a1a96287409
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-07-22T01:05:27.000Z'
    data:
      edited: false
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: '<p>GPU RAM requires more than  352 GB RAM (176B parameters in half-precision).  I
          can do the inference on 8 A6000 GPUs. However, there isn''t much room left
          for input tokens.</p>

          '
        raw: GPU RAM requires more than  352 GB RAM (176B parameters in half-precision).  I
          can do the inference on 8 A6000 GPUs. However, there isn't much room left
          for input tokens.
        updatedAt: '2022-07-22T01:05:27.160Z'
      numEdits: 0
      reactions: []
    id: 62d9f7d728f71aa2a1810410
    type: comment
  author: pai4451
  content: GPU RAM requires more than  352 GB RAM (176B parameters in half-precision).  I
    can do the inference on 8 A6000 GPUs. However, there isn't much room left for
    input tokens.
  created_at: 2022-07-22 00:05:27+00:00
  edited: false
  hidden: false
  id: 62d9f7d728f71aa2a1810410
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcc4d85164699cad4633e47bdbf2f07d.svg
      fullname: Ralph Schlosser
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bwv988
      type: user
    createdAt: '2022-07-22T11:27:28.000Z'
    data:
      edited: false
      editors:
      - bwv988
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcc4d85164699cad4633e47bdbf2f07d.svg
          fullname: Ralph Schlosser
          isHf: false
          isPro: false
          name: bwv988
          type: user
        html: '<blockquote>

          <p>Copying some data I found from other threads here:...</p>

          </blockquote>

          <p>Thanks for this, very helpful, was looking for the same information.
          No wonder I am failing to run the full model on a 64GB VM. ;)</p>

          <p>Have you come across any recommendations anywhere to reduce memory usage,
          say, for specific pipeline tasks?</p>

          '
        raw: '> Copying some data I found from other threads here:...


          Thanks for this, very helpful, was looking for the same information. No
          wonder I am failing to run the full model on a 64GB VM. ;)


          Have you come across any recommendations anywhere to reduce memory usage,
          say, for specific pipeline tasks?'
        updatedAt: '2022-07-22T11:27:28.687Z'
      numEdits: 0
      reactions: []
    id: 62da89a0a8ead43d20e20ee3
    type: comment
  author: bwv988
  content: '> Copying some data I found from other threads here:...


    Thanks for this, very helpful, was looking for the same information. No wonder
    I am failing to run the full model on a 64GB VM. ;)


    Have you come across any recommendations anywhere to reduce memory usage, say,
    for specific pipeline tasks?'
  created_at: 2022-07-22 10:27:28+00:00
  edited: false
  hidden: false
  id: 62da89a0a8ead43d20e20ee3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ceb79297ba2c688aa65279614b733c7.svg
      fullname: kiran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: snarik
      type: user
    createdAt: '2022-09-11T21:45:32.000Z'
    data:
      edited: false
      editors:
      - snarik
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ceb79297ba2c688aa65279614b733c7.svg
          fullname: kiran
          isHf: false
          isPro: false
          name: snarik
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;bwv988&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bwv988\">@<span class=\"\
          underline\">bwv988</span></a></span>\n\n\t</span></span> Your best bet is\
          \ to try out bitsandbytes. <a rel=\"nofollow\" href=\"https://github.com/TimDettmers/bitsandbytes\"\
          >https://github.com/TimDettmers/bitsandbytes</a></p>\n"
        raw: '@bwv988 Your best bet is to try out bitsandbytes. https://github.com/TimDettmers/bitsandbytes'
        updatedAt: '2022-09-11T21:45:32.012Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - bwv988
    id: 631e56fc6d6a5870f3d7fac3
    type: comment
  author: snarik
  content: '@bwv988 Your best bet is to try out bitsandbytes. https://github.com/TimDettmers/bitsandbytes'
  created_at: 2022-09-11 20:45:32+00:00
  edited: false
  hidden: false
  id: 631e56fc6d6a5870f3d7fac3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcc4d85164699cad4633e47bdbf2f07d.svg
      fullname: Ralph Schlosser
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bwv988
      type: user
    createdAt: '2022-10-01T10:31:01.000Z'
    data:
      edited: false
      editors:
      - bwv988
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcc4d85164699cad4633e47bdbf2f07d.svg
          fullname: Ralph Schlosser
          isHf: false
          isPro: false
          name: bwv988
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;snarik&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/snarik\"\
          >@<span class=\"underline\">snarik</span></a></span>\n\n\t</span></span>,\
          \ will give this a go!</p>\n"
        raw: Thanks @snarik, will give this a go!
        updatedAt: '2022-10-01T10:31:01.150Z'
      numEdits: 0
      reactions: []
    id: 633816e5322880a11fdcf198
    type: comment
  author: bwv988
  content: Thanks @snarik, will give this a go!
  created_at: 2022-10-01 09:31:01+00:00
  edited: false
  hidden: false
  id: 633816e5322880a11fdcf198
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7a228119da5bcc075e2467d4d0899c39.svg
      fullname: Maximilian Laurenz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: needfulthing
      type: user
    createdAt: '2023-04-24T08:02:20.000Z'
    data:
      edited: false
      editors:
      - needfulthing
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7a228119da5bcc075e2467d4d0899c39.svg
          fullname: Maximilian Laurenz
          isHf: false
          isPro: false
          name: needfulthing
          type: user
        html: '<p>This configuration claims to run on &gt;16&nbsp;GB RAM and a single
          CPU:</p>

          <p><a rel="nofollow" href="https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32">https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32</a></p>

          '
        raw: "This configuration claims to run on >16\_GB RAM and a single CPU:\n\n\
          https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32"
        updatedAt: '2023-04-24T08:02:20.146Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - bwv988
    id: 6446378ce1136600536eb9aa
    type: comment
  author: needfulthing
  content: "This configuration claims to run on >16\_GB RAM and a single CPU:\n\n\
    https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32"
  created_at: 2023-04-24 07:02:20+00:00
  edited: false
  hidden: false
  id: 6446378ce1136600536eb9aa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 58
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Hardware Requirements for CPU / GPU Inference
