!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mayank31398
conflicting_files: null
created_at: 2022-07-20 20:55:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2022-07-20T21:55:33.000Z'
    data:
      edited: true
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: '<p>I have a dedicated server (in complete isolation) running 8 A100
          80GBs.<br>The inference time using the HF checkpoints is:</p>

          <p>Input: The president of US is<br>top_k: 5, top_p: 0.9, temperature: 0.7,
          min_length: 1, max_length: 20<br>Output: The president of US is a man of
          his word. He has promised to make America great again.</p>

          <p>This took 90 seconds. Is this normal?</p>

          <p>All GPUs are being used:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1658354492351-62cd5057674cdb524450093d.png"><img
          alt="Screen Shot 2022-07-21 at 3.31.21 AM.png" src="https://cdn-uploads.huggingface.co/production/uploads/1658354492351-62cd5057674cdb524450093d.png"></a></p>

          '
        raw: 'I have a dedicated server (in complete isolation) running 8 A100 80GBs.

          The inference time using the HF checkpoints is:


          Input: The president of US is

          top_k: 5, top_p: 0.9, temperature: 0.7, min_length: 1, max_length: 20

          Output: The president of US is a man of his word. He has promised to make
          America great again.


          This took 90 seconds. Is this normal?


          All GPUs are being used:



          ![Screen Shot 2022-07-21 at 3.31.21 AM.png](https://cdn-uploads.huggingface.co/production/uploads/1658354492351-62cd5057674cdb524450093d.png)

          '
        updatedAt: '2022-07-20T22:01:34.131Z'
      numEdits: 1
      reactions: []
    id: 62d879d5acea6137d8b3b8de
    type: comment
  author: mayank31398
  content: 'I have a dedicated server (in complete isolation) running 8 A100 80GBs.

    The inference time using the HF checkpoints is:


    Input: The president of US is

    top_k: 5, top_p: 0.9, temperature: 0.7, min_length: 1, max_length: 20

    Output: The president of US is a man of his word. He has promised to make America
    great again.


    This took 90 seconds. Is this normal?


    All GPUs are being used:



    ![Screen Shot 2022-07-21 at 3.31.21 AM.png](https://cdn-uploads.huggingface.co/production/uploads/1658354492351-62cd5057674cdb524450093d.png)

    '
  created_at: 2022-07-20 20:55:33+00:00
  edited: true
  hidden: false
  id: 62d879d5acea6137d8b3b8de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2022-07-20T22:00:49.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: '<p>Also getting this warning every time:<br>huggingface/tokenizers:
          The current process just got forked, after parallelism has already been
          used. Disabling parallelism to avoid deadlocks...<br>To disable this warning,
          you can either:<br>        - Avoid using <code>tokenizers</code> before
          the fork if possible<br>        - Explicitly set the environment variable
          TOKENIZERS_PARALLELISM=(true | false)</p>

          <p>Not sure if it is related.</p>

          '
        raw: "Also getting this warning every time:\nhuggingface/tokenizers: The current\
          \ process just got forked, after parallelism has already been used. Disabling\
          \ parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\
          \        - Avoid using `tokenizers` before the fork if possible\n      \
          \  - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true\
          \ | false)\n\nNot sure if it is related."
        updatedAt: '2022-07-20T22:00:49.121Z'
      numEdits: 0
      reactions: []
    id: 62d87b1137b4ac79a1215267
    type: comment
  author: mayank31398
  content: "Also getting this warning every time:\nhuggingface/tokenizers: The current\
    \ process just got forked, after parallelism has already been used. Disabling\
    \ parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\
    \        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly\
    \ set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\nNot sure\
    \ if it is related."
  created_at: 2022-07-20 21:00:49+00:00
  edited: false
  hidden: false
  id: 62d87b1137b4ac79a1215267
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2022-07-20T23:36:51.000Z'
    data:
      edited: true
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: '<p>Should I use Megatron-DeepSpeed for inference?<br>This is quite
          slow<br>There are instance where generation speed can hit upto 500 seconds</p>

          <p>This is how I am using the model:<br> 43             self.model = AutoModelForCausalLM.from_pretrained(<br>
          44                 args.model_name,<br> 45                 device_map="auto",<br>
          46                 dtype="auto"<br> 47             )</p>

          <p> 65             output = self.model.generate(<br> 66                 input_ids=torch.tensor(x["input_ids"]),<br>
          67                 attention_mask=torch.tensor(x["attention_mask"]),<br>
          68                 top_k=top_k,<br> 69                 top_p=top_p,<br>
          70                 temperature=temperature,<br> 71                 min_length=min_length,<br>
          72                 max_length=max_length<br> 73             )</p>

          '
        raw: "Should I use Megatron-DeepSpeed for inference?\nThis is quite slow\n\
          There are instance where generation speed can hit upto 500 seconds\n\nThis\
          \ is how I am using the model:\n 43             self.model = AutoModelForCausalLM.from_pretrained(\n\
          \ 44                 args.model_name,\n 45                 device_map=\"\
          auto\",\n 46                 dtype=\"auto\"\n 47             )\n \n 65 \
          \            output = self.model.generate(\n 66                 input_ids=torch.tensor(x[\"\
          input_ids\"]),\n 67                 attention_mask=torch.tensor(x[\"attention_mask\"\
          ]),\n 68                 top_k=top_k,\n 69                 top_p=top_p,\n\
          \ 70                 temperature=temperature,\n 71                 min_length=min_length,\n\
          \ 72                 max_length=max_length\n 73             )"
        updatedAt: '2022-07-20T23:39:38.199Z'
      numEdits: 1
      reactions: []
    id: 62d89193acea6137d8b483a8
    type: comment
  author: mayank31398
  content: "Should I use Megatron-DeepSpeed for inference?\nThis is quite slow\nThere\
    \ are instance where generation speed can hit upto 500 seconds\n\nThis is how\
    \ I am using the model:\n 43             self.model = AutoModelForCausalLM.from_pretrained(\n\
    \ 44                 args.model_name,\n 45                 device_map=\"auto\"\
    ,\n 46                 dtype=\"auto\"\n 47             )\n \n 65             output\
    \ = self.model.generate(\n 66                 input_ids=torch.tensor(x[\"input_ids\"\
    ]),\n 67                 attention_mask=torch.tensor(x[\"attention_mask\"]),\n\
    \ 68                 top_k=top_k,\n 69                 top_p=top_p,\n 70     \
    \            temperature=temperature,\n 71                 min_length=min_length,\n\
    \ 72                 max_length=max_length\n 73             )"
  created_at: 2022-07-20 22:36:51+00:00
  edited: true
  hidden: false
  id: 62d89193acea6137d8b483a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1e984948a9ec5af6e4c6cc4e003fe0f.svg
      fullname: donut32
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: donut32
      type: user
    createdAt: '2022-07-21T15:32:23.000Z'
    data:
      edited: true
      editors:
      - donut32
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1e984948a9ec5af6e4c6cc4e003fe0f.svg
          fullname: donut32
          isHf: false
          isPro: false
          name: donut32
          type: user
        html: '<p>Hi, same issue here with 8 A6000 48GB.</p>

          <p>Takes up to 600 seconds for generating 700 tokens.</p>

          <p>I am wondering which speed up methods are most promising. Has someone
          already gathered experience or can provide any other suggestions?</p>

          '
        raw: 'Hi, same issue here with 8 A6000 48GB.


          Takes up to 600 seconds for generating 700 tokens.


          I am wondering which speed up methods are most promising. Has someone already
          gathered experience or can provide any other suggestions?'
        updatedAt: '2022-07-21T15:40:04.726Z'
      numEdits: 1
      reactions: []
    id: 62d97187e1c24f87e1838982
    type: comment
  author: donut32
  content: 'Hi, same issue here with 8 A6000 48GB.


    Takes up to 600 seconds for generating 700 tokens.


    I am wondering which speed up methods are most promising. Has someone already
    gathered experience or can provide any other suggestions?'
  created_at: 2022-07-21 14:32:23+00:00
  edited: true
  hidden: false
  id: 62d97187e1c24f87e1838982
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
      fullname: Ian Beaver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IanBeaver
      type: user
    createdAt: '2022-07-21T16:11:55.000Z'
    data:
      edited: false
      editors:
      - IanBeaver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
          fullname: Ian Beaver
          isHf: false
          isPro: false
          name: IanBeaver
          type: user
        html: '<blockquote>

          <p>Hi, same issue here with 8 A6000 48GB.</p>

          <p>Takes up to 600 seconds for generating 700 tokens.</p>

          <p>I am wondering which speed up methods are most promising. Has someone
          already gathered experience or can provide any other suggestions?</p>

          </blockquote>

          <p>Interesting, I was seeing times not much different than that using pure
          CPUs but I figured it was only so slow because it was on CPUs.  8x48GB is
          only 384GB of GPU RAM though so you may be offloading some to CPU RAM during
          inference.  The model consumes around 650GB of system RAM when using pure
          CPUs during inference.  Still, I would have expected it to be much faster
          than what I was seeing with no GPUs.  I would like to know the setup that
          HF is using to host the API, it seems decently performant for inference.</p>

          '
        raw: "> Hi, same issue here with 8 A6000 48GB.\n> \n> Takes up to 600 seconds\
          \ for generating 700 tokens.\n> \n> I am wondering which speed up methods\
          \ are most promising. Has someone already gathered experience or can provide\
          \ any other suggestions?\n\nInteresting, I was seeing times not much different\
          \ than that using pure CPUs but I figured it was only so slow because it\
          \ was on CPUs.  8x48GB is only 384GB of GPU RAM though so you may be offloading\
          \ some to CPU RAM during inference.  The model consumes around 650GB of\
          \ system RAM when using pure CPUs during inference.  Still, I would have\
          \ expected it to be much faster than what I was seeing with no GPUs.  I\
          \ would like to know the setup that HF is using to host the API, it seems\
          \ decently performant for inference."
        updatedAt: '2022-07-21T16:11:55.945Z'
      numEdits: 0
      reactions: []
    id: 62d97acb0f293ca5af5a97c4
    type: comment
  author: IanBeaver
  content: "> Hi, same issue here with 8 A6000 48GB.\n> \n> Takes up to 600 seconds\
    \ for generating 700 tokens.\n> \n> I am wondering which speed up methods are\
    \ most promising. Has someone already gathered experience or can provide any other\
    \ suggestions?\n\nInteresting, I was seeing times not much different than that\
    \ using pure CPUs but I figured it was only so slow because it was on CPUs.  8x48GB\
    \ is only 384GB of GPU RAM though so you may be offloading some to CPU RAM during\
    \ inference.  The model consumes around 650GB of system RAM when using pure CPUs\
    \ during inference.  Still, I would have expected it to be much faster than what\
    \ I was seeing with no GPUs.  I would like to know the setup that HF is using\
    \ to host the API, it seems decently performant for inference."
  created_at: 2022-07-21 15:11:55+00:00
  edited: false
  hidden: false
  id: 62d97acb0f293ca5af5a97c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1e984948a9ec5af6e4c6cc4e003fe0f.svg
      fullname: donut32
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: donut32
      type: user
    createdAt: '2022-07-21T16:18:04.000Z'
    data:
      edited: true
      editors:
      - donut32
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1e984948a9ec5af6e4c6cc4e003fe0f.svg
          fullname: donut32
          isHf: false
          isPro: false
          name: donut32
          type: user
        html: '<p>Hi, I used Huggingface Accelerate und checked the device map. Everything
          is indeed placed on the GPUs.<br>With following device map <a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/1658420149843-5f7deaceb1a525442ff96f27.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1658420149843-5f7deaceb1a525442ff96f27.png"></a><br>the
          output is <a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1658420096675-5f7deaceb1a525442ff96f27.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1658420096675-5f7deaceb1a525442ff96f27.png"></a><br>This
          is looking good so far I would say. I follow the same code as mayank for
          the forward run. Maybe there is something offloaded on CPU which is not
          directly visible from the device map (maybe some activations are placed
          into CPU memory).</p>

          '
        raw: "Hi, I used Huggingface Accelerate und checked the device map. Everything\
          \ is indeed placed on the GPUs. \nWith following device map ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1658420149843-5f7deaceb1a525442ff96f27.png)\n\
          the output is ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1658420096675-5f7deaceb1a525442ff96f27.png)\n\
          This is looking good so far I would say. I follow the same code as mayank\
          \ for the forward run. Maybe there is something offloaded on CPU which is\
          \ not directly visible from the device map (maybe some activations are placed\
          \ into CPU memory)."
        updatedAt: '2022-07-21T16:27:44.744Z'
      numEdits: 2
      reactions: []
    id: 62d97c3c2010d671bca14bf6
    type: comment
  author: donut32
  content: "Hi, I used Huggingface Accelerate und checked the device map. Everything\
    \ is indeed placed on the GPUs. \nWith following device map ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1658420149843-5f7deaceb1a525442ff96f27.png)\n\
    the output is ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1658420096675-5f7deaceb1a525442ff96f27.png)\n\
    This is looking good so far I would say. I follow the same code as mayank for\
    \ the forward run. Maybe there is something offloaded on CPU which is not directly\
    \ visible from the device map (maybe some activations are placed into CPU memory)."
  created_at: 2022-07-21 15:18:04+00:00
  edited: true
  hidden: false
  id: 62d97c3c2010d671bca14bf6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-07-21T21:40:55.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Hi all!<br>I highly suspect that the model is offloaded on the CPU.
          I also have access to a 8x A100 80GB and works fine<br>Can you try with
          <code>dtype=torch.bfloat16</code> instead and let us know? Also could you
          try with <code>accelerate==0.10.0</code> ?<br>Thanks</p>

          '
        raw: 'Hi all!

          I highly suspect that the model is offloaded on the CPU. I also have access
          to a 8x A100 80GB and works fine

          Can you try with `dtype=torch.bfloat16` instead and let us know? Also could
          you try with `accelerate==0.10.0` ?

          Thanks'
        updatedAt: '2022-07-21T21:40:55.631Z'
      numEdits: 0
      reactions: []
    id: 62d9c7e728f71aa2a17f83c6
    type: comment
  author: ybelkada
  content: 'Hi all!

    I highly suspect that the model is offloaded on the CPU. I also have access to
    a 8x A100 80GB and works fine

    Can you try with `dtype=torch.bfloat16` instead and let us know? Also could you
    try with `accelerate==0.10.0` ?

    Thanks'
  created_at: 2022-07-21 20:40:55+00:00
  edited: false
  hidden: false
  id: 62d9c7e728f71aa2a17f83c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2022-07-21T23:27:41.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: '<p>on 8x48, case the offloading is probably done to CPU so, that is
          expected to be slow. Anyways, I changed some things in my code and 8x A100
          80GB is working fine now.<br>Look at the <a rel="nofollow" href="https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/bloom-inference">https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/bloom-inference</a>
          branch in the parent repo.</p>

          '
        raw: 'on 8x48, case the offloading is probably done to CPU so, that is expected
          to be slow. Anyways, I changed some things in my code and 8x A100 80GB is
          working fine now.

          Look at the https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/bloom-inference
          branch in the parent repo.'
        updatedAt: '2022-07-21T23:27:41.086Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - pohunghuang
        - ybelkada
        - victor
        - pai4451
    id: 62d9e0edc43067695ddb2342
    type: comment
  author: mayank31398
  content: 'on 8x48, case the offloading is probably done to CPU so, that is expected
    to be slow. Anyways, I changed some things in my code and 8x A100 80GB is working
    fine now.

    Look at the https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/bloom-inference
    branch in the parent repo.'
  created_at: 2022-07-21 22:27:41+00:00
  edited: false
  hidden: false
  id: 62d9e0edc43067695ddb2342
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-07-22T07:23:33.000Z'
    data:
      edited: false
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: '<blockquote>

          <p>on 8x48, case the offloading is probably done to CPU so, that is expected
          to be slow. Anyways, I changed some things in my code and 8x A100 80GB is
          working fine now.<br>Look at the <a rel="nofollow" href="https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/bloom-inference">https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/bloom-inference</a>
          branch in the parent repo.</p>

          </blockquote>

          <p>Could I know what things you changed to make it working fine? Apply Megatron-DeepSpeed?
          or just doing some configuration to make sure all model parameters and activations
          not offload to CPU?</p>

          '
        raw: '> on 8x48, case the offloading is probably done to CPU so, that is expected
          to be slow. Anyways, I changed some things in my code and 8x A100 80GB is
          working fine now.

          > Look at the https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/bloom-inference
          branch in the parent repo.


          Could I know what things you changed to make it working fine? Apply Megatron-DeepSpeed?
          or just doing some configuration to make sure all model parameters and activations
          not offload to CPU?'
        updatedAt: '2022-07-22T07:23:33.054Z'
      numEdits: 0
      reactions: []
    id: 62da507564a4121e65f40f10
    type: comment
  author: pohunghuang
  content: '> on 8x48, case the offloading is probably done to CPU so, that is expected
    to be slow. Anyways, I changed some things in my code and 8x A100 80GB is working
    fine now.

    > Look at the https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/bloom-inference
    branch in the parent repo.


    Could I know what things you changed to make it working fine? Apply Megatron-DeepSpeed?
    or just doing some configuration to make sure all model parameters and activations
    not offload to CPU?'
  created_at: 2022-07-22 06:23:33+00:00
  edited: false
  hidden: false
  id: 62da507564a4121e65f40f10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2022-07-22T13:29:27.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;pohunghuang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pohunghuang\"\
          >@<span class=\"underline\">pohunghuang</span></a></span>\n\n\t</span></span>\
          \ , the max memory method in the following file did the trick:<br><a rel=\"\
          nofollow\" href=\"https://github.com/bigscience-workshop/Megatron-DeepSpeed/blob/fd26b9c4650c74e4159c7aed60a282176f87ac7f/scripts/inference/bloom-accelerate-inference.py\"\
          >https://github.com/bigscience-workshop/Megatron-DeepSpeed/blob/fd26b9c4650c74e4159c7aed60a282176f87ac7f/scripts/inference/bloom-accelerate-inference.py</a></p>\n\
          <p>I would recommend you to write your code by editing this file.<br>Its\
          \ considerably faster.<br>Also, make sure you have 8x A100 80GBs</p>\n"
        raw: 'Hi @pohunghuang , the max memory method in the following file did the
          trick:

          https://github.com/bigscience-workshop/Megatron-DeepSpeed/blob/fd26b9c4650c74e4159c7aed60a282176f87ac7f/scripts/inference/bloom-accelerate-inference.py


          I would recommend you to write your code by editing this file.

          Its considerably faster.

          Also, make sure you have 8x A100 80GBs'
        updatedAt: '2022-07-22T13:29:27.996Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pohunghuang
    id: 62daa6375575019463cc6396
    type: comment
  author: mayank31398
  content: 'Hi @pohunghuang , the max memory method in the following file did the
    trick:

    https://github.com/bigscience-workshop/Megatron-DeepSpeed/blob/fd26b9c4650c74e4159c7aed60a282176f87ac7f/scripts/inference/bloom-accelerate-inference.py


    I would recommend you to write your code by editing this file.

    Its considerably faster.

    Also, make sure you have 8x A100 80GBs'
  created_at: 2022-07-22 12:29:27+00:00
  edited: false
  hidden: false
  id: 62daa6375575019463cc6396
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
      fullname: Paul-PH Huang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pohunghuang
      type: user
    createdAt: '2022-07-25T00:44:41.000Z'
    data:
      edited: false
      editors:
      - pohunghuang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/661fbfb6c79ae209f0afcfebd60deb2e.svg
          fullname: Paul-PH Huang
          isHf: false
          isPro: false
          name: pohunghuang
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;mayank31398&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mayank31398\"\
          >@<span class=\"underline\">mayank31398</span></a></span>\n\n\t</span></span>\
          \ , it's really good and simple solution. Unfortunately we have only 2 x\
          \ (8 x A6000 48GBs), so multi-node distribution is only way we could take.</p>\n"
        raw: Thanks @mayank31398 , it's really good and simple solution. Unfortunately
          we have only 2 x (8 x A6000 48GBs), so multi-node distribution is only way
          we could take.
        updatedAt: '2022-07-25T00:44:41.315Z'
      numEdits: 0
      reactions: []
    id: 62dde779cccf06af63feff86
    type: comment
  author: pohunghuang
  content: Thanks @mayank31398 , it's really good and simple solution. Unfortunately
    we have only 2 x (8 x A6000 48GBs), so multi-node distribution is only way we
    could take.
  created_at: 2022-07-24 23:44:41+00:00
  edited: false
  hidden: false
  id: 62dde779cccf06af63feff86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-08-11T06:07:37.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;pohunghuang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pohunghuang\"\
          >@<span class=\"underline\">pohunghuang</span></a></span>\n\n\t</span></span>\
          \ and all!<br>We recently released a beta bitsandbytes integration with\
          \ HuggingFace that would work well for large language models: <a rel=\"\
          nofollow\" href=\"https://twitter.com/Tim_Dettmers/status/1557343499225219072\"\
          >https://twitter.com/Tim_Dettmers/status/1557343499225219072</a> I think\
          \ it would give decent inference speed (at least as fast as the native model)\
          \ and would work well on A6000 with 2x memory footprint gain. You would\
          \ just need to install the latest version of accelerate, transformers and\
          \ bitsandbytes (see precise instructions on the google docs that has been\
          \ shared on the tweet).<br>The release is still in beta so I would love\
          \ to hear from you if this is working on your hardware or not!<br>Thank\
          \ you :-) !</p>\n"
        raw: "Hi @pohunghuang and all!\nWe recently released a beta bitsandbytes integration\
          \ with HuggingFace that would work well for large language models: https://twitter.com/Tim_Dettmers/status/1557343499225219072\
          \ I think it would give decent inference speed (at least as fast as the\
          \ native model) and would work well on A6000 with 2x memory footprint gain.\
          \ You would just need to install the latest version of accelerate, transformers\
          \ and bitsandbytes (see precise instructions on the google docs that has\
          \ been shared on the tweet). \nThe release is still in beta so I would love\
          \ to hear from you if this is working on your hardware or not! \nThank you\
          \ :-) !"
        updatedAt: '2022-08-11T06:07:37.197Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - justheuristic
        - julien-c
        - pai4451
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - justheuristic
        - julien-c
    id: 62f49ca9aa46e53e058b3791
    type: comment
  author: ybelkada
  content: "Hi @pohunghuang and all!\nWe recently released a beta bitsandbytes integration\
    \ with HuggingFace that would work well for large language models: https://twitter.com/Tim_Dettmers/status/1557343499225219072\
    \ I think it would give decent inference speed (at least as fast as the native\
    \ model) and would work well on A6000 with 2x memory footprint gain. You would\
    \ just need to install the latest version of accelerate, transformers and bitsandbytes\
    \ (see precise instructions on the google docs that has been shared on the tweet).\
    \ \nThe release is still in beta so I would love to hear from you if this is working\
    \ on your hardware or not! \nThank you :-) !"
  created_at: 2022-08-11 05:07:37+00:00
  edited: false
  hidden: false
  id: 62f49ca9aa46e53e058b3791
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
      fullname: Pai Yao Wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pai4451
      type: user
    createdAt: '2022-08-13T03:32:45.000Z'
    data:
      edited: true
      editors:
      - pai4451
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a252dbedeb2256ba4264d6406c9ed6.svg
          fullname: Pai Yao Wei
          isHf: false
          isPro: false
          name: pai4451
          type: user
        html: "<blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;pohunghuang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pohunghuang\"\
          >@<span class=\"underline\">pohunghuang</span></a></span>\n\n\t</span></span>\
          \ and all!<br>We recently released a beta bitsandbytes integration with\
          \ HuggingFace that would work well for large language models: <a rel=\"\
          nofollow\" href=\"https://twitter.com/Tim_Dettmers/status/1557343499225219072\"\
          >https://twitter.com/Tim_Dettmers/status/1557343499225219072</a> I think\
          \ it would give decent inference speed (at least as fast as the native model)\
          \ and would work well on A6000 with 2x memory footprint gain. You would\
          \ just need to install the latest version of accelerate, transformers and\
          \ bitsandbytes (see precise instructions on the google docs that has been\
          \ shared on the tweet).<br>The release is still in beta so I would love\
          \ to hear from you if this is working on your hardware or not!<br>Thank\
          \ you :-) !</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ I want to try bitsandbytes on an 8x A6000 server (CUDA version 11.3) with\
          \ BLOOM. Unfortunately, the following error throws out.</p>\n<pre><code>RuntimeError:\
          \ Creating a Parameter from an instance of type Int8Params requires that\
          \ detach() returns an instance of the same type, but return type Tensor.\
          \ was found instead. To use the type as a Parameter, please correct the\
          \ detach() semantics defined by __torch_dispatch__() implementation.\n</code></pre>\n\
          <p>I use the code from (<a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4\"\
          >https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4</a>)\
          \ using the <code>model.generate()</code> way from HuggingFace. Do you know\
          \ how to solve the issue? I installed <code>bitsandbytes==0.31.8</code>\
          \ from <a rel=\"nofollow\" href=\"https://pypi.org/project/bitsandbytes/\"\
          >https://pypi.org/project/bitsandbytes/</a>; the latest <code>transformers</code>\
          \ package from the master branch also installed the latest <code>Accelerate</code>\
          \ from pip.</p>\n"
        raw: "> Hi @pohunghuang and all!\n> We recently released a beta bitsandbytes\
          \ integration with HuggingFace that would work well for large language models:\
          \ https://twitter.com/Tim_Dettmers/status/1557343499225219072 I think it\
          \ would give decent inference speed (at least as fast as the native model)\
          \ and would work well on A6000 with 2x memory footprint gain. You would\
          \ just need to install the latest version of accelerate, transformers and\
          \ bitsandbytes (see precise instructions on the google docs that has been\
          \ shared on the tweet). \n> The release is still in beta so I would love\
          \ to hear from you if this is working on your hardware or not! \n> Thank\
          \ you :-) !\n\n@ybelkada I want to try bitsandbytes on an 8x A6000 server\
          \ (CUDA version 11.3) with BLOOM. Unfortunately, the following error throws\
          \ out.\n\n```\nRuntimeError: Creating a Parameter from an instance of type\
          \ Int8Params requires that detach() returns an instance of the same type,\
          \ but return type Tensor. was found instead. To use the type as a Parameter,\
          \ please correct the detach() semantics defined by __torch_dispatch__()\
          \ implementation.\n```\n\nI use the code from (https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4)\
          \ using the `model.generate()` way from HuggingFace. Do you know how to\
          \ solve the issue? I installed `bitsandbytes==0.31.8` from https://pypi.org/project/bitsandbytes/;\
          \ the latest `transformers` package from the master branch also installed\
          \ the latest `Accelerate` from pip."
        updatedAt: '2022-08-13T03:51:43.760Z'
      numEdits: 5
      reactions: []
    id: 62f71b5dd3bdacb7eec43cf4
    type: comment
  author: pai4451
  content: "> Hi @pohunghuang and all!\n> We recently released a beta bitsandbytes\
    \ integration with HuggingFace that would work well for large language models:\
    \ https://twitter.com/Tim_Dettmers/status/1557343499225219072 I think it would\
    \ give decent inference speed (at least as fast as the native model) and would\
    \ work well on A6000 with 2x memory footprint gain. You would just need to install\
    \ the latest version of accelerate, transformers and bitsandbytes (see precise\
    \ instructions on the google docs that has been shared on the tweet). \n> The\
    \ release is still in beta so I would love to hear from you if this is working\
    \ on your hardware or not! \n> Thank you :-) !\n\n@ybelkada I want to try bitsandbytes\
    \ on an 8x A6000 server (CUDA version 11.3) with BLOOM. Unfortunately, the following\
    \ error throws out.\n\n```\nRuntimeError: Creating a Parameter from an instance\
    \ of type Int8Params requires that detach() returns an instance of the same type,\
    \ but return type Tensor. was found instead. To use the type as a Parameter, please\
    \ correct the detach() semantics defined by __torch_dispatch__() implementation.\n\
    ```\n\nI use the code from (https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4)\
    \ using the `model.generate()` way from HuggingFace. Do you know how to solve\
    \ the issue? I installed `bitsandbytes==0.31.8` from https://pypi.org/project/bitsandbytes/;\
    \ the latest `transformers` package from the master branch also installed the\
    \ latest `Accelerate` from pip."
  created_at: 2022-08-13 02:32:45+00:00
  edited: true
  hidden: false
  id: 62f71b5dd3bdacb7eec43cf4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2022-08-13T06:52:00.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;pai4451&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pai4451\">@<span class=\"\
          underline\">pai4451</span></a></span>\n\n\t</span></span> !<br>Thanks a\
          \ lot for the feedback!<br>I replied to you on the thread at <a rel=\"nofollow\"\
          \ href=\"https://github.com/huggingface/transformers/pull/17901#issuecomment-1213855206\"\
          >https://github.com/huggingface/transformers/pull/17901#issuecomment-1213855206</a>\
          \ and I think that this fix should work<br>Thanks!</p>\n"
        raw: "Hi @pai4451 !\nThanks a lot for the feedback! \nI replied to you on\
          \ the thread at https://github.com/huggingface/transformers/pull/17901#issuecomment-1213855206\
          \ and I think that this fix should work\nThanks!"
        updatedAt: '2022-08-13T06:52:00.744Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pai4451
    id: 62f74a105ad18b9403c737b6
    type: comment
  author: ybelkada
  content: "Hi @pai4451 !\nThanks a lot for the feedback! \nI replied to you on the\
    \ thread at https://github.com/huggingface/transformers/pull/17901#issuecomment-1213855206\
    \ and I think that this fix should work\nThanks!"
  created_at: 2022-08-13 05:52:00+00:00
  edited: false
  hidden: false
  id: 62f74a105ad18b9403c737b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661270230577-6303aac4a362e7e8b51a8e9f.jpeg?w=200&h=200&f=face
      fullname: Alexander Ljungberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sleven
      type: user
    createdAt: '2022-08-23T15:56:02.000Z'
    data:
      edited: false
      editors:
      - sleven
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661270230577-6303aac4a362e7e8b51a8e9f.jpeg?w=200&h=200&f=face
          fullname: Alexander Ljungberg
          isHf: false
          isPro: false
          name: sleven
          type: user
        html: '<p>I tested BLOOM with 16x A100 40 GB. That''s 640 GB of GPU RAM which
          should very comfortably fit the whole model. However, with a naive setup,
          generating 30 tokens took nearly 4 minutes.</p>

          <p>In this case, I''d expect <code>max_memory</code> to make no difference
          since there''s so much free VRAM that any reasonable defaults should be
          OK. However, using the <code>max_memory</code> calculator linked above took
          the generation time for the same example down to 7 seconds. (I don''t know
          why though. Without it, about 20231MiB are used per device, with it 24439MiB,
          but the last two devices are not filled.)</p>

          <p>This still feels a little slow on a 16 GPU machine but at least it''s
          measured in seconds rather than minutes.</p>

          '
        raw: 'I tested BLOOM with 16x A100 40 GB. That''s 640 GB of GPU RAM which
          should very comfortably fit the whole model. However, with a naive setup,
          generating 30 tokens took nearly 4 minutes.


          In this case, I''d expect `max_memory` to make no difference since there''s
          so much free VRAM that any reasonable defaults should be OK. However, using
          the `max_memory` calculator linked above took the generation time for the
          same example down to 7 seconds. (I don''t know why though. Without it, about
          20231MiB are used per device, with it 24439MiB, but the last two devices
          are not filled.)


          This still feels a little slow on a 16 GPU machine but at least it''s measured
          in seconds rather than minutes.'
        updatedAt: '2022-08-23T15:56:02.486Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hemangjoshi37a
    id: 6304f892ce6b12280b1d385b
    type: comment
  author: sleven
  content: 'I tested BLOOM with 16x A100 40 GB. That''s 640 GB of GPU RAM which should
    very comfortably fit the whole model. However, with a naive setup, generating
    30 tokens took nearly 4 minutes.


    In this case, I''d expect `max_memory` to make no difference since there''s so
    much free VRAM that any reasonable defaults should be OK. However, using the `max_memory`
    calculator linked above took the generation time for the same example down to
    7 seconds. (I don''t know why though. Without it, about 20231MiB are used per
    device, with it 24439MiB, but the last two devices are not filled.)


    This still feels a little slow on a 16 GPU machine but at least it''s measured
    in seconds rather than minutes.'
  created_at: 2022-08-23 14:56:02+00:00
  edited: false
  hidden: false
  id: 6304f892ce6b12280b1d385b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664607238774-60910a419a8bcaa437b234a6.jpeg?w=200&h=200&f=face
      fullname: Hemang Joshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hemangjoshi37a
      type: user
    createdAt: '2022-12-14T07:07:42.000Z'
    data:
      edited: false
      editors:
      - hemangjoshi37a
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664607238774-60910a419a8bcaa437b234a6.jpeg?w=200&h=200&f=face
          fullname: Hemang Joshi
          isHf: false
          isPro: false
          name: hemangjoshi37a
          type: user
        html: '<p>I just wanted to purchase new system/PC for high end large language
          model inferencing.  For what specification should I go?</p>

          '
        raw: I just wanted to purchase new system/PC for high end large language model
          inferencing.  For what specification should I go?
        updatedAt: '2022-12-14T07:07:42.682Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hemangjoshi37a
    id: 6399763ee995a103e3cf4abf
    type: comment
  author: hemangjoshi37a
  content: I just wanted to purchase new system/PC for high end large language model
    inferencing.  For what specification should I go?
  created_at: 2022-12-14 07:07:42+00:00
  edited: false
  hidden: false
  id: 6399763ee995a103e3cf4abf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 59
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Inference on BLOOM 165B is too slow
