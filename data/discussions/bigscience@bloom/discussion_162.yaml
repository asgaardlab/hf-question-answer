!!python/object:huggingface_hub.community.DiscussionWithDetails
author: boomer22
conflicting_files: null
created_at: 2022-12-31 00:41:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4240cc3c73250987c3c9e399e314380a.svg
      fullname: Vince Boomer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: boomer22
      type: user
    createdAt: '2022-12-31T00:41:29.000Z'
    data:
      edited: false
      editors:
      - boomer22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4240cc3c73250987c3c9e399e314380a.svg
          fullname: Vince Boomer
          isHf: false
          isPro: false
          name: boomer22
          type: user
        html: '<p>Hi,</p>

          <p>I have a set of domain-specific texts I''d like to train Bloom on for
          querying purposes. This is purely out of curiosity for Bloom.</p>

          <p>I am a developer with decades of experience so I''m not afraid to get
          my hands dirty, I just want some information on how/if I can do this and
          where to find more information. If possible, are there alternative systems
          you can recommend?</p>

          <p>Thanks in advance.</p>

          '
        raw: "Hi,\r\n\r\nI have a set of domain-specific texts I'd like to train Bloom\
          \ on for querying purposes. This is purely out of curiosity for Bloom.\r\
          \n\r\nI am a developer with decades of experience so I'm not afraid to get\
          \ my hands dirty, I just want some information on how/if I can do this and\
          \ where to find more information. If possible, are there alternative systems\
          \ you can recommend?\r\n\r\nThanks in advance."
        updatedAt: '2022-12-31T00:41:29.236Z'
      numEdits: 0
      reactions:
      - count: 11
        reaction: "\u2764\uFE0F"
        users:
        - jameshuntercarter
        - BenTeo
        - Priyankavanitha21
        - boomer22
        - nicolasmarb
        - Devk
        - Christ0pher
        - SmokeAndAsh
        - bhperform
        - NigelTheMaker
        - JDE65
    id: 63af8539b686a98ab828bbaa
    type: comment
  author: boomer22
  content: "Hi,\r\n\r\nI have a set of domain-specific texts I'd like to train Bloom\
    \ on for querying purposes. This is purely out of curiosity for Bloom.\r\n\r\n\
    I am a developer with decades of experience so I'm not afraid to get my hands\
    \ dirty, I just want some information on how/if I can do this and where to find\
    \ more information. If possible, are there alternative systems you can recommend?\r\
    \n\r\nThanks in advance."
  created_at: 2022-12-31 00:41:29+00:00
  edited: false
  hidden: false
  id: 63af8539b686a98ab828bbaa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3a07c42e7a5952295457f2108b9fd729.svg
      fullname: Bob Hannent
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bhperform
      type: user
    createdAt: '2023-01-09T10:40:22.000Z'
    data:
      edited: false
      editors:
      - bhperform
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3a07c42e7a5952295457f2108b9fd729.svg
          fullname: Bob Hannent
          isHf: false
          isPro: false
          name: bhperform
          type: user
        html: "<p>I have been looking to try and understand this as well, I'd like\
          \ to add knowledge to the corpus but I can't see a way that this can be\
          \ done.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;yongzx&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yongzx\"\
          >@<span class=\"underline\">yongzx</span></a></span>\n\n\t</span></span>\
          \ can you offer any advice? Or can the corpus only be expanded through the\
          \ original process?</p>\n"
        raw: 'I have been looking to try and understand this as well, I''d like to
          add knowledge to the corpus but I can''t see a way that this can be done.


          @yongzx can you offer any advice? Or can the corpus only be expanded through
          the original process?'
        updatedAt: '2023-01-09T10:40:22.234Z'
      numEdits: 0
      reactions: []
    id: 63bbef163c229a497ba62f0a
    type: comment
  author: bhperform
  content: 'I have been looking to try and understand this as well, I''d like to add
    knowledge to the corpus but I can''t see a way that this can be done.


    @yongzx can you offer any advice? Or can the corpus only be expanded through the
    original process?'
  created_at: 2023-01-09 10:40:22+00:00
  edited: false
  hidden: false
  id: 63bbef163c229a497ba62f0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7425d978d3c64972b01cfe9e060e3f43.svg
      fullname: Yong Zheng Xin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yongzx
      type: user
    createdAt: '2023-01-10T03:27:44.000Z'
    data:
      edited: false
      editors:
      - yongzx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7425d978d3c64972b01cfe9e060e3f43.svg
          fullname: Yong Zheng Xin
          isHf: false
          isPro: false
          name: yongzx
          type: user
        html: '<p>I think it really depends on the type of knowledge you want to add
          to the training corpus. </p>

          <p>What we''ve tried on is to add support to another language  (<a rel="nofollow"
          href="https://arxiv.org/abs/2212.09535">https://arxiv.org/abs/2212.09535</a>),
          where the new corpus is text in another language. We find that, given enough
          text, we can simply train on the new corpus with next word prediction objective
          (as in BLOOM pretraining). However, for bigger models exceeding 1.7B parameters,
          instead of finetuning the entire model, we recommend training only the adapters.
          </p>

          <p>Currently, we are still exploring how to best combine the new corpus
          and the original ROOT corpus.</p>

          '
        raw: "I think it really depends on the type of knowledge you want to add to\
          \ the training corpus. \n\nWhat we've tried on is to add support to another\
          \ language  (https://arxiv.org/abs/2212.09535), where the new corpus is\
          \ text in another language. We find that, given enough text, we can simply\
          \ train on the new corpus with next word prediction objective (as in BLOOM\
          \ pretraining). However, for bigger models exceeding 1.7B parameters, instead\
          \ of finetuning the entire model, we recommend training only the adapters.\
          \ \n\nCurrently, we are still exploring how to best combine the new corpus\
          \ and the original ROOT corpus."
        updatedAt: '2023-01-10T03:27:44.048Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - xin1111
    id: 63bcdb300718d5f0c1f76ef4
    type: comment
  author: yongzx
  content: "I think it really depends on the type of knowledge you want to add to\
    \ the training corpus. \n\nWhat we've tried on is to add support to another language\
    \  (https://arxiv.org/abs/2212.09535), where the new corpus is text in another\
    \ language. We find that, given enough text, we can simply train on the new corpus\
    \ with next word prediction objective (as in BLOOM pretraining). However, for\
    \ bigger models exceeding 1.7B parameters, instead of finetuning the entire model,\
    \ we recommend training only the adapters. \n\nCurrently, we are still exploring\
    \ how to best combine the new corpus and the original ROOT corpus."
  created_at: 2023-01-10 03:27:44+00:00
  edited: false
  hidden: false
  id: 63bcdb300718d5f0c1f76ef4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673670043032-62129484581b98bb1ad00a60.jpeg?w=200&h=200&f=face
      fullname: Nigel The Maker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NigelTheMaker
      type: user
    createdAt: '2023-01-14T05:03:22.000Z'
    data:
      edited: false
      editors:
      - NigelTheMaker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673670043032-62129484581b98bb1ad00a60.jpeg?w=200&h=200&f=face
          fullname: Nigel The Maker
          isHf: false
          isPro: false
          name: NigelTheMaker
          type: user
        html: "<p>Hi, I have been doing exactly what you are talking about with my\
          \ open source mobile text editor called Maker+ Ci. I have built a AI chat\
          \ interface on top of Maker+ Ci called chatLink which uses files stored\
          \ in M+ to set context/pre-prompts for the bloom model which is the LLM\
          \ that chatLink is using. So in chatLink the user can easily select the\
          \ file that they want to set as context, which tells bloom how to behave\
          \ when a prompt is submitted in chatLink. I have created lots of cool prompts\
          \ for bloom, including using it as a terminal, mental health chatbot, Ai\
          \ personal assistant, chatbots with different personalities such as the\
          \ intj personality and the infp personality and the most recent one which\
          \ is a qna model built from prompts and no fine tuning. You can jump between\
          \ all these different contexts in chatLink easily. I\u2019m happy to share\
          \ all these different prompts including teaching how to use Maker+ Ci and\
          \ chatLink which are both open source. If you are interested please join\
          \ my discord server <a rel=\"nofollow\" href=\"https://discord.gg/47pXk7CY\"\
          >https://discord.gg/47pXk7CY</a> and feel free to come and ask any questions\
          \ you like about how to work with bloom in your own projects or how work\
          \ with bloom in Maker+ Ci and chatLink there.</p>\n"
        raw: "Hi, I have been doing exactly what you are talking about with my open\
          \ source mobile text editor called Maker+ Ci. I have built a AI chat interface\
          \ on top of Maker+ Ci called chatLink which uses files stored in M+ to set\
          \ context/pre-prompts for the bloom model which is the LLM that chatLink\
          \ is using. So in chatLink the user can easily select the file that they\
          \ want to set as context, which tells bloom how to behave when a prompt\
          \ is submitted in chatLink. I have created lots of cool prompts for bloom,\
          \ including using it as a terminal, mental health chatbot, Ai personal assistant,\
          \ chatbots with different personalities such as the intj personality and\
          \ the infp personality and the most recent one which is a qna model built\
          \ from prompts and no fine tuning. You can jump between all these different\
          \ contexts in chatLink easily. I\u2019m happy to share all these different\
          \ prompts including teaching how to use Maker+ Ci and chatLink which are\
          \ both open source. If you are interested please join my discord server\
          \ https://discord.gg/47pXk7CY and feel free to come and ask any questions\
          \ you like about how to work with bloom in your own projects or how work\
          \ with bloom in Maker+ Ci and chatLink there."
        updatedAt: '2023-01-14T05:03:22.310Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - NigelTheMaker
    id: 63c2379af9453420b5df19c9
    type: comment
  author: NigelTheMaker
  content: "Hi, I have been doing exactly what you are talking about with my open\
    \ source mobile text editor called Maker+ Ci. I have built a AI chat interface\
    \ on top of Maker+ Ci called chatLink which uses files stored in M+ to set context/pre-prompts\
    \ for the bloom model which is the LLM that chatLink is using. So in chatLink\
    \ the user can easily select the file that they want to set as context, which\
    \ tells bloom how to behave when a prompt is submitted in chatLink. I have created\
    \ lots of cool prompts for bloom, including using it as a terminal, mental health\
    \ chatbot, Ai personal assistant, chatbots with different personalities such as\
    \ the intj personality and the infp personality and the most recent one which\
    \ is a qna model built from prompts and no fine tuning. You can jump between all\
    \ these different contexts in chatLink easily. I\u2019m happy to share all these\
    \ different prompts including teaching how to use Maker+ Ci and chatLink which\
    \ are both open source. If you are interested please join my discord server https://discord.gg/47pXk7CY\
    \ and feel free to come and ask any questions you like about how to work with\
    \ bloom in your own projects or how work with bloom in Maker+ Ci and chatLink\
    \ there."
  created_at: 2023-01-14 05:03:22+00:00
  edited: false
  hidden: false
  id: 63c2379af9453420b5df19c9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 162
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: How can I train Bloom on a specific set of texts?
