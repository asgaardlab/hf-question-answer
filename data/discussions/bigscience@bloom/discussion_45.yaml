!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NonStatic
conflicting_files: null
created_at: 2022-07-14 04:48:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643913667641-noauth.jpeg?w=200&h=200&f=face
      fullname: Non Static
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NonStatic
      type: user
    createdAt: '2022-07-14T05:48:47.000Z'
    data:
      edited: false
      editors:
      - NonStatic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643913667641-noauth.jpeg?w=200&h=200&f=face
          fullname: Non Static
          isHf: false
          isPro: false
          name: NonStatic
          type: user
        html: '<p>Hi guys, I''m very interested in run the 176B model locally. May
          I know how many A100-40GB GPUs are needed to host it? Thanks!</p>

          '
        raw: Hi guys, I'm very interested in run the 176B model locally. May I know
          how many A100-40GB GPUs are needed to host it? Thanks!
        updatedAt: '2022-07-14T05:48:47.903Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - willianz
        - rdecoupes
        - TheSecurityDev
    id: 62cfae3f180d2ba1cd06409b
    type: comment
  author: NonStatic
  content: Hi guys, I'm very interested in run the 176B model locally. May I know
    how many A100-40GB GPUs are needed to host it? Thanks!
  created_at: 2022-07-14 04:48:47+00:00
  edited: false
  hidden: false
  id: 62cfae3f180d2ba1cd06409b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
      fullname: Haris Jabbar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maveriq
      type: user
    createdAt: '2022-07-14T08:05:11.000Z'
    data:
      edited: false
      editors:
      - maveriq
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585493970035-noauth.jpeg?w=200&h=200&f=face
          fullname: Haris Jabbar
          isHf: false
          isPro: false
          name: maveriq
          type: user
        html: '<p>As a first order estimate, 176B parameters in half precision (16
          bits = 2 bytes) would need 352 GB RAM. But since some modules are 32-bit,
          it would be more. So about nine GPUs with 40-GB RAM, and it doesn''t take
          into account the input.</p>

          <p>However you can offload some parameters to CPU RAM, which means it will
          be slower but atleast you can run it.</p>

          '
        raw: 'As a first order estimate, 176B parameters in half precision (16 bits
          = 2 bytes) would need 352 GB RAM. But since some modules are 32-bit, it
          would be more. So about nine GPUs with 40-GB RAM, and it doesn''t take into
          account the input.


          However you can offload some parameters to CPU RAM, which means it will
          be slower but atleast you can run it.'
        updatedAt: '2022-07-14T08:05:11.109Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - NonStatic
        - ArthurZ
    id: 62cfce37473738061b3b909c
    type: comment
  author: maveriq
  content: 'As a first order estimate, 176B parameters in half precision (16 bits
    = 2 bytes) would need 352 GB RAM. But since some modules are 32-bit, it would
    be more. So about nine GPUs with 40-GB RAM, and it doesn''t take into account
    the input.


    However you can offload some parameters to CPU RAM, which means it will be slower
    but atleast you can run it.'
  created_at: 2022-07-14 07:05:11+00:00
  edited: false
  hidden: false
  id: 62cfce37473738061b3b909c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1622899554787-60593bdc87286eb598c9946f.jpeg?w=200&h=200&f=face
      fullname: "Jo\xE3o Paulo Reis Alvarenga"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joaoalvarenga
      type: user
    createdAt: '2022-07-14T14:12:01.000Z'
    data:
      edited: false
      editors:
      - joaoalvarenga
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1622899554787-60593bdc87286eb598c9946f.jpeg?w=200&h=200&f=face
          fullname: "Jo\xE3o Paulo Reis Alvarenga"
          isHf: false
          isPro: false
          name: joaoalvarenga
          type: user
        html: '<p>You can try a quantized version. Here''s a 8-bit version I created
          using LoRA: <a href="https://huggingface.co/joaoalvarenga/bloom-8bit">https://huggingface.co/joaoalvarenga/bloom-8bit</a></p>

          '
        raw: 'You can try a quantized version. Here''s a 8-bit version I created using
          LoRA: https://huggingface.co/joaoalvarenga/bloom-8bit'
        updatedAt: '2022-07-14T14:12:01.013Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - NonStatic
        - justheuristic
        - rorra
    id: 62d02431ad741b94f5c7a141
    type: comment
  author: joaoalvarenga
  content: 'You can try a quantized version. Here''s a 8-bit version I created using
    LoRA: https://huggingface.co/joaoalvarenga/bloom-8bit'
  created_at: 2022-07-14 13:12:01+00:00
  edited: false
  hidden: false
  id: 62d02431ad741b94f5c7a141
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
      fullname: Ian Beaver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IanBeaver
      type: user
    createdAt: '2022-07-14T19:00:35.000Z'
    data:
      edited: false
      editors:
      - IanBeaver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
          fullname: Ian Beaver
          isHf: false
          isPro: false
          name: IanBeaver
          type: user
        html: '<p>I have successfully loaded it on a single x2iezn.6xlarge instance
          in AWS but using only CPUs the model is very slow.  Text generation sampling
          for several sequences can take several minutes to return, but the full model
          is working and it is much cheaper for local evaluation than 9 GPUs!</p>

          '
        raw: I have successfully loaded it on a single x2iezn.6xlarge instance in
          AWS but using only CPUs the model is very slow.  Text generation sampling
          for several sequences can take several minutes to return, but the full model
          is working and it is much cheaper for local evaluation than 9 GPUs!
        updatedAt: '2022-07-14T19:00:35.047Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - ybelkada
        - victor
        - NonStatic
        - OElesin
        - LetterRip
    id: 62d067d3df819b6e93d2e6f9
    type: comment
  author: IanBeaver
  content: I have successfully loaded it on a single x2iezn.6xlarge instance in AWS
    but using only CPUs the model is very slow.  Text generation sampling for several
    sequences can take several minutes to return, but the full model is working and
    it is much cheaper for local evaluation than 9 GPUs!
  created_at: 2022-07-14 18:00:35+00:00
  edited: false
  hidden: false
  id: 62d067d3df819b6e93d2e6f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fffef2678319f9ff861bf5a7796400ec.svg
      fullname: Tallin Tallin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tallin
      type: user
    createdAt: '2022-07-16T07:54:58.000Z'
    data:
      edited: true
      editors:
      - Tallin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fffef2678319f9ff861bf5a7796400ec.svg
          fullname: Tallin Tallin
          isHf: false
          isPro: false
          name: Tallin
          type: user
        html: '<blockquote>

          <p>As a first order estimate, 176B parameters in half precision (16 bits
          = 2 bytes) would need 352 GB RAM. But since some modules are 32-bit, it
          would be more. So about nine GPUs with 40-GB RAM, and it doesn''t take into
          account the input.</p>

          </blockquote>

          <p>Thats a huge amount of cost right? I had a look at the <a rel="nofollow"
          href="https://aws.amazon.com/ec2/instance-types/">AWS instance types with
          GPU available</a> and of the G5 instance family, not even the biggest instance
          has ~400 GB GPU Memory.<br>Checking out the cost for the biggest G5 Instance
          <a rel="nofollow" href="https://aws.amazon.com/savingsplans/compute-pricing/">with
          the savings plan calculator</a> currently gives me $12.8319 per hour. </p>

          <p>So wanting to <strong>host that for one year is 12.8319*24*365 = $112407.44</strong>.
          </p>

          <p>Thats a huge amount of cost for hosting the <strong>inference endpoint</strong>,
          and I''m still only at 192GB GPU Memory, not sure if it would even work
          with only that instance. And I don''t want to retrain, but only generate
          text base on an input prompt.</p>

          <p>Do I have an error in my calculation and have misunderstood something?
          Or Is it really that expensive and difficult to host a full inference endpoint?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1657957213797-62ced33ea3a23014aca75ba5.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1657957213797-62ced33ea3a23014aca75ba5.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1657957299125-62ced33ea3a23014aca75ba5.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1657957299125-62ced33ea3a23014aca75ba5.png"></a></p>

          '
        raw: "> As a first order estimate, 176B parameters in half precision (16 bits\
          \ = 2 bytes) would need 352 GB RAM. But since some modules are 32-bit, it\
          \ would be more. So about nine GPUs with 40-GB RAM, and it doesn't take\
          \ into account the input.\n\nThats a huge amount of cost right? I had a\
          \ look at the [AWS instance types with GPU available](https://aws.amazon.com/ec2/instance-types/)\
          \ and of the G5 instance family, not even the biggest instance has ~400\
          \ GB GPU Memory.\nChecking out the cost for the biggest G5 Instance [with\
          \ the savings plan calculator](https://aws.amazon.com/savingsplans/compute-pricing/)\
          \ currently gives me $12.8319 per hour. \n\nSo wanting to **host that for\
          \ one year is 12.8319\\*24\\*365 = $112407.44**. \n\nThats a huge amount\
          \ of cost for hosting the **inference endpoint**, and I'm still only at\
          \ 192GB GPU Memory, not sure if it would even work with only that instance.\
          \ And I don't want to retrain, but only generate text base on an input prompt.\n\
          \nDo I have an error in my calculation and have misunderstood something?\
          \ Or Is it really that expensive and difficult to host a full inference\
          \ endpoint?\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1657957213797-62ced33ea3a23014aca75ba5.png)\n\
          \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1657957299125-62ced33ea3a23014aca75ba5.png)"
        updatedAt: '2022-07-16T07:59:38.837Z'
      numEdits: 1
      reactions: []
    id: 62d26ed2eaf3858ce24a85a5
    type: comment
  author: Tallin
  content: "> As a first order estimate, 176B parameters in half precision (16 bits\
    \ = 2 bytes) would need 352 GB RAM. But since some modules are 32-bit, it would\
    \ be more. So about nine GPUs with 40-GB RAM, and it doesn't take into account\
    \ the input.\n\nThats a huge amount of cost right? I had a look at the [AWS instance\
    \ types with GPU available](https://aws.amazon.com/ec2/instance-types/) and of\
    \ the G5 instance family, not even the biggest instance has ~400 GB GPU Memory.\n\
    Checking out the cost for the biggest G5 Instance [with the savings plan calculator](https://aws.amazon.com/savingsplans/compute-pricing/)\
    \ currently gives me $12.8319 per hour. \n\nSo wanting to **host that for one\
    \ year is 12.8319\\*24\\*365 = $112407.44**. \n\nThats a huge amount of cost for\
    \ hosting the **inference endpoint**, and I'm still only at 192GB GPU Memory,\
    \ not sure if it would even work with only that instance. And I don't want to\
    \ retrain, but only generate text base on an input prompt.\n\nDo I have an error\
    \ in my calculation and have misunderstood something? Or Is it really that expensive\
    \ and difficult to host a full inference endpoint?\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1657957213797-62ced33ea3a23014aca75ba5.png)\n\
    \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1657957299125-62ced33ea3a23014aca75ba5.png)"
  created_at: 2022-07-16 06:54:58+00:00
  edited: true
  hidden: false
  id: 62d26ed2eaf3858ce24a85a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7cf424de20692de7f5b7948785dba894.svg
      fullname: Chirag Singhal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chirag11
      type: user
    createdAt: '2022-07-17T15:53:07.000Z'
    data:
      edited: false
      editors:
      - chirag11
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7cf424de20692de7f5b7948785dba894.svg
          fullname: Chirag Singhal
          isHf: false
          isPro: false
          name: chirag11
          type: user
        html: '<blockquote>

          <blockquote>

          <p>As a first order estimate, 176B parameters in half precision (16 bits
          = 2 bytes) would need 352 GB RAM. But since some modules are 32-bit, it
          would be more. So about nine GPUs with 40-GB RAM, and it doesn''t take into
          account the input.</p>

          </blockquote>

          <p>Thats a huge amount of cost right? I had a look at the <a rel="nofollow"
          href="https://aws.amazon.com/ec2/instance-types/">AWS instance types with
          GPU available</a> and of the G5 instance family, not even the biggest instance
          has ~400 GB GPU Memory.<br>Checking out the cost for the biggest G5 Instance
          <a rel="nofollow" href="https://aws.amazon.com/savingsplans/compute-pricing/">with
          the savings plan calculator</a> currently gives me $12.8319 per hour. </p>

          <p>So wanting to <strong>host that for one year is 12.8319*24*365 = $112407.44</strong>.
          </p>

          <p>Thats a huge amount of cost for hosting the <strong>inference endpoint</strong>,
          and I''m still only at 192GB GPU Memory, not sure if it would even work
          with only that instance. And I don''t want to retrain, but only generate
          text base on an input prompt.</p>

          <p>Do I have an error in my calculation and have misunderstood something?
          Or Is it really that expensive and difficult to host a full inference endpoint?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1657957213797-62ced33ea3a23014aca75ba5.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1657957213797-62ced33ea3a23014aca75ba5.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1657957299125-62ced33ea3a23014aca75ba5.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1657957299125-62ced33ea3a23014aca75ba5.png"></a></p>

          </blockquote>

          <p>you don''t have any kind of error</p>

          '
        raw: "> > As a first order estimate, 176B parameters in half precision (16\
          \ bits = 2 bytes) would need 352 GB RAM. But since some modules are 32-bit,\
          \ it would be more. So about nine GPUs with 40-GB RAM, and it doesn't take\
          \ into account the input.\n> \n> Thats a huge amount of cost right? I had\
          \ a look at the [AWS instance types with GPU available](https://aws.amazon.com/ec2/instance-types/)\
          \ and of the G5 instance family, not even the biggest instance has ~400\
          \ GB GPU Memory.\n> Checking out the cost for the biggest G5 Instance [with\
          \ the savings plan calculator](https://aws.amazon.com/savingsplans/compute-pricing/)\
          \ currently gives me $12.8319 per hour. \n> \n> So wanting to **host that\
          \ for one year is 12.8319\\*24\\*365 = $112407.44**. \n> \n> Thats a huge\
          \ amount of cost for hosting the **inference endpoint**, and I'm still only\
          \ at 192GB GPU Memory, not sure if it would even work with only that instance.\
          \ And I don't want to retrain, but only generate text base on an input prompt.\n\
          > \n> Do I have an error in my calculation and have misunderstood something?\
          \ Or Is it really that expensive and difficult to host a full inference\
          \ endpoint?\n> \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1657957213797-62ced33ea3a23014aca75ba5.png)\n\
          > \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1657957299125-62ced33ea3a23014aca75ba5.png)\n\
          \nyou don't have any kind of error"
        updatedAt: '2022-07-17T15:53:07.612Z'
      numEdits: 0
      reactions: []
    id: 62d43063894e7fe42df3f0a7
    type: comment
  author: chirag11
  content: "> > As a first order estimate, 176B parameters in half precision (16 bits\
    \ = 2 bytes) would need 352 GB RAM. But since some modules are 32-bit, it would\
    \ be more. So about nine GPUs with 40-GB RAM, and it doesn't take into account\
    \ the input.\n> \n> Thats a huge amount of cost right? I had a look at the [AWS\
    \ instance types with GPU available](https://aws.amazon.com/ec2/instance-types/)\
    \ and of the G5 instance family, not even the biggest instance has ~400 GB GPU\
    \ Memory.\n> Checking out the cost for the biggest G5 Instance [with the savings\
    \ plan calculator](https://aws.amazon.com/savingsplans/compute-pricing/) currently\
    \ gives me $12.8319 per hour. \n> \n> So wanting to **host that for one year is\
    \ 12.8319\\*24\\*365 = $112407.44**. \n> \n> Thats a huge amount of cost for hosting\
    \ the **inference endpoint**, and I'm still only at 192GB GPU Memory, not sure\
    \ if it would even work with only that instance. And I don't want to retrain,\
    \ but only generate text base on an input prompt.\n> \n> Do I have an error in\
    \ my calculation and have misunderstood something? Or Is it really that expensive\
    \ and difficult to host a full inference endpoint?\n> \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1657957213797-62ced33ea3a23014aca75ba5.png)\n\
    > \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1657957299125-62ced33ea3a23014aca75ba5.png)\n\
    \nyou don't have any kind of error"
  created_at: 2022-07-17 14:53:07+00:00
  edited: false
  hidden: false
  id: 62d43063894e7fe42df3f0a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658242124804-62d6be40854bffffd678c626.png?w=200&h=200&f=face
      fullname: Denys
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Eldy
      type: user
    createdAt: '2022-07-19T14:48:07.000Z'
    data:
      edited: false
      editors:
      - Eldy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658242124804-62d6be40854bffffd678c626.png?w=200&h=200&f=face
          fullname: Denys
          isHf: false
          isPro: false
          name: Eldy
          type: user
        html: '<p>Look for Tesla M40 24gb, it''s cheap and support last CUDA drivers.</p>

          '
        raw: Look for Tesla M40 24gb, it's cheap and support last CUDA drivers.
        updatedAt: '2022-07-19T14:48:07.157Z'
      numEdits: 0
      reactions: []
    id: 62d6c4272bfde63354788814
    type: comment
  author: Eldy
  content: Look for Tesla M40 24gb, it's cheap and support last CUDA drivers.
  created_at: 2022-07-19 13:48:07+00:00
  edited: false
  hidden: false
  id: 62d6c4272bfde63354788814
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
      fullname: Ian Beaver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IanBeaver
      type: user
    createdAt: '2022-07-19T15:51:25.000Z'
    data:
      edited: false
      editors:
      - IanBeaver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
          fullname: Ian Beaver
          isHf: false
          isPro: false
          name: IanBeaver
          type: user
        html: '<p>For cloud hosting a p4d.24xlarge will fit most of it in GPU memory
          (320GB), a p4de.24xlarge will fit all of it in GPU memory (640GB), but your
          looking at $32 - $41 /hr.  Hosting these very large LMs for continuous use
          in a real time application is very cost prohibitive at this time unless
          you are selling a product using them where you can pass off the hosting
          cost onto customers, or you already own a bunch of expensive hardware.  Much
          cheaper to use the HF API unless you have data use restrictions that require
          controlling the complete environment.  In that case as I said above a x2iezn.6xlarge
          is the cheapest I have found to run it on AWS ($5/hr) but it is too slow
          on CPUs for real time applications.  Would work for offline/batch operations
          but throughput is very low as well so we have to run several instances.</p>

          '
        raw: For cloud hosting a p4d.24xlarge will fit most of it in GPU memory (320GB),
          a p4de.24xlarge will fit all of it in GPU memory (640GB), but your looking
          at $32 - $41 /hr.  Hosting these very large LMs for continuous use in a
          real time application is very cost prohibitive at this time unless you are
          selling a product using them where you can pass off the hosting cost onto
          customers, or you already own a bunch of expensive hardware.  Much cheaper
          to use the HF API unless you have data use restrictions that require controlling
          the complete environment.  In that case as I said above a x2iezn.6xlarge
          is the cheapest I have found to run it on AWS ($5/hr) but it is too slow
          on CPUs for real time applications.  Would work for offline/batch operations
          but throughput is very low as well so we have to run several instances.
        updatedAt: '2022-07-19T15:51:25.086Z'
      numEdits: 0
      reactions: []
    id: 62d6d2fd1c1bf309b848339c
    type: comment
  author: IanBeaver
  content: For cloud hosting a p4d.24xlarge will fit most of it in GPU memory (320GB),
    a p4de.24xlarge will fit all of it in GPU memory (640GB), but your looking at
    $32 - $41 /hr.  Hosting these very large LMs for continuous use in a real time
    application is very cost prohibitive at this time unless you are selling a product
    using them where you can pass off the hosting cost onto customers, or you already
    own a bunch of expensive hardware.  Much cheaper to use the HF API unless you
    have data use restrictions that require controlling the complete environment.  In
    that case as I said above a x2iezn.6xlarge is the cheapest I have found to run
    it on AWS ($5/hr) but it is too slow on CPUs for real time applications.  Would
    work for offline/batch operations but throughput is very low as well so we have
    to run several instances.
  created_at: 2022-07-19 14:51:25+00:00
  edited: false
  hidden: false
  id: 62d6d2fd1c1bf309b848339c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2022-07-20T12:34:55.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<blockquote>\n<p>Or Is it really that expensive [...] to host a full\
          \ inference endpoint</p>\n</blockquote>\n<p>Yes, in general, this will be\
          \ expensive. Those models are just very large \U0001F631</p>\n<p>Note however\
          \ that as <span data-props=\"{&quot;user&quot;:&quot;maveriq&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/maveriq\">@<span class=\"\
          underline\">maveriq</span></a></span>\n\n\t</span></span> says,</p>\n<blockquote>\n\
          <p>you can offload some parameters to CPU RAM, which means it will be slower\
          \ but atleast you can run it.</p>\n</blockquote>\n"
        raw: "> Or Is it really that expensive [...] to host a full inference endpoint\n\
          \nYes, in general, this will be expensive. Those models are just very large\
          \ \U0001F631\n\nNote however that as @maveriq says,\n\n> you can offload\
          \ some parameters to CPU RAM, which means it will be slower but atleast\
          \ you can run it."
        updatedAt: '2022-07-20T12:34:55.913Z'
      numEdits: 0
      reactions: []
    id: 62d7f66f102d144db4b40f7c
    type: comment
  author: julien-c
  content: "> Or Is it really that expensive [...] to host a full inference endpoint\n\
    \nYes, in general, this will be expensive. Those models are just very large \U0001F631\
    \n\nNote however that as @maveriq says,\n\n> you can offload some parameters to\
    \ CPU RAM, which means it will be slower but atleast you can run it."
  created_at: 2022-07-20 11:34:55+00:00
  edited: false
  hidden: false
  id: 62d7f66f102d144db4b40f7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef1d4387370689683a472a2fd000d0dd.svg
      fullname: dude
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jurassicpark
      type: user
    createdAt: '2022-07-20T16:42:25.000Z'
    data:
      edited: true
      editors:
      - jurassicpark
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef1d4387370689683a472a2fd000d0dd.svg
          fullname: dude
          isHf: false
          isPro: false
          name: jurassicpark
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Eldy&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Eldy\">@<span class=\"\
          underline\">Eldy</span></a></span>\n\n\t</span></span> Do you use tesla\
          \ m40s?<br>For 24gb you would need around 20 of them. </p>\n<p>What kind\
          \ of hardware do you run them on? Rack servers or perhaps a mining type\
          \ setup?</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;IanBeaver&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/IanBeaver\"\
          >@<span class=\"underline\">IanBeaver</span></a></span>\n\n\t</span></span><br>Have\
          \ you got it running on any of the gpu instances? I'm curious what the inference\
          \ times look like compared to cpu (a few minutes is painful for most use\
          \ cases).</p>\n"
        raw: "@Eldy Do you use tesla m40s? \nFor 24gb you would need around 20 of\
          \ them. \n\nWhat kind of hardware do you run them on? Rack servers or perhaps\
          \ a mining type setup?\n\n@IanBeaver \nHave you got it running on any of\
          \ the gpu instances? I'm curious what the inference times look like compared\
          \ to cpu (a few minutes is painful for most use cases)."
        updatedAt: '2022-07-20T16:50:29.570Z'
      numEdits: 1
      reactions: []
    id: 62d83071c60d1450a1e8209b
    type: comment
  author: jurassicpark
  content: "@Eldy Do you use tesla m40s? \nFor 24gb you would need around 20 of them.\
    \ \n\nWhat kind of hardware do you run them on? Rack servers or perhaps a mining\
    \ type setup?\n\n@IanBeaver \nHave you got it running on any of the gpu instances?\
    \ I'm curious what the inference times look like compared to cpu (a few minutes\
    \ is painful for most use cases)."
  created_at: 2022-07-20 15:42:25+00:00
  edited: true
  hidden: false
  id: 62d83071c60d1450a1e8209b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658242124804-62d6be40854bffffd678c626.png?w=200&h=200&f=face
      fullname: Denys
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Eldy
      type: user
    createdAt: '2022-07-20T17:05:46.000Z'
    data:
      edited: true
      editors:
      - Eldy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658242124804-62d6be40854bffffd678c626.png?w=200&h=200&f=face
          fullname: Denys
          isHf: false
          isPro: false
          name: Eldy
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;Eldy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Eldy\"\
          >@<span class=\"underline\">Eldy</span></a></span>\n\n\t</span></span> Do\
          \ you use tesla m40s?<br>For 24gb you would need around 20 of them. </p>\n\
          <p>What kind of hardware do you run them on? Rack servers or perhaps a mining\
          \ type setup?</p>\n</blockquote>\n<p>I plan to setup in Rack servers, but\
          \ did not decide yet.</p>\n<p>For OTP-175b enoght 16 x m40s. There are 2\
          \ instances (8 gc each) will work in Alpha parallelism <a rel=\"nofollow\"\
          \ href=\"https://alpa-projects.github.io/tutorials/opt_serving.html\">https://alpa-projects.github.io/tutorials/opt_serving.html</a></p>\n"
        raw: "> @Eldy Do you use tesla m40s? \n> For 24gb you would need around 20\
          \ of them. \n> \n> What kind of hardware do you run them on? Rack servers\
          \ or perhaps a mining type setup?\n> \n\nI plan to setup in Rack servers,\
          \ but did not decide yet.\n\nFor OTP-175b enoght 16 x m40s. There are 2\
          \ instances (8 gc each) will work in Alpha parallelism https://alpa-projects.github.io/tutorials/opt_serving.html"
        updatedAt: '2022-07-20T17:06:29.172Z'
      numEdits: 1
      reactions: []
    id: 62d835ea9d38820db260053a
    type: comment
  author: Eldy
  content: "> @Eldy Do you use tesla m40s? \n> For 24gb you would need around 20 of\
    \ them. \n> \n> What kind of hardware do you run them on? Rack servers or perhaps\
    \ a mining type setup?\n> \n\nI plan to setup in Rack servers, but did not decide\
    \ yet.\n\nFor OTP-175b enoght 16 x m40s. There are 2 instances (8 gc each) will\
    \ work in Alpha parallelism https://alpa-projects.github.io/tutorials/opt_serving.html"
  created_at: 2022-07-20 16:05:46+00:00
  edited: true
  hidden: false
  id: 62d835ea9d38820db260053a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
      fullname: Ian Beaver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IanBeaver
      type: user
    createdAt: '2022-07-21T16:15:54.000Z'
    data:
      edited: false
      editors:
      - IanBeaver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
          fullname: Ian Beaver
          isHf: false
          isPro: false
          name: IanBeaver
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;IanBeaver&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/IanBeaver\"\
          >@<span class=\"underline\">IanBeaver</span></a></span>\n\n\t</span></span><br>Have\
          \ you got it running on any of the gpu instances? I'm curious what the inference\
          \ times look like compared to cpu (a few minutes is painful for most use\
          \ cases).</p>\n</blockquote>\n<p>I have not tried yet, but you can see some\
          \ GPU times posted in <a href=\"https://huggingface.co/bigscience/bloom/discussions/59\"\
          >https://huggingface.co/bigscience/bloom/discussions/59</a>  however they\
          \ seem very slow so I am not sure if there is some issue with the environment\
          \ or configuration.</p>\n"
        raw: "> @IanBeaver \n> Have you got it running on any of the gpu instances?\
          \ I'm curious what the inference times look like compared to cpu (a few\
          \ minutes is painful for most use cases).\n\nI have not tried yet, but you\
          \ can see some GPU times posted in https://huggingface.co/bigscience/bloom/discussions/59\
          \  however they seem very slow so I am not sure if there is some issue with\
          \ the environment or configuration."
        updatedAt: '2022-07-21T16:15:54.880Z'
      numEdits: 0
      reactions: []
    id: 62d97bba810069dec2d3d8f8
    type: comment
  author: IanBeaver
  content: "> @IanBeaver \n> Have you got it running on any of the gpu instances?\
    \ I'm curious what the inference times look like compared to cpu (a few minutes\
    \ is painful for most use cases).\n\nI have not tried yet, but you can see some\
    \ GPU times posted in https://huggingface.co/bigscience/bloom/discussions/59 \
    \ however they seem very slow so I am not sure if there is some issue with the\
    \ environment or configuration."
  created_at: 2022-07-21 15:15:54+00:00
  edited: false
  hidden: false
  id: 62d97bba810069dec2d3d8f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eacbce0f8557002271bf92c0b557ba0b.svg
      fullname: Gregor Koch
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cronos3k
      type: user
    createdAt: '2022-09-13T17:21:27.000Z'
    data:
      edited: false
      editors:
      - cronos3k
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eacbce0f8557002271bf92c0b557ba0b.svg
          fullname: Gregor Koch
          isHf: false
          isPro: false
          name: cronos3k
          type: user
        html: '<p>I have an IBM x 3950 x5 server sitting around in my basement with
          512 GB of main memory and 80 CPU cores with an RTX Titan 24 GB in it. Do
          you think that one of the models will run that?</p>

          '
        raw: I have an IBM x 3950 x5 server sitting around in my basement with 512
          GB of main memory and 80 CPU cores with an RTX Titan 24 GB in it. Do you
          think that one of the models will run that?
        updatedAt: '2022-09-13T17:21:27.618Z'
      numEdits: 0
      reactions: []
    id: 6320bc17fbf4f954806e7841
    type: comment
  author: cronos3k
  content: I have an IBM x 3950 x5 server sitting around in my basement with 512 GB
    of main memory and 80 CPU cores with an RTX Titan 24 GB in it. Do you think that
    one of the models will run that?
  created_at: 2022-09-13 16:21:27+00:00
  edited: false
  hidden: false
  id: 6320bc17fbf4f954806e7841
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcc640f1508bcf1d07915d5ebaa0ab89.svg
      fullname: "Vinicius Guimar\xE3es"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viniciusguimaraes
      type: user
    createdAt: '2022-09-15T17:58:39.000Z'
    data:
      edited: false
      editors:
      - viniciusguimaraes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcc640f1508bcf1d07915d5ebaa0ab89.svg
          fullname: "Vinicius Guimar\xE3es"
          isHf: false
          isPro: false
          name: viniciusguimaraes
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;IanBeaver&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/IanBeaver\">@<span class=\"\
          underline\">IanBeaver</span></a></span>\n\n\t</span></span><br>How long\
          \ did it take to load the model? I tried to run Bloom-175B on a x2iezn.6xlarge\
          \ instance, but it seems to get stuck after loading most of the model (There\
          \ is almost no CPU usage after RAM reaches ~670GB of usage). I waited for\
          \ about 3 hours before giving up. Did you have this problem?</p>\n"
        raw: '@IanBeaver

          How long did it take to load the model? I tried to run Bloom-175B on a x2iezn.6xlarge
          instance, but it seems to get stuck after loading most of the model (There
          is almost no CPU usage after RAM reaches ~670GB of usage). I waited for
          about 3 hours before giving up. Did you have this problem?'
        updatedAt: '2022-09-15T17:58:39.317Z'
      numEdits: 0
      reactions: []
    id: 632367cf99c4839e1c5e2d90
    type: comment
  author: viniciusguimaraes
  content: '@IanBeaver

    How long did it take to load the model? I tried to run Bloom-175B on a x2iezn.6xlarge
    instance, but it seems to get stuck after loading most of the model (There is
    almost no CPU usage after RAM reaches ~670GB of usage). I waited for about 3 hours
    before giving up. Did you have this problem?'
  created_at: 2022-09-15 16:58:39+00:00
  edited: false
  hidden: false
  id: 632367cf99c4839e1c5e2d90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
      fullname: Ian Beaver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IanBeaver
      type: user
    createdAt: '2022-09-15T18:21:58.000Z'
    data:
      edited: false
      editors:
      - IanBeaver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
          fullname: Ian Beaver
          isHf: false
          isPro: false
          name: IanBeaver
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;viniciusguimaraes&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/viniciusguimaraes\"\
          >@<span class=\"underline\">viniciusguimaraes</span></a></span>\n\n\t</span></span>\
          \ I did not have that problem, but I was also not using the API to download\
          \ the model either.  I have had similar problems with the huggingface API\
          \ downloader with other large models like T0pp where I have seen it apparently\
          \ finish but the code never returns.  So anymore I always prefetch large\
          \ model files with git or curl and load them from a local path.  Loading\
          \ the model locally probably took 10 mins or more as I recall but I didn't\
          \ time it to know the actual load time.</p>\n"
        raw: '@viniciusguimaraes I did not have that problem, but I was also not using
          the API to download the model either.  I have had similar problems with
          the huggingface API downloader with other large models like T0pp where I
          have seen it apparently finish but the code never returns.  So anymore I
          always prefetch large model files with git or curl and load them from a
          local path.  Loading the model locally probably took 10 mins or more as
          I recall but I didn''t time it to know the actual load time.'
        updatedAt: '2022-09-15T18:21:58.531Z'
      numEdits: 0
      reactions: []
    id: 63236d46e007685e355b6f4d
    type: comment
  author: IanBeaver
  content: '@viniciusguimaraes I did not have that problem, but I was also not using
    the API to download the model either.  I have had similar problems with the huggingface
    API downloader with other large models like T0pp where I have seen it apparently
    finish but the code never returns.  So anymore I always prefetch large model files
    with git or curl and load them from a local path.  Loading the model locally probably
    took 10 mins or more as I recall but I didn''t time it to know the actual load
    time.'
  created_at: 2022-09-15 17:21:58+00:00
  edited: false
  hidden: false
  id: 63236d46e007685e355b6f4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
      fullname: Ian Beaver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IanBeaver
      type: user
    createdAt: '2022-09-15T20:32:36.000Z'
    data:
      edited: false
      editors:
      - IanBeaver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0817bb5ee816ee0f7511c8d392648db4.svg
          fullname: Ian Beaver
          isHf: false
          isPro: false
          name: IanBeaver
          type: user
        html: '<p>Out of curiosity I loaded it again on a x2iezn.6xlarge instance
          and timed it.  Interestingly, it took 61 minutes to load, but it seemed
          to be mostly loaded within 10 minutes and spent the remaining 50 minutes
          cycling back and forth between 670Gb and 675GB of RAM while using 100% of
          one CPU core.  Perhaps a recent update in transformers broke something in
          the load function?  It did complete the load from local disk though.</p>

          <p>This is the only code I executed for reference:</p>

          <pre><code>from transformers import AutoModelForCausalLM

          model = AutoModelForCausalLM.from_pretrained("/data/bloom")

          </code></pre>

          '
        raw: 'Out of curiosity I loaded it again on a x2iezn.6xlarge instance and
          timed it.  Interestingly, it took 61 minutes to load, but it seemed to be
          mostly loaded within 10 minutes and spent the remaining 50 minutes cycling
          back and forth between 670Gb and 675GB of RAM while using 100% of one CPU
          core.  Perhaps a recent update in transformers broke something in the load
          function?  It did complete the load from local disk though.


          This is the only code I executed for reference:

          ```

          from transformers import AutoModelForCausalLM

          model = AutoModelForCausalLM.from_pretrained("/data/bloom")

          ```'
        updatedAt: '2022-09-15T20:32:36.590Z'
      numEdits: 0
      reactions: []
    id: 63238be4e007685e355cd76d
    type: comment
  author: IanBeaver
  content: 'Out of curiosity I loaded it again on a x2iezn.6xlarge instance and timed
    it.  Interestingly, it took 61 minutes to load, but it seemed to be mostly loaded
    within 10 minutes and spent the remaining 50 minutes cycling back and forth between
    670Gb and 675GB of RAM while using 100% of one CPU core.  Perhaps a recent update
    in transformers broke something in the load function?  It did complete the load
    from local disk though.


    This is the only code I executed for reference:

    ```

    from transformers import AutoModelForCausalLM

    model = AutoModelForCausalLM.from_pretrained("/data/bloom")

    ```'
  created_at: 2022-09-15 19:32:36+00:00
  edited: false
  hidden: false
  id: 63238be4e007685e355cd76d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcc640f1508bcf1d07915d5ebaa0ab89.svg
      fullname: "Vinicius Guimar\xE3es"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viniciusguimaraes
      type: user
    createdAt: '2022-09-16T20:53:26.000Z'
    data:
      edited: false
      editors:
      - viniciusguimaraes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcc640f1508bcf1d07915d5ebaa0ab89.svg
          fullname: "Vinicius Guimar\xE3es"
          isHf: false
          isPro: false
          name: viniciusguimaraes
          type: user
        html: '<p>When I run my script I have the same RAM behavior but CPU drops
          to 0.7%. I''ve tried loading from disk and from the HuggingFace API but
          had no success with neither. </p>

          <p>The first time I tried to download the model I didn''t have enough disk
          space and had to manually download the missing files after increasing the
          disk volume. I will try to download Bloom again using only git-lfs and see
          if that solves it. Anyway, thank you for taking some time of your day to
          respond.</p>

          '
        raw: "When I run my script I have the same RAM behavior but CPU drops to 0.7%.\
          \ I've tried loading from disk and from the HuggingFace API but had no success\
          \ with neither. \n\nThe first time I tried to download the model I didn't\
          \ have enough disk space and had to manually download the missing files\
          \ after increasing the disk volume. I will try to download Bloom again using\
          \ only git-lfs and see if that solves it. Anyway, thank you for taking some\
          \ time of your day to respond."
        updatedAt: '2022-09-16T20:53:26.292Z'
      numEdits: 0
      reactions: []
    id: 6324e246fee2904e370bdac4
    type: comment
  author: viniciusguimaraes
  content: "When I run my script I have the same RAM behavior but CPU drops to 0.7%.\
    \ I've tried loading from disk and from the HuggingFace API but had no success\
    \ with neither. \n\nThe first time I tried to download the model I didn't have\
    \ enough disk space and had to manually download the missing files after increasing\
    \ the disk volume. I will try to download Bloom again using only git-lfs and see\
    \ if that solves it. Anyway, thank you for taking some time of your day to respond."
  created_at: 2022-09-16 19:53:26+00:00
  edited: false
  hidden: false
  id: 6324e246fee2904e370bdac4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674490197803-63ceae72257cc48a04f94ed3.jpeg?w=200&h=200&f=face
      fullname: "Marek \u0141abuzek"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marek-Labuzek-at-Capg
      type: user
    createdAt: '2023-01-23T16:39:48.000Z'
    data:
      edited: false
      editors:
      - Marek-Labuzek-at-Capg
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674490197803-63ceae72257cc48a04f94ed3.jpeg?w=200&h=200&f=face
          fullname: "Marek \u0141abuzek"
          isHf: false
          isPro: false
          name: Marek-Labuzek-at-Capg
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Tallin&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Tallin\">@<span class=\"\
          underline\">Tallin</span></a></span>\n\n\t</span></span> am late for the\
          \ party, I've just made BLOOM running on my local server (following <a href=\"\
          https://huggingface.co/bigscience/bloom/discussions/87\">https://huggingface.co/bigscience/bloom/discussions/87</a>)\
          \ and I was observing the on the CPU (without GPU) but with decent number\
          \ of cores, the calculations run quite fast. The VMs in cloud that come\
          \ with amount of memory needed to keep whole model, come with at least 48\
          \ cores, I wonder if pytorch will use them effectively.<br>A side note (see\
          \ cited discussion) is that not all CPUs run multithreaded, at least as\
          \ you just 'walk up and - try to - use' them.</p>\n"
        raw: '@Tallin am late for the party, I''ve just made BLOOM running on my local
          server (following https://huggingface.co/bigscience/bloom/discussions/87)
          and I was observing the on the CPU (without GPU) but with decent number
          of cores, the calculations run quite fast. The VMs in cloud that come with
          amount of memory needed to keep whole model, come with at least 48 cores,
          I wonder if pytorch will use them effectively.

          A side note (see cited discussion) is that not all CPUs run multithreaded,
          at least as you just ''walk up and - try to - use'' them.'
        updatedAt: '2023-01-23T16:39:48.767Z'
      numEdits: 0
      reactions: []
    id: 63ceb8542a7f32cbc460447a
    type: comment
  author: Marek-Labuzek-at-Capg
  content: '@Tallin am late for the party, I''ve just made BLOOM running on my local
    server (following https://huggingface.co/bigscience/bloom/discussions/87) and
    I was observing the on the CPU (without GPU) but with decent number of cores,
    the calculations run quite fast. The VMs in cloud that come with amount of memory
    needed to keep whole model, come with at least 48 cores, I wonder if pytorch will
    use them effectively.

    A side note (see cited discussion) is that not all CPUs run multithreaded, at
    least as you just ''walk up and - try to - use'' them.'
  created_at: 2023-01-23 16:39:48+00:00
  edited: false
  hidden: false
  id: 63ceb8542a7f32cbc460447a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 45
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Hosting bloom 176B model for inference
