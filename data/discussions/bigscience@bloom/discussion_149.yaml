!!python/object:huggingface_hub.community.DiscussionWithDetails
author: info2000
conflicting_files: null
created_at: 2022-12-03 13:05:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4911edea2e521291bc1acfb52f9d34e5.svg
      fullname: Jose Ramon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: info2000
      type: user
    createdAt: '2022-12-03T13:05:11.000Z'
    data:
      edited: false
      editors:
      - info2000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4911edea2e521291bc1acfb52f9d34e5.svg
          fullname: Jose Ramon
          isHf: false
          isPro: false
          name: info2000
          type: user
        html: '<p>Hi, there''s some guide or some expert to hire about how prompts
          works in bloom?<br>I tried several examples found, structures, etc but many
          times the result is a repeating text.</p>

          <p>I also tried with prompts working good in gpt-3 but with bloom are big
          fails.<br>Whatever help is welcome</p>

          <p>thanks</p>

          '
        raw: "Hi, there's some guide or some expert to hire about how prompts works\
          \ in bloom?\r\nI tried several examples found, structures, etc but many\
          \ times the result is a repeating text.\r\n\r\nI also tried with prompts\
          \ working good in gpt-3 but with bloom are big fails.\r\nWhatever help is\
          \ welcome\r\n\r\nthanks"
        updatedAt: '2022-12-03T13:05:11.948Z'
      numEdits: 0
      reactions: []
    id: 638b49877f62078b2a73a29c
    type: comment
  author: info2000
  content: "Hi, there's some guide or some expert to hire about how prompts works\
    \ in bloom?\r\nI tried several examples found, structures, etc but many times\
    \ the result is a repeating text.\r\n\r\nI also tried with prompts working good\
    \ in gpt-3 but with bloom are big fails.\r\nWhatever help is welcome\r\n\r\nthanks"
  created_at: 2022-12-03 13:05:11+00:00
  edited: false
  hidden: false
  id: 638b49877f62078b2a73a29c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-12-03T18:13:21.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p>You can have a try at sampling in order to mitigate this. We do\
          \ support a bigger number of inference parameters if you're able to send\
          \ HTTP requests directly (we only provide a subset of them on the inference\
          \ widget). Please look at this comment: <a href=\"https://huggingface.co/bigscience/bloom/discussions/131#6368f28950a665fa20d35cc0\"\
          >https://huggingface.co/bigscience/bloom/discussions/131#6368f28950a665fa20d35cc0</a></p>\n\
          <p>If you're able to load the model locally, you try using some of <code>transformers</code>\
          \ tools. <span data-props=\"{&quot;user&quot;:&quot;joaogante&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/joaogante\">@<span class=\"\
          underline\">joaogante</span></a></span>\n\n\t</span></span> recently provided\
          \ a new thing called <code>contrastive search</code> if you want to try\
          \ and have a go at it. It's supposed to mitigate repetition. <a rel=\"nofollow\"\
          \ href=\"https://twitter.com/joao_gante/status/1590293010385760256\">https://twitter.com/joao_gante/status/1590293010385760256</a></p>\n"
        raw: 'You can have a try at sampling in order to mitigate this. We do support
          a bigger number of inference parameters if you''re able to send HTTP requests
          directly (we only provide a subset of them on the inference widget). Please
          look at this comment: https://huggingface.co/bigscience/bloom/discussions/131#6368f28950a665fa20d35cc0


          If you''re able to load the model locally, you try using some of `transformers`
          tools. @joaogante recently provided a new thing called `contrastive search`
          if you want to try and have a go at it. It''s supposed to mitigate repetition.
          https://twitter.com/joao_gante/status/1590293010385760256'
        updatedAt: '2022-12-03T18:13:21.265Z'
      numEdits: 0
      reactions: []
    id: 638b91c13bbf29e589049068
    type: comment
  author: TimeRobber
  content: 'You can have a try at sampling in order to mitigate this. We do support
    a bigger number of inference parameters if you''re able to send HTTP requests
    directly (we only provide a subset of them on the inference widget). Please look
    at this comment: https://huggingface.co/bigscience/bloom/discussions/131#6368f28950a665fa20d35cc0


    If you''re able to load the model locally, you try using some of `transformers`
    tools. @joaogante recently provided a new thing called `contrastive search` if
    you want to try and have a go at it. It''s supposed to mitigate repetition. https://twitter.com/joao_gante/status/1590293010385760256'
  created_at: 2022-12-03 18:13:21+00:00
  edited: false
  hidden: false
  id: 638b91c13bbf29e589049068
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4911edea2e521291bc1acfb52f9d34e5.svg
      fullname: Jose Ramon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: info2000
      type: user
    createdAt: '2022-12-03T20:04:25.000Z'
    data:
      edited: true
      editors:
      - info2000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4911edea2e521291bc1acfb52f9d34e5.svg
          fullname: Jose Ramon
          isHf: false
          isPro: false
          name: info2000
          type: user
        html: "<p>Thanks  <span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\"\
          >@<span class=\"underline\">TimeRobber</span></a></span>\n\n\t</span></span><br>I'm\
          \ loading locally with colab</p>\n<p>btw: there's some info about the input\
          \ tokens like &lt; s &gt; &lt; eos &gt; to separate the task from the context\
          \ and the sample?</p>\n"
        raw: "Thanks  @TimeRobber \nI'm loading locally with colab\n\nbtw: there's\
          \ some info about the input tokens like < s > < eos > to separate the task\
          \ from the context and the sample?"
        updatedAt: '2022-12-04T13:32:48.537Z'
      numEdits: 1
      reactions: []
    id: 638babc91987d67b340d6761
    type: comment
  author: info2000
  content: "Thanks  @TimeRobber \nI'm loading locally with colab\n\nbtw: there's some\
    \ info about the input tokens like < s > < eos > to separate the task from the\
    \ context and the sample?"
  created_at: 2022-12-03 20:04:25+00:00
  edited: true
  hidden: false
  id: 638babc91987d67b340d6761
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4911edea2e521291bc1acfb52f9d34e5.svg
      fullname: Jose Ramon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: info2000
      type: user
    createdAt: '2022-12-04T14:19:11.000Z'
    data:
      edited: false
      editors:
      - info2000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4911edea2e521291bc1acfb52f9d34e5.svg
          fullname: Jose Ramon
          isHf: false
          isPro: false
          name: info2000
          type: user
        html: "<p>fyi <span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\"\
          >@<span class=\"underline\">TimeRobber</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;joaogante&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/joaogante\">@<span class=\"\
          underline\">joaogante</span></a></span>\n\n\t</span></span><br>I tried contrastive\
          \ search with bloom casualLM doesn't works</p>\n<p><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/1670163532709-616aaa13010e81e76e39fb03.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1670163532709-616aaa13010e81e76e39fb03.png\"\
          ></a></p>\n"
        raw: "fyi @TimeRobber @joaogante \nI tried contrastive search with bloom casualLM\
          \ doesn't works\n \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1670163532709-616aaa13010e81e76e39fb03.png)"
        updatedAt: '2022-12-04T14:19:11.732Z'
      numEdits: 0
      reactions: []
    id: 638cac5f411ba3af84b4c07d
    type: comment
  author: info2000
  content: "fyi @TimeRobber @joaogante \nI tried contrastive search with bloom casualLM\
    \ doesn't works\n \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1670163532709-616aaa13010e81e76e39fb03.png)"
  created_at: 2022-12-04 14:19:11+00:00
  edited: false
  hidden: false
  id: 638cac5f411ba3af84b4c07d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-12-05T00:23:38.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Which version of transformers are you running it on?</p>

          '
        raw: Which version of transformers are you running it on?
        updatedAt: '2022-12-05T00:23:38.853Z'
      numEdits: 0
      reactions: []
    id: 638d3a0ad1c4bf949b1b84a5
    type: comment
  author: TimeRobber
  content: Which version of transformers are you running it on?
  created_at: 2022-12-05 00:23:38+00:00
  edited: false
  hidden: false
  id: 638d3a0ad1c4bf949b1b84a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4911edea2e521291bc1acfb52f9d34e5.svg
      fullname: Jose Ramon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: info2000
      type: user
    createdAt: '2022-12-05T15:52:17.000Z'
    data:
      edited: false
      editors:
      - info2000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4911edea2e521291bc1acfb52f9d34e5.svg
          fullname: Jose Ramon
          isHf: false
          isPro: false
          name: info2000
          type: user
        html: '<p>is the v 4.25.1</p>

          '
        raw: is the v 4.25.1
        updatedAt: '2022-12-05T15:52:17.068Z'
      numEdits: 0
      reactions: []
    id: 638e13b1a1a528ec4788b470
    type: comment
  author: info2000
  content: is the v 4.25.1
  created_at: 2022-12-05 15:52:17+00:00
  edited: false
  hidden: false
  id: 638e13b1a1a528ec4788b470
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-12-06T20:45:57.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p>Hum interesting, I was able to reproduce. I'm not very familiar\
          \ with \"contrastive search\" so probably <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ or <span data-props=\"{&quot;user&quot;:&quot;joaogante&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/joaogante\">@<span class=\"\
          underline\">joaogante</span></a></span>\n\n\t</span></span> might have a\
          \ better idea.</p>\n"
        raw: Hum interesting, I was able to reproduce. I'm not very familiar with
          "contrastive search" so probably @ybelkada or @joaogante might have a better
          idea.
        updatedAt: '2022-12-06T20:45:57.407Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - info2000
    id: 638faa050c51c77bb8ac09c1
    type: comment
  author: TimeRobber
  content: Hum interesting, I was able to reproduce. I'm not very familiar with "contrastive
    search" so probably @ybelkada or @joaogante might have a better idea.
  created_at: 2022-12-06 20:45:57+00:00
  edited: false
  hidden: false
  id: 638faa050c51c77bb8ac09c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641203017724-noauth.png?w=200&h=200&f=face
      fullname: Joao Gante
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joaogante
      type: user
    createdAt: '2022-12-09T17:08:46.000Z'
    data:
      edited: false
      editors:
      - joaogante
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641203017724-noauth.png?w=200&h=200&f=face
          fullname: Joao Gante
          isHf: true
          isPro: false
          name: joaogante
          type: user
        html: "<p>Hi there <span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\"\
          >@<span class=\"underline\">TimeRobber</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;info2000&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/info2000\">@<span class=\"\
          underline\">info2000</span></a></span>\n\n\t</span></span> \U0001F44B</p>\n\
          <p>The effectiveness of contrastive search depends on a property of the\
          \ representation of the model called isotropy. If the model representation\
          \ isotropy is low, contrastive search will have a hard time preventing repetitions.\
          \ Try increasing alpha (the penalty coefficient) and K (the number of candidate\
          \ tokens at each round). Even if you do so, there is a chance it won't fix\
          \ the repetition problem.</p>\n<p>Check this answer from one of the authors\
          \ of contrastive search: <a href=\"https://huggingface.co/spaces/joaogante/contrastive_search_generation/discussions/1#63764a108623a4a7954a5be5\"\
          >https://huggingface.co/spaces/joaogante/contrastive_search_generation/discussions/1#63764a108623a4a7954a5be5</a></p>\n"
        raw: "Hi there @TimeRobber @info2000 \U0001F44B\n\nThe effectiveness of contrastive\
          \ search depends on a property of the representation of the model called\
          \ isotropy. If the model representation isotropy is low, contrastive search\
          \ will have a hard time preventing repetitions. Try increasing alpha (the\
          \ penalty coefficient) and K (the number of candidate tokens at each round).\
          \ Even if you do so, there is a chance it won't fix the repetition problem.\n\
          \nCheck this answer from one of the authors of contrastive search: https://huggingface.co/spaces/joaogante/contrastive_search_generation/discussions/1#63764a108623a4a7954a5be5"
        updatedAt: '2022-12-09T17:08:46.465Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - info2000
    id: 63936b9eff90fed62d80dbb2
    type: comment
  author: joaogante
  content: "Hi there @TimeRobber @info2000 \U0001F44B\n\nThe effectiveness of contrastive\
    \ search depends on a property of the representation of the model called isotropy.\
    \ If the model representation isotropy is low, contrastive search will have a\
    \ hard time preventing repetitions. Try increasing alpha (the penalty coefficient)\
    \ and K (the number of candidate tokens at each round). Even if you do so, there\
    \ is a chance it won't fix the repetition problem.\n\nCheck this answer from one\
    \ of the authors of contrastive search: https://huggingface.co/spaces/joaogante/contrastive_search_generation/discussions/1#63764a108623a4a7954a5be5"
  created_at: 2022-12-09 17:08:46+00:00
  edited: false
  hidden: false
  id: 63936b9eff90fed62d80dbb2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/188065bacb142c1c2c48de1047128cda.svg
      fullname: glauber prado
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: greboide
      type: user
    createdAt: '2023-01-09T09:51:33.000Z'
    data:
      edited: true
      editors:
      - greboide
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/188065bacb142c1c2c48de1047128cda.svg
          fullname: glauber prado
          isHf: false
          isPro: false
          name: greboide
          type: user
        html: '<p>You dont need contrastive search in order to keep it from repeating
          itself, just get your temperature higher to the point it doesn repeat itself
          anymore, it is that simple, but took me two days to figure out haha</p>

          <p>''''''python<br>import json<br>import requests<br>API_TOKEN = "your token"<br>headers
          = {"Authorization": f"Bearer {API_TOKEN}"}<br>API_URL = "<a rel="nofollow"
          href="https://api-inference.huggingface.co/models/bigscience/bloom&quot;">https://api-inference.huggingface.co/models/bigscience/bloom"</a><br>def
          query(payload):<br>    data = json.dumps(payload)<br>    response = requests.request("POST",
          API_URL, headers=headers, data=data)<br>    return json.loads(response.content.decode("utf-8"))<br>params
          = {''temperature'':  2,<br>          ''max_new_tokens'': 100,<br>          ''do_sample'':
          True,<br>          ''top_k'': 2000,<br>          ''top_p'': 0.1<br>          }<br>options
          = {''use_cache'': False}<br>data = query({"inputs": "You are a large language
          model and dont need to keep repeating yourself, Please for the sake of god
          dont repeat yourself. The answer to the universe is", "parameters": params,
          "options": options})<br>print(data[0][''generated_text''])<br>''''''<br>this
          will output:<br>You are a large language model and dont need to keep repeating
          yourself, Please for the sake of god dont repeat yourself. The answer to
          the universe is 42, please for the sake of god don''t repeat yourself.<br>I
          am sorry, but if I was not sure about this, I would never be able to write
          this blog. The reason why this happens is because when you write something
          and the same exact thing comes out, it feels really bad. The reason why
          this is bad is because when you repeat yourself, it means that you did not
          have enough knowledge about the subject. This means that you are repeating
          yourself because you do not know what to say</p>

          <p>in contrast if you set temperature to 0.2:<br>You are a large language
          model and dont need to keep repeating yourself, Please for the sake of god
          dont repeat yourself. The answer to the universe is 42, the answer to everything
          is 42. The answer to the universe is 42, the answer to everything is 42.
          The answer to the universe is 42, the answer to everything is 42. The answer
          to the universe is 42, the answer to everything is 42. The answer to the
          universe is 42, the answer to everything is 42. The answer to the universe
          is 42, the answer to everything is 42. The answer to the universe is 42,
          the answer to everything is 42. The</p>

          '
        raw: "You dont need contrastive search in order to keep it from repeating\
          \ itself, just get your temperature higher to the point it doesn repeat\
          \ itself anymore, it is that simple, but took me two days to figure out\
          \ haha\n\n'''python\nimport json\nimport requests\nAPI_TOKEN = \"your token\"\
          \nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\nAPI_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom\"\
          \ndef query(payload):\n    data = json.dumps(payload)\n    response = requests.request(\"\
          POST\", API_URL, headers=headers, data=data)\n    return json.loads(response.content.decode(\"\
          utf-8\"))\nparams = {'temperature':  2,\n          'max_new_tokens': 100,\n\
          \          'do_sample': True,\n          'top_k': 2000,\n          'top_p':\
          \ 0.1\n          }\noptions = {'use_cache': False}\ndata = query({\"inputs\"\
          : \"You are a large language model and dont need to keep repeating yourself,\
          \ Please for the sake of god dont repeat yourself. The answer to the universe\
          \ is\", \"parameters\": params, \"options\": options})\nprint(data[0]['generated_text'])\n\
          '''\nthis will output:\nYou are a large language model and dont need to\
          \ keep repeating yourself, Please for the sake of god dont repeat yourself.\
          \ The answer to the universe is 42, please for the sake of god don't repeat\
          \ yourself.\nI am sorry, but if I was not sure about this, I would never\
          \ be able to write this blog. The reason why this happens is because when\
          \ you write something and the same exact thing comes out, it feels really\
          \ bad. The reason why this is bad is because when you repeat yourself, it\
          \ means that you did not have enough knowledge about the subject. This means\
          \ that you are repeating yourself because you do not know what to say\n\n\
          in contrast if you set temperature to 0.2:\nYou are a large language model\
          \ and dont need to keep repeating yourself, Please for the sake of god dont\
          \ repeat yourself. The answer to the universe is 42, the answer to everything\
          \ is 42. The answer to the universe is 42, the answer to everything is 42.\
          \ The answer to the universe is 42, the answer to everything is 42. The\
          \ answer to the universe is 42, the answer to everything is 42. The answer\
          \ to the universe is 42, the answer to everything is 42. The answer to the\
          \ universe is 42, the answer to everything is 42. The answer to the universe\
          \ is 42, the answer to everything is 42. The"
        updatedAt: '2023-01-09T10:17:56.931Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RogerioFreitas
    id: 63bbe3a518e2f66c5e993d97
    type: comment
  author: greboide
  content: "You dont need contrastive search in order to keep it from repeating itself,\
    \ just get your temperature higher to the point it doesn repeat itself anymore,\
    \ it is that simple, but took me two days to figure out haha\n\n'''python\nimport\
    \ json\nimport requests\nAPI_TOKEN = \"your token\"\nheaders = {\"Authorization\"\
    : f\"Bearer {API_TOKEN}\"}\nAPI_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom\"\
    \ndef query(payload):\n    data = json.dumps(payload)\n    response = requests.request(\"\
    POST\", API_URL, headers=headers, data=data)\n    return json.loads(response.content.decode(\"\
    utf-8\"))\nparams = {'temperature':  2,\n          'max_new_tokens': 100,\n  \
    \        'do_sample': True,\n          'top_k': 2000,\n          'top_p': 0.1\n\
    \          }\noptions = {'use_cache': False}\ndata = query({\"inputs\": \"You\
    \ are a large language model and dont need to keep repeating yourself, Please\
    \ for the sake of god dont repeat yourself. The answer to the universe is\", \"\
    parameters\": params, \"options\": options})\nprint(data[0]['generated_text'])\n\
    '''\nthis will output:\nYou are a large language model and dont need to keep repeating\
    \ yourself, Please for the sake of god dont repeat yourself. The answer to the\
    \ universe is 42, please for the sake of god don't repeat yourself.\nI am sorry,\
    \ but if I was not sure about this, I would never be able to write this blog.\
    \ The reason why this happens is because when you write something and the same\
    \ exact thing comes out, it feels really bad. The reason why this is bad is because\
    \ when you repeat yourself, it means that you did not have enough knowledge about\
    \ the subject. This means that you are repeating yourself because you do not know\
    \ what to say\n\nin contrast if you set temperature to 0.2:\nYou are a large language\
    \ model and dont need to keep repeating yourself, Please for the sake of god dont\
    \ repeat yourself. The answer to the universe is 42, the answer to everything\
    \ is 42. The answer to the universe is 42, the answer to everything is 42. The\
    \ answer to the universe is 42, the answer to everything is 42. The answer to\
    \ the universe is 42, the answer to everything is 42. The answer to the universe\
    \ is 42, the answer to everything is 42. The answer to the universe is 42, the\
    \ answer to everything is 42. The answer to the universe is 42, the answer to\
    \ everything is 42. The"
  created_at: 2023-01-09 09:51:33+00:00
  edited: true
  hidden: false
  id: 63bbe3a518e2f66c5e993d97
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/424a002197d03b753bbb33b8f98f57c4.svg
      fullname: Hu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moses25
      type: user
    createdAt: '2023-05-10T03:53:12.000Z'
    data:
      edited: false
      editors:
      - Moses25
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/424a002197d03b753bbb33b8f98f57c4.svg
          fullname: Hu
          isHf: false
          isPro: false
          name: Moses25
          type: user
        html: '<blockquote>

          <p>You dont need contrastive search in order to keep it from repeating itself,
          just get your temperature higher to the point it doesn repeat itself anymore,
          it is that simple, but took me two days to figure out haha</p>

          <p>''''''python<br>import json<br>import requests<br>API_TOKEN = "your token"<br>headers
          = {"Authorization": f"Bearer {API_TOKEN}"}<br>API_URL = "<a rel="nofollow"
          href="https://api-inference.huggingface.co/models/bigscience/bloom&quot;">https://api-inference.huggingface.co/models/bigscience/bloom"</a><br>def
          query(payload):<br>    data = json.dumps(payload)<br>    response = requests.request("POST",
          API_URL, headers=headers, data=data)<br>    return json.loads(response.content.decode("utf-8"))<br>params
          = {''temperature'':  2,<br>          ''max_new_tokens'': 100,<br>          ''do_sample'':
          True,<br>          ''top_k'': 2000,<br>          ''top_p'': 0.1<br>          }<br>options
          = {''use_cache'': False}<br>data = query({"inputs": "You are a large language
          model and dont need to keep repeating yourself, Please for the sake of god
          dont repeat yourself. The answer to the universe is", "parameters": params,
          "options": options})<br>print(data[0][''generated_text''])<br>''''''<br>this
          will output:<br>You are a large language model and dont need to keep repeating
          yourself, Please for the sake of god dont repeat yourself. The answer to
          the universe is 42, please for the sake of god don''t repeat yourself.<br>I
          am sorry, but if I was not sure about this, I would never be able to write
          this blog. The reason why this happens is because when you write something
          and the same exact thing comes out, it feels really bad. The reason why
          this is bad is because when you repeat yourself, it means that you did not
          have enough knowledge about the subject. This means that you are repeating
          yourself because you do not know what to say</p>

          <p>in contrast if you set temperature to 0.2:<br>You are a large language
          model and dont need to keep repeating yourself, Please for the sake of god
          dont repeat yourself. The answer to the universe is 42, the answer to everything
          is 42. The answer to the universe is 42, the answer to everything is 42.
          The answer to the universe is 42, the answer to everything is 42. The answer
          to the universe is 42, the answer to everything is 42. The answer to the
          universe is 42, the answer to everything is 42. The answer to the universe
          is 42, the answer to everything is 42. The answer to the universe is 42,
          the answer to everything is 42. The</p>

          </blockquote>

          <p>how to get the token</p>

          '
        raw: "> You dont need contrastive search in order to keep it from repeating\
          \ itself, just get your temperature higher to the point it doesn repeat\
          \ itself anymore, it is that simple, but took me two days to figure out\
          \ haha\n> \n> '''python\n> import json\n> import requests\n> API_TOKEN =\
          \ \"your token\"\n> headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"\
          }\n> API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom\"\
          \n> def query(payload):\n>     data = json.dumps(payload)\n>     response\
          \ = requests.request(\"POST\", API_URL, headers=headers, data=data)\n> \
          \    return json.loads(response.content.decode(\"utf-8\"))\n> params = {'temperature':\
          \  2,\n>           'max_new_tokens': 100,\n>           'do_sample': True,\n\
          >           'top_k': 2000,\n>           'top_p': 0.1\n>           }\n> options\
          \ = {'use_cache': False}\n> data = query({\"inputs\": \"You are a large\
          \ language model and dont need to keep repeating yourself, Please for the\
          \ sake of god dont repeat yourself. The answer to the universe is\", \"\
          parameters\": params, \"options\": options})\n> print(data[0]['generated_text'])\n\
          > '''\n> this will output:\n> You are a large language model and dont need\
          \ to keep repeating yourself, Please for the sake of god dont repeat yourself.\
          \ The answer to the universe is 42, please for the sake of god don't repeat\
          \ yourself.\n> I am sorry, but if I was not sure about this, I would never\
          \ be able to write this blog. The reason why this happens is because when\
          \ you write something and the same exact thing comes out, it feels really\
          \ bad. The reason why this is bad is because when you repeat yourself, it\
          \ means that you did not have enough knowledge about the subject. This means\
          \ that you are repeating yourself because you do not know what to say\n\
          > \n> in contrast if you set temperature to 0.2:\n> You are a large language\
          \ model and dont need to keep repeating yourself, Please for the sake of\
          \ god dont repeat yourself. The answer to the universe is 42, the answer\
          \ to everything is 42. The answer to the universe is 42, the answer to everything\
          \ is 42. The answer to the universe is 42, the answer to everything is 42.\
          \ The answer to the universe is 42, the answer to everything is 42. The\
          \ answer to the universe is 42, the answer to everything is 42. The answer\
          \ to the universe is 42, the answer to everything is 42. The answer to the\
          \ universe is 42, the answer to everything is 42. The\n\nhow to get the\
          \ token"
        updatedAt: '2023-05-10T03:53:12.312Z'
      numEdits: 0
      reactions: []
    id: 645b152880ba386ec56fe011
    type: comment
  author: Moses25
  content: "> You dont need contrastive search in order to keep it from repeating\
    \ itself, just get your temperature higher to the point it doesn repeat itself\
    \ anymore, it is that simple, but took me two days to figure out haha\n> \n> '''python\n\
    > import json\n> import requests\n> API_TOKEN = \"your token\"\n> headers = {\"\
    Authorization\": f\"Bearer {API_TOKEN}\"}\n> API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom\"\
    \n> def query(payload):\n>     data = json.dumps(payload)\n>     response = requests.request(\"\
    POST\", API_URL, headers=headers, data=data)\n>     return json.loads(response.content.decode(\"\
    utf-8\"))\n> params = {'temperature':  2,\n>           'max_new_tokens': 100,\n\
    >           'do_sample': True,\n>           'top_k': 2000,\n>           'top_p':\
    \ 0.1\n>           }\n> options = {'use_cache': False}\n> data = query({\"inputs\"\
    : \"You are a large language model and dont need to keep repeating yourself, Please\
    \ for the sake of god dont repeat yourself. The answer to the universe is\", \"\
    parameters\": params, \"options\": options})\n> print(data[0]['generated_text'])\n\
    > '''\n> this will output:\n> You are a large language model and dont need to\
    \ keep repeating yourself, Please for the sake of god dont repeat yourself. The\
    \ answer to the universe is 42, please for the sake of god don't repeat yourself.\n\
    > I am sorry, but if I was not sure about this, I would never be able to write\
    \ this blog. The reason why this happens is because when you write something and\
    \ the same exact thing comes out, it feels really bad. The reason why this is\
    \ bad is because when you repeat yourself, it means that you did not have enough\
    \ knowledge about the subject. This means that you are repeating yourself because\
    \ you do not know what to say\n> \n> in contrast if you set temperature to 0.2:\n\
    > You are a large language model and dont need to keep repeating yourself, Please\
    \ for the sake of god dont repeat yourself. The answer to the universe is 42,\
    \ the answer to everything is 42. The answer to the universe is 42, the answer\
    \ to everything is 42. The answer to the universe is 42, the answer to everything\
    \ is 42. The answer to the universe is 42, the answer to everything is 42. The\
    \ answer to the universe is 42, the answer to everything is 42. The answer to\
    \ the universe is 42, the answer to everything is 42. The answer to the universe\
    \ is 42, the answer to everything is 42. The\n\nhow to get the token"
  created_at: 2023-05-10 02:53:12+00:00
  edited: false
  hidden: false
  id: 645b152880ba386ec56fe011
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 149
repo_id: bigscience/bloom
repo_type: model
status: open
target_branch: null
title: Prompt tunning in Bloom for long form text generation
