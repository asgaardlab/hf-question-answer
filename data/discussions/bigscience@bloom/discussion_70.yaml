!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jmarxza
conflicting_files: null
created_at: 2022-08-01 06:05:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b9fdb47615077fce1ba08b334fed407.svg
      fullname: Joshua Marx
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jmarxza
      type: user
    createdAt: '2022-08-01T07:05:00.000Z'
    data:
      edited: false
      editors:
      - jmarxza
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b9fdb47615077fce1ba08b334fed407.svg
          fullname: Joshua Marx
          isHf: false
          isPro: false
          name: jmarxza
          type: user
        html: '<p>Any way to stop this image from popping up?</p>

          '
        raw: Any way to stop this image from popping up?
        updatedAt: '2022-08-01T07:05:00.357Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - CostaFernando
        - FernandoCosta
        - chiyuri
        - Apps
        - CharlesHao
    id: 62e77b1cf97481d0a13f9d09
    type: comment
  author: jmarxza
  content: Any way to stop this image from popping up?
  created_at: 2022-08-01 06:05:00+00:00
  edited: false
  hidden: false
  id: 62e77b1cf97481d0a13f9d09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a87a2f60ac02dbf9db38c9f98fcb4756.svg
      fullname: Fernando Costa de Souza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FernandoCosta
      type: user
    createdAt: '2022-08-01T10:05:21.000Z'
    data:
      edited: true
      editors:
      - FernandoCosta
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a87a2f60ac02dbf9db38c9f98fcb4756.svg
          fullname: Fernando Costa de Souza
          isHf: false
          isPro: false
          name: FernandoCosta
          type: user
        html: '<p>I was facing this issue a few hours ago. But now it is working on
          Huggingface''s Accelerated Inference! It takes normally more than 90s to
          generate 64 tokens, with "use_gpu": True, but it runs.</p>

          '
        raw: 'I was facing this issue a few hours ago. But now it is working on Huggingface''s
          Accelerated Inference! It takes normally more than 90s to generate 64 tokens,
          with "use_gpu": True, but it runs.'
        updatedAt: '2022-08-01T10:05:35.189Z'
      numEdits: 1
      reactions: []
    id: 62e7a561c9e83245ea5d7032
    type: comment
  author: FernandoCosta
  content: 'I was facing this issue a few hours ago. But now it is working on Huggingface''s
    Accelerated Inference! It takes normally more than 90s to generate 64 tokens,
    with "use_gpu": True, but it runs.'
  created_at: 2022-08-01 09:05:21+00:00
  edited: true
  hidden: false
  id: 62e7a561c9e83245ea5d7032
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52f863c47ae48027a25f5e8600ab2f3e.svg
      fullname: Zian Su
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ziansu
      type: user
    createdAt: '2022-10-10T03:04:14.000Z'
    data:
      edited: false
      editors:
      - ziansu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52f863c47ae48027a25f5e8600ab2f3e.svg
          fullname: Zian Su
          isHf: false
          isPro: true
          name: ziansu
          type: user
        html: '<p>I have the same problem here. I can only get <code>generated_text</code>
          from bloom-3b and never succeeded with bloom. Any solutions?</p>

          '
        raw: I have the same problem here. I can only get `generated_text` from bloom-3b
          and never succeeded with bloom. Any solutions?
        updatedAt: '2022-10-10T03:04:14.874Z'
      numEdits: 0
      reactions: []
    id: 63438bae1bdd3dfa55dd7233
    type: comment
  author: ziansu
  content: I have the same problem here. I can only get `generated_text` from bloom-3b
    and never succeeded with bloom. Any solutions?
  created_at: 2022-10-10 02:04:14+00:00
  edited: false
  hidden: false
  id: 63438bae1bdd3dfa55dd7233
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-10-19T21:39:13.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Are you still having the issue? We''re recently been moving to AzureML
          and so service might have been disrupted at some point. But it should be
          a lot more stable now.</p>

          <p>Just to be clear we''re talking about the Inference API?</p>

          '
        raw: 'Are you still having the issue? We''re recently been moving to AzureML
          and so service might have been disrupted at some point. But it should be
          a lot more stable now.


          Just to be clear we''re talking about the Inference API?'
        updatedAt: '2022-10-19T21:39:13.948Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - julien-c
        - ziansu
    id: 63506e81229624a4d4a5164b
    type: comment
  author: TimeRobber
  content: 'Are you still having the issue? We''re recently been moving to AzureML
    and so service might have been disrupted at some point. But it should be a lot
    more stable now.


    Just to be clear we''re talking about the Inference API?'
  created_at: 2022-10-19 20:39:13+00:00
  edited: false
  hidden: false
  id: 63506e81229624a4d4a5164b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52f863c47ae48027a25f5e8600ab2f3e.svg
      fullname: Zian Su
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ziansu
      type: user
    createdAt: '2022-10-20T20:01:03.000Z'
    data:
      edited: false
      editors:
      - ziansu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52f863c47ae48027a25f5e8600ab2f3e.svg
          fullname: Zian Su
          isHf: false
          isPro: true
          name: ziansu
          type: user
        html: '<blockquote>

          <p>Are you still having the issue? We''re recently been moving to AzureML
          and so service might have been disrupted at some point. But it should be
          a lot more stable now.</p>

          <p>Just to be clear we''re talking about the Inference API?</p>

          </blockquote>

          <p>Now it''s working (yes, it is the Inference API). But now I have another
          problem. It seems that even if I specify <code>num_return_sequences</code>
          to be more than 1, I can only get 1 <code>generated_text</code> from bloom.
          I can get the right number of <code>generated_text</code> with bloom-3b.
          Is it because bloom is too big so that it can only do greedy decoding?</p>

          '
        raw: "> Are you still having the issue? We're recently been moving to AzureML\
          \ and so service might have been disrupted at some point. But it should\
          \ be a lot more stable now.\n> \n> Just to be clear we're talking about\
          \ the Inference API?\n\nNow it's working (yes, it is the Inference API).\
          \ But now I have another problem. It seems that even if I specify `num_return_sequences`\
          \ to be more than 1, I can only get 1 `generated_text` from bloom. I can\
          \ get the right number of `generated_text` with bloom-3b. Is it because\
          \ bloom is too big so that it can only do greedy decoding?"
        updatedAt: '2022-10-20T20:01:03.582Z'
      numEdits: 0
      reactions: []
    id: 6351a8ff29e0807aa1781edc
    type: comment
  author: ziansu
  content: "> Are you still having the issue? We're recently been moving to AzureML\
    \ and so service might have been disrupted at some point. But it should be a lot\
    \ more stable now.\n> \n> Just to be clear we're talking about the Inference API?\n\
    \nNow it's working (yes, it is the Inference API). But now I have another problem.\
    \ It seems that even if I specify `num_return_sequences` to be more than 1, I\
    \ can only get 1 `generated_text` from bloom. I can get the right number of `generated_text`\
    \ with bloom-3b. Is it because bloom is too big so that it can only do greedy\
    \ decoding?"
  created_at: 2022-10-20 19:01:03+00:00
  edited: false
  hidden: false
  id: 6351a8ff29e0807aa1781edc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-10-21T09:05:43.000Z'
    data:
      edited: true
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>We have a custom deployment setup right now for BLOOM (in order
          to improve inference speed and such), which doesn''t support all the options
          right now. We''ll try to support new options as the requests come in I guess.</p>

          <blockquote>

          <p>Is it because bloom is too big so that it can only do greedy decoding?</p>

          </blockquote>

          <p>Actually it does more than greedy decoding, you can add <code>top_k</code>
          and <code>top_p</code> options.</p>

          '
        raw: 'We have a custom deployment setup right now for BLOOM (in order to improve
          inference speed and such), which doesn''t support all the options right
          now. We''ll try to support new options as the requests come in I guess.


          > Is it because bloom is too big so that it can only do greedy decoding?


          Actually it does more than greedy decoding, you can add `top_k` and `top_p`
          options.'
        updatedAt: '2022-10-21T09:14:29.003Z'
      numEdits: 1
      reactions: []
    id: 635260e7507b679c3c5c2cdb
    type: comment
  author: TimeRobber
  content: 'We have a custom deployment setup right now for BLOOM (in order to improve
    inference speed and such), which doesn''t support all the options right now. We''ll
    try to support new options as the requests come in I guess.


    > Is it because bloom is too big so that it can only do greedy decoding?


    Actually it does more than greedy decoding, you can add `top_k` and `top_p` options.'
  created_at: 2022-10-21 08:05:43+00:00
  edited: true
  hidden: false
  id: 635260e7507b679c3c5c2cdb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641223205854-noauth.png?w=200&h=200&f=face
      fullname: Darragh Hanley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darragh
      type: user
    createdAt: '2022-10-26T16:51:01.000Z'
    data:
      edited: true
      editors:
      - darragh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641223205854-noauth.png?w=200&h=200&f=face
          fullname: Darragh Hanley
          isHf: false
          isPro: false
          name: darragh
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\"\
          >@<span class=\"underline\">TimeRobber</span></a></span>\n\n\t</span></span>\
          \ , The max tokens for the API seems to be 1024, although I believe the\
          \ bloom model can take longer sequences. Andy greater than 1024, and I get\
          \ the message - <code>Model is overloaded, please wait for a bit</code>.\
          \ Is this max length fixed for all users, or paid plans can increase this\
          \ ?</p>\n"
        raw: Hi @TimeRobber , The max tokens for the API seems to be 1024, although
          I believe the bloom model can take longer sequences. Andy greater than 1024,
          and I get the message - `Model is overloaded, please wait for a bit`. Is
          this max length fixed for all users, or paid plans can increase this ?
        updatedAt: '2022-10-26T16:52:06.518Z'
      numEdits: 1
      reactions: []
    id: 635965753568d26d3f09c12e
    type: comment
  author: darragh
  content: Hi @TimeRobber , The max tokens for the API seems to be 1024, although
    I believe the bloom model can take longer sequences. Andy greater than 1024, and
    I get the message - `Model is overloaded, please wait for a bit`. Is this max
    length fixed for all users, or paid plans can increase this ?
  created_at: 2022-10-26 15:51:01+00:00
  edited: true
  hidden: false
  id: 635965753568d26d3f09c12e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-10-27T10:32:49.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: "<p>I think we hard limit incoming requests that are beyond a specific\
          \ length so that people don't spam our service. In theory if you host the\
          \ model yourself, you can go to arbitrarily long sequences as it uses relative\
          \ positional embeddings system that can extrapolate to any length regardless\
          \ of what was the sequence length when training. More details can be found\
          \ here: <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2108.12409\">https://arxiv.org/abs/2108.12409</a></p>\n\
          <p>Max length should be fixed for all users. At least from this API endpoint.\
          \ cc <span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Concerning whether there's a paid plan you'd have to ask <span\
          \ data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/Narsil\">@<span class=\"underline\"\
          >Narsil</span></a></span>\n\n\t</span></span> to confirm, but I think there\
          \ should be none.</p>\n"
        raw: "I think we hard limit incoming requests that are beyond a specific length\
          \ so that people don't spam our service. In theory if you host the model\
          \ yourself, you can go to arbitrarily long sequences as it uses relative\
          \ positional embeddings system that can extrapolate to any length regardless\
          \ of what was the sequence length when training. More details can be found\
          \ here: https://arxiv.org/abs/2108.12409\n\nMax length should be fixed for\
          \ all users. At least from this API endpoint. cc @olivierdehaene \n\nConcerning\
          \ whether there's a paid plan you'd have to ask @Narsil to confirm, but\
          \ I think there should be none."
        updatedAt: '2022-10-27T10:32:49.041Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - bver
    id: 635a5e511e74f86419b3b920
    type: comment
  author: TimeRobber
  content: "I think we hard limit incoming requests that are beyond a specific length\
    \ so that people don't spam our service. In theory if you host the model yourself,\
    \ you can go to arbitrarily long sequences as it uses relative positional embeddings\
    \ system that can extrapolate to any length regardless of what was the sequence\
    \ length when training. More details can be found here: https://arxiv.org/abs/2108.12409\n\
    \nMax length should be fixed for all users. At least from this API endpoint. cc\
    \ @olivierdehaene \n\nConcerning whether there's a paid plan you'd have to ask\
    \ @Narsil to confirm, but I think there should be none."
  created_at: 2022-10-27 09:32:49+00:00
  edited: false
  hidden: false
  id: 635a5e511e74f86419b3b920
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641223205854-noauth.png?w=200&h=200&f=face
      fullname: Darragh Hanley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darragh
      type: user
    createdAt: '2022-10-27T10:40:05.000Z'
    data:
      edited: true
      editors:
      - darragh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641223205854-noauth.png?w=200&h=200&f=face
          fullname: Darragh Hanley
          isHf: false
          isPro: false
          name: darragh
          type: user
        html: '<p>Thanks, re: hosting this ourselves, can I confirm when I call <code>API_URL
          = "https://api-inference.huggingface.co/models/bigscience/bloom"</code>
          I am hitting large <code>bigscience/bloom</code> version and not one of
          the smaller versions, like <code>bigscience/bloom7b1</code>; and also can
          I confirm if this is running on CPU.<br>It seems very fast for a CPU model
          on large bloom - I get 10 seconds. If it was feasible to get this speed
          on onnx accelerated large bloom, I could try hosting myself.</p>

          <p>P.s. I saw in the docu to check <code>x-compute-type</code> in the headers
          of the response to check if it is CPU or GPU, but I could not see that values.</p>

          '
        raw: "Thanks, re: hosting this ourselves, can I confirm when I call `API_URL\
          \ = \"https://api-inference.huggingface.co/models/bigscience/bloom\"` I\
          \ am hitting large `bigscience/bloom` version and not one of the smaller\
          \ versions, like `bigscience/bloom7b1`; and also can I confirm if this is\
          \ running on CPU. \nIt seems very fast for a CPU model on large bloom -\
          \ I get 10 seconds. If it was feasible to get this speed on onnx accelerated\
          \ large bloom, I could try hosting myself.\n\nP.s. I saw in the docu to\
          \ check `x-compute-type` in the headers of the response to check if it is\
          \ CPU or GPU, but I could not see that values."
        updatedAt: '2022-10-27T10:52:07.916Z'
      numEdits: 3
      reactions: []
    id: 635a60053cc881811025bea4
    type: comment
  author: darragh
  content: "Thanks, re: hosting this ourselves, can I confirm when I call `API_URL\
    \ = \"https://api-inference.huggingface.co/models/bigscience/bloom\"` I am hitting\
    \ large `bigscience/bloom` version and not one of the smaller versions, like `bigscience/bloom7b1`;\
    \ and also can I confirm if this is running on CPU. \nIt seems very fast for a\
    \ CPU model on large bloom - I get 10 seconds. If it was feasible to get this\
    \ speed on onnx accelerated large bloom, I could try hosting myself.\n\nP.s. I\
    \ saw in the docu to check `x-compute-type` in the headers of the response to\
    \ check if it is CPU or GPU, but I could not see that values."
  created_at: 2022-10-27 09:40:05+00:00
  edited: true
  hidden: false
  id: 635a60053cc881811025bea4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-10-27T18:56:39.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<blockquote>

          <p> when I call API_URL = "<a rel="nofollow" href="https://api-inference.huggingface.co/models/bigscience/bloom&quot;">https://api-inference.huggingface.co/models/bigscience/bloom"</a>
          I am hitting large bigscience/bloom version and not one of the smaller versions,
          like bigscience/bloom7b1</p>

          </blockquote>

          <p>Yes you''re running the big model</p>

          <blockquote>

          <p>can I confirm if this is running on CPU</p>

          </blockquote>

          <p>No it runs on GPUs in a parallel fashion. You can find more details at
          <a href="https://huggingface.co/blog/bloom-inference-optimization">https://huggingface.co/blog/bloom-inference-optimization</a></p>

          <p>BLOOM is a more special deployment, and it''s currently being powered
          by AzureML. There won''t be cpu inference on BLOOM</p>

          '
        raw: '>  when I call API_URL = "https://api-inference.huggingface.co/models/bigscience/bloom"
          I am hitting large bigscience/bloom version and not one of the smaller versions,
          like bigscience/bloom7b1


          Yes you''re running the big model


          > can I confirm if this is running on CPU


          No it runs on GPUs in a parallel fashion. You can find more details at https://huggingface.co/blog/bloom-inference-optimization


          BLOOM is a more special deployment, and it''s currently being powered by
          AzureML. There won''t be cpu inference on BLOOM'
        updatedAt: '2022-10-27T18:56:39.256Z'
      numEdits: 0
      reactions: []
    id: 635ad467f9a004065f31ed13
    type: comment
  author: TimeRobber
  content: '>  when I call API_URL = "https://api-inference.huggingface.co/models/bigscience/bloom"
    I am hitting large bigscience/bloom version and not one of the smaller versions,
    like bigscience/bloom7b1


    Yes you''re running the big model


    > can I confirm if this is running on CPU


    No it runs on GPUs in a parallel fashion. You can find more details at https://huggingface.co/blog/bloom-inference-optimization


    BLOOM is a more special deployment, and it''s currently being powered by AzureML.
    There won''t be cpu inference on BLOOM'
  created_at: 2022-10-27 17:56:39+00:00
  edited: false
  hidden: false
  id: 635ad467f9a004065f31ed13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641223205854-noauth.png?w=200&h=200&f=face
      fullname: Darragh Hanley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: darragh
      type: user
    createdAt: '2022-10-28T06:08:19.000Z'
    data:
      edited: false
      editors:
      - darragh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641223205854-noauth.png?w=200&h=200&f=face
          fullname: Darragh Hanley
          isHf: false
          isPro: false
          name: darragh
          type: user
        html: "<p>Thanks a lot <span data-props=\"{&quot;user&quot;:&quot;TimeRobber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TimeRobber\"\
          >@<span class=\"underline\">TimeRobber</span></a></span>\n\n\t</span></span>\
          \ this is very helpful</p>\n"
        raw: Thanks a lot @TimeRobber this is very helpful
        updatedAt: '2022-10-28T06:08:19.437Z'
      numEdits: 0
      reactions: []
    id: 635b71d3010b9019d0891714
    type: comment
  author: darragh
  content: Thanks a lot @TimeRobber this is very helpful
  created_at: 2022-10-28 05:08:19+00:00
  edited: false
  hidden: false
  id: 635b71d3010b9019d0891714
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2022-10-28T08:33:28.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: '<p>If you are interested in the code behind our BLOOM deployment, you
          can find the new version currently running here: <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a>.<br>The
          original code described in the the blog post can also be found here: <a
          rel="nofollow" href="https://github.com/huggingface/transformers_bloom_parallel/">https://github.com/huggingface/transformers_bloom_parallel/</a>.</p>

          '
        raw: 'If you are interested in the code behind our BLOOM deployment, you can
          find the new version currently running here: https://github.com/huggingface/text-generation-inference.

          The original code described in the the blog post can also be found here:
          https://github.com/huggingface/transformers_bloom_parallel/.'
        updatedAt: '2022-10-28T08:33:28.463Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - darragh
    id: 635b93d8e1e7ef214265f78b
    type: comment
  author: olivierdehaene
  content: 'If you are interested in the code behind our BLOOM deployment, you can
    find the new version currently running here: https://github.com/huggingface/text-generation-inference.

    The original code described in the the blog post can also be found here: https://github.com/huggingface/transformers_bloom_parallel/.'
  created_at: 2022-10-28 07:33:28+00:00
  edited: false
  hidden: false
  id: 635b93d8e1e7ef214265f78b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/400e4899333aa0e439289b1460c044f4.svg
      fullname: Philip Jonsen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Johnlate
      type: user
    createdAt: '2022-12-13T06:02:15.000Z'
    data:
      edited: false
      editors:
      - Johnlate
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/400e4899333aa0e439289b1460c044f4.svg
          fullname: Philip Jonsen
          isHf: false
          isPro: false
          name: Johnlate
          type: user
        html: '<p>I have the same problem all night. I just wanted to try it out and
          see if I can get any kind of response, maybe I have to wait or call the
          API myself is working better, anyone has a solution?</p>

          '
        raw: I have the same problem all night. I just wanted to try it out and see
          if I can get any kind of response, maybe I have to wait or call the API
          myself is working better, anyone has a solution?
        updatedAt: '2022-12-13T06:02:15.599Z'
      numEdits: 0
      reactions: []
    id: 63981567ba4b76665d86f4e3
    type: comment
  author: Johnlate
  content: I have the same problem all night. I just wanted to try it out and see
    if I can get any kind of response, maybe I have to wait or call the API myself
    is working better, anyone has a solution?
  created_at: 2022-12-13 06:02:15+00:00
  edited: false
  hidden: false
  id: 63981567ba4b76665d86f4e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
      fullname: Thomas Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: TimeRobber
      type: user
    createdAt: '2022-12-15T16:57:05.000Z'
    data:
      edited: false
      editors:
      - TimeRobber
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d6e4b4d02eda7b5ef28e1cc0fb8e08a.svg
          fullname: Thomas Wang
          isHf: false
          isPro: false
          name: TimeRobber
          type: user
        html: '<p>Hi! Bloom hosting is currently undergoing maintenance by the AzureML
          team and will be back up as soon as this has been completed. We''ll try
          to get it back up ASAP.</p>

          '
        raw: Hi! Bloom hosting is currently undergoing maintenance by the AzureML
          team and will be back up as soon as this has been completed. We'll try to
          get it back up ASAP.
        updatedAt: '2022-12-15T16:57:05.644Z'
      numEdits: 0
      reactions: []
    id: 639b51e103a69e5c5f93b520
    type: comment
  author: TimeRobber
  content: Hi! Bloom hosting is currently undergoing maintenance by the AzureML team
    and will be back up as soon as this has been completed. We'll try to get it back
    up ASAP.
  created_at: 2022-12-15 16:57:05+00:00
  edited: false
  hidden: false
  id: 639b51e103a69e5c5f93b520
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2022-12-16T17:11:53.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: '<p>Model is back up.</p>

          '
        raw: Model is back up.
        updatedAt: '2022-12-16T17:11:53.260Z'
      numEdits: 0
      reactions: []
    id: 639ca6d9b2ef0504502366b8
    type: comment
  author: olivierdehaene
  content: Model is back up.
  created_at: 2022-12-16 17:11:53+00:00
  edited: false
  hidden: false
  id: 639ca6d9b2ef0504502366b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2022-12-16T17:15:21.000Z'
    data:
      edited: false
      editors:
      - cakiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
          fullname: Christopher Akiki
          isHf: false
          isPro: false
          name: cakiki
          type: user
        html: '<p>Indeed! <a rel="nofollow" href="https://twitter.com/julien_c/status/1603797500955181056">https://twitter.com/julien_c/status/1603797500955181056</a></p>

          '
        raw: Indeed! https://twitter.com/julien_c/status/1603797500955181056
        updatedAt: '2022-12-16T17:15:21.872Z'
      numEdits: 0
      reactions: []
      relatedEventId: 639ca7a9b2ef050450237ddb
    id: 639ca7a9b2ef050450237dda
    type: comment
  author: cakiki
  content: Indeed! https://twitter.com/julien_c/status/1603797500955181056
  created_at: 2022-12-16 17:15:21+00:00
  edited: false
  hidden: false
  id: 639ca7a9b2ef050450237dda
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1646492542174-5e70f6048ce3c604d78fe133.jpeg?w=200&h=200&f=face
      fullname: Christopher Akiki
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cakiki
      type: user
    createdAt: '2022-12-16T17:15:21.000Z'
    data:
      status: closed
    id: 639ca7a9b2ef050450237ddb
    type: status-change
  author: cakiki
  created_at: 2022-12-16 17:15:21+00:00
  id: 639ca7a9b2ef050450237ddb
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 70
repo_id: bigscience/bloom
repo_type: model
status: closed
target_branch: null
title: '"Model is overloaded, please wait for a bit"'
