!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BramVanroy
conflicting_files: null
created_at: 2023-10-27 08:34:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594192845975-5e1e17b6fcf41d740b6996a8.jpeg?w=200&h=200&f=face
      fullname: Bram Vanroy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BramVanroy
      type: user
    createdAt: '2023-10-27T09:34:42.000Z'
    data:
      edited: true
      editors:
      - BramVanroy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.218037948012352
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594192845975-5e1e17b6fcf41d740b6996a8.jpeg?w=200&h=200&f=face
          fullname: Bram Vanroy
          isHf: false
          isPro: false
          name: BramVanroy
          type: user
        html: "<p>Using the tokenizer, all my encodings end with <code>&lt;unk&gt;</code>.\
          \ Is this the intended behavior (some kind of undefined end-of-sentence\
          \ token that usually is <code>&lt;/s&gt;</code> like in RobBERT)?</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\
          \ntokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\"\
          >\"FremyCompany/roberta-large-nl-oscar23\"</span>)\nencoded = tokenizer(<span\
          \ class=\"hljs-string\">\"De volgende recensie is positief.\"</span>)\n\
          tokenizer.convert_ids_to_tokens(encoded.input_ids)\n<span class=\"hljs-comment\"\
          ># ['&lt;s&gt;', '\u0120De', '\u0120volgende', '\u0120recensie', '\u0120\
          is', '\u0120positief', '\u0120', '.', '&lt;unk&gt;']</span>\n\nencoded =\
          \ tokenizer(<span class=\"hljs-string\">\"I like cookies!\"</span>)\ntokenizer.convert_ids_to_tokens(encoded.input_ids)\n\
          <span class=\"hljs-comment\"># ['&lt;s&gt;', '\u0120I', '\u0120like', '\u0120\
          cookies', '\u0120', '!', '&lt;unk&gt;']</span>\n\nencoded = tokenizer(<span\
          \ class=\"hljs-string\">\"Ik wil naar huis gaan\"</span>)\ntokenizer.convert_ids_to_tokens(encoded.input_ids)\n\
          <span class=\"hljs-comment\"># ['&lt;s&gt;', '\u0120Ik', '\u0120wil', '\u0120\
          naar', '\u0120huis', '\u0120gaan', '&lt;unk&gt;']</span>\n</code></pre>\n"
        raw: "Using the tokenizer, all my encodings end with `<unk>`. Is this the\
          \ intended behavior (some kind of undefined end-of-sentence token that usually\
          \ is `</s>` like in RobBERT)?\n\n```python\nfrom transformers import AutoTokenizer\n\
          \ntokenizer = AutoTokenizer.from_pretrained(\"FremyCompany/roberta-large-nl-oscar23\"\
          )\nencoded = tokenizer(\"De volgende recensie is positief.\")\ntokenizer.convert_ids_to_tokens(encoded.input_ids)\n\
          # ['<s>', '\u0120De', '\u0120volgende', '\u0120recensie', '\u0120is', '\u0120\
          positief', '\u0120', '.', '<unk>']\n\nencoded = tokenizer(\"I like cookies!\"\
          )\ntokenizer.convert_ids_to_tokens(encoded.input_ids)\n# ['<s>', '\u0120\
          I', '\u0120like', '\u0120cookies', '\u0120', '!', '<unk>']\n\nencoded =\
          \ tokenizer(\"Ik wil naar huis gaan\")\ntokenizer.convert_ids_to_tokens(encoded.input_ids)\n\
          # ['<s>', '\u0120Ik', '\u0120wil', '\u0120naar', '\u0120huis', '\u0120gaan',\
          \ '<unk>']\n```"
        updatedAt: '2023-10-27T15:23:24.475Z'
      numEdits: 2
      reactions: []
    id: 653b8432f1017cf05bb090db
    type: comment
  author: BramVanroy
  content: "Using the tokenizer, all my encodings end with `<unk>`. Is this the intended\
    \ behavior (some kind of undefined end-of-sentence token that usually is `</s>`\
    \ like in RobBERT)?\n\n```python\nfrom transformers import AutoTokenizer\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"FremyCompany/roberta-large-nl-oscar23\")\n\
    encoded = tokenizer(\"De volgende recensie is positief.\")\ntokenizer.convert_ids_to_tokens(encoded.input_ids)\n\
    # ['<s>', '\u0120De', '\u0120volgende', '\u0120recensie', '\u0120is', '\u0120\
    positief', '\u0120', '.', '<unk>']\n\nencoded = tokenizer(\"I like cookies!\"\
    )\ntokenizer.convert_ids_to_tokens(encoded.input_ids)\n# ['<s>', '\u0120I', '\u0120\
    like', '\u0120cookies', '\u0120', '!', '<unk>']\n\nencoded = tokenizer(\"Ik wil\
    \ naar huis gaan\")\ntokenizer.convert_ids_to_tokens(encoded.input_ids)\n# ['<s>',\
    \ '\u0120Ik', '\u0120wil', '\u0120naar', '\u0120huis', '\u0120gaan', '<unk>']\n\
    ```"
  created_at: 2023-10-27 08:34:42+00:00
  edited: true
  hidden: false
  id: 653b8432f1017cf05bb090db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1647276617786-5f04e8865d08220171a0ad3f.png?w=200&h=200&f=face
      fullname: "Fran\xE7ois Remy"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: FremyCompany
      type: user
    createdAt: '2023-10-27T10:28:50.000Z'
    data:
      edited: true
      editors:
      - FremyCompany
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9681445956230164
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1647276617786-5f04e8865d08220171a0ad3f.png?w=200&h=200&f=face
          fullname: "Fran\xE7ois Remy"
          isHf: false
          isPro: false
          name: FremyCompany
          type: user
        html: "<p>Yes, there is an EOS token, but it was not supposed to be <code>&lt;unk&gt;</code>\
          \ but <code>&lt;/s&gt;</code>. The tokenizer was trained by Pieter, but\
          \ I had to reorder the tokens due to a bug the Transformers librairy, so\
          \ I might have introduced an inconsistency :/</p>\n<p>I took a quick look,\
          \ and this looks like this was caused an off-by-one error in the <code>tokenizer.json</code>\
          \ config file.</p>\n<pre><code>  \"post_processor\": {\n    \"type\": \"\
          RobertaProcessing\",\n    \"sep\": [\n      \"&lt;/s&gt;\",\n      2\n \
          \   ],\n</code></pre>\n<p>The number 2 is not correct, it should be 3 per\
          \ the vocab.</p>\n<pre><code>    \"vocab\": {\n      \"&lt;s&gt;\": 0,\n\
          \      \"&lt;pad&gt;\": 1,\n      \"&lt;unk&gt;\": 2,\n      \"&lt;/s&gt;\"\
          : 3,\n      \"&lt;mask&gt;\": 4,\n</code></pre>\n<p>That said, the model\
          \ has been trained with this config now, maybe it's best not to change anything?</p>\n"
        raw: "Yes, there is an EOS token, but it was not supposed to be `<unk>` but\
          \ `</s>`. The tokenizer was trained by Pieter, but I had to reorder the\
          \ tokens due to a bug the Transformers librairy, so I might have introduced\
          \ an inconsistency :/\n\nI took a quick look, and this looks like this was\
          \ caused an off-by-one error in the `tokenizer.json` config file.\n\n```\n\
          \  \"post_processor\": {\n    \"type\": \"RobertaProcessing\",\n    \"sep\"\
          : [\n      \"</s>\",\n      2\n    ],\n```\n\nThe number 2 is not correct,\
          \ it should be 3 per the vocab.\n\n```\n    \"vocab\": {\n      \"<s>\"\
          : 0,\n      \"<pad>\": 1,\n      \"<unk>\": 2,\n      \"</s>\": 3,\n   \
          \   \"<mask>\": 4,\n```\n\nThat said, the model has been trained with this\
          \ config now, maybe it's best not to change anything?"
        updatedAt: '2023-10-27T10:29:15.429Z'
      numEdits: 1
      reactions: []
    id: 653b90e2dad1e24b2c51c8e7
    type: comment
  author: FremyCompany
  content: "Yes, there is an EOS token, but it was not supposed to be `<unk>` but\
    \ `</s>`. The tokenizer was trained by Pieter, but I had to reorder the tokens\
    \ due to a bug the Transformers librairy, so I might have introduced an inconsistency\
    \ :/\n\nI took a quick look, and this looks like this was caused an off-by-one\
    \ error in the `tokenizer.json` config file.\n\n```\n  \"post_processor\": {\n\
    \    \"type\": \"RobertaProcessing\",\n    \"sep\": [\n      \"</s>\",\n     \
    \ 2\n    ],\n```\n\nThe number 2 is not correct, it should be 3 per the vocab.\n\
    \n```\n    \"vocab\": {\n      \"<s>\": 0,\n      \"<pad>\": 1,\n      \"<unk>\"\
    : 2,\n      \"</s>\": 3,\n      \"<mask>\": 4,\n```\n\nThat said, the model has\
    \ been trained with this config now, maybe it's best not to change anything?"
  created_at: 2023-10-27 09:28:50+00:00
  edited: true
  hidden: false
  id: 653b90e2dad1e24b2c51c8e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1647276617786-5f04e8865d08220171a0ad3f.png?w=200&h=200&f=face
      fullname: "Fran\xE7ois Remy"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: FremyCompany
      type: user
    createdAt: '2023-10-27T10:34:46.000Z'
    data:
      edited: true
      editors:
      - FremyCompany
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9509758353233337
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1647276617786-5f04e8865d08220171a0ad3f.png?w=200&h=200&f=face
          fullname: "Fran\xE7ois Remy"
          isHf: false
          isPro: false
          name: FremyCompany
          type: user
        html: '<p>I''ll talk to Pieter about this, I don''t think this changes much
          for the MLM pre-trained model, but maybe this can cause problems for NLI-type
          finetuning. An easy fix would be to fix the tokenizer for the final release,
          and copy the weights of <code>&lt;unk&gt;</code> to those of <code>&lt;/s&gt;</code>.
          This will yield the exact same results, but by doing this, we allow the
          model to finetune <code>&lt;/s&gt;</code> without affecting the <code>&lt;unk&gt;</code>
          token for tasks where this is relevant.</p>

          <p>Very good catch, thanks!</p>

          '
        raw: 'I''ll talk to Pieter about this, I don''t think this changes much for
          the MLM pre-trained model, but maybe this can cause problems for NLI-type
          finetuning. An easy fix would be to fix the tokenizer for the final release,
          and copy the weights of `<unk>` to those of `</s>`. This will yield the
          exact same results, but by doing this, we allow the model to finetune `</s>`
          without affecting the `<unk>` token for tasks where this is relevant.


          Very good catch, thanks!'
        updatedAt: '2023-10-27T10:35:19.845Z'
      numEdits: 1
      reactions: []
    id: 653b9246a5d883765621bcde
    type: comment
  author: FremyCompany
  content: 'I''ll talk to Pieter about this, I don''t think this changes much for
    the MLM pre-trained model, but maybe this can cause problems for NLI-type finetuning.
    An easy fix would be to fix the tokenizer for the final release, and copy the
    weights of `<unk>` to those of `</s>`. This will yield the exact same results,
    but by doing this, we allow the model to finetune `</s>` without affecting the
    `<unk>` token for tasks where this is relevant.


    Very good catch, thanks!'
  created_at: 2023-10-27 09:34:46+00:00
  edited: true
  hidden: false
  id: 653b9246a5d883765621bcde
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1647276617786-5f04e8865d08220171a0ad3f.png?w=200&h=200&f=face
      fullname: "Fran\xE7ois Remy"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: FremyCompany
      type: user
    createdAt: '2023-11-30T13:33:09.000Z'
    data:
      edited: false
      editors:
      - FremyCompany
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9533417820930481
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1647276617786-5f04e8865d08220171a0ad3f.png?w=200&h=200&f=face
          fullname: "Fran\xE7ois Remy"
          isHf: false
          isPro: false
          name: FremyCompany
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;BramVanroy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/BramVanroy\"\
          >@<span class=\"underline\">BramVanroy</span></a></span>\n\n\t</span></span>\
          \ for letting us know about this issue! I just pushed an update that fixes\
          \ the tokenizer to use <code>&lt;/s&gt;</code> as was originally intended.\
          \ This update will not affect the outputs of the model in any way (even\
          \ if it is accidentally used with the old version of the tokenizer).</p>\n\
          <p>However, the old model should however not be used with the new tokenizer,\
          \ but I don't think this combination is likely. We are also going to migrate\
          \ this model to the usual repository of RobBERT models, so new users will\
          \ get a clean start.</p>\n"
        raw: 'Thank you @BramVanroy for letting us know about this issue! I just pushed
          an update that fixes the tokenizer to use `</s>` as was originally intended.
          This update will not affect the outputs of the model in any way (even if
          it is accidentally used with the old version of the tokenizer).


          However, the old model should however not be used with the new tokenizer,
          but I don''t think this combination is likely. We are also going to migrate
          this model to the usual repository of RobBERT models, so new users will
          get a clean start.'
        updatedAt: '2023-11-30T13:33:09.797Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - BramVanroy
      relatedEventId: 65688f1526d6f74919fe1104
    id: 65688f1526d6f74919fe1102
    type: comment
  author: FremyCompany
  content: 'Thank you @BramVanroy for letting us know about this issue! I just pushed
    an update that fixes the tokenizer to use `</s>` as was originally intended. This
    update will not affect the outputs of the model in any way (even if it is accidentally
    used with the old version of the tokenizer).


    However, the old model should however not be used with the new tokenizer, but
    I don''t think this combination is likely. We are also going to migrate this model
    to the usual repository of RobBERT models, so new users will get a clean start.'
  created_at: 2023-11-30 13:33:09+00:00
  edited: false
  hidden: false
  id: 65688f1526d6f74919fe1102
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1647276617786-5f04e8865d08220171a0ad3f.png?w=200&h=200&f=face
      fullname: "Fran\xE7ois Remy"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: FremyCompany
      type: user
    createdAt: '2023-11-30T13:33:09.000Z'
    data:
      status: closed
    id: 65688f1526d6f74919fe1104
    type: status-change
  author: FremyCompany
  created_at: 2023-11-30 13:33:09+00:00
  id: 65688f1526d6f74919fe1104
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: FremyCompany/roberta-large-nl-oscar23
repo_type: model
status: closed
target_branch: null
title: Tokenizer <unk>s
