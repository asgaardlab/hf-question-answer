!!python/object:huggingface_hub.community.DiscussionWithDetails
author: shadowthecat1918
conflicting_files: null
created_at: 2023-06-22 23:53:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d656c1ebfbe870d35130eeb48faed2a4.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shadowthecat1918
      type: user
    createdAt: '2023-06-23T00:53:03.000Z'
    data:
      edited: false
      editors:
      - shadowthecat1918
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9629185199737549
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d656c1ebfbe870d35130eeb48faed2a4.svg
          fullname: David
          isHf: false
          isPro: false
          name: shadowthecat1918
          type: user
        html: '<p>If I understand correctly, 4-bit, 8-bit, and X-bit systems are developed
          to run on hardware more accessible to the general public. Is that correct?
          What is GGML and is it also meant to boost performance for the general public?</p>

          '
        raw: If I understand correctly, 4-bit, 8-bit, and X-bit systems are developed
          to run on hardware more accessible to the general public. Is that correct?
          What is GGML and is it also meant to boost performance for the general public?
        updatedAt: '2023-06-23T00:53:03.852Z'
      numEdits: 0
      reactions: []
    id: 6494ecefe7ed8fa81c61c5d0
    type: comment
  author: shadowthecat1918
  content: If I understand correctly, 4-bit, 8-bit, and X-bit systems are developed
    to run on hardware more accessible to the general public. Is that correct? What
    is GGML and is it also meant to boost performance for the general public?
  created_at: 2023-06-22 23:53:03+00:00
  edited: false
  hidden: false
  id: 6494ecefe7ed8fa81c61c5d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-23T20:49:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9750417470932007
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes that''s correct.</p>

          <p>GGML is a model format developed by a guy called Georgi Gerganov. It''s
          based around C++ code, rather than the Python code that powers Hugging Face
          transformers, GPTQ, and most other inference methods.</p>

          <p>GGML supports unquantised inference, but it''s almost always used with
          quantised models, in 2, 3, 4, 5, 6 or 8-bit. 4-bit being most common.</p>

          <p>GGML has always been able to run on smaller hardware than other formats
          as it runs far better on CPU than other formats. But recently it has also
          gained decent GPU acceleration, meaning it''s also now starting to be competitive
          on performance as well.</p>

          '
        raw: 'Yes that''s correct.


          GGML is a model format developed by a guy called Georgi Gerganov. It''s
          based around C++ code, rather than the Python code that powers Hugging Face
          transformers, GPTQ, and most other inference methods.


          GGML supports unquantised inference, but it''s almost always used with quantised
          models, in 2, 3, 4, 5, 6 or 8-bit. 4-bit being most common.


          GGML has always been able to run on smaller hardware than other formats
          as it runs far better on CPU than other formats. But recently it has also
          gained decent GPU acceleration, meaning it''s also now starting to be competitive
          on performance as well.'
        updatedAt: '2023-06-23T20:49:06.228Z'
      numEdits: 0
      reactions: []
    id: 649605421a0072545d034b57
    type: comment
  author: TheBloke
  content: 'Yes that''s correct.


    GGML is a model format developed by a guy called Georgi Gerganov. It''s based
    around C++ code, rather than the Python code that powers Hugging Face transformers,
    GPTQ, and most other inference methods.


    GGML supports unquantised inference, but it''s almost always used with quantised
    models, in 2, 3, 4, 5, 6 or 8-bit. 4-bit being most common.


    GGML has always been able to run on smaller hardware than other formats as it
    runs far better on CPU than other formats. But recently it has also gained decent
    GPU acceleration, meaning it''s also now starting to be competitive on performance
    as well.'
  created_at: 2023-06-23 19:49:06+00:00
  edited: false
  hidden: false
  id: 649605421a0072545d034b57
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/MPT-7B-Storywriter-GGML
repo_type: model
status: open
target_branch: null
title: X-bit Question
