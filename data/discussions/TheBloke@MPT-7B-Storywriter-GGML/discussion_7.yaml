!!python/object:huggingface_hub.community.DiscussionWithDetails
author: StableDiffusion69
conflicting_files: null
created_at: 2023-07-03 06:40:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
      fullname: Diffusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StableDiffusion69
      type: user
    createdAt: '2023-07-03T07:40:41.000Z'
    data:
      edited: false
      editors:
      - StableDiffusion69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4253585636615753
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
          fullname: Diffusion
          isHf: false
          isPro: false
          name: StableDiffusion69
          type: user
        html: "<p>When I try to load this model in ooba with my 8GB VRAM card, I get\
          \ the following:</p>\n<p>Traceback (most recent call last): File \u201C\
          F:\\Programme\\oobabooga_windows\\text-generation-webui\\server.py\u201D\
          , line 67, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader) File \u201CF:\\Programme\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 74, in load_model output = load_func_maploader\
          \ File \u201CF:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 255, in llamacpp_loader model, tokenizer = LlamaCppModel.from_pretrained(model_file)\
          \ File \u201CF:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\\
          llamacpp_model.py\u201D, line 55, in from_pretrained result.model = Llama(**params)\
          \ File \u201CF:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\llama_cpp\\llama.py\u201D, line 289, in init assert self.ctx\
          \ is not None AssertionError</p>\n<p>Console says:<br>2023-07-03 08:38:29\
          \ INFO:Loading TheBloke_MPT-7B-Storywriter-GGML...<br>2023-07-03 08:38:29\
          \ INFO:llama.cpp weights detected: models\\TheBloke_MPT-7B-Storywriter-GGML\\\
          mpt-7b-storywriter.ggmlv3.q5_1.bin</p>\n<p>2023-07-03 08:38:29 INFO:Cache\
          \ capacity is 0 bytes<br>llama.cpp: loading model from models\\TheBloke_MPT-7B-Storywriter-GGML\\\
          mpt-7b-storywriter.ggmlv3.q5_1.bin<br>error loading model: llama.cpp: tensor\
          \ 'rus' should not be 7-dimensional<br>llama_init_from_file: failed to load\
          \ model<br>2023-07-03 08:38:30 ERROR:Failed to load the model.<br>Traceback\
          \ (most recent call last):<br>  File \"F:\\Programme\\oobabooga_windows\\\
          text-generation-webui\\server.py\", line 67, in load_model_wrapper<br> \
          \   shared.model, shared.tokenizer = load_model(shared.model_name, loader)<br>\
          \  File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\", line 74, in load_model<br>    output = load_func_map<a rel=\"\
          nofollow\" href=\"model_name\">loader</a><br>  File \"F:\\Programme\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 255, in llamacpp_loader<br>\
          \    model, tokenizer = LlamaCppModel.from_pretrained(model_file)<br>  File\
          \ \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\llamacpp_model.py\"\
          , line 55, in from_pretrained<br>    result.model = Llama(**params)<br>\
          \  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\llama_cpp\\llama.py\", line 289, in <strong>init</strong><br>\
          \    assert self.ctx is not None<br>AssertionError</p>\n<p>Exception ignored\
          \ in: &lt;function LlamaCppModel.__del__ at 0x00000270ED8DCA60&gt;<br>Traceback\
          \ (most recent call last):<br>  File \"F:\\Programme\\oobabooga_windows\\\
          text-generation-webui\\modules\\llamacpp_model.py\", line 29, in <strong>del</strong><br>\
          \    self.model.<strong>del</strong>()<br>AttributeError: 'LlamaCppModel'\
          \ object has no attribute 'model'</p>\n<p>What are my chances, please? \U0001F914\
          </p>\n"
        raw: "When I try to load this model in ooba with my 8GB VRAM card, I get the\
          \ following:\r\n\r\nTraceback (most recent call last): File \u201CF:\\Programme\\\
          oobabooga_windows\\text-generation-webui\\server.py\u201D, line 67, in load_model_wrapper\
          \ shared.model, shared.tokenizer = load_model(shared.model_name, loader)\
          \ File \u201CF:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 74, in load_model output = load_func_maploader File\
          \ \u201CF:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 255, in llamacpp_loader model, tokenizer = LlamaCppModel.from_pretrained(model_file)\
          \ File \u201CF:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\\
          llamacpp_model.py\u201D, line 55, in from_pretrained result.model = Llama(**params)\
          \ File \u201CF:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\llama_cpp\\llama.py\u201D, line 289, in init assert self.ctx\
          \ is not None AssertionError\r\n\r\nConsole says:\r\n2023-07-03 08:38:29\
          \ INFO:Loading TheBloke_MPT-7B-Storywriter-GGML...\r\n2023-07-03 08:38:29\
          \ INFO:llama.cpp weights detected: models\\TheBloke_MPT-7B-Storywriter-GGML\\\
          mpt-7b-storywriter.ggmlv3.q5_1.bin\r\n\r\n2023-07-03 08:38:29 INFO:Cache\
          \ capacity is 0 bytes\r\nllama.cpp: loading model from models\\TheBloke_MPT-7B-Storywriter-GGML\\\
          mpt-7b-storywriter.ggmlv3.q5_1.bin\r\nerror loading model: llama.cpp: tensor\
          \ 'rus' should not be 7-dimensional\r\nllama_init_from_file: failed to load\
          \ model\r\n2023-07-03 08:38:30 ERROR:Failed to load the model.\r\nTraceback\
          \ (most recent call last):\r\n  File \"F:\\Programme\\oobabooga_windows\\\
          text-generation-webui\\server.py\", line 67, in load_model_wrapper\r\n \
          \   shared.model, shared.tokenizer = load_model(shared.model_name, loader)\r\
          \n  File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\", line 74, in load_model\r\n    output = load_func_map[loader](model_name)\r\
          \n  File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\", line 255, in llamacpp_loader\r\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
          \n  File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\\
          llamacpp_model.py\", line 55, in from_pretrained\r\n    result.model = Llama(**params)\r\
          \n  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\llama_cpp\\llama.py\", line 289, in __init__\r\n    assert\
          \ self.ctx is not None\r\nAssertionError\r\n\r\nException ignored in: <function\
          \ LlamaCppModel.__del__ at 0x00000270ED8DCA60>\r\nTraceback (most recent\
          \ call last):\r\n  File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\\
          modules\\llamacpp_model.py\", line 29, in __del__\r\n    self.model.__del__()\r\
          \nAttributeError: 'LlamaCppModel' object has no attribute 'model'\r\n\r\n\
          What are my chances, please? \U0001F914"
        updatedAt: '2023-07-03T07:40:41.059Z'
      numEdits: 0
      reactions: []
    id: 64a27b79eacb4b50ba34b28f
    type: comment
  author: StableDiffusion69
  content: "When I try to load this model in ooba with my 8GB VRAM card, I get the\
    \ following:\r\n\r\nTraceback (most recent call last): File \u201CF:\\Programme\\\
    oobabooga_windows\\text-generation-webui\\server.py\u201D, line 67, in load_model_wrapper\
    \ shared.model, shared.tokenizer = load_model(shared.model_name, loader) File\
    \ \u201CF:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
    , line 74, in load_model output = load_func_maploader File \u201CF:\\Programme\\\
    oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line 255,\
    \ in llamacpp_loader model, tokenizer = LlamaCppModel.from_pretrained(model_file)\
    \ File \u201CF:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\\
    llamacpp_model.py\u201D, line 55, in from_pretrained result.model = Llama(**params)\
    \ File \u201CF:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    llama_cpp\\llama.py\u201D, line 289, in init assert self.ctx is not None AssertionError\r\
    \n\r\nConsole says:\r\n2023-07-03 08:38:29 INFO:Loading TheBloke_MPT-7B-Storywriter-GGML...\r\
    \n2023-07-03 08:38:29 INFO:llama.cpp weights detected: models\\TheBloke_MPT-7B-Storywriter-GGML\\\
    mpt-7b-storywriter.ggmlv3.q5_1.bin\r\n\r\n2023-07-03 08:38:29 INFO:Cache capacity\
    \ is 0 bytes\r\nllama.cpp: loading model from models\\TheBloke_MPT-7B-Storywriter-GGML\\\
    mpt-7b-storywriter.ggmlv3.q5_1.bin\r\nerror loading model: llama.cpp: tensor 'rus'\
    \ should not be 7-dimensional\r\nllama_init_from_file: failed to load model\r\n\
    2023-07-03 08:38:30 ERROR:Failed to load the model.\r\nTraceback (most recent\
    \ call last):\r\n  File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\\
    server.py\", line 67, in load_model_wrapper\r\n    shared.model, shared.tokenizer\
    \ = load_model(shared.model_name, loader)\r\n  File \"F:\\Programme\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\", line 74, in load_model\r\n    output\
    \ = load_func_map[loader](model_name)\r\n  File \"F:\\Programme\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\", line 255, in llamacpp_loader\r\n\
    \    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\n  File \"\
    F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\llamacpp_model.py\"\
    , line 55, in from_pretrained\r\n    result.model = Llama(**params)\r\n  File\
    \ \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    llama_cpp\\llama.py\", line 289, in __init__\r\n    assert self.ctx is not None\r\
    \nAssertionError\r\n\r\nException ignored in: <function LlamaCppModel.__del__\
    \ at 0x00000270ED8DCA60>\r\nTraceback (most recent call last):\r\n  File \"F:\\\
    Programme\\oobabooga_windows\\text-generation-webui\\modules\\llamacpp_model.py\"\
    , line 29, in __del__\r\n    self.model.__del__()\r\nAttributeError: 'LlamaCppModel'\
    \ object has no attribute 'model'\r\n\r\nWhat are my chances, please? \U0001F914"
  created_at: 2023-07-03 06:40:41+00:00
  edited: false
  hidden: false
  id: 64a27b79eacb4b50ba34b28f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-03T07:43:17.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8009482026100159
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It''s not supported in text-generation-webui.  This is an older
          MPT model where I didn''t list all the UIs that are compatible, but check
          my recent MPT-7B-Chat GGML for a list of UIs it does work with: <a href="https://huggingface.co/TheBloke/mpt-7b-chat-GGML">https://huggingface.co/TheBloke/mpt-7b-chat-GGML</a></p>

          <p>I''ll update this README soon</p>

          '
        raw: 'It''s not supported in text-generation-webui.  This is an older MPT
          model where I didn''t list all the UIs that are compatible, but check my
          recent MPT-7B-Chat GGML for a list of UIs it does work with: https://huggingface.co/TheBloke/mpt-7b-chat-GGML


          I''ll update this README soon'
        updatedAt: '2023-07-03T07:43:17.065Z'
      numEdits: 0
      reactions: []
    id: 64a27c15f824e15453de0a08
    type: comment
  author: TheBloke
  content: 'It''s not supported in text-generation-webui.  This is an older MPT model
    where I didn''t list all the UIs that are compatible, but check my recent MPT-7B-Chat
    GGML for a list of UIs it does work with: https://huggingface.co/TheBloke/mpt-7b-chat-GGML


    I''ll update this README soon'
  created_at: 2023-07-03 06:43:17+00:00
  edited: false
  hidden: false
  id: 64a27c15f824e15453de0a08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
      fullname: Diffusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StableDiffusion69
      type: user
    createdAt: '2023-07-03T13:42:52.000Z'
    data:
      edited: false
      editors:
      - StableDiffusion69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.769455075263977
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
          fullname: Diffusion
          isHf: false
          isPro: false
          name: StableDiffusion69
          type: user
        html: "<blockquote>\n<p>It's not supported in text-generation-webui.  This\
          \ is an older MPT model where I didn't list all the UIs that are compatible,\
          \ but check my recent MPT-7B-Chat GGML for a list of UIs it does work with:\
          \ <a href=\"https://huggingface.co/TheBloke/mpt-7b-chat-GGML\">https://huggingface.co/TheBloke/mpt-7b-chat-GGML</a></p>\n\
          <p>I'll update this README soon</p>\n</blockquote>\n<p>Thanks for this info.\
          \ Unfortunately, the model you linked, isn't compatible, too.<br>It says:<br>These\
          \ files are not compatible with text-generation-webui, llama.cpp, or llama-cpp-python.<br>Too\
          \ bad ... \U0001F60C</p>\n"
        raw: "> It's not supported in text-generation-webui.  This is an older MPT\
          \ model where I didn't list all the UIs that are compatible, but check my\
          \ recent MPT-7B-Chat GGML for a list of UIs it does work with: https://huggingface.co/TheBloke/mpt-7b-chat-GGML\n\
          > \n> I'll update this README soon\n\nThanks for this info. Unfortunately,\
          \ the model you linked, isn't compatible, too.\nIt says:\nThese files are\
          \ not compatible with text-generation-webui, llama.cpp, or llama-cpp-python.\n\
          Too bad ... \U0001F60C"
        updatedAt: '2023-07-03T13:42:52.295Z'
      numEdits: 0
      reactions: []
    id: 64a2d05ced79432b347c76c4
    type: comment
  author: StableDiffusion69
  content: "> It's not supported in text-generation-webui.  This is an older MPT model\
    \ where I didn't list all the UIs that are compatible, but check my recent MPT-7B-Chat\
    \ GGML for a list of UIs it does work with: https://huggingface.co/TheBloke/mpt-7b-chat-GGML\n\
    > \n> I'll update this README soon\n\nThanks for this info. Unfortunately, the\
    \ model you linked, isn't compatible, too.\nIt says:\nThese files are not compatible\
    \ with text-generation-webui, llama.cpp, or llama-cpp-python.\nToo bad ... \U0001F60C"
  created_at: 2023-07-03 12:42:52+00:00
  edited: false
  hidden: false
  id: 64a2d05ced79432b347c76c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-03T14:10:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9262955784797668
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>No that''s right none of the MPTs are. I just meant look at hte
          README and it shows some UIs that are compatible, like KoboldCpp  and LoLLMS-WebUI</p>

          '
        raw: No that's right none of the MPTs are. I just meant look at hte README
          and it shows some UIs that are compatible, like KoboldCpp  and LoLLMS-WebUI
        updatedAt: '2023-07-03T14:10:16.447Z'
      numEdits: 0
      reactions: []
    id: 64a2d6c8ea94f2190beed44b
    type: comment
  author: TheBloke
  content: No that's right none of the MPTs are. I just meant look at hte README and
    it shows some UIs that are compatible, like KoboldCpp  and LoLLMS-WebUI
  created_at: 2023-07-03 13:10:16+00:00
  edited: false
  hidden: false
  id: 64a2d6c8ea94f2190beed44b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
      fullname: Diffusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StableDiffusion69
      type: user
    createdAt: '2023-07-03T17:36:37.000Z'
    data:
      edited: false
      editors:
      - StableDiffusion69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9032192826271057
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
          fullname: Diffusion
          isHf: false
          isPro: false
          name: StableDiffusion69
          type: user
        html: "<p>Ah, OK, I misunderstood what you have meant. OK.<br>Any chance we\
          \ get some models working with text-genearation-webui some time ... ? \U0001F64F\
          </p>\n"
        raw: "Ah, OK, I misunderstood what you have meant. OK.\nAny chance we get\
          \ some models working with text-genearation-webui some time ... ? \U0001F64F"
        updatedAt: '2023-07-03T17:36:37.131Z'
      numEdits: 0
      reactions: []
    id: 64a30725d9dd1da3507ff569
    type: comment
  author: StableDiffusion69
  content: "Ah, OK, I misunderstood what you have meant. OK.\nAny chance we get some\
    \ models working with text-genearation-webui some time ... ? \U0001F64F"
  created_at: 2023-07-03 16:36:37+00:00
  edited: false
  hidden: false
  id: 64a30725d9dd1da3507ff569
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-03T17:44:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9350444674491882
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Don''t know - that depends on whether text-generation-webui adds
          support for a backend that supports MPT GGML models.  There''s no sign of
          that happening right now, but most likely if someone submitted the code
          necessary for text-generation-webui to support them, oobabooga (the creator
          of text-gen-ui) would include it.  So it depends on someone wanting to write
          that.</p>

          <p>There''s a backend called ctransformers that supports all GGML format
          models include MPT, so if support for that was added to text-generation-webui,
          then it would work.</p>

          <p>In the meantime, KoboldCpp and LoLLMS-WebUI are both good UIs.  KoboldCpp
          even supports GPU acceleration with MPT GGML models.</p>

          <p>Alternatively, if you have an NVidia GPU you could try a GPTQ of MPT
          7B Storywriter. I haven''t made one myself, but there is one available on
          Hugging Face which should work with text-generation-webui using the GPTQ-for-LLaMa
          loader (it won''t work with ExLlama or AutoGPTQ).</p>

          '
        raw: 'Don''t know - that depends on whether text-generation-webui adds support
          for a backend that supports MPT GGML models.  There''s no sign of that happening
          right now, but most likely if someone submitted the code necessary for text-generation-webui
          to support them, oobabooga (the creator of text-gen-ui) would include it.  So
          it depends on someone wanting to write that.


          There''s a backend called ctransformers that supports all GGML format models
          include MPT, so if support for that was added to text-generation-webui,
          then it would work.


          In the meantime, KoboldCpp and LoLLMS-WebUI are both good UIs.  KoboldCpp
          even supports GPU acceleration with MPT GGML models.


          Alternatively, if you have an NVidia GPU you could try a GPTQ of MPT 7B
          Storywriter. I haven''t made one myself, but there is one available on Hugging
          Face which should work with text-generation-webui using the GPTQ-for-LLaMa
          loader (it won''t work with ExLlama or AutoGPTQ).'
        updatedAt: '2023-07-03T17:44:53.011Z'
      numEdits: 0
      reactions: []
    id: 64a309154c758629715981c7
    type: comment
  author: TheBloke
  content: 'Don''t know - that depends on whether text-generation-webui adds support
    for a backend that supports MPT GGML models.  There''s no sign of that happening
    right now, but most likely if someone submitted the code necessary for text-generation-webui
    to support them, oobabooga (the creator of text-gen-ui) would include it.  So
    it depends on someone wanting to write that.


    There''s a backend called ctransformers that supports all GGML format models include
    MPT, so if support for that was added to text-generation-webui, then it would
    work.


    In the meantime, KoboldCpp and LoLLMS-WebUI are both good UIs.  KoboldCpp even
    supports GPU acceleration with MPT GGML models.


    Alternatively, if you have an NVidia GPU you could try a GPTQ of MPT 7B Storywriter.
    I haven''t made one myself, but there is one available on Hugging Face which should
    work with text-generation-webui using the GPTQ-for-LLaMa loader (it won''t work
    with ExLlama or AutoGPTQ).'
  created_at: 2023-07-03 16:44:53+00:00
  edited: false
  hidden: false
  id: 64a309154c758629715981c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
      fullname: Diffusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StableDiffusion69
      type: user
    createdAt: '2023-07-03T19:28:49.000Z'
    data:
      edited: false
      editors:
      - StableDiffusion69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9828491806983948
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
          fullname: Diffusion
          isHf: false
          isPro: false
          name: StableDiffusion69
          type: user
        html: "<p>Oh, great! Many thanks for this information. I will look into that.<br>And\
          \ many thanks for your great work you do with those models. We all really\
          \ appreciate it. \U0001F44D</p>\n"
        raw: "Oh, great! Many thanks for this information. I will look into that.\n\
          And many thanks for your great work you do with those models. We all really\
          \ appreciate it. \U0001F44D"
        updatedAt: '2023-07-03T19:28:49.983Z'
      numEdits: 0
      reactions: []
    id: 64a32171171d9fe0062e7ba9
    type: comment
  author: StableDiffusion69
  content: "Oh, great! Many thanks for this information. I will look into that.\n\
    And many thanks for your great work you do with those models. We all really appreciate\
    \ it. \U0001F44D"
  created_at: 2023-07-03 18:28:49+00:00
  edited: false
  hidden: false
  id: 64a32171171d9fe0062e7ba9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/MPT-7B-Storywriter-GGML
repo_type: model
status: open
target_branch: null
title: Failed to load the model mpt-7b-storywriter.ggmlv3.q5_1.bin
