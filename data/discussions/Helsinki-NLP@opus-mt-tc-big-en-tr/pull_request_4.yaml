!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sgugger
conflicting_files: []
created_at: 2023-07-21 14:48:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
      fullname: Sylvain Gugger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgugger
      type: user
    createdAt: '2023-07-21T15:48:25.000Z'
    data:
      edited: false
      editors:
      - sgugger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9798444509506226
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
          fullname: Sylvain Gugger
          isHf: false
          isPro: false
          name: sgugger
          type: user
        html: '<p>There was probably a bug in the initial conversion script that created
          those models, as the weights they have have a<br>different value for lm_head.weight
          and model.decoder.embed_tokens.weight. Those models are tied though.</p>

          <p>This was not a problem until now as the model was tied after the load
          and the (wrong) value of lm_head.weight was<br>replaced by the value of
          model.decoder.embed_tokens.weight. This does not work any more if we tie
          the weights before<br>the load however, as the value picked might be the
          one from lm_head.weight depending on how the models are tied.<br>As far
          as I can see, the model stop generating properly on Transformers main.</p>

          <p>This should fix the bug without any side effect.</p>

          '
        raw: 'There was probably a bug in the initial conversion script that created
          those models, as the weights they have have a

          different value for lm_head.weight and model.decoder.embed_tokens.weight.
          Those models are tied though.


          This was not a problem until now as the model was tied after the load and
          the (wrong) value of lm_head.weight was

          replaced by the value of model.decoder.embed_tokens.weight. This does not
          work any more if we tie the weights before

          the load however, as the value picked might be the one from lm_head.weight
          depending on how the models are tied.

          As far as I can see, the model stop generating properly on Transformers
          main.


          This should fix the bug without any side effect.'
        updatedAt: '2023-07-21T15:48:25.728Z'
      numEdits: 0
      reactions: []
    id: 64baa8c9c27e16b83fb1101e
    type: comment
  author: sgugger
  content: 'There was probably a bug in the initial conversion script that created
    those models, as the weights they have have a

    different value for lm_head.weight and model.decoder.embed_tokens.weight. Those
    models are tied though.


    This was not a problem until now as the model was tied after the load and the
    (wrong) value of lm_head.weight was

    replaced by the value of model.decoder.embed_tokens.weight. This does not work
    any more if we tie the weights before

    the load however, as the value picked might be the one from lm_head.weight depending
    on how the models are tied.

    As far as I can see, the model stop generating properly on Transformers main.


    This should fix the bug without any side effect.'
  created_at: 2023-07-21 14:48:25+00:00
  edited: false
  hidden: false
  id: 64baa8c9c27e16b83fb1101e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
      fullname: Sylvain Gugger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgugger
      type: user
    createdAt: '2023-07-21T15:48:26.000Z'
    data:
      oid: 9eada10761668226d87fc69d44fd4fc88aa96ccf
      parents:
      - 07b3aac86a02597fec8f3883c03c50bc6cef272c
      subject: Fix weights by putting the right value in `lm_head.weight`
    id: 64baa8ca0000000000000000
    type: commit
  author: sgugger
  created_at: 2023-07-21 14:48:26+00:00
  id: 64baa8ca0000000000000000
  oid: 9eada10761668226d87fc69d44fd4fc88aa96ccf
  summary: Fix weights by putting the right value in `lm_head.weight`
  type: commit
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
      fullname: Sylvain Gugger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgugger
      type: user
    createdAt: '2023-07-21T15:50:00.000Z'
    data:
      status: merged
    id: 64baa92812afb2f119f18cbf
    type: status-change
  author: sgugger
  created_at: 2023-07-21 14:50:00+00:00
  id: 64baa92812afb2f119f18cbf
  new_status: merged
  type: status-change
is_pull_request: true
merge_commit_oid: 8dfb16221c84af408f4f08618f4e1df9ec8a172f
num: 4
repo_id: Helsinki-NLP/opus-mt-tc-big-en-tr
repo_type: model
status: merged
target_branch: refs/heads/main
title: Fix weights by putting the right value in `lm_head.weight`
