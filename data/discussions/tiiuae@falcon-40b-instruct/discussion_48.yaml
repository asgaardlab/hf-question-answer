!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mallorbc
conflicting_files: null
created_at: 2023-06-20 20:31:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c77c43bb70ad017bfa52c4caa0703a45.svg
      fullname: Blake Mallory
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mallorbc
      type: user
    createdAt: '2023-06-20T21:31:04.000Z'
    data:
      edited: false
      editors:
      - mallorbc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9444241523742676
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c77c43bb70ad017bfa52c4caa0703a45.svg
          fullname: Blake Mallory
          isHf: false
          isPro: false
          name: mallorbc
          type: user
        html: '<p>I looked at the modeling code, and it seems that the code supports
          Alibi positional encoding as well as rotary.  See the Alibi paper: <a rel="nofollow"
          href="https://arxiv.org/pdf/2108.12409.pdf">https://arxiv.org/pdf/2108.12409.pdf</a></p>

          <p>Alibi allows one to train a model on shorter sequences and inference
          on longer sequences.  MPT-7b is an example of another great model that uses
          Alibi.  With Alibi, you are able to easy change it so that instead of having
          a max context window of 2048, you can have a context window of 4096 with
          a few simple steps.<br><a href="https://huggingface.co/mosaicml/mpt-7b">https://huggingface.co/mosaicml/mpt-7b</a></p>

          <p>Perhaps this will be talked about in the paper that will be released,
          but if the results in the Alibi paper are correct, it is the superior positional
          encoding and should be used.</p>

          '
        raw: "I looked at the modeling code, and it seems that the code supports Alibi\
          \ positional encoding as well as rotary.  See the Alibi paper: https://arxiv.org/pdf/2108.12409.pdf\r\
          \n\r\nAlibi allows one to train a model on shorter sequences and inference\
          \ on longer sequences.  MPT-7b is an example of another great model that\
          \ uses Alibi.  With Alibi, you are able to easy change it so that instead\
          \ of having a max context window of 2048, you can have a context window\
          \ of 4096 with a few simple steps.\r\nhttps://huggingface.co/mosaicml/mpt-7b\r\
          \n\r\nPerhaps this will be talked about in the paper that will be released,\
          \ but if the results in the Alibi paper are correct, it is the superior\
          \ positional encoding and should be used."
        updatedAt: '2023-06-20T21:31:04.970Z'
      numEdits: 0
      reactions:
      - count: 9
        reaction: "\U0001F44D"
        users:
        - leocnj
        - feasible
        - avostryakov
        - JoaoLages
        - jangtu052
        - muhtasham
        - mylonasc
        - nick-coco
        - sytelus
    id: 64921a987883894c1314992f
    type: comment
  author: mallorbc
  content: "I looked at the modeling code, and it seems that the code supports Alibi\
    \ positional encoding as well as rotary.  See the Alibi paper: https://arxiv.org/pdf/2108.12409.pdf\r\
    \n\r\nAlibi allows one to train a model on shorter sequences and inference on\
    \ longer sequences.  MPT-7b is an example of another great model that uses Alibi.\
    \  With Alibi, you are able to easy change it so that instead of having a max\
    \ context window of 2048, you can have a context window of 4096 with a few simple\
    \ steps.\r\nhttps://huggingface.co/mosaicml/mpt-7b\r\n\r\nPerhaps this will be\
    \ talked about in the paper that will be released, but if the results in the Alibi\
    \ paper are correct, it is the superior positional encoding and should be used."
  created_at: 2023-06-20 20:31:04+00:00
  edited: false
  hidden: false
  id: 64921a987883894c1314992f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 48
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: open
target_branch: null
title: Why Rotary Positional Embeddings Over Alibi?
