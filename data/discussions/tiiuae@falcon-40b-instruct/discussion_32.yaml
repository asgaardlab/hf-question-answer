!!python/object:huggingface_hub.community.DiscussionWithDetails
author: leocheung
conflicting_files: null
created_at: 2023-06-06 04:39:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9bb5c6dd63b0ae6f787e00b0844cf03e.svg
      fullname: cheung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leocheung
      type: user
    createdAt: '2023-06-06T05:39:36.000Z'
    data:
      edited: false
      editors:
      - leocheung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5751398801803589
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9bb5c6dd63b0ae6f787e00b0844cf03e.svg
          fullname: cheung
          isHf: false
          isPro: false
          name: leocheung
          type: user
        html: '<p>File "/root/.cache/huggingface/modules/transformers_modules/falcon-40b/modelling_RW.py",
          line 93, in forward<br>    return (q * cos) + (rotate_half(q) * sin), (k
          * cos) + (rotate_half(k) * sin)<br>torch.cuda.OutOfMemoryError: CUDA out
          of memory. Tried to allocate 2.00 MiB (GPU 0; 79.35 GiB total capacity;
          77.18 GiB already allocated; 3.19 MiB free; 78.17 GiB reserved in total
          by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>here is my code:<br>from transformers import AutoTokenizer, AutoModelForCausalLM<br>import
          transformers<br>import torch</p>

          <p>model = "falcon-40b"</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model)<br>pipeline = transformers.pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>    device_map="auto",<br>)<br>sequences
          = pipeline(<br>   "Girafatron is obsessed with giraffes, the most glorious
          animal on the face of this Earth. Giraftron believes all other animals are
          irrelevant when compared to the glorious majesty of the giraffe.\nDaniel:
          Hello, Girafatron!\nGirafatron:",<br>    max_length=200,<br>    do_sample=True,<br>    top_k=5,<br>    num_return_sequences=1,<br>    eos_token_id=tokenizer.eos_token_id,<br>)<br>for
          seq in sequences:<br>    print(f"Result: {seq[''generated_text'']}")</p>

          '
        raw: "\r\nFile \"/root/.cache/huggingface/modules/transformers_modules/falcon-40b/modelling_RW.py\"\
          , line 93, in forward\r\n    return (q * cos) + (rotate_half(q) * sin),\
          \ (k * cos) + (rotate_half(k) * sin)\r\ntorch.cuda.OutOfMemoryError: CUDA\
          \ out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.35 GiB total capacity;\
          \ 77.18 GiB already allocated; 3.19 MiB free; 78.17 GiB reserved in total\
          \ by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
          \n\r\nhere is my code:\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
          \nimport transformers\r\nimport torch\r\n\r\nmodel = \"falcon-40b\"\r\n\r\
          \ntokenizer = AutoTokenizer.from_pretrained(model)\r\npipeline = transformers.pipeline(\r\
          \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n   \
          \ device_map=\"auto\",\r\n)\r\nsequences = pipeline(\r\n   \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",\r\n    max_length=200,\r\n    do_sample=True,\r\n    top_k=5,\r\
          \n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
          \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
          )"
        updatedAt: '2023-06-06T05:39:36.282Z'
      numEdits: 0
      reactions: []
    id: 647ec698e4d52fe0e02281b0
    type: comment
  author: leocheung
  content: "\r\nFile \"/root/.cache/huggingface/modules/transformers_modules/falcon-40b/modelling_RW.py\"\
    , line 93, in forward\r\n    return (q * cos) + (rotate_half(q) * sin), (k * cos)\
    \ + (rotate_half(k) * sin)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory.\
    \ Tried to allocate 2.00 MiB (GPU 0; 79.35 GiB total capacity; 77.18 GiB already\
    \ allocated; 3.19 MiB free; 78.17 GiB reserved in total by PyTorch) If reserved\
    \ memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
    \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\n\
    here is my code:\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
    \nimport transformers\r\nimport torch\r\n\r\nmodel = \"falcon-40b\"\r\n\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model)\r\npipeline = transformers.pipeline(\r\
    \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n\
    \    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n    device_map=\"\
    auto\",\r\n)\r\nsequences = pipeline(\r\n   \"Girafatron is obsessed with giraffes,\
    \ the most glorious animal on the face of this Earth. Giraftron believes all other\
    \ animals are irrelevant when compared to the glorious majesty of the giraffe.\\\
    nDaniel: Hello, Girafatron!\\nGirafatron:\",\r\n    max_length=200,\r\n    do_sample=True,\r\
    \n    top_k=5,\r\n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
    \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
    )"
  created_at: 2023-06-06 04:39:36+00:00
  edited: false
  hidden: false
  id: 647ec698e4d52fe0e02281b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6336e1c33331b6c67713defdcf1013cb.svg
      fullname: Mason Bray
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: masonbraysx
      type: user
    createdAt: '2023-06-07T16:41:12.000Z'
    data:
      edited: false
      editors:
      - masonbraysx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8778265714645386
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6336e1c33331b6c67713defdcf1013cb.svg
          fullname: Mason Bray
          isHf: false
          isPro: true
          name: masonbraysx
          type: user
        html: '<p>A 40-B parameter model will not fit on and A100-80GB if it is in
          bf16 or fp16. In 16-bit precision the amount of VRAM needed to run a given
          model is at least 2GB per 1B parameters, and some models are closer to 3GB
          per 1B parameters. This does not include the amount of memory needed to
          actually run any type of inferencing. Two easy options: 1) run it on a node
          with multiple A100 80GB GPUs.  2) load the model in 8bit precision. This
          requires the package "bitsandbytes". This reduces the necessary VRAM to
          about 45GB. I have successfully loaded and performed inference with the
          falcon-40b-instruct model on a system with 4 A4500''s (each GPU has 20GB
          VRAM) using this method.</p>

          '
        raw: 'A 40-B parameter model will not fit on and A100-80GB if it is in bf16
          or fp16. In 16-bit precision the amount of VRAM needed to run a given model
          is at least 2GB per 1B parameters, and some models are closer to 3GB per
          1B parameters. This does not include the amount of memory needed to actually
          run any type of inferencing. Two easy options: 1) run it on a node with
          multiple A100 80GB GPUs.  2) load the model in 8bit precision. This requires
          the package "bitsandbytes". This reduces the necessary VRAM to about 45GB.
          I have successfully loaded and performed inference with the falcon-40b-instruct
          model on a system with 4 A4500''s (each GPU has 20GB VRAM) using this method.'
        updatedAt: '2023-06-07T16:41:12.706Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F91D"
        users:
        - taesiri
        - leocheung
        - dataviral
        - Ichsan2895
    id: 6480b32840facadc5570ad42
    type: comment
  author: masonbraysx
  content: 'A 40-B parameter model will not fit on and A100-80GB if it is in bf16
    or fp16. In 16-bit precision the amount of VRAM needed to run a given model is
    at least 2GB per 1B parameters, and some models are closer to 3GB per 1B parameters.
    This does not include the amount of memory needed to actually run any type of
    inferencing. Two easy options: 1) run it on a node with multiple A100 80GB GPUs.  2)
    load the model in 8bit precision. This requires the package "bitsandbytes". This
    reduces the necessary VRAM to about 45GB. I have successfully loaded and performed
    inference with the falcon-40b-instruct model on a system with 4 A4500''s (each
    GPU has 20GB VRAM) using this method.'
  created_at: 2023-06-07 15:41:12+00:00
  edited: false
  hidden: false
  id: 6480b32840facadc5570ad42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
      fullname: Muhammad Ichsan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ichsan2895
      type: user
    createdAt: '2023-06-09T12:15:41.000Z'
    data:
      edited: false
      editors:
      - Ichsan2895
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7594523429870605
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
          fullname: Muhammad Ichsan
          isHf: false
          isPro: false
          name: Ichsan2895
          type: user
        html: '<p>I successfully using single 48GB VRAM A6000. As masonbraysx said,
          you need the bitsandbytes library. I prefer to install that package with
          github repo for the latest dev version+transformer+accelerate.</p>

          <p>Activating bitsandbytes config which enabling load_in_4bit, nf4 quant
          type (QLora version), and bfloat16.</p>

          '
        raw: 'I successfully using single 48GB VRAM A6000. As masonbraysx said, you
          need the bitsandbytes library. I prefer to install that package with github
          repo for the latest dev version+transformer+accelerate.


          Activating bitsandbytes config which enabling load_in_4bit, nf4 quant type
          (QLora version), and bfloat16.'
        updatedAt: '2023-06-09T12:15:41.915Z'
      numEdits: 0
      reactions: []
    id: 648317ed54f7513db6db915d
    type: comment
  author: Ichsan2895
  content: 'I successfully using single 48GB VRAM A6000. As masonbraysx said, you
    need the bitsandbytes library. I prefer to install that package with github repo
    for the latest dev version+transformer+accelerate.


    Activating bitsandbytes config which enabling load_in_4bit, nf4 quant type (QLora
    version), and bfloat16.'
  created_at: 2023-06-09 11:15:41+00:00
  edited: false
  hidden: false
  id: 648317ed54f7513db6db915d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:59:15.000Z'
    data:
      edited: true
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9636918306350708
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>For the base model in <code>bfloat16</code>, we recommend 85-100GB
          of memory. </p>

          <p>There has been some efforts, such as <a rel="nofollow" href="https://github.com/rmihaylov/falcontune">FalconTune</a>,
          to have the model in 4 bits (~20-30GB only).</p>

          '
        raw: "For the base model in `bfloat16`, we recommend 85-100GB of memory. \n\
          \nThere has been some efforts, such as [FalconTune](https://github.com/rmihaylov/falcontune),\
          \ to have the model in 4 bits (~20-30GB only)."
        updatedAt: '2023-06-09T14:59:35.788Z'
      numEdits: 1
      reactions: []
    id: 64833e4368af3aed0a53ed51
    type: comment
  author: FalconLLM
  content: "For the base model in `bfloat16`, we recommend 85-100GB of memory. \n\n\
    There has been some efforts, such as [FalconTune](https://github.com/rmihaylov/falcontune),\
    \ to have the model in 4 bits (~20-30GB only)."
  created_at: 2023-06-09 13:59:15+00:00
  edited: true
  hidden: false
  id: 64833e4368af3aed0a53ed51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ea2e7170c4af21403a4d8fd3ba5023f.svg
      fullname: Tiago Freitas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tiagofreitas87
      type: user
    createdAt: '2023-06-09T16:20:23.000Z'
    data:
      edited: false
      editors:
      - tiagofreitas87
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9914467930793762
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ea2e7170c4af21403a4d8fd3ba5023f.svg
          fullname: Tiago Freitas
          isHf: false
          isPro: false
          name: tiagofreitas87
          type: user
        html: '<p>Can anyone share the inference speeds on each setup? Knowing the
          speed is as important as being able to load it..</p>

          '
        raw: Can anyone share the inference speeds on each setup? Knowing the speed
          is as important as being able to load it..
        updatedAt: '2023-06-09T16:20:23.767Z'
      numEdits: 0
      reactions: []
    id: 6483514705f00df6db50ab7d
    type: comment
  author: tiagofreitas87
  content: Can anyone share the inference speeds on each setup? Knowing the speed
    is as important as being able to load it..
  created_at: 2023-06-09 15:20:23+00:00
  edited: false
  hidden: false
  id: 6483514705f00df6db50ab7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
      fullname: Muhammad Ichsan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ichsan2895
      type: user
    createdAt: '2023-06-10T07:41:43.000Z'
    data:
      edited: true
      editors:
      - Ichsan2895
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9748571515083313
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
          fullname: Muhammad Ichsan
          isHf: false
          isPro: false
          name: Ichsan2895
          type: user
        html: '<blockquote>

          <p>Can anyone share the inference speeds on each setup? Knowing the speed
          is as important as being able to load it..</p>

          </blockquote>

          <p>Mine in cloud environment with single RTX A6000 48 GB VRAM got 1-2 tokens/second.
          Pretty slow but okay its running.</p>

          '
        raw: '> Can anyone share the inference speeds on each setup? Knowing the speed
          is as important as being able to load it..


          Mine in cloud environment with single RTX A6000 48 GB VRAM got 1-2 tokens/second.
          Pretty slow but okay its running.'
        updatedAt: '2023-06-10T07:42:05.028Z'
      numEdits: 1
      reactions: []
    id: 648429370447196fa20c3d77
    type: comment
  author: Ichsan2895
  content: '> Can anyone share the inference speeds on each setup? Knowing the speed
    is as important as being able to load it..


    Mine in cloud environment with single RTX A6000 48 GB VRAM got 1-2 tokens/second.
    Pretty slow but okay its running.'
  created_at: 2023-06-10 06:41:43+00:00
  edited: true
  hidden: false
  id: 648429370447196fa20c3d77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a318b0f650eadfe51a4f967180e8c871.svg
      fullname: Tommy Wohlfahrt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tommyw
      type: user
    createdAt: '2023-06-14T21:48:51.000Z'
    data:
      edited: true
      editors:
      - tommyw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9521072506904602
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a318b0f650eadfe51a4f967180e8c871.svg
          fullname: Tommy Wohlfahrt
          isHf: false
          isPro: false
          name: tommyw
          type: user
        html: '<blockquote>

          <p>Can anyone share the inference speeds on each setup? Knowing the speed
          is as important as being able to load it..</p>

          </blockquote>

          <p>Running it on 4 A100-80GB and it takes between 23 and 24 ms per token
          (using this <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a>
          to serve it).</p>

          '
        raw: '

          > Can anyone share the inference speeds on each setup? Knowing the speed
          is as important as being able to load it..


          Running it on 4 A100-80GB and it takes between 23 and 24 ms per token (using
          this https://github.com/huggingface/text-generation-inference to serve it).

          '
        updatedAt: '2023-06-14T21:48:59.415Z'
      numEdits: 1
      reactions: []
    id: 648a35c3a44dfb19e65a7b66
    type: comment
  author: tommyw
  content: '

    > Can anyone share the inference speeds on each setup? Knowing the speed is as
    important as being able to load it..


    Running it on 4 A100-80GB and it takes between 23 and 24 ms per token (using this
    https://github.com/huggingface/text-generation-inference to serve it).

    '
  created_at: 2023-06-14 20:48:51+00:00
  edited: true
  hidden: false
  id: 648a35c3a44dfb19e65a7b66
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 32
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: open
target_branch: null
title: a100-80g memory but still call error
