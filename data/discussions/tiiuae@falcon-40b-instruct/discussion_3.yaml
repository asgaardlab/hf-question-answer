!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Minami-su
conflicting_files: null
created_at: 2023-05-28 04:21:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2023-05-28T05:21:31.000Z'
    data:
      edited: true
      editors:
      - Minami-su
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
          fullname: "\u5357\u6816"
          isHf: false
          isPro: false
          name: Minami-su
          type: user
        html: '<p>&lt;3090gpux2 &gt;</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>import
          transformers<br>import torch</p>

          <p>model = "falcon40binstruction"</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model)<br>pipeline = transformers.pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>    device_map="auto",<br>)<br>sequences
          = pipeline(<br>   "Girafatron is obsessed with giraffes, the most glorious
          animal on the face of this Earth. Giraftron believes all other animals are
          irrelevant when compared to the glorious majesty of the giraffe.\nDaniel:
          Hello, Girafatron!\nGirafatron:",<br>    max_length=200,<br>    do_sample=True,<br>    top_k=10,<br>    num_return_sequences=1,<br>    eos_token_id=tokenizer.eos_token_id,<br>)<br>for
          seq in sequences:<br>    print(f"Result: {seq[''generated_text'']}")</p>

          <p>The model ''RWForCausalLM'' is not supported for text-generation. Supported
          models are [''BartForCausalLM'', ''BertLMHeadModel'', ''BertGenerationDecoder'',
          ''BigBirdForCausalLM'', ''BigBirdPegasusForCausalLM'', ''BioGptForCausalLM'',
          ''BlenderbotForCausalLM'', ''BlenderbotSmallForCausalLM'', ''BloomForCausalLM'',
          ''CamembertForCausalLM'', ''CodeGenForCausalLM'', ''CpmAntForCausalLM'',
          ''CTRLLMHeadModel'', ''Data2VecTextForCausalLM'', ''ElectraForCausalLM'',
          ''ErnieForCausalLM'', ''GitForCausalLM'', ''GPT2LMHeadModel'', ''GPT2LMHeadModel'',
          ''GPTBigCodeForCausalLM'', ''GPTNeoForCausalLM'', ''GPTNeoXForCausalLM'',
          ''GPTNeoXJapaneseForCausalLM'', ''GPTJForCausalLM'', ''LlamaForCausalLM'',
          ''MarianForCausalLM'', ''MBartForCausalLM'', ''MegaForCausalLM'', ''MegatronBertForCausalLM'',
          ''MvpForCausalLM'', ''OpenLlamaForCausalLM'', ''OpenAIGPTLMHeadModel'',
          ''OPTForCausalLM'', ''PegasusForCausalLM'', ''PLBartForCausalLM'', ''ProphetNetForCausalLM'',
          ''QDQBertLMHeadModel'', ''ReformerModelWithLMHead'', ''RemBertForCausalLM'',
          ''RobertaForCausalLM'', ''RobertaPreLayerNormForCausalLM'', ''RoCBertForCausalLM'',
          ''RoFormerForCausalLM'', ''RwkvForCausalLM'', ''Speech2Text2ForCausalLM'',
          ''TransfoXLLMHeadModel'', ''TrOCRForCausalLM'', ''XGLMForCausalLM'', ''XLMWithLMHeadModel'',
          ''XLMProphetNetForCausalLM'', ''XLMRobertaForCausalLM'', ''XLMRobertaXLForCausalLM'',
          ''XLNetLMHeadModel'', ''XmodForCausalLM''].<br>Setting <code>pad_token_id</code>
          to <code>eos_token_id</code>:11 for open-end generation.</p>

          <hr>

          <p>RuntimeError                              Traceback (most recent call
          last)<br>Cell In[1], line 16<br>      7 tokenizer = AutoTokenizer.from_pretrained(model)<br>      8
          pipeline = transformers.pipeline(<br>      9     "text-generation",<br>     10     model=model,<br>   (...)<br>     14     device_map="auto",<br>     15
          )<br>---&gt; 16 sequences = pipeline(<br>     17    "Girafatron is obsessed
          with giraffes, the most glorious animal on the face of this Earth. Giraftron
          believes all other animals are irrelevant when compared to the glorious
          majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",<br>     18     max_length=200,<br>     19     do_sample=True,<br>     20     top_k=10,<br>     21     num_return_sequences=1,<br>     22     eos_token_id=tokenizer.eos_token_id,<br>     23
          )<br>     24 for seq in sequences:<br>     25     print(f"Result: {seq[''generated_text'']}")</p>

          <p>File ~/autodl-tmp/transformers/pipelines/text_generation.py:201, in TextGenerationPipeline.<strong>call</strong>(self,
          text_inputs, **kwargs)<br>    160 def <strong>call</strong>(self, text_inputs,
          **kwargs):<br>    161     """<br>    162     Complete the prompt(s) given
          as inputs.<br>    163<br>   (...)<br>    199           ids of the generated
          text.<br>    200     """<br>--&gt; 201     return super().<strong>call</strong>(text_inputs,
          **kwargs)</p>

          <p>File ~/autodl-tmp/transformers/pipelines/base.py:1118, in Pipeline.<strong>call</strong>(self,
          inputs, num_workers, batch_size, *args, **kwargs)<br>   1110     return
          next(<br>   1111         iter(<br>   1112             self.get_iterator(<br>   (...)<br>   1115         )<br>   1116     )<br>   1117
          else:<br>-&gt; 1118     return self.run_single(inputs, preprocess_params,
          forward_params, postprocess_params)</p>

          <p>File ~/autodl-tmp/transformers/pipelines/base.py:1125, in Pipeline.run_single(self,
          inputs, preprocess_params, forward_params, postprocess_params)<br>   1123
          def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):<br>   1124     model_inputs
          = self.preprocess(inputs, **preprocess_params)<br>-&gt; 1125     model_outputs
          = self.forward(model_inputs, **forward_params)<br>   1126     outputs =
          self.postprocess(model_outputs, **postprocess_params)<br>   1127     return
          outputs</p>

          <p>File ~/autodl-tmp/transformers/pipelines/base.py:1024, in Pipeline.forward(self,
          model_inputs, **forward_params)<br>   1022     with inference_context():<br>   1023         model_inputs
          = self._ensure_tensor_on_device(model_inputs, device=self.device)<br>-&gt;
          1024         model_outputs = self._forward(model_inputs, **forward_params)<br>   1025         model_outputs
          = self._ensure_tensor_on_device(model_outputs, device=torch.device("cpu"))<br>   1026
          else:</p>

          <p>File ~/autodl-tmp/transformers/pipelines/text_generation.py:263, in TextGenerationPipeline._forward(self,
          model_inputs, **generate_kwargs)<br>    260         generate_kwargs["min_length"]
          += prefix_length<br>    262 # BS x SL<br>--&gt; 263 generated_sequence =
          self.model.generate(input_ids=input_ids, attention_mask=attention_mask,
          **generate_kwargs)<br>    264 out_b = generated_sequence.shape[0]<br>    265
          if self.framework == "pt":</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115,
          in context_decorator..decorate_context(*args, **kwargs)<br>    112 @functools.wraps(func)<br>    113
          def decorate_context(*args, **kwargs):<br>    114     with ctx_factory():<br>--&gt;
          115         return func(*args, **kwargs)</p>

          <p>File ~/autodl-tmp/transformers/generation/utils.py:1568, in GenerationMixin.generate(self,
          inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn,
          synced_gpus, assistant_model, streamer, **kwargs)<br>   1560     input_ids,
          model_kwargs = self._expand_inputs_for_generation(<br>   1561         input_ids=input_ids,<br>   1562         expand_size=generation_config.num_return_sequences,<br>   1563         is_encoder_decoder=self.config.is_encoder_decoder,<br>   1564         **model_kwargs,<br>   1565     )<br>   1567     #
          13. run sample<br>-&gt; 1568     return self.sample(<br>   1569         input_ids,<br>   1570         logits_processor=logits_processor,<br>   1571         logits_warper=logits_warper,<br>   1572         stopping_criteria=stopping_criteria,<br>   1573         pad_token_id=generation_config.pad_token_id,<br>   1574         eos_token_id=generation_config.eos_token_id,<br>   1575         output_scores=generation_config.output_scores,<br>   1576         return_dict_in_generate=generation_config.return_dict_in_generate,<br>   1577         synced_gpus=synced_gpus,<br>   1578         streamer=streamer,<br>   1579         **model_kwargs,<br>   1580     )<br>   1582
          elif is_beam_gen_mode:<br>   1583     if generation_config.num_return_sequences
          &gt; generation_config.num_beams:</p>

          <p>File ~/autodl-tmp/transformers/generation/utils.py:2615, in GenerationMixin.sample(self,
          input_ids, logits_processor, stopping_criteria, logits_warper, max_length,
          pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores,
          return_dict_in_generate, synced_gpus, streamer, **model_kwargs)<br>   2612
          model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)<br>   2614
          # forward pass to get next token<br>-&gt; 2615 outputs = self(<br>   2616     **model_inputs,<br>   2617     return_dict=True,<br>   2618     output_attentions=output_attentions,<br>   2619     output_hidden_states=output_hidden_states,<br>   2620
          )<br>   2622 if synced_gpus and this_peer_finished:<br>   2623     continue  #
          don''t waste resources running the code we don''t need</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,
          in Module._call_impl(self, *args, **kwargs)<br>   1496 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1497 # this function,
          and just call forward.<br>   1498 if not (self._backward_hooks or self._backward_pre_hooks
          or self._forward_hooks or self._forward_pre_hooks<br>   1499         or
          _global_backward_pre_hooks or _global_backward_hooks<br>   1500         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1501     return
          forward_call(*args, **kwargs)<br>   1502 # Do not call functions when jit
          is used<br>   1503 full_backward_hooks, non_full_backward_hooks = [], []</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/accelerate/hooks.py:165,
          in add_hook_to_module..new_forward(*args, **kwargs)<br>    163         output
          = old_forward(*args, **kwargs)<br>    164 else:<br>--&gt; 165     output
          = old_forward(*args, **kwargs)<br>    166 return module._hf_hook.post_forward(module,
          output)</p>

          <p>File ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:759,
          in RWForCausalLM.forward(self, input_ids, past_key_values, attention_mask,
          head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states,
          return_dict, **deprecated_arguments)<br>    755     raise ValueError(f"Got
          unexpected arguments: {deprecated_arguments}")<br>    757 return_dict =
          return_dict if return_dict is not None else self.config.use_return_dict<br>--&gt;
          759 transformer_outputs = self.transformer(<br>    760     input_ids,<br>    761     past_key_values=past_key_values,<br>    762     attention_mask=attention_mask,<br>    763     head_mask=head_mask,<br>    764     inputs_embeds=inputs_embeds,<br>    765     use_cache=use_cache,<br>    766     output_attentions=output_attentions,<br>    767     output_hidden_states=output_hidden_states,<br>    768     return_dict=return_dict,<br>    769
          )<br>    770 hidden_states = transformer_outputs[0]<br>    772 lm_logits
          = self.lm_head(hidden_states)</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,
          in Module._call_impl(self, *args, **kwargs)<br>   1496 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1497 # this function,
          and just call forward.<br>   1498 if not (self._backward_hooks or self._backward_pre_hooks
          or self._forward_hooks or self._forward_pre_hooks<br>   1499         or
          _global_backward_pre_hooks or _global_backward_hooks<br>   1500         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1501     return
          forward_call(*args, **kwargs)<br>   1502 # Do not call functions when jit
          is used<br>   1503 full_backward_hooks, non_full_backward_hooks = [], []</p>

          <p>File ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:654,
          in RWModel.forward(self, input_ids, past_key_values, attention_mask, head_mask,
          inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict,
          **deprecated_arguments)<br>    646     outputs = torch.utils.checkpoint.checkpoint(<br>    647         create_custom_forward(block),<br>    648         hidden_states,<br>   (...)<br>    651         head_mask[i],<br>    652     )<br>    653
          else:<br>--&gt; 654     outputs = block(<br>    655         hidden_states,<br>    656         layer_past=layer_past,<br>    657         attention_mask=causal_mask,<br>    658         head_mask=head_mask[i],<br>    659         use_cache=use_cache,<br>    660         output_attentions=output_attentions,<br>    661         alibi=alibi,<br>    662     )<br>    664
          hidden_states = outputs[0]<br>    665 if use_cache is True:</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,
          in Module._call_impl(self, *args, **kwargs)<br>   1496 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1497 # this function,
          and just call forward.<br>   1498 if not (self._backward_hooks or self._backward_pre_hooks
          or self._forward_hooks or self._forward_pre_hooks<br>   1499         or
          _global_backward_pre_hooks or _global_backward_hooks<br>   1500         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1501     return
          forward_call(*args, **kwargs)<br>   1502 # Do not call functions when jit
          is used<br>   1503 full_backward_hooks, non_full_backward_hooks = [], []</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/accelerate/hooks.py:165,
          in add_hook_to_module..new_forward(*args, **kwargs)<br>    163         output
          = old_forward(*args, **kwargs)<br>    164 else:<br>--&gt; 165     output
          = old_forward(*args, **kwargs)<br>    166 return module._hf_hook.post_forward(module,
          output)</p>

          <p>File ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:396,
          in DecoderLayer.forward(self, hidden_states, alibi, attention_mask, layer_past,
          head_mask, use_cache, output_attentions)<br>    393 residual = hidden_states<br>    395
          # Self attention.<br>--&gt; 396 attn_outputs = self.self_attention(<br>    397     ln_attn,<br>    398     layer_past=layer_past,<br>    399     attention_mask=attention_mask,<br>    400     alibi=alibi,<br>    401     head_mask=head_mask,<br>    402     use_cache=use_cache,<br>    403     output_attentions=output_attentions,<br>    404
          )<br>    406 attention_output = attn_outputs[0]<br>    408 outputs = attn_outputs[1:]</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,
          in Module._call_impl(self, *args, **kwargs)<br>   1496 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1497 # this function,
          and just call forward.<br>   1498 if not (self._backward_hooks or self._backward_pre_hooks
          or self._forward_hooks or self._forward_pre_hooks<br>   1499         or
          _global_backward_pre_hooks or _global_backward_hooks<br>   1500         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1501     return
          forward_call(*args, **kwargs)<br>   1502 # Do not call functions when jit
          is used<br>   1503 full_backward_hooks, non_full_backward_hooks = [], []</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/accelerate/hooks.py:165,
          in add_hook_to_module..new_forward(*args, **kwargs)<br>    163         output
          = old_forward(*args, **kwargs)<br>    164 else:<br>--&gt; 165     output
          = old_forward(*args, **kwargs)<br>    166 return module._hf_hook.post_forward(module,
          output)</p>

          <p>File ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:252,
          in Attention.forward(self, hidden_states, alibi, attention_mask, layer_past,
          head_mask, use_cache, output_attentions)<br>    242 def forward(<br>    243     self,<br>    244     hidden_states:
          torch.Tensor,<br>   (...)<br>    250     output_attentions: bool = False,<br>    251
          ):<br>--&gt; 252     fused_qkv = self.query_key_value(hidden_states)  #
          [batch_size, seq_length, 3 x hidden_size]<br>    254     # 3 x [batch_size,
          seq_length, num_heads, head_dim]<br>    255     (query_layer, key_layer,
          value_layer) = self._split_heads(fused_qkv)</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,
          in Module._call_impl(self, *args, **kwargs)<br>   1496 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1497 # this function,
          and just call forward.<br>   1498 if not (self._backward_hooks or self._backward_pre_hooks
          or self._forward_hooks or self._forward_pre_hooks<br>   1499         or
          _global_backward_pre_hooks or _global_backward_hooks<br>   1500         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1501     return
          forward_call(*args, **kwargs)<br>   1502 # Do not call functions when jit
          is used<br>   1503 full_backward_hooks, non_full_backward_hooks = [], []</p>

          <p>File ~/miniconda3/lib/python3.8/site-packages/accelerate/hooks.py:165,
          in add_hook_to_module..new_forward(*args, **kwargs)<br>    163         output
          = old_forward(*args, **kwargs)<br>    164 else:<br>--&gt; 165     output
          = old_forward(*args, **kwargs)<br>    166 return module._hf_hook.post_forward(module,
          output)</p>

          <p>File ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:32,
          in Linear.forward(self, input)<br>     31 def forward(self, input: torch.Tensor)
          -&gt; torch.Tensor:<br>---&gt; 32     ret = input @ self.weight.T<br>     33     if
          self.bias is None:<br>     34         return ret</p>

          <p>RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling
          <code>cublasCreate(handle)</code></p>

          '
        raw: "<3090gpux2 >\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          import transformers\nimport torch\n\nmodel = \"falcon40binstruction\"\n\n\
          tokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes,\
          \ the most glorious animal on the face of this Earth. Giraftron believes\
          \ all other animals are irrelevant when compared to the glorious majesty\
          \ of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n\
          \    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n\n\nThe model 'RWForCausalLM' is not supported for text-generation. Supported\
          \ models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
          \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM',\
          \ 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM',\
          \ 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel',\
          \ 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM',\
          \ 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM',\
          \ 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
          \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
          \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
          \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
          \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
          \ 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
          \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
          \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel',\
          \ 'XmodForCausalLM'].\nSetting `pad_token_id` to `eos_token_id`:11 for open-end\
          \ generation.\n---------------------------------------------------------------------------\n\
          RuntimeError                              Traceback (most recent call last)\n\
          Cell In[1], line 16\n      7 tokenizer = AutoTokenizer.from_pretrained(model)\n\
          \      8 pipeline = transformers.pipeline(\n      9     \"text-generation\"\
          ,\n     10     model=model,\n   (...)\n     14     device_map=\"auto\",\n\
          \     15 )\n---> 16 sequences = pipeline(\n     17    \"Girafatron is obsessed\
          \ with giraffes, the most glorious animal on the face of this Earth. Giraftron\
          \ believes all other animals are irrelevant when compared to the glorious\
          \ majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n\
          \     18     max_length=200,\n     19     do_sample=True,\n     20     top_k=10,\n\
          \     21     num_return_sequences=1,\n     22     eos_token_id=tokenizer.eos_token_id,\n\
          \     23 )\n     24 for seq in sequences:\n     25     print(f\"Result:\
          \ {seq['generated_text']}\")\n\nFile ~/autodl-tmp/transformers/pipelines/text_generation.py:201,\
          \ in TextGenerationPipeline.__call__(self, text_inputs, **kwargs)\n    160\
          \ def __call__(self, text_inputs, **kwargs):\n    161     \"\"\"\n    162\
          \     Complete the prompt(s) given as inputs.\n    163 \n   (...)\n    199\
          \           ids of the generated text.\n    200     \"\"\"\n--> 201    \
          \ return super().__call__(text_inputs, **kwargs)\n\nFile ~/autodl-tmp/transformers/pipelines/base.py:1118,\
          \ in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n\
          \   1110     return next(\n   1111         iter(\n   1112             self.get_iterator(\n\
          \   (...)\n   1115         )\n   1116     )\n   1117 else:\n-> 1118    \
          \ return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n\
          \nFile ~/autodl-tmp/transformers/pipelines/base.py:1125, in Pipeline.run_single(self,\
          \ inputs, preprocess_params, forward_params, postprocess_params)\n   1123\
          \ def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):\n\
          \   1124     model_inputs = self.preprocess(inputs, **preprocess_params)\n\
          -> 1125     model_outputs = self.forward(model_inputs, **forward_params)\n\
          \   1126     outputs = self.postprocess(model_outputs, **postprocess_params)\n\
          \   1127     return outputs\n\nFile ~/autodl-tmp/transformers/pipelines/base.py:1024,\
          \ in Pipeline.forward(self, model_inputs, **forward_params)\n   1022   \
          \  with inference_context():\n   1023         model_inputs = self._ensure_tensor_on_device(model_inputs,\
          \ device=self.device)\n-> 1024         model_outputs = self._forward(model_inputs,\
          \ **forward_params)\n   1025         model_outputs = self._ensure_tensor_on_device(model_outputs,\
          \ device=torch.device(\"cpu\"))\n   1026 else:\n\nFile ~/autodl-tmp/transformers/pipelines/text_generation.py:263,\
          \ in TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)\n\
          \    260         generate_kwargs[\"min_length\"] += prefix_length\n    262\
          \ # BS x SL\n--> 263 generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=attention_mask, **generate_kwargs)\n    264 out_b = generated_sequence.shape[0]\n\
          \    265 if self.framework == \"pt\":\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115,\
          \ in context_decorator.<locals>.decorate_context(*args, **kwargs)\n    112\
          \ @functools.wraps(func)\n    113 def decorate_context(*args, **kwargs):\n\
          \    114     with ctx_factory():\n--> 115         return func(*args, **kwargs)\n\
          \nFile ~/autodl-tmp/transformers/generation/utils.py:1568, in GenerationMixin.generate(self,\
          \ inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn,\
          \ synced_gpus, assistant_model, streamer, **kwargs)\n   1560     input_ids,\
          \ model_kwargs = self._expand_inputs_for_generation(\n   1561         input_ids=input_ids,\n\
          \   1562         expand_size=generation_config.num_return_sequences,\n \
          \  1563         is_encoder_decoder=self.config.is_encoder_decoder,\n   1564\
          \         **model_kwargs,\n   1565     )\n   1567     # 13. run sample\n\
          -> 1568     return self.sample(\n   1569         input_ids,\n   1570   \
          \      logits_processor=logits_processor,\n   1571         logits_warper=logits_warper,\n\
          \   1572         stopping_criteria=stopping_criteria,\n   1573         pad_token_id=generation_config.pad_token_id,\n\
          \   1574         eos_token_id=generation_config.eos_token_id,\n   1575 \
          \        output_scores=generation_config.output_scores,\n   1576       \
          \  return_dict_in_generate=generation_config.return_dict_in_generate,\n\
          \   1577         synced_gpus=synced_gpus,\n   1578         streamer=streamer,\n\
          \   1579         **model_kwargs,\n   1580     )\n   1582 elif is_beam_gen_mode:\n\
          \   1583     if generation_config.num_return_sequences > generation_config.num_beams:\n\
          \nFile ~/autodl-tmp/transformers/generation/utils.py:2615, in GenerationMixin.sample(self,\
          \ input_ids, logits_processor, stopping_criteria, logits_warper, max_length,\
          \ pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores,\
          \ return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\n   2612\
          \ model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\
          \   2614 # forward pass to get next token\n-> 2615 outputs = self(\n   2616\
          \     **model_inputs,\n   2617     return_dict=True,\n   2618     output_attentions=output_attentions,\n\
          \   2619     output_hidden_states=output_hidden_states,\n   2620 )\n   2622\
          \ if synced_gpus and this_peer_finished:\n   2623     continue  # don't\
          \ waste resources running the code we don't need\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501     return\
          \ forward_call(*args, **kwargs)\n   1502 # Do not call functions when jit\
          \ is used\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\n\
          \nFile ~/miniconda3/lib/python3.8/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163 \
          \        output = old_forward(*args, **kwargs)\n    164 else:\n--> 165 \
          \    output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:759,\
          \ in RWForCausalLM.forward(self, input_ids, past_key_values, attention_mask,\
          \ head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states,\
          \ return_dict, **deprecated_arguments)\n    755     raise ValueError(f\"\
          Got unexpected arguments: {deprecated_arguments}\")\n    757 return_dict\
          \ = return_dict if return_dict is not None else self.config.use_return_dict\n\
          --> 759 transformer_outputs = self.transformer(\n    760     input_ids,\n\
          \    761     past_key_values=past_key_values,\n    762     attention_mask=attention_mask,\n\
          \    763     head_mask=head_mask,\n    764     inputs_embeds=inputs_embeds,\n\
          \    765     use_cache=use_cache,\n    766     output_attentions=output_attentions,\n\
          \    767     output_hidden_states=output_hidden_states,\n    768     return_dict=return_dict,\n\
          \    769 )\n    770 hidden_states = transformer_outputs[0]\n    772 lm_logits\
          \ = self.lm_head(hidden_states)\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501     return\
          \ forward_call(*args, **kwargs)\n   1502 # Do not call functions when jit\
          \ is used\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\n\
          \nFile ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:654,\
          \ in RWModel.forward(self, input_ids, past_key_values, attention_mask, head_mask,\
          \ inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict,\
          \ **deprecated_arguments)\n    646     outputs = torch.utils.checkpoint.checkpoint(\n\
          \    647         create_custom_forward(block),\n    648         hidden_states,\n\
          \   (...)\n    651         head_mask[i],\n    652     )\n    653 else:\n\
          --> 654     outputs = block(\n    655         hidden_states,\n    656  \
          \       layer_past=layer_past,\n    657         attention_mask=causal_mask,\n\
          \    658         head_mask=head_mask[i],\n    659         use_cache=use_cache,\n\
          \    660         output_attentions=output_attentions,\n    661         alibi=alibi,\n\
          \    662     )\n    664 hidden_states = outputs[0]\n    665 if use_cache\
          \ is True:\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501     return\
          \ forward_call(*args, **kwargs)\n   1502 # Do not call functions when jit\
          \ is used\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\n\
          \nFile ~/miniconda3/lib/python3.8/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163 \
          \        output = old_forward(*args, **kwargs)\n    164 else:\n--> 165 \
          \    output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:396,\
          \ in DecoderLayer.forward(self, hidden_states, alibi, attention_mask, layer_past,\
          \ head_mask, use_cache, output_attentions)\n    393 residual = hidden_states\n\
          \    395 # Self attention.\n--> 396 attn_outputs = self.self_attention(\n\
          \    397     ln_attn,\n    398     layer_past=layer_past,\n    399     attention_mask=attention_mask,\n\
          \    400     alibi=alibi,\n    401     head_mask=head_mask,\n    402   \
          \  use_cache=use_cache,\n    403     output_attentions=output_attentions,\n\
          \    404 )\n    406 attention_output = attn_outputs[0]\n    408 outputs\
          \ = attn_outputs[1:]\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501     return\
          \ forward_call(*args, **kwargs)\n   1502 # Do not call functions when jit\
          \ is used\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\n\
          \nFile ~/miniconda3/lib/python3.8/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163 \
          \        output = old_forward(*args, **kwargs)\n    164 else:\n--> 165 \
          \    output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:252,\
          \ in Attention.forward(self, hidden_states, alibi, attention_mask, layer_past,\
          \ head_mask, use_cache, output_attentions)\n    242 def forward(\n    243\
          \     self,\n    244     hidden_states: torch.Tensor,\n   (...)\n    250\
          \     output_attentions: bool = False,\n    251 ):\n--> 252     fused_qkv\
          \ = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x\
          \ hidden_size]\n    254     # 3 x [batch_size, seq_length, num_heads, head_dim]\n\
          \    255     (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n\
          \nFile ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1497 # this function,\
          \ and just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or\
          \ _global_backward_pre_hooks or _global_backward_hooks\n   1500        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1501     return\
          \ forward_call(*args, **kwargs)\n   1502 # Do not call functions when jit\
          \ is used\n   1503 full_backward_hooks, non_full_backward_hooks = [], []\n\
          \nFile ~/miniconda3/lib/python3.8/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163 \
          \        output = old_forward(*args, **kwargs)\n    164 else:\n--> 165 \
          \    output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:32,\
          \ in Linear.forward(self, input)\n     31 def forward(self, input: torch.Tensor)\
          \ -> torch.Tensor:\n---> 32     ret = input @ self.weight.T\n     33   \
          \  if self.bias is None:\n     34         return ret\n\nRuntimeError: CUDA\
          \ error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
        updatedAt: '2023-05-28T05:24:17.217Z'
      numEdits: 1
      reactions: []
    id: 6472e4db22016353ae3e2b29
    type: comment
  author: Minami-su
  content: "<3090gpux2 >\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    import transformers\nimport torch\n\nmodel = \"falcon40binstruction\"\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
    \    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n\
    \   \"Girafatron is obsessed with giraffes, the most glorious animal on the face\
    \ of this Earth. Giraftron believes all other animals are irrelevant when compared\
    \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\
    ,\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
    \    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"\
    Result: {seq['generated_text']}\")\n\n\nThe model 'RWForCausalLM' is not supported\
    \ for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel',\
    \ 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
    \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
    \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
    \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
    \ 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM',\
    \ 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
    \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
    \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
    \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
    \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM',\
    \ 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM',\
    \ 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM',\
    \ 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM',\
    \ 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\nSetting\
    \ `pad_token_id` to `eos_token_id`:11 for open-end generation.\n---------------------------------------------------------------------------\n\
    RuntimeError                              Traceback (most recent call last)\n\
    Cell In[1], line 16\n      7 tokenizer = AutoTokenizer.from_pretrained(model)\n\
    \      8 pipeline = transformers.pipeline(\n      9     \"text-generation\",\n\
    \     10     model=model,\n   (...)\n     14     device_map=\"auto\",\n     15\
    \ )\n---> 16 sequences = pipeline(\n     17    \"Girafatron is obsessed with giraffes,\
    \ the most glorious animal on the face of this Earth. Giraftron believes all other\
    \ animals are irrelevant when compared to the glorious majesty of the giraffe.\\\
    nDaniel: Hello, Girafatron!\\nGirafatron:\",\n     18     max_length=200,\n  \
    \   19     do_sample=True,\n     20     top_k=10,\n     21     num_return_sequences=1,\n\
    \     22     eos_token_id=tokenizer.eos_token_id,\n     23 )\n     24 for seq\
    \ in sequences:\n     25     print(f\"Result: {seq['generated_text']}\")\n\nFile\
    \ ~/autodl-tmp/transformers/pipelines/text_generation.py:201, in TextGenerationPipeline.__call__(self,\
    \ text_inputs, **kwargs)\n    160 def __call__(self, text_inputs, **kwargs):\n\
    \    161     \"\"\"\n    162     Complete the prompt(s) given as inputs.\n   \
    \ 163 \n   (...)\n    199           ids of the generated text.\n    200     \"\
    \"\"\n--> 201     return super().__call__(text_inputs, **kwargs)\n\nFile ~/autodl-tmp/transformers/pipelines/base.py:1118,\
    \ in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n\
    \   1110     return next(\n   1111         iter(\n   1112             self.get_iterator(\n\
    \   (...)\n   1115         )\n   1116     )\n   1117 else:\n-> 1118     return\
    \ self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n\
    \nFile ~/autodl-tmp/transformers/pipelines/base.py:1125, in Pipeline.run_single(self,\
    \ inputs, preprocess_params, forward_params, postprocess_params)\n   1123 def\
    \ run_single(self, inputs, preprocess_params, forward_params, postprocess_params):\n\
    \   1124     model_inputs = self.preprocess(inputs, **preprocess_params)\n-> 1125\
    \     model_outputs = self.forward(model_inputs, **forward_params)\n   1126  \
    \   outputs = self.postprocess(model_outputs, **postprocess_params)\n   1127 \
    \    return outputs\n\nFile ~/autodl-tmp/transformers/pipelines/base.py:1024,\
    \ in Pipeline.forward(self, model_inputs, **forward_params)\n   1022     with\
    \ inference_context():\n   1023         model_inputs = self._ensure_tensor_on_device(model_inputs,\
    \ device=self.device)\n-> 1024         model_outputs = self._forward(model_inputs,\
    \ **forward_params)\n   1025         model_outputs = self._ensure_tensor_on_device(model_outputs,\
    \ device=torch.device(\"cpu\"))\n   1026 else:\n\nFile ~/autodl-tmp/transformers/pipelines/text_generation.py:263,\
    \ in TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)\n\
    \    260         generate_kwargs[\"min_length\"] += prefix_length\n    262 # BS\
    \ x SL\n--> 263 generated_sequence = self.model.generate(input_ids=input_ids,\
    \ attention_mask=attention_mask, **generate_kwargs)\n    264 out_b = generated_sequence.shape[0]\n\
    \    265 if self.framework == \"pt\":\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115,\
    \ in context_decorator.<locals>.decorate_context(*args, **kwargs)\n    112 @functools.wraps(func)\n\
    \    113 def decorate_context(*args, **kwargs):\n    114     with ctx_factory():\n\
    --> 115         return func(*args, **kwargs)\n\nFile ~/autodl-tmp/transformers/generation/utils.py:1568,\
    \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
    \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer,\
    \ **kwargs)\n   1560     input_ids, model_kwargs = self._expand_inputs_for_generation(\n\
    \   1561         input_ids=input_ids,\n   1562         expand_size=generation_config.num_return_sequences,\n\
    \   1563         is_encoder_decoder=self.config.is_encoder_decoder,\n   1564 \
    \        **model_kwargs,\n   1565     )\n   1567     # 13. run sample\n-> 1568\
    \     return self.sample(\n   1569         input_ids,\n   1570         logits_processor=logits_processor,\n\
    \   1571         logits_warper=logits_warper,\n   1572         stopping_criteria=stopping_criteria,\n\
    \   1573         pad_token_id=generation_config.pad_token_id,\n   1574       \
    \  eos_token_id=generation_config.eos_token_id,\n   1575         output_scores=generation_config.output_scores,\n\
    \   1576         return_dict_in_generate=generation_config.return_dict_in_generate,\n\
    \   1577         synced_gpus=synced_gpus,\n   1578         streamer=streamer,\n\
    \   1579         **model_kwargs,\n   1580     )\n   1582 elif is_beam_gen_mode:\n\
    \   1583     if generation_config.num_return_sequences > generation_config.num_beams:\n\
    \nFile ~/autodl-tmp/transformers/generation/utils.py:2615, in GenerationMixin.sample(self,\
    \ input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id,\
    \ eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate,\
    \ synced_gpus, streamer, **model_kwargs)\n   2612 model_inputs = self.prepare_inputs_for_generation(input_ids,\
    \ **model_kwargs)\n   2614 # forward pass to get next token\n-> 2615 outputs =\
    \ self(\n   2616     **model_inputs,\n   2617     return_dict=True,\n   2618 \
    \    output_attentions=output_attentions,\n   2619     output_hidden_states=output_hidden_states,\n\
    \   2620 )\n   2622 if synced_gpus and this_peer_finished:\n   2623     continue\
    \  # don't waste resources running the code we don't need\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1497 # this function, and\
    \ just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1501     return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
    \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\n\nFile ~/miniconda3/lib/python3.8/site-packages/accelerate/hooks.py:165,\
    \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163       \
    \  output = old_forward(*args, **kwargs)\n    164 else:\n--> 165     output =\
    \ old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
    \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:759,\
    \ in RWForCausalLM.forward(self, input_ids, past_key_values, attention_mask, head_mask,\
    \ inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict,\
    \ **deprecated_arguments)\n    755     raise ValueError(f\"Got unexpected arguments:\
    \ {deprecated_arguments}\")\n    757 return_dict = return_dict if return_dict\
    \ is not None else self.config.use_return_dict\n--> 759 transformer_outputs =\
    \ self.transformer(\n    760     input_ids,\n    761     past_key_values=past_key_values,\n\
    \    762     attention_mask=attention_mask,\n    763     head_mask=head_mask,\n\
    \    764     inputs_embeds=inputs_embeds,\n    765     use_cache=use_cache,\n\
    \    766     output_attentions=output_attentions,\n    767     output_hidden_states=output_hidden_states,\n\
    \    768     return_dict=return_dict,\n    769 )\n    770 hidden_states = transformer_outputs[0]\n\
    \    772 lm_logits = self.lm_head(hidden_states)\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1497 # this function, and\
    \ just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1501     return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
    \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\n\nFile ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:654,\
    \ in RWModel.forward(self, input_ids, past_key_values, attention_mask, head_mask,\
    \ inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict,\
    \ **deprecated_arguments)\n    646     outputs = torch.utils.checkpoint.checkpoint(\n\
    \    647         create_custom_forward(block),\n    648         hidden_states,\n\
    \   (...)\n    651         head_mask[i],\n    652     )\n    653 else:\n--> 654\
    \     outputs = block(\n    655         hidden_states,\n    656         layer_past=layer_past,\n\
    \    657         attention_mask=causal_mask,\n    658         head_mask=head_mask[i],\n\
    \    659         use_cache=use_cache,\n    660         output_attentions=output_attentions,\n\
    \    661         alibi=alibi,\n    662     )\n    664 hidden_states = outputs[0]\n\
    \    665 if use_cache is True:\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1497 # this function, and\
    \ just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1501     return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
    \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\n\nFile ~/miniconda3/lib/python3.8/site-packages/accelerate/hooks.py:165,\
    \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163       \
    \  output = old_forward(*args, **kwargs)\n    164 else:\n--> 165     output =\
    \ old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
    \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:396,\
    \ in DecoderLayer.forward(self, hidden_states, alibi, attention_mask, layer_past,\
    \ head_mask, use_cache, output_attentions)\n    393 residual = hidden_states\n\
    \    395 # Self attention.\n--> 396 attn_outputs = self.self_attention(\n    397\
    \     ln_attn,\n    398     layer_past=layer_past,\n    399     attention_mask=attention_mask,\n\
    \    400     alibi=alibi,\n    401     head_mask=head_mask,\n    402     use_cache=use_cache,\n\
    \    403     output_attentions=output_attentions,\n    404 )\n    406 attention_output\
    \ = attn_outputs[0]\n    408 outputs = attn_outputs[1:]\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1497 # this function, and\
    \ just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1501     return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
    \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\n\nFile ~/miniconda3/lib/python3.8/site-packages/accelerate/hooks.py:165,\
    \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163       \
    \  output = old_forward(*args, **kwargs)\n    164 else:\n--> 165     output =\
    \ old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
    \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:252,\
    \ in Attention.forward(self, hidden_states, alibi, attention_mask, layer_past,\
    \ head_mask, use_cache, output_attentions)\n    242 def forward(\n    243    \
    \ self,\n    244     hidden_states: torch.Tensor,\n   (...)\n    250     output_attentions:\
    \ bool = False,\n    251 ):\n--> 252     fused_qkv = self.query_key_value(hidden_states)\
    \  # [batch_size, seq_length, 3 x hidden_size]\n    254     # 3 x [batch_size,\
    \ seq_length, num_heads, head_dim]\n    255     (query_layer, key_layer, value_layer)\
    \ = self._split_heads(fused_qkv)\n\nFile ~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1497 # this function, and\
    \ just call forward.\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\n   1499         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1501     return forward_call(*args, **kwargs)\n   1502 # Do not call functions\
    \ when jit is used\n   1503 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\n\nFile ~/miniconda3/lib/python3.8/site-packages/accelerate/hooks.py:165,\
    \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163       \
    \  output = old_forward(*args, **kwargs)\n    164 else:\n--> 165     output =\
    \ old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
    \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:32,\
    \ in Linear.forward(self, input)\n     31 def forward(self, input: torch.Tensor)\
    \ -> torch.Tensor:\n---> 32     ret = input @ self.weight.T\n     33     if self.bias\
    \ is None:\n     34         return ret\n\nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED\
    \ when calling `cublasCreate(handle)`"
  created_at: 2023-05-28 04:21:31+00:00
  edited: true
  hidden: false
  id: 6472e4db22016353ae3e2b29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2023-05-28T22:50:15.000Z'
    data:
      edited: false
      editors:
      - Minami-su
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
          fullname: "\u5357\u6816"
          isHf: false
          isPro: false
          name: Minami-su
          type: user
        html: "<p>\u95EE\u9898\u89E3\u51B3\u4E86\uFF0C3090x2 --&gt;  a40x2</p>\n"
        raw: "\u95EE\u9898\u89E3\u51B3\u4E86\uFF0C3090x2 -->  a40x2"
        updatedAt: '2023-05-28T22:50:15.321Z'
      numEdits: 0
      reactions: []
    id: 6473daa76cff2f86720c13f5
    type: comment
  author: Minami-su
  content: "\u95EE\u9898\u89E3\u51B3\u4E86\uFF0C3090x2 -->  a40x2"
  created_at: 2023-05-28 21:50:15+00:00
  edited: false
  hidden: false
  id: 6473daa76cff2f86720c13f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2023-05-28T22:50:17.000Z'
    data:
      status: closed
    id: 6473daa9352c94a20ddcbd90
    type: status-change
  author: Minami-su
  created_at: 2023-05-28 21:50:17+00:00
  id: 6473daa9352c94a20ddcbd90
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`'
