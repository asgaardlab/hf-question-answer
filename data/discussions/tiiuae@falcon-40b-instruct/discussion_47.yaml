!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Holynull
conflicting_files: null
created_at: 2023-06-20 05:47:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647803cd6e6c7ac608c55b29/JycqSxVxHDV43coJ2_7BG.jpeg?w=200&h=200&f=face
      fullname: Eddie Chueng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Holynull
      type: user
    createdAt: '2023-06-20T06:47:17.000Z'
    data:
      edited: false
      editors:
      - Holynull
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.632312536239624
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647803cd6e6c7ac608c55b29/JycqSxVxHDV43coJ2_7BG.jpeg?w=200&h=200&f=face
          fullname: Eddie Chueng
          isHf: false
          isPro: false
          name: Holynull
          type: user
        html: '<p>I always got an following error from AWS Sagemaker Inference Endpoint.
          When I use langchain create a qa chain, its prompt template has more   then
          1000 tokens in general. How to solve this issue? </p>

          <pre><code>{"error":"Input validation error: `inputs` tokens + `max_new_tokens`
          must be &lt;= 1512. Given: 1682 `inputs` tokens and 200 `max_new_tokens`","error_type":"validation"}

          </code></pre>

          '
        raw: "I always got an following error from AWS Sagemaker Inference Endpoint.\
          \ When I use langchain create a qa chain, its prompt template has more \
          \  then 1000 tokens in general. How to solve this issue? \r\n```\r\n{\"\
          error\":\"Input validation error: `inputs` tokens + `max_new_tokens` must\
          \ be <= 1512. Given: 1682 `inputs` tokens and 200 `max_new_tokens`\",\"\
          error_type\":\"validation\"}\r\n```"
        updatedAt: '2023-06-20T06:47:17.239Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F91D"
        users:
        - angelinayu
        - Garmisch
        - elodium
        - makeColabFree
    id: 64914b75ac1ff3475651356e
    type: comment
  author: Holynull
  content: "I always got an following error from AWS Sagemaker Inference Endpoint.\
    \ When I use langchain create a qa chain, its prompt template has more   then\
    \ 1000 tokens in general. How to solve this issue? \r\n```\r\n{\"error\":\"Input\
    \ validation error: `inputs` tokens + `max_new_tokens` must be <= 1512. Given:\
    \ 1682 `inputs` tokens and 200 `max_new_tokens`\",\"error_type\":\"validation\"\
    }\r\n```"
  created_at: 2023-06-20 05:47:17+00:00
  edited: false
  hidden: false
  id: 64914b75ac1ff3475651356e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8606a113b3b75430fdc43d95e7d2f7e7.svg
      fullname: Wong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Garmisch
      type: user
    createdAt: '2023-07-10T08:06:02.000Z'
    data:
      edited: false
      editors:
      - Garmisch
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9614667892456055
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8606a113b3b75430fdc43d95e7d2f7e7.svg
          fullname: Wong
          isHf: false
          isPro: false
          name: Garmisch
          type: user
        html: '<p>I also encountered the same problem. Did you find the solution?</p>

          '
        raw: I also encountered the same problem. Did you find the solution?
        updatedAt: '2023-07-10T08:06:02.338Z'
      numEdits: 0
      reactions: []
    id: 64abbbeabaf671fbebf7e1ce
    type: comment
  author: Garmisch
  content: I also encountered the same problem. Did you find the solution?
  created_at: 2023-07-10 07:06:02+00:00
  edited: false
  hidden: false
  id: 64abbbeabaf671fbebf7e1ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/15ed95c9a122b2affea4f4222511a323.svg
      fullname: Jay Pinho
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jaypinho
      type: user
    createdAt: '2023-07-20T12:23:46.000Z'
    data:
      edited: false
      editors:
      - jaypinho
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7759490013122559
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/15ed95c9a122b2affea4f4222511a323.svg
          fullname: Jay Pinho
          isHf: false
          isPro: false
          name: jaypinho
          type: user
        html: '<p>Having the same issue with meta-llama/Llama-2-13b-chat-hf. Where
          is this 1512 limitation coming from?</p>

          '
        raw: Having the same issue with meta-llama/Llama-2-13b-chat-hf. Where is this
          1512 limitation coming from?
        updatedAt: '2023-07-20T12:23:46.795Z'
      numEdits: 0
      reactions: []
    id: 64b9275249bde5d94833ddb9
    type: comment
  author: jaypinho
  content: Having the same issue with meta-llama/Llama-2-13b-chat-hf. Where is this
    1512 limitation coming from?
  created_at: 2023-07-20 11:23:46+00:00
  edited: false
  hidden: false
  id: 64b9275249bde5d94833ddb9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a9aa2745dc8415c2ef2ce41c5034c8f.svg
      fullname: Emil Wandel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ewandel
      type: user
    createdAt: '2023-07-21T11:39:09.000Z'
    data:
      edited: false
      editors:
      - ewandel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9015036225318909
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a9aa2745dc8415c2ef2ce41c5034c8f.svg
          fullname: Emil Wandel
          isHf: false
          isPro: true
          name: ewandel
          type: user
        html: '<p>Support explained: You can configure those when creating your endpoint.
          On inference endpoints in the UI when creating, in the advanced configuration
          section there are two fields: "max input length" and "max number of tokens".
          The latter is where the 1512 limitation is coming from. Change to your needs.
          I find it tricky to figure out which values are ok for the model and the
          platform, so far it''s been try&amp;error.</p>

          '
        raw: 'Support explained: You can configure those when creating your endpoint.
          On inference endpoints in the UI when creating, in the advanced configuration
          section there are two fields: "max input length" and "max number of tokens".
          The latter is where the 1512 limitation is coming from. Change to your needs.
          I find it tricky to figure out which values are ok for the model and the
          platform, so far it''s been try&error.'
        updatedAt: '2023-07-21T11:39:09.139Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jaypinho
    id: 64ba6e5d77dd483716ae22c2
    type: comment
  author: ewandel
  content: 'Support explained: You can configure those when creating your endpoint.
    On inference endpoints in the UI when creating, in the advanced configuration
    section there are two fields: "max input length" and "max number of tokens". The
    latter is where the 1512 limitation is coming from. Change to your needs. I find
    it tricky to figure out which values are ok for the model and the platform, so
    far it''s been try&error.'
  created_at: 2023-07-21 10:39:09+00:00
  edited: false
  hidden: false
  id: 64ba6e5d77dd483716ae22c2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 47
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: open
target_branch: null
title: 'About Input validation error: `inputs` tokens + `max_new_tokens` must be <=
  1512.'
