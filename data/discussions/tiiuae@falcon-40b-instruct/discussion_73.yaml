!!python/object:huggingface_hub.community.DiscussionWithDetails
author: humza-sami
conflicting_files: null
created_at: 2023-08-17 10:28:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633d6d4f48ab6a0add2ce1a3/qTO75kR0hk1Yn1SaP7ZPb.jpeg?w=200&h=200&f=face
      fullname: Humza Sami
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: humza-sami
      type: user
    createdAt: '2023-08-17T11:28:16.000Z'
    data:
      edited: false
      editors:
      - humza-sami
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6961754560470581
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633d6d4f48ab6a0add2ce1a3/qTO75kR0hk1Yn1SaP7ZPb.jpeg?w=200&h=200&f=face
          fullname: Humza Sami
          isHf: false
          isPro: false
          name: humza-sami
          type: user
        html: "<p>How can I optimize the inference time of my 4-bit quantized Falcon\
          \ 7B, which was trained on a chat dataset using Qlora+PEFT. During inference,\
          \ I loaded the model in 4 bits using the bits and bytes library. While the\
          \ model performs well in inference, I've observed that the inference time\
          \ increases significantly as the length of the chat grows. To give you an\
          \ idea, here's an example of how I'm using the model:</p>\n<p>First Message\
          \ prompt:<br>&lt; user&gt;: Hi ..<br>&lt; bot&gt;:</p>\n<p>2nd Message prompt:<br>&lt;\
          \ user&gt;: Hi ..<br>&lt; bot&gt;: How are you ?<br>&lt; user&gt;: I am\
          \ good thanks. I need your help !<br>&lt; bot&gt;: </p>\n<p>As the length\
          \ of the chat increases, the inference time sometimes doubles and can take\
          \ up to 2-3 minutes per prompt. I'm using an NVIDIA RTX 3090 Ti for inference.\
          \ Below is the code snippet I'm using for prediction:</p>\n<pre><code class=\"\
          language-python\">MODEL_NAME = <span class=\"hljs-string\">\"tiiuae/falcon-7b\"\
          </span>\n\nbnb_config = BitsAndBytesConfig(load_in_4bit=<span class=\"hljs-literal\"\
          >True</span>, bnb_4bit_use_double_quant=<span class=\"hljs-literal\">True</span>,\n\
          \                                bnb_4bit_quant_type=<span class=\"hljs-string\"\
          >\"nf4\"</span>, bnb_4bit_compute_dtype=torch.bfloat16)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\
          tokenizer.pad_token = tokenizer.eos_token\n\nSAVED_MODEL = <span class=\"\
          hljs-string\">\"Saved_models/model_path\"</span>\n\nsaved_model_config =\
          \ PeftConfig.from_pretrained(SAVED_MODEL)\nsaved_model = AutoModelForCausalLM.from_pretrained(saved_model_config.base_model_name_or_path,\n\
          \                                             return_dict=<span class=\"\
          hljs-literal\">True</span>,\n                                          \
          \   quantization_config=bnb_config,\n                                  \
          \           device_map=<span class=\"hljs-string\">\"auto\"</span>,\n  \
          \                                           trust_remote_code=<span class=\"\
          hljs-literal\">True</span>\n                                           \
          \ )\n\ntokenizer = AutoTokenizer.from_pretrained(saved_model_config.base_model_name_or_path)\n\
          tokenizer.pad_token = tokenizer.eos_token\n\nsaved_model = PeftModel.from_pretrained(saved_model,\
          \ SAVED_MODEL)\n\npipeline = transformers.pipeline(\n    <span class=\"\
          hljs-string\">\"text-generation\"</span>,\n    model=saved_model,\n    tokenizer=tokenizer,\n\
          \    torch_dtype=torch.bfloat16,\n    trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,\n    device_map=<span class=\"hljs-string\">\"auto\"</span>,\n\
          )\n\nprompt = <span class=\"hljs-string\">\"&lt; user&gt;: Hi ..</span>\n\
          <span class=\"hljs-string\">&lt; bot&gt;: How are you ?</span>\n<span class=\"\
          hljs-string\">&lt; user&gt;: I am good thanks. I need your help ! </span>\n\
          <span class=\"hljs-string\">&lt; bot&gt;: \"</span>\n\nresponse = pipeline(\n\
          \        prompt,\n        bos_token_id=<span class=\"hljs-number\">11</span>,\n\
          \        max_length=<span class=\"hljs-number\">2000</span>,\n        temperature=<span\
          \ class=\"hljs-number\">0.7</span>,\n        top_p=<span class=\"hljs-number\"\
          >0.7</span>,\n        do_sample=<span class=\"hljs-literal\">True</span>,\n\
          \        num_return_sequences=<span class=\"hljs-number\">1</span>,\n  \
          \      eos_token_id=[<span class=\"hljs-number\">15564</span>]\n       \
          \ )[<span class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'generated_text'</span>]\n\
          </code></pre>\n<p>I would greatly appreciate any insights or suggestions\
          \ on how to improve the inference time of model while dealing with longer\
          \ chat interactions. Thank you in advance for your assistance!</p>\n"
        raw: "How can I optimize the inference time of my 4-bit quantized Falcon 7B,\
          \ which was trained on a chat dataset using Qlora+PEFT. During inference,\
          \ I loaded the model in 4 bits using the bits and bytes library. While the\
          \ model performs well in inference, I've observed that the inference time\
          \ increases significantly as the length of the chat grows. To give you an\
          \ idea, here's an example of how I'm using the model:\r\n\r\nFirst Message\
          \ prompt: \r\n< user>: Hi ..\r\n< bot>:\r\n\r\n2nd Message prompt:\r\n<\
          \ user>: Hi ..\r\n< bot>: How are you ?\r\n< user>: I am good thanks. I\
          \ need your help ! \r\n< bot>: \r\n\r\nAs the length of the chat increases,\
          \ the inference time sometimes doubles and can take up to 2-3 minutes per\
          \ prompt. I'm using an NVIDIA RTX 3090 Ti for inference. Below is the code\
          \ snippet I'm using for prediction:\r\n\r\n```python\r\nMODEL_NAME = \"\
          tiiuae/falcon-7b\"\r\n\r\nbnb_config = BitsAndBytesConfig(load_in_4bit=True,\
          \ bnb_4bit_use_double_quant=True,\r\n                                bnb_4bit_quant_type=\"\
          nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\r\
          \ntokenizer.pad_token = tokenizer.eos_token\r\n\r\nSAVED_MODEL = \"Saved_models/model_path\"\
          \r\n\r\nsaved_model_config = PeftConfig.from_pretrained(SAVED_MODEL)\r\n\
          saved_model = AutoModelForCausalLM.from_pretrained(saved_model_config.base_model_name_or_path,\r\
          \n                                             return_dict=True,\r\n   \
          \                                          quantization_config=bnb_config,\r\
          \n                                             device_map=\"auto\",\r\n\
          \                                             trust_remote_code=True\r\n\
          \                                            )\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(saved_model_config.base_model_name_or_path)\r\
          \ntokenizer.pad_token = tokenizer.eos_token\r\n\r\nsaved_model = PeftModel.from_pretrained(saved_model,\
          \ SAVED_MODEL)\r\n\r\npipeline = transformers.pipeline(\r\n    \"text-generation\"\
          ,\r\n    model=saved_model,\r\n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\
          \n    trust_remote_code=True,\r\n    device_map=\"auto\",\r\n)\r\n\r\nprompt\
          \ = \"< user>: Hi ..\r\n< bot>: How are you ?\r\n< user>: I am good thanks.\
          \ I need your help ! \r\n< bot>: \"\r\n\r\nresponse = pipeline(\r\n    \
          \    prompt,\r\n        bos_token_id=11,\r\n        max_length=2000,\r\n\
          \        temperature=0.7,\r\n        top_p=0.7,\r\n        do_sample=True,\r\
          \n        num_return_sequences=1,\r\n        eos_token_id=[15564]\r\n  \
          \      )[0]['generated_text']\r\n```\r\n\r\nI would greatly appreciate any\
          \ insights or suggestions on how to improve the inference time of model\
          \ while dealing with longer chat interactions. Thank you in advance for\
          \ your assistance!\r\n"
        updatedAt: '2023-08-17T11:28:16.201Z'
      numEdits: 0
      reactions: []
    id: 64de04505a8a9efea8d8c702
    type: comment
  author: humza-sami
  content: "How can I optimize the inference time of my 4-bit quantized Falcon 7B,\
    \ which was trained on a chat dataset using Qlora+PEFT. During inference, I loaded\
    \ the model in 4 bits using the bits and bytes library. While the model performs\
    \ well in inference, I've observed that the inference time increases significantly\
    \ as the length of the chat grows. To give you an idea, here's an example of how\
    \ I'm using the model:\r\n\r\nFirst Message prompt: \r\n< user>: Hi ..\r\n< bot>:\r\
    \n\r\n2nd Message prompt:\r\n< user>: Hi ..\r\n< bot>: How are you ?\r\n< user>:\
    \ I am good thanks. I need your help ! \r\n< bot>: \r\n\r\nAs the length of the\
    \ chat increases, the inference time sometimes doubles and can take up to 2-3\
    \ minutes per prompt. I'm using an NVIDIA RTX 3090 Ti for inference. Below is\
    \ the code snippet I'm using for prediction:\r\n\r\n```python\r\nMODEL_NAME =\
    \ \"tiiuae/falcon-7b\"\r\n\r\nbnb_config = BitsAndBytesConfig(load_in_4bit=True,\
    \ bnb_4bit_use_double_quant=True,\r\n                                bnb_4bit_quant_type=\"\
    nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\r\
    \ntokenizer.pad_token = tokenizer.eos_token\r\n\r\nSAVED_MODEL = \"Saved_models/model_path\"\
    \r\n\r\nsaved_model_config = PeftConfig.from_pretrained(SAVED_MODEL)\r\nsaved_model\
    \ = AutoModelForCausalLM.from_pretrained(saved_model_config.base_model_name_or_path,\r\
    \n                                             return_dict=True,\r\n         \
    \                                    quantization_config=bnb_config,\r\n     \
    \                                        device_map=\"auto\",\r\n            \
    \                                 trust_remote_code=True\r\n                 \
    \                           )\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(saved_model_config.base_model_name_or_path)\r\
    \ntokenizer.pad_token = tokenizer.eos_token\r\n\r\nsaved_model = PeftModel.from_pretrained(saved_model,\
    \ SAVED_MODEL)\r\n\r\npipeline = transformers.pipeline(\r\n    \"text-generation\"\
    ,\r\n    model=saved_model,\r\n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\
    \n    trust_remote_code=True,\r\n    device_map=\"auto\",\r\n)\r\n\r\nprompt =\
    \ \"< user>: Hi ..\r\n< bot>: How are you ?\r\n< user>: I am good thanks. I need\
    \ your help ! \r\n< bot>: \"\r\n\r\nresponse = pipeline(\r\n        prompt,\r\n\
    \        bos_token_id=11,\r\n        max_length=2000,\r\n        temperature=0.7,\r\
    \n        top_p=0.7,\r\n        do_sample=True,\r\n        num_return_sequences=1,\r\
    \n        eos_token_id=[15564]\r\n        )[0]['generated_text']\r\n```\r\n\r\n\
    I would greatly appreciate any insights or suggestions on how to improve the inference\
    \ time of model while dealing with longer chat interactions. Thank you in advance\
    \ for your assistance!\r\n"
  created_at: 2023-08-17 10:28:16+00:00
  edited: false
  hidden: false
  id: 64de04505a8a9efea8d8c702
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 73
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: open
target_branch: null
title: Optimizing Inference Time for Chat Conversations on Falcon
