!!python/object:huggingface_hub.community.DiscussionWithDetails
author: eastwind
conflicting_files: null
created_at: 2023-06-02 09:34:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-02T10:34:29.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: "<p>Here is the code</p>\n<pre><code>from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"\
          tiiuae/falcon-40b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=\"auto\",\n)\n\ntext = f\"\"\"\nUser: What is quantum tunneling?\n\
          Assistant:\n\"\"\"\n\nsequences = pipeline(\n    text,\n    max_length=200,\n\
          \    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n</code></pre>\n<p>When running this on 4 Tesla v100s I get this output</p>\n\
          <pre><code>(Result: \nUser: What is quantum tunneling?\nAssistant:\n&lt;function(4\n\
          (1\"(`var\n\n//\"`\"\"31\"\n1\n\n44&lt;1\n&lt;function&lt;function(`&lt;`2\n\
          \n\"(&lt;(function(1\n&lt;var2(\n(4&lt;(\n4\n1(\n&lt;\n12\"\"(3'1(3\n44&lt;\"\
          34&lt;\n1`(\n\u5404\u79CD=(2\"\nfunction&gt;\n21&lt;function\"\n\n\n&lt;p`function\n\
          (2'4'3'4\n22\n11\n123\n//(((4\"`3\n1'2'\nfunction(4\n\n\"3(2(4-\"(\n\"``\n\
          (\"\"`(\"(4\"\"'21\n&lt;((\"12(2``)\n</code></pre>\n<p>side note took 10mins\
          \ for this btw.</p>\n"
        raw: "Here is the code\r\n\r\n```\r\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\r\nimport transformers\r\nimport torch\r\n\r\nmodel\
          \ = \"tiiuae/falcon-40b-instruct\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\
          \npipeline = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
          \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\
          \n    device_map=\"auto\",\r\n)\r\n\r\ntext = f\"\"\"\r\nUser: What is quantum\
          \ tunneling?\r\nAssistant:\r\n\"\"\"\r\n\r\nsequences = pipeline(\r\n  \
          \  text,\r\n    max_length=200,\r\n    do_sample=True,\r\n    top_k=10,\r\
          \n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
          \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
          )\r\n```\r\n\r\nWhen running this on 4 Tesla v100s I get this output\r\n\
          \r\n```\r\n(Result: \r\nUser: What is quantum tunneling?\r\nAssistant:\r\
          \n<function(4\r\n(1\"(`var\r\n\r\n//\"`\"\"31\"\r\n1\r\n\r\n44<1\r\n<function<function(`<`2\r\
          \n\r\n\"(<(function(1\r\n<var2(\r\n(4<(\r\n4\r\n1(\r\n<\r\n12\"\"(3'1(3\r\
          \n44<\"34<\r\n1`(\r\n\u5404\u79CD=(2\"\r\nfunction>\r\n21<function\"\r\n\
          \r\n\r\n<p`function\r\n(2'4'3'4\r\n22\r\n11\r\n123\r\n//(((4\"`3\r\n1'2'\r\
          \nfunction(4\r\n\r\n\"3(2(4-\"(\r\n\"``\r\n(\"\"`(\"(4\"\"'21\r\n<((\"12(2``)\r\
          \n```\r\n\r\nside note took 10mins for this btw."
        updatedAt: '2023-06-02T10:34:29.495Z'
      numEdits: 0
      reactions: []
    id: 6479c5b5ed75e95d3e969c7b
    type: comment
  author: eastwind
  content: "Here is the code\r\n\r\n```\r\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\r\nimport transformers\r\nimport torch\r\n\r\nmodel = \"\
    tiiuae/falcon-40b-instruct\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\
    \npipeline = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
    \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\
    \n    device_map=\"auto\",\r\n)\r\n\r\ntext = f\"\"\"\r\nUser: What is quantum\
    \ tunneling?\r\nAssistant:\r\n\"\"\"\r\n\r\nsequences = pipeline(\r\n    text,\r\
    \n    max_length=200,\r\n    do_sample=True,\r\n    top_k=10,\r\n    num_return_sequences=1,\r\
    \n    eos_token_id=tokenizer.eos_token_id,\r\n)\r\nfor seq in sequences:\r\n \
    \   print(f\"Result: {seq['generated_text']}\")\r\n```\r\n\r\nWhen running this\
    \ on 4 Tesla v100s I get this output\r\n\r\n```\r\n(Result: \r\nUser: What is\
    \ quantum tunneling?\r\nAssistant:\r\n<function(4\r\n(1\"(`var\r\n\r\n//\"`\"\"\
    31\"\r\n1\r\n\r\n44<1\r\n<function<function(`<`2\r\n\r\n\"(<(function(1\r\n<var2(\r\
    \n(4<(\r\n4\r\n1(\r\n<\r\n12\"\"(3'1(3\r\n44<\"34<\r\n1`(\r\n\u5404\u79CD=(2\"\
    \r\nfunction>\r\n21<function\"\r\n\r\n\r\n<p`function\r\n(2'4'3'4\r\n22\r\n11\r\
    \n123\r\n//(((4\"`3\r\n1'2'\r\nfunction(4\r\n\r\n\"3(2(4-\"(\r\n\"``\r\n(\"\"\
    `(\"(4\"\"'21\r\n<((\"12(2``)\r\n```\r\n\r\nside note took 10mins for this btw."
  created_at: 2023-06-02 09:34:29+00:00
  edited: false
  hidden: false
  id: 6479c5b5ed75e95d3e969c7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-02T14:34:29.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p><code>use_cache = False</code></p>

          <p>Seems to have fixed the issue for now, but I assume that this makes it
          very slow as it took 5 mins for 100tokens</p>

          '
        raw: '```use_cache = False```


          Seems to have fixed the issue for now, but I assume that this makes it very
          slow as it took 5 mins for 100tokens'
        updatedAt: '2023-06-02T14:34:29.108Z'
      numEdits: 0
      reactions: []
    id: 6479fdf5a84498f2af4c055b
    type: comment
  author: eastwind
  content: '```use_cache = False```


    Seems to have fixed the issue for now, but I assume that this makes it very slow
    as it took 5 mins for 100tokens'
  created_at: 2023-06-02 13:34:29+00:00
  edited: false
  hidden: false
  id: 6479fdf5a84498f2af4c055b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a748c1c117ac45df99f8ba950131bb67.svg
      fullname: Kai Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkdtckk
      type: user
    createdAt: '2023-06-03T15:04:20.000Z'
    data:
      edited: false
      editors:
      - zkdtckk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9309449195861816
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a748c1c117ac45df99f8ba950131bb67.svg
          fullname: Kai Zhang
          isHf: false
          isPro: false
          name: zkdtckk
          type: user
        html: '<p>v100 does not support bfloat16, thus it probably is using fp32,
          resulting a decrease of several times of tflops. Change to A10G or better
          like A100 GPU</p>

          '
        raw: v100 does not support bfloat16, thus it probably is using fp32, resulting
          a decrease of several times of tflops. Change to A10G or better like A100
          GPU
        updatedAt: '2023-06-03T15:04:20.222Z'
      numEdits: 0
      reactions: []
    id: 647b56746dbad6ab057b78a1
    type: comment
  author: zkdtckk
  content: v100 does not support bfloat16, thus it probably is using fp32, resulting
    a decrease of several times of tflops. Change to A10G or better like A100 GPU
  created_at: 2023-06-03 14:04:20+00:00
  edited: false
  hidden: false
  id: 647b56746dbad6ab057b78a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
      fullname: Bernd Brassel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captain-fim
      type: user
    createdAt: '2023-06-04T09:03:46.000Z'
    data:
      edited: false
      editors:
      - captain-fim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9778430461883545
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
          fullname: Bernd Brassel
          isHf: false
          isPro: false
          name: captain-fim
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;zkdtckk&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zkdtckk\">@<span class=\"\
          underline\">zkdtckk</span></a></span>\n\n\t</span></span> I had it running\
          \ on a machine with 8 x A100 80GB GPUs.<br>It took some 10 minutes to give\
          \ a response to that Giraffe prompt given in the example script.<br>Did\
          \ you fare better? If so, how?</p>\n<p>It also produced gibberish as described\
          \ by <span data-props=\"{&quot;user&quot;:&quot;eastwind&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/eastwind\">@<span class=\"\
          underline\">eastwind</span></a></span>\n\n\t</span></span>.<br>But I am\
          \ not sure I want to try the <code>use_cache</code> option.<br>If it is\
          \ that slow it is far to costly to use.</p>\n"
        raw: "@zkdtckk I had it running on a machine with 8 x A100 80GB GPUs. \nIt\
          \ took some 10 minutes to give a response to that Giraffe prompt given in\
          \ the example script.\nDid you fare better? If so, how?\n\nIt also produced\
          \ gibberish as described by @eastwind. \nBut I am not sure I want to try\
          \ the `use_cache` option.\nIf it is that slow it is far to costly to use."
        updatedAt: '2023-06-04T09:03:46.293Z'
      numEdits: 0
      reactions: []
    id: 647c53721c0644de8d1f8616
    type: comment
  author: captain-fim
  content: "@zkdtckk I had it running on a machine with 8 x A100 80GB GPUs. \nIt took\
    \ some 10 minutes to give a response to that Giraffe prompt given in the example\
    \ script.\nDid you fare better? If so, how?\n\nIt also produced gibberish as described\
    \ by @eastwind. \nBut I am not sure I want to try the `use_cache` option.\nIf\
    \ it is that slow it is far to costly to use."
  created_at: 2023-06-04 08:03:46+00:00
  edited: false
  hidden: false
  id: 647c53721c0644de8d1f8616
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a748c1c117ac45df99f8ba950131bb67.svg
      fullname: Kai Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkdtckk
      type: user
    createdAt: '2023-06-04T14:48:51.000Z'
    data:
      edited: false
      editors:
      - zkdtckk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9777300953865051
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a748c1c117ac45df99f8ba950131bb67.svg
          fullname: Kai Zhang
          isHf: false
          isPro: false
          name: zkdtckk
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;captain-fim&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/captain-fim\"\
          >@<span class=\"underline\">captain-fim</span></a></span>\n\n\t</span></span>\
          \ 10min on 8 A100 (p4d?) is really slow, there should be something wrong.\
          \ It took me a few mins on 8 A10 GPU setup to run the Giraffe prompt.<br>The\
          \ speed of inference is really a problem for this model, we need to figure\
          \ out a way to speed it up.</p>\n"
        raw: "@captain-fim 10min on 8 A100 (p4d?) is really slow, there should be\
          \ something wrong. It took me a few mins on 8 A10 GPU setup to run the Giraffe\
          \ prompt. \nThe speed of inference is really a problem for this model, we\
          \ need to figure out a way to speed it up."
        updatedAt: '2023-06-04T14:48:51.080Z'
      numEdits: 0
      reactions: []
    id: 647ca453c788767ab5d00794
    type: comment
  author: zkdtckk
  content: "@captain-fim 10min on 8 A100 (p4d?) is really slow, there should be something\
    \ wrong. It took me a few mins on 8 A10 GPU setup to run the Giraffe prompt. \n\
    The speed of inference is really a problem for this model, we need to figure out\
    \ a way to speed it up."
  created_at: 2023-06-04 13:48:51+00:00
  edited: false
  hidden: false
  id: 647ca453c788767ab5d00794
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
      fullname: Bernd Brassel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captain-fim
      type: user
    createdAt: '2023-06-04T15:29:49.000Z'
    data:
      edited: false
      editors:
      - captain-fim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9740353226661682
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
          fullname: Bernd Brassel
          isHf: false
          isPro: false
          name: captain-fim
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;zkdtckk&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zkdtckk\">@<span class=\"\
          underline\">zkdtckk</span></a></span>\n\n\t</span></span> </p>\n<blockquote>\n\
          <p>10min on 8 A100 (p4d?) is really slow, there should be something wrong.</p>\n\
          </blockquote>\n<p>Yes, that is what I thought. And also it will be even\
          \ slower with turning the cache off as <span data-props=\"{&quot;user&quot;:&quot;eastwind&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/eastwind\"\
          >@<span class=\"underline\">eastwind</span></a></span>\n\n\t</span></span>\
          \ proposes, will it not? :-(<br>Did you need to turn the cache off, too,\
          \ <span data-props=\"{&quot;user&quot;:&quot;zkdtckk&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zkdtckk\">@<span class=\"\
          underline\">zkdtckk</span></a></span>\n\n\t</span></span> ?<br>I still hope\
          \ something is wrong with our setup.</p>\n"
        raw: "@zkdtckk \n\n> 10min on 8 A100 (p4d?) is really slow, there should be\
          \ something wrong.\n\nYes, that is what I thought. And also it will be even\
          \ slower with turning the cache off as @eastwind proposes, will it not?\
          \ :-(\nDid you need to turn the cache off, too, @zkdtckk ?\nI still hope\
          \ something is wrong with our setup."
        updatedAt: '2023-06-04T15:29:49.183Z'
      numEdits: 0
      reactions: []
    id: 647caded83c62f32491d49f8
    type: comment
  author: captain-fim
  content: "@zkdtckk \n\n> 10min on 8 A100 (p4d?) is really slow, there should be\
    \ something wrong.\n\nYes, that is what I thought. And also it will be even slower\
    \ with turning the cache off as @eastwind proposes, will it not? :-(\nDid you\
    \ need to turn the cache off, too, @zkdtckk ?\nI still hope something is wrong\
    \ with our setup."
  created_at: 2023-06-04 14:29:49+00:00
  edited: false
  hidden: false
  id: 647caded83c62f32491d49f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-04T16:37:48.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9461271166801453
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>I think it''s an issue with multi GPU Inference. Due to the custom
          code implemented by falcon. Maybe there''s an issue with sharding the model
          over multiple gpus using the device map from accelerate.</p>

          '
        raw: I think it's an issue with multi GPU Inference. Due to the custom code
          implemented by falcon. Maybe there's an issue with sharding the model over
          multiple gpus using the device map from accelerate.
        updatedAt: '2023-06-04T16:37:48.290Z'
      numEdits: 0
      reactions: []
    id: 647cbddcc788767ab5d2ebbe
    type: comment
  author: eastwind
  content: I think it's an issue with multi GPU Inference. Due to the custom code
    implemented by falcon. Maybe there's an issue with sharding the model over multiple
    gpus using the device map from accelerate.
  created_at: 2023-06-04 15:37:48+00:00
  edited: false
  hidden: false
  id: 647cbddcc788767ab5d2ebbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-04T16:43:14.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9780795574188232
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;captain-fim&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/captain-fim\"\
          >@<span class=\"underline\">captain-fim</span></a></span>\n\n\t</span></span>\
          \ it didn't take that long. I used 4 v100s. Took 3 mins for 200 tokens</p>\n"
        raw: '@captain-fim it didn''t take that long. I used 4 v100s. Took 3 mins
          for 200 tokens'
        updatedAt: '2023-06-04T16:43:14.241Z'
      numEdits: 0
      reactions: []
    id: 647cbf22c788767ab5d31f56
    type: comment
  author: eastwind
  content: '@captain-fim it didn''t take that long. I used 4 v100s. Took 3 mins for
    200 tokens'
  created_at: 2023-06-04 15:43:14+00:00
  edited: false
  hidden: false
  id: 647cbf22c788767ab5d31f56
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-04T17:04:24.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8222621083259583
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: "<p><a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/15399\"\
          >https://github.com/huggingface/transformers/issues/15399</a></p>\n<p>This\
          \ mentions that it might be an issue with fp16. Or bfloat16. Like <span\
          \ data-props=\"{&quot;user&quot;:&quot;zkdtckk&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/zkdtckk\">@<span class=\"underline\"\
          >zkdtckk</span></a></span>\n\n\t</span></span> mentioned. I will try tomorrow\
          \ without quantisation.</p>\n"
        raw: 'https://github.com/huggingface/transformers/issues/15399


          This mentions that it might be an issue with fp16. Or bfloat16. Like @zkdtckk
          mentioned. I will try tomorrow without quantisation.'
        updatedAt: '2023-06-04T17:04:24.548Z'
      numEdits: 0
      reactions: []
    id: 647cc41860dfe0f35d57f0ca
    type: comment
  author: eastwind
  content: 'https://github.com/huggingface/transformers/issues/15399


    This mentions that it might be an issue with fp16. Or bfloat16. Like @zkdtckk
    mentioned. I will try tomorrow without quantisation.'
  created_at: 2023-06-04 16:04:24+00:00
  edited: false
  hidden: false
  id: 647cc41860dfe0f35d57f0ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-05T20:13:43.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9669739007949829
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>Working theory is that the shared KV cache doesn''t work for multi
          GPU.</p>

          '
        raw: Working theory is that the shared KV cache doesn't work for multi GPU.
        updatedAt: '2023-06-05T20:13:43.543Z'
      numEdits: 0
      reactions: []
    id: 647e41f732c471a7fa96a9cd
    type: comment
  author: eastwind
  content: Working theory is that the shared KV cache doesn't work for multi GPU.
  created_at: 2023-06-05 19:13:43+00:00
  edited: false
  hidden: false
  id: 647e41f732c471a7fa96a9cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:52:54.000Z'
    data:
      edited: true
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7340664267539978
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>We would recommend using <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference">Text
          Generation Inference</a> for fast inference with Falcon. See this <a href="https://huggingface.co/blog/falcon">blog</a>
          for more information.</p>

          '
        raw: We would recommend using [Text Generation Inference](https://github.com/huggingface/text-generation-inference)
          for fast inference with Falcon. See this [blog](https://huggingface.co/blog/falcon)
          for more information.
        updatedAt: '2023-06-09T14:54:11.862Z'
      numEdits: 1
      reactions: []
    id: 64833cc612e9cb42413161e7
    type: comment
  author: FalconLLM
  content: We would recommend using [Text Generation Inference](https://github.com/huggingface/text-generation-inference)
    for fast inference with Falcon. See this [blog](https://huggingface.co/blog/falcon)
    for more information.
  created_at: 2023-06-09 13:52:54+00:00
  edited: true
  hidden: false
  id: 64833cc612e9cb42413161e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7e67c379762f11e75410b53fd8ca42e.svg
      fullname: Deepak Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thedkk
      type: user
    createdAt: '2023-06-26T21:10:47.000Z'
    data:
      edited: false
      editors:
      - thedkk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7447699904441833
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7e67c379762f11e75410b53fd8ca42e.svg
          fullname: Deepak Kumar
          isHf: false
          isPro: false
          name: thedkk
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;FalconLLM&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/FalconLLM\">@<span class=\"\
          underline\">FalconLLM</span></a></span>\n\n\t</span></span> Have we benchmarked\
          \ response time of falcon 40-b when we deploy the endpoint over sagemaker?</p>\n"
        raw: '@FalconLLM Have we benchmarked response time of falcon 40-b when we
          deploy the endpoint over sagemaker?

          '
        updatedAt: '2023-06-26T21:10:47.122Z'
      numEdits: 0
      reactions: []
    id: 6499fed7d03d717811423f1e
    type: comment
  author: thedkk
  content: '@FalconLLM Have we benchmarked response time of falcon 40-b when we deploy
    the endpoint over sagemaker?

    '
  created_at: 2023-06-26 20:10:47+00:00
  edited: false
  hidden: false
  id: 6499fed7d03d717811423f1e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: open
target_branch: null
title: Slow and Gibberish when inferencing
