!!python/object:huggingface_hub.community.DiscussionWithDetails
author: peyers
conflicting_files: null
created_at: 2023-07-13 12:55:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c65eef1c23bdda12ed0e6a6538e793f5.svg
      fullname: Simon Peyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: peyers
      type: user
    createdAt: '2023-07-13T13:55:46.000Z'
    data:
      edited: false
      editors:
      - peyers
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6152457594871521
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c65eef1c23bdda12ed0e6a6538e793f5.svg
          fullname: Simon Peyer
          isHf: false
          isPro: false
          name: peyers
          type: user
        html: '<p>Using the latest model of falcon-40b-instruct, there is a problem,
          when run on Sagemaker.<br>The endpoint can not be started when using these
          instructions <a rel="nofollow" href="https://github.com/marshmellow77/falcon-document-chatbot/blob/main/deploy-falcon-40b-instruct.ipynb">https://github.com/marshmellow77/falcon-document-chatbot/blob/main/deploy-falcon-40b-instruct.ipynb</a></p>

          <p>Yesterday everything worked fine.</p>

          <p>The error message ist, the following:<br>ValueError: sharded is not supported
          for AutoModel</p>

          <p>Current Workaround is to use the latest Revision which works:</p>

          <p>llm_model = HuggingFaceModel(<br>  role=role,<br>  image_uri=llm_image,<br>  env={<br>    ''HF_MODEL_ID'':
          hf_model_id,<br>    ''HF_MODEL_REVISION'': "1e7fdcc9f45d13704f3826e99937917e007cd975",<br>    #
          ''HF_MODEL_QUANTIZE'': "bitsandbytes", # comment in to quantize<br>    ''SM_NUM_GPUS'':
          json.dumps(number_of_gpu),<br>    ''MAX_INPUT_LENGTH'': json.dumps(1900),  #
          Max length of input text<br>    ''MAX_TOTAL_TOKENS'': json.dumps(2048),  #
          Max length of the generation (including input text)<br>  }<br>)</p>

          '
        raw: "Using the latest model of falcon-40b-instruct, there is a problem, when\
          \ run on Sagemaker.\r\nThe endpoint can not be started when using these\
          \ instructions https://github.com/marshmellow77/falcon-document-chatbot/blob/main/deploy-falcon-40b-instruct.ipynb\r\
          \n\r\nYesterday everything worked fine.\r\n\r\nThe error message ist, the\
          \ following:\r\nValueError: sharded is not supported for AutoModel\r\n \r\
          \nCurrent Workaround is to use the latest Revision which works:\r\n\r\n\
          llm_model = HuggingFaceModel(\r\n  role=role,\r\n  image_uri=llm_image,\r\
          \n  env={\r\n    'HF_MODEL_ID': hf_model_id,\r\n    'HF_MODEL_REVISION':\
          \ \"1e7fdcc9f45d13704f3826e99937917e007cd975\",\r\n    # 'HF_MODEL_QUANTIZE':\
          \ \"bitsandbytes\", # comment in to quantize\r\n    'SM_NUM_GPUS': json.dumps(number_of_gpu),\r\
          \n    'MAX_INPUT_LENGTH': json.dumps(1900),  # Max length of input text\r\
          \n    'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation\
          \ (including input text)\r\n  }\r\n)\r\n"
        updatedAt: '2023-07-13T13:55:46.526Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - eisenzopf
    id: 64b0026251e993d33be1aa41
    type: comment
  author: peyers
  content: "Using the latest model of falcon-40b-instruct, there is a problem, when\
    \ run on Sagemaker.\r\nThe endpoint can not be started when using these instructions\
    \ https://github.com/marshmellow77/falcon-document-chatbot/blob/main/deploy-falcon-40b-instruct.ipynb\r\
    \n\r\nYesterday everything worked fine.\r\n\r\nThe error message ist, the following:\r\
    \nValueError: sharded is not supported for AutoModel\r\n \r\nCurrent Workaround\
    \ is to use the latest Revision which works:\r\n\r\nllm_model = HuggingFaceModel(\r\
    \n  role=role,\r\n  image_uri=llm_image,\r\n  env={\r\n    'HF_MODEL_ID': hf_model_id,\r\
    \n    'HF_MODEL_REVISION': \"1e7fdcc9f45d13704f3826e99937917e007cd975\",\r\n \
    \   # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\r\n    'SM_NUM_GPUS':\
    \ json.dumps(number_of_gpu),\r\n    'MAX_INPUT_LENGTH': json.dumps(1900),  # Max\
    \ length of input text\r\n    'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length\
    \ of the generation (including input text)\r\n  }\r\n)\r\n"
  created_at: 2023-07-13 12:55:46+00:00
  edited: false
  hidden: false
  id: 64b0026251e993d33be1aa41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0856af4f9179a463e17d165bab27e409.svg
      fullname: Abhigyan Shivaditya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ashivadi
      type: user
    createdAt: '2023-07-13T15:55:02.000Z'
    data:
      edited: true
      editors:
      - ashivadi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6680406332015991
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0856af4f9179a463e17d165bab27e409.svg
          fullname: Abhigyan Shivaditya
          isHf: false
          isPro: false
          name: ashivadi
          type: user
        html: '<p>Could be because of this change: <a href="https://huggingface.co/tiiuae/falcon-40b/commit/f1ba7d328c06aa6fbb4a8afd3c756f46d7e6b232">https://huggingface.co/tiiuae/falcon-40b/commit/f1ba7d328c06aa6fbb4a8afd3c756f46d7e6b232</a>
          and line here<br><a rel="nofollow" href="https://github.com/huggingface/text-generation-inference/blob/b7327205a6f2f2c6349e75b8ea484e1e2823075a/server/text_generation_server/models/__init__.py#L233">https://github.com/huggingface/text-generation-inference/blob/b7327205a6f2f2c6349e75b8ea484e1e2823075a/server/text_generation_server/models/__init__.py#L233</a></p>

          '
        raw: 'Could be because of this change: https://huggingface.co/tiiuae/falcon-40b/commit/f1ba7d328c06aa6fbb4a8afd3c756f46d7e6b232
          and line here

          https://github.com/huggingface/text-generation-inference/blob/b7327205a6f2f2c6349e75b8ea484e1e2823075a/server/text_generation_server/models/__init__.py#L233'
        updatedAt: '2023-07-13T15:56:09.546Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mazeiar
    id: 64b01e5642e482f17ebae67b
    type: comment
  author: ashivadi
  content: 'Could be because of this change: https://huggingface.co/tiiuae/falcon-40b/commit/f1ba7d328c06aa6fbb4a8afd3c756f46d7e6b232
    and line here

    https://github.com/huggingface/text-generation-inference/blob/b7327205a6f2f2c6349e75b8ea484e1e2823075a/server/text_generation_server/models/__init__.py#L233'
  created_at: 2023-07-13 14:55:02+00:00
  edited: true
  hidden: false
  id: 64b01e5642e482f17ebae67b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fcce68169f1dce23bbe6b/tpJfj85JXK4TvO6XQXEW1.png?w=200&h=200&f=face
      fullname: Rodger Moore
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rodgermoore
      type: user
    createdAt: '2023-07-14T05:01:57.000Z'
    data:
      edited: false
      editors:
      - rodgermoore
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8545600771903992
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fcce68169f1dce23bbe6b/tpJfj85JXK4TvO6XQXEW1.png?w=200&h=200&f=face
          fullname: Rodger Moore
          isHf: false
          isPro: false
          name: rodgermoore
          type: user
        html: '<blockquote>

          <p>Could be because of this change: <a href="https://huggingface.co/tiiuae/falcon-40b/commit/f1ba7d328c06aa6fbb4a8afd3c756f46d7e6b232">https://huggingface.co/tiiuae/falcon-40b/commit/f1ba7d328c06aa6fbb4a8afd3c756f46d7e6b232</a>
          and line here<br><a rel="nofollow" href="https://github.com/huggingface/text-generation-inference/blob/b7327205a6f2f2c6349e75b8ea484e1e2823075a/server/text_generation_server/models/__init__.py#L233">https://github.com/huggingface/text-generation-inference/blob/b7327205a6f2f2c6349e75b8ea484e1e2823075a/server/text_generation_server/models/__init__.py#L233</a></p>

          </blockquote>

          <p>This is exactly what I''m running into when trying to make this work.
          I thought this was an issue with the HF inference server, thanks for pointing
          this out!</p>

          '
        raw: '> Could be because of this change: https://huggingface.co/tiiuae/falcon-40b/commit/f1ba7d328c06aa6fbb4a8afd3c756f46d7e6b232
          and line here

          > https://github.com/huggingface/text-generation-inference/blob/b7327205a6f2f2c6349e75b8ea484e1e2823075a/server/text_generation_server/models/__init__.py#L233


          This is exactly what I''m running into when trying to make this work. I
          thought this was an issue with the HF inference server, thanks for pointing
          this out!'
        updatedAt: '2023-07-14T05:01:57.105Z'
      numEdits: 0
      reactions: []
    id: 64b0d6c5f12b47366668ee6d
    type: comment
  author: rodgermoore
  content: '> Could be because of this change: https://huggingface.co/tiiuae/falcon-40b/commit/f1ba7d328c06aa6fbb4a8afd3c756f46d7e6b232
    and line here

    > https://github.com/huggingface/text-generation-inference/blob/b7327205a6f2f2c6349e75b8ea484e1e2823075a/server/text_generation_server/models/__init__.py#L233


    This is exactly what I''m running into when trying to make this work. I thought
    this was an issue with the HF inference server, thanks for pointing this out!'
  created_at: 2023-07-14 04:01:57+00:00
  edited: false
  hidden: false
  id: 64b0d6c5f12b47366668ee6d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f7282a1710c3bcf1f8a7a5fa1d16e1b.svg
      fullname: Hugging Fan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hugefan
      type: user
    createdAt: '2023-07-14T08:25:16.000Z'
    data:
      edited: false
      editors:
      - hugefan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9085131883621216
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f7282a1710c3bcf1f8a7a5fa1d16e1b.svg
          fullname: Hugging Fan
          isHf: false
          isPro: false
          name: hugefan
          type: user
        html: '<p>The problematic change has been reverted with <a href="https://huggingface.co/tiiuae/falcon-40b-instruct/commit/ca78eac0ed45bf64445ff0687fabba1598daebf3">https://huggingface.co/tiiuae/falcon-40b-instruct/commit/ca78eac0ed45bf64445ff0687fabba1598daebf3</a>
          , everything works like before now with the currently released files on
          main.</p>

          '
        raw: The problematic change has been reverted with https://huggingface.co/tiiuae/falcon-40b-instruct/commit/ca78eac0ed45bf64445ff0687fabba1598daebf3
          , everything works like before now with the currently released files on
          main.
        updatedAt: '2023-07-14T08:25:16.886Z'
      numEdits: 0
      reactions: []
    id: 64b1066c34139a0ee441103b
    type: comment
  author: hugefan
  content: The problematic change has been reverted with https://huggingface.co/tiiuae/falcon-40b-instruct/commit/ca78eac0ed45bf64445ff0687fabba1598daebf3
    , everything works like before now with the currently released files on main.
  created_at: 2023-07-14 07:25:16+00:00
  edited: false
  hidden: false
  id: 64b1066c34139a0ee441103b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cad37c8eb7cf73d5b72a9031585813b0.svg
      fullname: Sergey Mkrtchyan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mkserge
      type: user
    createdAt: '2023-07-20T21:11:42.000Z'
    data:
      edited: false
      editors:
      - mkserge
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6801224946975708
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cad37c8eb7cf73d5b72a9031585813b0.svg
          fullname: Sergey Mkrtchyan
          isHf: false
          isPro: false
          name: mkserge
          type: user
        html: "<p>Hello,</p>\n<p>I am still running into the same issue, with 7b-instruct\
          \ version when explicitly pointing to the commit that reverts the change</p>\n\
          <pre><code>config = {\n  'HF_MODEL_ID': \"tiiuae/falcon-7b-instruct\", #\
          \ model_id from hf.co/models\n  'HF_MODEL_REVISION': \"eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\"\
          ,\n  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per\
          \ replica\n  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input\
          \ text\n  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation\
          \ (including input text)\n  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment\
          \ in to quantize\n}\n</code></pre>\n<p>Running on <code>ml.g5.48xlarge</code>\
          \ with <code>number_of_gpu = 8</code> above.</p>\n<p>Any ideas what could\
          \ be wrong in my setup?</p>\n"
        raw: "Hello,\n\nI am still running into the same issue, with 7b-instruct version\
          \ when explicitly pointing to the commit that reverts the change\n```\n\
          config = {\n  'HF_MODEL_ID': \"tiiuae/falcon-7b-instruct\", # model_id from\
          \ hf.co/models\n  'HF_MODEL_REVISION': \"eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\"\
          ,\n  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per\
          \ replica\n  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input\
          \ text\n  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation\
          \ (including input text)\n  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment\
          \ in to quantize\n}\n```\n\nRunning on `ml.g5.48xlarge` with `number_of_gpu\
          \ = 8` above.\n\nAny ideas what could be wrong in my setup?"
        updatedAt: '2023-07-20T21:11:42.924Z'
      numEdits: 0
      reactions: []
    id: 64b9a30eb7d190d775ced97e
    type: comment
  author: mkserge
  content: "Hello,\n\nI am still running into the same issue, with 7b-instruct version\
    \ when explicitly pointing to the commit that reverts the change\n```\nconfig\
    \ = {\n  'HF_MODEL_ID': \"tiiuae/falcon-7b-instruct\", # model_id from hf.co/models\n\
    \  'HF_MODEL_REVISION': \"eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\",\n  'SM_NUM_GPUS':\
    \ json.dumps(number_of_gpu), # Number of GPU used per replica\n  'MAX_INPUT_LENGTH':\
    \ json.dumps(1024),  # Max length of input text\n  'MAX_TOTAL_TOKENS': json.dumps(2048),\
    \  # Max length of the generation (including input text)\n  # 'HF_MODEL_QUANTIZE':\
    \ \"bitsandbytes\", # comment in to quantize\n}\n```\n\nRunning on `ml.g5.48xlarge`\
    \ with `number_of_gpu = 8` above.\n\nAny ideas what could be wrong in my setup?"
  created_at: 2023-07-20 20:11:42+00:00
  edited: false
  hidden: false
  id: 64b9a30eb7d190d775ced97e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b249e9ee9b50554abf9f89c2e92126c0.svg
      fullname: Isaac Hernandez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: axiom-of-choice
      type: user
    createdAt: '2023-07-27T01:02:00.000Z'
    data:
      edited: true
      editors:
      - axiom-of-choice
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9334532618522644
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b249e9ee9b50554abf9f89c2e92126c0.svg
          fullname: Isaac Hernandez
          isHf: false
          isPro: false
          name: axiom-of-choice
          type: user
        html: '<p>Still having the issue...</p>

          '
        raw: Still having the issue...
        updatedAt: '2023-07-27T01:02:08.772Z'
      numEdits: 1
      reactions: []
    id: 64c1c2083754435cac95421c
    type: comment
  author: axiom-of-choice
  content: Still having the issue...
  created_at: 2023-07-27 00:02:00+00:00
  edited: true
  hidden: false
  id: 64c1c2083754435cac95421c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cad37c8eb7cf73d5b72a9031585813b0.svg
      fullname: Sergey Mkrtchyan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mkserge
      type: user
    createdAt: '2023-07-27T01:28:30.000Z'
    data:
      edited: true
      editors:
      - mkserge
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.933408260345459
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cad37c8eb7cf73d5b72a9031585813b0.svg
          fullname: Sergey Mkrtchyan
          isHf: false
          isPro: false
          name: mkserge
          type: user
        html: '<blockquote>

          <p>Still having the issue...</p>

          </blockquote>

          <p>It turns out sharding is not supported for 7B variants. You need to make
          sure to either choose an instance that has a single GPU, or explicitly choose
          <code>number_of_gpu = 1</code> in your config.</p>

          '
        raw: '> Still having the issue...


          It turns out sharding is not supported for 7B variants. You need to make
          sure to either choose an instance that has a single GPU, or explicitly choose
          `number_of_gpu = 1` in your config.

          '
        updatedAt: '2023-07-27T01:28:38.159Z'
      numEdits: 1
      reactions: []
    id: 64c1c83eb005aab93d5d3ef7
    type: comment
  author: mkserge
  content: '> Still having the issue...


    It turns out sharding is not supported for 7B variants. You need to make sure
    to either choose an instance that has a single GPU, or explicitly choose `number_of_gpu
    = 1` in your config.

    '
  created_at: 2023-07-27 00:28:30+00:00
  edited: true
  hidden: false
  id: 64c1c83eb005aab93d5d3ef7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/13399747961057fc86f4115f5f37f9f5.svg
      fullname: Valentin Lopez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: valenlopez3
      type: user
    createdAt: '2023-10-10T12:16:25.000Z'
    data:
      edited: false
      editors:
      - valenlopez3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7638996243476868
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/13399747961057fc86f4115f5f37f9f5.svg
          fullname: Valentin Lopez
          isHf: false
          isPro: false
          name: valenlopez3
          type: user
        html: '<p>Hello, I re-opened this discussion because I found the issue reported
          again after the last commit.</p>

          <p>Here is my configuration:<br>{<br>       "HF_MODEL_ID": "tiiuae/falcon-40b-instruct",<br>       "SM_NUM_GPUS":
          "4",<br>       "HF_MODEL_QUANTIZE": "bitsandbytes",<br>       "MAX_INPUT_LENGTH":
          "1024",<br>       "MAX_TOTAL_TOKENS": "2048"<br>}</p>

          <p>With this configuration and using an instance <code>ml.g5.12xlarge</code>
          I get the error message: ValueError: sharded is not supported for AutoModel</p>

          <p>Adding "HF_MODEL_REVISION": <a href="/tiiuae/falcon-40b-instruct/commit/ca78eac0ed45bf64445ff0687fabba1598daebf3">ca78eac0ed45bf64445ff0687fabba1598daebf3</a>
          to deploy the previous commit works perfectly fine.</p>

          <p>The issue is again in the last commit uploaded: <a href="/tiiuae/falcon-40b-instruct/commit/ecb78d97ac356d098e79f0db222c9ce7c5d9ee5f">ecb78d97ac356d098e79f0db222c9ce7c5d9ee5f</a></p>

          '
        raw: "Hello, I re-opened this discussion because I found the issue reported\
          \ again after the last commit.\n\nHere is my configuration:\n{\n       \"\
          HF_MODEL_ID\": \"tiiuae/falcon-40b-instruct\",\n       \"SM_NUM_GPUS\":\
          \ \"4\",\n       \"HF_MODEL_QUANTIZE\": \"bitsandbytes\",\n       \"MAX_INPUT_LENGTH\"\
          : \"1024\",\n       \"MAX_TOTAL_TOKENS\": \"2048\"\n}\n\nWith this configuration\
          \ and using an instance `ml.g5.12xlarge` I get the error message: ValueError:\
          \ sharded is not supported for AutoModel\n\nAdding \"HF_MODEL_REVISION\"\
          : ca78eac0ed45bf64445ff0687fabba1598daebf3 to deploy the previous commit\
          \ works perfectly fine.\n\nThe issue is again in the last commit uploaded:\
          \ ecb78d97ac356d098e79f0db222c9ce7c5d9ee5f"
        updatedAt: '2023-10-10T12:16:25.010Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - bikalnetomi
      - count: 1
        reaction: "\U0001F614"
        users:
        - bikalnetomi
    id: 65254099a7cd6e9a8dc9fc93
    type: comment
  author: valenlopez3
  content: "Hello, I re-opened this discussion because I found the issue reported\
    \ again after the last commit.\n\nHere is my configuration:\n{\n       \"HF_MODEL_ID\"\
    : \"tiiuae/falcon-40b-instruct\",\n       \"SM_NUM_GPUS\": \"4\",\n       \"HF_MODEL_QUANTIZE\"\
    : \"bitsandbytes\",\n       \"MAX_INPUT_LENGTH\": \"1024\",\n       \"MAX_TOTAL_TOKENS\"\
    : \"2048\"\n}\n\nWith this configuration and using an instance `ml.g5.12xlarge`\
    \ I get the error message: ValueError: sharded is not supported for AutoModel\n\
    \nAdding \"HF_MODEL_REVISION\": ca78eac0ed45bf64445ff0687fabba1598daebf3 to deploy\
    \ the previous commit works perfectly fine.\n\nThe issue is again in the last\
    \ commit uploaded: ecb78d97ac356d098e79f0db222c9ce7c5d9ee5f"
  created_at: 2023-10-10 11:16:25+00:00
  edited: false
  hidden: false
  id: 65254099a7cd6e9a8dc9fc93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/214be746a5f6c72bed1faa584496c821.svg
      fullname: Bikal Basnet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bikalnetomi
      type: user
    createdAt: '2023-10-12T15:24:54.000Z'
    data:
      edited: false
      editors:
      - bikalnetomi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9797680377960205
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/214be746a5f6c72bed1faa584496c821.svg
          fullname: Bikal Basnet
          isHf: false
          isPro: false
          name: bikalnetomi
          type: user
        html: '<p>I ran into the same issue today.<br>Changed the revision and it
          works fine as mentioned by valenlopez3  in the above thread</p>

          '
        raw: "I ran into the same issue today. \nChanged the revision and it works\
          \ fine as mentioned by valenlopez3  in the above thread"
        updatedAt: '2023-10-12T15:24:54.350Z'
      numEdits: 0
      reactions: []
    id: 65280fc64126af677af3ecae
    type: comment
  author: bikalnetomi
  content: "I ran into the same issue today. \nChanged the revision and it works fine\
    \ as mentioned by valenlopez3  in the above thread"
  created_at: 2023-10-12 14:24:54+00:00
  edited: false
  hidden: false
  id: 65280fc64126af677af3ecae
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 68
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: open
target_branch: null
title: 'ValueError: sharded is not supported for AutoModel ERROR'
