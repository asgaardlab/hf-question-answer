!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zkdtckk
conflicting_files: null
created_at: 2023-05-31 21:18:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a748c1c117ac45df99f8ba950131bb67.svg
      fullname: Kai Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkdtckk
      type: user
    createdAt: '2023-05-31T22:18:45.000Z'
    data:
      edited: true
      editors:
      - zkdtckk
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a748c1c117ac45df99f8ba950131bb67.svg
          fullname: Kai Zhang
          isHf: false
          isPro: false
          name: zkdtckk
          type: user
        html: '<p>It takes me 7-8 min to do one inference, why?<br>Hardware:<br>AWS
          sagemaker ml.p3dn.24xlarge<br>8 Nvidia Tesla V100 GPU, 24GB*8</p>

          <p>Environment<br>Torch: 2.0.1+cu117<br>Accelerate: 0.19.0<br>transformers:
          4.29.1</p>

          <p>Use following lines to load the model and do inference:</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>from
          instruct_pipeline import InstructionTextGenerationPipeline<br>base_model
          = ''tiiuae/falcon-40b-instruct'' # Does not work with load_8bit, inference
          taks 8 min...<br>load_8bit = False<br>tokenizer = AutoTokenizer.from_pretrained(base_model,
          padding_side="left")<br>model_llm = AutoModelForCausalLM.from_pretrained(<br>    base_model,
          load_in_8bit=load_8bit, device_map="auto", torch_dtype=torch.bfloat16,<br>    cache_dir="/home/ec2-user/SageMaker/model_cache/",trust_remote_code=True)</p>

          <p>model_llm.eval()<br>pipe = InstructionTextGenerationPipeline(model=model_llm,
          tokenizer=tokenizer)<br>pipe(''your prompt'')</p>

          '
        raw: "It takes me 7-8 min to do one inference, why?\nHardware:\nAWS sagemaker\
          \ ml.p3dn.24xlarge\n8 Nvidia Tesla V100 GPU, 24GB*8\n\nEnvironment\nTorch:\
          \ 2.0.1+cu117\nAccelerate: 0.19.0\ntransformers: 4.29.1\n\nUse following\
          \ lines to load the model and do inference:\n\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\nfrom instruct_pipeline import InstructionTextGenerationPipeline\n\
          base_model = 'tiiuae/falcon-40b-instruct' # Does not work with load_8bit,\
          \ inference taks 8 min... \nload_8bit = False\ntokenizer = AutoTokenizer.from_pretrained(base_model,\
          \ padding_side=\"left\")\nmodel_llm = AutoModelForCausalLM.from_pretrained(\n\
          \    base_model, load_in_8bit=load_8bit, device_map=\"auto\", torch_dtype=torch.bfloat16,\n\
          \    cache_dir=\"/home/ec2-user/SageMaker/model_cache/\",trust_remote_code=True)\n\
          \nmodel_llm.eval()\npipe = InstructionTextGenerationPipeline(model=model_llm,\
          \ tokenizer=tokenizer)\npipe('your prompt')"
        updatedAt: '2023-06-01T17:05:44.040Z'
      numEdits: 1
      reactions: []
    id: 6477c7c58ab7e732b6d929a9
    type: comment
  author: zkdtckk
  content: "It takes me 7-8 min to do one inference, why?\nHardware:\nAWS sagemaker\
    \ ml.p3dn.24xlarge\n8 Nvidia Tesla V100 GPU, 24GB*8\n\nEnvironment\nTorch: 2.0.1+cu117\n\
    Accelerate: 0.19.0\ntransformers: 4.29.1\n\nUse following lines to load the model\
    \ and do inference:\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
    from instruct_pipeline import InstructionTextGenerationPipeline\nbase_model =\
    \ 'tiiuae/falcon-40b-instruct' # Does not work with load_8bit, inference taks\
    \ 8 min... \nload_8bit = False\ntokenizer = AutoTokenizer.from_pretrained(base_model,\
    \ padding_side=\"left\")\nmodel_llm = AutoModelForCausalLM.from_pretrained(\n\
    \    base_model, load_in_8bit=load_8bit, device_map=\"auto\", torch_dtype=torch.bfloat16,\n\
    \    cache_dir=\"/home/ec2-user/SageMaker/model_cache/\",trust_remote_code=True)\n\
    \nmodel_llm.eval()\npipe = InstructionTextGenerationPipeline(model=model_llm,\
    \ tokenizer=tokenizer)\npipe('your prompt')"
  created_at: 2023-05-31 21:18:45+00:00
  edited: true
  hidden: false
  id: 6477c7c58ab7e732b6d929a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-01T06:49:16.000Z'
    data:
      edited: true
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>p3dn is V100, which does not natively support bfloat16. I''m not
          sure exactly what the fallback is in this case and why it does not throw
          any errors, maybe it''s running on fp32 behind the scenes?</p>

          <p>At this time I would recommend switching to <code>ml.g5.24xlarge</code>
          or <code>ml.g5.48xlarge</code> while we look into how to best support older
          hardware.</p>

          '
        raw: 'p3dn is V100, which does not natively support bfloat16. I''m not sure
          exactly what the fallback is in this case and why it does not throw any
          errors, maybe it''s running on fp32 behind the scenes?


          At this time I would recommend switching to `ml.g5.24xlarge` or `ml.g5.48xlarge`
          while we look into how to best support older hardware.'
        updatedAt: '2023-06-01T07:04:46.022Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - yuchsiao
    id: 64783f6c9c1f42c1f4d6eb13
    type: comment
  author: FalconLLM
  content: 'p3dn is V100, which does not natively support bfloat16. I''m not sure
    exactly what the fallback is in this case and why it does not throw any errors,
    maybe it''s running on fp32 behind the scenes?


    At this time I would recommend switching to `ml.g5.24xlarge` or `ml.g5.48xlarge`
    while we look into how to best support older hardware.'
  created_at: 2023-06-01 05:49:16+00:00
  edited: true
  hidden: false
  id: 64783f6c9c1f42c1f4d6eb13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a748c1c117ac45df99f8ba950131bb67.svg
      fullname: Kai Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkdtckk
      type: user
    createdAt: '2023-06-01T17:17:13.000Z'
    data:
      edited: false
      editors:
      - zkdtckk
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a748c1c117ac45df99f8ba950131bb67.svg
          fullname: Kai Zhang
          isHf: false
          isPro: false
          name: zkdtckk
          type: user
        html: '<p>Thank you for the correction and advice, never realized that V100
          did not support bfloat16. A10 has 125TFLOPS, while V100 fp32 has 7.8TFLOPS,
          this explains the huge difference in performance.</p>

          '
        raw: Thank you for the correction and advice, never realized that V100 did
          not support bfloat16. A10 has 125TFLOPS, while V100 fp32 has 7.8TFLOPS,
          this explains the huge difference in performance.
        updatedAt: '2023-06-01T17:17:13.312Z'
      numEdits: 0
      reactions: []
    id: 6478d29925e06d2ffe89cb6c
    type: comment
  author: zkdtckk
  content: Thank you for the correction and advice, never realized that V100 did not
    support bfloat16. A10 has 125TFLOPS, while V100 fp32 has 7.8TFLOPS, this explains
    the huge difference in performance.
  created_at: 2023-06-01 16:17:13+00:00
  edited: false
  hidden: false
  id: 6478d29925e06d2ffe89cb6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a748c1c117ac45df99f8ba950131bb67.svg
      fullname: Kai Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkdtckk
      type: user
    createdAt: '2023-06-01T20:12:43.000Z'
    data:
      edited: false
      editors:
      - zkdtckk
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a748c1c117ac45df99f8ba950131bb67.svg
          fullname: Kai Zhang
          isHf: false
          isPro: false
          name: zkdtckk
          type: user
        html: '<p>The time of inference is reduced from 8min to 1min on ml.g5.12xlarge
          with 4 A10 GPU. Should be even faster if using 48xlarge or A100 instance.
          Thanks again for your help. </p>

          '
        raw: 'The time of inference is reduced from 8min to 1min on ml.g5.12xlarge
          with 4 A10 GPU. Should be even faster if using 48xlarge or A100 instance.
          Thanks again for your help. '
        updatedAt: '2023-06-01T20:12:43.922Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - FalconLLM
      relatedEventId: 6478fbbbdbf97e0b5cc7ad37
    id: 6478fbbbdbf97e0b5cc7ad36
    type: comment
  author: zkdtckk
  content: 'The time of inference is reduced from 8min to 1min on ml.g5.12xlarge with
    4 A10 GPU. Should be even faster if using 48xlarge or A100 instance. Thanks again
    for your help. '
  created_at: 2023-06-01 19:12:43+00:00
  edited: false
  hidden: false
  id: 6478fbbbdbf97e0b5cc7ad36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a748c1c117ac45df99f8ba950131bb67.svg
      fullname: Kai Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkdtckk
      type: user
    createdAt: '2023-06-01T20:12:43.000Z'
    data:
      status: closed
    id: 6478fbbbdbf97e0b5cc7ad37
    type: status-change
  author: zkdtckk
  created_at: 2023-06-01 19:12:43+00:00
  id: 6478fbbbdbf97e0b5cc7ad37
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/a748c1c117ac45df99f8ba950131bb67.svg
      fullname: Kai Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkdtckk
      type: user
    createdAt: '2023-06-03T15:05:38.000Z'
    data:
      from: Inference takes 8 min with 8 Nvidia A100 GPU
      to: Inference takes 8 min with 8 Nvidia V100 GPU
    id: 647b56c26dbad6ab057b82b1
    type: title-change
  author: zkdtckk
  created_at: 2023-06-03 14:05:38+00:00
  id: 647b56c26dbad6ab057b82b1
  new_title: Inference takes 8 min with 8 Nvidia V100 GPU
  old_title: Inference takes 8 min with 8 Nvidia A100 GPU
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: closed
target_branch: null
title: Inference takes 8 min with 8 Nvidia V100 GPU
