!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bshongwe
conflicting_files: null
created_at: 2023-09-07 13:44:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/26dcc290033f770ca053659d67c64614.svg
      fullname: Bongani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bshongwe
      type: user
    createdAt: '2023-09-07T14:44:37.000Z'
    data:
      edited: false
      editors:
      - bshongwe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.767497718334198
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/26dcc290033f770ca053659d67c64614.svg
          fullname: Bongani
          isHf: false
          isPro: false
          name: bshongwe
          type: user
        html: '<p>I am trying to get Falcon 40B inference to run on a GKE using the
          text-generation-inference docker image.<br>However the model encounters
          the following error when loading </p>

          <pre><code>Error when initializing model

          ...

          RuntimeError: CUDA error: the provided PTX was compiled with an unsupported
          toolchain.

          CUDA kernel errors might be asynchronously reported at some other API call,
          so the stacktrace below might be incorrect.

          For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

          </code></pre>

          <p>the run command is</p>

          <pre><code>text-generation-launcher --model-id tiiuae/falcon-40b-instruct
          --quantize bitsandbytes-nf4 --num-shard 1 --huggingface-hub-cache", "/usr/src/falcon-40b-instruct
          --weights-cache-override /usr/src/falcon-40b-instruct

          </code></pre>

          <p>I am using A100 40GB, as I struggle to get A100 80GB.</p>

          <p>Is there anything particular I need to change to get it working?</p>

          '
        raw: "I am trying to get Falcon 40B inference to run on a GKE using the text-generation-inference\
          \ docker image.\r\nHowever the model encounters the following error when\
          \ loading \r\n```\r\nError when initializing model\r\n...\r\nRuntimeError:\
          \ CUDA error: the provided PTX was compiled with an unsupported toolchain.\r\
          \nCUDA kernel errors might be asynchronously reported at some other API\
          \ call, so the stacktrace below might be incorrect.\r\nFor debugging consider\
          \ passing CUDA_LAUNCH_BLOCKING=1.\r\n```\r\nthe run command is\r\n\r\n```\r\
          \ntext-generation-launcher --model-id tiiuae/falcon-40b-instruct --quantize\
          \ bitsandbytes-nf4 --num-shard 1 --huggingface-hub-cache\", \"/usr/src/falcon-40b-instruct\
          \ --weights-cache-override /usr/src/falcon-40b-instruct\r\n```\r\n\r\nI\
          \ am using A100 40GB, as I struggle to get A100 80GB.\r\n\r\nIs there anything\
          \ particular I need to change to get it working?\r\n"
        updatedAt: '2023-09-07T14:44:37.863Z'
      numEdits: 0
      reactions: []
    id: 64f9e1d55e217ed9d72f2286
    type: comment
  author: bshongwe
  content: "I am trying to get Falcon 40B inference to run on a GKE using the text-generation-inference\
    \ docker image.\r\nHowever the model encounters the following error when loading\
    \ \r\n```\r\nError when initializing model\r\n...\r\nRuntimeError: CUDA error:\
    \ the provided PTX was compiled with an unsupported toolchain.\r\nCUDA kernel\
    \ errors might be asynchronously reported at some other API call, so the stacktrace\
    \ below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\
    \n```\r\nthe run command is\r\n\r\n```\r\ntext-generation-launcher --model-id\
    \ tiiuae/falcon-40b-instruct --quantize bitsandbytes-nf4 --num-shard 1 --huggingface-hub-cache\"\
    , \"/usr/src/falcon-40b-instruct --weights-cache-override /usr/src/falcon-40b-instruct\r\
    \n```\r\n\r\nI am using A100 40GB, as I struggle to get A100 80GB.\r\n\r\nIs there\
    \ anything particular I need to change to get it working?\r\n"
  created_at: 2023-09-07 13:44:37+00:00
  edited: false
  hidden: false
  id: 64f9e1d55e217ed9d72f2286
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/88cb4f806c100a1ee7fef81d9e244206.svg
      fullname: Ryan Stanley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rstanley90
      type: user
    createdAt: '2023-10-27T15:21:23.000Z'
    data:
      edited: false
      editors:
      - rstanley90
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9943101406097412
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/88cb4f806c100a1ee7fef81d9e244206.svg
          fullname: Ryan Stanley
          isHf: false
          isPro: false
          name: rstanley90
          type: user
        html: '<p>Did you ever find a solution to this? I''m having the same issue
          on my A100 as well.</p>

          '
        raw: Did you ever find a solution to this? I'm having the same issue on my
          A100 as well.
        updatedAt: '2023-10-27T15:21:23.657Z'
      numEdits: 0
      reactions: []
    id: 653bd573cc16aca91805c8a5
    type: comment
  author: rstanley90
  content: Did you ever find a solution to this? I'm having the same issue on my A100
    as well.
  created_at: 2023-10-27 14:21:23+00:00
  edited: false
  hidden: false
  id: 653bd573cc16aca91805c8a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/26dcc290033f770ca053659d67c64614.svg
      fullname: Bongani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bshongwe
      type: user
    createdAt: '2023-10-30T11:03:41.000Z'
    data:
      edited: false
      editors:
      - bshongwe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9400687217712402
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/26dcc290033f770ca053659d67c64614.svg
          fullname: Bongani
          isHf: false
          isPro: false
          name: bshongwe
          type: user
        html: '<p>Hey. No I didn''t find a solution. Instead of running it on GKE,
          I switched to using dedicated GCP VMs to deploy the inference end point</p>

          '
        raw: Hey. No I didn't find a solution. Instead of running it on GKE, I switched
          to using dedicated GCP VMs to deploy the inference end point
        updatedAt: '2023-10-30T11:03:41.680Z'
      numEdits: 0
      reactions: []
    id: 653f8d8d676ce0e27ab9e469
    type: comment
  author: bshongwe
  content: Hey. No I didn't find a solution. Instead of running it on GKE, I switched
    to using dedicated GCP VMs to deploy the inference end point
  created_at: 2023-10-30 10:03:41+00:00
  edited: false
  hidden: false
  id: 653f8d8d676ce0e27ab9e469
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/88cb4f806c100a1ee7fef81d9e244206.svg
      fullname: Ryan Stanley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rstanley90
      type: user
    createdAt: '2023-10-30T14:44:14.000Z'
    data:
      edited: false
      editors:
      - rstanley90
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9241246581077576
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/88cb4f806c100a1ee7fef81d9e244206.svg
          fullname: Ryan Stanley
          isHf: false
          isPro: false
          name: rstanley90
          type: user
        html: '<p>I was able to find a solution that may be helpful for you. Try disabling
          the custom kernels via the environment variable <code>DISABLE_CUSTOM_KERNELS=true</code>.
          This has been suggested in <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference/issues/739#issuecomment-1669143730">other</a>
          posts on the github page for the HF text-generation-inference server with
          success. I''m not familiar with the inner workings of the system to know
          what exactly this is doing but the server appears to be running fine with
          this flag set. </p>

          '
        raw: 'I was able to find a solution that may be helpful for you. Try disabling
          the custom kernels via the environment variable `DISABLE_CUSTOM_KERNELS=true`.
          This has been suggested in [other](https://github.com/huggingface/text-generation-inference/issues/739#issuecomment-1669143730)
          posts on the github page for the HF text-generation-inference server with
          success. I''m not familiar with the inner workings of the system to know
          what exactly this is doing but the server appears to be running fine with
          this flag set. '
        updatedAt: '2023-10-30T14:44:14.210Z'
      numEdits: 0
      reactions: []
    id: 653fc13e68d8f1436b73b339
    type: comment
  author: rstanley90
  content: 'I was able to find a solution that may be helpful for you. Try disabling
    the custom kernels via the environment variable `DISABLE_CUSTOM_KERNELS=true`.
    This has been suggested in [other](https://github.com/huggingface/text-generation-inference/issues/739#issuecomment-1669143730)
    posts on the github page for the HF text-generation-inference server with success.
    I''m not familiar with the inner workings of the system to know what exactly this
    is doing but the server appears to be running fine with this flag set. '
  created_at: 2023-10-30 13:44:14+00:00
  edited: false
  hidden: false
  id: 653fc13e68d8f1436b73b339
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 82
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: open
target_branch: null
title: Falcon 40B Inference on GKE Autopilot A100 40GB
