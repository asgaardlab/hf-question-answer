!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NickyNicky
conflicting_files: null
created_at: 2023-06-05 00:16:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-06-05T01:16:49.000Z'
    data:
      edited: false
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7333968877792358
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: "<p><strong>Thanks for such a great model to be able to play with but\
          \ I have some doubts</strong></p>\n<p>according to Falcon's card it says\
          \ that it is used multiquery , FlashAttention and alibi.<br>I would like\
          \ to know how to implement it in the code since it will not be shown in\
          \ the examples, but rather basic ones</p>\n<p>Positionnal embeddings: rotary\
          \ (Su et al., 2021);<br>Attention: multiquery (Shazeer et al., 2019) and\
          \ FlashAttention (Dao et al., 2022);<br>Decoder-block: parallel attention/MLP\
          \ with a single layer norm.</p>\n<p>Example basic.</p>\n<pre><code>from\
          \ transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
          import torch\n\nmodel = \"tiiuae/falcon-40b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is\
          \ obsessed with giraffes, the most glorious animal on the face of this Earth.\
          \ Giraftron believes all other animals are irrelevant when compared to the\
          \ glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\
          ,\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
          \    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n  \
          \  print(f\"Result: {seq['generated_text']}\")\n</code></pre>\n<p>example\
          \ multiquery, FlashAttention and alibi.</p>\n<pre><code>????\n</code></pre>\n\
          <p>thanks.</p>\n"
        raw: "**Thanks for such a great model to be able to play with but I have some\
          \ doubts**\r\n\r\naccording to Falcon's card it says that it is used multiquery\
          \ , FlashAttention and alibi.\r\nI would like to know how to implement it\
          \ in the code since it will not be shown in the examples, but rather basic\
          \ ones\r\n\r\nPositionnal embeddings: rotary (Su et al., 2021);\r\nAttention:\
          \ multiquery (Shazeer et al., 2019) and FlashAttention (Dao et al., 2022);\r\
          \nDecoder-block: parallel attention/MLP with a single layer norm.\r\n\r\n\
          Example basic.\r\n\r\n```\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
          \nimport transformers\r\nimport torch\r\n\r\nmodel = \"tiiuae/falcon-40b-instruct\"\
          \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\npipeline = transformers.pipeline(\r\
          \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n   \
          \ device_map=\"auto\",\r\n)\r\nsequences = pipeline(\r\n   \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",\r\n    max_length=200,\r\n    do_sample=True,\r\n    top_k=10,\r\
          \n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
          \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
          )\r\n```\r\n\r\nexample multiquery, FlashAttention and alibi.\r\n```\r\n\
          ????\r\n```\r\n\r\nthanks."
        updatedAt: '2023-06-05T01:16:49.844Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - iceisfun
        - ravindra34
        - letianlee
    id: 647d3781e07cf9bb2d4a7d3b
    type: comment
  author: NickyNicky
  content: "**Thanks for such a great model to be able to play with but I have some\
    \ doubts**\r\n\r\naccording to Falcon's card it says that it is used multiquery\
    \ , FlashAttention and alibi.\r\nI would like to know how to implement it in the\
    \ code since it will not be shown in the examples, but rather basic ones\r\n\r\
    \nPositionnal embeddings: rotary (Su et al., 2021);\r\nAttention: multiquery (Shazeer\
    \ et al., 2019) and FlashAttention (Dao et al., 2022);\r\nDecoder-block: parallel\
    \ attention/MLP with a single layer norm.\r\n\r\nExample basic.\r\n\r\n```\r\n\
    from transformers import AutoTokenizer, AutoModelForCausalLM\r\nimport transformers\r\
    \nimport torch\r\n\r\nmodel = \"tiiuae/falcon-40b-instruct\"\r\n\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model)\r\npipeline = transformers.pipeline(\r\
    \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n\
    \    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n    device_map=\"\
    auto\",\r\n)\r\nsequences = pipeline(\r\n   \"Girafatron is obsessed with giraffes,\
    \ the most glorious animal on the face of this Earth. Giraftron believes all other\
    \ animals are irrelevant when compared to the glorious majesty of the giraffe.\\\
    nDaniel: Hello, Girafatron!\\nGirafatron:\",\r\n    max_length=200,\r\n    do_sample=True,\r\
    \n    top_k=10,\r\n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
    \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
    )\r\n```\r\n\r\nexample multiquery, FlashAttention and alibi.\r\n```\r\n????\r\
    \n```\r\n\r\nthanks."
  created_at: 2023-06-05 00:16:49+00:00
  edited: false
  hidden: false
  id: 647d3781e07cf9bb2d4a7d3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:57:27.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5419390797615051
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>All implementation details are in <a href="https://huggingface.co/tiiuae/falcon-40b-instruct/blob/main/modelling_RW.py">modelling_RW.py</a></p>

          '
        raw: All implementation details are in [modelling_RW.py](https://huggingface.co/tiiuae/falcon-40b-instruct/blob/main/modelling_RW.py)
        updatedAt: '2023-06-09T14:57:27.549Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64833dd712e9cb424131dfb7
    id: 64833dd712e9cb424131dfaf
    type: comment
  author: FalconLLM
  content: All implementation details are in [modelling_RW.py](https://huggingface.co/tiiuae/falcon-40b-instruct/blob/main/modelling_RW.py)
  created_at: 2023-06-09 13:57:27+00:00
  edited: false
  hidden: false
  id: 64833dd712e9cb424131dfaf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:57:27.000Z'
    data:
      status: closed
    id: 64833dd712e9cb424131dfb7
    type: status-change
  author: FalconLLM
  created_at: 2023-06-09 13:57:27+00:00
  id: 64833dd712e9cb424131dfb7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 29
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: closed
target_branch: null
title: how to implement multiquery, FlashAttention and alibi.
