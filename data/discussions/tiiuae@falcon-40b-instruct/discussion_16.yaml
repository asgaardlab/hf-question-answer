!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Shridharalve
conflicting_files: null
created_at: 2023-06-01 09:26:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
      fullname: Shridhar Alve
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shridharalve
      type: user
    createdAt: '2023-06-01T10:26:52.000Z'
    data:
      edited: false
      editors:
      - Shridharalve
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
          fullname: Shridhar Alve
          isHf: false
          isPro: false
          name: Shridharalve
          type: user
        html: '<p>Tried model inference using ml.g5.24xlarge SageMaker endpoint. Getting
          the below mentioned error</p>

          <p><code> An error occurred (ModelError) when calling the InvokeEndpoint
          operation: Received client error (400) from primary with message "{   "code":
          400,   "type": "InternalServerException",   "message": "[Errno 28] No space
          left on device" }</code></p>

          '
        raw: "Tried model inference using ml.g5.24xlarge SageMaker endpoint. Getting\
          \ the below mentioned error\r\n```\r\nAn error occurred (ModelError) when\
          \ calling the InvokeEndpoint operation: Received client error (400) from\
          \ primary with message \"{\r\n  \"code\": 400,\r\n  \"type\": \"InternalServerException\"\
          ,\r\n  \"message\": \"[Errno 28] No space left on device\"\r\n}```"
        updatedAt: '2023-06-01T10:26:52.966Z'
      numEdits: 0
      reactions: []
    id: 6478726c403cd7ae4b79fe18
    type: comment
  author: Shridharalve
  content: "Tried model inference using ml.g5.24xlarge SageMaker endpoint. Getting\
    \ the below mentioned error\r\n```\r\nAn error occurred (ModelError) when calling\
    \ the InvokeEndpoint operation: Received client error (400) from primary with\
    \ message \"{\r\n  \"code\": 400,\r\n  \"type\": \"InternalServerException\",\r\
    \n  \"message\": \"[Errno 28] No space left on device\"\r\n}```"
  created_at: 2023-06-01 09:26:52+00:00
  edited: false
  hidden: false
  id: 6478726c403cd7ae4b79fe18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-01T11:02:34.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>Looks to be an issue with too little memory on the instance. What
          size EBS volume did you attach?</p>

          '
        raw: Looks to be an issue with too little memory on the instance. What size
          EBS volume did you attach?
        updatedAt: '2023-06-01T11:02:34.845Z'
      numEdits: 0
      reactions: []
    id: 64787aca1f9756aa89cf80e1
    type: comment
  author: FalconLLM
  content: Looks to be an issue with too little memory on the instance. What size
    EBS volume did you attach?
  created_at: 2023-06-01 10:02:34+00:00
  edited: false
  hidden: false
  id: 64787aca1f9756aa89cf80e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
      fullname: Shridhar Alve
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shridharalve
      type: user
    createdAt: '2023-06-01T11:15:10.000Z'
    data:
      edited: false
      editors:
      - Shridharalve
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
          fullname: Shridhar Alve
          isHf: false
          isPro: false
          name: Shridharalve
          type: user
        html: "<p>As per the AWS Docs, the EBS volume size is 3800 GB. </p>\n<pre><code>Type\
          \            | CPU    |     Memory        | GPUS | GPU Memory  | Storage\n\
          ml.g5.24xlarge\t|   96   |     384  GB\t     |  4\t |    96 GB   |1x3800\n\
          </code></pre>\n<p>Here I also tried with a larger instance type ml.g5.48xlarge\
          \ which has double the specs</p>\n"
        raw: "As per the AWS Docs, the EBS volume size is 3800 GB. \n```\nType   \
          \         | CPU    |     Memory        | GPUS | GPU Memory  | Storage\n\
          ml.g5.24xlarge\t|   96   |     384  GB\t     |  4\t |    96 GB   |1x3800\n\
          ```\nHere I also tried with a larger instance type ml.g5.48xlarge which\
          \ has double the specs"
        updatedAt: '2023-06-01T11:15:10.606Z'
      numEdits: 0
      reactions: []
    id: 64787dbe1f9756aa89cfc08b
    type: comment
  author: Shridharalve
  content: "As per the AWS Docs, the EBS volume size is 3800 GB. \n```\nType     \
    \       | CPU    |     Memory        | GPUS | GPU Memory  | Storage\nml.g5.24xlarge\t\
    |   96   |     384  GB\t     |  4\t |    96 GB   |1x3800\n```\nHere I also tried\
    \ with a larger instance type ml.g5.48xlarge which has double the specs"
  created_at: 2023-06-01 10:15:10+00:00
  edited: false
  hidden: false
  id: 64787dbe1f9756aa89cfc08b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
      fullname: Shridhar Alve
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shridharalve
      type: user
    createdAt: '2023-06-01T11:22:42.000Z'
    data:
      edited: false
      editors:
      - Shridharalve
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
          fullname: Shridhar Alve
          isHf: false
          isPro: false
          name: Shridharalve
          type: user
        html: '<p>Adding more details about the error on sagemaker endpoint.</p>

          <p>Caused by: java.io.IOException: No space left on device</p>

          <pre><code>pool-2-thread-6 ERROR An exception occurred processing Appender
          access_log org.apache.logging.log4j.core.appender.AppenderLoggingException:
          Error writing to stream logs/access_log.log

          2023-06-01 10:33:43,006 pool-2-thread-6 ERROR An exception occurred processing
          Appender access_log org.apache.logging.log4j.core.appender.AppenderLoggingException:
          Error writing to stream logs/access_log.log

          </code></pre>

          '
        raw: 'Adding more details about the error on sagemaker endpoint.


          Caused by: java.io.IOException: No space left on device

          ```

          pool-2-thread-6 ERROR An exception occurred processing Appender access_log
          org.apache.logging.log4j.core.appender.AppenderLoggingException: Error writing
          to stream logs/access_log.log

          2023-06-01 10:33:43,006 pool-2-thread-6 ERROR An exception occurred processing
          Appender access_log org.apache.logging.log4j.core.appender.AppenderLoggingException:
          Error writing to stream logs/access_log.log

          ```'
        updatedAt: '2023-06-01T11:22:42.232Z'
      numEdits: 0
      reactions: []
    id: 64787f82159a889d00230bf9
    type: comment
  author: Shridharalve
  content: 'Adding more details about the error on sagemaker endpoint.


    Caused by: java.io.IOException: No space left on device

    ```

    pool-2-thread-6 ERROR An exception occurred processing Appender access_log org.apache.logging.log4j.core.appender.AppenderLoggingException:
    Error writing to stream logs/access_log.log

    2023-06-01 10:33:43,006 pool-2-thread-6 ERROR An exception occurred processing
    Appender access_log org.apache.logging.log4j.core.appender.AppenderLoggingException:
    Error writing to stream logs/access_log.log

    ```'
  created_at: 2023-06-01 10:22:42+00:00
  edited: false
  hidden: false
  id: 64787f82159a889d00230bf9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-01T12:11:43.000Z'
    data:
      edited: true
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>The volume you mentioned is typically mounted to <code>/tmp/</code>
          while there is a separate volume mounted to <code>/opt/ml/checkpoints</code>
          which you specify when launching the instance. I believe what is happening
          is that the model is downloaded under <code>/opt/ml/checkpoints</code> which
          then get''s exhausted.  Assuming you use the HF estimator, could you try
          specifying <code>volume_size = 200</code>?</p>

          '
        raw: The volume you mentioned is typically mounted to `/tmp/` while there
          is a separate volume mounted to `/opt/ml/checkpoints` which you specify
          when launching the instance. I believe what is happening is that the model
          is downloaded under `/opt/ml/checkpoints` which then get's exhausted.  Assuming
          you use the HF estimator, could you try specifying `volume_size = 200`?
        updatedAt: '2023-06-01T12:14:13.976Z'
      numEdits: 2
      reactions: []
    id: 64788aff1f9756aa89d0d97c
    type: comment
  author: FalconLLM
  content: The volume you mentioned is typically mounted to `/tmp/` while there is
    a separate volume mounted to `/opt/ml/checkpoints` which you specify when launching
    the instance. I believe what is happening is that the model is downloaded under
    `/opt/ml/checkpoints` which then get's exhausted.  Assuming you use the HF estimator,
    could you try specifying `volume_size = 200`?
  created_at: 2023-06-01 11:11:43+00:00
  edited: true
  hidden: false
  id: 64788aff1f9756aa89d0d97c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
      fullname: Shridhar Alve
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shridharalve
      type: user
    createdAt: '2023-06-01T13:07:24.000Z'
    data:
      edited: false
      editors:
      - Shridharalve
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
          fullname: Shridhar Alve
          isHf: false
          isPro: false
          name: Shridharalve
          type: user
        html: '<p>These instance types ml.g5.24xlarge and ml.g5.48xlarge do not support
          the volume_size parameter as they have a 3800 GB volume with the inference
          endpoint.  If instance type is an issue can you suggest an appropriate one
          which can run the model without issues. I was able to run falcon-7B-instruct
          without any issues...</p>

          '
        raw: These instance types ml.g5.24xlarge and ml.g5.48xlarge do not support
          the volume_size parameter as they have a 3800 GB volume with the inference
          endpoint.  If instance type is an issue can you suggest an appropriate one
          which can run the model without issues. I was able to run falcon-7B-instruct
          without any issues...
        updatedAt: '2023-06-01T13:07:24.421Z'
      numEdits: 0
      reactions: []
    id: 6478980c159a889d002550e0
    type: comment
  author: Shridharalve
  content: These instance types ml.g5.24xlarge and ml.g5.48xlarge do not support the
    volume_size parameter as they have a 3800 GB volume with the inference endpoint.  If
    instance type is an issue can you suggest an appropriate one which can run the
    model without issues. I was able to run falcon-7B-instruct without any issues...
  created_at: 2023-06-01 12:07:24+00:00
  edited: false
  hidden: false
  id: 6478980c159a889d002550e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-01T16:23:49.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>At this point I unfortunately do not understand sagemaker endpoints
          with huggingface models well enough to be able to assist you, the issue
          is definitely related to the disk space though, as the error indicates<code>[Errno
          28] No space left on device"</code>. The 7B might work because it fits in
          the standard 30GB EBS volume. Over the coming weeks we hope to be able to
          provide easier ways to deploy the models.</p>

          '
        raw: At this point I unfortunately do not understand sagemaker endpoints with
          huggingface models well enough to be able to assist you, the issue is definitely
          related to the disk space though, as the error indicates`[Errno 28] No space
          left on device"`. The 7B might work because it fits in the standard 30GB
          EBS volume. Over the coming weeks we hope to be able to provide easier ways
          to deploy the models.
        updatedAt: '2023-06-01T16:23:49.978Z'
      numEdits: 0
      reactions: []
    id: 6478c61542b1805ae2ab5866
    type: comment
  author: FalconLLM
  content: At this point I unfortunately do not understand sagemaker endpoints with
    huggingface models well enough to be able to assist you, the issue is definitely
    related to the disk space though, as the error indicates`[Errno 28] No space left
    on device"`. The 7B might work because it fits in the standard 30GB EBS volume.
    Over the coming weeks we hope to be able to provide easier ways to deploy the
    models.
  created_at: 2023-06-01 15:23:49+00:00
  edited: false
  hidden: false
  id: 6478c61542b1805ae2ab5866
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bc44e454a77181b60f3c05a7fb5bc9.svg
      fullname: Mario
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mariolr
      type: user
    createdAt: '2023-06-01T21:54:47.000Z'
    data:
      edited: false
      editors:
      - mariolr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bc44e454a77181b60f3c05a7fb5bc9.svg
          fullname: Mario
          isHf: false
          isPro: false
          name: mariolr
          type: user
        html: '<p>The transformers library downloads the model on the default cache
          location: ~/.cache/huggingface/hub<br>However, the EBS volume is mounted
          on /home/ec2-user/SageMaker<br>You can check by running <code>df</code>
          on a terminal.</p>

          <p>You can change the transformers cache location to a directory by running
          this before importing the transformers library:</p>

          <pre><code>import os

          os.environ[''TRANSFORMERS_CACHE''] = ''/home/ec2-user/SageMaker/transformers-cache/''

          </code></pre>

          <p>Here''s the relevant documentation: <a href="https://huggingface.co/docs/transformers/v4.29.1/en/installation#cache-setup">https://huggingface.co/docs/transformers/v4.29.1/en/installation#cache-setup</a></p>

          '
        raw: 'The transformers library downloads the model on the default cache location:
          ~/.cache/huggingface/hub

          However, the EBS volume is mounted on /home/ec2-user/SageMaker

          You can check by running `df` on a terminal.


          You can change the transformers cache location to a directory by running
          this before importing the transformers library:


          ```

          import os

          os.environ[''TRANSFORMERS_CACHE''] = ''/home/ec2-user/SageMaker/transformers-cache/''

          ```


          Here''s the relevant documentation: https://huggingface.co/docs/transformers/v4.29.1/en/installation#cache-setup'
        updatedAt: '2023-06-01T21:54:47.422Z'
      numEdits: 0
      reactions: []
    id: 647913a7dbf97e0b5cc95c1e
    type: comment
  author: mariolr
  content: 'The transformers library downloads the model on the default cache location:
    ~/.cache/huggingface/hub

    However, the EBS volume is mounted on /home/ec2-user/SageMaker

    You can check by running `df` on a terminal.


    You can change the transformers cache location to a directory by running this
    before importing the transformers library:


    ```

    import os

    os.environ[''TRANSFORMERS_CACHE''] = ''/home/ec2-user/SageMaker/transformers-cache/''

    ```


    Here''s the relevant documentation: https://huggingface.co/docs/transformers/v4.29.1/en/installation#cache-setup'
  created_at: 2023-06-01 20:54:47+00:00
  edited: false
  hidden: false
  id: 647913a7dbf97e0b5cc95c1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
      fullname: Shridhar Alve
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shridharalve
      type: user
    createdAt: '2023-06-02T07:02:37.000Z'
    data:
      edited: true
      editors:
      - Shridharalve
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
          fullname: Shridhar Alve
          isHf: false
          isPro: false
          name: Shridharalve
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;FalconLLM&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/FalconLLM\">@<span class=\"\
          underline\">FalconLLM</span></a></span>\n\n\t</span></span> </p>\n<p>I am\
          \ no longer facing the \"storage space\" issue. Seems it got resolved using\
          \ the below snipped in inference.py</p>\n<pre><code>model = AutoModelForCausalLM.from_pretrained(\"\
          tiiuae/falcon-40b-instruct\", trust_remote_code=True,load_in_8bit=False,\
          \ torch_dtype=torch.bfloat16, device_map=\"auto\", cache_dir=\"/tmp/model_cache/\"\
          )\n</code></pre>\n<p>The model got deployed on the SG endpoint. However\
          \ when I look at the instance metrics, the GPU Memory usage balloons to\
          \ 788% and all the 8 GPUs are utilized on ml.g5.48xlarge (8 NVIDIA A10G\
          \ GPUs and 192 GiB GPU memory) . We do not get an inference output as often\
          \ it times out if there's no response for a minute. Is this machine enough\
          \ for the model inference hosting. Should we wait longer for response.<br>Any\
          \ other ml instance type I can try?</p>\n"
        raw: "@FalconLLM \n\nI am no longer facing the \"storage space\" issue. Seems\
          \ it got resolved using the below snipped in inference.py\n```\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-40b-instruct\", trust_remote_code=True,load_in_8bit=False,\
          \ torch_dtype=torch.bfloat16, device_map=\"auto\", cache_dir=\"/tmp/model_cache/\"\
          )\n```\n\nThe model got deployed on the SG endpoint. However when I look\
          \ at the instance metrics, the GPU Memory usage balloons to 788% and all\
          \ the 8 GPUs are utilized on ml.g5.48xlarge (8 NVIDIA A10G GPUs and 192\
          \ GiB GPU memory) . We do not get an inference output as often it times\
          \ out if there's no response for a minute. Is this machine enough for the\
          \ model inference hosting. Should we wait longer for response. \nAny other\
          \ ml instance type I can try?"
        updatedAt: '2023-06-02T13:34:33.187Z'
      numEdits: 1
      reactions: []
    id: 6479940dd3c161f32e60fb87
    type: comment
  author: Shridharalve
  content: "@FalconLLM \n\nI am no longer facing the \"storage space\" issue. Seems\
    \ it got resolved using the below snipped in inference.py\n```\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    tiiuae/falcon-40b-instruct\", trust_remote_code=True,load_in_8bit=False, torch_dtype=torch.bfloat16,\
    \ device_map=\"auto\", cache_dir=\"/tmp/model_cache/\")\n```\n\nThe model got\
    \ deployed on the SG endpoint. However when I look at the instance metrics, the\
    \ GPU Memory usage balloons to 788% and all the 8 GPUs are utilized on ml.g5.48xlarge\
    \ (8 NVIDIA A10G GPUs and 192 GiB GPU memory) . We do not get an inference output\
    \ as often it times out if there's no response for a minute. Is this machine enough\
    \ for the model inference hosting. Should we wait longer for response. \nAny other\
    \ ml instance type I can try?"
  created_at: 2023-06-02 06:02:37+00:00
  edited: true
  hidden: false
  id: 6479940dd3c161f32e60fb87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/823db0f57cebc8fd19aac73635d44200.svg
      fullname: Austin Welch
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: austinmw
      type: user
    createdAt: '2023-06-08T23:50:06.000Z'
    data:
      edited: false
      editors:
      - austinmw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9094175100326538
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/823db0f57cebc8fd19aac73635d44200.svg
          fullname: Austin Welch
          isHf: false
          isPro: false
          name: austinmw
          type: user
        html: '<p>ml.g5.12xlarge instance was enough for me to deploy Falcon-40B in
          the HF TGI DLC</p>

          '
        raw: ml.g5.12xlarge instance was enough for me to deploy Falcon-40B in the
          HF TGI DLC
        updatedAt: '2023-06-08T23:50:06.918Z'
      numEdits: 0
      reactions: []
    id: 6482692e2bc908e2d9e13fbf
    type: comment
  author: austinmw
  content: ml.g5.12xlarge instance was enough for me to deploy Falcon-40B in the HF
    TGI DLC
  created_at: 2023-06-08 22:50:06+00:00
  edited: false
  hidden: false
  id: 6482692e2bc908e2d9e13fbf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:51:24.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.879569411277771
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>For running on SageMaker we would recommend having a look at this
          blogpost: <a rel="nofollow" href="https://www.philschmid.de/sagemaker-falcon-llm">https://www.philschmid.de/sagemaker-falcon-llm</a></p>

          '
        raw: 'For running on SageMaker we would recommend having a look at this blogpost:
          https://www.philschmid.de/sagemaker-falcon-llm'
        updatedAt: '2023-06-09T14:51:24.237Z'
      numEdits: 0
      reactions: []
    id: 64833c6c4dd86d1e18df8464
    type: comment
  author: FalconLLM
  content: 'For running on SageMaker we would recommend having a look at this blogpost:
    https://www.philschmid.de/sagemaker-falcon-llm'
  created_at: 2023-06-09 13:51:24+00:00
  edited: false
  hidden: false
  id: 64833c6c4dd86d1e18df8464
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
      fullname: Shridhar Alve
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shridharalve
      type: user
    createdAt: '2023-06-10T04:48:11.000Z'
    data:
      edited: false
      editors:
      - Shridharalve
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.989654004573822
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
          fullname: Shridhar Alve
          isHf: false
          isPro: false
          name: Shridharalve
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;FalconLLM&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/FalconLLM\">@<span class=\"\
          underline\">FalconLLM</span></a></span>\n\n\t</span></span> Yes I looked\
          \ at that and we have already deployed your model.. Thanks for the help.\
          \ I have posted this link for others as well in this community</p>\n"
        raw: '@FalconLLM Yes I looked at that and we have already deployed your model..
          Thanks for the help. I have posted this link for others as well in this
          community'
        updatedAt: '2023-06-10T04:48:11.565Z'
      numEdits: 0
      reactions: []
    id: 6484008b295256340e439c84
    type: comment
  author: Shridharalve
  content: '@FalconLLM Yes I looked at that and we have already deployed your model..
    Thanks for the help. I have posted this link for others as well in this community'
  created_at: 2023-06-10 03:48:11+00:00
  edited: false
  hidden: false
  id: 6484008b295256340e439c84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/399a4ea2c22aa2c731e34216e2106877.svg
      fullname: Shridhar Alve
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shridharalve
      type: user
    createdAt: '2023-06-10T09:44:49.000Z'
    data:
      status: closed
    id: 64844611be2d6cb877e39f2d
    type: status-change
  author: Shridharalve
  created_at: 2023-06-10 08:44:49+00:00
  id: 64844611be2d6cb877e39f2d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fbc7b12ae726d7437cd6da83573b4b20.svg
      fullname: Riccardo Rolando
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rikirolly
      type: user
    createdAt: '2023-06-27T21:06:44.000Z'
    data:
      edited: false
      editors:
      - rikirolly
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7236562371253967
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fbc7b12ae726d7437cd6da83573b4b20.svg
          fullname: Riccardo Rolando
          isHf: false
          isPro: false
          name: rikirolly
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;austinmw&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/austinmw\">@<span class=\"\
          underline\">austinmw</span></a></span>\n\n\t</span></span> Could you provide\
          \ a shell script for the deployment in the HF TGI DLC?</p>\n"
        raw: '@austinmw Could you provide a shell script for the deployment in the
          HF TGI DLC?'
        updatedAt: '2023-06-27T21:06:44.770Z'
      numEdits: 0
      reactions: []
    id: 649b4f64ecfe2086e5510dd8
    type: comment
  author: rikirolly
  content: '@austinmw Could you provide a shell script for the deployment in the HF
    TGI DLC?'
  created_at: 2023-06-27 20:06:44+00:00
  edited: false
  hidden: false
  id: 649b4f64ecfe2086e5510dd8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: closed
target_branch: null
title: SageMaker Endpoint error during inference
