!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andee96
conflicting_files: null
created_at: 2023-06-12 20:21:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fed6accea61d0e165f7e561e53e62f0.svg
      fullname: Andreas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andee96
      type: user
    createdAt: '2023-06-12T21:21:45.000Z'
    data:
      edited: false
      editors:
      - andee96
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.95754075050354
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fed6accea61d0e165f7e561e53e62f0.svg
          fullname: Andreas
          isHf: false
          isPro: false
          name: andee96
          type: user
        html: '<p>Hey everyone,<br>apologies if this is a silly question, I am a bit
          new to this. I''ve started playing around with falcon-40b-instruct and I
          have noticed that regardless of what prompt I give it, it always returns
          the entire prompt as well as the output. </p>

          <p>Example:<br>Prompt: "User: Hello, how are you?\n Assistant:"<br>Generated
          text: "User: Hello, how are you?\n Assistant: I''m fine, how can I help
          you?"</p>

          <p>This makes it pretty difficult to chain prompts together using langchain.
          Is this how the model is supposed to behave? If no then what do people think
          I''m doing wrong? If yes then what is the most appropriate way to handle
          this? </p>

          <p>I''ve deployed falcon-40b-instruct on sagemaker using the template provided
          by hugging face. </p>

          <p>Thank you in advance :) </p>

          '
        raw: "Hey everyone, \r\napologies if this is a silly question, I am a bit\
          \ new to this. I've started playing around with falcon-40b-instruct and\
          \ I have noticed that regardless of what prompt I give it, it always returns\
          \ the entire prompt as well as the output. \r\n\r\nExample: \r\nPrompt:\
          \ \"User: Hello, how are you?\\n Assistant:\"\r\nGenerated text: \"User:\
          \ Hello, how are you?\\n Assistant: I'm fine, how can I help you?\"\r\n\r\
          \nThis makes it pretty difficult to chain prompts together using langchain.\
          \ Is this how the model is supposed to behave? If no then what do people\
          \ think I'm doing wrong? If yes then what is the most appropriate way to\
          \ handle this? \r\n\r\nI've deployed falcon-40b-instruct on sagemaker using\
          \ the template provided by hugging face. \r\n\r\nThank you in advance :) "
        updatedAt: '2023-06-12T21:21:45.070Z'
      numEdits: 0
      reactions: []
    id: 64878c69cc0e54f272b42219
    type: comment
  author: andee96
  content: "Hey everyone, \r\napologies if this is a silly question, I am a bit new\
    \ to this. I've started playing around with falcon-40b-instruct and I have noticed\
    \ that regardless of what prompt I give it, it always returns the entire prompt\
    \ as well as the output. \r\n\r\nExample: \r\nPrompt: \"User: Hello, how are you?\\\
    n Assistant:\"\r\nGenerated text: \"User: Hello, how are you?\\n Assistant: I'm\
    \ fine, how can I help you?\"\r\n\r\nThis makes it pretty difficult to chain prompts\
    \ together using langchain. Is this how the model is supposed to behave? If no\
    \ then what do people think I'm doing wrong? If yes then what is the most appropriate\
    \ way to handle this? \r\n\r\nI've deployed falcon-40b-instruct on sagemaker using\
    \ the template provided by hugging face. \r\n\r\nThank you in advance :) "
  created_at: 2023-06-12 20:21:45+00:00
  edited: false
  hidden: false
  id: 64878c69cc0e54f272b42219
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca85bfde075954f5d4123337acfe3b68.svg
      fullname: bond
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yi1
      type: user
    createdAt: '2023-06-13T07:46:19.000Z'
    data:
      edited: false
      editors:
      - yi1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6135058999061584
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca85bfde075954f5d4123337acfe3b68.svg
          fullname: bond
          isHf: false
          isPro: false
          name: yi1
          type: user
        html: "<p>model.generate(<br>    text=[\"def fibonnaci(\", \"User: How are\
          \ you doing? Bot:\"],<br>    max_length=64,<br>    include_prompt_in_result=False<br>\uFF09\
          </p>\n"
        raw: "model.generate(\n    text=[\"def fibonnaci(\", \"User: How are you doing?\
          \ Bot:\"],\n    max_length=64,\n    include_prompt_in_result=False\n\uFF09"
        updatedAt: '2023-06-13T07:46:19.994Z'
      numEdits: 0
      reactions: []
    id: 64881ecb43dbd2a0e13301c0
    type: comment
  author: yi1
  content: "model.generate(\n    text=[\"def fibonnaci(\", \"User: How are you doing?\
    \ Bot:\"],\n    max_length=64,\n    include_prompt_in_result=False\n\uFF09"
  created_at: 2023-06-13 06:46:19+00:00
  edited: false
  hidden: false
  id: 64881ecb43dbd2a0e13301c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca85bfde075954f5d4123337acfe3b68.svg
      fullname: bond
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yi1
      type: user
    createdAt: '2023-06-13T07:46:41.000Z'
    data:
      edited: false
      editors:
      - yi1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.41114234924316406
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca85bfde075954f5d4123337acfe3b68.svg
          fullname: bond
          isHf: false
          isPro: false
          name: yi1
          type: user
        html: '<p>Add "include_prompt_in_result=False" in model.generate(</p>

          '
        raw: Add "include_prompt_in_result=False" in model.generate(
        updatedAt: '2023-06-13T07:46:41.721Z'
      numEdits: 0
      reactions: []
    id: 64881ee18e004bb92b0f9410
    type: comment
  author: yi1
  content: Add "include_prompt_in_result=False" in model.generate(
  created_at: 2023-06-13 06:46:41+00:00
  edited: false
  hidden: false
  id: 64881ee18e004bb92b0f9410
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fed6accea61d0e165f7e561e53e62f0.svg
      fullname: Andreas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andee96
      type: user
    createdAt: '2023-06-13T07:50:30.000Z'
    data:
      edited: false
      editors:
      - andee96
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9338654279708862
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fed6accea61d0e165f7e561e53e62f0.svg
          fullname: Andreas
          isHf: false
          isPro: false
          name: andee96
          type: user
        html: '<p>lol that does make me feel pretty silly, i will give that a try.<br>Do
          you know where I am supposed to pass this parameter in the case where i''ve
          deployed the model using aws sagemaker?</p>

          '
        raw: "lol that does make me feel pretty silly, i will give that a try. \n\
          Do you know where I am supposed to pass this parameter in the case where\
          \ i've deployed the model using aws sagemaker?"
        updatedAt: '2023-06-13T07:50:30.122Z'
      numEdits: 0
      reactions: []
    id: 64881fc68174cb391bcdd664
    type: comment
  author: andee96
  content: "lol that does make me feel pretty silly, i will give that a try. \nDo\
    \ you know where I am supposed to pass this parameter in the case where i've deployed\
    \ the model using aws sagemaker?"
  created_at: 2023-06-13 06:50:30+00:00
  edited: false
  hidden: false
  id: 64881fc68174cb391bcdd664
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fed6accea61d0e165f7e561e53e62f0.svg
      fullname: Andreas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andee96
      type: user
    createdAt: '2023-06-13T08:16:51.000Z'
    data:
      edited: false
      editors:
      - andee96
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6224743723869324
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fed6accea61d0e165f7e561e53e62f0.svg
          fullname: Andreas
          isHf: false
          isPro: false
          name: andee96
          type: user
        html: "<p>I tried what you suggested in the following way: </p>\n<pre><code\
          \ class=\"language-python3\">instance_type = \"ml.g4dn.12xlarge\"\nnumber_of_gpu\
          \ = 4\nhealth_check_timeout = 300\n\nmodel_name = \"falcon-40b-instruct\"\
          \ + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\nprint(model_name)\n\
          \n\n# TGI config\nconfig = {\n  'HF_MODEL_ID': \"tiiuae/falcon-40b-instruct\"\
          , # model_id from hf.co/models\n  'SM_NUM_GPUS': json.dumps(number_of_gpu),\
          \ # Number of GPU used per replica\n  'HF_MODEL_QUANTIZE': \"bitsandbytes\"\
          , # comment in to quantize,\n  'HF_TASK': 'text-generation'\n}\n\nmodel\
          \ = HuggingFaceModel(\n    name=model_name,\n    role=role,\n    image_uri=image_uri,\n\
          \    env=config,\n)\npredictor = model.deploy(\n  initial_instance_count=1,\n\
          \  instance_type=instance_type,\n  endpoint_name=model_name,\n)\n\ninput_data\
          \ = {\n  \"inputs\": \"User: Hello, how are you?\\n Assistant:\",\n  \"\
          parameters\": {\n    \"do_sample\": True,\n    \"top_k\": 1,\n    \"max_length\"\
          : 100,\n    \"include_prompt_in_result\": False\n  }\n}\n</code></pre>\n\
          <p>And unfortunately I still get the following response:<br><code>[{'generated_text':\
          \ \"User: Hello, how are you?\\n Assistant: I'm fine, how can I help you?\"\
          }]</code></p>\n"
        raw: "I tried what you suggested in the following way: \n```python3\ninstance_type\
          \ = \"ml.g4dn.12xlarge\"\nnumber_of_gpu = 4\nhealth_check_timeout = 300\n\
          \nmodel_name = \"falcon-40b-instruct\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\"\
          , time.gmtime())\nprint(model_name)\n\n\n# TGI config\nconfig = {\n  'HF_MODEL_ID':\
          \ \"tiiuae/falcon-40b-instruct\", # model_id from hf.co/models\n  'SM_NUM_GPUS':\
          \ json.dumps(number_of_gpu), # Number of GPU used per replica\n  'HF_MODEL_QUANTIZE':\
          \ \"bitsandbytes\", # comment in to quantize,\n  'HF_TASK': 'text-generation'\n\
          }\n\nmodel = HuggingFaceModel(\n    name=model_name,\n    role=role,\n \
          \   image_uri=image_uri,\n    env=config,\n)\npredictor = model.deploy(\n\
          \  initial_instance_count=1,\n  instance_type=instance_type,\n  endpoint_name=model_name,\n\
          )\n\ninput_data = {\n  \"inputs\": \"User: Hello, how are you?\\n Assistant:\"\
          ,\n  \"parameters\": {\n    \"do_sample\": True,\n    \"top_k\": 1,\n  \
          \  \"max_length\": 100,\n    \"include_prompt_in_result\": False\n  }\n\
          }\n```\n\nAnd unfortunately I still get the following response: \n`[{'generated_text':\
          \ \"User: Hello, how are you?\\n Assistant: I'm fine, how can I help you?\"\
          }]`"
        updatedAt: '2023-06-13T08:16:51.669Z'
      numEdits: 0
      reactions: []
    id: 648825f36cc0f5798901258f
    type: comment
  author: andee96
  content: "I tried what you suggested in the following way: \n```python3\ninstance_type\
    \ = \"ml.g4dn.12xlarge\"\nnumber_of_gpu = 4\nhealth_check_timeout = 300\n\nmodel_name\
    \ = \"falcon-40b-instruct\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n\
    print(model_name)\n\n\n# TGI config\nconfig = {\n  'HF_MODEL_ID': \"tiiuae/falcon-40b-instruct\"\
    , # model_id from hf.co/models\n  'SM_NUM_GPUS': json.dumps(number_of_gpu), #\
    \ Number of GPU used per replica\n  'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment\
    \ in to quantize,\n  'HF_TASK': 'text-generation'\n}\n\nmodel = HuggingFaceModel(\n\
    \    name=model_name,\n    role=role,\n    image_uri=image_uri,\n    env=config,\n\
    )\npredictor = model.deploy(\n  initial_instance_count=1,\n  instance_type=instance_type,\n\
    \  endpoint_name=model_name,\n)\n\ninput_data = {\n  \"inputs\": \"User: Hello,\
    \ how are you?\\n Assistant:\",\n  \"parameters\": {\n    \"do_sample\": True,\n\
    \    \"top_k\": 1,\n    \"max_length\": 100,\n    \"include_prompt_in_result\"\
    : False\n  }\n}\n```\n\nAnd unfortunately I still get the following response:\
    \ \n`[{'generated_text': \"User: Hello, how are you?\\n Assistant: I'm fine, how\
    \ can I help you?\"}]`"
  created_at: 2023-06-13 07:16:51+00:00
  edited: false
  hidden: false
  id: 648825f36cc0f5798901258f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/36ae94c3537063dee49095e97414fc83.svg
      fullname: Vaidyanathan P K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vaidyank
      type: user
    createdAt: '2023-06-14T03:49:41.000Z'
    data:
      edited: false
      editors:
      - vaidyank
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6060052514076233
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/36ae94c3537063dee49095e97414fc83.svg
          fullname: Vaidyanathan P K
          isHf: false
          isPro: false
          name: vaidyank
          type: user
        html: '<p>Use  "return_full_text": False in the parameters to resolve this
          issue. Thank me later :) </p>

          '
        raw: "Use  \"return_full_text\": False in the parameters to resolve this issue.\
          \ Thank me later :) \n"
        updatedAt: '2023-06-14T03:49:41.048Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - vaidyank
        - max0uu
        - AliYoussef97
        - jjphorn
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - vaidyank
        - andee96
      - count: 1
        reaction: "\U0001F92F"
        users:
        - vaidyank
    id: 648938d54b47b34bd3d2d53e
    type: comment
  author: vaidyank
  content: "Use  \"return_full_text\": False in the parameters to resolve this issue.\
    \ Thank me later :) \n"
  created_at: 2023-06-14 02:49:41+00:00
  edited: false
  hidden: false
  id: 648938d54b47b34bd3d2d53e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fed6accea61d0e165f7e561e53e62f0.svg
      fullname: Andreas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andee96
      type: user
    createdAt: '2023-06-14T08:27:41.000Z'
    data:
      edited: false
      editors:
      - andee96
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8271241784095764
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fed6accea61d0e165f7e561e53e62f0.svg
          fullname: Andreas
          isHf: false
          isPro: false
          name: andee96
          type: user
        html: "<p>That's amazing, thank you! Can confirm that this worked! Follow-up\
          \ question, I am still getting the output returned when I try to use langchain\
          \ with the deployed endpoint. I would have thought that passing <code>include_prompt_in_result=False</code>\
          \ to the <code>model_kwargs</code> parameter would do the trick but that\
          \ does not seem to be the case. </p>\n<pre><code class=\"language-python3\"\
          >from langchain import SagemakerEndpoint\nllm = SagemakerEndpoint(\n   \
          \     endpoint_name=predictor.endpoint_name, \n        credentials_profile_name=\"\
          dev\", \n        region_name=\"eu-west-2\", \n        model_kwargs={\"temperature\"\
          :0.7, \"max_length\": 1024, \"return_full_text\": False},\n        content_handler=content_handler\n\
          )\n</code></pre>\n<p>However, if i use this llm in any chain, the initial\
          \ prompt gets returned again... Any clue what I am doing wrong here? :)\
          \ <span data-props=\"{&quot;user&quot;:&quot;vaidyank&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vaidyank\">@<span class=\"\
          underline\">vaidyank</span></a></span>\n\n\t</span></span> </p>\n"
        raw: "That's amazing, thank you! Can confirm that this worked! Follow-up question,\
          \ I am still getting the output returned when I try to use langchain with\
          \ the deployed endpoint. I would have thought that passing `include_prompt_in_result=False`\
          \ to the `model_kwargs` parameter would do the trick but that does not seem\
          \ to be the case. \n\n```python3\nfrom langchain import SagemakerEndpoint\n\
          llm = SagemakerEndpoint(\n        endpoint_name=predictor.endpoint_name,\
          \ \n        credentials_profile_name=\"dev\", \n        region_name=\"eu-west-2\"\
          , \n        model_kwargs={\"temperature\":0.7, \"max_length\": 1024, \"\
          return_full_text\": False},\n        content_handler=content_handler\n)\n\
          \n```\n\nHowever, if i use this llm in any chain, the initial prompt gets\
          \ returned again... Any clue what I am doing wrong here? :) @vaidyank \n\
          \n"
        updatedAt: '2023-06-14T08:27:41.305Z'
      numEdits: 0
      reactions: []
    id: 648979fd639f0780c6a44de7
    type: comment
  author: andee96
  content: "That's amazing, thank you! Can confirm that this worked! Follow-up question,\
    \ I am still getting the output returned when I try to use langchain with the\
    \ deployed endpoint. I would have thought that passing `include_prompt_in_result=False`\
    \ to the `model_kwargs` parameter would do the trick but that does not seem to\
    \ be the case. \n\n```python3\nfrom langchain import SagemakerEndpoint\nllm =\
    \ SagemakerEndpoint(\n        endpoint_name=predictor.endpoint_name, \n      \
    \  credentials_profile_name=\"dev\", \n        region_name=\"eu-west-2\", \n \
    \       model_kwargs={\"temperature\":0.7, \"max_length\": 1024, \"return_full_text\"\
    : False},\n        content_handler=content_handler\n)\n\n```\n\nHowever, if i\
    \ use this llm in any chain, the initial prompt gets returned again... Any clue\
    \ what I am doing wrong here? :) @vaidyank \n\n"
  created_at: 2023-06-14 07:27:41+00:00
  edited: false
  hidden: false
  id: 648979fd639f0780c6a44de7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94d67795a09f818311e9259ebae334e6.svg
      fullname: Damon M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: damontrp
      type: user
    createdAt: '2023-06-14T17:46:11.000Z'
    data:
      edited: false
      editors:
      - damontrp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9289069175720215
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94d67795a09f818311e9259ebae334e6.svg
          fullname: Damon M
          isHf: false
          isPro: false
          name: damontrp
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;andee96&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/andee96\">@<span class=\"\
          underline\">andee96</span></a></span>\n\n\t</span></span> I have falcon-40b\
          \ deployed on sagemaker and I use  </p>\n<p> \"return_full_text\": false</p>\n\
          <p>To get this to stop doing the behavior your describing. The inference\
          \ container is written in rust it seems like and when it does json serialization\
          \ it might not like False.</p>\n<p>LMK if that works! </p>\n"
        raw: "@andee96 I have falcon-40b deployed on sagemaker and I use  \n\n \"\
          return_full_text\": false\n\nTo get this to stop doing the behavior your\
          \ describing. The inference container is written in rust it seems like and\
          \ when it does json serialization it might not like False.\n\nLMK if that\
          \ works! \n"
        updatedAt: '2023-06-14T17:46:11.106Z'
      numEdits: 0
      reactions: []
    id: 6489fce3221aa2df254b79ea
    type: comment
  author: damontrp
  content: "@andee96 I have falcon-40b deployed on sagemaker and I use  \n\n \"return_full_text\"\
    : false\n\nTo get this to stop doing the behavior your describing. The inference\
    \ container is written in rust it seems like and when it does json serialization\
    \ it might not like False.\n\nLMK if that works! \n"
  created_at: 2023-06-14 16:46:11+00:00
  edited: false
  hidden: false
  id: 6489fce3221aa2df254b79ea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 43
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: open
target_branch: null
title: Model returns entire input prompt together with output
