!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DioulaD
conflicting_files: null
created_at: 2023-07-20 08:58:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6375da34e3413701a9f141a7/M-dT9IkhIM-8DzRfnbAMo.jpeg?w=200&h=200&f=face
      fullname: Dioula Doucoure
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DioulaD
      type: user
    createdAt: '2023-07-20T09:58:21.000Z'
    data:
      edited: false
      editors:
      - DioulaD
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7052288055419922
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6375da34e3413701a9f141a7/M-dT9IkhIM-8DzRfnbAMo.jpeg?w=200&h=200&f=face
          fullname: Dioula Doucoure
          isHf: false
          isPro: false
          name: DioulaD
          type: user
        html: "<p>Hi everyone,<br>I have a finetuned model that i have tested on colab\
          \ and it runs without any issue. I am trying to do the same locally but\
          \ I gettting some issues.</p>\n<pre><code>ValueError: The current 'device_map'\
          \  had weights offloaded to the disk. Please provide an 'offload_folder'\
          \ for them. Alternatively, make sure you have 'safe tensors' installed if\
          \ the model you are using offers weights in this format\n</code></pre>\n\
          <p>I added the offload folder as suggested but still unable to run the model\
          \ successfully. Below is the code:</p>\n<pre><code>from transformers import\
          \ AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel, PeftConfig\n\
          import torch\n\n\ndef load_peft_model():\n    peft_model_id = \"DioulaD/falcon-7b-instruct-qlora-ge-dq-v2\"\
          \    \n    model = AutoModelForCausalLM.from_pretrained(\n            \"\
          tiiuae/falcon-7b-instruct\",\n            torch_dtype=torch.bfloat16,\n\
          \            device_map=\"auto\",\n            trust_remote_code=True,\n\
          \            offload_folder = \"offload\",\n            offload_state_dict\
          \ = True\n        )\n    model = PeftModel.from_pretrained(model, peft_model_id,\
          \ offload_folder=\"offload\" )\n    model = model.merge_and_unload()\n \
          \   \n\n    config = PeftConfig.from_pretrained(peft_model_id)\n\n    tknizer\
          \ = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n    tknizer.pad_token\
          \ = tknizer.eos_token\n    return model, tknizer\n\ndef get_expectations(prompt,\
          \ model, tknizer):\n  \"\"\"\n  Convert natural language query to great\
          \ expectation methods using finetuned falcon 7b\n  Params:\n    prompt :\
          \ Natural language query\n    model : Model download from huggingface hub\n\
          \    tknizer = Tokenizer from peft model\n  \"\"\"\n  try:\n    # If CUDA\
          \ support is not available, encoding will silenty fail if cuda:0 is hardcoded\n\
          \    if torch.cuda.is_available():\n      device = 'cuda:0'\n    else:\n\
          \      device = 'cpu'\n    \n    encoding = tknizer(prompt, return_tensors=\"\
          pt\").to(device)\n    model.to(device)\n\n    with torch.inference_mode():\n\
          \      out = model.generate(\n          input_ids=encoding.input_ids,\n\
          \          attention_mask=encoding.attention_mask,\n          max_new_tokens=100,\
          \ do_sample=True, temperature=0.3,\n          eos_token_id=tknizer.eos_token_id,\n\
          \          top_k=0\n      )\n    response = tknizer.decode(out[0], skip_special_tokens=True)\n\
          \    return response.split(\"\\n\")[1]\n\n  except Exception as e:\n   \
          \ print(\"An error occurred: \", e)\n</code></pre>\n<pre><code>model, tknizer\
          \ = load_peft_model()\nprompt = \"Verify that the userID column does not\
          \ contain null values\"\noutput = get_expectations(prompt, model, tknizer)\n\
          </code></pre>\n<p>I added model.to(device) because I was getting an error\
          \ (inputs_ids was on cuda and model on cpu). No I am getting the following\
          \ error:</p>\n<pre><code>An error occurred:  Cannot copy out of meta tensor;\
          \ no data!\n</code></pre>\n<p>Thanks  in advance for you help!</p>\n<p><a\
          \ rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6375da34e3413701a9f141a7/9JQwufpHLxtA9aOCTyQDy.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6375da34e3413701a9f141a7/9JQwufpHLxtA9aOCTyQDy.png\"\
          ></a></p>\n"
        raw: "Hi everyone, \r\nI have a finetuned model that i have tested on colab\
          \ and it runs without any issue. I am trying to do the same locally but\
          \ I gettting some issues.\r\n```\r\nValueError: The current 'device_map'\
          \  had weights offloaded to the disk. Please provide an 'offload_folder'\
          \ for them. Alternatively, make sure you have 'safe tensors' installed if\
          \ the model you are using offers weights in this format\r\n```\r\nI added\
          \ the offload folder as suggested but still unable to run the model successfully.\
          \ Below is the code:\r\n\r\n```\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\r\nfrom peft import PeftModel, PeftConfig\r\nimport torch\r\
          \n\r\n\r\ndef load_peft_model():\r\n    peft_model_id = \"DioulaD/falcon-7b-instruct-qlora-ge-dq-v2\"\
          \    \r\n    model = AutoModelForCausalLM.from_pretrained(\r\n         \
          \   \"tiiuae/falcon-7b-instruct\",\r\n            torch_dtype=torch.bfloat16,\r\
          \n            device_map=\"auto\",\r\n            trust_remote_code=True,\r\
          \n            offload_folder = \"offload\",\r\n            offload_state_dict\
          \ = True\r\n        )\r\n    model = PeftModel.from_pretrained(model, peft_model_id,\
          \ offload_folder=\"offload\" )\r\n    model = model.merge_and_unload()\r\
          \n    \r\n\r\n    config = PeftConfig.from_pretrained(peft_model_id)\r\n\
          \r\n    tknizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\r\
          \n    tknizer.pad_token = tknizer.eos_token\r\n    return model, tknizer\r\
          \n\r\ndef get_expectations(prompt, model, tknizer):\r\n  \"\"\"\r\n  Convert\
          \ natural language query to great expectation methods using finetuned falcon\
          \ 7b\r\n  Params:\r\n    prompt : Natural language query\r\n    model :\
          \ Model download from huggingface hub\r\n    tknizer = Tokenizer from peft\
          \ model\r\n  \"\"\"\r\n  try:\r\n    # If CUDA support is not available,\
          \ encoding will silenty fail if cuda:0 is hardcoded\r\n    if torch.cuda.is_available():\r\
          \n      device = 'cuda:0'\r\n    else:\r\n      device = 'cpu'\r\n    \r\
          \n    encoding = tknizer(prompt, return_tensors=\"pt\").to(device)\r\n \
          \   model.to(device)\r\n\r\n    with torch.inference_mode():\r\n      out\
          \ = model.generate(\r\n          input_ids=encoding.input_ids,\r\n     \
          \     attention_mask=encoding.attention_mask,\r\n          max_new_tokens=100,\
          \ do_sample=True, temperature=0.3,\r\n          eos_token_id=tknizer.eos_token_id,\r\
          \n          top_k=0\r\n      )\r\n    response = tknizer.decode(out[0],\
          \ skip_special_tokens=True)\r\n    return response.split(\"\\n\")[1]\r\n\
          \r\n  except Exception as e:\r\n    print(\"An error occurred: \", e)\r\n\
          ```\r\n```\r\nmodel, tknizer = load_peft_model()\r\nprompt = \"Verify that\
          \ the userID column does not contain null values\"\r\noutput = get_expectations(prompt,\
          \ model, tknizer)\r\n```\r\nI added model.to(device) because I was getting\
          \ an error (inputs_ids was on cuda and model on cpu). No I am getting the\
          \ following error:\r\n\r\n```\r\nAn error occurred:  Cannot copy out of\
          \ meta tensor; no data!\r\n```\r\n\r\nThanks  in advance for you help!\r\
          \n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6375da34e3413701a9f141a7/9JQwufpHLxtA9aOCTyQDy.png)\r\
          \n\r\n"
        updatedAt: '2023-07-20T09:58:21.541Z'
      numEdits: 0
      reactions: []
    id: 64b9053d6b5ee8c388694178
    type: comment
  author: DioulaD
  content: "Hi everyone, \r\nI have a finetuned model that i have tested on colab\
    \ and it runs without any issue. I am trying to do the same locally but I gettting\
    \ some issues.\r\n```\r\nValueError: The current 'device_map'  had weights offloaded\
    \ to the disk. Please provide an 'offload_folder' for them. Alternatively, make\
    \ sure you have 'safe tensors' installed if the model you are using offers weights\
    \ in this format\r\n```\r\nI added the offload folder as suggested but still unable\
    \ to run the model successfully. Below is the code:\r\n\r\n```\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\r\nfrom peft import PeftModel, PeftConfig\r\
    \nimport torch\r\n\r\n\r\ndef load_peft_model():\r\n    peft_model_id = \"DioulaD/falcon-7b-instruct-qlora-ge-dq-v2\"\
    \    \r\n    model = AutoModelForCausalLM.from_pretrained(\r\n            \"tiiuae/falcon-7b-instruct\"\
    ,\r\n            torch_dtype=torch.bfloat16,\r\n            device_map=\"auto\"\
    ,\r\n            trust_remote_code=True,\r\n            offload_folder = \"offload\"\
    ,\r\n            offload_state_dict = True\r\n        )\r\n    model = PeftModel.from_pretrained(model,\
    \ peft_model_id, offload_folder=\"offload\" )\r\n    model = model.merge_and_unload()\r\
    \n    \r\n\r\n    config = PeftConfig.from_pretrained(peft_model_id)\r\n\r\n \
    \   tknizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\r\n\
    \    tknizer.pad_token = tknizer.eos_token\r\n    return model, tknizer\r\n\r\n\
    def get_expectations(prompt, model, tknizer):\r\n  \"\"\"\r\n  Convert natural\
    \ language query to great expectation methods using finetuned falcon 7b\r\n  Params:\r\
    \n    prompt : Natural language query\r\n    model : Model download from huggingface\
    \ hub\r\n    tknizer = Tokenizer from peft model\r\n  \"\"\"\r\n  try:\r\n   \
    \ # If CUDA support is not available, encoding will silenty fail if cuda:0 is\
    \ hardcoded\r\n    if torch.cuda.is_available():\r\n      device = 'cuda:0'\r\n\
    \    else:\r\n      device = 'cpu'\r\n    \r\n    encoding = tknizer(prompt, return_tensors=\"\
    pt\").to(device)\r\n    model.to(device)\r\n\r\n    with torch.inference_mode():\r\
    \n      out = model.generate(\r\n          input_ids=encoding.input_ids,\r\n \
    \         attention_mask=encoding.attention_mask,\r\n          max_new_tokens=100,\
    \ do_sample=True, temperature=0.3,\r\n          eos_token_id=tknizer.eos_token_id,\r\
    \n          top_k=0\r\n      )\r\n    response = tknizer.decode(out[0], skip_special_tokens=True)\r\
    \n    return response.split(\"\\n\")[1]\r\n\r\n  except Exception as e:\r\n  \
    \  print(\"An error occurred: \", e)\r\n```\r\n```\r\nmodel, tknizer = load_peft_model()\r\
    \nprompt = \"Verify that the userID column does not contain null values\"\r\n\
    output = get_expectations(prompt, model, tknizer)\r\n```\r\nI added model.to(device)\
    \ because I was getting an error (inputs_ids was on cuda and model on cpu). No\
    \ I am getting the following error:\r\n\r\n```\r\nAn error occurred:  Cannot copy\
    \ out of meta tensor; no data!\r\n```\r\n\r\nThanks  in advance for you help!\r\
    \n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6375da34e3413701a9f141a7/9JQwufpHLxtA9aOCTyQDy.png)\r\
    \n\r\n"
  created_at: 2023-07-20 08:58:21+00:00
  edited: false
  hidden: false
  id: 64b9053d6b5ee8c388694178
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6375da34e3413701a9f141a7/M-dT9IkhIM-8DzRfnbAMo.jpeg?w=200&h=200&f=face
      fullname: Dioula Doucoure
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DioulaD
      type: user
    createdAt: '2023-07-20T10:17:23.000Z'
    data:
      from: Unable to load and run finetuned falcon 7b model
      to: Unable to load and run finetuned falcon model
    id: 64b909b32fccad9f5f115c46
    type: title-change
  author: DioulaD
  created_at: 2023-07-20 09:17:23+00:00
  id: 64b909b32fccad9f5f115c46
  new_title: Unable to load and run finetuned falcon model
  old_title: Unable to load and run finetuned falcon 7b model
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 70
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: open
target_branch: null
title: Unable to load and run finetuned falcon model
