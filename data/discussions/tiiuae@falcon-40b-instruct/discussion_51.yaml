!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TMTechnology
conflicting_files: null
created_at: 2023-06-23 16:37:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf8c54146db71eb47bfb4dc5a04e8b24.svg
      fullname: Paul Doan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TMTechnology
      type: user
    createdAt: '2023-06-23T17:37:03.000Z'
    data:
      edited: false
      editors:
      - TMTechnology
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7269359230995178
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf8c54146db71eb47bfb4dc5a04e8b24.svg
          fullname: Paul Doan
          isHf: false
          isPro: false
          name: TMTechnology
          type: user
        html: "<p>Hi all</p>\n<p>I am using a hosted inference endpoint on HF, and\
          \ calling it through the HuggingFace endpoint provided by langchain.</p>\n\
          <p>When I ask any question, the output seems to be truncated, any idea as\
          \ to why that might be the case?</p>\n<p>Following is my code:</p>\n<pre><code>from\
          \ langchain.llms import HuggingFaceEndpoint\nfrom langchain import HuggingFaceHub\n\
          from langchain import PromptTemplate, LLMChain\n\nendpoint_url = (\n   \
          \         'ENDPOINT_URL'\n)\nhf = HuggingFaceEndpoint(\n    endpoint_url=endpoint_url,\n\
          \    huggingfacehub_api_token= TOKEN,\n    task = 'text-generation' \n)\n\
          \ntemplate = \"\"\"Question: {question}\n\nAnswer: \"\"\"\n\nprompt = PromptTemplate(template=template,\
          \ input_variables=[\"question\"])\nllm_chain = LLMChain(prompt=prompt, llm=hf)\n\
          \nquestion = \"When did Germany unite? \"\n\nprint(llm_chain.run(question))\n\
          </code></pre>\n<p>And the following is my output:</p>\n<pre><code> 1990,\
          \ following the reunification of East\n</code></pre>\n<p>Any help please?</p>\n\
          <p>Thanks</p>\n"
        raw: "Hi all\r\n\r\nI am using a hosted inference endpoint on HF, and calling\
          \ it through the HuggingFace endpoint provided by langchain.\r\n\r\nWhen\
          \ I ask any question, the output seems to be truncated, any idea as to why\
          \ that might be the case?\r\n\r\nFollowing is my code:\r\n\r\n```\r\nfrom\
          \ langchain.llms import HuggingFaceEndpoint\r\nfrom langchain import HuggingFaceHub\r\
          \nfrom langchain import PromptTemplate, LLMChain\r\n\r\nendpoint_url = (\r\
          \n            'ENDPOINT_URL'\r\n)\r\nhf = HuggingFaceEndpoint(\r\n    endpoint_url=endpoint_url,\r\
          \n    huggingfacehub_api_token= TOKEN,\r\n    task = 'text-generation' \r\
          \n)\r\n\r\ntemplate = \"\"\"Question: {question}\r\n\r\nAnswer: \"\"\"\r\
          \n\r\nprompt = PromptTemplate(template=template, input_variables=[\"question\"\
          ])\r\nllm_chain = LLMChain(prompt=prompt, llm=hf)\r\n\r\nquestion = \"When\
          \ did Germany unite? \"\r\n\r\nprint(llm_chain.run(question))\r\n```\r\n\
          \r\nAnd the following is my output:\r\n\r\n```\r\n 1990, following the reunification\
          \ of East\r\n```\r\n\r\nAny help please?\r\n\r\nThanks"
        updatedAt: '2023-06-23T17:37:03.311Z'
      numEdits: 0
      reactions: []
    id: 6495d83f25b6d0ef5573bc1d
    type: comment
  author: TMTechnology
  content: "Hi all\r\n\r\nI am using a hosted inference endpoint on HF, and calling\
    \ it through the HuggingFace endpoint provided by langchain.\r\n\r\nWhen I ask\
    \ any question, the output seems to be truncated, any idea as to why that might\
    \ be the case?\r\n\r\nFollowing is my code:\r\n\r\n```\r\nfrom langchain.llms\
    \ import HuggingFaceEndpoint\r\nfrom langchain import HuggingFaceHub\r\nfrom langchain\
    \ import PromptTemplate, LLMChain\r\n\r\nendpoint_url = (\r\n            'ENDPOINT_URL'\r\
    \n)\r\nhf = HuggingFaceEndpoint(\r\n    endpoint_url=endpoint_url,\r\n    huggingfacehub_api_token=\
    \ TOKEN,\r\n    task = 'text-generation' \r\n)\r\n\r\ntemplate = \"\"\"Question:\
    \ {question}\r\n\r\nAnswer: \"\"\"\r\n\r\nprompt = PromptTemplate(template=template,\
    \ input_variables=[\"question\"])\r\nllm_chain = LLMChain(prompt=prompt, llm=hf)\r\
    \n\r\nquestion = \"When did Germany unite? \"\r\n\r\nprint(llm_chain.run(question))\r\
    \n```\r\n\r\nAnd the following is my output:\r\n\r\n```\r\n 1990, following the\
    \ reunification of East\r\n```\r\n\r\nAny help please?\r\n\r\nThanks"
  created_at: 2023-06-23 16:37:03+00:00
  edited: false
  hidden: false
  id: 6495d83f25b6d0ef5573bc1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da02c09512daba37691f4a31c72c4991.svg
      fullname: Troy Vo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: troybvo
      type: user
    createdAt: '2023-06-25T05:30:13.000Z'
    data:
      edited: false
      editors:
      - troybvo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6238678097724915
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da02c09512daba37691f4a31c72c4991.svg
          fullname: Troy Vo
          isHf: false
          isPro: false
          name: troybvo
          type: user
        html: '<p>HuggingFaceEndpoint truncates the text because it assumes the endpoint
          returns the prompt together with generated text. You need to modify the
          _call method of HuggingFaceEndpoint so that it doesn''t substring the generated_text
          and return the whole text.</p>

          '
        raw: HuggingFaceEndpoint truncates the text because it assumes the endpoint
          returns the prompt together with generated text. You need to modify the
          _call method of HuggingFaceEndpoint so that it doesn't substring the generated_text
          and return the whole text.
        updatedAt: '2023-06-25T05:30:13.589Z'
      numEdits: 0
      reactions: []
    id: 6497d0e5290b63c3880da4d5
    type: comment
  author: troybvo
  content: HuggingFaceEndpoint truncates the text because it assumes the endpoint
    returns the prompt together with generated text. You need to modify the _call
    method of HuggingFaceEndpoint so that it doesn't substring the generated_text
    and return the whole text.
  created_at: 2023-06-25 04:30:13+00:00
  edited: false
  hidden: false
  id: 6497d0e5290b63c3880da4d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf8c54146db71eb47bfb4dc5a04e8b24.svg
      fullname: Paul Doan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TMTechnology
      type: user
    createdAt: '2023-06-27T03:35:30.000Z'
    data:
      edited: false
      editors:
      - TMTechnology
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7178057432174683
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf8c54146db71eb47bfb4dc5a04e8b24.svg
          fullname: Paul Doan
          isHf: false
          isPro: false
          name: TMTechnology
          type: user
        html: "<blockquote>\n<p>HuggingFaceEndpoint truncates the text because it\
          \ assumes the endpoint returns the prompt together with generated text.\
          \ You need to modify the _call method of HuggingFaceEndpoint so that it\
          \ doesn't substring the generated_text and return the whole text.</p>\n\
          </blockquote>\n<p>So you mean the following part specifically in the _call\
          \ method?:</p>\n<pre><code class=\"language-if\">            # Text generation\
          \ return includes the starter text.\n            text = generated_text[0][\"\
          generated_text\"][len(prompt) :]\n</code></pre>\n<p>I have to play with\
          \ the indexing which is currently done to get the part after the prompt\
          \ length?</p>\n<p><a rel=\"nofollow\" href=\"https://github.com/hwchase17/langchain/blob/master/langchain/llms/huggingface_endpoint.py\"\
          >https://github.com/hwchase17/langchain/blob/master/langchain/llms/huggingface_endpoint.py</a></p>\n"
        raw: "> HuggingFaceEndpoint truncates the text because it assumes the endpoint\
          \ returns the prompt together with generated text. You need to modify the\
          \ _call method of HuggingFaceEndpoint so that it doesn't substring the generated_text\
          \ and return the whole text.\n\nSo you mean the following part specifically\
          \ in the _call method?:\n\n```if self.task == \"text-generation\":\n   \
          \         # Text generation return includes the starter text.\n        \
          \    text = generated_text[0][\"generated_text\"][len(prompt) :]\n```\n\n\
          I have to play with the indexing which is currently done to get the part\
          \ after the prompt length?\n\nhttps://github.com/hwchase17/langchain/blob/master/langchain/llms/huggingface_endpoint.py"
        updatedAt: '2023-06-27T03:35:30.884Z'
      numEdits: 0
      reactions: []
    id: 649a5902685215f724866ef6
    type: comment
  author: TMTechnology
  content: "> HuggingFaceEndpoint truncates the text because it assumes the endpoint\
    \ returns the prompt together with generated text. You need to modify the _call\
    \ method of HuggingFaceEndpoint so that it doesn't substring the generated_text\
    \ and return the whole text.\n\nSo you mean the following part specifically in\
    \ the _call method?:\n\n```if self.task == \"text-generation\":\n            #\
    \ Text generation return includes the starter text.\n            text = generated_text[0][\"\
    generated_text\"][len(prompt) :]\n```\n\nI have to play with the indexing which\
    \ is currently done to get the part after the prompt length?\n\nhttps://github.com/hwchase17/langchain/blob/master/langchain/llms/huggingface_endpoint.py"
  created_at: 2023-06-27 02:35:30+00:00
  edited: false
  hidden: false
  id: 649a5902685215f724866ef6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da02c09512daba37691f4a31c72c4991.svg
      fullname: Troy Vo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: troybvo
      type: user
    createdAt: '2023-06-27T03:55:03.000Z'
    data:
      edited: false
      editors:
      - troybvo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7046374678611755
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da02c09512daba37691f4a31c72c4991.svg
          fullname: Troy Vo
          isHf: false
          isPro: false
          name: troybvo
          type: user
        html: '<p>No just remove the indexing. The indexing assumes that the generated_text
          includes the prompt (hence it''s substring the generated_text from len(prompt)
          to the end. Just modify it to be<br>text = generated_text[0]["generated_text"].</p>

          '
        raw: "No just remove the indexing. The indexing assumes that the generated_text\
          \ includes the prompt (hence it's substring the generated_text from len(prompt)\
          \ to the end. Just modify it to be \ntext = generated_text[0][\"generated_text\"\
          ]."
        updatedAt: '2023-06-27T03:55:03.883Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TMTechnology
    id: 649a5d97748afb170bcde010
    type: comment
  author: troybvo
  content: "No just remove the indexing. The indexing assumes that the generated_text\
    \ includes the prompt (hence it's substring the generated_text from len(prompt)\
    \ to the end. Just modify it to be \ntext = generated_text[0][\"generated_text\"\
    ]."
  created_at: 2023-06-27 02:55:03+00:00
  edited: false
  hidden: false
  id: 649a5d97748afb170bcde010
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf8c54146db71eb47bfb4dc5a04e8b24.svg
      fullname: Paul Doan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TMTechnology
      type: user
    createdAt: '2023-06-28T03:50:55.000Z'
    data:
      edited: false
      editors:
      - TMTechnology
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8147653937339783
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf8c54146db71eb47bfb4dc5a04e8b24.svg
          fullname: Paul Doan
          isHf: false
          isPro: false
          name: TMTechnology
          type: user
        html: '<blockquote>

          <p>No just remove the indexing. The indexing assumes that the generated_text
          includes the prompt (hence it''s substring the generated_text from len(prompt)
          to the end. Just modify it to be<br>text = generated_text[0]["generated_text"].</p>

          </blockquote>

          <p>Yup that''s what I meant. Thank you.</p>

          '
        raw: "> No just remove the indexing. The indexing assumes that the generated_text\
          \ includes the prompt (hence it's substring the generated_text from len(prompt)\
          \ to the end. Just modify it to be \n> text = generated_text[0][\"generated_text\"\
          ].\n\nYup that's what I meant. Thank you."
        updatedAt: '2023-06-28T03:50:55.405Z'
      numEdits: 0
      reactions: []
    id: 649bae1fb04c54a9375a4da5
    type: comment
  author: TMTechnology
  content: "> No just remove the indexing. The indexing assumes that the generated_text\
    \ includes the prompt (hence it's substring the generated_text from len(prompt)\
    \ to the end. Just modify it to be \n> text = generated_text[0][\"generated_text\"\
    ].\n\nYup that's what I meant. Thank you."
  created_at: 2023-06-28 02:50:55+00:00
  edited: false
  hidden: false
  id: 649bae1fb04c54a9375a4da5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 51
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: open
target_branch: null
title: Truncated output from API call through langchain
