!!python/object:huggingface_hub.community.DiscussionWithDetails
author: catid
conflicting_files: null
created_at: 2023-05-26 18:37:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661210778957-noauth.jpeg?w=200&h=200&f=face
      fullname: Chris Taylor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: catid
      type: user
    createdAt: '2023-05-26T19:37:02.000Z'
    data:
      edited: true
      editors:
      - catid
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661210778957-noauth.jpeg?w=200&h=200&f=face
          fullname: Chris Taylor
          isHf: false
          isPro: false
          name: catid
          type: user
        html: "<p>Created a conda environment with latest transformers and pytorch\
          \ and einops.</p>\n<p>Getting this error from the provided example script:</p>\n\
          <p>(supercharger) \u279C  supercharger git:(main) \u2717 python test_falcon.py<br>Traceback\
          \ (most recent call last):<br>  File \"/home/catid/sources/supercharger/test_falcon.py\"\
          , line 8, in <br>    pipeline = transformers.pipeline(<br>  File \"/home/catid/mambaforge/envs/supercharger/lib/python3.10/site-packages/transformers/pipelines/<strong>init</strong>.py\"\
          , line 788, in pipeline<br>    framework, model = infer_framework_load_model(<br>\
          \  File \"/home/catid/mambaforge/envs/supercharger/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 278, in infer_framework_load_model<br>    raise ValueError(f\"Could\
          \ not load model {model} with any of the following classes: {class_tuple}.\"\
          )<br>ValueError: Could not load model tiiuae/falcon-40b-instruct with any\
          \ of the following classes: (&lt;class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,).</p>\n"
        raw: "Created a conda environment with latest transformers and pytorch and\
          \ einops.\n\nGetting this error from the provided example script:\n\n(supercharger)\
          \ \u279C  supercharger git:(main) \u2717 python test_falcon.py\nTraceback\
          \ (most recent call last):\n  File \"/home/catid/sources/supercharger/test_falcon.py\"\
          , line 8, in <module>\n    pipeline = transformers.pipeline(\n  File \"\
          /home/catid/mambaforge/envs/supercharger/lib/python3.10/site-packages/transformers/pipelines/__init__.py\"\
          , line 788, in pipeline\n    framework, model = infer_framework_load_model(\n\
          \  File \"/home/catid/mambaforge/envs/supercharger/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 278, in infer_framework_load_model\n    raise ValueError(f\"Could\
          \ not load model {model} with any of the following classes: {class_tuple}.\"\
          )\nValueError: Could not load model tiiuae/falcon-40b-instruct with any\
          \ of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
        updatedAt: '2023-05-26T19:37:28.692Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - Crenox
    id: 64710a5ec6b7783c87b82929
    type: comment
  author: catid
  content: "Created a conda environment with latest transformers and pytorch and einops.\n\
    \nGetting this error from the provided example script:\n\n(supercharger) \u279C\
    \  supercharger git:(main) \u2717 python test_falcon.py\nTraceback (most recent\
    \ call last):\n  File \"/home/catid/sources/supercharger/test_falcon.py\", line\
    \ 8, in <module>\n    pipeline = transformers.pipeline(\n  File \"/home/catid/mambaforge/envs/supercharger/lib/python3.10/site-packages/transformers/pipelines/__init__.py\"\
    , line 788, in pipeline\n    framework, model = infer_framework_load_model(\n\
    \  File \"/home/catid/mambaforge/envs/supercharger/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 278, in infer_framework_load_model\n    raise ValueError(f\"Could not load\
    \ model {model} with any of the following classes: {class_tuple}.\")\nValueError:\
    \ Could not load model tiiuae/falcon-40b-instruct with any of the following classes:\
    \ (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
  created_at: 2023-05-26 18:37:02+00:00
  edited: true
  hidden: false
  id: 64710a5ec6b7783c87b82929
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645010dc33ac8f46fa0f9a90/lRQjpCryb_sU-PFd5PUIR.jpeg?w=200&h=200&f=face
      fullname: Crenox Marvin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Crenox
      type: user
    createdAt: '2023-05-27T14:56:26.000Z'
    data:
      edited: false
      editors:
      - Crenox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645010dc33ac8f46fa0f9a90/lRQjpCryb_sU-PFd5PUIR.jpeg?w=200&h=200&f=face
          fullname: Crenox Marvin
          isHf: false
          isPro: false
          name: Crenox
          type: user
        html: '<p>Yeah, same problem here. It doesn''t even use the AutoModelForCausalLM.
          When I try to use it like: model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-40b-instruct")
          it still throws an error because the trust_remote_code parameter isn''t
          set to True. I don''t know how to fix it tho.</p>

          '
        raw: 'Yeah, same problem here. It doesn''t even use the AutoModelForCausalLM.
          When I try to use it like: model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-40b-instruct")
          it still throws an error because the trust_remote_code parameter isn''t
          set to True. I don''t know how to fix it tho.'
        updatedAt: '2023-05-27T14:56:26.543Z'
      numEdits: 0
      reactions: []
    id: 64721a1a0211f852700255f8
    type: comment
  author: Crenox
  content: 'Yeah, same problem here. It doesn''t even use the AutoModelForCausalLM.
    When I try to use it like: model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-40b-instruct")
    it still throws an error because the trust_remote_code parameter isn''t set to
    True. I don''t know how to fix it tho.'
  created_at: 2023-05-27 13:56:26+00:00
  edited: false
  hidden: false
  id: 64721a1a0211f852700255f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/2HsEDUGMNLTe8osS1EBhb.jpeg?w=200&h=200&f=face
      fullname: Supun Dewaraja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: supdeva
      type: user
    createdAt: '2023-05-28T14:01:22.000Z'
    data:
      edited: false
      editors:
      - supdeva
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/2HsEDUGMNLTe8osS1EBhb.jpeg?w=200&h=200&f=face
          fullname: Supun Dewaraja
          isHf: false
          isPro: false
          name: supdeva
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Crenox&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Crenox\">@<span class=\"\
          underline\">Crenox</span></a></span>\n\n\t</span></span> </p>\n<p>This worked\
          \ for me</p>\n<p>model = AutoModelForCausalLM.from_pretrained(<br>    \"\
          tiiuae/falcon-40b-instruct\", trust_remote_code=True<br>)</p>\n"
        raw: "@Crenox \n\nThis worked for me\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    \"tiiuae/falcon-40b-instruct\", trust_remote_code=True\n)"
        updatedAt: '2023-05-28T14:01:22.630Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - coed
        - FalconLLM
        - BCenti
        - Joey
        - Crenox
    id: 64735eb2352c94a20dd4b1d3
    type: comment
  author: supdeva
  content: "@Crenox \n\nThis worked for me\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    \"tiiuae/falcon-40b-instruct\", trust_remote_code=True\n)"
  created_at: 2023-05-28 13:01:22+00:00
  edited: false
  hidden: false
  id: 64735eb2352c94a20dd4b1d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f84c77b2777aa4f7be84fd61a3962a29.svg
      fullname: Shikha Singhal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: singhalshikha518
      type: user
    createdAt: '2023-06-01T17:43:43.000Z'
    data:
      edited: false
      editors:
      - singhalshikha518
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f84c77b2777aa4f7be84fd61a3962a29.svg
          fullname: Shikha Singhal
          isHf: false
          isPro: false
          name: singhalshikha518
          type: user
        html: '<p>While generating text with falcon 40b instruct. Getting below error:<br>AttributeError:
          module ''torch.nn.functional'' has no attribute ''scaled_dot_product_attention''</p>

          '
        raw: 'While generating text with falcon 40b instruct. Getting below error:

          AttributeError: module ''torch.nn.functional'' has no attribute ''scaled_dot_product_attention'''
        updatedAt: '2023-06-01T17:43:43.068Z'
      numEdits: 0
      reactions: []
    id: 6478d8cf25e06d2ffe8a4185
    type: comment
  author: singhalshikha518
  content: 'While generating text with falcon 40b instruct. Getting below error:

    AttributeError: module ''torch.nn.functional'' has no attribute ''scaled_dot_product_attention'''
  created_at: 2023-06-01 16:43:43+00:00
  edited: false
  hidden: false
  id: 6478d8cf25e06d2ffe8a4185
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-02T10:26:10.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>you need to upgrade to torch 2.0. That fixed the attribute error
          for me</p>

          '
        raw: you need to upgrade to torch 2.0. That fixed the attribute error for
          me
        updatedAt: '2023-06-02T10:26:10.202Z'
      numEdits: 0
      reactions: []
    id: 6479c3c23a17d5e00acbf3ca
    type: comment
  author: eastwind
  content: you need to upgrade to torch 2.0. That fixed the attribute error for me
  created_at: 2023-06-02 09:26:10+00:00
  edited: false
  hidden: false
  id: 6479c3c23a17d5e00acbf3ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
      fullname: Bernd Brassel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captain-fim
      type: user
    createdAt: '2023-06-03T09:12:15.000Z'
    data:
      edited: false
      editors:
      - captain-fim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44299304485321045
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
          fullname: Bernd Brassel
          isHf: false
          isPro: false
          name: captain-fim
          type: user
        html: "<p>After trying <span data-props=\"{&quot;user&quot;:&quot;supdeva&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/supdeva\"\
          >@<span class=\"underline\">supdeva</span></a></span>\n\n\t</span></span>\
          \ 's fix to create the model I have this code:</p>\n<pre><code>model_name\
          \ = \"tiiuae/falcon-40b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n\
          </code></pre>\n<p>The tokenizer is created successfully, but the model creation\
          \ yields the error:</p>\n<p>The model 'RWForCausalLM' is not supported for\
          \ text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel',\
          \ 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
          \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
          \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
          \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
          \ 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM',\
          \ 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM',\
          \ 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM',\
          \ 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM',\
          \ 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM',\
          \ 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead',\
          \ 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM',\
          \ 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM',\
          \ 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel',\
          \ 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM',\
          \ 'XLNetLMHeadModel', 'XmodForCausalLM'].</p>\n<p>What am I doing wrong?</p>\n"
        raw: 'After trying @supdeva ''s fix to create the model I have this code:


          ```

          model_name = "tiiuae/falcon-40b-instruct"


          tokenizer = AutoTokenizer.from_pretrained(model_name)

          model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)

          ```


          The tokenizer is created successfully, but the model creation yields the
          error:


          The model ''RWForCausalLM'' is not supported for text-generation. Supported
          models are [''BartForCausalLM'', ''BertLMHeadModel'', ''BertGenerationDecoder'',
          ''BigBirdForCausalLM'', ''BigBirdPegasusForCausalLM'', ''BioGptForCausalLM'',
          ''BlenderbotForCausalLM'', ''BlenderbotSmallForCausalLM'', ''BloomForCausalLM'',
          ''CamembertForCausalLM'', ''CodeGenForCausalLM'', ''CpmAntForCausalLM'',
          ''CTRLLMHeadModel'', ''Data2VecTextForCausalLM'', ''ElectraForCausalLM'',
          ''ErnieForCausalLM'', ''GitForCausalLM'', ''GPT2LMHeadModel'', ''GPT2LMHeadModel'',
          ''GPTBigCodeForCausalLM'', ''GPTNeoForCausalLM'', ''GPTNeoXForCausalLM'',
          ''GPTNeoXJapaneseForCausalLM'', ''GPTJForCausalLM'', ''LlamaForCausalLM'',
          ''MarianForCausalLM'', ''MBartForCausalLM'', ''MegaForCausalLM'', ''MegatronBertForCausalLM'',
          ''MvpForCausalLM'', ''OpenLlamaForCausalLM'', ''OpenAIGPTLMHeadModel'',
          ''OPTForCausalLM'', ''PegasusForCausalLM'', ''PLBartForCausalLM'', ''ProphetNetForCausalLM'',
          ''QDQBertLMHeadModel'', ''ReformerModelWithLMHead'', ''RemBertForCausalLM'',
          ''RobertaForCausalLM'', ''RobertaPreLayerNormForCausalLM'', ''RoCBertForCausalLM'',
          ''RoFormerForCausalLM'', ''RwkvForCausalLM'', ''Speech2Text2ForCausalLM'',
          ''TransfoXLLMHeadModel'', ''TrOCRForCausalLM'', ''XGLMForCausalLM'', ''XLMWithLMHeadModel'',
          ''XLMProphetNetForCausalLM'', ''XLMRobertaForCausalLM'', ''XLMRobertaXLForCausalLM'',
          ''XLNetLMHeadModel'', ''XmodForCausalLM''].


          What am I doing wrong?'
        updatedAt: '2023-06-03T09:12:15.287Z'
      numEdits: 0
      reactions: []
    id: 647b03efe8b73330589bac2d
    type: comment
  author: captain-fim
  content: 'After trying @supdeva ''s fix to create the model I have this code:


    ```

    model_name = "tiiuae/falcon-40b-instruct"


    tokenizer = AutoTokenizer.from_pretrained(model_name)

    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)

    ```


    The tokenizer is created successfully, but the model creation yields the error:


    The model ''RWForCausalLM'' is not supported for text-generation. Supported models
    are [''BartForCausalLM'', ''BertLMHeadModel'', ''BertGenerationDecoder'', ''BigBirdForCausalLM'',
    ''BigBirdPegasusForCausalLM'', ''BioGptForCausalLM'', ''BlenderbotForCausalLM'',
    ''BlenderbotSmallForCausalLM'', ''BloomForCausalLM'', ''CamembertForCausalLM'',
    ''CodeGenForCausalLM'', ''CpmAntForCausalLM'', ''CTRLLMHeadModel'', ''Data2VecTextForCausalLM'',
    ''ElectraForCausalLM'', ''ErnieForCausalLM'', ''GitForCausalLM'', ''GPT2LMHeadModel'',
    ''GPT2LMHeadModel'', ''GPTBigCodeForCausalLM'', ''GPTNeoForCausalLM'', ''GPTNeoXForCausalLM'',
    ''GPTNeoXJapaneseForCausalLM'', ''GPTJForCausalLM'', ''LlamaForCausalLM'', ''MarianForCausalLM'',
    ''MBartForCausalLM'', ''MegaForCausalLM'', ''MegatronBertForCausalLM'', ''MvpForCausalLM'',
    ''OpenLlamaForCausalLM'', ''OpenAIGPTLMHeadModel'', ''OPTForCausalLM'', ''PegasusForCausalLM'',
    ''PLBartForCausalLM'', ''ProphetNetForCausalLM'', ''QDQBertLMHeadModel'', ''ReformerModelWithLMHead'',
    ''RemBertForCausalLM'', ''RobertaForCausalLM'', ''RobertaPreLayerNormForCausalLM'',
    ''RoCBertForCausalLM'', ''RoFormerForCausalLM'', ''RwkvForCausalLM'', ''Speech2Text2ForCausalLM'',
    ''TransfoXLLMHeadModel'', ''TrOCRForCausalLM'', ''XGLMForCausalLM'', ''XLMWithLMHeadModel'',
    ''XLMProphetNetForCausalLM'', ''XLMRobertaForCausalLM'', ''XLMRobertaXLForCausalLM'',
    ''XLNetLMHeadModel'', ''XmodForCausalLM''].


    What am I doing wrong?'
  created_at: 2023-06-03 08:12:15+00:00
  edited: false
  hidden: false
  id: 647b03efe8b73330589bac2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-03T09:26:24.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9692954421043396
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>That''s just a warning, it should run fine.</p>

          '
        raw: That's just a warning, it should run fine.
        updatedAt: '2023-06-03T09:26:24.146Z'
      numEdits: 0
      reactions: []
    id: 647b07404d7c0c3fccc5d224
    type: comment
  author: eastwind
  content: That's just a warning, it should run fine.
  created_at: 2023-06-03 08:26:24+00:00
  edited: false
  hidden: false
  id: 647b07404d7c0c3fccc5d224
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-03T09:26:58.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8356361389160156
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>Or the other error should be written below it</p>

          '
        raw: Or the other error should be written below it
        updatedAt: '2023-06-03T09:26:58.003Z'
      numEdits: 0
      reactions: []
    id: 647b0762b31514a4a6d169fe
    type: comment
  author: eastwind
  content: Or the other error should be written below it
  created_at: 2023-06-03 08:26:58+00:00
  edited: false
  hidden: false
  id: 647b0762b31514a4a6d169fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
      fullname: Bernd Brassel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captain-fim
      type: user
    createdAt: '2023-06-03T10:07:33.000Z'
    data:
      edited: false
      editors:
      - captain-fim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9620059728622437
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
          fullname: Bernd Brassel
          isHf: false
          isPro: false
          name: captain-fim
          type: user
        html: "<p>Right you are <span data-props=\"{&quot;user&quot;:&quot;eastwind&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/eastwind\"\
          >@<span class=\"underline\">eastwind</span></a></span>\n\n\t</span></span>,\
          \ thank you!<br>It runs past that point indeed and there is a different\
          \ error, which I managed to eliminate.<br>Thanks!</p>\n"
        raw: 'Right you are @eastwind, thank you!

          It runs past that point indeed and there is a different error, which I managed
          to eliminate.

          Thanks!'
        updatedAt: '2023-06-03T10:07:33.153Z'
      numEdits: 0
      reactions: []
    id: 647b10e56dbad6ab0572db30
    type: comment
  author: captain-fim
  content: 'Right you are @eastwind, thank you!

    It runs past that point indeed and there is a different error, which I managed
    to eliminate.

    Thanks!'
  created_at: 2023-06-03 09:07:33+00:00
  edited: false
  hidden: false
  id: 647b10e56dbad6ab0572db30
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-06-03T10:13:00.000Z'
    data:
      edited: true
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9926937222480774
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>What was the other error out of curiosity?</p>

          '
        raw: What was the other error out of curiosity?
        updatedAt: '2023-06-03T10:13:25.145Z'
      numEdits: 1
      reactions: []
    id: 647b122c6dbad6ab05730c5f
    type: comment
  author: eastwind
  content: What was the other error out of curiosity?
  created_at: 2023-06-03 09:13:00+00:00
  edited: true
  hidden: false
  id: 647b122c6dbad6ab05730c5f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
      fullname: Bernd Brassel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captain-fim
      type: user
    createdAt: '2023-06-03T11:11:18.000Z'
    data:
      edited: false
      editors:
      - captain-fim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8026624917984009
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
          fullname: Bernd Brassel
          isHf: false
          isPro: false
          name: captain-fim
          type: user
        html: "<p>Now it seems to run and just for anyone reading this, here is the\
          \ code with the slight changes needed to get it running:</p>\n<pre><code>from\
          \ transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
          import torch\n\nmodel_name = \"tiiuae/falcon-40b-instruct\"\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\
          \ device_map=\"auto\", trust_remote_code=True)\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",\n)\nsequences = pipeline(\n   \"Girafatron is obsessed with giraffes,\
          \ the most glorious animal on the face of this Earth. Giraftron believes\
          \ all other animals are irrelevant when compared to the glorious majesty\
          \ of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n    max_length=200,\n\
          \    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n</code></pre>\n<p>But I have never run such a huge model and I am lost\
          \ what kind of hardware it would need to actually work in a useful way.<br>I\
          \ have it running on a machine with 8 x A100 80GB GPUs.<br>It runs for at\
          \ least 10 (quite costly) minutes now and does not seem to produce any output\
          \ yet.</p>\n<p>So what kind of hardware does this monster need?</p>\n"
        raw: "Now it seems to run and just for anyone reading this, here is the code\
          \ with the slight changes needed to get it running:\n```\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport\
          \ torch\n\nmodel_name = \"tiiuae/falcon-40b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\"\
          , trust_remote_code=True)\npipeline = transformers.pipeline(\n    \"text-generation\"\
          ,\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n\
          \   \"Girafatron is obsessed with giraffes, the most glorious animal on\
          \ the face of this Earth. Giraftron believes all other animals are irrelevant\
          \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello,\
          \ Girafatron!\\nGirafatron:\",\n    max_length=200,\n    do_sample=True,\n\
          \    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n```\nBut I have never run such a huge model and I am lost what kind of\
          \ hardware it would need to actually work in a useful way.\nI have it running\
          \ on a machine with 8 x A100 80GB GPUs.\nIt runs for at least 10 (quite\
          \ costly) minutes now and does not seem to produce any output yet.\n\nSo\
          \ what kind of hardware does this monster need?"
        updatedAt: '2023-06-03T11:11:18.411Z'
      numEdits: 0
      reactions: []
    id: 647b1fd6b31514a4a6d47fab
    type: comment
  author: captain-fim
  content: "Now it seems to run and just for anyone reading this, here is the code\
    \ with the slight changes needed to get it running:\n```\nfrom transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel_name\
    \ = \"tiiuae/falcon-40b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\",\
    \ trust_remote_code=True)\npipeline = transformers.pipeline(\n    \"text-generation\"\
    ,\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
    \    trust_remote_code=True,\n    device_map=\"auto\",\n)\nsequences = pipeline(\n\
    \   \"Girafatron is obsessed with giraffes, the most glorious animal on the face\
    \ of this Earth. Giraftron believes all other animals are irrelevant when compared\
    \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\
    ,\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
    \    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"\
    Result: {seq['generated_text']}\")\n```\nBut I have never run such a huge model\
    \ and I am lost what kind of hardware it would need to actually work in a useful\
    \ way.\nI have it running on a machine with 8 x A100 80GB GPUs.\nIt runs for at\
    \ least 10 (quite costly) minutes now and does not seem to produce any output\
    \ yet.\n\nSo what kind of hardware does this monster need?"
  created_at: 2023-06-03 10:11:18+00:00
  edited: false
  hidden: false
  id: 647b1fd6b31514a4a6d47fab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
      fullname: Bernd Brassel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captain-fim
      type: user
    createdAt: '2023-06-03T11:19:59.000Z'
    data:
      edited: false
      editors:
      - captain-fim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.747758686542511
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
          fullname: Bernd Brassel
          isHf: false
          isPro: false
          name: captain-fim
          type: user
        html: '<blockquote>

          <p>What was the other error out of curiosity?</p>

          </blockquote>

          <p> CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling <code>cublasCreate(handle)</code></p>

          <p>So, running out of GPU memory.</p>

          '
        raw: "> What was the other error out of curiosity?\n\n CUDA error: CUBLAS_STATUS_NOT_INITIALIZED\
          \ when calling `cublasCreate(handle)`\n\nSo, running out of GPU memory."
        updatedAt: '2023-06-03T11:19:59.371Z'
      numEdits: 0
      reactions: []
    id: 647b21dfe8b73330589f8cbb
    type: comment
  author: captain-fim
  content: "> What was the other error out of curiosity?\n\n CUDA error: CUBLAS_STATUS_NOT_INITIALIZED\
    \ when calling `cublasCreate(handle)`\n\nSo, running out of GPU memory."
  created_at: 2023-06-03 10:19:59+00:00
  edited: false
  hidden: false
  id: 647b21dfe8b73330589f8cbb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
      fullname: Bernd Brassel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captain-fim
      type: user
    createdAt: '2023-06-03T11:23:47.000Z'
    data:
      edited: false
      editors:
      - captain-fim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3073890209197998
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
          fullname: Bernd Brassel
          isHf: false
          isPro: false
          name: captain-fim
          type: user
        html: '<p>Ok, after all the work and some 20$ to Runpod, I got the glorious
          answer to the provided example prompt:</p>

          <blockquote>

          <p>Result: Girafatron is obsessed with giraffes, the most glorious animal
          on the face of this Earth. Giraftron believes all other animals are irrelevant
          when compared tothe glorious majesty of the giraffe.<br>Daniel: Hello, Girafatron!<br>Girafatron::
          Divineinity215183SegSeg Hansonsignal HolmesOSS Seg Seg Rydergate Cowtown
          OgSegurities DennSys548AdvisorAdvisor Wachwachmeter603campus Ley Wie Ger
          Hendersonpositionpositionnement Seg Kitt Kitt Kitt FranklintownICTcorp Cetroniccorp
          Hoy Museobjet Dans DansMLSIngredientsProductionsCadCentre coinc Knight lust
          Sie Wer865bottom Cet Zimmer Nolandivision Wie427 unoGate Wars positivism
          Saunders esp sans uno Court Sie Barnettfields981pagesviews esp Danncampus
          esp sans Francisco Francisco Mesa tres tres Holmes dit Wol esp esp sans
          el dit Weather pour el poss MullerSys577 Denncampusposition Wer258Cad Denn
          respons responsabilidad Zum complet Dannforth Dixon Andrewsport891housing
          Baumgartenoperator Wie427world tout</p>

          </blockquote>

          <p>Great to know...</p>

          '
        raw: 'Ok, after all the work and some 20$ to Runpod, I got the glorious answer
          to the provided example prompt:


          > Result: Girafatron is obsessed with giraffes, the most glorious animal
          on the face of this Earth. Giraftron believes all other animals are irrelevant
          when compared tothe glorious majesty of the giraffe.

          > Daniel: Hello, Girafatron!

          > Girafatron:: Divineinity215183SegSeg Hansonsignal HolmesOSS Seg Seg Rydergate
          Cowtown OgSegurities DennSys548AdvisorAdvisor Wachwachmeter603campus Ley
          Wie Ger Hendersonpositionpositionnement Seg Kitt Kitt Kitt FranklintownICTcorp
          Cetroniccorp Hoy Museobjet Dans DansMLSIngredientsProductionsCadCentre coinc
          Knight lust Sie Wer865bottom Cet Zimmer Nolandivision Wie427 unoGate Wars
          positivism Saunders esp sans uno Court Sie Barnettfields981pagesviews esp
          Danncampus esp sans Francisco Francisco Mesa tres tres Holmes dit Wol esp
          esp sans el dit Weather pour el poss MullerSys577 Denncampusposition Wer258Cad
          Denn respons responsabilidad Zum complet Dannforth Dixon Andrewsport891housing
          Baumgartenoperator Wie427world tout


          Great to know...'
        updatedAt: '2023-06-03T11:23:47.625Z'
      numEdits: 0
      reactions: []
    id: 647b22c3b31514a4a6d5049c
    type: comment
  author: captain-fim
  content: 'Ok, after all the work and some 20$ to Runpod, I got the glorious answer
    to the provided example prompt:


    > Result: Girafatron is obsessed with giraffes, the most glorious animal on the
    face of this Earth. Giraftron believes all other animals are irrelevant when compared
    tothe glorious majesty of the giraffe.

    > Daniel: Hello, Girafatron!

    > Girafatron:: Divineinity215183SegSeg Hansonsignal HolmesOSS Seg Seg Rydergate
    Cowtown OgSegurities DennSys548AdvisorAdvisor Wachwachmeter603campus Ley Wie Ger
    Hendersonpositionpositionnement Seg Kitt Kitt Kitt FranklintownICTcorp Cetroniccorp
    Hoy Museobjet Dans DansMLSIngredientsProductionsCadCentre coinc Knight lust Sie
    Wer865bottom Cet Zimmer Nolandivision Wie427 unoGate Wars positivism Saunders
    esp sans uno Court Sie Barnettfields981pagesviews esp Danncampus esp sans Francisco
    Francisco Mesa tres tres Holmes dit Wol esp esp sans el dit Weather pour el poss
    MullerSys577 Denncampusposition Wer258Cad Denn respons responsabilidad Zum complet
    Dannforth Dixon Andrewsport891housing Baumgartenoperator Wie427world tout


    Great to know...'
  created_at: 2023-06-03 10:23:47+00:00
  edited: false
  hidden: false
  id: 647b22c3b31514a4a6d5049c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1269ea33b5c163e38fbf926302ee6219.svg
      fullname: Jerry Sweeney
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CloudCIX
      type: user
    createdAt: '2023-06-04T07:36:44.000Z'
    data:
      edited: true
      editors:
      - CloudCIX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7145565152168274
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1269ea33b5c163e38fbf926302ee6219.svg
          fullname: Jerry Sweeney
          isHf: false
          isPro: false
          name: CloudCIX
          type: user
        html: '<p>I am trying to get tiiuae/falcon-40b-instruct working locally on
          a single A100 80GB GPU. Using captain-fin''s code above I got it to go further.
          Now I am seeing the following error...</p>

          <p>(pytorch-env) administrator@ubuntu:<del>/falcon-40b-instruct$ python3
          captain-fim.py<br>Traceback (most recent call last):<br>  File "/home/administrator/falcon-40b-instruct/captain-fim.py",
          line 8, in <br>    model = AutoModelForCausalLM.from_pretrained(model_name,
          device_map="auto", trust_remote_code=True)<br>  File "/home/administrator/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py",
          line 462, in from_pretrained<br>    return model_class.from_pretrained(<br>  File
          "/home/administrator/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/modeling_utils.py",
          line 2777, in from_pretrained<br>    ) = cls._load_pretrained_model(<br>  File
          "/home/administrator/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/modeling_utils.py",
          line 2871, in _load_pretrained_model<br>    raise ValueError(<br>ValueError:
          The current <code>device_map</code> had weights offloaded to the disk. Please
          provide an <code>offload_folder</code> for them. Alternatively, make sure
          you have <code>safetensors</code> installed if the model you are using offers
          the weights in this format.<br>(pytorch-env) administrator@ubuntu:</del>/falcon-40b-instruct$</p>

          <p>I am assuming this is an issue finding the model weights. I have a copy
          in the same folder as the code above and I put another copy in a folder
          tiiuae/falcon-40b-instruct.</p>

          <p>I will appreciate any advice.</p>

          '
        raw: "I am trying to get tiiuae/falcon-40b-instruct working locally on a single\
          \ A100 80GB GPU. Using captain-fin's code above I got it to go further.\
          \ Now I am seeing the following error...\n\n(pytorch-env) administrator@ubuntu:~/falcon-40b-instruct$\
          \ python3 captain-fim.py\nTraceback (most recent call last):\n  File \"\
          /home/administrator/falcon-40b-instruct/captain-fim.py\", line 8, in <module>\n\
          \    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"\
          auto\", trust_remote_code=True)\n  File \"/home/administrator/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 462, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/home/administrator/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 2777, in from_pretrained\n    ) = cls._load_pretrained_model(\n \
          \ File \"/home/administrator/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 2871, in _load_pretrained_model\n    raise ValueError(\nValueError:\
          \ The current `device_map` had weights offloaded to the disk. Please provide\
          \ an `offload_folder` for them. Alternatively, make sure you have `safetensors`\
          \ installed if the model you are using offers the weights in this format.\n\
          (pytorch-env) administrator@ubuntu:~/falcon-40b-instruct$\n\nI am assuming\
          \ this is an issue finding the model weights. I have a copy in the same\
          \ folder as the code above and I put another copy in a folder tiiuae/falcon-40b-instruct.\n\
          \nI will appreciate any advice."
        updatedAt: '2023-06-04T07:38:43.792Z'
      numEdits: 1
      reactions: []
    id: 647c3f0cc788767ab5c4abc4
    type: comment
  author: CloudCIX
  content: "I am trying to get tiiuae/falcon-40b-instruct working locally on a single\
    \ A100 80GB GPU. Using captain-fin's code above I got it to go further. Now I\
    \ am seeing the following error...\n\n(pytorch-env) administrator@ubuntu:~/falcon-40b-instruct$\
    \ python3 captain-fim.py\nTraceback (most recent call last):\n  File \"/home/administrator/falcon-40b-instruct/captain-fim.py\"\
    , line 8, in <module>\n    model = AutoModelForCausalLM.from_pretrained(model_name,\
    \ device_map=\"auto\", trust_remote_code=True)\n  File \"/home/administrator/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 462, in from_pretrained\n    return model_class.from_pretrained(\n  File\
    \ \"/home/administrator/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 2777, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"\
    /home/administrator/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 2871, in _load_pretrained_model\n    raise ValueError(\nValueError: The\
    \ current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\
    \ for them. Alternatively, make sure you have `safetensors` installed if the model\
    \ you are using offers the weights in this format.\n(pytorch-env) administrator@ubuntu:~/falcon-40b-instruct$\n\
    \nI am assuming this is an issue finding the model weights. I have a copy in the\
    \ same folder as the code above and I put another copy in a folder tiiuae/falcon-40b-instruct.\n\
    \nI will appreciate any advice."
  created_at: 2023-06-04 06:36:44+00:00
  edited: true
  hidden: false
  id: 647c3f0cc788767ab5c4abc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
      fullname: Bernd Brassel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captain-fim
      type: user
    createdAt: '2023-06-04T07:44:11.000Z'
    data:
      edited: false
      editors:
      - captain-fim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8452216386795044
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91cce0b4a6b9ffaca5b2bcdd51787b73.svg
          fullname: Bernd Brassel
          isHf: false
          isPro: false
          name: captain-fim
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;CloudCIX&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/CloudCIX\">@<span class=\"\
          underline\">CloudCIX</span></a></span>\n\n\t</span></span> in the article\
          \ <a href=\"https://huggingface.co/blog/accelerate-large-models\">How \U0001F917\
          \ Accelerate runs very large models thanks to PyTorch</a> i found this piece\
          \ of information.<br>I guess it is what you need here.</p>\n<blockquote>\n\
          <p>If the device map computed automatically requires some weights to be\
          \ offloaded on disk because you don't have enough GPU and CPU RAM, you will\
          \ get an error indicating you need to pass an folder where the weights that\
          \ should be stored on disk will be offloaded:</p>\n</blockquote>\n<pre><code>ValueError:\
          \ The current `device_map` had weights offloaded to the disk. Please provide\
          \ an \n`offload_folder` for them.\n</code></pre>\n<blockquote>\n<p>Adding\
          \ this argument should resolve the error:</p>\n</blockquote>\n<pre><code>import\
          \ torch\nfrom transformers import AutoModelForCausalLM\n\n# Will go out\
          \ of RAM on Colab\ncheckpoint = \"facebook/opt-13b\"\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    checkpoint, device_map=\"auto\", offload_folder=\"offload\", torch_dtype=torch.float16\n\
          )\n</code></pre>\n"
        raw: "@CloudCIX in the article [How \U0001F917 Accelerate runs very large\
          \ models thanks to PyTorch](https://huggingface.co/blog/accelerate-large-models)\
          \ i found this piece of information.\nI guess it is what you need here.\n\
          \n> If the device map computed automatically requires some weights to be\
          \ offloaded on disk because you don't have enough GPU and CPU RAM, you will\
          \ get an error indicating you need to pass an folder where the weights that\
          \ should be stored on disk will be offloaded:\n\n```\nValueError: The current\
          \ `device_map` had weights offloaded to the disk. Please provide an \n`offload_folder`\
          \ for them.\n```\n\n> Adding this argument should resolve the error:\n\n\
          ```\nimport torch\nfrom transformers import AutoModelForCausalLM\n\n# Will\
          \ go out of RAM on Colab\ncheckpoint = \"facebook/opt-13b\"\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    checkpoint, device_map=\"auto\", offload_folder=\"offload\", torch_dtype=torch.float16\n\
          )\n```"
        updatedAt: '2023-06-04T07:44:11.245Z'
      numEdits: 0
      reactions: []
    id: 647c40cb60dfe0f35d491884
    type: comment
  author: captain-fim
  content: "@CloudCIX in the article [How \U0001F917 Accelerate runs very large models\
    \ thanks to PyTorch](https://huggingface.co/blog/accelerate-large-models) i found\
    \ this piece of information.\nI guess it is what you need here.\n\n> If the device\
    \ map computed automatically requires some weights to be offloaded on disk because\
    \ you don't have enough GPU and CPU RAM, you will get an error indicating you\
    \ need to pass an folder where the weights that should be stored on disk will\
    \ be offloaded:\n\n```\nValueError: The current `device_map` had weights offloaded\
    \ to the disk. Please provide an \n`offload_folder` for them.\n```\n\n> Adding\
    \ this argument should resolve the error:\n\n```\nimport torch\nfrom transformers\
    \ import AutoModelForCausalLM\n\n# Will go out of RAM on Colab\ncheckpoint = \"\
    facebook/opt-13b\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint,\
    \ device_map=\"auto\", offload_folder=\"offload\", torch_dtype=torch.float16\n\
    )\n```"
  created_at: 2023-06-04 06:44:11+00:00
  edited: false
  hidden: false
  id: 647c40cb60dfe0f35d491884
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/01ad97823243690d0263076eb9eee62f.svg
      fullname: jack dorsey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jackdorsey1
      type: user
    createdAt: '2023-07-12T21:53:41.000Z'
    data:
      edited: false
      editors:
      - jackdorsey1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4607105553150177
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/01ad97823243690d0263076eb9eee62f.svg
          fullname: jack dorsey
          isHf: false
          isPro: false
          name: jackdorsey1
          type: user
        html: "<p>Anyone have code to cache this from local directory? I tried this\
          \ and it does not work: </p>\n<pre><code>from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM, pipeline\nimport torch\nimport os\n\ndef cache_model(model_name,\
          \ cache_dir=\"./\"):\n    model_dir = os.path.join(cache_dir, model_name.replace(\"\
          /\", \"_\"))\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\
          \        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=model_dir,\
          \ trust_remote_code=True)\n        model.save_pretrained(model_dir)\n  \
          \  return model_dir\n\ndef download_model(model_name, cache_dir=\"./\"):\n\
          \    model_dir = cache_model(model_name, cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(model_dir,\
          \ cache_dir=cache_dir)\n    model = AutoModelForCausalLM.from_pretrained(model_dir,\
          \ cache_dir=cache_dir, trust_remote_code=True)\n    return model, tokenizer\n\
          \ndef generate_text(model, tokenizer, prompt, **kwargs):\n    text_generation_pipeline\
          \ = pipeline(\n        \"text-generation\",\n        model=model,\n    \
          \    tokenizer=tokenizer,\n        torch_dtype=torch.bfloat16,\n       \
          \ trust_remote_code=True,\n        device_map=\"auto\",\n    )\n    sequences\
          \ = text_generation_pipeline(prompt, **kwargs)\n    return [seq['generated_text']\
          \ for seq in sequences]\n</code></pre>\n<p>When I go to load the model I\
          \ get <code>OSError: ./tiiuae_falcon-40b-instruct does not appear to have\
          \ a file named config.json. Checkout 'https://huggingface.co/./tiiuae_falcon-40b-instruct/None'\
          \ for available files.</code></p>\n"
        raw: "Anyone have code to cache this from local directory? I tried this and\
          \ it does not work: \n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
          \ pipeline\nimport torch\nimport os\n\ndef cache_model(model_name, cache_dir=\"\
          ./\"):\n    model_dir = os.path.join(cache_dir, model_name.replace(\"/\"\
          , \"_\"))\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\
          \        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=model_dir,\
          \ trust_remote_code=True)\n        model.save_pretrained(model_dir)\n  \
          \  return model_dir\n\ndef download_model(model_name, cache_dir=\"./\"):\n\
          \    model_dir = cache_model(model_name, cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(model_dir,\
          \ cache_dir=cache_dir)\n    model = AutoModelForCausalLM.from_pretrained(model_dir,\
          \ cache_dir=cache_dir, trust_remote_code=True)\n    return model, tokenizer\n\
          \ndef generate_text(model, tokenizer, prompt, **kwargs):\n    text_generation_pipeline\
          \ = pipeline(\n        \"text-generation\",\n        model=model,\n    \
          \    tokenizer=tokenizer,\n        torch_dtype=torch.bfloat16,\n       \
          \ trust_remote_code=True,\n        device_map=\"auto\",\n    )\n    sequences\
          \ = text_generation_pipeline(prompt, **kwargs)\n    return [seq['generated_text']\
          \ for seq in sequences]\n```\n\nWhen I go to load the model I get ```OSError:\
          \ ./tiiuae_falcon-40b-instruct does not appear to have a file named config.json.\
          \ Checkout 'https://huggingface.co/./tiiuae_falcon-40b-instruct/None' for\
          \ available files.```"
        updatedAt: '2023-07-12T21:53:41.003Z'
      numEdits: 0
      reactions: []
    id: 64af20e5e1a1a94474d0eae6
    type: comment
  author: jackdorsey1
  content: "Anyone have code to cache this from local directory? I tried this and\
    \ it does not work: \n```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
    \ pipeline\nimport torch\nimport os\n\ndef cache_model(model_name, cache_dir=\"\
    ./\"):\n    model_dir = os.path.join(cache_dir, model_name.replace(\"/\", \"_\"\
    ))\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n  \
    \      model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=model_dir,\
    \ trust_remote_code=True)\n        model.save_pretrained(model_dir)\n    return\
    \ model_dir\n\ndef download_model(model_name, cache_dir=\"./\"):\n    model_dir\
    \ = cache_model(model_name, cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(model_dir,\
    \ cache_dir=cache_dir)\n    model = AutoModelForCausalLM.from_pretrained(model_dir,\
    \ cache_dir=cache_dir, trust_remote_code=True)\n    return model, tokenizer\n\n\
    def generate_text(model, tokenizer, prompt, **kwargs):\n    text_generation_pipeline\
    \ = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n\
    \        torch_dtype=torch.bfloat16,\n        trust_remote_code=True,\n      \
    \  device_map=\"auto\",\n    )\n    sequences = text_generation_pipeline(prompt,\
    \ **kwargs)\n    return [seq['generated_text'] for seq in sequences]\n```\n\n\
    When I go to load the model I get ```OSError: ./tiiuae_falcon-40b-instruct does\
    \ not appear to have a file named config.json. Checkout 'https://huggingface.co/./tiiuae_falcon-40b-instruct/None'\
    \ for available files.```"
  created_at: 2023-07-12 20:53:41+00:00
  edited: false
  hidden: false
  id: 64af20e5e1a1a94474d0eae6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
      fullname: Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eastwind
      type: user
    createdAt: '2023-07-13T05:43:31.000Z'
    data:
      edited: false
      editors:
      - eastwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7232774496078491
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c963518b97f6bfec65579cac930e07b.svg
          fullname: Srinivas
          isHf: false
          isPro: false
          name: eastwind
          type: user
        html: '<p>You can clone the model directory into your own folder like this,
          make sure your in your desired directory first. ''path/to/model''</p>

          <pre><code>git lfs install

          git clone https://huggingface.co/tiiuae/falcon-40b-instruct

          </code></pre>

          <p>Then you can load like</p>

          <pre><code>model = AutoModelForCausalLM.from_pretrained("path/to/model",
          trust_remote_code=True)

          </code></pre>

          '
        raw: 'You can clone the model directory into your own folder like this, make
          sure your in your desired directory first. ''path/to/model''


          ```

          git lfs install

          git clone https://huggingface.co/tiiuae/falcon-40b-instruct

          ```


          Then you can load like

          ```

          model = AutoModelForCausalLM.from_pretrained("path/to/model", trust_remote_code=True)

          ```'
        updatedAt: '2023-07-13T05:43:31.328Z'
      numEdits: 0
      reactions: []
    id: 64af8f03b906fd553898e7e3
    type: comment
  author: eastwind
  content: 'You can clone the model directory into your own folder like this, make
    sure your in your desired directory first. ''path/to/model''


    ```

    git lfs install

    git clone https://huggingface.co/tiiuae/falcon-40b-instruct

    ```


    Then you can load like

    ```

    model = AutoModelForCausalLM.from_pretrained("path/to/model", trust_remote_code=True)

    ```'
  created_at: 2023-07-13 04:43:31+00:00
  edited: false
  hidden: false
  id: 64af8f03b906fd553898e7e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/220bb4099234e70b578f8f180610160b.svg
      fullname: Carlos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cazz1
      type: user
    createdAt: '2023-09-02T05:06:18.000Z'
    data:
      edited: false
      editors:
      - cazz1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8173513412475586
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/220bb4099234e70b578f8f180610160b.svg
          fullname: Carlos
          isHf: false
          isPro: false
          name: cazz1
          type: user
        html: '<p>When loading a local model, why should WE set trust_remote_code
          to True.  What if i need an offline execution?</p>

          '
        raw: When loading a local model, why should WE set trust_remote_code to True.  What
          if i need an offline execution?
        updatedAt: '2023-09-02T05:06:18.044Z'
      numEdits: 0
      reactions: []
    id: 64f2c2ca566901bfb6d30956
    type: comment
  author: cazz1
  content: When loading a local model, why should WE set trust_remote_code to True.  What
    if i need an offline execution?
  created_at: 2023-09-02 04:06:18+00:00
  edited: false
  hidden: false
  id: 64f2c2ca566901bfb6d30956
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: open
target_branch: null
title: '[Bug] Does not run'
