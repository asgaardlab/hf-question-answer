!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nx
conflicting_files: null
created_at: 2023-06-01 06:25:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/868624d0a715ccf038eb7b4904b4c060.svg
      fullname: Shawn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nx
      type: user
    createdAt: '2023-06-01T07:25:16.000Z'
    data:
      edited: false
      editors:
      - nx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/868624d0a715ccf038eb7b4904b4c060.svg
          fullname: Shawn
          isHf: false
          isPro: false
          name: nx
          type: user
        html: "<p>I've got 3 x V100, are they enough to load the model?</p>\n<p>The\
          \ current code will cause out-of-memory issue:</p>\n<pre><code>from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport\
          \ torch\n\nmodel = \"tiiuae/falcon-40b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=\"auto\",\n)\nsequences = pipeline(\n   \"Girafatron is\
          \ obsessed with giraffes, the most glorious animal on the face of this Earth.\
          \ Giraftron believes all other animals are irrelevant when compared to the\
          \ glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\
          ,\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
          \    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n  \
          \  print(f\"Result: {seq['generated_text']}\")\n</code></pre>\n"
        raw: "I've got 3 x V100, are they enough to load the model?\r\n\r\nThe current\
          \ code will cause out-of-memory issue:\r\n\r\n```\r\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\r\nimport transformers\r\nimport torch\r\
          \n\r\nmodel = \"tiiuae/falcon-40b-instruct\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\
          \npipeline = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
          \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\
          \n    device_map=\"auto\",\r\n)\r\nsequences = pipeline(\r\n   \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",\r\n    max_length=200,\r\n    do_sample=True,\r\n    top_k=10,\r\
          \n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
          \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
          )\r\n```"
        updatedAt: '2023-06-01T07:25:16.227Z'
      numEdits: 0
      reactions: []
    id: 647847dcc43296134a42d591
    type: comment
  author: nx
  content: "I've got 3 x V100, are they enough to load the model?\r\n\r\nThe current\
    \ code will cause out-of-memory issue:\r\n\r\n```\r\nfrom transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\r\nimport transformers\r\nimport torch\r\
    \n\r\nmodel = \"tiiuae/falcon-40b-instruct\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\
    \npipeline = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
    \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\
    \n    device_map=\"auto\",\r\n)\r\nsequences = pipeline(\r\n   \"Girafatron is\
    \ obsessed with giraffes, the most glorious animal on the face of this Earth.\
    \ Giraftron believes all other animals are irrelevant when compared to the glorious\
    \ majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\r\n  \
    \  max_length=200,\r\n    do_sample=True,\r\n    top_k=10,\r\n    num_return_sequences=1,\r\
    \n    eos_token_id=tokenizer.eos_token_id,\r\n)\r\nfor seq in sequences:\r\n \
    \   print(f\"Result: {seq['generated_text']}\")\r\n```"
  created_at: 2023-06-01 06:25:16+00:00
  edited: false
  hidden: false
  id: 647847dcc43296134a42d591
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/868624d0a715ccf038eb7b4904b4c060.svg
      fullname: Shawn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nx
      type: user
    createdAt: '2023-06-01T07:26:03.000Z'
    data:
      edited: false
      editors:
      - nx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/868624d0a715ccf038eb7b4904b4c060.svg
          fullname: Shawn
          isHf: false
          isPro: false
          name: nx
          type: user
        html: "<p>ImportError: Using <code>low_cpu_mem_usage=True</code> or a <code>device_map</code>\
          \ requires Accelerate: <code>pip install accelerate</code></p>\n<p>on this\
          \ part</p>\n<pre><code>pipeline = transformers.pipeline(\n    \"text-generation\"\
          ,\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\",\n)\n</code></pre>\n"
        raw: "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires\
          \ Accelerate: `pip install accelerate`\n\non this part\n```\npipeline =\
          \ transformers.pipeline(\n    \"text-generation\",\n    model=model,\n \
          \   tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=\"auto\",\n)\n```"
        updatedAt: '2023-06-01T07:26:03.264Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - thiner
    id: 6478480b159a889d001db5a2
    type: comment
  author: nx
  content: "ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires\
    \ Accelerate: `pip install accelerate`\n\non this part\n```\npipeline = transformers.pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
    \    trust_remote_code=True,\n    device_map=\"auto\",\n)\n```"
  created_at: 2023-06-01 06:26:03+00:00
  edited: false
  hidden: false
  id: 6478480b159a889d001db5a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669642878663-62d13bb5b473530ce564631b.png?w=200&h=200&f=face
      fullname: Mariusz Woloszyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kil3r
      type: user
    createdAt: '2023-06-01T21:13:15.000Z'
    data:
      edited: true
      editors:
      - kil3r
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669642878663-62d13bb5b473530ce564631b.png?w=200&h=200&f=face
          fullname: Mariusz Woloszyn
          isHf: false
          isPro: false
          name: kil3r
          type: user
        html: '<p>in bfloat16 it takes ~65GB of VRAM on A1000 80GB, in 8bit ~46GB</p>

          '
        raw: in bfloat16 it takes ~65GB of VRAM on A1000 80GB, in 8bit ~46GB
        updatedAt: '2023-06-02T14:00:46.073Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nx
    id: 647909eb25e06d2ffe8df6ea
    type: comment
  author: kil3r
  content: in bfloat16 it takes ~65GB of VRAM on A1000 80GB, in 8bit ~46GB
  created_at: 2023-06-01 20:13:15+00:00
  edited: true
  hidden: false
  id: 647909eb25e06d2ffe8df6ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/868624d0a715ccf038eb7b4904b4c060.svg
      fullname: Shawn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nx
      type: user
    createdAt: '2023-06-02T21:15:19.000Z'
    data:
      edited: false
      editors:
      - nx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9326368570327759
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/868624d0a715ccf038eb7b4904b4c060.svg
          fullname: Shawn
          isHf: false
          isPro: false
          name: nx
          type: user
        html: "<p>I don\u2019t have any A1000, but 3x V100 (32g each). Is there some\
          \ sample script that I can use to run the model with all 3 cards?</p>\n"
        raw: "I don\u2019t have any A1000, but 3x V100 (32g each). Is there some sample\
          \ script that I can use to run the model with all 3 cards?"
        updatedAt: '2023-06-02T21:15:19.436Z'
      numEdits: 0
      reactions: []
    id: 647a5be744b6a3ae9d24311a
    type: comment
  author: nx
  content: "I don\u2019t have any A1000, but 3x V100 (32g each). Is there some sample\
    \ script that I can use to run the model with all 3 cards?"
  created_at: 2023-06-02 20:15:19+00:00
  edited: false
  hidden: false
  id: 647a5be744b6a3ae9d24311a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9bb5c6dd63b0ae6f787e00b0844cf03e.svg
      fullname: cheung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leocheung
      type: user
    createdAt: '2023-06-06T03:41:32.000Z'
    data:
      edited: false
      editors:
      - leocheung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6094381809234619
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9bb5c6dd63b0ae6f787e00b0844cf03e.svg
          fullname: cheung
          isHf: false
          isPro: false
          name: leocheung
          type: user
        html: '<blockquote>

          <p>in bfloat16 it takes ~65GB of VRAM on A1000 80GB, in 8bit ~46GB</p>

          </blockquote>

          <p>why i use a100-80GB ,  still report cuda memory error?<br>  File "/root/.cache/huggingface/modules/transformers_modules/falcon-40b/modelling_RW.py",
          line 93, in forward<br>    return (q * cos) + (rotate_half(q) * sin), (k
          * cos) + (rotate_half(k) * sin)<br>torch.cuda.OutOfMemoryError: CUDA out
          of memory. Tried to allocate 2.00 MiB (GPU 0; 79.35 GiB total capacity;
          77.18 GiB already allocated; 3.19 MiB free; 78.17 GiB reserved in total
          by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF</p>

          '
        raw: "> in bfloat16 it takes ~65GB of VRAM on A1000 80GB, in 8bit ~46GB\n\n\
          why i use a100-80GB ,  still report cuda memory error?\n  File \"/root/.cache/huggingface/modules/transformers_modules/falcon-40b/modelling_RW.py\"\
          , line 93, in forward\n    return (q * cos) + (rotate_half(q) * sin), (k\
          \ * cos) + (rotate_half(k) * sin)\ntorch.cuda.OutOfMemoryError: CUDA out\
          \ of memory. Tried to allocate 2.00 MiB (GPU 0; 79.35 GiB total capacity;\
          \ 77.18 GiB already allocated; 3.19 MiB free; 78.17 GiB reserved in total\
          \ by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
        updatedAt: '2023-06-06T03:41:32.493Z'
      numEdits: 0
      reactions: []
    id: 647eaaec5214d172cbcfcb15
    type: comment
  author: leocheung
  content: "> in bfloat16 it takes ~65GB of VRAM on A1000 80GB, in 8bit ~46GB\n\n\
    why i use a100-80GB ,  still report cuda memory error?\n  File \"/root/.cache/huggingface/modules/transformers_modules/falcon-40b/modelling_RW.py\"\
    , line 93, in forward\n    return (q * cos) + (rotate_half(q) * sin), (k * cos)\
    \ + (rotate_half(k) * sin)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried\
    \ to allocate 2.00 MiB (GPU 0; 79.35 GiB total capacity; 77.18 GiB already allocated;\
    \ 3.19 MiB free; 78.17 GiB reserved in total by PyTorch) If reserved memory is\
    \ >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See\
    \ documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
  created_at: 2023-06-06 02:41:32+00:00
  edited: false
  hidden: false
  id: 647eaaec5214d172cbcfcb15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
      fullname: a749734
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: a749734
      type: user
    createdAt: '2023-06-07T10:16:35.000Z'
    data:
      edited: false
      editors:
      - a749734
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44357943534851074
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76f92b41f85d3a901911f962660830ed.svg
          fullname: a749734
          isHf: false
          isPro: false
          name: a749734
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;leocheung&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/leocheung\">@<span class=\"\
          underline\">leocheung</span></a></span>\n\n\t</span></span>  same i hv 96GB\
          \ still same error , any solution u got?</p>\n"
        raw: '@leocheung  same i hv 96GB still same error , any solution u got?'
        updatedAt: '2023-06-07T10:16:35.630Z'
      numEdits: 0
      reactions: []
    id: 64805903e1421e205fd4d149
    type: comment
  author: a749734
  content: '@leocheung  same i hv 96GB still same error , any solution u got?'
  created_at: 2023-06-07 09:16:35+00:00
  edited: false
  hidden: false
  id: 64805903e1421e205fd4d149
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9bb5c6dd63b0ae6f787e00b0844cf03e.svg
      fullname: cheung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leocheung
      type: user
    createdAt: '2023-06-08T03:33:52.000Z'
    data:
      edited: true
      editors:
      - leocheung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8757580518722534
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9bb5c6dd63b0ae6f787e00b0844cf03e.svg
          fullname: cheung
          isHf: false
          isPro: false
          name: leocheung
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;a749734&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/a749734\">@<span class=\"\
          underline\">a749734</span></a></span>\n\n\t</span></span>  i got suggestion\
          \ from <span data-props=\"{&quot;user&quot;:&quot;masonbraysx&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/masonbraysx\">@<span\
          \ class=\"underline\">masonbraysx</span></a></span>\n\n\t</span></span><br>A\
          \ 40-B parameter model will not fit on and A100-80GB if it is in bf16 or\
          \ fp16. In 16-bit precision the amount of VRAM needed to run a given model\
          \ is at least 2GB per 1B parameters, and some models are closer to 3GB per\
          \ 1B parameters. This does not include the amount of memory needed to actually\
          \ run any type of inferencing. Two easy options: 1) run it on a node with\
          \ multiple A100 80GB GPUs. 2) load the model in 8bit precision. This requires\
          \ the package \"bitsandbytes\". This reduces the necessary VRAM to about\
          \ 45GB. I have successfully loaded and performed inference with the falcon-40b-instruct\
          \ model on a system with 4 A4500's (each GPU has 20GB VRAM) using this method.</p>\n"
        raw: "@a749734  i got suggestion from @masonbraysx \nA 40-B parameter model\
          \ will not fit on and A100-80GB if it is in bf16 or fp16. In 16-bit precision\
          \ the amount of VRAM needed to run a given model is at least 2GB per 1B\
          \ parameters, and some models are closer to 3GB per 1B parameters. This\
          \ does not include the amount of memory needed to actually run any type\
          \ of inferencing. Two easy options: 1) run it on a node with multiple A100\
          \ 80GB GPUs. 2) load the model in 8bit precision. This requires the package\
          \ \"bitsandbytes\". This reduces the necessary VRAM to about 45GB. I have\
          \ successfully loaded and performed inference with the falcon-40b-instruct\
          \ model on a system with 4 A4500's (each GPU has 20GB VRAM) using this method."
        updatedAt: '2023-06-08T03:34:13.389Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - HazemMonir
        - prasantapanja
        - usernametba
    id: 64814c2040facadc557a0595
    type: comment
  author: leocheung
  content: "@a749734  i got suggestion from @masonbraysx \nA 40-B parameter model\
    \ will not fit on and A100-80GB if it is in bf16 or fp16. In 16-bit precision\
    \ the amount of VRAM needed to run a given model is at least 2GB per 1B parameters,\
    \ and some models are closer to 3GB per 1B parameters. This does not include the\
    \ amount of memory needed to actually run any type of inferencing. Two easy options:\
    \ 1) run it on a node with multiple A100 80GB GPUs. 2) load the model in 8bit\
    \ precision. This requires the package \"bitsandbytes\". This reduces the necessary\
    \ VRAM to about 45GB. I have successfully loaded and performed inference with\
    \ the falcon-40b-instruct model on a system with 4 A4500's (each GPU has 20GB\
    \ VRAM) using this method."
  created_at: 2023-06-08 02:33:52+00:00
  edited: true
  hidden: false
  id: 64814c2040facadc557a0595
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b54907c6a1ea90a1242f26e03e117af.svg
      fullname: Ayush Agrawal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ayush1801
      type: user
    createdAt: '2023-06-09T07:26:55.000Z'
    data:
      edited: false
      editors:
      - ayush1801
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8763113617897034
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b54907c6a1ea90a1242f26e03e117af.svg
          fullname: Ayush Agrawal
          isHf: false
          isPro: false
          name: ayush1801
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;leocheung&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/leocheung\">@<span class=\"\
          underline\">leocheung</span></a></span>\n\n\t</span></span>  how did you\
          \ run the inference part of the model on the cluster of GPUs. Did you change\
          \ the model's scripts or passed some arguments for reading the devices?</p>\n"
        raw: '@leocheung  how did you run the inference part of the model on the cluster
          of GPUs. Did you change the model''s scripts or passed some arguments for
          reading the devices?'
        updatedAt: '2023-06-09T07:26:55.888Z'
      numEdits: 0
      reactions: []
    id: 6482d43f9184b579bac98402
    type: comment
  author: ayush1801
  content: '@leocheung  how did you run the inference part of the model on the cluster
    of GPUs. Did you change the model''s scripts or passed some arguments for reading
    the devices?'
  created_at: 2023-06-09 06:26:55+00:00
  edited: false
  hidden: false
  id: 6482d43f9184b579bac98402
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: tiiuae/falcon-40b-instruct
repo_type: model
status: open
target_branch: null
title: What is the required GPU memory to load the model?
