!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Youssefmahboub
conflicting_files: null
created_at: 2023-09-06 00:42:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d344c8e04d4ee1860564cb4de384342.svg
      fullname: Youssef mahboub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Youssefmahboub
      type: user
    createdAt: '2023-09-06T01:42:09.000Z'
    data:
      edited: false
      editors:
      - Youssefmahboub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8629465103149414
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d344c8e04d4ee1860564cb4de384342.svg
          fullname: Youssef mahboub
          isHf: false
          isPro: false
          name: Youssefmahboub
          type: user
        html: '<p>I hope you show us some python code example on how to use this model
          locally<br>thank you</p>

          '
        raw: "I hope you show us some python code example on how to use this model\
          \ locally\r\nthank you"
        updatedAt: '2023-09-06T01:42:09.191Z'
      numEdits: 0
      reactions: []
    id: 64f7d8f1766ff9f3d28ad192
    type: comment
  author: Youssefmahboub
  content: "I hope you show us some python code example on how to use this model locally\r\
    \nthank you"
  created_at: 2023-09-06 00:42:09+00:00
  edited: false
  hidden: false
  id: 64f7d8f1766ff9f3d28ad192
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg?w=200&h=200&f=face
      fullname: Zhang Peiyuan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: PY007
      type: user
    createdAt: '2023-09-06T02:31:21.000Z'
    data:
      edited: false
      editors:
      - PY007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7521238327026367
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg?w=200&h=200&f=face
          fullname: Zhang Peiyuan
          isHf: false
          isPro: false
          name: PY007
          type: user
        html: "<p>This is an intermediate checkpoint that has not undergone full training/learning\
          \ rate cool-down. It is not intended for usage yet. But if you really want\
          \ to try:</p>\n<pre><code>from transformers import AutoTokenizer\nimport\
          \ transformers \nimport torch\nmodel = \"PY007/TinyLlama-1.1B-step-50K-105b\"\
          \ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n\
          \    device_map=\"auto\",\n)\n\nsequences = pipeline(\n    'The TinyLlama\
          \ project aims to pretrain a 1.1B Llama model on 3 trillion tokens. With\
          \ some proper optimization, we can achieve this within a span of \"just\"\
          \ 90 days using 16 A100-40G GPUs \U0001F680\U0001F680. The training has\
          \ started on 2023-09-01.',\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
          \    repetition_penalty=1.5,\n    eos_token_id=tokenizer.eos_token_id,\n\
          \    max_length=500,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n</code></pre>\n"
        raw: "This is an intermediate checkpoint that has not undergone full training/learning\
          \ rate cool-down. It is not intended for usage yet. But if you really want\
          \ to try:\n```\nfrom transformers import AutoTokenizer\nimport transformers\
          \ \nimport torch\nmodel = \"PY007/TinyLlama-1.1B-step-50K-105b\"\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n\
          \    device_map=\"auto\",\n)\n\nsequences = pipeline(\n    'The TinyLlama\
          \ project aims to pretrain a 1.1B Llama model on 3 trillion tokens. With\
          \ some proper optimization, we can achieve this within a span of \"just\"\
          \ 90 days using 16 A100-40G GPUs \U0001F680\U0001F680. The training has\
          \ started on 2023-09-01.',\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
          \    repetition_penalty=1.5,\n    eos_token_id=tokenizer.eos_token_id,\n\
          \    max_length=500,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n```\n"
        updatedAt: '2023-09-06T02:31:21.790Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - marutichintan
    id: 64f7e479c29cdd96943f9e3d
    type: comment
  author: PY007
  content: "This is an intermediate checkpoint that has not undergone full training/learning\
    \ rate cool-down. It is not intended for usage yet. But if you really want to\
    \ try:\n```\nfrom transformers import AutoTokenizer\nimport transformers \nimport\
    \ torch\nmodel = \"PY007/TinyLlama-1.1B-step-50K-105b\"\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
    pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nsequences = pipeline(\n\
    \    'The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion\
    \ tokens. With some proper optimization, we can achieve this within a span of\
    \ \"just\" 90 days using 16 A100-40G GPUs \U0001F680\U0001F680. The training has\
    \ started on 2023-09-01.',\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
    \    repetition_penalty=1.5,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=500,\n\
    )\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n```\n"
  created_at: 2023-09-06 01:31:21+00:00
  edited: false
  hidden: false
  id: 64f7e479c29cdd96943f9e3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a7fdf69466040b0b1b0b4b74f6683270.svg
      fullname: Nikita
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DetoDeto
      type: user
    createdAt: '2023-09-11T19:14:18.000Z'
    data:
      edited: false
      editors:
      - DetoDeto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6475679278373718
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a7fdf69466040b0b1b0b4b74f6683270.svg
          fullname: Nikita
          isHf: false
          isPro: false
          name: DetoDeto
          type: user
        html: "<blockquote>\n<p>This is an intermediate checkpoint that has not undergone\
          \ full training/learning rate cool-down. It is not intended for usage yet.\
          \ But if you really want to try:</p>\n<pre><code>from transformers import\
          \ AutoTokenizer\nimport transformers \nimport torch\nmodel = \"PY007/TinyLlama-1.1B-step-50K-105b\"\
          \ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n\
          \    device_map=\"auto\",\n)\n\nsequences = pipeline(\n    'The TinyLlama\
          \ project aims to pretrain a 1.1B Llama model on 3 trillion tokens. With\
          \ some proper optimization, we can achieve this within a span of \"just\"\
          \ 90 days using 16 A100-40G GPUs \U0001F680\U0001F680. The training has\
          \ started on 2023-09-01.',\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
          \    repetition_penalty=1.5,\n    eos_token_id=tokenizer.eos_token_id,\n\
          \    max_length=500,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n</code></pre>\n</blockquote>\n<p>I'm getting the following ValueError\
          \ when i try to run the pipeline (i've excluded the torch code, as i'm using\
          \ tensorflow):</p>\n<p><code>ValueError: Unrecognized configuration class\
          \ &lt;class 'transformers.models.llama.configuration_llama.LlamaConfig'&gt;\
          \ for this kind of AutoModel: TFAutoModelForCausalLM. Model type should\
          \ be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config,\
          \ GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig,\
          \ RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig,\
          \ XLMConfig, XLMRobertaConfig, XLNetConfig.</code><br>Any ideas on what\
          \ might help here?</p>\n"
        raw: "> This is an intermediate checkpoint that has not undergone full training/learning\
          \ rate cool-down. It is not intended for usage yet. But if you really want\
          \ to try:\n> ```\n> from transformers import AutoTokenizer\n> import transformers\
          \ \n> import torch\n> model = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n>\
          \ tokenizer = AutoTokenizer.from_pretrained(model)\n> pipeline = transformers.pipeline(\n\
          >     \"text-generation\",\n>     model=model,\n>     torch_dtype=torch.float16,\n\
          >     device_map=\"auto\",\n> )\n> \n> sequences = pipeline(\n>     'The\
          \ TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens.\
          \ With some proper optimization, we can achieve this within a span of \"\
          just\" 90 days using 16 A100-40G GPUs \U0001F680\U0001F680. The training\
          \ has started on 2023-09-01.',\n>     do_sample=True,\n>     top_k=10,\n\
          >     num_return_sequences=1,\n>     repetition_penalty=1.5,\n>     eos_token_id=tokenizer.eos_token_id,\n\
          >     max_length=500,\n> )\n> for seq in sequences:\n>     print(f\"Result:\
          \ {seq['generated_text']}\")\n> ```\n\nI'm getting the following ValueError\
          \ when i try to run the pipeline (i've excluded the torch code, as i'm using\
          \ tensorflow):\n```ValueError: Unrecognized configuration class <class 'transformers.models.llama.configuration_llama.LlamaConfig'>\
          \ for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should\
          \ be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config,\
          \ GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig,\
          \ RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig,\
          \ XLMConfig, XLMRobertaConfig, XLNetConfig.```\nAny ideas on what might\
          \ help here?"
        updatedAt: '2023-09-11T19:14:18.148Z'
      numEdits: 0
      reactions: []
    id: 64ff670a1e170b857421b9e9
    type: comment
  author: DetoDeto
  content: "> This is an intermediate checkpoint that has not undergone full training/learning\
    \ rate cool-down. It is not intended for usage yet. But if you really want to\
    \ try:\n> ```\n> from transformers import AutoTokenizer\n> import transformers\
    \ \n> import torch\n> model = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n> tokenizer\
    \ = AutoTokenizer.from_pretrained(model)\n> pipeline = transformers.pipeline(\n\
    >     \"text-generation\",\n>     model=model,\n>     torch_dtype=torch.float16,\n\
    >     device_map=\"auto\",\n> )\n> \n> sequences = pipeline(\n>     'The TinyLlama\
    \ project aims to pretrain a 1.1B Llama model on 3 trillion tokens. With some\
    \ proper optimization, we can achieve this within a span of \"just\" 90 days using\
    \ 16 A100-40G GPUs \U0001F680\U0001F680. The training has started on 2023-09-01.',\n\
    >     do_sample=True,\n>     top_k=10,\n>     num_return_sequences=1,\n>     repetition_penalty=1.5,\n\
    >     eos_token_id=tokenizer.eos_token_id,\n>     max_length=500,\n> )\n> for\
    \ seq in sequences:\n>     print(f\"Result: {seq['generated_text']}\")\n> ```\n\
    \nI'm getting the following ValueError when i try to run the pipeline (i've excluded\
    \ the torch code, as i'm using tensorflow):\n```ValueError: Unrecognized configuration\
    \ class <class 'transformers.models.llama.configuration_llama.LlamaConfig'> for\
    \ this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of\
    \ BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig,\
    \ OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig,\
    \ RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.```\n\
    Any ideas on what might help here?"
  created_at: 2023-09-11 18:14:18+00:00
  edited: false
  hidden: false
  id: 64ff670a1e170b857421b9e9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg?w=200&h=200&f=face
      fullname: Zhang Peiyuan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: PY007
      type: user
    createdAt: '2023-09-13T10:50:20.000Z'
    data:
      edited: false
      editors:
      - PY007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8145273923873901
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg?w=200&h=200&f=face
          fullname: Zhang Peiyuan
          isHf: false
          isPro: false
          name: PY007
          type: user
        html: '<p>you need transformers &gt;= 4.31</p>

          '
        raw: you need transformers >= 4.31
        updatedAt: '2023-09-13T10:50:20.372Z'
      numEdits: 0
      reactions: []
    id: 650193ec59d9a6bcc405bb64
    type: comment
  author: PY007
  content: you need transformers >= 4.31
  created_at: 2023-09-13 09:50:20+00:00
  edited: false
  hidden: false
  id: 650193ec59d9a6bcc405bb64
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TinyLlama/TinyLlama-1.1B-step-50K-105b
repo_type: model
status: open
target_branch: null
title: How can I use this?
