!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ggerganov
conflicting_files: null
created_at: 2023-09-04 06:43:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/625a6cd91acd8921c72ed02eaa78c746.svg
      fullname: Georgi Gerganov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ggerganov
      type: user
    createdAt: '2023-09-04T07:43:44.000Z'
    data:
      edited: false
      editors:
      - ggerganov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8716021776199341
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/625a6cd91acd8921c72ed02eaa78c746.svg
          fullname: Georgi Gerganov
          isHf: false
          isPro: false
          name: ggerganov
          type: user
        html: '<p>Just 131 bytes - does not look right</p>

          '
        raw: Just 131 bytes - does not look right
        updatedAt: '2023-09-04T07:43:44.777Z'
      numEdits: 0
      reactions: []
    id: 64f58ab0bce84cd8b1c26bb0
    type: comment
  author: ggerganov
  content: Just 131 bytes - does not look right
  created_at: 2023-09-04 06:43:44+00:00
  edited: false
  hidden: false
  id: 64f58ab0bce84cd8b1c26bb0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
      fullname: Erik Scholz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Green-Sky
      type: user
    createdAt: '2023-09-04T10:25:15.000Z'
    data:
      edited: false
      editors:
      - Green-Sky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9731013774871826
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
          fullname: Erik Scholz
          isHf: false
          isPro: false
          name: Green-Sky
          type: user
        html: '<p>tokenizer.model seems to be a git lfs link checked in as text file.
          this is clearly wrong.</p>

          '
        raw: tokenizer.model seems to be a git lfs link checked in as text file. this
          is clearly wrong.
        updatedAt: '2023-09-04T10:25:15.013Z'
      numEdits: 0
      reactions: []
    id: 64f5b08b5e2a868aa398b154
    type: comment
  author: Green-Sky
  content: tokenizer.model seems to be a git lfs link checked in as text file. this
    is clearly wrong.
  created_at: 2023-09-04 09:25:15+00:00
  edited: false
  hidden: false
  id: 64f5b08b5e2a868aa398b154
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
      fullname: Erik Scholz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Green-Sky
      type: user
    createdAt: '2023-09-04T10:35:28.000Z'
    data:
      edited: false
      editors:
      - Green-Sky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7162610292434692
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
          fullname: Erik Scholz
          isHf: false
          isPro: false
          name: Green-Sky
          type: user
        html: '<p>it seems to be the same as llama2, atleast the hash it says it is
          , is the same.<br><code>oid sha256:9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347</code><br><a
          href="https://huggingface.co/meta-llama/Llama-2-7b/blob/main/tokenizer.model">https://huggingface.co/meta-llama/Llama-2-7b/blob/main/tokenizer.model</a></p>

          '
        raw: 'it seems to be the same as llama2, atleast the hash it says it is ,
          is the same.

          `oid sha256:9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347`

          https://huggingface.co/meta-llama/Llama-2-7b/blob/main/tokenizer.model


          '
        updatedAt: '2023-09-04T10:35:28.364Z'
      numEdits: 0
      reactions: []
    id: 64f5b2f062205905dec08d04
    type: comment
  author: Green-Sky
  content: 'it seems to be the same as llama2, atleast the hash it says it is , is
    the same.

    `oid sha256:9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347`

    https://huggingface.co/meta-llama/Llama-2-7b/blob/main/tokenizer.model


    '
  created_at: 2023-09-04 09:35:28+00:00
  edited: false
  hidden: false
  id: 64f5b2f062205905dec08d04
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
      fullname: Erik Scholz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Green-Sky
      type: user
    createdAt: '2023-09-04T10:36:13.000Z'
    data:
      edited: true
      editors:
      - Green-Sky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2565256655216217
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645ce413a19f3e64bbeece31/UjVzo2J3imLBM9GCmN9q_.png?w=200&h=200&f=face
          fullname: Erik Scholz
          isHf: false
          isPro: false
          name: Green-Sky
          type: user
        html: "<p>update: tested and works. it is indeed the llama2 tokenizer.model</p>\n\
          <p>edit: Q8_0 wikitest perplexity comes out to be <code>354.0950 +/- 2.40774</code>\
          \ right now. which sounds about right.<br>f32 : <code>352.8210 +/- 2.39951</code></p>\n\
          <pre><code>$ bin/main -m ../models/TinyLlama-1.1B-step-50K-105b/ggml-model-f32.gguf\
          \ -t 10 -p \"The meaning of life\"\nLog start\nmain: build = 1173 (e4386f4)\n\
          main: seed  = 1693823704\nllama_model_loader: loaded meta data with 17 key-value\
          \ pairs and 201 tensors from ../models/TinyLlama-1.1B-step-50K-105b/ggml-model-f32.gguf\
          \ (version GGUF V2 (latest))\nllama_model_loader: - tensor    0:       \
          \             output.weight f32      [  2048, 32000,     1,     1 ]\nllama_model_loader:\
          \ - tensor    1:                token_embd.weight f32      [  2048, 32000,\
          \     1,     1 ]\nllama_model_loader: - tensor    2:           blk.0.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \    3:              blk.0.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor    4:              blk.0.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \    5:              blk.0.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor    6:         blk.0.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \    7:            blk.0.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor    8:            blk.0.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \    9:              blk.0.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   10:            blk.0.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   11:           blk.1.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   13:              blk.1.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   14:              blk.1.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   15:         blk.1.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   16:            blk.1.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   17:            blk.1.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   18:              blk.1.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   19:            blk.1.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   20:           blk.2.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   21:              blk.2.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   22:              blk.2.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   23:              blk.2.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   24:         blk.2.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   25:            blk.2.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   26:            blk.2.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   27:              blk.2.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   28:            blk.2.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   29:           blk.3.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   31:              blk.3.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   32:              blk.3.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   33:         blk.3.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   34:            blk.3.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   35:            blk.3.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   36:              blk.3.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   37:            blk.3.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   38:           blk.4.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   39:              blk.4.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   40:              blk.4.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   41:              blk.4.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   42:         blk.4.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   43:            blk.4.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   44:            blk.4.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   45:              blk.4.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   46:            blk.4.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   47:           blk.5.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   49:              blk.5.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   50:              blk.5.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   51:         blk.5.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   52:            blk.5.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   53:            blk.5.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   54:              blk.5.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   55:            blk.5.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   56:           blk.6.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   57:              blk.6.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   58:              blk.6.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   59:              blk.6.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   60:         blk.6.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   61:            blk.6.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   62:            blk.6.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   63:              blk.6.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   64:            blk.6.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   65:           blk.7.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   67:              blk.7.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   68:              blk.7.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   69:         blk.7.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   70:            blk.7.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   71:            blk.7.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   72:              blk.7.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   73:            blk.7.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   74:           blk.8.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   75:              blk.8.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   76:              blk.8.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   77:              blk.8.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   78:         blk.8.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   79:            blk.8.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   80:            blk.8.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   81:              blk.8.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   82:            blk.8.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   83:           blk.9.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   85:              blk.9.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   86:              blk.9.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   87:         blk.9.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   88:            blk.9.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   89:            blk.9.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   90:              blk.9.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   91:            blk.9.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   92:          blk.10.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   93:             blk.10.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   94:             blk.10.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   95:             blk.10.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   96:        blk.10.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   97:           blk.10.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   98:           blk.10.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   99:             blk.10.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  100:           blk.10.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  101:          blk.11.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  103:             blk.11.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  104:             blk.11.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  105:        blk.11.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  106:           blk.11.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  107:           blk.11.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  108:             blk.11.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  109:           blk.11.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  110:          blk.12.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  111:             blk.12.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  112:             blk.12.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  113:             blk.12.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  114:        blk.12.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  115:           blk.12.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  116:           blk.12.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  117:             blk.12.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  118:           blk.12.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  119:          blk.13.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  121:             blk.13.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  122:             blk.13.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  123:        blk.13.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  124:           blk.13.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  125:           blk.13.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  126:             blk.13.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  127:           blk.13.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  128:          blk.14.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  129:             blk.14.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  130:             blk.14.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  131:             blk.14.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  132:        blk.14.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  133:           blk.14.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  134:           blk.14.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  135:             blk.14.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  136:           blk.14.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  137:          blk.15.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  139:             blk.15.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  140:             blk.15.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  141:        blk.15.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  142:           blk.15.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  143:           blk.15.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  144:             blk.15.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  145:           blk.15.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  146:          blk.16.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  147:             blk.16.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  148:             blk.16.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  149:             blk.16.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  150:        blk.16.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  151:           blk.16.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  152:           blk.16.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  153:             blk.16.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  154:           blk.16.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  155:          blk.17.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  157:             blk.17.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  158:             blk.17.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  159:        blk.17.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  160:           blk.17.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  161:           blk.17.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  162:             blk.17.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  163:           blk.17.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  164:          blk.18.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  165:             blk.18.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  166:             blk.18.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  167:             blk.18.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  168:        blk.18.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  169:           blk.18.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  170:           blk.18.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  171:             blk.18.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  172:           blk.18.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  173:          blk.19.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  175:             blk.19.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  176:             blk.19.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  177:        blk.19.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  178:           blk.19.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  179:           blk.19.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  180:             blk.19.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  181:           blk.19.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  182:          blk.20.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  183:             blk.20.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  184:             blk.20.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  185:             blk.20.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  186:        blk.20.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  187:           blk.20.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  188:           blk.20.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  189:             blk.20.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  190:           blk.20.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  191:          blk.21.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  193:             blk.21.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  194:             blk.21.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  195:        blk.21.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  196:           blk.21.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  197:           blk.21.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  198:             blk.21.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  199:           blk.21.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  200:               output_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - kv   0:\
          \                       general.architecture str\nllama_model_loader: -\
          \ kv   1:                               general.name str\nllama_model_loader:\
          \ - kv   2:                       llama.context_length u32\nllama_model_loader:\
          \ - kv   3:                     llama.embedding_length u32\nllama_model_loader:\
          \ - kv   4:                          llama.block_count u32\nllama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32\nllama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32\nllama_model_loader:\
          \ - kv   7:                 llama.attention.head_count u32\nllama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32\nllama_model_loader:\
          \ - kv   9:     llama.attention.layer_norm_rms_epsilon f32\nllama_model_loader:\
          \ - kv  10:                       tokenizer.ggml.model str\nllama_model_loader:\
          \ - kv  11:                      tokenizer.ggml.tokens arr\nllama_model_loader:\
          \ - kv  12:                      tokenizer.ggml.scores arr\nllama_model_loader:\
          \ - kv  13:                  tokenizer.ggml.token_type arr\nllama_model_loader:\
          \ - kv  14:                tokenizer.ggml.bos_token_id u32\nllama_model_loader:\
          \ - kv  15:                tokenizer.ggml.eos_token_id u32\nllama_model_loader:\
          \ - kv  16:            tokenizer.ggml.unknown_token_id u32\nllama_model_loader:\
          \ - type  f32:  201 tensors\nllm_load_print_meta: format         = GGUF\
          \ V2 (latest)\nllm_load_print_meta: arch           = llama\nllm_load_print_meta:\
          \ vocab type     = SPM\nllm_load_print_meta: n_vocab        = 32000\nllm_load_print_meta:\
          \ n_merges       = 0\nllm_load_print_meta: n_ctx_train    = 2048\nllm_load_print_meta:\
          \ n_ctx          = 512\nllm_load_print_meta: n_embd         = 2048\nllm_load_print_meta:\
          \ n_head         = 32\nllm_load_print_meta: n_head_kv      = 4\nllm_load_print_meta:\
          \ n_layer        = 22\nllm_load_print_meta: n_rot          = 64\nllm_load_print_meta:\
          \ n_gqa          = 8\nllm_load_print_meta: f_norm_eps     = 1,0e-05\nllm_load_print_meta:\
          \ f_norm_rms_eps = 1,0e-05\nllm_load_print_meta: n_ff           = 5632\n\
          llm_load_print_meta: freq_base      = 10000,0\nllm_load_print_meta: freq_scale\
          \     = 1\nllm_load_print_meta: model type     = ?B\nllm_load_print_meta:\
          \ model ftype    = all F32 (guessed)\nllm_load_print_meta: model size  \
          \   = 1,10 B\nllm_load_print_meta: general.name   = models\nllm_load_print_meta:\
          \ BOS token = 1 '&lt;s&gt;'\nllm_load_print_meta: EOS token = 2 '&lt;/s&gt;'\n\
          llm_load_print_meta: UNK token = 0 '&lt;unk&gt;'\nllm_load_print_meta: LF\
          \ token  = 13 '&lt;0x0A&gt;'\nllm_load_tensors: ggml ctx size =    0,06\
          \ MB\nllm_load_tensors: mem required  = 4196,42 MB (+   11,00 MB per state)\n\
          ...........................................................................................\n\
          llama_new_context_with_model: kv self size  =   11,00 MB\nllama_new_context_with_model:\
          \ compute buffer total size =   67,97 MB\n\nsystem_info: n_threads = 10\
          \ / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
          \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
          \ = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\nsampling: repeat_last_n\
          \ = 64, repeat_penalty = 1,100000, presence_penalty = 0,000000, frequency_penalty\
          \ = 0,000000, top_k = 40, tfs_z = 1,000000, top_p = 0,950000, typical_p\
          \ = 1,000000, temp = 0,800000, mirostat = 0, mirostat_lr = 0,100000, mirostat_ent\
          \ = 5,000000\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep\
          \ = 0\n\n\n The meaning of life and the meaning to be to be able to exist.\
          \ I can say to have any knowledge and has the the opportunity to come in\
          \ the experience a few days, a little time as long as far.\nthe most probably\
          \ was to me. The most and the most difficult one of in, you. To have been\
          \ with them, the next that it is not a lot on the fact so and would think\
          \ the more than the other is\n</code></pre>\n"
        raw: "update: tested and works. it is indeed the llama2 tokenizer.model\n\n\
          edit: Q8_0 wikitest perplexity comes out to be `354.0950 +/- 2.40774` right\
          \ now. which sounds about right.\nf32 : `352.8210 +/- 2.39951`\n\n```\n\
          $ bin/main -m ../models/TinyLlama-1.1B-step-50K-105b/ggml-model-f32.gguf\
          \ -t 10 -p \"The meaning of life\"\nLog start\nmain: build = 1173 (e4386f4)\n\
          main: seed  = 1693823704\nllama_model_loader: loaded meta data with 17 key-value\
          \ pairs and 201 tensors from ../models/TinyLlama-1.1B-step-50K-105b/ggml-model-f32.gguf\
          \ (version GGUF V2 (latest))\nllama_model_loader: - tensor    0:       \
          \             output.weight f32      [  2048, 32000,     1,     1 ]\nllama_model_loader:\
          \ - tensor    1:                token_embd.weight f32      [  2048, 32000,\
          \     1,     1 ]\nllama_model_loader: - tensor    2:           blk.0.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \    3:              blk.0.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor    4:              blk.0.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \    5:              blk.0.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor    6:         blk.0.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \    7:            blk.0.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor    8:            blk.0.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \    9:              blk.0.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   10:            blk.0.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   11:           blk.1.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   13:              blk.1.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   14:              blk.1.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   15:         blk.1.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   16:            blk.1.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   17:            blk.1.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   18:              blk.1.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   19:            blk.1.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   20:           blk.2.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   21:              blk.2.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   22:              blk.2.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   23:              blk.2.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   24:         blk.2.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   25:            blk.2.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   26:            blk.2.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   27:              blk.2.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   28:            blk.2.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   29:           blk.3.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   31:              blk.3.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   32:              blk.3.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   33:         blk.3.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   34:            blk.3.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   35:            blk.3.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   36:              blk.3.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   37:            blk.3.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   38:           blk.4.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   39:              blk.4.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   40:              blk.4.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   41:              blk.4.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   42:         blk.4.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   43:            blk.4.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   44:            blk.4.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   45:              blk.4.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   46:            blk.4.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   47:           blk.5.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   49:              blk.5.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   50:              blk.5.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   51:         blk.5.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   52:            blk.5.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   53:            blk.5.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   54:              blk.5.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   55:            blk.5.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   56:           blk.6.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   57:              blk.6.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   58:              blk.6.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   59:              blk.6.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   60:         blk.6.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   61:            blk.6.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   62:            blk.6.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   63:              blk.6.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   64:            blk.6.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   65:           blk.7.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   67:              blk.7.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   68:              blk.7.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   69:         blk.7.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   70:            blk.7.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   71:            blk.7.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   72:              blk.7.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   73:            blk.7.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   74:           blk.8.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   75:              blk.8.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   76:              blk.8.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   77:              blk.8.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   78:         blk.8.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   79:            blk.8.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   80:            blk.8.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   81:              blk.8.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   82:            blk.8.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   83:           blk.9.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   85:              blk.9.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   86:              blk.9.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   87:         blk.9.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   88:            blk.9.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   89:            blk.9.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor   90:              blk.9.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   91:            blk.9.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   92:          blk.10.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   93:             blk.10.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor   94:             blk.10.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \   95:             blk.10.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor   96:        blk.10.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \   97:           blk.10.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   98:           blk.10.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \   99:             blk.10.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  100:           blk.10.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  101:          blk.11.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  103:             blk.11.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  104:             blk.11.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  105:        blk.11.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  106:           blk.11.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  107:           blk.11.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  108:             blk.11.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  109:           blk.11.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  110:          blk.12.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  111:             blk.12.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  112:             blk.12.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  113:             blk.12.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  114:        blk.12.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  115:           blk.12.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  116:           blk.12.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  117:             blk.12.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  118:           blk.12.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  119:          blk.13.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  121:             blk.13.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  122:             blk.13.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  123:        blk.13.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  124:           blk.13.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  125:           blk.13.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  126:             blk.13.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  127:           blk.13.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  128:          blk.14.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  129:             blk.14.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  130:             blk.14.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  131:             blk.14.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  132:        blk.14.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  133:           blk.14.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  134:           blk.14.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  135:             blk.14.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  136:           blk.14.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  137:          blk.15.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  139:             blk.15.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  140:             blk.15.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  141:        blk.15.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  142:           blk.15.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  143:           blk.15.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  144:             blk.15.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  145:           blk.15.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  146:          blk.16.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  147:             blk.16.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  148:             blk.16.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  149:             blk.16.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  150:        blk.16.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  151:           blk.16.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  152:           blk.16.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  153:             blk.16.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  154:           blk.16.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  155:          blk.17.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  157:             blk.17.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  158:             blk.17.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  159:        blk.17.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  160:           blk.17.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  161:           blk.17.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  162:             blk.17.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  163:           blk.17.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  164:          blk.18.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  165:             blk.18.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  166:             blk.18.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  167:             blk.18.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  168:        blk.18.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  169:           blk.18.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  170:           blk.18.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  171:             blk.18.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  172:           blk.18.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  173:          blk.19.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  175:             blk.19.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  176:             blk.19.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  177:        blk.19.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  178:           blk.19.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  179:           blk.19.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  180:             blk.19.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  181:           blk.19.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  182:          blk.20.attn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  183:             blk.20.attn_q.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  184:             blk.20.attn_k.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  185:             blk.20.attn_v.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  186:        blk.20.attn_output.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  187:           blk.20.ffn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  188:           blk.20.ffn_gate.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  189:             blk.20.ffn_up.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  190:           blk.20.ffn_down.weight\
          \ f32      [  5632,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  191:          blk.21.attn_norm.weight f32      [  2048,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_q.weight\
          \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor\
          \  193:             blk.21.attn_k.weight f32      [  2048,   256,     1,\
          \     1 ]\nllama_model_loader: - tensor  194:             blk.21.attn_v.weight\
          \ f32      [  2048,   256,     1,     1 ]\nllama_model_loader: - tensor\
          \  195:        blk.21.attn_output.weight f32      [  2048,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  196:           blk.21.ffn_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  197:           blk.21.ffn_gate.weight f32      [  2048,  5632,     1,\
          \     1 ]\nllama_model_loader: - tensor  198:             blk.21.ffn_up.weight\
          \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor\
          \  199:           blk.21.ffn_down.weight f32      [  5632,  2048,     1,\
          \     1 ]\nllama_model_loader: - tensor  200:               output_norm.weight\
          \ f32      [  2048,     1,     1,     1 ]\nllama_model_loader: - kv   0:\
          \                       general.architecture str\nllama_model_loader: -\
          \ kv   1:                               general.name str\nllama_model_loader:\
          \ - kv   2:                       llama.context_length u32\nllama_model_loader:\
          \ - kv   3:                     llama.embedding_length u32\nllama_model_loader:\
          \ - kv   4:                          llama.block_count u32\nllama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32\nllama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32\nllama_model_loader:\
          \ - kv   7:                 llama.attention.head_count u32\nllama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32\nllama_model_loader:\
          \ - kv   9:     llama.attention.layer_norm_rms_epsilon f32\nllama_model_loader:\
          \ - kv  10:                       tokenizer.ggml.model str\nllama_model_loader:\
          \ - kv  11:                      tokenizer.ggml.tokens arr\nllama_model_loader:\
          \ - kv  12:                      tokenizer.ggml.scores arr\nllama_model_loader:\
          \ - kv  13:                  tokenizer.ggml.token_type arr\nllama_model_loader:\
          \ - kv  14:                tokenizer.ggml.bos_token_id u32\nllama_model_loader:\
          \ - kv  15:                tokenizer.ggml.eos_token_id u32\nllama_model_loader:\
          \ - kv  16:            tokenizer.ggml.unknown_token_id u32\nllama_model_loader:\
          \ - type  f32:  201 tensors\nllm_load_print_meta: format         = GGUF\
          \ V2 (latest)\nllm_load_print_meta: arch           = llama\nllm_load_print_meta:\
          \ vocab type     = SPM\nllm_load_print_meta: n_vocab        = 32000\nllm_load_print_meta:\
          \ n_merges       = 0\nllm_load_print_meta: n_ctx_train    = 2048\nllm_load_print_meta:\
          \ n_ctx          = 512\nllm_load_print_meta: n_embd         = 2048\nllm_load_print_meta:\
          \ n_head         = 32\nllm_load_print_meta: n_head_kv      = 4\nllm_load_print_meta:\
          \ n_layer        = 22\nllm_load_print_meta: n_rot          = 64\nllm_load_print_meta:\
          \ n_gqa          = 8\nllm_load_print_meta: f_norm_eps     = 1,0e-05\nllm_load_print_meta:\
          \ f_norm_rms_eps = 1,0e-05\nllm_load_print_meta: n_ff           = 5632\n\
          llm_load_print_meta: freq_base      = 10000,0\nllm_load_print_meta: freq_scale\
          \     = 1\nllm_load_print_meta: model type     = ?B\nllm_load_print_meta:\
          \ model ftype    = all F32 (guessed)\nllm_load_print_meta: model size  \
          \   = 1,10 B\nllm_load_print_meta: general.name   = models\nllm_load_print_meta:\
          \ BOS token = 1 '<s>'\nllm_load_print_meta: EOS token = 2 '</s>'\nllm_load_print_meta:\
          \ UNK token = 0 '<unk>'\nllm_load_print_meta: LF token  = 13 '<0x0A>'\n\
          llm_load_tensors: ggml ctx size =    0,06 MB\nllm_load_tensors: mem required\
          \  = 4196,42 MB (+   11,00 MB per state)\n...........................................................................................\n\
          llama_new_context_with_model: kv self size  =   11,00 MB\nllama_new_context_with_model:\
          \ compute buffer total size =   67,97 MB\n\nsystem_info: n_threads = 10\
          \ / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
          \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
          \ = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\nsampling: repeat_last_n\
          \ = 64, repeat_penalty = 1,100000, presence_penalty = 0,000000, frequency_penalty\
          \ = 0,000000, top_k = 40, tfs_z = 1,000000, top_p = 0,950000, typical_p\
          \ = 1,000000, temp = 0,800000, mirostat = 0, mirostat_lr = 0,100000, mirostat_ent\
          \ = 5,000000\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep\
          \ = 0\n\n\n The meaning of life and the meaning to be to be able to exist.\
          \ I can say to have any knowledge and has the the opportunity to come in\
          \ the experience a few days, a little time as long as far.\nthe most probably\
          \ was to me. The most and the most difficult one of in, you. To have been\
          \ with them, the next that it is not a lot on the fact so and would think\
          \ the more than the other is\n```"
        updatedAt: '2023-09-04T11:09:36.075Z'
      numEdits: 3
      reactions: []
    id: 64f5b31d91e25a285416bd38
    type: comment
  author: Green-Sky
  content: "update: tested and works. it is indeed the llama2 tokenizer.model\n\n\
    edit: Q8_0 wikitest perplexity comes out to be `354.0950 +/- 2.40774` right now.\
    \ which sounds about right.\nf32 : `352.8210 +/- 2.39951`\n\n```\n$ bin/main -m\
    \ ../models/TinyLlama-1.1B-step-50K-105b/ggml-model-f32.gguf -t 10 -p \"The meaning\
    \ of life\"\nLog start\nmain: build = 1173 (e4386f4)\nmain: seed  = 1693823704\n\
    llama_model_loader: loaded meta data with 17 key-value pairs and 201 tensors from\
    \ ../models/TinyLlama-1.1B-step-50K-105b/ggml-model-f32.gguf (version GGUF V2\
    \ (latest))\nllama_model_loader: - tensor    0:                    output.weight\
    \ f32      [  2048, 32000,     1,     1 ]\nllama_model_loader: - tensor    1:\
    \                token_embd.weight f32      [  2048, 32000,     1,     1 ]\nllama_model_loader:\
    \ - tensor    2:           blk.0.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor    3:              blk.0.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor    4:\
    \              blk.0.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor    5:              blk.0.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor    6:         blk.0.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor    7:\
    \            blk.0.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor    8:            blk.0.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor    9:              blk.0.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor   10:\
    \            blk.0.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor   11:           blk.1.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   13:\
    \              blk.1.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor   14:              blk.1.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor   15:         blk.1.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   16:\
    \            blk.1.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   17:            blk.1.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor   18:              blk.1.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor   19:\
    \            blk.1.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor   20:           blk.2.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   21:              blk.2.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   22:\
    \              blk.2.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor   23:              blk.2.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor   24:         blk.2.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   25:\
    \            blk.2.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   26:            blk.2.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor   27:              blk.2.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor   28:\
    \            blk.2.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor   29:           blk.3.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   31:\
    \              blk.3.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor   32:              blk.3.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor   33:         blk.3.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   34:\
    \            blk.3.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   35:            blk.3.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor   36:              blk.3.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor   37:\
    \            blk.3.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor   38:           blk.4.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   39:              blk.4.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   40:\
    \              blk.4.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor   41:              blk.4.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor   42:         blk.4.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   43:\
    \            blk.4.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   44:            blk.4.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor   45:              blk.4.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor   46:\
    \            blk.4.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor   47:           blk.5.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   49:\
    \              blk.5.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor   50:              blk.5.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor   51:         blk.5.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   52:\
    \            blk.5.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   53:            blk.5.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor   54:              blk.5.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor   55:\
    \            blk.5.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor   56:           blk.6.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   57:              blk.6.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   58:\
    \              blk.6.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor   59:              blk.6.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor   60:         blk.6.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   61:\
    \            blk.6.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   62:            blk.6.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor   63:              blk.6.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor   64:\
    \            blk.6.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor   65:           blk.7.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   67:\
    \              blk.7.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor   68:              blk.7.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor   69:         blk.7.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   70:\
    \            blk.7.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   71:            blk.7.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor   72:              blk.7.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor   73:\
    \            blk.7.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor   74:           blk.8.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   75:              blk.8.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   76:\
    \              blk.8.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor   77:              blk.8.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor   78:         blk.8.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   79:\
    \            blk.8.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   80:            blk.8.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor   81:              blk.8.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor   82:\
    \            blk.8.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor   83:           blk.9.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   85:\
    \              blk.9.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor   86:              blk.9.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor   87:         blk.9.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   88:\
    \            blk.9.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   89:            blk.9.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor   90:              blk.9.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor   91:\
    \            blk.9.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor   92:          blk.10.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   93:             blk.10.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   94:\
    \             blk.10.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor   95:             blk.10.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor   96:        blk.10.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor   97:\
    \           blk.10.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor   98:           blk.10.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor   99:             blk.10.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor  100:\
    \           blk.10.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor  101:          blk.11.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  103:\
    \             blk.11.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor  104:             blk.11.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor  105:        blk.11.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  106:\
    \           blk.11.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  107:           blk.11.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor  108:             blk.11.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor  109:\
    \           blk.11.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor  110:          blk.12.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  111:             blk.12.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  112:\
    \             blk.12.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor  113:             blk.12.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor  114:        blk.12.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  115:\
    \           blk.12.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  116:           blk.12.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor  117:             blk.12.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor  118:\
    \           blk.12.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor  119:          blk.13.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  121:\
    \             blk.13.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor  122:             blk.13.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor  123:        blk.13.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  124:\
    \           blk.13.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  125:           blk.13.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor  126:             blk.13.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor  127:\
    \           blk.13.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor  128:          blk.14.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  129:             blk.14.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  130:\
    \             blk.14.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor  131:             blk.14.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor  132:        blk.14.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  133:\
    \           blk.14.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  134:           blk.14.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor  135:             blk.14.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor  136:\
    \           blk.14.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor  137:          blk.15.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  139:\
    \             blk.15.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor  140:             blk.15.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor  141:        blk.15.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  142:\
    \           blk.15.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  143:           blk.15.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor  144:             blk.15.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor  145:\
    \           blk.15.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor  146:          blk.16.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  147:             blk.16.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  148:\
    \             blk.16.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor  149:             blk.16.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor  150:        blk.16.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  151:\
    \           blk.16.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  152:           blk.16.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor  153:             blk.16.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor  154:\
    \           blk.16.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor  155:          blk.17.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  157:\
    \             blk.17.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor  158:             blk.17.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor  159:        blk.17.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  160:\
    \           blk.17.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  161:           blk.17.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor  162:             blk.17.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor  163:\
    \           blk.17.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor  164:          blk.18.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  165:             blk.18.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  166:\
    \             blk.18.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor  167:             blk.18.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor  168:        blk.18.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  169:\
    \           blk.18.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  170:           blk.18.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor  171:             blk.18.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor  172:\
    \           blk.18.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor  173:          blk.19.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  175:\
    \             blk.19.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor  176:             blk.19.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor  177:        blk.19.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  178:\
    \           blk.19.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  179:           blk.19.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor  180:             blk.19.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor  181:\
    \           blk.19.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor  182:          blk.20.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  183:             blk.20.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  184:\
    \             blk.20.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor  185:             blk.20.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor  186:        blk.20.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  187:\
    \           blk.20.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  188:           blk.20.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor  189:             blk.20.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor  190:\
    \           blk.20.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor  191:          blk.21.attn_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_q.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  193:\
    \             blk.21.attn_k.weight f32      [  2048,   256,     1,     1 ]\nllama_model_loader:\
    \ - tensor  194:             blk.21.attn_v.weight f32      [  2048,   256,   \
    \  1,     1 ]\nllama_model_loader: - tensor  195:        blk.21.attn_output.weight\
    \ f32      [  2048,  2048,     1,     1 ]\nllama_model_loader: - tensor  196:\
    \           blk.21.ffn_norm.weight f32      [  2048,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  197:           blk.21.ffn_gate.weight f32      [  2048,  5632,   \
    \  1,     1 ]\nllama_model_loader: - tensor  198:             blk.21.ffn_up.weight\
    \ f32      [  2048,  5632,     1,     1 ]\nllama_model_loader: - tensor  199:\
    \           blk.21.ffn_down.weight f32      [  5632,  2048,     1,     1 ]\nllama_model_loader:\
    \ - tensor  200:               output_norm.weight f32      [  2048,     1,   \
    \  1,     1 ]\nllama_model_loader: - kv   0:                       general.architecture\
    \ str\nllama_model_loader: - kv   1:                               general.name\
    \ str\nllama_model_loader: - kv   2:                       llama.context_length\
    \ u32\nllama_model_loader: - kv   3:                     llama.embedding_length\
    \ u32\nllama_model_loader: - kv   4:                          llama.block_count\
    \ u32\nllama_model_loader: - kv   5:                  llama.feed_forward_length\
    \ u32\nllama_model_loader: - kv   6:                 llama.rope.dimension_count\
    \ u32\nllama_model_loader: - kv   7:                 llama.attention.head_count\
    \ u32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
    \ u32\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
    \ f32\nllama_model_loader: - kv  10:                       tokenizer.ggml.model\
    \ str\nllama_model_loader: - kv  11:                      tokenizer.ggml.tokens\
    \ arr\nllama_model_loader: - kv  12:                      tokenizer.ggml.scores\
    \ arr\nllama_model_loader: - kv  13:                  tokenizer.ggml.token_type\
    \ arr\nllama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id\
    \ u32\nllama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id\
    \ u32\nllama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id\
    \ u32\nllama_model_loader: - type  f32:  201 tensors\nllm_load_print_meta: format\
    \         = GGUF V2 (latest)\nllm_load_print_meta: arch           = llama\nllm_load_print_meta:\
    \ vocab type     = SPM\nllm_load_print_meta: n_vocab        = 32000\nllm_load_print_meta:\
    \ n_merges       = 0\nllm_load_print_meta: n_ctx_train    = 2048\nllm_load_print_meta:\
    \ n_ctx          = 512\nllm_load_print_meta: n_embd         = 2048\nllm_load_print_meta:\
    \ n_head         = 32\nllm_load_print_meta: n_head_kv      = 4\nllm_load_print_meta:\
    \ n_layer        = 22\nllm_load_print_meta: n_rot          = 64\nllm_load_print_meta:\
    \ n_gqa          = 8\nllm_load_print_meta: f_norm_eps     = 1,0e-05\nllm_load_print_meta:\
    \ f_norm_rms_eps = 1,0e-05\nllm_load_print_meta: n_ff           = 5632\nllm_load_print_meta:\
    \ freq_base      = 10000,0\nllm_load_print_meta: freq_scale     = 1\nllm_load_print_meta:\
    \ model type     = ?B\nllm_load_print_meta: model ftype    = all F32 (guessed)\n\
    llm_load_print_meta: model size     = 1,10 B\nllm_load_print_meta: general.name\
    \   = models\nllm_load_print_meta: BOS token = 1 '<s>'\nllm_load_print_meta: EOS\
    \ token = 2 '</s>'\nllm_load_print_meta: UNK token = 0 '<unk>'\nllm_load_print_meta:\
    \ LF token  = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0,06 MB\nllm_load_tensors:\
    \ mem required  = 4196,42 MB (+   11,00 MB per state)\n...........................................................................................\n\
    llama_new_context_with_model: kv self size  =   11,00 MB\nllama_new_context_with_model:\
    \ compute buffer total size =   67,97 MB\n\nsystem_info: n_threads = 10 / 24 |\
    \ AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA =\
    \ 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS =\
    \ 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\nsampling: repeat_last_n = 64, repeat_penalty\
    \ = 1,100000, presence_penalty = 0,000000, frequency_penalty = 0,000000, top_k\
    \ = 40, tfs_z = 1,000000, top_p = 0,950000, typical_p = 1,000000, temp = 0,800000,\
    \ mirostat = 0, mirostat_lr = 0,100000, mirostat_ent = 5,000000\ngenerate: n_ctx\
    \ = 512, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n The meaning of life and\
    \ the meaning to be to be able to exist. I can say to have any knowledge and has\
    \ the the opportunity to come in the experience a few days, a little time as long\
    \ as far.\nthe most probably was to me. The most and the most difficult one of\
    \ in, you. To have been with them, the next that it is not a lot on the fact so\
    \ and would think the more than the other is\n```"
  created_at: 2023-09-04 09:36:13+00:00
  edited: true
  hidden: false
  id: 64f5b31d91e25a285416bd38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg?w=200&h=200&f=face
      fullname: Zhang Peiyuan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: PY007
      type: user
    createdAt: '2023-09-04T11:06:58.000Z'
    data:
      edited: false
      editors:
      - PY007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8334450125694275
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg?w=200&h=200&f=face
          fullname: Zhang Peiyuan
          isHf: false
          isPro: false
          name: PY007
          type: user
        html: '<p>Thanks for spotting this! I guess HuggingFace might substitute the
          tokenizer with a softlink for less storage. I can successfully load with
          AutoModel.from_pretrained(). But it might not work with git clone. Will
          update the file!</p>

          '
        raw: Thanks for spotting this! I guess HuggingFace might substitute the tokenizer
          with a softlink for less storage. I can successfully load with AutoModel.from_pretrained().
          But it might not work with git clone. Will update the file!
        updatedAt: '2023-09-04T11:06:58.286Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PY007
    id: 64f5ba5268047192d6cce48a
    type: comment
  author: PY007
  content: Thanks for spotting this! I guess HuggingFace might substitute the tokenizer
    with a softlink for less storage. I can successfully load with AutoModel.from_pretrained().
    But it might not work with git clone. Will update the file!
  created_at: 2023-09-04 10:06:58+00:00
  edited: false
  hidden: false
  id: 64f5ba5268047192d6cce48a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg?w=200&h=200&f=face
      fullname: Zhang Peiyuan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: PY007
      type: user
    createdAt: '2023-09-04T11:09:37.000Z'
    data:
      edited: false
      editors:
      - PY007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9575601816177368
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg?w=200&h=200&f=face
          fullname: Zhang Peiyuan
          isHf: false
          isPro: false
          name: PY007
          type: user
        html: '<p>I have uploaded the correct tokenizer. Will close this issue. Feel
          free to reopen it if the problem persists.</p>

          '
        raw: I have uploaded the correct tokenizer. Will close this issue. Feel free
          to reopen it if the problem persists.
        updatedAt: '2023-09-04T11:09:37.299Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Green-Sky
        - ggerganov
      relatedEventId: 64f5baf165a4b1acb20ddfee
    id: 64f5baf165a4b1acb20ddfea
    type: comment
  author: PY007
  content: I have uploaded the correct tokenizer. Will close this issue. Feel free
    to reopen it if the problem persists.
  created_at: 2023-09-04 10:09:37+00:00
  edited: false
  hidden: false
  id: 64f5baf165a4b1acb20ddfea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg?w=200&h=200&f=face
      fullname: Zhang Peiyuan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: PY007
      type: user
    createdAt: '2023-09-04T11:09:37.000Z'
    data:
      status: closed
    id: 64f5baf165a4b1acb20ddfee
    type: status-change
  author: PY007
  created_at: 2023-09-04 10:09:37+00:00
  id: 64f5baf165a4b1acb20ddfee
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TinyLlama/TinyLlama-1.1B-step-50K-105b
repo_type: model
status: closed
target_branch: null
title: tokenizer.model seems to be empty?
