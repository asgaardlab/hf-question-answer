!!python/object:huggingface_hub.community.DiscussionWithDetails
author: IronicKeyboard
conflicting_files: null
created_at: 2024-01-05 23:29:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/46fac6fae7aabbc82ec2a58c410c4028.svg
      fullname: Ironic Keyboard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IronicKeyboard
      type: user
    createdAt: '2024-01-05T23:29:54.000Z'
    data:
      edited: false
      editors:
      - IronicKeyboard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5077657699584961
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/46fac6fae7aabbc82ec2a58c410c4028.svg
          fullname: Ironic Keyboard
          isHf: false
          isPro: false
          name: IronicKeyboard
          type: user
        html: '<p>I''m using the template for "TheBloke Local LLMs One-Click UI and
          API" template using an L40. When I attempt to load the model, I get the
          following error:<br>Traceback (most recent call last):</p>

          <p>File "/workspace/text-generation-webui/modules/ui_model_menu.py", line
          213, in load_model_wrapper</p>

          <p>shared.model, shared.tokenizer = load_model(selected_model, loader)<br>File
          "/workspace/text-generation-webui/modules/models.py", line 87, in load_model</p>

          <p>output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>File
          "/workspace/text-generation-webui/modules/models.py", line 311, in AutoAWQ_loader</p>

          <p>model = AutoAWQForCausalLM.from_quantized(<br>File "/usr/local/lib/python3.10/dist-packages/awq/models/auto.py",
          line 50, in from_quantized</p>

          <p>model_type = check_and_get_model_type(quant_path, trust_remote_code)<br>File
          "/usr/local/lib/python3.10/dist-packages/awq/models/auto.py", line 25, in
          check_and_get_model_type</p>

          <p>raise TypeError(f"{config.model_type} isn''t supported yet.")<br>TypeError:
          mixtral isn''t supported yet.</p>

          '
        raw: "I'm using the template for \"TheBloke Local LLMs One-Click UI and API\"\
          \ template using an L40. When I attempt to load the model, I get the following\
          \ error:\r\nTraceback (most recent call last):\r\n\r\nFile \"/workspace/text-generation-webui/modules/ui_model_menu.py\"\
          , line 213, in load_model_wrapper\r\n\r\nshared.model, shared.tokenizer\
          \ = load_model(selected_model, loader)\r\nFile \"/workspace/text-generation-webui/modules/models.py\"\
          , line 87, in load_model\r\n\r\noutput = load_func_map[loader](model_name)\r\
          \nFile \"/workspace/text-generation-webui/modules/models.py\", line 311,\
          \ in AutoAWQ_loader\r\n\r\nmodel = AutoAWQForCausalLM.from_quantized(\r\n\
          File \"/usr/local/lib/python3.10/dist-packages/awq/models/auto.py\", line\
          \ 50, in from_quantized\r\n\r\nmodel_type = check_and_get_model_type(quant_path,\
          \ trust_remote_code)\r\nFile \"/usr/local/lib/python3.10/dist-packages/awq/models/auto.py\"\
          , line 25, in check_and_get_model_type\r\n\r\nraise TypeError(f\"{config.model_type}\
          \ isn't supported yet.\")\r\nTypeError: mixtral isn't supported yet."
        updatedAt: '2024-01-05T23:29:54.517Z'
      numEdits: 0
      reactions: []
    id: 659890f23b0b56c5e0802624
    type: comment
  author: IronicKeyboard
  content: "I'm using the template for \"TheBloke Local LLMs One-Click UI and API\"\
    \ template using an L40. When I attempt to load the model, I get the following\
    \ error:\r\nTraceback (most recent call last):\r\n\r\nFile \"/workspace/text-generation-webui/modules/ui_model_menu.py\"\
    , line 213, in load_model_wrapper\r\n\r\nshared.model, shared.tokenizer = load_model(selected_model,\
    \ loader)\r\nFile \"/workspace/text-generation-webui/modules/models.py\", line\
    \ 87, in load_model\r\n\r\noutput = load_func_map[loader](model_name)\r\nFile\
    \ \"/workspace/text-generation-webui/modules/models.py\", line 311, in AutoAWQ_loader\r\
    \n\r\nmodel = AutoAWQForCausalLM.from_quantized(\r\nFile \"/usr/local/lib/python3.10/dist-packages/awq/models/auto.py\"\
    , line 50, in from_quantized\r\n\r\nmodel_type = check_and_get_model_type(quant_path,\
    \ trust_remote_code)\r\nFile \"/usr/local/lib/python3.10/dist-packages/awq/models/auto.py\"\
    , line 25, in check_and_get_model_type\r\n\r\nraise TypeError(f\"{config.model_type}\
    \ isn't supported yet.\")\r\nTypeError: mixtral isn't supported yet."
  created_at: 2024-01-05 23:29:54+00:00
  edited: false
  hidden: false
  id: 659890f23b0b56c5e0802624
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/dolphin-2.7-mixtral-8x7b-AWQ
repo_type: model
status: open
target_branch: null
title: Doesn't load on Runpod
