!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rjmehta
conflicting_files: null
created_at: 2023-09-14 15:07:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-09-14T16:07:34.000Z'
    data:
      edited: true
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3318425118923187
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span>  Please help.\
          \ Thank you</p>\n<p>Getting this error: </p>\n<hr>\n<p>ModuleNotFoundError\
          \                       Traceback (most recent call last)<br>Cell In[5],\
          \ line 6<br>      3 model_name_or_path = \"TheBloke/Airoboros-L2-13B-2.1-YaRN-64K-GPTQ\"\
          <br>      4 # To use a different branch, change revision<br>      5 # For\
          \ example: revision=\"gptq-4bit-32g-actorder_True\"<br>----&gt; 6 model\
          \ = AutoModelForCausalLM.from_pretrained(model_name_or_path,<br>      7\
          \                                              device=\"cuda:0\",<br>  \
          \    8                                              trust_remote_code=True,<br>\
          \      9                                              revision=\"main\"\
          )<br>     11 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)</p>\n<p>File ~/.conda/envs/summr/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:527,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)<br>    524 if kwargs.get(\"quantization_config\"\
          , None) is not None:<br>    525     _ = kwargs.pop(\"quantization_config\"\
          )<br>--&gt; 527 config, kwargs = AutoConfig.from_pretrained(<br>    528\
          \     pretrained_model_name_or_path,<br>    529     return_unused_kwargs=True,<br>\
          \    530     trust_remote_code=trust_remote_code,<br>    531     code_revision=code_revision,<br>\
          \    532     _commit_hash=commit_hash,<br>    533     **hub_kwargs,<br>\
          \    534     **kwargs,<br>    535 )<br>    537 # if torch_dtype=auto was\
          \ passed here, ensure to pass it on<br>    538 if kwargs_orig.get(\"torch_dtype\"\
          , None) == \"auto\":</p>\n<p>File ~/.conda/envs/summr/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1032,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)<br>\
          \   1030 if has_remote_code and trust_remote_code:<br>   1031     class_ref\
          \ = config_dict[\"auto_map\"][\"AutoConfig\"]<br>-&gt; 1032     config_class\
          \ = get_class_from_dynamic_module(<br>   1033         class_ref, pretrained_model_name_or_path,\
          \ code_revision=code_revision, **kwargs<br>   1034     )<br>   1035    \
          \ if os.path.isdir(pretrained_model_name_or_path):<br>   1036         config_class.register_for_auto_class()</p>\n\
          <p>File ~/.conda/envs/summr/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:497,\
          \ in get_class_from_dynamic_module(class_reference, pretrained_model_name_or_path,\
          \ cache_dir, force_download, resume_download, proxies, token, revision,\
          \ local_files_only, repo_type, code_revision, **kwargs)<br>    484 # And\
          \ lastly we get the class inside our newly created module<br>    485 final_module\
          \ = get_cached_module_file(<br>    486     repo_id,<br>    487     module_file\
          \ + \".py\",<br>   (...)<br>    495     repo_type=repo_type,<br>    496\
          \ )<br>--&gt; 497 return get_class_in_module(class_name, final_module.replace(\"\
          .py\", \"\"))</p>\n<p>File ~/.conda/envs/summr/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:199,\
          \ in get_class_in_module(class_name, module_path)<br>    188 \"\"\"<br>\
          \    189 Import a module on the cache directory for modules and extract\
          \ a class from it.<br>    190<br>   (...)<br>    196     <code>typing.Type</code>:\
          \ The class looked for.<br>    197 \"\"\"<br>    198 module_path = module_path.replace(os.path.sep,\
          \ \".\")<br>--&gt; 199 module = importlib.import_module(module_path)<br>\
          \    200 return getattr(module, class_name)</p>\n<p>File ~/.conda/envs/summr/lib/python3.10/importlib/<strong>init</strong>.py:126,\
          \ in import_module(name, package)<br>    124             break<br>    125\
          \         level += 1<br>--&gt; 126 return _bootstrap._gcd_import(name[level:],\
          \ package, level)</p>\n<p>File :1050, in _gcd_import(name, package, level)</p>\n\
          <p>File :1027, in <em>find_and_load(name, import</em>)</p>\n<p>File :992,\
          \ in <em>find_and_load_unlocked(name, import</em>)</p>\n<p>File :241, in\
          \ _call_with_frames_removed(f, *args, **kwds)</p>\n<p>File :1050, in _gcd_import(name,\
          \ package, level)</p>\n<p>File :1027, in <em>find_and_load(name, import</em>)</p>\n\
          <p>File :992, in <em>find_and_load_unlocked(name, import</em>)</p>\n<p>File\
          \ :241, in _call_with_frames_removed(f, *args, **kwds)</p>\n<p>File :1050,\
          \ in _gcd_import(name, package, level)</p>\n<p>File :1027, in <em>find_and_load(name,\
          \ import</em>)</p>\n<p>File :992, in <em>find_and_load_unlocked(name, import</em>)</p>\n\
          <p>File :241, in _call_with_frames_removed(f, *args, **kwds)</p>\n<p>File\
          \ :1050, in _gcd_import(name, package, level)</p>\n<p>File :1027, in <em>find_and_load(name,\
          \ import</em>)</p>\n<p>File :1004, in <em>find_and_load_unlocked(name, import</em>)</p>\n\
          <p>ModuleNotFoundError: No module named 'transformers_modules.TheBloke.Airoboros-L2-13B-2'</p>\n\
          <p>Transformers == 4.33.1</p>\n"
        raw: "@TheBloke  Please help. Thank you\n\n\nGetting this error: \n\n---------------------------------------------------------------------------\n\
          ModuleNotFoundError                       Traceback (most recent call last)\n\
          Cell In[5], line 6\n      3 model_name_or_path = \"TheBloke/Airoboros-L2-13B-2.1-YaRN-64K-GPTQ\"\
          \n      4 # To use a different branch, change revision\n      5 # For example:\
          \ revision=\"gptq-4bit-32g-actorder_True\"\n----> 6 model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \      7                                              device=\"cuda:0\"\
          ,\n      8                                              trust_remote_code=True,\n\
          \      9                                              revision=\"main\"\
          )\n     11 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nFile ~/.conda/envs/summr/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:527,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    524 if kwargs.get(\"quantization_config\"\
          , None) is not None:\n    525     _ = kwargs.pop(\"quantization_config\"\
          )\n--> 527 config, kwargs = AutoConfig.from_pretrained(\n    528     pretrained_model_name_or_path,\n\
          \    529     return_unused_kwargs=True,\n    530     trust_remote_code=trust_remote_code,\n\
          \    531     code_revision=code_revision,\n    532     _commit_hash=commit_hash,\n\
          \    533     **hub_kwargs,\n    534     **kwargs,\n    535 )\n    537 #\
          \ if torch_dtype=auto was passed here, ensure to pass it on\n    538 if\
          \ kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\n\nFile ~/.conda/envs/summr/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1032,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \   1030 if has_remote_code and trust_remote_code:\n   1031     class_ref\
          \ = config_dict[\"auto_map\"][\"AutoConfig\"]\n-> 1032     config_class\
          \ = get_class_from_dynamic_module(\n   1033         class_ref, pretrained_model_name_or_path,\
          \ code_revision=code_revision, **kwargs\n   1034     )\n   1035     if os.path.isdir(pretrained_model_name_or_path):\n\
          \   1036         config_class.register_for_auto_class()\n\nFile ~/.conda/envs/summr/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:497,\
          \ in get_class_from_dynamic_module(class_reference, pretrained_model_name_or_path,\
          \ cache_dir, force_download, resume_download, proxies, token, revision,\
          \ local_files_only, repo_type, code_revision, **kwargs)\n    484 # And lastly\
          \ we get the class inside our newly created module\n    485 final_module\
          \ = get_cached_module_file(\n    486     repo_id,\n    487     module_file\
          \ + \".py\",\n   (...)\n    495     repo_type=repo_type,\n    496 )\n-->\
          \ 497 return get_class_in_module(class_name, final_module.replace(\".py\"\
          , \"\"))\n\nFile ~/.conda/envs/summr/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:199,\
          \ in get_class_in_module(class_name, module_path)\n    188 \"\"\"\n    189\
          \ Import a module on the cache directory for modules and extract a class\
          \ from it.\n    190 \n   (...)\n    196     `typing.Type`: The class looked\
          \ for.\n    197 \"\"\"\n    198 module_path = module_path.replace(os.path.sep,\
          \ \".\")\n--> 199 module = importlib.import_module(module_path)\n    200\
          \ return getattr(module, class_name)\n\nFile ~/.conda/envs/summr/lib/python3.10/importlib/__init__.py:126,\
          \ in import_module(name, package)\n    124             break\n    125  \
          \       level += 1\n--> 126 return _bootstrap._gcd_import(name[level:],\
          \ package, level)\n\nFile <frozen importlib._bootstrap>:1050, in _gcd_import(name,\
          \ package, level)\n\nFile <frozen importlib._bootstrap>:1027, in _find_and_load(name,\
          \ import_)\n\nFile <frozen importlib._bootstrap>:992, in _find_and_load_unlocked(name,\
          \ import_)\n\nFile <frozen importlib._bootstrap>:241, in _call_with_frames_removed(f,\
          \ *args, **kwds)\n\nFile <frozen importlib._bootstrap>:1050, in _gcd_import(name,\
          \ package, level)\n\nFile <frozen importlib._bootstrap>:1027, in _find_and_load(name,\
          \ import_)\n\nFile <frozen importlib._bootstrap>:992, in _find_and_load_unlocked(name,\
          \ import_)\n\nFile <frozen importlib._bootstrap>:241, in _call_with_frames_removed(f,\
          \ *args, **kwds)\n\nFile <frozen importlib._bootstrap>:1050, in _gcd_import(name,\
          \ package, level)\n\nFile <frozen importlib._bootstrap>:1027, in _find_and_load(name,\
          \ import_)\n\nFile <frozen importlib._bootstrap>:992, in _find_and_load_unlocked(name,\
          \ import_)\n\nFile <frozen importlib._bootstrap>:241, in _call_with_frames_removed(f,\
          \ *args, **kwds)\n\nFile <frozen importlib._bootstrap>:1050, in _gcd_import(name,\
          \ package, level)\n\nFile <frozen importlib._bootstrap>:1027, in _find_and_load(name,\
          \ import_)\n\nFile <frozen importlib._bootstrap>:1004, in _find_and_load_unlocked(name,\
          \ import_)\n\nModuleNotFoundError: No module named 'transformers_modules.TheBloke.Airoboros-L2-13B-2'\n\
          \n\n\n\nTransformers == 4.33.1"
        updatedAt: '2023-09-14T16:07:53.773Z'
      numEdits: 1
      reactions: []
    id: 65032fc647e72ff44bb18911
    type: comment
  author: rjmehta
  content: "@TheBloke  Please help. Thank you\n\n\nGetting this error: \n\n---------------------------------------------------------------------------\n\
    ModuleNotFoundError                       Traceback (most recent call last)\n\
    Cell In[5], line 6\n      3 model_name_or_path = \"TheBloke/Airoboros-L2-13B-2.1-YaRN-64K-GPTQ\"\
    \n      4 # To use a different branch, change revision\n      5 # For example:\
    \ revision=\"gptq-4bit-32g-actorder_True\"\n----> 6 model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
    \      7                                              device=\"cuda:0\",\n   \
    \   8                                              trust_remote_code=True,\n \
    \     9                                              revision=\"main\")\n    \
    \ 11 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
    \nFile ~/.conda/envs/summr/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:527,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    524 if kwargs.get(\"quantization_config\", None) is not None:\n\
    \    525     _ = kwargs.pop(\"quantization_config\")\n--> 527 config, kwargs =\
    \ AutoConfig.from_pretrained(\n    528     pretrained_model_name_or_path,\n  \
    \  529     return_unused_kwargs=True,\n    530     trust_remote_code=trust_remote_code,\n\
    \    531     code_revision=code_revision,\n    532     _commit_hash=commit_hash,\n\
    \    533     **hub_kwargs,\n    534     **kwargs,\n    535 )\n    537 # if torch_dtype=auto\
    \ was passed here, ensure to pass it on\n    538 if kwargs_orig.get(\"torch_dtype\"\
    , None) == \"auto\":\n\nFile ~/.conda/envs/summr/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1032,\
    \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
    \   1030 if has_remote_code and trust_remote_code:\n   1031     class_ref = config_dict[\"\
    auto_map\"][\"AutoConfig\"]\n-> 1032     config_class = get_class_from_dynamic_module(\n\
    \   1033         class_ref, pretrained_model_name_or_path, code_revision=code_revision,\
    \ **kwargs\n   1034     )\n   1035     if os.path.isdir(pretrained_model_name_or_path):\n\
    \   1036         config_class.register_for_auto_class()\n\nFile ~/.conda/envs/summr/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:497,\
    \ in get_class_from_dynamic_module(class_reference, pretrained_model_name_or_path,\
    \ cache_dir, force_download, resume_download, proxies, token, revision, local_files_only,\
    \ repo_type, code_revision, **kwargs)\n    484 # And lastly we get the class inside\
    \ our newly created module\n    485 final_module = get_cached_module_file(\n \
    \   486     repo_id,\n    487     module_file + \".py\",\n   (...)\n    495  \
    \   repo_type=repo_type,\n    496 )\n--> 497 return get_class_in_module(class_name,\
    \ final_module.replace(\".py\", \"\"))\n\nFile ~/.conda/envs/summr/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:199,\
    \ in get_class_in_module(class_name, module_path)\n    188 \"\"\"\n    189 Import\
    \ a module on the cache directory for modules and extract a class from it.\n \
    \   190 \n   (...)\n    196     `typing.Type`: The class looked for.\n    197\
    \ \"\"\"\n    198 module_path = module_path.replace(os.path.sep, \".\")\n--> 199\
    \ module = importlib.import_module(module_path)\n    200 return getattr(module,\
    \ class_name)\n\nFile ~/.conda/envs/summr/lib/python3.10/importlib/__init__.py:126,\
    \ in import_module(name, package)\n    124             break\n    125        \
    \ level += 1\n--> 126 return _bootstrap._gcd_import(name[level:], package, level)\n\
    \nFile <frozen importlib._bootstrap>:1050, in _gcd_import(name, package, level)\n\
    \nFile <frozen importlib._bootstrap>:1027, in _find_and_load(name, import_)\n\n\
    File <frozen importlib._bootstrap>:992, in _find_and_load_unlocked(name, import_)\n\
    \nFile <frozen importlib._bootstrap>:241, in _call_with_frames_removed(f, *args,\
    \ **kwds)\n\nFile <frozen importlib._bootstrap>:1050, in _gcd_import(name, package,\
    \ level)\n\nFile <frozen importlib._bootstrap>:1027, in _find_and_load(name, import_)\n\
    \nFile <frozen importlib._bootstrap>:992, in _find_and_load_unlocked(name, import_)\n\
    \nFile <frozen importlib._bootstrap>:241, in _call_with_frames_removed(f, *args,\
    \ **kwds)\n\nFile <frozen importlib._bootstrap>:1050, in _gcd_import(name, package,\
    \ level)\n\nFile <frozen importlib._bootstrap>:1027, in _find_and_load(name, import_)\n\
    \nFile <frozen importlib._bootstrap>:992, in _find_and_load_unlocked(name, import_)\n\
    \nFile <frozen importlib._bootstrap>:241, in _call_with_frames_removed(f, *args,\
    \ **kwds)\n\nFile <frozen importlib._bootstrap>:1050, in _gcd_import(name, package,\
    \ level)\n\nFile <frozen importlib._bootstrap>:1027, in _find_and_load(name, import_)\n\
    \nFile <frozen importlib._bootstrap>:1004, in _find_and_load_unlocked(name, import_)\n\
    \nModuleNotFoundError: No module named 'transformers_modules.TheBloke.Airoboros-L2-13B-2'\n\
    \n\n\n\nTransformers == 4.33.1"
  created_at: 2023-09-14 15:07:34+00:00
  edited: true
  hidden: false
  id: 65032fc647e72ff44bb18911
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-14T16:09:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9645100235939026
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>ah damn. This is a long standing Transformers bug. It can''t load
          <code>trust_remote_code=True</code> modules from repos with a <code>.</code>
          in their name.  I''ll have to rename it. I will do that now</p>

          '
        raw: ah damn. This is a long standing Transformers bug. It can't load `trust_remote_code=True`
          modules from repos with a `.` in their name.  I'll have to rename it. I
          will do that now
        updatedAt: '2023-09-14T16:09:21.479Z'
      numEdits: 0
      reactions: []
    id: 650330312e728903c282a488
    type: comment
  author: TheBloke
  content: ah damn. This is a long standing Transformers bug. It can't load `trust_remote_code=True`
    modules from repos with a `.` in their name.  I'll have to rename it. I will do
    that now
  created_at: 2023-09-14 15:09:21+00:00
  edited: false
  hidden: false
  id: 650330312e728903c282a488
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-09-14T17:01:07.000Z'
    data:
      edited: false
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5037460327148438
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: '<p>Ahh okay. Got it. Hence the error "transformers_modules.TheBloke.Airoboros-L2-13B-2".
          Thanks </p>

          '
        raw: 'Ahh okay. Got it. Hence the error "transformers_modules.TheBloke.Airoboros-L2-13B-2".
          Thanks '
        updatedAt: '2023-09-14T17:01:07.359Z'
      numEdits: 0
      reactions: []
    id: 65033c532e728903c284f01c
    type: comment
  author: rjmehta
  content: 'Ahh okay. Got it. Hence the error "transformers_modules.TheBloke.Airoboros-L2-13B-2".
    Thanks '
  created_at: 2023-09-14 16:01:07+00:00
  edited: false
  hidden: false
  id: 65033c532e728903c284f01c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-14T17:02:42.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9839620590209961
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''ve renamed it, please re-test with the new name</p>

          '
        raw: I've renamed it, please re-test with the new name
        updatedAt: '2023-09-14T17:02:42.138Z'
      numEdits: 0
      reactions: []
    id: 65033cb2a450492f841ebf29
    type: comment
  author: TheBloke
  content: I've renamed it, please re-test with the new name
  created_at: 2023-09-14 16:02:42+00:00
  edited: false
  hidden: false
  id: 65033cb2a450492f841ebf29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-09-14T17:33:50.000Z'
    data:
      edited: false
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6850027441978455
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: '<p>It is working now but the model always goes OOM.  Even after changing
          max embeddings to 11k, it goes OOM. I have 4XA10 96GB GPU Mem.</p>

          <p>from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline<br>model_name_or_path
          = "TheBloke/Airoboros-L2-13B-2.1-YaRN-64K-GPTQ"<br>model = AutoModelForCausalLM.from_pretrained(model_name_or_path,<br>                                             device_map="auto",<br>                                             trust_remote_code=True,<br>                                             revision="main")<br>#model
          = model.to_bettertransformer()<br>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,
          use_fast=True)<br>model.max_position_embeddings = 11000<br>import torch<br>from
          auto_gptq import exllama_set_max_input_length<br>model = exllama_set_max_input_length(model,
          11000)<br>input_ids = tokenizer(prompt, return_tensors=''pt'').input_ids.cuda()<br>output
          = model.generate(inputs=input_ids, temperature=0.35,max_new_tokens=500)</p>

          <p>ERROR: OutOfMemoryError: CUDA out of memory. Tried to allocate 16.10
          GiB. GPU 0 has a total capacty of 21.99 GiB of which 9.96 GiB is free. Including
          non-PyTorch memory, this process has 12.02 GiB memory in use. Of the allocated
          memory 11.28 GiB is allocated by PyTorch, and 445.42 MiB is reserved by
          PyTorch but unallocated. If reserved but unallocated memory is large try
          setting max_split_size_mb to avoid fragmentation.  See documentation for
          Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          '
        raw: "It is working now but the model always goes OOM.  Even after changing\
          \ max embeddings to 11k, it goes OOM. I have 4XA10 96GB GPU Mem.\n\nfrom\
          \ transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nmodel_name_or_path\
          \ = \"TheBloke/Airoboros-L2-13B-2.1-YaRN-64K-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \                                             device_map=\"auto\",\n   \
          \                                          trust_remote_code=True,\n   \
          \                                          revision=\"main\")\n#model =\
          \ model.to_bettertransformer()\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\nmodel.max_position_embeddings = 11000\nimport torch\n\
          from auto_gptq import exllama_set_max_input_length\nmodel = exllama_set_max_input_length(model,\
          \ 11000)\ninput_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.35,max_new_tokens=500)\n\
          \n\nERROR: OutOfMemoryError: CUDA out of memory. Tried to allocate 16.10\
          \ GiB. GPU 0 has a total capacty of 21.99 GiB of which 9.96 GiB is free.\
          \ Including non-PyTorch memory, this process has 12.02 GiB memory in use.\
          \ Of the allocated memory 11.28 GiB is allocated by PyTorch, and 445.42\
          \ MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
          \ memory is large try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          \n"
        updatedAt: '2023-09-14T17:33:50.070Z'
      numEdits: 0
      reactions: []
    id: 650343fe00df6073ba64443a
    type: comment
  author: rjmehta
  content: "It is working now but the model always goes OOM.  Even after changing\
    \ max embeddings to 11k, it goes OOM. I have 4XA10 96GB GPU Mem.\n\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer, pipeline\nmodel_name_or_path = \"\
    TheBloke/Airoboros-L2-13B-2.1-YaRN-64K-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
    \                                             device_map=\"auto\",\n         \
    \                                    trust_remote_code=True,\n               \
    \                              revision=\"main\")\n#model = model.to_bettertransformer()\n\
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
    model.max_position_embeddings = 11000\nimport torch\nfrom auto_gptq import exllama_set_max_input_length\n\
    model = exllama_set_max_input_length(model, 11000)\ninput_ids = tokenizer(prompt,\
    \ return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
    \ temperature=0.35,max_new_tokens=500)\n\n\nERROR: OutOfMemoryError: CUDA out\
    \ of memory. Tried to allocate 16.10 GiB. GPU 0 has a total capacty of 21.99 GiB\
    \ of which 9.96 GiB is free. Including non-PyTorch memory, this process has 12.02\
    \ GiB memory in use. Of the allocated memory 11.28 GiB is allocated by PyTorch,\
    \ and 445.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
    \ memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\n"
  created_at: 2023-09-14 16:33:50+00:00
  edited: false
  hidden: false
  id: 650343fe00df6073ba64443a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Airoboros-L2-13B-2_1-YaRN-64K-GPTQ
repo_type: model
status: open
target_branch: null
title: 'ModuleNotFoundError: No module named ''transformers_modules.TheBloke.Airoboros-L2-13B-2'''
