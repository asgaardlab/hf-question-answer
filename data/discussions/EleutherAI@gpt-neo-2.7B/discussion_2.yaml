!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LavGadewar
conflicting_files: null
created_at: 2022-12-22 09:12:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/50bb062f9987788ed7d86ae4d461f9cc.svg
      fullname: Lav Vivek Gadewar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LavGadewar
      type: user
    createdAt: '2022-12-22T09:12:09.000Z'
    data:
      edited: true
      editors:
      - LavGadewar
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/50bb062f9987788ed7d86ae4d461f9cc.svg
          fullname: Lav Vivek Gadewar
          isHf: false
          isPro: false
          name: LavGadewar
          type: user
        html: '<p>there is a parameter like "stop sequence " which is present in  GPT-
          3 is there are similar parameter in  GPT_neo -2.7B model to stop the generation
          of tokens . and will not contain that sequence ?</p>

          '
        raw: there is a parameter like "stop sequence " which is present in  GPT-
          3 is there are similar parameter in  GPT_neo -2.7B model to stop the generation
          of tokens . and will not contain that sequence ?
        updatedAt: '2022-12-22T09:12:53.293Z'
      numEdits: 1
      reactions: []
    id: 63a41f69412fd71fb7ecb106
    type: comment
  author: LavGadewar
  content: there is a parameter like "stop sequence " which is present in  GPT- 3
    is there are similar parameter in  GPT_neo -2.7B model to stop the generation
    of tokens . and will not contain that sequence ?
  created_at: 2022-12-22 09:12:09+00:00
  edited: true
  hidden: false
  id: 63a41f69412fd71fb7ecb106
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/50bb062f9987788ed7d86ae4d461f9cc.svg
      fullname: Lav Vivek Gadewar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LavGadewar
      type: user
    createdAt: '2022-12-22T09:13:10.000Z'
    data:
      status: closed
    id: 63a41fa60cf4daf616686191
    type: status-change
  author: LavGadewar
  created_at: 2022-12-22 09:13:10+00:00
  id: 63a41fa60cf4daf616686191
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/50bb062f9987788ed7d86ae4d461f9cc.svg
      fullname: Lav Vivek Gadewar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LavGadewar
      type: user
    createdAt: '2022-12-22T09:13:18.000Z'
    data:
      status: open
    id: 63a41fae84a6a25c65bdddd6
    type: status-change
  author: LavGadewar
  created_at: 2022-12-22 09:13:18+00:00
  id: 63a41fae84a6a25c65bdddd6
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a813dedbb9e28866a91b27/i2pGYzY1htiY-L3WFPSgl.jpeg?w=200&h=200&f=face
      fullname: appvoid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appvoid
      type: user
    createdAt: '2022-12-27T17:52:29.000Z'
    data:
      edited: false
      editors:
      - appvoid
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a813dedbb9e28866a91b27/i2pGYzY1htiY-L3WFPSgl.jpeg?w=200&h=200&f=face
          fullname: appvoid
          isHf: false
          isPro: false
          name: appvoid
          type: user
        html: "<p>Yes, modify the code as needed:</p>\n<pre><code>import torch; device\
          \ = torch.device(\"cuda\")\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
          \ StoppingCriteria, StoppingCriteriaList\n\nclass KeywordsStoppingCriteria(StoppingCriteria):\n\
          \    def __init__(self, keywords_ids:list):\n        self.keywords = keywords_ids\n\
          \    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor,\
          \ **kwargs) -&gt; bool:\n        if input_ids[0][-1] in self.keywords:\n\
          \            return True\n        return False\n\nsequence = ['\\n','\\\
          n\\n', '.\\n', '.  ', '. \\n', '?', '!']\n\noutput = tokenizer.decode(model.generate(\
          \ \n            **tokenizer( prompt, return_tensors='pt' ).to(device), \n\
          \            top_p=1,\n            top_k=0,\n            temperature=0.2,\n\
          \            max_new_tokens=18,\n            pad_token_id=50256,\n     \
          \       no_repeat_ngram_size = 2,\n            stopping_criteria=StoppingCriteriaList([KeywordsStoppingCriteria([tokenizer.encode(w)[0]\
          \ for w in sequence])]),\n            early_stopping=True,\n           \
          \ do_sample=True,\n            )[0],\n            skip_special_tokens=True\n\
          \        )\n</code></pre>\n"
        raw: "Yes, modify the code as needed:\n```\nimport torch; device = torch.device(\"\
          cuda\")\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria,\
          \ StoppingCriteriaList\n\nclass KeywordsStoppingCriteria(StoppingCriteria):\n\
          \    def __init__(self, keywords_ids:list):\n        self.keywords = keywords_ids\n\
          \    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor,\
          \ **kwargs) -> bool:\n        if input_ids[0][-1] in self.keywords:\n  \
          \          return True\n        return False\n\nsequence = ['\\n','\\n\\\
          n', '.\\n', '.  ', '. \\n', '?', '!']\n\noutput = tokenizer.decode(model.generate(\
          \ \n            **tokenizer( prompt, return_tensors='pt' ).to(device), \n\
          \            top_p=1,\n            top_k=0,\n            temperature=0.2,\n\
          \            max_new_tokens=18,\n            pad_token_id=50256,\n     \
          \       no_repeat_ngram_size = 2,\n            stopping_criteria=StoppingCriteriaList([KeywordsStoppingCriteria([tokenizer.encode(w)[0]\
          \ for w in sequence])]),\n            early_stopping=True,\n           \
          \ do_sample=True,\n            )[0],\n            skip_special_tokens=True\n\
          \        )\n\n```"
        updatedAt: '2022-12-27T17:52:29.700Z'
      numEdits: 0
      reactions: []
    id: 63ab30dde0e2bf05cd4d2ed3
    type: comment
  author: appvoid
  content: "Yes, modify the code as needed:\n```\nimport torch; device = torch.device(\"\
    cuda\")\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria,\
    \ StoppingCriteriaList\n\nclass KeywordsStoppingCriteria(StoppingCriteria):\n\
    \    def __init__(self, keywords_ids:list):\n        self.keywords = keywords_ids\n\
    \    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor,\
    \ **kwargs) -> bool:\n        if input_ids[0][-1] in self.keywords:\n        \
    \    return True\n        return False\n\nsequence = ['\\n','\\n\\n', '.\\n',\
    \ '.  ', '. \\n', '?', '!']\n\noutput = tokenizer.decode(model.generate( \n  \
    \          **tokenizer( prompt, return_tensors='pt' ).to(device), \n         \
    \   top_p=1,\n            top_k=0,\n            temperature=0.2,\n           \
    \ max_new_tokens=18,\n            pad_token_id=50256,\n            no_repeat_ngram_size\
    \ = 2,\n            stopping_criteria=StoppingCriteriaList([KeywordsStoppingCriteria([tokenizer.encode(w)[0]\
    \ for w in sequence])]),\n            early_stopping=True,\n            do_sample=True,\n\
    \            )[0],\n            skip_special_tokens=True\n        )\n\n```"
  created_at: 2022-12-27 17:52:29+00:00
  edited: false
  hidden: false
  id: 63ab30dde0e2bf05cd4d2ed3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: EleutherAI/gpt-neo-2.7B
repo_type: model
status: open
target_branch: null
title: stop sequence
