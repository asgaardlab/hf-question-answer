!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RASMUS
conflicting_files: null
created_at: 2024-01-23 11:42:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652881095493-60d36ba0cec45518d9df5f1c.png?w=200&h=200&f=face
      fullname: TOIVANEN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RASMUS
      type: user
    createdAt: '2024-01-23T11:42:05.000Z'
    data:
      edited: true
      editors:
      - RASMUS
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9834532737731934
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652881095493-60d36ba0cec45518d9df5f1c.png?w=200&h=200&f=face
          fullname: TOIVANEN
          isHf: false
          isPro: false
          name: RASMUS
          type: user
        html: '<p>Hi,</p>

          <p>First of all, great work on Korean models with TPUs. We at Finnish-NLP
          (<a href="https://huggingface.co/Finnish-NLP">https://huggingface.co/Finnish-NLP</a>
          ) are inspired by your work and are planning on doing something similar.
          (Eg. continued pretraining from Mistral for example like here <a rel="nofollow"
          href="https://www.reddit.com/r/LocalLLaMA/comments/174i0vh/em_german_mistral_continous_pretraining/">https://www.reddit.com/r/LocalLLaMA/comments/174i0vh/em_german_mistral_continous_pretraining/</a>
          or some other model)<br>We would be very pleased if you could share how
          you are doing continued pretraining on TPUs (Through TRC program as we also).
          Which framework are you using, how you modify tokenizer etch.</p>

          <p>I asked this kind of question at EleutherAI discord and was adviced to
          look into your work.</p>

          <p>Sincerely,<br>Rasmus T,<br>Finnish-NLP</p>

          '
        raw: 'Hi,


          First of all, great work on Korean models with TPUs. We at Finnish-NLP (https://huggingface.co/Finnish-NLP
          ) are inspired by your work and are planning on doing something similar.
          (Eg. continued pretraining from Mistral for example like here https://www.reddit.com/r/LocalLLaMA/comments/174i0vh/em_german_mistral_continous_pretraining/
          or some other model)

          We would be very pleased if you could share how you are doing continued
          pretraining on TPUs (Through TRC program as we also). Which framework are
          you using, how you modify tokenizer etch.


          I asked this kind of question at EleutherAI discord and was adviced to look
          into your work.


          Sincerely,

          Rasmus T,

          Finnish-NLP'
        updatedAt: '2024-01-23T11:55:25.148Z'
      numEdits: 2
      reactions: []
    id: 65afa60d66ad44e2d5e54c76
    type: comment
  author: RASMUS
  content: 'Hi,


    First of all, great work on Korean models with TPUs. We at Finnish-NLP (https://huggingface.co/Finnish-NLP
    ) are inspired by your work and are planning on doing something similar. (Eg.
    continued pretraining from Mistral for example like here https://www.reddit.com/r/LocalLLaMA/comments/174i0vh/em_german_mistral_continous_pretraining/
    or some other model)

    We would be very pleased if you could share how you are doing continued pretraining
    on TPUs (Through TRC program as we also). Which framework are you using, how you
    modify tokenizer etch.


    I asked this kind of question at EleutherAI discord and was adviced to look into
    your work.


    Sincerely,

    Rasmus T,

    Finnish-NLP'
  created_at: 2024-01-23 11:42:05+00:00
  edited: true
  hidden: false
  id: 65afa60d66ad44e2d5e54c76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
      fullname: Lee Junbum
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: beomi
      type: user
    createdAt: '2024-01-24T09:43:40.000Z'
    data:
      edited: false
      editors:
      - beomi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9428381323814392
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623302508605-5e56829137cb5b49818287ea.jpeg?w=200&h=200&f=face
          fullname: Lee Junbum
          isHf: false
          isPro: false
          name: beomi
          type: user
        html: "<p>Hello Rasmus,</p>\n<p>It's a pleasure to connect with a fellow researcher\
          \ who shares an interest in multilingual projects like me.</p>\n<p>To start\
          \ with, I've been using EasyLM (<a rel=\"nofollow\" href=\"https://github.com/young-geng/EasyLM\"\
          >https://github.com/young-geng/EasyLM</a>), a tool instrumental in training\
          \ OpenLLAMA (<a rel=\"nofollow\" href=\"https://github.com/openlm-research/open_llama\"\
          >https://github.com/openlm-research/open_llama</a>). However, since EasyLM\
          \ doesn't natively support GQA, I took the initiative to implement it myself.</p>\n\
          <p>Regarding tokenization, I've been working with corpora in both Korean\
          \ (like Korean Wiki) and English (such as Pile). I opted for the SentencePiece\
          \ library by Google to develop a new tokenizer from the ground up. This\
          \ involved training on a mixed sample (1%) of Korean and English texts.\
          \ The process led to a refined set of scored vocabularies and merges. I\
          \ specifically focused on Korean vocabulary, using Regex to filter and retain\
          \ the top ~15,000 tokens. This seemed sufficient to cover the majority of\
          \ Korean vocabulary. My aim was to enhance Korean tokenization efficiency,\
          \ so these tokens were integrated into the base model, SOLAR, and other\
          \ models like Llama2. I moved away from the SPM (.model) tokenizer file,\
          \ typically used with the SentencePiece Library, due to its complexity in\
          \ updates. My advice would be to prefer the <code>tokenizer.json</code>\
          \ file for compatibility with Huggingface's Tokenizers, as it can streamline\
          \ your workflow and avoid unnecessary delays.</p>\n<p>In terms of model\
          \ expansion, I worked on enlarging the base model's embeddings and lm_head\
          \ through the Hugging Face Transformers Library. There are multiple ways\
          \ to initialize new vectors, but I recommend using the mean vector approach.\
          \ For instance, if you have a new token \"pajama,\" and the original tokenizer\
          \ splits it into {\"_pa\", \"ja\", \"ma\"}, then the new \"_pajama\" token\
          \ should be initialized with the average of these component vectors. This\
          \ approach tends to stabilize training during the warm-up phase, reducing\
          \ the likelihood of significant forgetting in the base model.</p>\n<p>The\
          \ rest of the process mirrors what you're already familiar with \u2013 training\
          \ the model with your specific corpus.</p>\n<p>I sincerely hope you find\
          \ this information useful for your work. </p>\n<p>Best regards,<br>Junbum\
          \ Lee</p>\n"
        raw: "Hello Rasmus,\n\nIt's a pleasure to connect with a fellow researcher\
          \ who shares an interest in multilingual projects like me.\n\nTo start with,\
          \ I've been using EasyLM (https://github.com/young-geng/EasyLM), a tool\
          \ instrumental in training OpenLLAMA (https://github.com/openlm-research/open_llama).\
          \ However, since EasyLM doesn't natively support GQA, I took the initiative\
          \ to implement it myself.\n\nRegarding tokenization, I've been working with\
          \ corpora in both Korean (like Korean Wiki) and English (such as Pile).\
          \ I opted for the SentencePiece library by Google to develop a new tokenizer\
          \ from the ground up. This involved training on a mixed sample (1%) of Korean\
          \ and English texts. The process led to a refined set of scored vocabularies\
          \ and merges. I specifically focused on Korean vocabulary, using Regex to\
          \ filter and retain the top ~15,000 tokens. This seemed sufficient to cover\
          \ the majority of Korean vocabulary. My aim was to enhance Korean tokenization\
          \ efficiency, so these tokens were integrated into the base model, SOLAR,\
          \ and other models like Llama2. I moved away from the SPM (.model) tokenizer\
          \ file, typically used with the SentencePiece Library, due to its complexity\
          \ in updates. My advice would be to prefer the `tokenizer.json` file for\
          \ compatibility with Huggingface's Tokenizers, as it can streamline your\
          \ workflow and avoid unnecessary delays.\n\nIn terms of model expansion,\
          \ I worked on enlarging the base model's embeddings and lm_head through\
          \ the Hugging Face Transformers Library. There are multiple ways to initialize\
          \ new vectors, but I recommend using the mean vector approach. For instance,\
          \ if you have a new token \"pajama,\" and the original tokenizer splits\
          \ it into {\"_pa\", \"ja\", \"ma\"}, then the new \"_pajama\" token should\
          \ be initialized with the average of these component vectors. This approach\
          \ tends to stabilize training during the warm-up phase, reducing the likelihood\
          \ of significant forgetting in the base model.\n\nThe rest of the process\
          \ mirrors what you're already familiar with \u2013 training the model with\
          \ your specific corpus.\n\nI sincerely hope you find this information useful\
          \ for your work. \n\nBest regards,\nJunbum Lee"
        updatedAt: '2024-01-24T09:43:40.062Z'
      numEdits: 0
      reactions: []
    id: 65b0dbcc6f4ce85ebfd61dac
    type: comment
  author: beomi
  content: "Hello Rasmus,\n\nIt's a pleasure to connect with a fellow researcher who\
    \ shares an interest in multilingual projects like me.\n\nTo start with, I've\
    \ been using EasyLM (https://github.com/young-geng/EasyLM), a tool instrumental\
    \ in training OpenLLAMA (https://github.com/openlm-research/open_llama). However,\
    \ since EasyLM doesn't natively support GQA, I took the initiative to implement\
    \ it myself.\n\nRegarding tokenization, I've been working with corpora in both\
    \ Korean (like Korean Wiki) and English (such as Pile). I opted for the SentencePiece\
    \ library by Google to develop a new tokenizer from the ground up. This involved\
    \ training on a mixed sample (1%) of Korean and English texts. The process led\
    \ to a refined set of scored vocabularies and merges. I specifically focused on\
    \ Korean vocabulary, using Regex to filter and retain the top ~15,000 tokens.\
    \ This seemed sufficient to cover the majority of Korean vocabulary. My aim was\
    \ to enhance Korean tokenization efficiency, so these tokens were integrated into\
    \ the base model, SOLAR, and other models like Llama2. I moved away from the SPM\
    \ (.model) tokenizer file, typically used with the SentencePiece Library, due\
    \ to its complexity in updates. My advice would be to prefer the `tokenizer.json`\
    \ file for compatibility with Huggingface's Tokenizers, as it can streamline your\
    \ workflow and avoid unnecessary delays.\n\nIn terms of model expansion, I worked\
    \ on enlarging the base model's embeddings and lm_head through the Hugging Face\
    \ Transformers Library. There are multiple ways to initialize new vectors, but\
    \ I recommend using the mean vector approach. For instance, if you have a new\
    \ token \"pajama,\" and the original tokenizer splits it into {\"_pa\", \"ja\"\
    , \"ma\"}, then the new \"_pajama\" token should be initialized with the average\
    \ of these component vectors. This approach tends to stabilize training during\
    \ the warm-up phase, reducing the likelihood of significant forgetting in the\
    \ base model.\n\nThe rest of the process mirrors what you're already familiar\
    \ with \u2013 training the model with your specific corpus.\n\nI sincerely hope\
    \ you find this information useful for your work. \n\nBest regards,\nJunbum Lee"
  created_at: 2024-01-24 09:43:40+00:00
  edited: false
  hidden: false
  id: 65b0dbcc6f4ce85ebfd61dac
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: beomi/OPEN-SOLAR-KO-10.7B
repo_type: model
status: open
target_branch: null
title: TPU continued pretraining, questions
