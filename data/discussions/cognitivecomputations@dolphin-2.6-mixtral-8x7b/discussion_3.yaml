!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FiditeNemini
conflicting_files: null
created_at: 2023-12-22 23:17:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642a59708136224fee0c8ec1/w2q6ohxM2Mc_WU-d11kAi.png?w=200&h=200&f=face
      fullname: Fidite Nemini
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FiditeNemini
      type: user
    createdAt: '2023-12-22T23:17:07.000Z'
    data:
      edited: false
      editors:
      - FiditeNemini
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9009228348731995
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642a59708136224fee0c8ec1/w2q6ohxM2Mc_WU-d11kAi.png?w=200&h=200&f=face
          fullname: Fidite Nemini
          isHf: false
          isPro: false
          name: FiditeNemini
          type: user
        html: '<p>Hi,<br>I''ve noticed that the model''s output seems to repeat the
          last sentence infinitely, or just output random words in the final sentence,
          every one or two interactions.  This behaviour seems to be specific to the
          Dolphin 2.6 Mixtral version.  I''m using both 2.5 and 2.6, both Q8.0 GGUF
          quants from Tom.  I''m running both using LMStudio on MacOS, using exactly
          the same config.  System prompt is the default  suggested in this repo (save
          the kittens makes me laugh every time), ChatML prompt format, temp 0.8,
          tokens -1, top_k 40, repeat_penalty 1.1 (tried up to 1.5, no difference),
          min_p 0.05, top_p 0.95, n_ctx 32768.  I have noticed this behaviour on other
          Mixtral-based models in the past, but Dolphin 2.5 Mixtral seemed to fix
          it.  Any ideas on how to fix this repeating/random gibberish problem?  Any
          help/suggestions appreciated.<br>Thanks,<br>Will.</p>

          '
        raw: "Hi,\r\nI've noticed that the model's output seems to repeat the last\
          \ sentence infinitely, or just output random words in the final sentence,\
          \ every one or two interactions.  This behaviour seems to be specific to\
          \ the Dolphin 2.6 Mixtral version.  I'm using both 2.5 and 2.6, both Q8.0\
          \ GGUF quants from Tom.  I'm running both using LMStudio on MacOS, using\
          \ exactly the same config.  System prompt is the default  suggested in this\
          \ repo (save the kittens makes me laugh every time), ChatML prompt format,\
          \ temp 0.8, tokens -1, top_k 40, repeat_penalty 1.1 (tried up to 1.5, no\
          \ difference), min_p 0.05, top_p 0.95, n_ctx 32768.  I have noticed this\
          \ behaviour on other Mixtral-based models in the past, but Dolphin 2.5 Mixtral\
          \ seemed to fix it.  Any ideas on how to fix this repeating/random gibberish\
          \ problem?  Any help/suggestions appreciated.\r\nThanks,\r\nWill."
        updatedAt: '2023-12-22T23:17:07.446Z'
      numEdits: 0
      reactions: []
    id: 658618f3af21ea88a4eeb1fc
    type: comment
  author: FiditeNemini
  content: "Hi,\r\nI've noticed that the model's output seems to repeat the last sentence\
    \ infinitely, or just output random words in the final sentence, every one or\
    \ two interactions.  This behaviour seems to be specific to the Dolphin 2.6 Mixtral\
    \ version.  I'm using both 2.5 and 2.6, both Q8.0 GGUF quants from Tom.  I'm running\
    \ both using LMStudio on MacOS, using exactly the same config.  System prompt\
    \ is the default  suggested in this repo (save the kittens makes me laugh every\
    \ time), ChatML prompt format, temp 0.8, tokens -1, top_k 40, repeat_penalty 1.1\
    \ (tried up to 1.5, no difference), min_p 0.05, top_p 0.95, n_ctx 32768.  I have\
    \ noticed this behaviour on other Mixtral-based models in the past, but Dolphin\
    \ 2.5 Mixtral seemed to fix it.  Any ideas on how to fix this repeating/random\
    \ gibberish problem?  Any help/suggestions appreciated.\r\nThanks,\r\nWill."
  created_at: 2023-12-22 23:17:07+00:00
  edited: false
  hidden: false
  id: 658618f3af21ea88a4eeb1fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1135a65a8922b14b5bb7e1ff58ddbfdf.svg
      fullname: The Grease Machine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Enginseer-Rogal
      type: user
    createdAt: '2023-12-23T05:48:42.000Z'
    data:
      edited: false
      editors:
      - Enginseer-Rogal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9850346446037292
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1135a65a8922b14b5bb7e1ff58ddbfdf.svg
          fullname: The Grease Machine
          isHf: false
          isPro: false
          name: Enginseer-Rogal
          type: user
        html: '<p>I noticed a similar issue with the Q5 and Q4_K_M, every GGUF quant
          in fact ended up repeating lines after only a few generations.<br>But I
          had no such problems with the GPTQ quants, in fact with those I had to tone
          down repetition penalty quite a bit.</p>

          '
        raw: "I noticed a similar issue with the Q5 and Q4_K_M, every GGUF quant in\
          \ fact ended up repeating lines after only a few generations. \nBut I had\
          \ no such problems with the GPTQ quants, in fact with those I had to tone\
          \ down repetition penalty quite a bit."
        updatedAt: '2023-12-23T05:48:42.276Z'
      numEdits: 0
      reactions: []
    id: 658674ba92c8cc471f164480
    type: comment
  author: Enginseer-Rogal
  content: "I noticed a similar issue with the Q5 and Q4_K_M, every GGUF quant in\
    \ fact ended up repeating lines after only a few generations. \nBut I had no such\
    \ problems with the GPTQ quants, in fact with those I had to tone down repetition\
    \ penalty quite a bit."
  created_at: 2023-12-23 05:48:42+00:00
  edited: false
  hidden: false
  id: 658674ba92c8cc471f164480
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd4e93b9b50cc2850afdceb96696f147.svg
      fullname: Maddog
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Madd0g
      type: user
    createdAt: '2023-12-23T21:05:29.000Z'
    data:
      edited: false
      editors:
      - Madd0g
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9942217469215393
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd4e93b9b50cc2850afdceb96696f147.svg
          fullname: Maddog
          isHf: false
          isPro: false
          name: Madd0g
          type: user
        html: '<p>I''ve had this problem with every GGUF I''ve tried of mixtral finetunes,
          including dolphin 2.5. I would be thrilled if anyone has insights.</p>

          '
        raw: I've had this problem with every GGUF I've tried of mixtral finetunes,
          including dolphin 2.5. I would be thrilled if anyone has insights.
        updatedAt: '2023-12-23T21:05:29.732Z'
      numEdits: 0
      reactions: []
    id: 65874b991b44d0e694fe3930
    type: comment
  author: Madd0g
  content: I've had this problem with every GGUF I've tried of mixtral finetunes,
    including dolphin 2.5. I would be thrilled if anyone has insights.
  created_at: 2023-12-23 21:05:29+00:00
  edited: false
  hidden: false
  id: 65874b991b44d0e694fe3930
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1135a65a8922b14b5bb7e1ff58ddbfdf.svg
      fullname: The Grease Machine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Enginseer-Rogal
      type: user
    createdAt: '2023-12-23T23:22:47.000Z'
    data:
      edited: false
      editors:
      - Enginseer-Rogal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9863008856773376
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1135a65a8922b14b5bb7e1ff58ddbfdf.svg
          fullname: The Grease Machine
          isHf: false
          isPro: false
          name: Enginseer-Rogal
          type: user
        html: '<p>I heard it mentioned somewhere that MoE models were considerably
          more sensitive to learning rate, Possible contributing factor?</p>

          '
        raw: I heard it mentioned somewhere that MoE models were considerably more
          sensitive to learning rate, Possible contributing factor?
        updatedAt: '2023-12-23T23:22:47.493Z'
      numEdits: 0
      reactions: []
    id: 65876bc72021ba68d775dd2d
    type: comment
  author: Enginseer-Rogal
  content: I heard it mentioned somewhere that MoE models were considerably more sensitive
    to learning rate, Possible contributing factor?
  created_at: 2023-12-23 23:22:47+00:00
  edited: false
  hidden: false
  id: 65876bc72021ba68d775dd2d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: cognitivecomputations/dolphin-2.6-mixtral-8x7b
repo_type: model
status: open
target_branch: null
title: Odd repeating behaviour on Q8 GGUF
