!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joorei
conflicting_files: null
created_at: 2023-12-22 20:31:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa3f84916189294a07623f1df5859916.svg
      fullname: Juraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joorei
      type: user
    createdAt: '2023-12-22T20:31:12.000Z'
    data:
      edited: false
      editors:
      - joorei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5720948576927185
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa3f84916189294a07623f1df5859916.svg
          fullname: Juraj
          isHf: false
          isPro: false
          name: joorei
          type: user
        html: "<p>Hello,</p>\n<p>I am fine-tuning dolphin-mixtral with axolotl. I\
          \ am inspired by your config, I chose qlora modules. What is interesting,\
          \ that I can fine-tune dolphin-2.5-mixtral-8x7b, but when I just change\
          \ \"5\" to \"6\" and otherwise keep the config the same (and remove and\
          \ recreate the output directory), I get the following error:</p>\n<pre><code>\
          \ File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1854, in _inner_training_loop\n    tr_loss_step = self.training_step(model,\
          \ inputs)\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2735, in training_step\n    loss = self.compute_loss(model, inputs)\n\
          \  File \"/workspace/axolotl/src/axolotl/core/trainer_builder.py\", line\
          \ 291, in compute_loss\n    return super().compute_loss(model, inputs, return_outputs=return_outputs)\n\
          \  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2758, in compute_loss\n    outputs = model(**inputs)\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/utils/operations.py\"\
          , line 659, in forward\n    return model_forward(*args, **kwargs)\n  File\
          \ \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/utils/operations.py\"\
          , line 647, in __call__\n    return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/amp/autocast_mode.py\"\
          , line 14, in decorate_autocast\n    return func(*args, **kwargs)\n  File\
          \ \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/peft/peft_model.py\"\
          , line 977, in forward\n    return self.base_model(\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\"\
          , line 106, in forward\n    return self.model.forward(*args, **kwargs)\n\
          \  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 164, in new_forward\n    output = module._old_forward(*args, **kwargs)\n\
          \  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\"\
          , line 1258, in forward\n    loss += self.router_aux_loss_coef * aux_loss\n\
          RuntimeError: Expected all tensors to be on the same device, but found at\
          \ least two devices, cuda:6 and cuda:0!\n</code></pre>\n<p>Any idea what\
          \ could be different about 2.5 vs 2.6 that could cause this?</p>\n"
        raw: "Hello,\r\n\r\nI am fine-tuning dolphin-mixtral with axolotl. I am inspired\
          \ by your config, I chose qlora modules. What is interesting, that I can\
          \ fine-tune dolphin-2.5-mixtral-8x7b, but when I just change \"5\" to \"\
          6\" and otherwise keep the config the same (and remove and recreate the\
          \ output directory), I get the following error:\r\n\r\n\r\n```\r\n File\
          \ \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 1854, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model,\
          \ inputs)\r\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2735, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\
          \n  File \"/workspace/axolotl/src/axolotl/core/trainer_builder.py\", line\
          \ 291, in compute_loss\r\n    return super().compute_loss(model, inputs,\
          \ return_outputs=return_outputs)\r\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py\"\
          , line 2758, in compute_loss\r\n    outputs = model(**inputs)\r\n  File\
          \ \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/utils/operations.py\"\
          , line 659, in forward\r\n    return model_forward(*args, **kwargs)\r\n\
          \  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/utils/operations.py\"\
          , line 647, in __call__\r\n    return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))\r\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/amp/autocast_mode.py\"\
          , line 14, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/peft/peft_model.py\"\
          , line 977, in forward\r\n    return self.base_model(\r\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\"\
          , line 106, in forward\r\n    return self.model.forward(*args, **kwargs)\r\
          \n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 164, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\
          \n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\"\
          , line 1258, in forward\r\n    loss += self.router_aux_loss_coef * aux_loss\r\
          \nRuntimeError: Expected all tensors to be on the same device, but found\
          \ at least two devices, cuda:6 and cuda:0!\r\n```\r\n\r\nAny idea what could\
          \ be different about 2.5 vs 2.6 that could cause this?"
        updatedAt: '2023-12-22T20:31:12.998Z'
      numEdits: 0
      reactions: []
    id: 6585f210240d604c8195960c
    type: comment
  author: joorei
  content: "Hello,\r\n\r\nI am fine-tuning dolphin-mixtral with axolotl. I am inspired\
    \ by your config, I chose qlora modules. What is interesting, that I can fine-tune\
    \ dolphin-2.5-mixtral-8x7b, but when I just change \"5\" to \"6\" and otherwise\
    \ keep the config the same (and remove and recreate the output directory), I get\
    \ the following error:\r\n\r\n\r\n```\r\n File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 1854, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model,\
    \ inputs)\r\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 2735, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\
    \n  File \"/workspace/axolotl/src/axolotl/core/trainer_builder.py\", line 291,\
    \ in compute_loss\r\n    return super().compute_loss(model, inputs, return_outputs=return_outputs)\r\
    \n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py\"\
    , line 2758, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/utils/operations.py\"\
    , line 659, in forward\r\n    return model_forward(*args, **kwargs)\r\n  File\
    \ \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/utils/operations.py\"\
    , line 647, in __call__\r\n    return convert_to_fp32(self.model_forward(*args,\
    \ **kwargs))\r\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/amp/autocast_mode.py\"\
    , line 14, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/peft/peft_model.py\"\
    , line 977, in forward\r\n    return self.base_model(\r\n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\"\
    , line 106, in forward\r\n    return self.model.forward(*args, **kwargs)\r\n \
    \ File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 164, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\
    \n  File \"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\"\
    , line 1258, in forward\r\n    loss += self.router_aux_loss_coef * aux_loss\r\n\
    RuntimeError: Expected all tensors to be on the same device, but found at least\
    \ two devices, cuda:6 and cuda:0!\r\n```\r\n\r\nAny idea what could be different\
    \ about 2.5 vs 2.6 that could cause this?"
  created_at: 2023-12-22 20:31:12+00:00
  edited: false
  hidden: false
  id: 6585f210240d604c8195960c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
      fullname: God
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gqd
      type: user
    createdAt: '2023-12-27T13:09:27.000Z'
    data:
      edited: false
      editors:
      - gqd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6268748044967651
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
          fullname: God
          isHf: false
          isPro: false
          name: gqd
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/OpenAccess-AI-Collective/axolotl/issues/998#issuecomment-1869140534">https://github.com/OpenAccess-AI-Collective/axolotl/issues/998#issuecomment-1869140534</a>
          might be related</p>

          '
        raw: https://github.com/OpenAccess-AI-Collective/axolotl/issues/998#issuecomment-1869140534
          might be related
        updatedAt: '2023-12-27T13:09:27.028Z'
      numEdits: 0
      reactions: []
    id: 658c2207a6d1ddb4005264c0
    type: comment
  author: gqd
  content: https://github.com/OpenAccess-AI-Collective/axolotl/issues/998#issuecomment-1869140534
    might be related
  created_at: 2023-12-27 13:09:27+00:00
  edited: false
  hidden: false
  id: 658c2207a6d1ddb4005264c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa3f84916189294a07623f1df5859916.svg
      fullname: Juraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joorei
      type: user
    createdAt: '2024-01-04T14:38:40.000Z'
    data:
      edited: false
      editors:
      - joorei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9376845359802246
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa3f84916189294a07623f1df5859916.svg
          fullname: Juraj
          isHf: false
          isPro: false
          name: joorei
          type: user
        html: '<p>2.7 is the same btw.</p>

          <p>I''m trying, but changing output_router_logits to false does not help</p>

          '
        raw: '2.7 is the same btw.


          I''m trying, but changing output_router_logits to false does not help'
        updatedAt: '2024-01-04T14:38:40.657Z'
      numEdits: 0
      reactions: []
    id: 6596c2f0282cb769eb66baf2
    type: comment
  author: joorei
  content: '2.7 is the same btw.


    I''m trying, but changing output_router_logits to false does not help'
  created_at: 2024-01-04 14:38:40+00:00
  edited: false
  hidden: false
  id: 6596c2f0282cb769eb66baf2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: cognitivecomputations/dolphin-2.6-mixtral-8x7b
repo_type: model
status: open
target_branch: null
title: Weird fine-tuning problem
