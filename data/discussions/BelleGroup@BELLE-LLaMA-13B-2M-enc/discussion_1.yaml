!!python/object:huggingface_hub.community.DiscussionWithDetails
author: doublelift
conflicting_files: null
created_at: 2023-04-13 05:46:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba571846402807e54ce2f186bb34bfa3.svg
      fullname: Sheng Xu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: doublelift
      type: user
    createdAt: '2023-04-13T06:46:50.000Z'
    data:
      edited: false
      editors:
      - doublelift
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba571846402807e54ce2f186bb34bfa3.svg
          fullname: Sheng Xu
          isHf: false
          isPro: false
          name: doublelift
          type: user
        html: "<p>ValueError: Couldn't instantiate the backend tokenizer from one\
          \ of:<br>(1) a <code>tokenizers</code> library serialization file,<br>(2)\
          \ a slow tokenizer instance to convert or<br>(3) an equivalent slow tokenizer\
          \ class to instantiate and convert.<br>You need to have sentencepiece installed\
          \ to convert a slow tokenizer to a fast one.</p>\n<p>\u5DF2\u7ECF\u5B89\u88C5\
          \u4E86sentencepiece\uFF0C\u4F46\u662F\u8FD8\u662F\u62A5\u9519\u4E86\u3002\
          <br>\u5C1D\u8BD5\u4F7F\u7528LlamaTokenizer\u4EE3\u66FFAutoTokenizer\u5219\
          \u8FD0\u884C\u6210\u529F\u4E86\u3002<br>\u8BF7\u95EE\u8FD9\u6837\u7684\u89E3\
          \u51B3\u65B9\u6848\u5408\u7406\u4E48\uFF0C\u95EE\u9898\u51FA\u5728\u54EA\
          \u91CC\uFF1F</p>\n"
        raw: "ValueError: Couldn't instantiate the backend tokenizer from one of:\
          \ \r\n(1) a `tokenizers` library serialization file, \r\n(2) a slow tokenizer\
          \ instance to convert or \r\n(3) an equivalent slow tokenizer class to instantiate\
          \ and convert. \r\nYou need to have sentencepiece installed to convert a\
          \ slow tokenizer to a fast one.\r\n\r\n\u5DF2\u7ECF\u5B89\u88C5\u4E86sentencepiece\uFF0C\
          \u4F46\u662F\u8FD8\u662F\u62A5\u9519\u4E86\u3002\r\n\u5C1D\u8BD5\u4F7F\u7528\
          LlamaTokenizer\u4EE3\u66FFAutoTokenizer\u5219\u8FD0\u884C\u6210\u529F\u4E86\
          \u3002\r\n\u8BF7\u95EE\u8FD9\u6837\u7684\u89E3\u51B3\u65B9\u6848\u5408\u7406\
          \u4E48\uFF0C\u95EE\u9898\u51FA\u5728\u54EA\u91CC\uFF1F"
        updatedAt: '2023-04-13T06:46:50.979Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - Hinova
    id: 6437a55a1ee0e43f14d377e3
    type: comment
  author: doublelift
  content: "ValueError: Couldn't instantiate the backend tokenizer from one of: \r\
    \n(1) a `tokenizers` library serialization file, \r\n(2) a slow tokenizer instance\
    \ to convert or \r\n(3) an equivalent slow tokenizer class to instantiate and\
    \ convert. \r\nYou need to have sentencepiece installed to convert a slow tokenizer\
    \ to a fast one.\r\n\r\n\u5DF2\u7ECF\u5B89\u88C5\u4E86sentencepiece\uFF0C\u4F46\
    \u662F\u8FD8\u662F\u62A5\u9519\u4E86\u3002\r\n\u5C1D\u8BD5\u4F7F\u7528LlamaTokenizer\u4EE3\
    \u66FFAutoTokenizer\u5219\u8FD0\u884C\u6210\u529F\u4E86\u3002\r\n\u8BF7\u95EE\u8FD9\
    \u6837\u7684\u89E3\u51B3\u65B9\u6848\u5408\u7406\u4E48\uFF0C\u95EE\u9898\u51FA\
    \u5728\u54EA\u91CC\uFF1F"
  created_at: 2023-04-13 05:46:50+00:00
  edited: false
  hidden: false
  id: 6437a55a1ee0e43f14d377e3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: BelleGroup/BELLE-LLaMA-13B-2M-enc
repo_type: model
status: open
target_branch: null
title: 'ValueError: Couldn''t instantiate the backend tokenizer from one of: '
