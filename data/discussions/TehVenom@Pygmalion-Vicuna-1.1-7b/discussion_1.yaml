!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nixudos
conflicting_files: null
created_at: 2023-05-03 18:51:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c338032633ae561c71f57e83d6b5a5f.svg
      fullname: Nikolaj Sorensen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nixudos
      type: user
    createdAt: '2023-05-03T19:51:08.000Z'
    data:
      edited: false
      editors:
      - nixudos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c338032633ae561c71f57e83d6b5a5f.svg
          fullname: Nikolaj Sorensen
          isHf: false
          isPro: false
          name: nixudos
          type: user
        html: '<p>Thank you for this model! I was looking for a mix exacly like this
          and thought a Pymalion Lora might be the only solution, but this blend seems
          to do what I was loking for.<br>I only have 12 gb VRAM so it runs a bit
          slower than the 4bit 128g models I have played with, but the result is much
          more coherent so far.<br>Do you know if a 4bit 128g model would lose a lot
          of it''s fidelity?</p>

          '
        raw: "Thank you for this model! I was looking for a mix exacly like this and\
          \ thought a Pymalion Lora might be the only solution, but this blend seems\
          \ to do what I was loking for.\r\nI only have 12 gb VRAM so it runs a bit\
          \ slower than the 4bit 128g models I have played with, but the result is\
          \ much more coherent so far.\r\nDo you know if a 4bit 128g model would lose\
          \ a lot of it's fidelity?"
        updatedAt: '2023-05-03T19:51:08.526Z'
      numEdits: 0
      reactions: []
    id: 6452bb2c0a19adf0445fc188
    type: comment
  author: nixudos
  content: "Thank you for this model! I was looking for a mix exacly like this and\
    \ thought a Pymalion Lora might be the only solution, but this blend seems to\
    \ do what I was loking for.\r\nI only have 12 gb VRAM so it runs a bit slower\
    \ than the 4bit 128g models I have played with, but the result is much more coherent\
    \ so far.\r\nDo you know if a 4bit 128g model would lose a lot of it's fidelity?"
  created_at: 2023-05-03 18:51:08+00:00
  edited: false
  hidden: false
  id: 6452bb2c0a19adf0445fc188
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
      fullname: TeH_Venom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TehVenom
      type: user
    createdAt: '2023-05-03T22:06:40.000Z'
    data:
      edited: false
      editors:
      - TehVenom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
          fullname: TeH_Venom
          isHf: false
          isPro: false
          name: TehVenom
          type: user
        html: '<p>I''ve seen people mention having a noticeable improvement on response
          quality depending on quantization. I plan to quantize these mixes to 4bit
          with a 32 group size, which peaks a little below 10GB VRAM usage at max
          context, so hopefully it maintains more of it''s big brother''s coherence
          while still being usable on most cards.</p>

          '
        raw: I've seen people mention having a noticeable improvement on response
          quality depending on quantization. I plan to quantize these mixes to 4bit
          with a 32 group size, which peaks a little below 10GB VRAM usage at max
          context, so hopefully it maintains more of it's big brother's coherence
          while still being usable on most cards.
        updatedAt: '2023-05-03T22:06:40.195Z'
      numEdits: 0
      reactions: []
    id: 6452daf05ac68a5b01a7fbb1
    type: comment
  author: TehVenom
  content: I've seen people mention having a noticeable improvement on response quality
    depending on quantization. I plan to quantize these mixes to 4bit with a 32 group
    size, which peaks a little below 10GB VRAM usage at max context, so hopefully
    it maintains more of it's big brother's coherence while still being usable on
    most cards.
  created_at: 2023-05-03 21:06:40+00:00
  edited: false
  hidden: false
  id: 6452daf05ac68a5b01a7fbb1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
      fullname: TeH_Venom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TehVenom
      type: user
    createdAt: '2023-05-03T22:08:04.000Z'
    data:
      edited: false
      editors:
      - TehVenom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
          fullname: TeH_Venom
          isHf: false
          isPro: false
          name: TehVenom
          type: user
        html: '<p>By the way, any feedback into how this model feels compared to base
          Pygmalion-7b, or even Metharme-7b? I haven''t had the chance to really test
          it myself yet.</p>

          '
        raw: By the way, any feedback into how this model feels compared to base Pygmalion-7b,
          or even Metharme-7b? I haven't had the chance to really test it myself yet.
        updatedAt: '2023-05-03T22:08:04.093Z'
      numEdits: 0
      reactions: []
    id: 6452db448fe6558e32861715
    type: comment
  author: TehVenom
  content: By the way, any feedback into how this model feels compared to base Pygmalion-7b,
    or even Metharme-7b? I haven't had the chance to really test it myself yet.
  created_at: 2023-05-03 21:08:04+00:00
  edited: false
  hidden: false
  id: 6452db448fe6558e32861715
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c338032633ae561c71f57e83d6b5a5f.svg
      fullname: Nikolaj Sorensen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nixudos
      type: user
    createdAt: '2023-05-04T19:21:52.000Z'
    data:
      edited: false
      editors:
      - nixudos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c338032633ae561c71f57e83d6b5a5f.svg
          fullname: Nikolaj Sorensen
          isHf: false
          isPro: false
          name: nixudos
          type: user
        html: '<p>I tested it with a storywriting experiment (NSFW), where I gave
          it a 258 token prompt; A story title, 5 lines on character and situation
          description and a couple of lines of the start of the story.<br>And it behaved
          much mre coherent than regular pygamilion 7b and more "colorful" than the
          free Vicuna. I only got to run a couple of scenarios that I have tried with
          other uncensored models and pygmalion, but I felt there were a markedly
          difference in quality!<br>If you get a 4bit version done that I can run
          exclusively on GPU, I''ll be happy to test Cards and RP as well!</p>

          '
        raw: 'I tested it with a storywriting experiment (NSFW), where I gave it a
          258 token prompt; A story title, 5 lines on character and situation description
          and a couple of lines of the start of the story.

          And it behaved much mre coherent than regular pygamilion 7b and more "colorful"
          than the free Vicuna. I only got to run a couple of scenarios that I have
          tried with other uncensored models and pygmalion, but I felt there were
          a markedly difference in quality!

          If you get a 4bit version done that I can run exclusively on GPU, I''ll
          be happy to test Cards and RP as well!'
        updatedAt: '2023-05-04T19:21:52.770Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - XYNTAXIS
    id: 645405d0b8c58783d66990a3
    type: comment
  author: nixudos
  content: 'I tested it with a storywriting experiment (NSFW), where I gave it a 258
    token prompt; A story title, 5 lines on character and situation description and
    a couple of lines of the start of the story.

    And it behaved much mre coherent than regular pygamilion 7b and more "colorful"
    than the free Vicuna. I only got to run a couple of scenarios that I have tried
    with other uncensored models and pygmalion, but I felt there were a markedly difference
    in quality!

    If you get a 4bit version done that I can run exclusively on GPU, I''ll be happy
    to test Cards and RP as well!'
  created_at: 2023-05-04 18:21:52+00:00
  edited: false
  hidden: false
  id: 645405d0b8c58783d66990a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/798e2e78ad3813be39b0dd851410345a.svg
      fullname: Vlad Orlov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vlad100
      type: user
    createdAt: '2023-05-07T20:44:22.000Z'
    data:
      edited: true
      editors:
      - Vlad100
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/798e2e78ad3813be39b0dd851410345a.svg
          fullname: Vlad Orlov
          isHf: false
          isPro: false
          name: Vlad100
          type: user
        html: '<blockquote>

          <p>By the way, any feedback into how this model feels compared to base Pygmalion-7b,
          or even Metharme-7b? I haven''t had the chance to really test it myself
          yet.</p>

          </blockquote>

          <p>The mix is very successful. I would like to try ggml version. With quantization
          5.1 or higher. Is this possible?</p>

          '
        raw: '> By the way, any feedback into how this model feels compared to base
          Pygmalion-7b, or even Metharme-7b? I haven''t had the chance to really test
          it myself yet.


          The mix is very successful. I would like to try ggml version. With quantization
          5.1 or higher. Is this possible?'
        updatedAt: '2023-05-07T20:45:45.221Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - XYNTAXIS
    id: 64580da65fc3b8a21ea63732
    type: comment
  author: Vlad100
  content: '> By the way, any feedback into how this model feels compared to base
    Pygmalion-7b, or even Metharme-7b? I haven''t had the chance to really test it
    myself yet.


    The mix is very successful. I would like to try ggml version. With quantization
    5.1 or higher. Is this possible?'
  created_at: 2023-05-07 19:44:22+00:00
  edited: true
  hidden: false
  id: 64580da65fc3b8a21ea63732
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TehVenom/Pygmalion-Vicuna-1.1-7b
repo_type: model
status: open
target_branch: null
title: Work of art!
