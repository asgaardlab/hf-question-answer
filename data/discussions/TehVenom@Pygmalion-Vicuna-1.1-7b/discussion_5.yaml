!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sharpy66
conflicting_files: null
created_at: 2023-05-21 08:23:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aeb455bf713d728c968271c95d3327c.svg
      fullname: Luke Defriez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sharpy66
      type: user
    createdAt: '2023-05-21T09:23:55.000Z'
    data:
      edited: true
      editors:
      - Sharpy66
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aeb455bf713d728c968271c95d3327c.svg
          fullname: Luke Defriez
          isHf: false
          isPro: false
          name: Sharpy66
          type: user
        html: '<p>I tried to quantize this model to 4 bits, but I consistently run
          out of vram when quantizing layer 27/32. If somebody else could quantize
          the model, or suggest a way for me to do it myself, that would be great.
          </p>

          <p>I tried running gptq-for-llama on my own computer... out of memory on
          layer 27/32,<br>tried running autogptq... Finished, but doesn''t save file
          due to error with cpu offloading on autogptq</p>

          <p>Tried running gptq-for-llama on a Google colab.... not enough ram.<br>Tried
          running autogptq... seems to succeed but refuses to save the output file...</p>

          '
        raw: "I tried to quantize this model to 4 bits, but I consistently run out\
          \ of vram when quantizing layer 27/32. If somebody else could quantize the\
          \ model, or suggest a way for me to do it myself, that would be great. \n\
          \nI tried running gptq-for-llama on my own computer... out of memory on\
          \ layer 27/32, \ntried running autogptq... Finished, but doesn't save file\
          \ due to error with cpu offloading on autogptq\n\nTried running gptq-for-llama\
          \ on a Google colab.... not enough ram.\nTried running autogptq... seems\
          \ to succeed but refuses to save the output file..."
        updatedAt: '2023-05-22T22:06:54.367Z'
      numEdits: 2
      reactions: []
    id: 6469e32ba5dd10c9a49df027
    type: comment
  author: Sharpy66
  content: "I tried to quantize this model to 4 bits, but I consistently run out of\
    \ vram when quantizing layer 27/32. If somebody else could quantize the model,\
    \ or suggest a way for me to do it myself, that would be great. \n\nI tried running\
    \ gptq-for-llama on my own computer... out of memory on layer 27/32, \ntried running\
    \ autogptq... Finished, but doesn't save file due to error with cpu offloading\
    \ on autogptq\n\nTried running gptq-for-llama on a Google colab.... not enough\
    \ ram.\nTried running autogptq... seems to succeed but refuses to save the output\
    \ file..."
  created_at: 2023-05-21 08:23:55+00:00
  edited: true
  hidden: false
  id: 6469e32ba5dd10c9a49df027
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TehVenom/Pygmalion-Vicuna-1.1-7b
repo_type: model
status: open
target_branch: null
title: Quantization
