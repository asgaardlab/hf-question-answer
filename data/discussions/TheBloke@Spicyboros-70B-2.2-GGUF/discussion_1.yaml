!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wolfram
conflicting_files: null
created_at: 2023-09-12 18:33:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-09-12T19:33:21.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.877170979976654
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<p>Same as with airoboros-l2-70b-2.1-creative.Q4_0.gguf which I <a\
          \ href=\"https://huggingface.co/TheBloke/Airoboros-L2-70B-2.1-Creative-GGUF/discussions/1\"\
          >reported</a> to you before, this is also generating nonsense text for me\
          \ with koboldcpp-1.43:</p>\n<blockquote>\n<p>brie\u25C4\xDDiglia cabgenommenbriebriebriebriebriebriebriebriebriebriebriebriebriebriebrie\
          \ (repeating that all the way until max new context limit is reached...)</p>\n\
          </blockquote>\n<p>Again, I checked the file hash to make sure my download\
          \ didn't get corrupted, and I'm using the same settings as all the other\
          \ (70B) models I use, so it looks to me like it's a model issue. Anyone\
          \ able to confirm that or who has this particular file working properly\
          \ for them?</p>\n"
        raw: "Same as with airoboros-l2-70b-2.1-creative.Q4_0.gguf which I [reported](https://huggingface.co/TheBloke/Airoboros-L2-70B-2.1-Creative-GGUF/discussions/1)\
          \ to you before, this is also generating nonsense text for me with koboldcpp-1.43:\r\
          \n\r\n> brie\u25C4\xDDiglia cabgenommenbriebriebriebriebriebriebriebriebriebriebriebriebriebriebrie\
          \ (repeating that all the way until max new context limit is reached...)\r\
          \n\r\nAgain, I checked the file hash to make sure my download didn't get\
          \ corrupted, and I'm using the same settings as all the other (70B) models\
          \ I use, so it looks to me like it's a model issue. Anyone able to confirm\
          \ that or who has this particular file working properly for them?"
        updatedAt: '2023-09-12T19:33:21.509Z'
      numEdits: 0
      reactions: []
    id: 6500bd010339dae3dbb8bea0
    type: comment
  author: wolfram
  content: "Same as with airoboros-l2-70b-2.1-creative.Q4_0.gguf which I [reported](https://huggingface.co/TheBloke/Airoboros-L2-70B-2.1-Creative-GGUF/discussions/1)\
    \ to you before, this is also generating nonsense text for me with koboldcpp-1.43:\r\
    \n\r\n> brie\u25C4\xDDiglia cabgenommenbriebriebriebriebriebriebriebriebriebriebriebriebriebriebrie\
    \ (repeating that all the way until max new context limit is reached...)\r\n\r\
    \nAgain, I checked the file hash to make sure my download didn't get corrupted,\
    \ and I'm using the same settings as all the other (70B) models I use, so it looks\
    \ to me like it's a model issue. Anyone able to confirm that or who has this particular\
    \ file working properly for them?"
  created_at: 2023-09-12 18:33:21+00:00
  edited: false
  hidden: false
  id: 6500bd010339dae3dbb8bea0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-12T19:39:54.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9554389715194702
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh damn you''re right. Probably a llama.cpp bug. I''ll report it.</p>

          '
        raw: Oh damn you're right. Probably a llama.cpp bug. I'll report it.
        updatedAt: '2023-09-12T19:39:54.405Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - wolfram
    id: 6500be8aaf1e4021438cef6d
    type: comment
  author: TheBloke
  content: Oh damn you're right. Probably a llama.cpp bug. I'll report it.
  created_at: 2023-09-12 18:39:54+00:00
  edited: false
  hidden: false
  id: 6500be8aaf1e4021438cef6d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-12T19:51:35.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7027727961540222
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/3148">https://github.com/ggerganov/llama.cpp/issues/3148</a></p>

          '
        raw: https://github.com/ggerganov/llama.cpp/issues/3148
        updatedAt: '2023-09-12T19:51:35.046Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - wolfram
        - mirek190
    id: 6500c147b2078f22bac0c8a4
    type: comment
  author: TheBloke
  content: https://github.com/ggerganov/llama.cpp/issues/3148
  created_at: 2023-09-12 18:51:35+00:00
  edited: false
  hidden: false
  id: 6500c147b2078f22bac0c8a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-14T10:56:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9243971705436707
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Try re-downloading.  Jon re-uploaded the source weights using the
          standard LoRA merge method, not the new method that causes this problem.  And
          I have re-done my GGUFs.</p>

          '
        raw: Try re-downloading.  Jon re-uploaded the source weights using the standard
          LoRA merge method, not the new method that causes this problem.  And I have
          re-done my GGUFs.
        updatedAt: '2023-09-14T10:56:38.184Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - PrimeD
        - ntc-ai
        - wolfram
        - mirek190
    id: 6502e6e6b6595dc45c5404a4
    type: comment
  author: TheBloke
  content: Try re-downloading.  Jon re-uploaded the source weights using the standard
    LoRA merge method, not the new method that causes this problem.  And I have re-done
    my GGUFs.
  created_at: 2023-09-14 09:56:38+00:00
  edited: false
  hidden: false
  id: 6502e6e6b6595dc45c5404a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-09-14T18:26:43.000Z'
    data:
      edited: true
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9795017838478088
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<p>Great, thanks! Redownloaded and testing it now...</p>\n<p>Good news:\
          \ The \"brie\u25C4\xDDiglia\" problem is gone. It is outputting normal responses\
          \ now.</p>\n<p>Or is it? I'm getting much worse quality than with the other\
          \ 70Bs I tried: Spelling/grammar errors, weird way of speaking, with runaway\
          \ sentences without much logic and sometimes missing words. And that's early\
          \ on already, not just when the context is full later.</p>\n<p><strong>Edit:</strong>\
          \ I've now also tested Spicyboros-c34b-2.2-GGUF (Q4_K_M) and Spicyboros-13B-2.2-GGUF\
          \ (Q8_0). They all exhibited the same terrible quality in my tests, from\
          \ 13B to 70B. I've been using koboldcpp-1.43 with the same settings for\
          \ all the tests, and other models (e. g. Synthia-70B-v1.2-GGUF and Nous-Hermes-Llama2-70B-GGUF)\
          \ were perfect.</p>\n<p>Anyone having success with these Spicyboros quants\
          \ in longer chats/roleplays?</p>\n"
        raw: "Great, thanks! Redownloaded and testing it now...\n\nGood news: The\
          \ \"brie\u25C4\xDDiglia\" problem is gone. It is outputting normal responses\
          \ now.\n\nOr is it? I'm getting much worse quality than with the other 70Bs\
          \ I tried: Spelling/grammar errors, weird way of speaking, with runaway\
          \ sentences without much logic and sometimes missing words. And that's early\
          \ on already, not just when the context is full later.\n\n**Edit:** I've\
          \ now also tested Spicyboros-c34b-2.2-GGUF (Q4_K_M) and Spicyboros-13B-2.2-GGUF\
          \ (Q8_0). They all exhibited the same terrible quality in my tests, from\
          \ 13B to 70B. I've been using koboldcpp-1.43 with the same settings for\
          \ all the tests, and other models (e. g. Synthia-70B-v1.2-GGUF and Nous-Hermes-Llama2-70B-GGUF)\
          \ were perfect.\n\nAnyone having success with these Spicyboros quants in\
          \ longer chats/roleplays?"
        updatedAt: '2023-09-14T19:35:03.038Z'
      numEdits: 4
      reactions: []
    id: 65035063c6ce94dc0e0db5e0
    type: comment
  author: wolfram
  content: "Great, thanks! Redownloaded and testing it now...\n\nGood news: The \"\
    brie\u25C4\xDDiglia\" problem is gone. It is outputting normal responses now.\n\
    \nOr is it? I'm getting much worse quality than with the other 70Bs I tried: Spelling/grammar\
    \ errors, weird way of speaking, with runaway sentences without much logic and\
    \ sometimes missing words. And that's early on already, not just when the context\
    \ is full later.\n\n**Edit:** I've now also tested Spicyboros-c34b-2.2-GGUF (Q4_K_M)\
    \ and Spicyboros-13B-2.2-GGUF (Q8_0). They all exhibited the same terrible quality\
    \ in my tests, from 13B to 70B. I've been using koboldcpp-1.43 with the same settings\
    \ for all the tests, and other models (e. g. Synthia-70B-v1.2-GGUF and Nous-Hermes-Llama2-70B-GGUF)\
    \ were perfect.\n\nAnyone having success with these Spicyboros quants in longer\
    \ chats/roleplays?"
  created_at: 2023-09-14 17:26:43+00:00
  edited: true
  hidden: false
  id: 65035063c6ce94dc0e0db5e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e3b34f0605e6e2c9b5c5beb1a9c192f.svg
      fullname: Xiao Jin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mljxy
      type: user
    createdAt: '2023-09-14T22:53:33.000Z'
    data:
      edited: false
      editors:
      - mljxy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9432023167610168
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e3b34f0605e6e2c9b5c5beb1a9c192f.svg
          fullname: Xiao Jin
          isHf: false
          isPro: false
          name: mljxy
          type: user
        html: '<p>Yeah, something seems off in these llama.cpp quants from qlora finetunes.
          I tried the new 70B Q4_K_M here, and it''s very prone to generate spelling/grammar
          errors (just saw a few " let'' ", where it clearly want to say "let''s"),
          and the overall quality of the output feels lower too.</p>

          '
        raw: Yeah, something seems off in these llama.cpp quants from qlora finetunes.
          I tried the new 70B Q4_K_M here, and it's very prone to generate spelling/grammar
          errors (just saw a few " let' ", where it clearly want to say "let's"),
          and the overall quality of the output feels lower too.
        updatedAt: '2023-09-14T22:53:33.681Z'
      numEdits: 0
      reactions: []
    id: 65038eed55837b784fa454ba
    type: comment
  author: mljxy
  content: Yeah, something seems off in these llama.cpp quants from qlora finetunes.
    I tried the new 70B Q4_K_M here, and it's very prone to generate spelling/grammar
    errors (just saw a few " let' ", where it clearly want to say "let's"), and the
    overall quality of the output feels lower too.
  created_at: 2023-09-14 21:53:33+00:00
  edited: false
  hidden: false
  id: 65038eed55837b784fa454ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-09-14T23:44:35.000Z'
    data:
      edited: true
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8207062482833862
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>Good to know it''s not just on my end. Do you use llama.cpp or koboldcpp
          or what''s your backend?</p>

          <p>By the way, I''ve seen similar problems with another recent but entirely
          different model: Nous-Puffin-70B-GGUF (Q4_0). Its sibling model, Nous-Hermes-Llama2-70B-GGUF
          (Q4_0), is fine, though.</p>

          '
        raw: 'Good to know it''s not just on my end. Do you use llama.cpp or koboldcpp
          or what''s your backend?


          By the way, I''ve seen similar problems with another recent but entirely
          different model: Nous-Puffin-70B-GGUF (Q4_0). Its sibling model, Nous-Hermes-Llama2-70B-GGUF
          (Q4_0), is fine, though.'
        updatedAt: '2023-09-14T23:45:44.914Z'
      numEdits: 1
      reactions: []
    id: 65039ae336bc3431218476aa
    type: comment
  author: wolfram
  content: 'Good to know it''s not just on my end. Do you use llama.cpp or koboldcpp
    or what''s your backend?


    By the way, I''ve seen similar problems with another recent but entirely different
    model: Nous-Puffin-70B-GGUF (Q4_0). Its sibling model, Nous-Hermes-Llama2-70B-GGUF
    (Q4_0), is fine, though.'
  created_at: 2023-09-14 22:44:35+00:00
  edited: true
  hidden: false
  id: 65039ae336bc3431218476aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba4c44b78c2e4066ac4a6ba276e68f6e.svg
      fullname: jp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jp02
      type: user
    createdAt: '2023-09-16T02:15:06.000Z'
    data:
      edited: false
      editors:
      - jp02
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9567234516143799
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba4c44b78c2e4066ac4a6ba276e68f6e.svg
          fullname: jp
          isHf: false
          isPro: false
          name: jp02
          type: user
        html: '<p>I have a similar problem with the q6 quant.<br>Any word containing
          an apostrophe sends the eos_token, if I ban said token I get a space between
          the apostrophe and the rest of the word.<br>One thing that I also noticed
          with the new quants compared to the first ones is that at least to me the
          output quality seems higher than before.</p>

          '
        raw: 'I have a similar problem with the q6 quant.

          Any word containing an apostrophe sends the eos_token, if I ban said token
          I get a space between the apostrophe and the rest of the word.

          One thing that I also noticed with the new quants compared to the first
          ones is that at least to me the output quality seems higher than before.'
        updatedAt: '2023-09-16T02:15:06.048Z'
      numEdits: 0
      reactions: []
    id: 65050faa3740c884002b55dc
    type: comment
  author: jp02
  content: 'I have a similar problem with the q6 quant.

    Any word containing an apostrophe sends the eos_token, if I ban said token I get
    a space between the apostrophe and the rest of the word.

    One thing that I also noticed with the new quants compared to the first ones is
    that at least to me the output quality seems higher than before.'
  created_at: 2023-09-16 01:15:06+00:00
  edited: false
  hidden: false
  id: 65050faa3740c884002b55dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/w5LUxucAXEHsh3RnGCTJ6.jpeg?w=200&h=200&f=face
      fullname: Bartek Wreczycki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AMDBartek
      type: user
    createdAt: '2023-09-17T16:06:15.000Z'
    data:
      edited: true
      editors:
      - AMDBartek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8948996067047119
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/w5LUxucAXEHsh3RnGCTJ6.jpeg?w=200&h=200&f=face
          fullname: Bartek Wreczycki
          isHf: false
          isPro: false
          name: AMDBartek
          type: user
        html: "<p>I am using the Q4_K_M quant with koboldcpp as my backend, and I\
          \ can confirm the issue that <span data-props=\"{&quot;user&quot;:&quot;jp02&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jp02\"\
          >@<span class=\"underline\">jp02</span></a></span>\n\n\t</span></span> is\
          \ having where some words that contain an apostrophe cause the EOS token\
          \ to be generated. This doesn't always happen, but when it does, it seems\
          \ to happen consistently. I'm not sure if there is a specific scenario that\
          \ causes this but it only happens in a few cases which is kind of annoying.\
          \ I have attached a screenshot of this happening when using the model as\
          \ an assistant in VSCodium (yes, I am aware that it says \"ChatGPT\", it\
          \ is actually going to a custom local OpenAI-compatible proxy that I wrote\
          \ which directs the request to koboldcpp running on the local machine).</p>\n\
          <p>In this case, it didn't generate the EOS token but rather produced a\
          \ very low-quality output:<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/643423c79bd5a84b5dcaccaa/MipW113dg7pzXxC0zb_PW.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/643423c79bd5a84b5dcaccaa/MipW113dg7pzXxC0zb_PW.png\"\
          ></a></p>\n<p>The prompt (I am aware that it refers to the model as ChatGPT,\
          \ it's going to koboldcpp running this model on the local machine):</p>\n\
          <pre><code>You are ChatGPT helping the User with coding. You are intelligent,\
          \ helpful and an expert developer, who always gives the correct answer and\
          \ only does what instructed. If you show code, your response must always\
          \ be markdown with any code inside markdown codeblocks. If the user is trying\
          \ to do a bad programming practice, helpfully let them know and mention\
          \ an alternative. When responding to the following prompt, please make sure\
          \ to properly style your response using Github Flavored Markdown. Use markdown\
          \ syntax for text like headings, lists, colored text, code blocks, highlights\
          \ etc.\\nUSER: Explain the following code. The following code is in javascript\
          \ programming language. Code in question:\\n###\\n```javascript\\nconst\
          \ prompt = require(\\\"prompt-sync\\\")({sigint: true});\\nvar question\
          \ = prompt(\\\"What is your name? \\\");\\nconsole.log(\\\"Your name is\
          \ \\\" + question);\\n```\\nASSISTANT: \n</code></pre>\n<p>Settings:</p>\n\
          <pre><code>\"n\": 1, \"rep_pen\": 1.05, \"temperature\": 1.07, \"top_p\"\
          : 1, \"top_k\": 100, \"top_a\": 0, \"typical\": 1, \"tfs\": 0.93, \"rep_pen_range\"\
          : 404, \"rep_pen_slope\": 0.8, \"sampler_order\": [6, 0, 5, 3, 2, 1, 4],\
          \ \"quiet\": false, \"max_context_length\": 8192, \"max_length\": 3072,\
          \ \"stop_sequence\": [\"\\nUSER:\", \"\\nASSISTANT:\"]\n</code></pre>\n\
          <p>If more information is needed, I will gladly provide it.</p>\n"
        raw: "I am using the Q4_K_M quant with koboldcpp as my backend, and I can\
          \ confirm the issue that @jp02 is having where some words that contain an\
          \ apostrophe cause the EOS token to be generated. This doesn't always happen,\
          \ but when it does, it seems to happen consistently. I'm not sure if there\
          \ is a specific scenario that causes this but it only happens in a few cases\
          \ which is kind of annoying. I have attached a screenshot of this happening\
          \ when using the model as an assistant in VSCodium (yes, I am aware that\
          \ it says \"ChatGPT\", it is actually going to a custom local OpenAI-compatible\
          \ proxy that I wrote which directs the request to koboldcpp running on the\
          \ local machine).\n\nIn this case, it didn't generate the EOS token but\
          \ rather produced a very low-quality output:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643423c79bd5a84b5dcaccaa/MipW113dg7pzXxC0zb_PW.png)\n\
          \nThe prompt (I am aware that it refers to the model as ChatGPT, it's going\
          \ to koboldcpp running this model on the local machine):\n````\nYou are\
          \ ChatGPT helping the User with coding. You are intelligent, helpful and\
          \ an expert developer, who always gives the correct answer and only does\
          \ what instructed. If you show code, your response must always be markdown\
          \ with any code inside markdown codeblocks. If the user is trying to do\
          \ a bad programming practice, helpfully let them know and mention an alternative.\
          \ When responding to the following prompt, please make sure to properly\
          \ style your response using Github Flavored Markdown. Use markdown syntax\
          \ for text like headings, lists, colored text, code blocks, highlights etc.\\\
          nUSER: Explain the following code. The following code is in javascript programming\
          \ language. Code in question:\\n###\\n```javascript\\nconst prompt = require(\\\
          \"prompt-sync\\\")({sigint: true});\\nvar question = prompt(\\\"What is\
          \ your name? \\\");\\nconsole.log(\\\"Your name is \\\" + question);\\n```\\\
          nASSISTANT: \n````\n\nSettings:\n```\n\"n\": 1, \"rep_pen\": 1.05, \"temperature\"\
          : 1.07, \"top_p\": 1, \"top_k\": 100, \"top_a\": 0, \"typical\": 1, \"tfs\"\
          : 0.93, \"rep_pen_range\": 404, \"rep_pen_slope\": 0.8, \"sampler_order\"\
          : [6, 0, 5, 3, 2, 1, 4], \"quiet\": false, \"max_context_length\": 8192,\
          \ \"max_length\": 3072, \"stop_sequence\": [\"\\nUSER:\", \"\\nASSISTANT:\"\
          ]\n```\n\nIf more information is needed, I will gladly provide it."
        updatedAt: '2023-09-17T16:07:22.286Z'
      numEdits: 1
      reactions: []
    id: 650723f79310ce8c40396212
    type: comment
  author: AMDBartek
  content: "I am using the Q4_K_M quant with koboldcpp as my backend, and I can confirm\
    \ the issue that @jp02 is having where some words that contain an apostrophe cause\
    \ the EOS token to be generated. This doesn't always happen, but when it does,\
    \ it seems to happen consistently. I'm not sure if there is a specific scenario\
    \ that causes this but it only happens in a few cases which is kind of annoying.\
    \ I have attached a screenshot of this happening when using the model as an assistant\
    \ in VSCodium (yes, I am aware that it says \"ChatGPT\", it is actually going\
    \ to a custom local OpenAI-compatible proxy that I wrote which directs the request\
    \ to koboldcpp running on the local machine).\n\nIn this case, it didn't generate\
    \ the EOS token but rather produced a very low-quality output:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643423c79bd5a84b5dcaccaa/MipW113dg7pzXxC0zb_PW.png)\n\
    \nThe prompt (I am aware that it refers to the model as ChatGPT, it's going to\
    \ koboldcpp running this model on the local machine):\n````\nYou are ChatGPT helping\
    \ the User with coding. You are intelligent, helpful and an expert developer,\
    \ who always gives the correct answer and only does what instructed. If you show\
    \ code, your response must always be markdown with any code inside markdown codeblocks.\
    \ If the user is trying to do a bad programming practice, helpfully let them know\
    \ and mention an alternative. When responding to the following prompt, please\
    \ make sure to properly style your response using Github Flavored Markdown. Use\
    \ markdown syntax for text like headings, lists, colored text, code blocks, highlights\
    \ etc.\\nUSER: Explain the following code. The following code is in javascript\
    \ programming language. Code in question:\\n###\\n```javascript\\nconst prompt\
    \ = require(\\\"prompt-sync\\\")({sigint: true});\\nvar question = prompt(\\\"\
    What is your name? \\\");\\nconsole.log(\\\"Your name is \\\" + question);\\n```\\\
    nASSISTANT: \n````\n\nSettings:\n```\n\"n\": 1, \"rep_pen\": 1.05, \"temperature\"\
    : 1.07, \"top_p\": 1, \"top_k\": 100, \"top_a\": 0, \"typical\": 1, \"tfs\": 0.93,\
    \ \"rep_pen_range\": 404, \"rep_pen_slope\": 0.8, \"sampler_order\": [6, 0, 5,\
    \ 3, 2, 1, 4], \"quiet\": false, \"max_context_length\": 8192, \"max_length\"\
    : 3072, \"stop_sequence\": [\"\\nUSER:\", \"\\nASSISTANT:\"]\n```\n\nIf more information\
    \ is needed, I will gladly provide it."
  created_at: 2023-09-17 15:06:15+00:00
  edited: true
  hidden: false
  id: 650723f79310ce8c40396212
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/w5LUxucAXEHsh3RnGCTJ6.jpeg?w=200&h=200&f=face
      fullname: Bartek Wreczycki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AMDBartek
      type: user
    createdAt: '2023-09-17T16:13:43.000Z'
    data:
      edited: true
      editors:
      - AMDBartek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6133747100830078
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/w5LUxucAXEHsh3RnGCTJ6.jpeg?w=200&h=200&f=face
          fullname: Bartek Wreczycki
          isHf: false
          isPro: false
          name: AMDBartek
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/643423c79bd5a84b5dcaccaa/NIV0M2dDKd5F1SO4nL3Kh.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/643423c79bd5a84b5dcaccaa/NIV0M2dDKd5F1SO4nL3Kh.png"></a><br>Same
          prompt and settings, got an example of the early EOS token.</p>

          <p>EDIT: Running koboldcpp on <code>concedo_experimental</code> branch,
          commit <code>34930bfdc22acc18f1ddf906d517b9d5fee03bc1</code>.</p>

          '
        raw: '![image.png](https://cdn-uploads.huggingface.co/production/uploads/643423c79bd5a84b5dcaccaa/NIV0M2dDKd5F1SO4nL3Kh.png)

          Same prompt and settings, got an example of the early EOS token.


          EDIT: Running koboldcpp on `concedo_experimental` branch, commit `34930bfdc22acc18f1ddf906d517b9d5fee03bc1`.'
        updatedAt: '2023-09-17T16:28:44.959Z'
      numEdits: 1
      reactions: []
    id: 650725b7d5578ef7e2f11b11
    type: comment
  author: AMDBartek
  content: '![image.png](https://cdn-uploads.huggingface.co/production/uploads/643423c79bd5a84b5dcaccaa/NIV0M2dDKd5F1SO4nL3Kh.png)

    Same prompt and settings, got an example of the early EOS token.


    EDIT: Running koboldcpp on `concedo_experimental` branch, commit `34930bfdc22acc18f1ddf906d517b9d5fee03bc1`.'
  created_at: 2023-09-17 15:13:43+00:00
  edited: true
  hidden: false
  id: 650725b7d5578ef7e2f11b11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/w5LUxucAXEHsh3RnGCTJ6.jpeg?w=200&h=200&f=face
      fullname: Bartek Wreczycki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AMDBartek
      type: user
    createdAt: '2023-09-17T22:48:51.000Z'
    data:
      edited: true
      editors:
      - AMDBartek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7038963437080383
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/w5LUxucAXEHsh3RnGCTJ6.jpeg?w=200&h=200&f=face
          fullname: Bartek Wreczycki
          isHf: false
          isPro: false
          name: AMDBartek
          type: user
        html: '<p>Okay, what''s even weirder is that the previous version of the model
          seems to work fine (still same settings, same prompt, but the older Q4_K_M
          quant).</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/643423c79bd5a84b5dcaccaa/b5VKvZstishjkid7psbI7.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/643423c79bd5a84b5dcaccaa/b5VKvZstishjkid7psbI7.png"></a></p>

          <p>The older, working model: <a href="https://huggingface.co/TheBloke/Spicyboros-70B-2.2-GGUF/blob/f6d627f5a30aad981bc539047ea71374813777ca/spicyboros-70b-2.2.Q4_K_M.gguf">https://huggingface.co/TheBloke/Spicyboros-70B-2.2-GGUF/blob/f6d627f5a30aad981bc539047ea71374813777ca/spicyboros-70b-2.2.Q4_K_M.gguf</a></p>

          <p>So, there seems to be something weird going on here, especially with
          the Q4_0 non k-quant.</p>

          '
        raw: 'Okay, what''s even weirder is that the previous version of the model
          seems to work fine (still same settings, same prompt, but the older Q4_K_M
          quant).


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/643423c79bd5a84b5dcaccaa/b5VKvZstishjkid7psbI7.png)


          The older, working model: https://huggingface.co/TheBloke/Spicyboros-70B-2.2-GGUF/blob/f6d627f5a30aad981bc539047ea71374813777ca/spicyboros-70b-2.2.Q4_K_M.gguf


          So, there seems to be something weird going on here, especially with the
          Q4_0 non k-quant.'
        updatedAt: '2023-09-17T22:50:02.898Z'
      numEdits: 1
      reactions: []
    id: 65078253ad3134ed7e924c47
    type: comment
  author: AMDBartek
  content: 'Okay, what''s even weirder is that the previous version of the model seems
    to work fine (still same settings, same prompt, but the older Q4_K_M quant).


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/643423c79bd5a84b5dcaccaa/b5VKvZstishjkid7psbI7.png)


    The older, working model: https://huggingface.co/TheBloke/Spicyboros-70B-2.2-GGUF/blob/f6d627f5a30aad981bc539047ea71374813777ca/spicyboros-70b-2.2.Q4_K_M.gguf


    So, there seems to be something weird going on here, especially with the Q4_0
    non k-quant.'
  created_at: 2023-09-17 21:48:51+00:00
  edited: true
  hidden: false
  id: 65078253ad3134ed7e924c47
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Spicyboros-70B-2.2-GGUF
repo_type: model
status: open
target_branch: null
title: spicyboros-70b-2.2.Q4_0.gguf broken as well?
