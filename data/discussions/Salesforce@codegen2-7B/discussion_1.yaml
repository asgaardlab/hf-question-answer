!!python/object:huggingface_hub.community.DiscussionWithDetails
author: martiwey
conflicting_files: null
created_at: 2023-11-02 18:08:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640678858556-noauth.jpeg?w=200&h=200&f=face
      fullname: Martin Weyssow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: martiwey
      type: user
    createdAt: '2023-11-02T19:08:38.000Z'
    data:
      edited: false
      editors:
      - martiwey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6788497567176819
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640678858556-noauth.jpeg?w=200&h=200&f=face
          fullname: Martin Weyssow
          isHf: false
          isPro: false
          name: martiwey
          type: user
        html: '<p>Hi,</p>

          <p>I am using CodeGen2-1B, CodeGen2-3_7B and CodeGen-7B, and I encounter
          some serious latency at inference compared to other LLMs of the same size
          (e.g., CodeLlama-7b). </p>

          <p>I used PEFT to fine-tune the models on a custom dataset, and then use
          the model at inference on a test set.<br>Here is the code I use to load
          the model using a local checkpoint containing the adapter (<em>args.adapter_path</em>):
          </p>

          <pre><code>model = AutoModelForCausalLM(args.model_name_or_path, trust_remote_code=True,
          torch_dtype=torch.float16)

          model = PeftModel.from_pretrained(model, args.adapter_path).to(args.device)

          tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)

          </code></pre>

          <p>Then I simply loop over my test set and call <code>model.generate</code>.
          Here are the inference times I get on the same machine, using a single GPU
          and the same datasets:</p>

          <pre><code>CodeLlama-7b: ~11min (540 samples, max_new_tokens=64)

          CodeGen2-7B: ~45min  (540 samples, max_new_tokens=64)

          </code></pre>

          <p>Using CodeGen2, the inference time dramatically increases as I attempt
          to generate more tokens. </p>

          <p>Am I missing something specific to CodeGen2 when loading the model?</p>

          <p>Thanks for your help!</p>

          '
        raw: "Hi,\r\n\r\nI am using CodeGen2-1B, CodeGen2-3_7B and CodeGen-7B, and\
          \ I encounter some serious latency at inference compared to other LLMs of\
          \ the same size (e.g., CodeLlama-7b). \r\n\r\nI used PEFT to fine-tune the\
          \ models on a custom dataset, and then use the model at inference on a test\
          \ set.\r\nHere is the code I use to load the model using a local checkpoint\
          \ containing the adapter (*args.adapter_path*): \r\n```\r\nmodel = AutoModelForCausalLM(args.model_name_or_path,\
          \ trust_remote_code=True, torch_dtype=torch.float16)\r\nmodel = PeftModel.from_pretrained(model,\
          \ args.adapter_path).to(args.device)\r\ntokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path,\
          \ trust_remote_code=True)\r\n```\r\n\r\nThen I simply loop over my test\
          \ set and call `model.generate`. Here are the inference times I get on the\
          \ same machine, using a single GPU and the same datasets:\r\n```\r\nCodeLlama-7b:\
          \ ~11min (540 samples, max_new_tokens=64)\r\nCodeGen2-7B: ~45min  (540 samples,\
          \ max_new_tokens=64)\r\n```\r\nUsing CodeGen2, the inference time dramatically\
          \ increases as I attempt to generate more tokens. \r\n\r\nAm I missing something\
          \ specific to CodeGen2 when loading the model?\r\n\r\nThanks for your help!"
        updatedAt: '2023-11-02T19:08:38.378Z'
      numEdits: 0
      reactions: []
    id: 6543f3b641540cfc6ba8b9b8
    type: comment
  author: martiwey
  content: "Hi,\r\n\r\nI am using CodeGen2-1B, CodeGen2-3_7B and CodeGen-7B, and I\
    \ encounter some serious latency at inference compared to other LLMs of the same\
    \ size (e.g., CodeLlama-7b). \r\n\r\nI used PEFT to fine-tune the models on a\
    \ custom dataset, and then use the model at inference on a test set.\r\nHere is\
    \ the code I use to load the model using a local checkpoint containing the adapter\
    \ (*args.adapter_path*): \r\n```\r\nmodel = AutoModelForCausalLM(args.model_name_or_path,\
    \ trust_remote_code=True, torch_dtype=torch.float16)\r\nmodel = PeftModel.from_pretrained(model,\
    \ args.adapter_path).to(args.device)\r\ntokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path,\
    \ trust_remote_code=True)\r\n```\r\n\r\nThen I simply loop over my test set and\
    \ call `model.generate`. Here are the inference times I get on the same machine,\
    \ using a single GPU and the same datasets:\r\n```\r\nCodeLlama-7b: ~11min (540\
    \ samples, max_new_tokens=64)\r\nCodeGen2-7B: ~45min  (540 samples, max_new_tokens=64)\r\
    \n```\r\nUsing CodeGen2, the inference time dramatically increases as I attempt\
    \ to generate more tokens. \r\n\r\nAm I missing something specific to CodeGen2\
    \ when loading the model?\r\n\r\nThanks for your help!"
  created_at: 2023-11-02 18:08:38+00:00
  edited: false
  hidden: false
  id: 6543f3b641540cfc6ba8b9b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-11-02T19:13:47.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7806274890899658
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>I think the overhead is expected, when you use Peft models, especially
          lora, during inference there is an overhead due to the LoRA layers - see
          figure below</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62441d1d9fdefb55a0b7d12c/EMtsDOf4Wh7aveX0wy-OZ.gif"><img
          alt="lora-animated.gif" src="https://cdn-uploads.huggingface.co/production/uploads/62441d1d9fdefb55a0b7d12c/EMtsDOf4Wh7aveX0wy-OZ.gif"></a></p>

          <p>As you perform at the same time the computation on the left and on the
          right and sum the final results, this creates an overhead that can be quite
          considerable during generation.</p>

          <p>However you can overcome this by "merging" the adapter weights into the
          base model as LoRA can be simply rewritten as a refactorization of simple
          matrix multiplication<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62441d1d9fdefb55a0b7d12c/Oc_cbRvTyGZIOp_SyAX7b.png"><img
          alt="Screenshot 2023-11-02 at 20.12.17.png" src="https://cdn-uploads.huggingface.co/production/uploads/62441d1d9fdefb55a0b7d12c/Oc_cbRvTyGZIOp_SyAX7b.png"></a><br>Therefore
          you can merge everything in a single weight matrix and retrive base model''s
          performance. You can do that as follows:</p>

          <pre><code class="language-diff">model = AutoModelForCausalLM(args.model_name_or_path,
          trust_remote_code=True, torch_dtype=torch.float16)

          model = PeftModel.from_pretrained(model, args.adapter_path).to(args.device)

          <span class="hljs-addition">+ model = model.merge_and_unload()</span>

          tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)

          </code></pre>

          <p>More details about it here: <a href="https://huggingface.co/docs/peft/conceptual_guides/lora#merge-lora-weights-into-the-base-model">https://huggingface.co/docs/peft/conceptual_guides/lora#merge-lora-weights-into-the-base-model</a></p>

          '
        raw: "I think the overhead is expected, when you use Peft models, especially\
          \ lora, during inference there is an overhead due to the LoRA layers - see\
          \ figure below\n\n![lora-animated.gif](https://cdn-uploads.huggingface.co/production/uploads/62441d1d9fdefb55a0b7d12c/EMtsDOf4Wh7aveX0wy-OZ.gif)\n\
          \nAs you perform at the same time the computation on the left and on the\
          \ right and sum the final results, this creates an overhead that can be\
          \ quite considerable during generation.\n\nHowever you can overcome this\
          \ by \"merging\" the adapter weights into the base model as LoRA can be\
          \ simply rewritten as a refactorization of simple matrix multiplication\n\
          ![Screenshot 2023-11-02 at 20.12.17.png](https://cdn-uploads.huggingface.co/production/uploads/62441d1d9fdefb55a0b7d12c/Oc_cbRvTyGZIOp_SyAX7b.png)\n\
          Therefore you can merge everything in a single weight matrix and retrive\
          \ base model's performance. You can do that as follows:\n\n```diff\nmodel\
          \ = AutoModelForCausalLM(args.model_name_or_path, trust_remote_code=True,\
          \ torch_dtype=torch.float16)\nmodel = PeftModel.from_pretrained(model, args.adapter_path).to(args.device)\n\
          + model = model.merge_and_unload()\ntokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path,\
          \ trust_remote_code=True)\n``` \n\nMore details about it here: https://huggingface.co/docs/peft/conceptual_guides/lora#merge-lora-weights-into-the-base-model"
        updatedAt: '2023-11-02T19:14:29.452Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - martiwey
    id: 6543f4eb2996405c23882b03
    type: comment
  author: ybelkada
  content: "I think the overhead is expected, when you use Peft models, especially\
    \ lora, during inference there is an overhead due to the LoRA layers - see figure\
    \ below\n\n![lora-animated.gif](https://cdn-uploads.huggingface.co/production/uploads/62441d1d9fdefb55a0b7d12c/EMtsDOf4Wh7aveX0wy-OZ.gif)\n\
    \nAs you perform at the same time the computation on the left and on the right\
    \ and sum the final results, this creates an overhead that can be quite considerable\
    \ during generation.\n\nHowever you can overcome this by \"merging\" the adapter\
    \ weights into the base model as LoRA can be simply rewritten as a refactorization\
    \ of simple matrix multiplication\n![Screenshot 2023-11-02 at 20.12.17.png](https://cdn-uploads.huggingface.co/production/uploads/62441d1d9fdefb55a0b7d12c/Oc_cbRvTyGZIOp_SyAX7b.png)\n\
    Therefore you can merge everything in a single weight matrix and retrive base\
    \ model's performance. You can do that as follows:\n\n```diff\nmodel = AutoModelForCausalLM(args.model_name_or_path,\
    \ trust_remote_code=True, torch_dtype=torch.float16)\nmodel = PeftModel.from_pretrained(model,\
    \ args.adapter_path).to(args.device)\n+ model = model.merge_and_unload()\ntokenizer\
    \ = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)\n\
    ``` \n\nMore details about it here: https://huggingface.co/docs/peft/conceptual_guides/lora#merge-lora-weights-into-the-base-model"
  created_at: 2023-11-02 18:13:47+00:00
  edited: true
  hidden: false
  id: 6543f4eb2996405c23882b03
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640678858556-noauth.jpeg?w=200&h=200&f=face
      fullname: Martin Weyssow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: martiwey
      type: user
    createdAt: '2023-11-02T19:23:13.000Z'
    data:
      edited: false
      editors:
      - martiwey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9654576182365417
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640678858556-noauth.jpeg?w=200&h=200&f=face
          fullname: Martin Weyssow
          isHf: false
          isPro: false
          name: martiwey
          type: user
        html: '<p>Thanks, Younes for the informative reply! It''s working perfectly
          now.</p>

          '
        raw: Thanks, Younes for the informative reply! It's working perfectly now.
        updatedAt: '2023-11-02T19:23:13.442Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
      relatedEventId: 6543f721998b3248a1060b72
    id: 6543f721998b3248a1060b71
    type: comment
  author: martiwey
  content: Thanks, Younes for the informative reply! It's working perfectly now.
  created_at: 2023-11-02 18:23:13+00:00
  edited: false
  hidden: false
  id: 6543f721998b3248a1060b71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640678858556-noauth.jpeg?w=200&h=200&f=face
      fullname: Martin Weyssow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: martiwey
      type: user
    createdAt: '2023-11-02T19:23:13.000Z'
    data:
      status: closed
    id: 6543f721998b3248a1060b72
    type: status-change
  author: martiwey
  created_at: 2023-11-02 18:23:13+00:00
  id: 6543f721998b3248a1060b72
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640678858556-noauth.jpeg?w=200&h=200&f=face
      fullname: Martin Weyssow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: martiwey
      type: user
    createdAt: '2023-11-05T14:35:56.000Z'
    data:
      edited: true
      editors:
      - martiwey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6941967010498047
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640678858556-noauth.jpeg?w=200&h=200&f=face
          fullname: Martin Weyssow
          isHf: false
          isPro: false
          name: martiwey
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span>  I reopen this\
          \ thread as I still observe a very slow inference for CodeGen2 models even\
          \ after merging LoRA's layers with the base model. I do not observe latency\
          \ for other models such as Llama/CodeLlama/CodeT5+ when generating under\
          \ identical settings.</p>\n<p>As I increase the prompt length, the inference\
          \ is getting dramatically slow. For instance, it takes about 7 to 8 hours\
          \ to complete 628 generations, whereas CodeLlama-7b-hf takes about 30 minutes.<br>Here's\
          \ my code (I do not paste everything here as I do not think the problem\
          \ stems from the data preprocessing or postprocessing): </p>\n<pre><code>model\
          \ = AutoModelForCausalLM(args.model_name_or_path, trust_remote_code=True,\
          \ torch_dtype=torch.float16)\nmodel = PeftModel.from_pretrained(model, args.adapter_path).to(args.device)\n\
          model = model.merge_and_unload()\ntokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path,\
          \ trust_remote_code=True)\n\n# preprocessing ...\n\n# prediction loop ...\n\
          \ngenerated_sequences = model.generate(\n        input_ids=sample[\"input_ids\"\
          ].to(args.device),\n        num_beams=10,\n        num_return_sequences=10,\n\
          \        max_new_tokens=args.max_target_length,\n        stopping_criteria=StoppingCriteriaList(\n\
          \                [EndOfFunctionCriteria(sample[\"input_ids\"].shape[1],\
          \ eof_string, tokenizer)]\n        )\n )\n\n# postprocessing ...\n</code></pre>\n\
          <p>I do not do batch generation, use a single GPU, and <code>args.max_target_length</code>\
          \ = 128. I get about the same latency for any CodeGen2-1B, CodeGen2-3_7B\
          \ and CodeGen2-7B models.</p>\n<p>Could it be because of the custom modeling\
          \ script?</p>\n"
        raw: "@ybelkada  I reopen this thread as I still observe a very slow inference\
          \ for CodeGen2 models even after merging LoRA's layers with the base model.\
          \ I do not observe latency for other models such as Llama/CodeLlama/CodeT5+\
          \ when generating under identical settings.\n\nAs I increase the prompt\
          \ length, the inference is getting dramatically slow. For instance, it takes\
          \ about 7 to 8 hours to complete 628 generations, whereas CodeLlama-7b-hf\
          \ takes about 30 minutes.\nHere's my code (I do not paste everything here\
          \ as I do not think the problem stems from the data preprocessing or postprocessing):\
          \ \n\n```\nmodel = AutoModelForCausalLM(args.model_name_or_path, trust_remote_code=True,\
          \ torch_dtype=torch.float16)\nmodel = PeftModel.from_pretrained(model, args.adapter_path).to(args.device)\n\
          model = model.merge_and_unload()\ntokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path,\
          \ trust_remote_code=True)\n\n# preprocessing ...\n\n# prediction loop ...\n\
          \ngenerated_sequences = model.generate(\n        input_ids=sample[\"input_ids\"\
          ].to(args.device),\n        num_beams=10,\n        num_return_sequences=10,\n\
          \        max_new_tokens=args.max_target_length,\n        stopping_criteria=StoppingCriteriaList(\n\
          \                [EndOfFunctionCriteria(sample[\"input_ids\"].shape[1],\
          \ eof_string, tokenizer)]\n        )\n )\n\n# postprocessing ...\n```\n\n\
          I do not do batch generation, use a single GPU, and `args.max_target_length`\
          \ = 128. I get about the same latency for any CodeGen2-1B, CodeGen2-3_7B\
          \ and CodeGen2-7B models.\n\nCould it be because of the custom modeling\
          \ script?"
        updatedAt: '2023-11-05T14:36:31.050Z'
      numEdits: 1
      reactions: []
      relatedEventId: 6547a84d650796837487c462
    id: 6547a84c650796837487c457
    type: comment
  author: martiwey
  content: "@ybelkada  I reopen this thread as I still observe a very slow inference\
    \ for CodeGen2 models even after merging LoRA's layers with the base model. I\
    \ do not observe latency for other models such as Llama/CodeLlama/CodeT5+ when\
    \ generating under identical settings.\n\nAs I increase the prompt length, the\
    \ inference is getting dramatically slow. For instance, it takes about 7 to 8\
    \ hours to complete 628 generations, whereas CodeLlama-7b-hf takes about 30 minutes.\n\
    Here's my code (I do not paste everything here as I do not think the problem stems\
    \ from the data preprocessing or postprocessing): \n\n```\nmodel = AutoModelForCausalLM(args.model_name_or_path,\
    \ trust_remote_code=True, torch_dtype=torch.float16)\nmodel = PeftModel.from_pretrained(model,\
    \ args.adapter_path).to(args.device)\nmodel = model.merge_and_unload()\ntokenizer\
    \ = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)\n\
    \n# preprocessing ...\n\n# prediction loop ...\n\ngenerated_sequences = model.generate(\n\
    \        input_ids=sample[\"input_ids\"].to(args.device),\n        num_beams=10,\n\
    \        num_return_sequences=10,\n        max_new_tokens=args.max_target_length,\n\
    \        stopping_criteria=StoppingCriteriaList(\n                [EndOfFunctionCriteria(sample[\"\
    input_ids\"].shape[1], eof_string, tokenizer)]\n        )\n )\n\n# postprocessing\
    \ ...\n```\n\nI do not do batch generation, use a single GPU, and `args.max_target_length`\
    \ = 128. I get about the same latency for any CodeGen2-1B, CodeGen2-3_7B and CodeGen2-7B\
    \ models.\n\nCould it be because of the custom modeling script?"
  created_at: 2023-11-05 14:35:56+00:00
  edited: true
  hidden: false
  id: 6547a84c650796837487c457
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640678858556-noauth.jpeg?w=200&h=200&f=face
      fullname: Martin Weyssow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: martiwey
      type: user
    createdAt: '2023-11-05T14:35:57.000Z'
    data:
      status: open
    id: 6547a84d650796837487c462
    type: status-change
  author: martiwey
  created_at: 2023-11-05 14:35:57+00:00
  id: 6547a84d650796837487c462
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-11-05T20:17:42.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8288559317588806
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;martiwey&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/martiwey\"\
          >@<span class=\"underline\">martiwey</span></a></span>\n\n\t</span></span><br>Hmm\
          \ yes that could be it, what is the relative different between the custom\
          \ and non custom model?<br>Usually what we can do in this case is to profile\
          \ the generation script using torch profiler: <a rel=\"nofollow\" href=\"\
          https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\">https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html</a>\
          \ and try to identify which operation creates a potential overhead.<br>Operations\
          \ that can take overhead can be CPU-&gt;GPU device placement, for example\
          \ here: <a href=\"https://huggingface.co/Salesforce/codegen2-7B/blob/main/modeling_codegen.py#L62\"\
          >https://huggingface.co/Salesforce/codegen2-7B/blob/main/modeling_codegen.py#L62</a>\
          \ I can see that a tensor is being created on CPU (default) then potentially\
          \ moved to GPU.<br>Another room of improvement could be to replace these\
          \ lines: <a href=\"https://huggingface.co/Salesforce/codegen2-7B/blob/main/modeling_codegen.py#L157-L181\"\
          >https://huggingface.co/Salesforce/codegen2-7B/blob/main/modeling_codegen.py#L157-L181</a>\
          \ with <code>torch.scaled_dot_product_attention</code> from pytorch: <a\
          \ rel=\"nofollow\" href=\"https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\"\
          >https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html</a>\
          \ you can create your own fork of this model and implement these features\
          \ (+ fix the issue described above).<br>Let me know how that goes! </p>\n"
        raw: "Hi @martiwey \nHmm yes that could be it, what is the relative different\
          \ between the custom and non custom model? \nUsually what we can do in this\
          \ case is to profile the generation script using torch profiler: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\
          \ and try to identify which operation creates a potential overhead. \nOperations\
          \ that can take overhead can be CPU->GPU device placement, for example here:\
          \ https://huggingface.co/Salesforce/codegen2-7B/blob/main/modeling_codegen.py#L62\
          \ I can see that a tensor is being created on CPU (default) then potentially\
          \ moved to GPU.\nAnother room of improvement could be to replace these lines:\
          \ https://huggingface.co/Salesforce/codegen2-7B/blob/main/modeling_codegen.py#L157-L181\
          \ with `torch.scaled_dot_product_attention` from pytorch: https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\
          \ you can create your own fork of this model and implement these features\
          \ (+ fix the issue described above).\nLet me know how that goes! "
        updatedAt: '2023-11-05T20:17:42.237Z'
      numEdits: 0
      reactions: []
    id: 6547f866cd0a562139503cc6
    type: comment
  author: ybelkada
  content: "Hi @martiwey \nHmm yes that could be it, what is the relative different\
    \ between the custom and non custom model? \nUsually what we can do in this case\
    \ is to profile the generation script using torch profiler: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\
    \ and try to identify which operation creates a potential overhead. \nOperations\
    \ that can take overhead can be CPU->GPU device placement, for example here: https://huggingface.co/Salesforce/codegen2-7B/blob/main/modeling_codegen.py#L62\
    \ I can see that a tensor is being created on CPU (default) then potentially moved\
    \ to GPU.\nAnother room of improvement could be to replace these lines: https://huggingface.co/Salesforce/codegen2-7B/blob/main/modeling_codegen.py#L157-L181\
    \ with `torch.scaled_dot_product_attention` from pytorch: https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\
    \ you can create your own fork of this model and implement these features (+ fix\
    \ the issue described above).\nLet me know how that goes! "
  created_at: 2023-11-05 20:17:42+00:00
  edited: false
  hidden: false
  id: 6547f866cd0a562139503cc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640678858556-noauth.jpeg?w=200&h=200&f=face
      fullname: Martin Weyssow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: martiwey
      type: user
    createdAt: '2023-11-06T22:13:46.000Z'
    data:
      edited: true
      editors:
      - martiwey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5933526754379272
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640678858556-noauth.jpeg?w=200&h=200&f=face
          fullname: Martin Weyssow
          isHf: false
          isPro: false
          name: martiwey
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<p>Thanks for the feedback, I ran torch profiler on <code>module.generate()</code>,\
          \ and I could spot high CPU time spent on data transfer CPU-&gt;GPU (<code>&nbsp;aten::copy_</code>).<br><a\
          \ rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/61cac619165c2e25bd0a74ac/y_02kF_FTjnNrnm5aYXz1.png\"\
          ><img alt=\"codegen2_modeling.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/61cac619165c2e25bd0a74ac/y_02kF_FTjnNrnm5aYXz1.png\"\
          ></a></p>\n<p>I believe the following code fixes the issue at #L62, as <code>aten::copy_</code>\
          \ disappeared from the profiler report:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >fixed_pos_embedding</span>(<span class=\"hljs-params\">x, seq_dim=<span\
          \ class=\"hljs-number\">1</span>, seq_len=<span class=\"hljs-literal\">None</span></span>):\n\
          \    dim = x.shape[-<span class=\"hljs-number\">1</span>]\n    <span class=\"\
          hljs-keyword\">if</span> seq_len <span class=\"hljs-keyword\">is</span>\
          \ <span class=\"hljs-literal\">None</span>:\n        seq_len = x.shape[seq_dim]\n\
          \    inv_freq = <span class=\"hljs-number\">1.0</span> / (<span class=\"\
          hljs-number\">10000</span> ** (torch.arange(<span class=\"hljs-number\"\
          >0</span>, dim, <span class=\"hljs-number\">2</span>, device=x.device) /\
          \ dim))\n    sinusoid_inp = (\n        torch.einsum(<span class=\"hljs-string\"\
          >\"i , j -&gt; i j\"</span>, torch.arange(seq_len, dtype=torch.<span class=\"\
          hljs-built_in\">float</span>, device=x.device), inv_freq).<span class=\"\
          hljs-built_in\">float</span>()\n    )\n    <span class=\"hljs-keyword\"\
          >return</span> torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)\n</code></pre>\n\
          <p>However, the overall execution time remains identical. I am going to\
          \ continue investigating the issue.</p>\n"
        raw: "Hi @ybelkada ,\n\nThanks for the feedback, I ran torch profiler on `module.generate()`,\
          \ and I could spot high CPU time spent on data transfer CPU->GPU (`\_aten::copy_`).\
          \ \n![codegen2_modeling.png](https://cdn-uploads.huggingface.co/production/uploads/61cac619165c2e25bd0a74ac/y_02kF_FTjnNrnm5aYXz1.png)\n\
          \nI believe the following code fixes the issue at #L62, as `aten::copy_`\
          \ disappeared from the profiler report:\n```python\ndef fixed_pos_embedding(x,\
          \ seq_dim=1, seq_len=None):\n    dim = x.shape[-1]\n    if seq_len is None:\n\
          \        seq_len = x.shape[seq_dim]\n    inv_freq = 1.0 / (10000 ** (torch.arange(0,\
          \ dim, 2, device=x.device) / dim))\n    sinusoid_inp = (\n        torch.einsum(\"\
          i , j -> i j\", torch.arange(seq_len, dtype=torch.float, device=x.device),\
          \ inv_freq).float()\n    )\n    return torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)\n\
          ```\nHowever, the overall execution time remains identical. I am going to\
          \ continue investigating the issue."
        updatedAt: '2023-11-06T22:14:34.310Z'
      numEdits: 1
      reactions: []
      relatedEventId: 6549651ac8b11b6271088413
    id: 6549651ac8b11b627108840d
    type: comment
  author: martiwey
  content: "Hi @ybelkada ,\n\nThanks for the feedback, I ran torch profiler on `module.generate()`,\
    \ and I could spot high CPU time spent on data transfer CPU->GPU (`\_aten::copy_`).\
    \ \n![codegen2_modeling.png](https://cdn-uploads.huggingface.co/production/uploads/61cac619165c2e25bd0a74ac/y_02kF_FTjnNrnm5aYXz1.png)\n\
    \nI believe the following code fixes the issue at #L62, as `aten::copy_` disappeared\
    \ from the profiler report:\n```python\ndef fixed_pos_embedding(x, seq_dim=1,\
    \ seq_len=None):\n    dim = x.shape[-1]\n    if seq_len is None:\n        seq_len\
    \ = x.shape[seq_dim]\n    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2,\
    \ device=x.device) / dim))\n    sinusoid_inp = (\n        torch.einsum(\"i , j\
    \ -> i j\", torch.arange(seq_len, dtype=torch.float, device=x.device), inv_freq).float()\n\
    \    )\n    return torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)\n```\nHowever,\
    \ the overall execution time remains identical. I am going to continue investigating\
    \ the issue."
  created_at: 2023-11-06 22:13:46+00:00
  edited: true
  hidden: false
  id: 6549651ac8b11b627108840d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640678858556-noauth.jpeg?w=200&h=200&f=face
      fullname: Martin Weyssow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: martiwey
      type: user
    createdAt: '2023-11-06T22:13:46.000Z'
    data:
      status: closed
    id: 6549651ac8b11b6271088413
    type: status-change
  author: martiwey
  created_at: 2023-11-06 22:13:46+00:00
  id: 6549651ac8b11b6271088413
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640678858556-noauth.jpeg?w=200&h=200&f=face
      fullname: Martin Weyssow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: martiwey
      type: user
    createdAt: '2023-11-06T22:14:08.000Z'
    data:
      status: open
    id: 65496530d984a612e9674cad
    type: status-change
  author: martiwey
  created_at: 2023-11-06 22:14:08+00:00
  id: 65496530d984a612e9674cad
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-11-07T09:19:12.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8806033730506897
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>OK thanks! Let me know how it goes</p>

          '
        raw: OK thanks! Let me know how it goes
        updatedAt: '2023-11-07T09:19:12.078Z'
      numEdits: 0
      reactions: []
    id: 654a011008775ce78e61be84
    type: comment
  author: ybelkada
  content: OK thanks! Let me know how it goes
  created_at: 2023-11-07 09:19:12+00:00
  edited: false
  hidden: false
  id: 654a011008775ce78e61be84
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Salesforce/codegen2-7B
repo_type: model
status: open
target_branch: null
title: Slow inference CodeGen2 + PEFT
