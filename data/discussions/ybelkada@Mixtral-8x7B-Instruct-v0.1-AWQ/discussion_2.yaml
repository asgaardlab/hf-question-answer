!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vince62s
conflicting_files: null
created_at: 2023-12-13 10:04:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6495b47a74ce69cc4eab61f0/2eg17fMXjshpfQfSq5jyP.png?w=200&h=200&f=face
      fullname: VincentN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vince62s
      type: user
    createdAt: '2023-12-13T10:04:52.000Z'
    data:
      edited: true
      editors:
      - vince62s
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.933514416217804
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6495b47a74ce69cc4eab61f0/2eg17fMXjshpfQfSq5jyP.png?w=200&h=200&f=face
          fullname: VincentN
          isHf: false
          isPro: false
          name: vince62s
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> can you confirm\
          \ you still need 2 GPU to load this, and what kind of speed are you getting\
          \ with plain python code ?<br>thanks<br>(I did the same and getting 13 tok/sec\
          \ on 2GPU 3090+4090 which is kinda slow)</p>\n"
        raw: '@ybelkada can you confirm you still need 2 GPU to load this, and what
          kind of speed are you getting with plain python code ?

          thanks

          (I did the same and getting 13 tok/sec on 2GPU 3090+4090 which is kinda
          slow)'
        updatedAt: '2023-12-13T10:05:13.184Z'
      numEdits: 1
      reactions: []
    id: 657981c4283d9184138d4770
    type: comment
  author: vince62s
  content: '@ybelkada can you confirm you still need 2 GPU to load this, and what
    kind of speed are you getting with plain python code ?

    thanks

    (I did the same and getting 13 tok/sec on 2GPU 3090+4090 which is kinda slow)'
  created_at: 2023-12-13 10:04:52+00:00
  edited: true
  hidden: false
  id: 657981c4283d9184138d4770
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-13T16:10:36.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9234116673469543
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;vince62s&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/vince62s\"\
          >@<span class=\"underline\">vince62s</span></a></span>\n\n\t</span></span><br>How\
          \ did you run inference? I am using this PR for now: <a rel=\"nofollow\"\
          \ href=\"https://github.com/huggingface/transformers/pull/27950\">https://github.com/huggingface/transformers/pull/27950</a>\
          \ as the gate layers need to stay un-quantized</p>\n"
        raw: "Hi @vince62s \nHow did you run inference? I am using this PR for now:\
          \ https://github.com/huggingface/transformers/pull/27950 as the gate layers\
          \ need to stay un-quantized"
        updatedAt: '2023-12-13T16:10:36.822Z'
      numEdits: 0
      reactions: []
    id: 6579d77c0543cdd9dada87cc
    type: comment
  author: ybelkada
  content: "Hi @vince62s \nHow did you run inference? I am using this PR for now:\
    \ https://github.com/huggingface/transformers/pull/27950 as the gate layers need\
    \ to stay un-quantized"
  created_at: 2023-12-13 16:10:36+00:00
  edited: false
  hidden: false
  id: 6579d77c0543cdd9dada87cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6495b47a74ce69cc4eab61f0/2eg17fMXjshpfQfSq5jyP.png?w=200&h=200&f=face
      fullname: VincentN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vince62s
      type: user
    createdAt: '2023-12-13T16:15:44.000Z'
    data:
      edited: true
      editors:
      - vince62s
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8995236754417419
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6495b47a74ce69cc4eab61f0/2eg17fMXjshpfQfSq5jyP.png?w=200&h=200&f=face
          fullname: VincentN
          isHf: false
          isPro: false
          name: vince62s
          type: user
        html: '<p>I''m using this PR for now :) <a rel="nofollow" href="https://github.com/OpenNMT/OpenNMT-py/pull/2535/files">https://github.com/OpenNMT/OpenNMT-py/pull/2535/files</a><br>but
          using some llm-awq code to quantize (indeed without the gate layer) works
          fine, but slow. I''ll try to implement QuIP 2-bit to make it fit on a single
          card.</p>

          <p>what annoys me is that with llama2-70B-chat-AWQ I can get 16 tok/sec
          on the same 2GPU(3090+4090), and this has more active params at inference.<br>maybe
          the gating code is not optimized on my side.</p>

          <p>EDIT: I''m closer to 20 tok/sec in fact on mixtral AWQ</p>

          '
        raw: 'I''m using this PR for now :) https://github.com/OpenNMT/OpenNMT-py/pull/2535/files

          but using some llm-awq code to quantize (indeed without the gate layer)
          works fine, but slow. I''ll try to implement QuIP 2-bit to make it fit on
          a single card.


          what annoys me is that with llama2-70B-chat-AWQ I can get 16 tok/sec on
          the same 2GPU(3090+4090), and this has more active params at inference.

          maybe the gating code is not optimized on my side.


          EDIT: I''m closer to 20 tok/sec in fact on mixtral AWQ'
        updatedAt: '2023-12-13T16:58:17.615Z'
      numEdits: 2
      reactions: []
    id: 6579d8b0b90b4221ec0201bc
    type: comment
  author: vince62s
  content: 'I''m using this PR for now :) https://github.com/OpenNMT/OpenNMT-py/pull/2535/files

    but using some llm-awq code to quantize (indeed without the gate layer) works
    fine, but slow. I''ll try to implement QuIP 2-bit to make it fit on a single card.


    what annoys me is that with llama2-70B-chat-AWQ I can get 16 tok/sec on the same
    2GPU(3090+4090), and this has more active params at inference.

    maybe the gating code is not optimized on my side.


    EDIT: I''m closer to 20 tok/sec in fact on mixtral AWQ'
  created_at: 2023-12-13 16:15:44+00:00
  edited: true
  hidden: false
  id: 6579d8b0b90b4221ec0201bc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ
repo_type: model
status: open
target_branch: null
title: speed?
