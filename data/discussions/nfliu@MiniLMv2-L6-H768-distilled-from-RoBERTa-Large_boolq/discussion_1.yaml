!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Akshayextreme
conflicting_files: null
created_at: 2023-08-19 13:31:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/361a6928c5a8bc4776d8a74c5c8b1d6f.svg
      fullname: Akshay Nadgire
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Akshayextreme
      type: user
    createdAt: '2023-08-19T14:31:11.000Z'
    data:
      edited: false
      editors:
      - Akshayextreme
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6922414302825928
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/361a6928c5a8bc4776d8a74c5c8b1d6f.svg
          fullname: Akshay Nadgire
          isHf: false
          isPro: false
          name: Akshayextreme
          type: user
        html: '<p>Could you please add inference example to use this model?</p>

          '
        raw: Could you please add inference example to use this model?
        updatedAt: '2023-08-19T14:31:11.364Z'
      numEdits: 0
      reactions: []
    id: 64e0d22f2436a8c762840624
    type: comment
  author: Akshayextreme
  content: Could you please add inference example to use this model?
  created_at: 2023-08-19 13:31:11+00:00
  edited: false
  hidden: false
  id: 64e0d22f2436a8c762840624
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/96173d687ad9502c47cb438d1266ec82.svg
      fullname: Nelson Liu
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nfliu
      type: user
    createdAt: '2023-08-20T05:45:53.000Z'
    data:
      edited: false
      editors:
      - nfliu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6146132946014404
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/96173d687ad9502c47cb438d1266ec82.svg
          fullname: Nelson Liu
          isHf: false
          isPro: false
          name: nfliu
          type: user
        html: "<p>Added to readme:</p>\n<pre><code>import torch\nfrom transformers\
          \ import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"\
          nfliu/MiniLMv2-L6-H768-distilled-from-RoBERTa-Large_boolq\")\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"nfliu/MiniLMv2-L6-H768-distilled-from-RoBERTa-Large_boolq\"\
          )\n\n# Each example is a (question, context) pair.\nexamples = [\n    (\"\
          Lake Tahoe is in California\", \"Lake Tahoe is a popular tourist spot in\
          \ California.\"),\n    (\"Water is wet\", \"Contrary to popular belief,\
          \ water is not wet.\")\n]\n\nencoded_input = tokenizer(examples, padding=True,\
          \ truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n   \
          \ model_output = model(**encoded_input)\n    probabilities = torch.softmax(model_output.logits,\
          \ dim=-1).cpu().tolist()\n\nprobability_no = [round(prob[0], 2) for prob\
          \ in probabilities]\nprobability_yes = [round(prob[1], 2) for prob in probabilities]\n\
          \nfor example, p_no, p_yes in zip(examples, probability_no, probability_yes):\n\
          \    print(f\"Question: {example[0]}\")\n    print(f\"Context: {example[1]}\"\
          )\n    print(f\"p(No | question, context): {p_no}\")\n    print(f\"p(Yes\
          \ | question, context): {p_yes}\")\n    print()\n</code></pre>\n"
        raw: "Added to readme:\n\n```\nimport torch\nfrom transformers import AutoModelForSequenceClassification,\
          \ AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"\
          nfliu/MiniLMv2-L6-H768-distilled-from-RoBERTa-Large_boolq\")\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"nfliu/MiniLMv2-L6-H768-distilled-from-RoBERTa-Large_boolq\"\
          )\n\n# Each example is a (question, context) pair.\nexamples = [\n    (\"\
          Lake Tahoe is in California\", \"Lake Tahoe is a popular tourist spot in\
          \ California.\"),\n    (\"Water is wet\", \"Contrary to popular belief,\
          \ water is not wet.\")\n]\n\nencoded_input = tokenizer(examples, padding=True,\
          \ truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n   \
          \ model_output = model(**encoded_input)\n    probabilities = torch.softmax(model_output.logits,\
          \ dim=-1).cpu().tolist()\n\nprobability_no = [round(prob[0], 2) for prob\
          \ in probabilities]\nprobability_yes = [round(prob[1], 2) for prob in probabilities]\n\
          \nfor example, p_no, p_yes in zip(examples, probability_no, probability_yes):\n\
          \    print(f\"Question: {example[0]}\")\n    print(f\"Context: {example[1]}\"\
          )\n    print(f\"p(No | question, context): {p_no}\")\n    print(f\"p(Yes\
          \ | question, context): {p_yes}\")\n    print()\n```"
        updatedAt: '2023-08-20T05:45:53.990Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Akshayextreme
      relatedEventId: 64e1a8924c78e1eba52b8767
    id: 64e1a8914c78e1eba52b8766
    type: comment
  author: nfliu
  content: "Added to readme:\n\n```\nimport torch\nfrom transformers import AutoModelForSequenceClassification,\
    \ AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"\
    nfliu/MiniLMv2-L6-H768-distilled-from-RoBERTa-Large_boolq\")\ntokenizer = AutoTokenizer.from_pretrained(\"\
    nfliu/MiniLMv2-L6-H768-distilled-from-RoBERTa-Large_boolq\")\n\n# Each example\
    \ is a (question, context) pair.\nexamples = [\n    (\"Lake Tahoe is in California\"\
    , \"Lake Tahoe is a popular tourist spot in California.\"),\n    (\"Water is wet\"\
    , \"Contrary to popular belief, water is not wet.\")\n]\n\nencoded_input = tokenizer(examples,\
    \ padding=True, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n\
    \    model_output = model(**encoded_input)\n    probabilities = torch.softmax(model_output.logits,\
    \ dim=-1).cpu().tolist()\n\nprobability_no = [round(prob[0], 2) for prob in probabilities]\n\
    probability_yes = [round(prob[1], 2) for prob in probabilities]\n\nfor example,\
    \ p_no, p_yes in zip(examples, probability_no, probability_yes):\n    print(f\"\
    Question: {example[0]}\")\n    print(f\"Context: {example[1]}\")\n    print(f\"\
    p(No | question, context): {p_no}\")\n    print(f\"p(Yes | question, context):\
    \ {p_yes}\")\n    print()\n```"
  created_at: 2023-08-20 04:45:53+00:00
  edited: false
  hidden: false
  id: 64e1a8914c78e1eba52b8766
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/96173d687ad9502c47cb438d1266ec82.svg
      fullname: Nelson Liu
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nfliu
      type: user
    createdAt: '2023-08-20T05:45:54.000Z'
    data:
      status: closed
    id: 64e1a8924c78e1eba52b8767
    type: status-change
  author: nfliu
  created_at: 2023-08-20 04:45:54+00:00
  id: 64e1a8924c78e1eba52b8767
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: nfliu/MiniLMv2-L6-H768-distilled-from-RoBERTa-Large_boolq
repo_type: model
status: closed
target_branch: null
title: Inference example
