!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Florisvdf
conflicting_files: null
created_at: 2023-02-17 14:01:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a34571f85d15b904a947dcf1f2f5b546.svg
      fullname: Floris van der Flier
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Florisvdf
      type: user
    createdAt: '2023-02-17T14:01:14.000Z'
    data:
      edited: true
      editors:
      - Florisvdf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a34571f85d15b904a947dcf1f2f5b546.svg
          fullname: Floris van der Flier
          isHf: false
          isPro: false
          name: Florisvdf
          type: user
        html: "<p>I'm running into an error when I try to load this model. When I\
          \ run:</p>\n<p><code>model = AutoModelForCausalLM.from_pretrained(\"OATML-Markslab/Tranception_Large\"\
          )</code></p>\n<p>I get:</p>\n<pre><code>KeyError                       \
          \           Traceback (most recent call last)\nInput In [15], in &lt;cell\
          \ line: 2&gt;()\n      1 #tokenizer = AutoTokenizer.from_pretrained(\"OATML-Markslab/Tranception_Large\"\
          )\n----&gt; 2 model = AutoModelForCausalLM.from_pretrained(\"OATML-Markslab/Tranception_Large\"\
          )\n\nFile ~/work/phd/projects/protein-engineering-benchmark/conda_env/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:434,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    432 hub_kwargs = {name: kwargs.pop(name) for\
          \ name in hub_kwargs_names if name in kwargs}\n    433 if not isinstance(config,\
          \ PretrainedConfig):\n--&gt; 434     config, kwargs = AutoConfig.from_pretrained(\n\
          \    435         pretrained_model_name_or_path,\n    436         return_unused_kwargs=True,\n\
          \    437         trust_remote_code=trust_remote_code,\n    438         **hub_kwargs,\n\
          \    439         **kwargs,\n    440     )\n    441 if hasattr(config, \"\
          auto_map\") and cls.__name__ in config.auto_map:\n    442     if not trust_remote_code:\n\
          \nFile ~/work/phd/projects/protein-engineering-benchmark/conda_env/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:873,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \    871     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\n    872 elif \"model_type\" in config_dict:\n--&gt; 873   \
          \  config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\n    874 \
          \    return config_class.from_dict(config_dict, **unused_kwargs)\n    875\
          \ else:\n    876     # Fallback: use pattern matching on the string.\n \
          \   877     # We go from longer names to shorter names to catch roberta\
          \ before bert (for instance)\n\nFile ~/work/phd/projects/protein-engineering-benchmark/conda_env/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:579,\
          \ in _LazyConfigMapping.__getitem__(self, key)\n    577     return self._extra_content[key]\n\
          \    578 if key not in self._mapping:\n--&gt; 579     raise KeyError(key)\n\
          \    580 value = self._mapping[key]\n    581 module_name = model_type_to_module_name(key)\n\
          \nKeyError: 'tranception'\n</code></pre>\n<p>Is this a bug or is there something\
          \ I can do to fix this?</p>\n<p>OS: macOS 12.6 21G115 arm64<br>Transformers\
          \ version 4.26.1</p>\n"
        raw: "I'm running into an error when I try to load this model. When I run:\n\
          \n`model = AutoModelForCausalLM.from_pretrained(\"OATML-Markslab/Tranception_Large\"\
          )`\n\nI get:\n\n\n```\nKeyError                                  Traceback\
          \ (most recent call last)\nInput In [15], in <cell line: 2>()\n      1 #tokenizer\
          \ = AutoTokenizer.from_pretrained(\"OATML-Markslab/Tranception_Large\")\n\
          ----> 2 model = AutoModelForCausalLM.from_pretrained(\"OATML-Markslab/Tranception_Large\"\
          )\n\nFile ~/work/phd/projects/protein-engineering-benchmark/conda_env/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:434,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    432 hub_kwargs = {name: kwargs.pop(name) for\
          \ name in hub_kwargs_names if name in kwargs}\n    433 if not isinstance(config,\
          \ PretrainedConfig):\n--> 434     config, kwargs = AutoConfig.from_pretrained(\n\
          \    435         pretrained_model_name_or_path,\n    436         return_unused_kwargs=True,\n\
          \    437         trust_remote_code=trust_remote_code,\n    438         **hub_kwargs,\n\
          \    439         **kwargs,\n    440     )\n    441 if hasattr(config, \"\
          auto_map\") and cls.__name__ in config.auto_map:\n    442     if not trust_remote_code:\n\
          \nFile ~/work/phd/projects/protein-engineering-benchmark/conda_env/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:873,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \    871     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\n    872 elif \"model_type\" in config_dict:\n--> 873     config_class\
          \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\n    874     return config_class.from_dict(config_dict,\
          \ **unused_kwargs)\n    875 else:\n    876     # Fallback: use pattern matching\
          \ on the string.\n    877     # We go from longer names to shorter names\
          \ to catch roberta before bert (for instance)\n\nFile ~/work/phd/projects/protein-engineering-benchmark/conda_env/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:579,\
          \ in _LazyConfigMapping.__getitem__(self, key)\n    577     return self._extra_content[key]\n\
          \    578 if key not in self._mapping:\n--> 579     raise KeyError(key)\n\
          \    580 value = self._mapping[key]\n    581 module_name = model_type_to_module_name(key)\n\
          \nKeyError: 'tranception'\n```\n\nIs this a bug or is there something I\
          \ can do to fix this?\n\nOS: macOS 12.6 21G115 arm64 \nTransformers version\
          \ 4.26.1"
        updatedAt: '2023-02-17T14:05:10.518Z'
      numEdits: 1
      reactions: []
    id: 63ef88aae2319f35318b97a4
    type: comment
  author: Florisvdf
  content: "I'm running into an error when I try to load this model. When I run:\n\
    \n`model = AutoModelForCausalLM.from_pretrained(\"OATML-Markslab/Tranception_Large\"\
    )`\n\nI get:\n\n\n```\nKeyError                                  Traceback (most\
    \ recent call last)\nInput In [15], in <cell line: 2>()\n      1 #tokenizer =\
    \ AutoTokenizer.from_pretrained(\"OATML-Markslab/Tranception_Large\")\n----> 2\
    \ model = AutoModelForCausalLM.from_pretrained(\"OATML-Markslab/Tranception_Large\"\
    )\n\nFile ~/work/phd/projects/protein-engineering-benchmark/conda_env/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:434,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    432 hub_kwargs = {name: kwargs.pop(name) for name in hub_kwargs_names\
    \ if name in kwargs}\n    433 if not isinstance(config, PretrainedConfig):\n-->\
    \ 434     config, kwargs = AutoConfig.from_pretrained(\n    435         pretrained_model_name_or_path,\n\
    \    436         return_unused_kwargs=True,\n    437         trust_remote_code=trust_remote_code,\n\
    \    438         **hub_kwargs,\n    439         **kwargs,\n    440     )\n   \
    \ 441 if hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map:\n\
    \    442     if not trust_remote_code:\n\nFile ~/work/phd/projects/protein-engineering-benchmark/conda_env/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:873,\
    \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
    \    871     return config_class.from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs)\n    872 elif \"model_type\" in config_dict:\n--> 873     config_class\
    \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\n    874     return config_class.from_dict(config_dict,\
    \ **unused_kwargs)\n    875 else:\n    876     # Fallback: use pattern matching\
    \ on the string.\n    877     # We go from longer names to shorter names to catch\
    \ roberta before bert (for instance)\n\nFile ~/work/phd/projects/protein-engineering-benchmark/conda_env/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:579,\
    \ in _LazyConfigMapping.__getitem__(self, key)\n    577     return self._extra_content[key]\n\
    \    578 if key not in self._mapping:\n--> 579     raise KeyError(key)\n    580\
    \ value = self._mapping[key]\n    581 module_name = model_type_to_module_name(key)\n\
    \nKeyError: 'tranception'\n```\n\nIs this a bug or is there something I can do\
    \ to fix this?\n\nOS: macOS 12.6 21G115 arm64 \nTransformers version 4.26.1"
  created_at: 2023-02-17 14:01:14+00:00
  edited: true
  hidden: false
  id: 63ef88aae2319f35318b97a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/188067aa1e3f8e69fe252a0346209f28.svg
      fullname: Zach Nussbaum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zpn
      type: user
    createdAt: '2023-07-20T14:24:36.000Z'
    data:
      edited: false
      editors:
      - zpn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9688465595245361
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/188067aa1e3f8e69fe252a0346209f28.svg
          fullname: Zach Nussbaum
          isHf: false
          isPro: false
          name: zpn
          type: user
        html: '<p>Since the model type <code>Tranception</code> isn''t part of Huggingface,
          you can''t directly call AutoModel. The model code needs to be uploaded
          here (similar to how they have it for other models like <code>Bert</code>)
          or it needs to be added to the Transformers library. If the modeling code
          was uploaded here and in the correct format, you could load it with <code>AutoModelForCausalLM.from_pretrained(...,
          trust_remote_code=True)</code> but would require decent chunk of time to
          part over if I had to guess. Your best bet might be following the repo to
          download the weights and load them with the code there</p>

          '
        raw: Since the model type `Tranception` isn't part of Huggingface, you can't
          directly call AutoModel. The model code needs to be uploaded here (similar
          to how they have it for other models like `Bert`) or it needs to be added
          to the Transformers library. If the modeling code was uploaded here and
          in the correct format, you could load it with `AutoModelForCausalLM.from_pretrained(...,
          trust_remote_code=True)` but would require decent chunk of time to part
          over if I had to guess. Your best bet might be following the repo to download
          the weights and load them with the code there
        updatedAt: '2023-07-20T14:24:36.481Z'
      numEdits: 0
      reactions: []
    id: 64b943a4f8bf823a61f96546
    type: comment
  author: zpn
  content: Since the model type `Tranception` isn't part of Huggingface, you can't
    directly call AutoModel. The model code needs to be uploaded here (similar to
    how they have it for other models like `Bert`) or it needs to be added to the
    Transformers library. If the modeling code was uploaded here and in the correct
    format, you could load it with `AutoModelForCausalLM.from_pretrained(..., trust_remote_code=True)`
    but would require decent chunk of time to part over if I had to guess. Your best
    bet might be following the repo to download the weights and load them with the
    code there
  created_at: 2023-07-20 13:24:36+00:00
  edited: false
  hidden: false
  id: 64b943a4f8bf823a61f96546
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: OATML-Markslab/Tranception_Large
repo_type: model
status: open
target_branch: null
title: Key error when loading the model using AutoModelForCausalLM or AutoModelWithLMHead
