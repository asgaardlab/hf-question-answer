!!python/object:huggingface_hub.community.DiscussionWithDetails
author: arjan-hada
conflicting_files: null
created_at: 2023-09-11 19:19:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/de32edb65d18364f34b851e6a05032c2.svg
      fullname: Arjan Hada
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arjan-hada
      type: user
    createdAt: '2023-09-11T20:19:06.000Z'
    data:
      edited: false
      editors:
      - arjan-hada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33147355914115906
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/de32edb65d18364f34b851e6a05032c2.svg
          fullname: Arjan Hada
          isHf: false
          isPro: false
          name: arjan-hada
          type: user
        html: '<p>model_ckpt = "OATML-Markslab/Tranception_Large"<br>tokenizer_tranc
          = AutoTokenizer.from_pretrained(model_ckpt)</p>

          <h2 id="i-get">I get:</h2>

          <p>KeyError                                  Traceback (most recent call
          last)<br>Cell In[36], line 2<br>      1 model_ckpt_tranc = "OATML-Markslab/Tranception_Large"<br>----&gt;
          2 tokenizer_tranc = AutoTokenizer.from_pretrained(model_ckpt_tranc)</p>

          <p>File ~/micromamba/envs/data-science/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:701,
          in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,
          **kwargs)<br>    699 if config_tokenizer_class is None:<br>    700     if
          not isinstance(config, PretrainedConfig):<br>--&gt; 701         config =
          AutoConfig.from_pretrained(<br>    702             pretrained_model_name_or_path,
          trust_remote_code=trust_remote_code, **kwargs<br>    703         )<br>    704     config_tokenizer_class
          = config.tokenizer_class<br>    705     if hasattr(config, "auto_map") and
          "AutoTokenizer" in config.auto_map:</p>

          <p>File ~/micromamba/envs/data-science/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1039,
          in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)<br>   1037     return
          config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)<br>   1038
          elif "model_type" in config_dict:<br>-&gt; 1039     config_class = CONFIG_MAPPING[config_dict["model_type"]]<br>   1040     return
          config_class.from_dict(config_dict, **unused_kwargs)<br>   1041 else:<br>   1042     #
          Fallback: use pattern matching on the string.<br>   1043     # We go from
          longer names to shorter names to catch roberta before bert (for instance)</p>

          <p>File ~/micromamba/envs/data-science/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:734,
          in _LazyConfigMapping.<strong>getitem</strong>(self, key)<br>    732     return
          self._extra_content[key]<br>    733 if key not in self._mapping:<br>--&gt;
          734     raise KeyError(key)<br>    735 value = self._mapping[key]<br>    736
          module_name = model_type_to_module_name(key)</p>

          <p>KeyError: ''tranception''</p>

          '
        raw: "model_ckpt = \"OATML-Markslab/Tranception_Large\"\r\ntokenizer_tranc\
          \ = AutoTokenizer.from_pretrained(model_ckpt)\r\n\r\nI get:\r\n---------------------------------------------------------------------------\r\
          \nKeyError                                  Traceback (most recent call\
          \ last)\r\nCell In[36], line 2\r\n      1 model_ckpt_tranc = \"OATML-Markslab/Tranception_Large\"\
          \r\n----> 2 tokenizer_tranc = AutoTokenizer.from_pretrained(model_ckpt_tranc)\r\
          \n\r\nFile ~/micromamba/envs/data-science/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:701,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\r\n    699 if config_tokenizer_class is None:\r\n    700   \
          \  if not isinstance(config, PretrainedConfig):\r\n--> 701         config\
          \ = AutoConfig.from_pretrained(\r\n    702             pretrained_model_name_or_path,\
          \ trust_remote_code=trust_remote_code, **kwargs\r\n    703         )\r\n\
          \    704     config_tokenizer_class = config.tokenizer_class\r\n    705\
          \     if hasattr(config, \"auto_map\") and \"AutoTokenizer\" in config.auto_map:\r\
          \n\r\nFile ~/micromamba/envs/data-science/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1039,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\
          \n   1037     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\r\n   1038 elif \"model_type\" in config_dict:\r\n-> 1039  \
          \   config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\n   1040\
          \     return config_class.from_dict(config_dict, **unused_kwargs)\r\n  \
          \ 1041 else:\r\n   1042     # Fallback: use pattern matching on the string.\r\
          \n   1043     # We go from longer names to shorter names to catch roberta\
          \ before bert (for instance)\r\n\r\nFile ~/micromamba/envs/data-science/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:734,\
          \ in _LazyConfigMapping.__getitem__(self, key)\r\n    732     return self._extra_content[key]\r\
          \n    733 if key not in self._mapping:\r\n--> 734     raise KeyError(key)\r\
          \n    735 value = self._mapping[key]\r\n    736 module_name = model_type_to_module_name(key)\r\
          \n\r\nKeyError: 'tranception'\r\n"
        updatedAt: '2023-09-11T20:19:06.346Z'
      numEdits: 0
      reactions: []
    id: 64ff763af322f915664f4f7b
    type: comment
  author: arjan-hada
  content: "model_ckpt = \"OATML-Markslab/Tranception_Large\"\r\ntokenizer_tranc =\
    \ AutoTokenizer.from_pretrained(model_ckpt)\r\n\r\nI get:\r\n---------------------------------------------------------------------------\r\
    \nKeyError                                  Traceback (most recent call last)\r\
    \nCell In[36], line 2\r\n      1 model_ckpt_tranc = \"OATML-Markslab/Tranception_Large\"\
    \r\n----> 2 tokenizer_tranc = AutoTokenizer.from_pretrained(model_ckpt_tranc)\r\
    \n\r\nFile ~/micromamba/envs/data-science/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:701,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    699 if config_tokenizer_class is None:\r\n    700     if not\
    \ isinstance(config, PretrainedConfig):\r\n--> 701         config = AutoConfig.from_pretrained(\r\
    \n    702             pretrained_model_name_or_path, trust_remote_code=trust_remote_code,\
    \ **kwargs\r\n    703         )\r\n    704     config_tokenizer_class = config.tokenizer_class\r\
    \n    705     if hasattr(config, \"auto_map\") and \"AutoTokenizer\" in config.auto_map:\r\
    \n\r\nFile ~/micromamba/envs/data-science/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1039,\
    \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\
    \n   1037     return config_class.from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs)\r\n   1038 elif \"model_type\" in config_dict:\r\n-> 1039     config_class\
    \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\n   1040     return config_class.from_dict(config_dict,\
    \ **unused_kwargs)\r\n   1041 else:\r\n   1042     # Fallback: use pattern matching\
    \ on the string.\r\n   1043     # We go from longer names to shorter names to\
    \ catch roberta before bert (for instance)\r\n\r\nFile ~/micromamba/envs/data-science/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:734,\
    \ in _LazyConfigMapping.__getitem__(self, key)\r\n    732     return self._extra_content[key]\r\
    \n    733 if key not in self._mapping:\r\n--> 734     raise KeyError(key)\r\n\
    \    735 value = self._mapping[key]\r\n    736 module_name = model_type_to_module_name(key)\r\
    \n\r\nKeyError: 'tranception'\r\n"
  created_at: 2023-09-11 19:19:06+00:00
  edited: false
  hidden: false
  id: 64ff763af322f915664f4f7b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: OATML-Markslab/Tranception_Large
repo_type: model
status: open
target_branch: null
title: Key error when loading the model using AutoTokenizer.from_pretrained
