!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Pradeep1995
conflicting_files: null
created_at: 2023-12-28 12:50:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
      fullname: Pradeep T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pradeep1995
      type: user
    createdAt: '2023-12-28T12:50:05.000Z'
    data:
      edited: false
      editors:
      - Pradeep1995
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5866125226020813
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599822346546-noauth.jpeg?w=200&h=200&f=face
          fullname: Pradeep T
          isHf: false
          isPro: false
          name: Pradeep1995
          type: user
        html: "<p>I am finetuning the mistral model using the following configurations</p>\n\
          <pre><code>training_arguments = TrainingArguments(\n    output_dir=output_dir,\n\
          \    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n\
          \    optim=optim,\n    save_steps=save_steps,\n    logging_strategy=\"steps\"\
          ,\n    logging_steps=10,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n\
          \    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=13000,\n\
          \    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n\
          \    lr_scheduler_type=lr_scheduler_type\n)\ntrainer = SFTTrainer(\n   \
          \ model=peft_model,\n    train_dataset=data,\n    peft_config=peft_config,\n\
          \    dataset_text_field=\" column name\",\n    max_seq_length=3000,\n  \
          \  tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n\
          )\ntrainer.train()\n</code></pre>\n<p>during this training I am getting\
          \ the multiple checkpoints in the specified output directory <code>output_dir</code>.</p>\n\
          <p>Once the model training is over I can save the model using </p>\n<pre><code>trainer.save_model()\n\
          </code></pre>\n<p>Not only that i can save the final model using</p>\n<pre><code>trainer.model.save_pretrained(\"\
          path\")\n</code></pre>\n<p>So I bit confused. Which is the actual way to\
          \ store the adapter after PEFT based lora fine-tuning</p>\n<p>whether it\
          \ is<br>1 - Take the least loss checkpoint folder from the <code>output_dir</code><br>or<br>2\
          \ - save the adapter using </p>\n<pre><code>trainer.save_model()\n</code></pre>\n\
          <p>or<br>3 - this method</p>\n<pre><code>trainer.model.save_pretrained(\"\
          path\")\n</code></pre>\n"
        raw: "I am finetuning the mistral model using the following configurations\r\
          \n```\r\ntraining_arguments = TrainingArguments(\r\n    output_dir=output_dir,\r\
          \n    per_device_train_batch_size=per_device_train_batch_size,\r\n    gradient_accumulation_steps=gradient_accumulation_steps,\r\
          \n    optim=optim,\r\n    save_steps=save_steps,\r\n    logging_strategy=\"\
          steps\",\r\n    logging_steps=10,\r\n    learning_rate=learning_rate,\r\n\
          \    weight_decay=weight_decay,\r\n    fp16=fp16,\r\n    bf16=bf16,\r\n\
          \    max_grad_norm=max_grad_norm,\r\n    max_steps=13000,\r\n    warmup_ratio=warmup_ratio,\r\
          \n    group_by_length=group_by_length,\r\n    lr_scheduler_type=lr_scheduler_type\r\
          \n)\r\ntrainer = SFTTrainer(\r\n    model=peft_model,\r\n    train_dataset=data,\r\
          \n    peft_config=peft_config,\r\n    dataset_text_field=\" column name\"\
          ,\r\n    max_seq_length=3000,\r\n    tokenizer=tokenizer,\r\n    args=training_arguments,\r\
          \n    packing=packing,\r\n)\r\ntrainer.train()\r\n```\r\n\r\nduring this\
          \ training I am getting the multiple checkpoints in the specified output\
          \ directory ```output_dir```.\r\n\r\nOnce the model training is over I can\
          \ save the model using \r\n```\r\ntrainer.save_model()\r\n```\r\nNot only\
          \ that i can save the final model using\r\n```\r\ntrainer.model.save_pretrained(\"\
          path\")\r\n```\r\n\r\nSo I bit confused. Which is the actual way to store\
          \ the adapter after PEFT based lora fine-tuning\r\n\r\nwhether it is\r\n\
          1 - Take the least loss checkpoint folder from the ```output_dir```\r\n\
          or\r\n2 - save the adapter using \r\n```\r\ntrainer.save_model()\r\n```\r\
          \nor\r\n3 - this method\r\n```\r\ntrainer.model.save_pretrained(\"path\"\
          )\r\n```"
        updatedAt: '2023-12-28T12:50:05.705Z'
      numEdits: 0
      reactions: []
    id: 658d6efd6f797d950bb07518
    type: comment
  author: Pradeep1995
  content: "I am finetuning the mistral model using the following configurations\r\
    \n```\r\ntraining_arguments = TrainingArguments(\r\n    output_dir=output_dir,\r\
    \n    per_device_train_batch_size=per_device_train_batch_size,\r\n    gradient_accumulation_steps=gradient_accumulation_steps,\r\
    \n    optim=optim,\r\n    save_steps=save_steps,\r\n    logging_strategy=\"steps\"\
    ,\r\n    logging_steps=10,\r\n    learning_rate=learning_rate,\r\n    weight_decay=weight_decay,\r\
    \n    fp16=fp16,\r\n    bf16=bf16,\r\n    max_grad_norm=max_grad_norm,\r\n   \
    \ max_steps=13000,\r\n    warmup_ratio=warmup_ratio,\r\n    group_by_length=group_by_length,\r\
    \n    lr_scheduler_type=lr_scheduler_type\r\n)\r\ntrainer = SFTTrainer(\r\n  \
    \  model=peft_model,\r\n    train_dataset=data,\r\n    peft_config=peft_config,\r\
    \n    dataset_text_field=\" column name\",\r\n    max_seq_length=3000,\r\n   \
    \ tokenizer=tokenizer,\r\n    args=training_arguments,\r\n    packing=packing,\r\
    \n)\r\ntrainer.train()\r\n```\r\n\r\nduring this training I am getting the multiple\
    \ checkpoints in the specified output directory ```output_dir```.\r\n\r\nOnce\
    \ the model training is over I can save the model using \r\n```\r\ntrainer.save_model()\r\
    \n```\r\nNot only that i can save the final model using\r\n```\r\ntrainer.model.save_pretrained(\"\
    path\")\r\n```\r\n\r\nSo I bit confused. Which is the actual way to store the\
    \ adapter after PEFT based lora fine-tuning\r\n\r\nwhether it is\r\n1 - Take the\
    \ least loss checkpoint folder from the ```output_dir```\r\nor\r\n2 - save the\
    \ adapter using \r\n```\r\ntrainer.save_model()\r\n```\r\nor\r\n3 - this method\r\
    \n```\r\ntrainer.model.save_pretrained(\"path\")\r\n```"
  created_at: 2023-12-28 12:50:05+00:00
  edited: false
  hidden: false
  id: 658d6efd6f797d950bb07518
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 42
repo_id: openchat/openchat_3.5
repo_type: model
status: open
target_branch: null
title: Which is the actual way to store the Adapter after PEFT finetuning
