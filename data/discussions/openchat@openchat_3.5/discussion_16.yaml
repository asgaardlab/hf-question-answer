!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aslawliet
conflicting_files: null
created_at: 2023-11-13 15:59:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
      fullname: Lawliet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aslawliet
      type: user
    createdAt: '2023-11-13T15:59:01.000Z'
    data:
      edited: false
      editors:
      - aslawliet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3292107880115509
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
          fullname: Lawliet
          isHf: false
          isPro: false
          name: aslawliet
          type: user
        html: "<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\
          import torch\n\nrepo = \"openchat/openchat_3.5\"\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          repo, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"\
          auto\")\ntokenizer = AutoTokenizer.from_pretrained(repo)\n</code></pre>\n\
          <p>Error:</p>\n<pre><code>---------------------------------------------------------------------------\n\
          KeyError                                  Traceback (most recent call last)\n\
          Cell In[21], line 5\n      2 import torch\n      4 repo = \"openchat/openchat_3.5\"\
          \n----&gt; 5 model = AutoModelForCausalLM.from_pretrained(\n      6 repo,\
          \ torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\"\
          )\n      7 tokenizer = AutoTokenizer.from_pretrained(repo)\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:527,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    524 if kwargs.get(\"quantization_config\"\
          , None) is not None:\n    525     _ = kwargs.pop(\"quantization_config\"\
          )\n--&gt; 527 config, kwargs = AutoConfig.from_pretrained(\n    528    \
          \ pretrained_model_name_or_path,\n    529     return_unused_kwargs=True,\n\
          \    530     trust_remote_code=trust_remote_code,\n    531     code_revision=code_revision,\n\
          \    532     _commit_hash=commit_hash,\n    533     **hub_kwargs,\n    534\
          \     **kwargs,\n    535 )\n    537 # if torch_dtype=auto was passed here,\
          \ ensure to pass it on\n    538 if kwargs_orig.get(\"torch_dtype\", None)\
          \ == \"auto\":\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1041,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \   1039     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\n   1040 elif \"model_type\" in config_dict:\n-&gt; 1041   \
          \  config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\n   1042 \
          \    return config_class.from_dict(config_dict, **unused_kwargs)\n   1043\
          \ else:\n   1044     # Fallback: use pattern matching on the string.\n \
          \  1045     # We go from longer names to shorter names to catch roberta\
          \ before bert (for instance)\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:734,\
          \ in _LazyConfigMapping.__getitem__(self, key)\n    732     return self._extra_content[key]\n\
          \    733 if key not in self._mapping:\n--&gt; 734     raise KeyError(key)\n\
          \    735 value = self._mapping[key]\n    736 module_name = model_type_to_module_name(key)\n\
          \nKeyError: 'mistral'\n</code></pre>\n"
        raw: "```\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
          \nimport torch\r\n\r\nrepo = \"openchat/openchat_3.5\"\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\
          \nrepo, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"\
          auto\")\r\ntokenizer = AutoTokenizer.from_pretrained(repo)\r\n```\r\nError:\r\
          \n```\r\n---------------------------------------------------------------------------\r\
          \nKeyError                                  Traceback (most recent call\
          \ last)\r\nCell In[21], line 5\r\n      2 import torch\r\n      4 repo =\
          \ \"openchat/openchat_3.5\"\r\n----> 5 model = AutoModelForCausalLM.from_pretrained(\r\
          \n      6 repo, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"\
          auto\")\r\n      7 tokenizer = AutoTokenizer.from_pretrained(repo)\r\n\r\
          \nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:527,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\r\n    524 if kwargs.get(\"quantization_config\"\
          , None) is not None:\r\n    525     _ = kwargs.pop(\"quantization_config\"\
          )\r\n--> 527 config, kwargs = AutoConfig.from_pretrained(\r\n    528   \
          \  pretrained_model_name_or_path,\r\n    529     return_unused_kwargs=True,\r\
          \n    530     trust_remote_code=trust_remote_code,\r\n    531     code_revision=code_revision,\r\
          \n    532     _commit_hash=commit_hash,\r\n    533     **hub_kwargs,\r\n\
          \    534     **kwargs,\r\n    535 )\r\n    537 # if torch_dtype=auto was\
          \ passed here, ensure to pass it on\r\n    538 if kwargs_orig.get(\"torch_dtype\"\
          , None) == \"auto\":\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1041,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\
          \n   1039     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\r\n   1040 elif \"model_type\" in config_dict:\r\n-> 1041  \
          \   config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\n   1042\
          \     return config_class.from_dict(config_dict, **unused_kwargs)\r\n  \
          \ 1043 else:\r\n   1044     # Fallback: use pattern matching on the string.\r\
          \n   1045     # We go from longer names to shorter names to catch roberta\
          \ before bert (for instance)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:734,\
          \ in _LazyConfigMapping.__getitem__(self, key)\r\n    732     return self._extra_content[key]\r\
          \n    733 if key not in self._mapping:\r\n--> 734     raise KeyError(key)\r\
          \n    735 value = self._mapping[key]\r\n    736 module_name = model_type_to_module_name(key)\r\
          \n\r\nKeyError: 'mistral'\r\n```\r\n"
        updatedAt: '2023-11-13T15:59:01.076Z'
      numEdits: 0
      reactions: []
    id: 655247c546569cb3b447e422
    type: comment
  author: aslawliet
  content: "```\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\
    import torch\r\n\r\nrepo = \"openchat/openchat_3.5\"\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\
    \nrepo, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\"\
    )\r\ntokenizer = AutoTokenizer.from_pretrained(repo)\r\n```\r\nError:\r\n```\r\
    \n---------------------------------------------------------------------------\r\
    \nKeyError                                  Traceback (most recent call last)\r\
    \nCell In[21], line 5\r\n      2 import torch\r\n      4 repo = \"openchat/openchat_3.5\"\
    \r\n----> 5 model = AutoModelForCausalLM.from_pretrained(\r\n      6 repo, torch_dtype=torch.bfloat16,\
    \ trust_remote_code=True, device_map=\"auto\")\r\n      7 tokenizer = AutoTokenizer.from_pretrained(repo)\r\
    \n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:527,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\r\n    524 if kwargs.get(\"quantization_config\", None) is not None:\r\
    \n    525     _ = kwargs.pop(\"quantization_config\")\r\n--> 527 config, kwargs\
    \ = AutoConfig.from_pretrained(\r\n    528     pretrained_model_name_or_path,\r\
    \n    529     return_unused_kwargs=True,\r\n    530     trust_remote_code=trust_remote_code,\r\
    \n    531     code_revision=code_revision,\r\n    532     _commit_hash=commit_hash,\r\
    \n    533     **hub_kwargs,\r\n    534     **kwargs,\r\n    535 )\r\n    537 #\
    \ if torch_dtype=auto was passed here, ensure to pass it on\r\n    538 if kwargs_orig.get(\"\
    torch_dtype\", None) == \"auto\":\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1041,\
    \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\
    \n   1039     return config_class.from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs)\r\n   1040 elif \"model_type\" in config_dict:\r\n-> 1041     config_class\
    \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\r\n   1042     return config_class.from_dict(config_dict,\
    \ **unused_kwargs)\r\n   1043 else:\r\n   1044     # Fallback: use pattern matching\
    \ on the string.\r\n   1045     # We go from longer names to shorter names to\
    \ catch roberta before bert (for instance)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:734,\
    \ in _LazyConfigMapping.__getitem__(self, key)\r\n    732     return self._extra_content[key]\r\
    \n    733 if key not in self._mapping:\r\n--> 734     raise KeyError(key)\r\n\
    \    735 value = self._mapping[key]\r\n    736 module_name = model_type_to_module_name(key)\r\
    \n\r\nKeyError: 'mistral'\r\n```\r\n"
  created_at: 2023-11-13 15:59:01+00:00
  edited: false
  hidden: false
  id: 655247c546569cb3b447e422
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-11-16T07:04:59.000Z'
    data:
      edited: false
      editors:
      - s3nh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8811227083206177
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
          fullname: s3nh
          isHf: false
          isPro: false
          name: s3nh
          type: user
        html: '<p>Hello, this error strictly indicate that you have to upgrade your
          version of transformers library. simple pip install --upgrade transformers
          should help, </p>

          '
        raw: 'Hello, this error strictly indicate that you have to upgrade your version
          of transformers library. simple pip install --upgrade transformers should
          help, '
        updatedAt: '2023-11-16T07:04:59.533Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - imone
        - aslawliet
    id: 6555bf1b38985840b3743e66
    type: comment
  author: s3nh
  content: 'Hello, this error strictly indicate that you have to upgrade your version
    of transformers library. simple pip install --upgrade transformers should help, '
  created_at: 2023-11-16 07:04:59+00:00
  edited: false
  hidden: false
  id: 6555bf1b38985840b3743e66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
      fullname: Lawliet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aslawliet
      type: user
    createdAt: '2023-11-16T08:37:03.000Z'
    data:
      edited: false
      editors:
      - aslawliet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9351059794425964
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
          fullname: Lawliet
          isHf: false
          isPro: false
          name: aslawliet
          type: user
        html: '<blockquote>

          <p>Hello, this error strictly indicate that you have to upgrade your version
          of transformers library. simple pip install --upgrade transformers should
          help,</p>

          </blockquote>

          <p>Tried many a times, its an issue which is occurring while using notebook
          like kaggle, paperspace </p>

          '
        raw: '> Hello, this error strictly indicate that you have to upgrade your
          version of transformers library. simple pip install --upgrade transformers
          should help,


          Tried many a times, its an issue which is occurring while using notebook
          like kaggle, paperspace '
        updatedAt: '2023-11-16T08:37:03.616Z'
      numEdits: 0
      reactions: []
    id: 6555d4af20dfec9e26ced816
    type: comment
  author: aslawliet
  content: '> Hello, this error strictly indicate that you have to upgrade your version
    of transformers library. simple pip install --upgrade transformers should help,


    Tried many a times, its an issue which is occurring while using notebook like
    kaggle, paperspace '
  created_at: 2023-11-16 08:37:03+00:00
  edited: false
  hidden: false
  id: 6555d4af20dfec9e26ced816
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-11-16T09:39:28.000Z'
    data:
      edited: false
      editors:
      - s3nh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6029964089393616
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
          fullname: s3nh
          isHf: false
          isPro: false
          name: s3nh
          type: user
        html: "<pre><code>(\u2026)esolve/main/pytorch_model.bin.index.json:   0%|\
          \          | 0.00/23.9k [00:00&lt;?, ?B/s]\nDownloading shards:   0%|  \
          \        | 0/2 [00:00&lt;?, ?it/s]\npytorch_model-00001-of-00002.bin:  \
          \ 0%|          | 0.00/9.94G [00:00&lt;?, ?B/s]\npytorch_model-00002-of-00002.bin:\
          \   0%|          | 0.00/4.54G [00:00&lt;?, ?B/s]\nLoading checkpoint shards:\
          \   0%|          | 0/2 [00:00&lt;?, ?it/s]\n(\u2026)t_3.5/resolve/main/tokenizer_config.json:\
          \   0%|          | 0.00/1.38k [00:00&lt;?, ?B/s]\ntokenizer.model:   0%|\
          \          | 0.00/493k [00:00&lt;?, ?B/s]\n(\u2026)openchat_3.5/resolve/main/tokenizer.json:\
          \   0%|          | 0.00/1.80M [00:00&lt;?, ?B/s]\n(\u2026)nchat_3.5/resolve/main/added_tokens.json:\
          \   0%|          | 0.00/53.0 [00:00&lt;?, ?B/s]\n(\u2026)3.5/resolve/main/special_tokens_map.json:\
          \   0%|          | 0.00/491 [00:00&lt;?, ?B/s]\nSpecial tokens have been\
          \ added in the vocabulary, make sure the associated word embeddings are\
          \ fine-tuned or trained.\n</code></pre>\n<p>Works fine for me with transformers\
          \ version 4.35.2</p>\n"
        raw: "```\n(\u2026)esolve/main/pytorch_model.bin.index.json:   0%|       \
          \   | 0.00/23.9k [00:00<?, ?B/s]\nDownloading shards:   0%|          | 0/2\
          \ [00:00<?, ?it/s]\npytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G\
          \ [00:00<?, ?B/s]\npytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.54G\
          \ [00:00<?, ?B/s]\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?,\
          \ ?it/s]\n(\u2026)t_3.5/resolve/main/tokenizer_config.json:   0%|      \
          \    | 0.00/1.38k [00:00<?, ?B/s]\ntokenizer.model:   0%|          | 0.00/493k\
          \ [00:00<?, ?B/s]\n(\u2026)openchat_3.5/resolve/main/tokenizer.json:   0%|\
          \          | 0.00/1.80M [00:00<?, ?B/s]\n(\u2026)nchat_3.5/resolve/main/added_tokens.json:\
          \   0%|          | 0.00/53.0 [00:00<?, ?B/s]\n(\u2026)3.5/resolve/main/special_tokens_map.json:\
          \   0%|          | 0.00/491 [00:00<?, ?B/s]\nSpecial tokens have been added\
          \ in the vocabulary, make sure the associated word embeddings are fine-tuned\
          \ or trained.\n```\n\nWorks fine for me with transformers version 4.35.2"
        updatedAt: '2023-11-16T09:39:28.037Z'
      numEdits: 0
      reactions: []
    id: 6555e350bce11b9cde99bfde
    type: comment
  author: s3nh
  content: "```\n(\u2026)esolve/main/pytorch_model.bin.index.json:   0%|         \
    \ | 0.00/23.9k [00:00<?, ?B/s]\nDownloading shards:   0%|          | 0/2 [00:00<?,\
    \ ?it/s]\npytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?,\
    \ ?B/s]\npytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?,\
    \ ?B/s]\nLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n(\u2026\
    )t_3.5/resolve/main/tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?,\
    \ ?B/s]\ntokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]\n(\u2026\
    )openchat_3.5/resolve/main/tokenizer.json:   0%|          | 0.00/1.80M [00:00<?,\
    \ ?B/s]\n(\u2026)nchat_3.5/resolve/main/added_tokens.json:   0%|          | 0.00/53.0\
    \ [00:00<?, ?B/s]\n(\u2026)3.5/resolve/main/special_tokens_map.json:   0%|   \
    \       | 0.00/491 [00:00<?, ?B/s]\nSpecial tokens have been added in the vocabulary,\
    \ make sure the associated word embeddings are fine-tuned or trained.\n```\n\n\
    Works fine for me with transformers version 4.35.2"
  created_at: 2023-11-16 09:39:28+00:00
  edited: false
  hidden: false
  id: 6555e350bce11b9cde99bfde
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
      fullname: Lawliet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aslawliet
      type: user
    createdAt: '2023-11-16T09:42:11.000Z'
    data:
      edited: false
      editors:
      - aslawliet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6279255151748657
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
          fullname: Lawliet
          isHf: false
          isPro: false
          name: aslawliet
          type: user
        html: "<blockquote>\n<pre><code>(\u2026)esolve/main/pytorch_model.bin.index.json:\
          \   0%|          | 0.00/23.9k [00:00&lt;?, ?B/s]\nDownloading shards:  \
          \ 0%|          | 0/2 [00:00&lt;?, ?it/s]\npytorch_model-00001-of-00002.bin:\
          \   0%|          | 0.00/9.94G [00:00&lt;?, ?B/s]\npytorch_model-00002-of-00002.bin:\
          \   0%|          | 0.00/4.54G [00:00&lt;?, ?B/s]\nLoading checkpoint shards:\
          \   0%|          | 0/2 [00:00&lt;?, ?it/s]\n(\u2026)t_3.5/resolve/main/tokenizer_config.json:\
          \   0%|          | 0.00/1.38k [00:00&lt;?, ?B/s]\ntokenizer.model:   0%|\
          \          | 0.00/493k [00:00&lt;?, ?B/s]\n(\u2026)openchat_3.5/resolve/main/tokenizer.json:\
          \   0%|          | 0.00/1.80M [00:00&lt;?, ?B/s]\n(\u2026)nchat_3.5/resolve/main/added_tokens.json:\
          \   0%|          | 0.00/53.0 [00:00&lt;?, ?B/s]\n(\u2026)3.5/resolve/main/special_tokens_map.json:\
          \   0%|          | 0.00/491 [00:00&lt;?, ?B/s]\nSpecial tokens have been\
          \ added in the vocabulary, make sure the associated word embeddings are\
          \ fine-tuned or trained.\n</code></pre>\n<p>Works fine for me with transformers\
          \ version 4.35.2</p>\n</blockquote>\n<p>I was using 4.34.1</p>\n"
        raw: "> ```\n> (\u2026)esolve/main/pytorch_model.bin.index.json:   0%|   \
          \       | 0.00/23.9k [00:00<?, ?B/s]\n> Downloading shards:   0%|      \
          \    | 0/2 [00:00<?, ?it/s]\n> pytorch_model-00001-of-00002.bin:   0%| \
          \         | 0.00/9.94G [00:00<?, ?B/s]\n> pytorch_model-00002-of-00002.bin:\
          \   0%|          | 0.00/4.54G [00:00<?, ?B/s]\n> Loading checkpoint shards:\
          \   0%|          | 0/2 [00:00<?, ?it/s]\n> (\u2026)t_3.5/resolve/main/tokenizer_config.json:\
          \   0%|          | 0.00/1.38k [00:00<?, ?B/s]\n> tokenizer.model:   0%|\
          \          | 0.00/493k [00:00<?, ?B/s]\n> (\u2026)openchat_3.5/resolve/main/tokenizer.json:\
          \   0%|          | 0.00/1.80M [00:00<?, ?B/s]\n> (\u2026)nchat_3.5/resolve/main/added_tokens.json:\
          \   0%|          | 0.00/53.0 [00:00<?, ?B/s]\n> (\u2026)3.5/resolve/main/special_tokens_map.json:\
          \   0%|          | 0.00/491 [00:00<?, ?B/s]\n> Special tokens have been\
          \ added in the vocabulary, make sure the associated word embeddings are\
          \ fine-tuned or trained.\n> ```\n> \n> Works fine for me with transformers\
          \ version 4.35.2\n\nI was using 4.34.1"
        updatedAt: '2023-11-16T09:42:11.595Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6555e3f3f96492362e402d15
    id: 6555e3f3f96492362e402d0c
    type: comment
  author: aslawliet
  content: "> ```\n> (\u2026)esolve/main/pytorch_model.bin.index.json:   0%|     \
    \     | 0.00/23.9k [00:00<?, ?B/s]\n> Downloading shards:   0%|          | 0/2\
    \ [00:00<?, ?it/s]\n> pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G\
    \ [00:00<?, ?B/s]\n> pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.54G\
    \ [00:00<?, ?B/s]\n> Loading checkpoint shards:   0%|          | 0/2 [00:00<?,\
    \ ?it/s]\n> (\u2026)t_3.5/resolve/main/tokenizer_config.json:   0%|          |\
    \ 0.00/1.38k [00:00<?, ?B/s]\n> tokenizer.model:   0%|          | 0.00/493k [00:00<?,\
    \ ?B/s]\n> (\u2026)openchat_3.5/resolve/main/tokenizer.json:   0%|          |\
    \ 0.00/1.80M [00:00<?, ?B/s]\n> (\u2026)nchat_3.5/resolve/main/added_tokens.json:\
    \   0%|          | 0.00/53.0 [00:00<?, ?B/s]\n> (\u2026)3.5/resolve/main/special_tokens_map.json:\
    \   0%|          | 0.00/491 [00:00<?, ?B/s]\n> Special tokens have been added\
    \ in the vocabulary, make sure the associated word embeddings are fine-tuned or\
    \ trained.\n> ```\n> \n> Works fine for me with transformers version 4.35.2\n\n\
    I was using 4.34.1"
  created_at: 2023-11-16 09:42:11+00:00
  edited: false
  hidden: false
  id: 6555e3f3f96492362e402d0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
      fullname: Lawliet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aslawliet
      type: user
    createdAt: '2023-11-16T09:42:11.000Z'
    data:
      status: closed
    id: 6555e3f3f96492362e402d15
    type: status-change
  author: aslawliet
  created_at: 2023-11-16 09:42:11+00:00
  id: 6555e3f3f96492362e402d15
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
      fullname: Lawliet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aslawliet
      type: user
    createdAt: '2023-11-16T09:42:20.000Z'
    data:
      status: open
    id: 6555e3fc9518733aca254f5d
    type: status-change
  author: aslawliet
  created_at: 2023-11-16 09:42:20+00:00
  id: 6555e3fc9518733aca254f5d
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
      fullname: Lawliet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aslawliet
      type: user
    createdAt: '2023-11-16T09:42:25.000Z'
    data:
      status: closed
    id: 6555e401f0cf8603f387e376
    type: status-change
  author: aslawliet
  created_at: 2023-11-16 09:42:25+00:00
  id: 6555e401f0cf8603f387e376
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: openchat/openchat_3.5
repo_type: model
status: closed
target_branch: null
title: Model Not Loading
