!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nicolaschaillan
conflicting_files: null
created_at: 2023-05-21 18:39:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
      fullname: Nicolas Chaillan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicolaschaillan
      type: user
    createdAt: '2023-05-21T19:39:19.000Z'
    data:
      edited: false
      editors:
      - nicolaschaillan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
          fullname: Nicolas Chaillan
          isHf: false
          isPro: false
          name: nicolaschaillan
          type: user
        html: '<p>Expected all tensors to be on the same device, but found at least
          two devices, cuda:3 and cuda:0! (when checking argument for argument mat2
          in method wrapper_CUDA_mm)</p>

          <p>Any clue how to solve that? Thanks!</p>

          '
        raw: "Expected all tensors to be on the same device, but found at least two\
          \ devices, cuda:3 and cuda:0! (when checking argument for argument mat2\
          \ in method wrapper_CUDA_mm)\r\n\r\nAny clue how to solve that? Thanks!"
        updatedAt: '2023-05-21T19:39:19.607Z'
      numEdits: 0
      reactions: []
    id: 646a73671556443f24b77e44
    type: comment
  author: nicolaschaillan
  content: "Expected all tensors to be on the same device, but found at least two\
    \ devices, cuda:3 and cuda:0! (when checking argument for argument mat2 in method\
    \ wrapper_CUDA_mm)\r\n\r\nAny clue how to solve that? Thanks!"
  created_at: 2023-05-21 18:39:19+00:00
  edited: false
  hidden: false
  id: 646a73671556443f24b77e44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-05-22T16:14:15.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: "<p>Hi - thanks for bringing this up! I haven't tested with multi-GPU,\
          \ so it's likely there are some updates needed to the MPT modeling code.\
          \ Could you paste/provide the detailed error message including all python\
          \ lines/traceback? we can go from there.</p>\n<p>If I had to guess, some\
          \ custom matrix multiplication going on in one or more of the MPT-specific\
          \ modeling files is not properly handling torch math with two separate devices\
          \ when using <code>device_map</code> (<em>this would make sense since the\
          \ original model doesn't support <code>accelerate</code> at all and I added\
          \ the basics</em>). It's hard to say without the details. You can also check\
          \ out <a href=\"https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling#designing-a-device-map\"\
          >this section in the accelerate docs</a> as a possible starting point for\
          \ this rabbit hole \U0001F407</p>\n"
        raw: "Hi - thanks for bringing this up! I haven't tested with multi-GPU, so\
          \ it's likely there are some updates needed to the MPT modeling code. Could\
          \ you paste/provide the detailed error message including all python lines/traceback?\
          \ we can go from there.\n\nIf I had to guess, some custom matrix multiplication\
          \ going on in one or more of the MPT-specific modeling files is not properly\
          \ handling torch math with two separate devices when using `device_map`\
          \ (_this would make sense since the original model doesn't support `accelerate`\
          \ at all and I added the basics_). It's hard to say without the details.\
          \ You can also check out [this section in the accelerate docs](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling#designing-a-device-map)\
          \ as a possible starting point for this rabbit hole \U0001F407"
        updatedAt: '2023-05-22T16:14:15.847Z'
      numEdits: 0
      reactions: []
    id: 646b94d75d68f5c15a280beb
    type: comment
  author: pszemraj
  content: "Hi - thanks for bringing this up! I haven't tested with multi-GPU, so\
    \ it's likely there are some updates needed to the MPT modeling code. Could you\
    \ paste/provide the detailed error message including all python lines/traceback?\
    \ we can go from there.\n\nIf I had to guess, some custom matrix multiplication\
    \ going on in one or more of the MPT-specific modeling files is not properly handling\
    \ torch math with two separate devices when using `device_map` (_this would make\
    \ sense since the original model doesn't support `accelerate` at all and I added\
    \ the basics_). It's hard to say without the details. You can also check out [this\
    \ section in the accelerate docs](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling#designing-a-device-map)\
    \ as a possible starting point for this rabbit hole \U0001F407"
  created_at: 2023-05-22 15:14:15+00:00
  edited: false
  hidden: false
  id: 646b94d75d68f5c15a280beb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
      fullname: Nicolas Chaillan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicolaschaillan
      type: user
    createdAt: '2023-05-22T18:26:26.000Z'
    data:
      edited: false
      editors:
      - nicolaschaillan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cec054b5b25602071cccda685e0e03e.svg
          fullname: Nicolas Chaillan
          isHf: false
          isPro: false
          name: nicolaschaillan
          type: user
        html: '<p>I see. I was using DataParallel but I would love instead to use
          device_map, is there any plan from you guys to natively support device_map?
          Would be much easier for most people to be able to spread among a few GPUs...
          many cloud offerings offer this on Azure nowadays etc.</p>

          '
        raw: I see. I was using DataParallel but I would love instead to use device_map,
          is there any plan from you guys to natively support device_map? Would be
          much easier for most people to be able to spread among a few GPUs... many
          cloud offerings offer this on Azure nowadays etc.
        updatedAt: '2023-05-22T18:26:26.890Z'
      numEdits: 0
      reactions: []
    id: 646bb3d2f85ebf65c538af9a
    type: comment
  author: nicolaschaillan
  content: I see. I was using DataParallel but I would love instead to use device_map,
    is there any plan from you guys to natively support device_map? Would be much
    easier for most people to be able to spread among a few GPUs... many cloud offerings
    offer this on Azure nowadays etc.
  created_at: 2023-05-22 17:26:26+00:00
  edited: false
  hidden: false
  id: 646bb3d2f85ebf65c538af9a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2023-05-22T19:43:33.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>Ah, sorry, let me clarify a bit: so it <strong>should</strong> already
          support device_map with the changes I''ve made. I''ve been able to do inference
          with device_map and also do training on a single GPU. </p>

          <p>That said, I''ve only tested a single GPU setup, and from the error you''re
          getting, it seems like adjustments need to be made to the model''s custom
          code. This is because mosaicML did not implement device_map, in the original
          model and their custom code. I haven''t seen the errors in a single GPU
          setup, and so it will be an iterative process to understand what isn''t
          working with multiple devices &amp; debug. For that, I''ll need your full
          error logs, as I''m not using multi-GPU at the moment.</p>

          '
        raw: "Ah, sorry, let me clarify a bit: so it **should** already support device_map\
          \ with the changes I've made. I've been able to do inference with device_map\
          \ and also do training on a single GPU. \n\nThat said, I've only tested\
          \ a single GPU setup, and from the error you're getting, it seems like adjustments\
          \ need to be made to the model's custom code. This is because mosaicML did\
          \ not implement device_map, in the original model and their custom code.\
          \ I haven't seen the errors in a single GPU setup, and so it will be an\
          \ iterative process to understand what isn't working with multiple devices\
          \ & debug. For that, I'll need your full error logs, as I'm not using multi-GPU\
          \ at the moment."
        updatedAt: '2023-05-22T19:43:33.691Z'
      numEdits: 0
      reactions: []
    id: 646bc5e5db697c798a495c4d
    type: comment
  author: pszemraj
  content: "Ah, sorry, let me clarify a bit: so it **should** already support device_map\
    \ with the changes I've made. I've been able to do inference with device_map and\
    \ also do training on a single GPU. \n\nThat said, I've only tested a single GPU\
    \ setup, and from the error you're getting, it seems like adjustments need to\
    \ be made to the model's custom code. This is because mosaicML did not implement\
    \ device_map, in the original model and their custom code. I haven't seen the\
    \ errors in a single GPU setup, and so it will be an iterative process to understand\
    \ what isn't working with multiple devices & debug. For that, I'll need your full\
    \ error logs, as I'm not using multi-GPU at the moment."
  created_at: 2023-05-22 18:43:33+00:00
  edited: false
  hidden: false
  id: 646bc5e5db697c798a495c4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fe0d85b58a49d77bcf100db5ae6872b3.svg
      fullname: Elias
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eliaz
      type: user
    createdAt: '2023-05-28T21:26:27.000Z'
    data:
      edited: true
      editors:
      - eliaz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fe0d85b58a49d77bcf100db5ae6872b3.svg
          fullname: Elias
          isHf: false
          isPro: false
          name: eliaz
          type: user
        html: '<p>I am on very thin ice as I really have no clue what I''m doing here...
          But I would love for this code+log''s to help out solving the multiple GPU
          issue.<br>Feel free to ignore this if the code doesn''t make sense though.<br>The
          execution below is from a multi gpu environment with two Tesla P100-SXM2-16GB   </p>

          <h2 id="code">Code:</h2>

          <p>import os<br>os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"<br>import torch<br>import
          transformers<br>model = transformers.AutoModelForCausalLM.from_pretrained(<br>  ''ethzanalytics/mpt-7b-storywriter-sharded'',<br>  trust_remote_code=True,<br>  torch_dtype=torch.bfloat16,<br>   load_in_8bit=False,<br>   device_map=''auto''<br>)<br>model.eval()<br>#model.to("cuda:0"),
          already done by magic<br>!nvidia-smi</p>

          <p>model_size = sum(t.numel() for t in model.parameters())<br>print(f"Modelsize:
          {model_size/1000**2:.1f}M parameters")<br>tokenizer = transformers.AutoTokenizer.from_pretrained(''ethzanalytics/mpt-7b-storywriter-sharded'')<br>txt
          = """<br>#What is the difference between an Alpaca and<br>"""<br>tokenized_example
          = tokenizer(txt, return_tensors=''pt'')<br>tokenized_example[''input_ids'']<br>outputs
          = model.generate(tokenized_example[''input_ids''].to(0), max_new_tokens=150,
          do_sample=False, top_k=5, top_p=0.95)</p>

          <h2 id="logs-from-execution-below">Logs from execution below:</h2>

          <p>/root/.cache/huggingface/modules/transformers_modules/ethzanalytics/mpt-7b-storywriter-sharded/26347e86eca753ce5dd9a50f21201976f2e8a9b9/attention.py:269:
          UserWarning: Using <code>attn_impl: torch</code>. If your model does not
          use <code>alibi</code> or <code>prefix_lm</code> we recommend using <code>attn_impl:
          flash</code> otherwise we recommend using <code>attn_impl: triton</code>.<br>  warnings.warn(<br>/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1383:
          UserWarning: positional arguments and argument "destination" are deprecated.
          nn.Module.state_dict will not accept them in the future. Refer to <a rel="nofollow"
          href="https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict">https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict</a>
          for details.<br>  warnings.warn(<br>Loading checkpoint shards: 100%<br>7/7
          [00:13&lt;00:00, 1.95s/it]<br>Modelsize: 6649.3M parameters</p>

          <hr>

          <p>RuntimeError                              Traceback (most recent call
          last)<br>Input In [2], in &lt;cell line: 23&gt;()<br>     21 tokenized_example
          = tokenizer(txt, return_tensors=''pt'')<br>     22 tokenized_example[''input_ids'']<br>---&gt;
          23 outputs = model.generate(tokenized_example[''input_ids''].to(0), max_new_tokens=150,
          do_sample=False, top_k=5, top_p=0.95)</p>

          <p>File /usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27,
          in _DecoratorContextManager.<strong>call</strong>..decorate_context(*args,
          **kwargs)<br>     24 @functools.wraps(func)<br>     25 def decorate_context(*args,
          **kwargs):<br>     26     with self.clone():<br>---&gt; 27         return
          func(*args, **kwargs)</p>

          <p>File /usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1515,
          in GenerationMixin.generate(self, inputs, generation_config, logits_processor,
          stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,
          streamer, **kwargs)<br>   1509         raise ValueError(<br>   1510             "num_return_sequences
          has to be 1 when doing greedy search, "<br>   1511             f"but is
          {generation_config.num_return_sequences}."<br>   1512         )<br>   1514     #
          11. run greedy search<br>-&gt; 1515     return self.greedy_search(<br>   1516         input_ids,<br>   1517         logits_processor=logits_processor,<br>   1518         stopping_criteria=stopping_criteria,<br>   1519         pad_token_id=generation_config.pad_token_id,<br>   1520         eos_token_id=generation_config.eos_token_id,<br>   1521         output_scores=generation_config.output_scores,<br>   1522         return_dict_in_generate=generation_config.return_dict_in_generate,<br>   1523         synced_gpus=synced_gpus,<br>   1524         streamer=streamer,<br>   1525         **model_kwargs,<br>   1526     )<br>   1528
          elif is_contrastive_search_gen_mode:<br>   1529     if generation_config.num_return_sequences
          &gt; 1:</p>

          <p>File /usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:2332,
          in GenerationMixin.greedy_search(self, input_ids, logits_processor, stopping_criteria,
          max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states,
          output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)<br>   2329
          model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)<br>   2331
          # forward pass to get next token<br>-&gt; 2332 outputs = self(<br>   2333     **model_inputs,<br>   2334     return_dict=True,<br>   2335     output_attentions=output_attentions,<br>   2336     output_hidden_states=output_hidden_states,<br>   2337
          )<br>   2339 if synced_gpus and this_peer_finished:<br>   2340     continue  #
          don''t waste resources running the code we don''t need</p>

          <p>File /usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110,
          in Module._call_impl(self, *input, **kwargs)<br>   1106 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1107 # this function,
          and just call forward.<br>   1108 if not (self._backward_hooks or self._forward_hooks
          or self._forward_pre_hooks or _global_backward_hooks<br>   1109         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1110     return
          forward_call(*input, **kwargs)<br>   1111 # Do not call functions when jit
          is used<br>   1112 full_backward_hooks, non_full_backward_hooks = [], []</p>

          <p>File /usr/local/lib/python3.8/dist-packages/accelerate/hooks.py:165,
          in add_hook_to_module..new_forward(*args, **kwargs)<br>    163         output
          = old_forward(*args, **kwargs)<br>    164 else:<br>--&gt; 165     output
          = old_forward(*args, **kwargs)<br>    166 return module._hf_hook.post_forward(module,
          output)</p>

          <p>File ~/.cache/huggingface/modules/transformers_modules/ethzanalytics/mpt-7b-storywriter-sharded/26347e86eca753ce5dd9a50f21201976f2e8a9b9/modeling_mpt.py:406,
          in MPTForCausalLM.forward(self, input_ids, past_key_values, attention_mask,
          prefix_mask, sequence_id, labels, return_dict, output_attentions, output_hidden_states,
          use_cache)<br>    394 use_cache = use_cache if use_cache is not None else
          self.config.use_cache<br>    395 outputs = self.transformer(<br>    396     input_ids=input_ids,<br>    397     past_key_values=past_key_values,<br>   (...)<br>    404     use_cache=use_cache,<br>    405
          )<br>--&gt; 406 logits = F.linear(outputs.last_hidden_state, self.transformer.wte.weight)<br>    407
          if self.logit_scale is not None:<br>    408     if self.logit_scale == 0:</p>

          <p>RuntimeError: Expected all tensors to be on the same device, but found
          at least two devices, cuda:1 and cuda:0! (when checking argument for argument
          mat2 in method wrapper_mm)</p>

          '
        raw: "I am on very thin ice as I really have no clue what I'm doing here...\
          \ But I would love for this code+log's to help out solving the multiple\
          \ GPU issue. \nFeel free to ignore this if the code doesn't make sense though.\
          \ \nThe execution below is from a multi gpu environment with two Tesla P100-SXM2-16GB\
          \   \n\nCode:\n---------------------------------------------------------------------------\n\
          import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nimport torch\n\
          import transformers \nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \  'ethzanalytics/mpt-7b-storywriter-sharded',\n  trust_remote_code=True,\n\
          \  torch_dtype=torch.bfloat16,\n   load_in_8bit=False,\n   device_map='auto'\n\
          )\nmodel.eval()\n#model.to(\"cuda:0\"), already done by magic\n!nvidia-smi\n\
          \nmodel_size = sum(t.numel() for t in model.parameters())\nprint(f\"Modelsize:\
          \ {model_size/1000**2:.1f}M parameters\")\ntokenizer = transformers.AutoTokenizer.from_pretrained('ethzanalytics/mpt-7b-storywriter-sharded')\n\
          txt = \"\"\"\\\n#What is the difference between an Alpaca and \n\"\"\"\n\
          tokenized_example = tokenizer(txt, return_tensors='pt')\ntokenized_example['input_ids']\n\
          outputs = model.generate(tokenized_example['input_ids'].to(0), max_new_tokens=150,\
          \ do_sample=False, top_k=5, top_p=0.95)\n\n\nLogs from execution below:\n\
          ---------------------------------------------------------------------------\n\
          \n\n/root/.cache/huggingface/modules/transformers_modules/ethzanalytics/mpt-7b-storywriter-sharded/26347e86eca753ce5dd9a50f21201976f2e8a9b9/attention.py:269:\
          \ UserWarning: Using `attn_impl: torch`. If your model does not use `alibi`\
          \ or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend\
          \ using `attn_impl: triton`.\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1383:\
          \ UserWarning: positional arguments and argument \"destination\" are deprecated.\
          \ nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict\
          \ for details.\n  warnings.warn(\nLoading checkpoint shards: 100%\n7/7 [00:13<00:00,\
          \ 1.95s/it]\nModelsize: 6649.3M parameters\n---------------------------------------------------------------------------\n\
          RuntimeError                              Traceback (most recent call last)\n\
          Input In [2], in <cell line: 23>()\n     21 tokenized_example = tokenizer(txt,\
          \ return_tensors='pt')\n     22 tokenized_example['input_ids']\n---> 23\
          \ outputs = model.generate(tokenized_example['input_ids'].to(0), max_new_tokens=150,\
          \ do_sample=False, top_k=5, top_p=0.95)\n\nFile /usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27,\
          \ in _DecoratorContextManager.__call__.<locals>.decorate_context(*args,\
          \ **kwargs)\n     24 @functools.wraps(func)\n     25 def decorate_context(*args,\
          \ **kwargs):\n     26     with self.clone():\n---> 27         return func(*args,\
          \ **kwargs)\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1515,\
          \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, **kwargs)\n   1509         raise ValueError(\n   1510      \
          \       \"num_return_sequences has to be 1 when doing greedy search, \"\n\
          \   1511             f\"but is {generation_config.num_return_sequences}.\"\
          \n   1512         )\n   1514     # 11. run greedy search\n-> 1515     return\
          \ self.greedy_search(\n   1516         input_ids,\n   1517         logits_processor=logits_processor,\n\
          \   1518         stopping_criteria=stopping_criteria,\n   1519         pad_token_id=generation_config.pad_token_id,\n\
          \   1520         eos_token_id=generation_config.eos_token_id,\n   1521 \
          \        output_scores=generation_config.output_scores,\n   1522       \
          \  return_dict_in_generate=generation_config.return_dict_in_generate,\n\
          \   1523         synced_gpus=synced_gpus,\n   1524         streamer=streamer,\n\
          \   1525         **model_kwargs,\n   1526     )\n   1528 elif is_contrastive_search_gen_mode:\n\
          \   1529     if generation_config.num_return_sequences > 1:\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:2332,\
          \ in GenerationMixin.greedy_search(self, input_ids, logits_processor, stopping_criteria,\
          \ max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states,\
          \ output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\n\
          \   2329 model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\
          \   2331 # forward pass to get next token\n-> 2332 outputs = self(\n   2333\
          \     **model_inputs,\n   2334     return_dict=True,\n   2335     output_attentions=output_attentions,\n\
          \   2336     output_hidden_states=output_hidden_states,\n   2337 )\n   2339\
          \ if synced_gpus and this_peer_finished:\n   2340     continue  # don't\
          \ waste resources running the code we don't need\n\nFile /usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110,\
          \ in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\n   1107 # this function,\
          \ and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks\
          \ or self._forward_pre_hooks or _global_backward_hooks\n   1109        \
          \ or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1110     return\
          \ forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit\
          \ is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\
          \nFile /usr/local/lib/python3.8/dist-packages/accelerate/hooks.py:165, in\
          \ add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163    \
          \     output = old_forward(*args, **kwargs)\n    164 else:\n--> 165    \
          \ output = old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
          \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/ethzanalytics/mpt-7b-storywriter-sharded/26347e86eca753ce5dd9a50f21201976f2e8a9b9/modeling_mpt.py:406,\
          \ in MPTForCausalLM.forward(self, input_ids, past_key_values, attention_mask,\
          \ prefix_mask, sequence_id, labels, return_dict, output_attentions, output_hidden_states,\
          \ use_cache)\n    394 use_cache = use_cache if use_cache is not None else\
          \ self.config.use_cache\n    395 outputs = self.transformer(\n    396  \
          \   input_ids=input_ids,\n    397     past_key_values=past_key_values,\n\
          \   (...)\n    404     use_cache=use_cache,\n    405 )\n--> 406 logits =\
          \ F.linear(outputs.last_hidden_state, self.transformer.wte.weight)\n   \
          \ 407 if self.logit_scale is not None:\n    408     if self.logit_scale\
          \ == 0:\n\nRuntimeError: Expected all tensors to be on the same device,\
          \ but found at least two devices, cuda:1 and cuda:0! (when checking argument\
          \ for argument mat2 in method wrapper_mm)"
        updatedAt: '2023-05-28T21:28:16.374Z'
      numEdits: 1
      reactions: []
    id: 6473c7036cff2f86720ac0c0
    type: comment
  author: eliaz
  content: "I am on very thin ice as I really have no clue what I'm doing here...\
    \ But I would love for this code+log's to help out solving the multiple GPU issue.\
    \ \nFeel free to ignore this if the code doesn't make sense though. \nThe execution\
    \ below is from a multi gpu environment with two Tesla P100-SXM2-16GB   \n\nCode:\n\
    ---------------------------------------------------------------------------\n\
    import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nimport torch\nimport\
    \ transformers \nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
    \  'ethzanalytics/mpt-7b-storywriter-sharded',\n  trust_remote_code=True,\n  torch_dtype=torch.bfloat16,\n\
    \   load_in_8bit=False,\n   device_map='auto'\n)\nmodel.eval()\n#model.to(\"cuda:0\"\
    ), already done by magic\n!nvidia-smi\n\nmodel_size = sum(t.numel() for t in model.parameters())\n\
    print(f\"Modelsize: {model_size/1000**2:.1f}M parameters\")\ntokenizer = transformers.AutoTokenizer.from_pretrained('ethzanalytics/mpt-7b-storywriter-sharded')\n\
    txt = \"\"\"\\\n#What is the difference between an Alpaca and \n\"\"\"\ntokenized_example\
    \ = tokenizer(txt, return_tensors='pt')\ntokenized_example['input_ids']\noutputs\
    \ = model.generate(tokenized_example['input_ids'].to(0), max_new_tokens=150, do_sample=False,\
    \ top_k=5, top_p=0.95)\n\n\nLogs from execution below:\n---------------------------------------------------------------------------\n\
    \n\n/root/.cache/huggingface/modules/transformers_modules/ethzanalytics/mpt-7b-storywriter-sharded/26347e86eca753ce5dd9a50f21201976f2e8a9b9/attention.py:269:\
    \ UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or\
    \ `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using\
    \ `attn_impl: triton`.\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1383:\
    \ UserWarning: positional arguments and argument \"destination\" are deprecated.\
    \ nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict\
    \ for details.\n  warnings.warn(\nLoading checkpoint shards: 100%\n7/7 [00:13<00:00,\
    \ 1.95s/it]\nModelsize: 6649.3M parameters\n---------------------------------------------------------------------------\n\
    RuntimeError                              Traceback (most recent call last)\n\
    Input In [2], in <cell line: 23>()\n     21 tokenized_example = tokenizer(txt,\
    \ return_tensors='pt')\n     22 tokenized_example['input_ids']\n---> 23 outputs\
    \ = model.generate(tokenized_example['input_ids'].to(0), max_new_tokens=150, do_sample=False,\
    \ top_k=5, top_p=0.95)\n\nFile /usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27,\
    \ in _DecoratorContextManager.__call__.<locals>.decorate_context(*args, **kwargs)\n\
    \     24 @functools.wraps(func)\n     25 def decorate_context(*args, **kwargs):\n\
    \     26     with self.clone():\n---> 27         return func(*args, **kwargs)\n\
    \nFile /usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1515,\
    \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
    \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer,\
    \ **kwargs)\n   1509         raise ValueError(\n   1510             \"num_return_sequences\
    \ has to be 1 when doing greedy search, \"\n   1511             f\"but is {generation_config.num_return_sequences}.\"\
    \n   1512         )\n   1514     # 11. run greedy search\n-> 1515     return self.greedy_search(\n\
    \   1516         input_ids,\n   1517         logits_processor=logits_processor,\n\
    \   1518         stopping_criteria=stopping_criteria,\n   1519         pad_token_id=generation_config.pad_token_id,\n\
    \   1520         eos_token_id=generation_config.eos_token_id,\n   1521       \
    \  output_scores=generation_config.output_scores,\n   1522         return_dict_in_generate=generation_config.return_dict_in_generate,\n\
    \   1523         synced_gpus=synced_gpus,\n   1524         streamer=streamer,\n\
    \   1525         **model_kwargs,\n   1526     )\n   1528 elif is_contrastive_search_gen_mode:\n\
    \   1529     if generation_config.num_return_sequences > 1:\n\nFile /usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:2332,\
    \ in GenerationMixin.greedy_search(self, input_ids, logits_processor, stopping_criteria,\
    \ max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states,\
    \ output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\n\
    \   2329 model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\
    \   2331 # forward pass to get next token\n-> 2332 outputs = self(\n   2333  \
    \   **model_inputs,\n   2334     return_dict=True,\n   2335     output_attentions=output_attentions,\n\
    \   2336     output_hidden_states=output_hidden_states,\n   2337 )\n   2339 if\
    \ synced_gpus and this_peer_finished:\n   2340     continue  # don't waste resources\
    \ running the code we don't need\n\nFile /usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110,\
    \ in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\n   1107 # this function, and\
    \ just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks\
    \ or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks\
    \ or _global_forward_pre_hooks):\n-> 1110     return forward_call(*input, **kwargs)\n\
    \   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks,\
    \ non_full_backward_hooks = [], []\n\nFile /usr/local/lib/python3.8/dist-packages/accelerate/hooks.py:165,\
    \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\n    163       \
    \  output = old_forward(*args, **kwargs)\n    164 else:\n--> 165     output =\
    \ old_forward(*args, **kwargs)\n    166 return module._hf_hook.post_forward(module,\
    \ output)\n\nFile ~/.cache/huggingface/modules/transformers_modules/ethzanalytics/mpt-7b-storywriter-sharded/26347e86eca753ce5dd9a50f21201976f2e8a9b9/modeling_mpt.py:406,\
    \ in MPTForCausalLM.forward(self, input_ids, past_key_values, attention_mask,\
    \ prefix_mask, sequence_id, labels, return_dict, output_attentions, output_hidden_states,\
    \ use_cache)\n    394 use_cache = use_cache if use_cache is not None else self.config.use_cache\n\
    \    395 outputs = self.transformer(\n    396     input_ids=input_ids,\n    397\
    \     past_key_values=past_key_values,\n   (...)\n    404     use_cache=use_cache,\n\
    \    405 )\n--> 406 logits = F.linear(outputs.last_hidden_state, self.transformer.wte.weight)\n\
    \    407 if self.logit_scale is not None:\n    408     if self.logit_scale ==\
    \ 0:\n\nRuntimeError: Expected all tensors to be on the same device, but found\
    \ at least two devices, cuda:1 and cuda:0! (when checking argument for argument\
    \ mat2 in method wrapper_mm)"
  created_at: 2023-05-28 20:26:27+00:00
  edited: true
  hidden: false
  id: 6473c7036cff2f86720ac0c0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: ethzanalytics/mpt-7b-storywriter-sharded
repo_type: model
status: open
target_branch: null
title: Well done, I got an error when running on multiple GPU
