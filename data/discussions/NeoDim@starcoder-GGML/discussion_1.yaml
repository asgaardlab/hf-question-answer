!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FenixInDarkSolo
conflicting_files: null
created_at: 2023-05-17 07:36:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-05-17T08:36:59.000Z'
    data:
      edited: false
      editors:
      - FenixInDarkSolo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: '<p>I have download the q5_1.bin and try to run in llama.cpp and koboldcpp,
          but it does not work.<br>I have checked the file SHA256 and it is the same.<br>Here
          is the llama.cpp''s error code:</p>

          <pre><code>main -m ./models/starcoder-ggml-q5_1.bin -t 12 -n -1 -c 2048
          --keep -1 --repeat_last_n 2048 --top_k 160 --top_p 0.95 --color -ins -r
          "User:" --keep -1 --interactive-first

          main: build = 536 (cdd5350)

          main: seed  = 1684312164

          llama.cpp: loading model from ./models/starcoder-ggml-q5_1.bin

          error loading model: missing tok_embeddings.weight

          llama_init_from_file: failed to load model

          llama_init_from_gpt_params: error: failed to load model ''./models/starcoder-ggml-q5_1.bin''

          main: error: unable to load model


          certutil -hashfile starcoder-ggml-q5_1.bin SHA256

          SHA256 hash of starcoder-ggml-q5_1.bin:

          c52e0cd23878c3373a8a7f6adb484a00dbae11b1d6bbd84aa20e82378cbb4bfa

          CertUtil: -hashfile command completed successfully.

          </code></pre>

          <p>And here is koboldcpp:</p>

          <pre><code>Welcome to KoboldCpp - Version 1.21.1

          For command line arguments, please refer to --help

          Otherwise, please manually select ggml file:

          Attempting to use OpenBLAS library for faster prompt ingestion. A compatible
          libopenblas will be required.

          Initializing dynamic library: koboldcpp_openblas.dll

          ==========

          Loading model: D:\program\koboldcpp\starcoder-ggml-q5_1.bin

          [Threads: 12, BlasThreads: 12, SmartContext: True]


          ---

          Identified as GPT-NEO-X model: (ver 401)

          Attempting to Load...

          ---

          System Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI
          = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD
          = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |

          stablelm_model_load: loading model from ''D:\program\koboldcpp\starcoder-ggml-q5_1.bin''
          - please wait ...

          stablelm_model_load: n_vocab = 49152

          stablelm_model_load: n_ctx   = 8192

          stablelm_model_load: n_embd  = 6144

          stablelm_model_load: n_head  = 48

          stablelm_model_load: n_layer = 40

          stablelm_model_load: n_rot   = 1009

          stablelm_model_load: ftype   = 49152

          GGML_ASSERT: ggml.c:3446: wtype != GGML_TYPE_COUNT

          </code></pre>

          '
        raw: "I have download the q5_1.bin and try to run in llama.cpp and koboldcpp,\
          \ but it does not work.\r\nI have checked the file SHA256 and it is the\
          \ same.\r\nHere is the llama.cpp's error code:\r\n```\r\nmain -m ./models/starcoder-ggml-q5_1.bin\
          \ -t 12 -n -1 -c 2048 --keep -1 --repeat_last_n 2048 --top_k 160 --top_p\
          \ 0.95 --color -ins -r \"User:\" --keep -1 --interactive-first\r\nmain:\
          \ build = 536 (cdd5350)\r\nmain: seed  = 1684312164\r\nllama.cpp: loading\
          \ model from ./models/starcoder-ggml-q5_1.bin\r\nerror loading model: missing\
          \ tok_embeddings.weight\r\nllama_init_from_file: failed to load model\r\n\
          llama_init_from_gpt_params: error: failed to load model './models/starcoder-ggml-q5_1.bin'\r\
          \nmain: error: unable to load model\r\n\r\ncertutil -hashfile starcoder-ggml-q5_1.bin\
          \ SHA256\r\nSHA256 hash of starcoder-ggml-q5_1.bin:\r\nc52e0cd23878c3373a8a7f6adb484a00dbae11b1d6bbd84aa20e82378cbb4bfa\r\
          \nCertUtil: -hashfile command completed successfully.\r\n```\r\nAnd here\
          \ is koboldcpp:\r\n```\r\nWelcome to KoboldCpp - Version 1.21.1\r\nFor command\
          \ line arguments, please refer to --help\r\nOtherwise, please manually select\
          \ ggml file:\r\nAttempting to use OpenBLAS library for faster prompt ingestion.\
          \ A compatible libopenblas will be required.\r\nInitializing dynamic library:\
          \ koboldcpp_openblas.dll\r\n==========\r\nLoading model: D:\\program\\koboldcpp\\\
          starcoder-ggml-q5_1.bin\r\n[Threads: 12, BlasThreads: 12, SmartContext:\
          \ True]\r\n\r\n---\r\nIdentified as GPT-NEO-X model: (ver 401)\r\nAttempting\
          \ to Load...\r\n---\r\nSystem Info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI\
          \ = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 |\
          \ FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\r\nstablelm_model_load:\
          \ loading model from 'D:\\program\\koboldcpp\\starcoder-ggml-q5_1.bin' -\
          \ please wait ...\r\nstablelm_model_load: n_vocab = 49152\r\nstablelm_model_load:\
          \ n_ctx   = 8192\r\nstablelm_model_load: n_embd  = 6144\r\nstablelm_model_load:\
          \ n_head  = 48\r\nstablelm_model_load: n_layer = 40\r\nstablelm_model_load:\
          \ n_rot   = 1009\r\nstablelm_model_load: ftype   = 49152\r\nGGML_ASSERT:\
          \ ggml.c:3446: wtype != GGML_TYPE_COUNT\r\n```\r\n"
        updatedAt: '2023-05-17T08:36:59.308Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - JeddyH
        - AIMLNewbie
        - xhyi
    id: 6464922b07a7f745c59b73f0
    type: comment
  author: FenixInDarkSolo
  content: "I have download the q5_1.bin and try to run in llama.cpp and koboldcpp,\
    \ but it does not work.\r\nI have checked the file SHA256 and it is the same.\r\
    \nHere is the llama.cpp's error code:\r\n```\r\nmain -m ./models/starcoder-ggml-q5_1.bin\
    \ -t 12 -n -1 -c 2048 --keep -1 --repeat_last_n 2048 --top_k 160 --top_p 0.95\
    \ --color -ins -r \"User:\" --keep -1 --interactive-first\r\nmain: build = 536\
    \ (cdd5350)\r\nmain: seed  = 1684312164\r\nllama.cpp: loading model from ./models/starcoder-ggml-q5_1.bin\r\
    \nerror loading model: missing tok_embeddings.weight\r\nllama_init_from_file:\
    \ failed to load model\r\nllama_init_from_gpt_params: error: failed to load model\
    \ './models/starcoder-ggml-q5_1.bin'\r\nmain: error: unable to load model\r\n\r\
    \ncertutil -hashfile starcoder-ggml-q5_1.bin SHA256\r\nSHA256 hash of starcoder-ggml-q5_1.bin:\r\
    \nc52e0cd23878c3373a8a7f6adb484a00dbae11b1d6bbd84aa20e82378cbb4bfa\r\nCertUtil:\
    \ -hashfile command completed successfully.\r\n```\r\nAnd here is koboldcpp:\r\
    \n```\r\nWelcome to KoboldCpp - Version 1.21.1\r\nFor command line arguments,\
    \ please refer to --help\r\nOtherwise, please manually select ggml file:\r\nAttempting\
    \ to use OpenBLAS library for faster prompt ingestion. A compatible libopenblas\
    \ will be required.\r\nInitializing dynamic library: koboldcpp_openblas.dll\r\n\
    ==========\r\nLoading model: D:\\program\\koboldcpp\\starcoder-ggml-q5_1.bin\r\
    \n[Threads: 12, BlasThreads: 12, SmartContext: True]\r\n\r\n---\r\nIdentified\
    \ as GPT-NEO-X model: (ver 401)\r\nAttempting to Load...\r\n---\r\nSystem Info:\
    \ AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA =\
    \ 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS =\
    \ 1 | SSE3 = 1 | VSX = 0 |\r\nstablelm_model_load: loading model from 'D:\\program\\\
    koboldcpp\\starcoder-ggml-q5_1.bin' - please wait ...\r\nstablelm_model_load:\
    \ n_vocab = 49152\r\nstablelm_model_load: n_ctx   = 8192\r\nstablelm_model_load:\
    \ n_embd  = 6144\r\nstablelm_model_load: n_head  = 48\r\nstablelm_model_load:\
    \ n_layer = 40\r\nstablelm_model_load: n_rot   = 1009\r\nstablelm_model_load:\
    \ ftype   = 49152\r\nGGML_ASSERT: ggml.c:3446: wtype != GGML_TYPE_COUNT\r\n```\r\
    \n"
  created_at: 2023-05-17 07:36:59+00:00
  edited: false
  hidden: false
  id: 6464922b07a7f745c59b73f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6fb4c41a5080aaf44d130e0de5a2df1.svg
      fullname: Neo Dim
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: NeoDim
      type: user
    createdAt: '2023-05-17T18:07:48.000Z'
    data:
      edited: false
      editors:
      - NeoDim
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6fb4c41a5080aaf44d130e0de5a2df1.svg
          fullname: Neo Dim
          isHf: false
          isPro: false
          name: NeoDim
          type: user
        html: '<p>There is an issue in llama.cpp repo - <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/1441">https://github.com/ggerganov/llama.cpp/issues/1441</a></p>

          <p>For now there is only example code here - <a rel="nofollow" href="https://github.com/ggerganov/ggml/tree/master/examples/starcoder">https://github.com/ggerganov/ggml/tree/master/examples/starcoder</a></p>

          <p>This code works, but not very useful: it loads model, generates reply
          to single prompt and shutting down. Now I keep experimenting with this code
          to get conversation loop, but have troubles with it - looks like I didn''t
          get how to correctly manage memory. It breaks after single iteration of
          loop with "not enough memory in context". Will see if I can do better.</p>

          '
        raw: 'There is an issue in llama.cpp repo - https://github.com/ggerganov/llama.cpp/issues/1441


          For now there is only example code here - https://github.com/ggerganov/ggml/tree/master/examples/starcoder


          This code works, but not very useful: it loads model, generates reply to
          single prompt and shutting down. Now I keep experimenting with this code
          to get conversation loop, but have troubles with it - looks like I didn''t
          get how to correctly manage memory. It breaks after single iteration of
          loop with "not enough memory in context". Will see if I can do better.'
        updatedAt: '2023-05-17T18:07:48.681Z'
      numEdits: 0
      reactions: []
    id: 646517f486e668ad22e557d2
    type: comment
  author: NeoDim
  content: 'There is an issue in llama.cpp repo - https://github.com/ggerganov/llama.cpp/issues/1441


    For now there is only example code here - https://github.com/ggerganov/ggml/tree/master/examples/starcoder


    This code works, but not very useful: it loads model, generates reply to single
    prompt and shutting down. Now I keep experimenting with this code to get conversation
    loop, but have troubles with it - looks like I didn''t get how to correctly manage
    memory. It breaks after single iteration of loop with "not enough memory in context".
    Will see if I can do better.'
  created_at: 2023-05-17 17:07:48+00:00
  edited: false
  hidden: false
  id: 646517f486e668ad22e557d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6fb4c41a5080aaf44d130e0de5a2df1.svg
      fullname: Neo Dim
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: NeoDim
      type: user
    createdAt: '2023-05-17T19:52:56.000Z'
    data:
      edited: false
      editors:
      - NeoDim
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6fb4c41a5080aaf44d130e0de5a2df1.svg
          fullname: Neo Dim
          isHf: false
          isPro: false
          name: NeoDim
          type: user
        html: '<p>Also relates - <a rel="nofollow" href="https://github.com/LostRuins/koboldcpp/issues/181">https://github.com/LostRuins/koboldcpp/issues/181</a></p>

          '
        raw: Also relates - https://github.com/LostRuins/koboldcpp/issues/181
        updatedAt: '2023-05-17T19:52:56.757Z'
      numEdits: 0
      reactions: []
    id: 64653098a0748f9aa4c8184b
    type: comment
  author: NeoDim
  content: Also relates - https://github.com/LostRuins/koboldcpp/issues/181
  created_at: 2023-05-17 18:52:56+00:00
  edited: false
  hidden: false
  id: 64653098a0748f9aa4c8184b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6fb4c41a5080aaf44d130e0de5a2df1.svg
      fullname: Neo Dim
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: NeoDim
      type: user
    createdAt: '2023-05-27T18:21:01.000Z'
    data:
      edited: false
      editors:
      - NeoDim
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6fb4c41a5080aaf44d130e0de5a2df1.svg
          fullname: Neo Dim
          isHf: false
          isPro: false
          name: NeoDim
          type: user
        html: '<p>For now koboldcpp supports starcoder gglm models.</p>

          '
        raw: For now koboldcpp supports starcoder gglm models.
        updatedAt: '2023-05-27T18:21:01.893Z'
      numEdits: 0
      reactions: []
    id: 64724a0d0211f85270082e72
    type: comment
  author: NeoDim
  content: For now koboldcpp supports starcoder gglm models.
  created_at: 2023-05-27 17:21:01+00:00
  edited: false
  hidden: false
  id: 64724a0d0211f85270082e72
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: NeoDim/starcoder-GGML
repo_type: model
status: open
target_branch: null
title: Cannot run on llama.cpp and koboldcpp
