!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BlueRey
conflicting_files: null
created_at: 2023-02-26 04:44:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d9dc9ae1c25abad5624e6f84df2876af.svg
      fullname: Reyhan Patria
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BlueRey
      type: user
    createdAt: '2023-02-26T04:44:57.000Z'
    data:
      edited: false
      editors:
      - BlueRey
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d9dc9ae1c25abad5624e6f84df2876af.svg
          fullname: Reyhan Patria
          isHf: false
          isPro: false
          name: BlueRey
          type: user
        html: "<p>When I initialized AutoModelForQuestionAnswering from pretrained\
          \ indobenchmark/indobert-base-p1 it won\u2019t produce any outputs and shows\
          \ ValueError, below is the code:</p>\n<pre><code>from transformers import\
          \ AutoModelForQuestionAnswering, AutoTokenizer\n\nmodel = AutoModelForQuestionAnswering.from_pretrained('indobenchmark/indobert-base-p1')\n\
          tokenizer = AutoTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n\
          \ncontext = 'Sindrom Bazex: acrokeratosis paraneoplastica.'\nquestion =\
          \ 'Nama sinonim dari Acrokeratosis paraneoplastica.'\n\ninputs = tokenizer(question,\
          \ context, return_tensors='pt')\noutputs = model(**inputs)\n\ninputs, outputs\n\
          </code></pre>\n<p>The error shown is:</p>\n<pre><code>ValueError       \
          \                         Traceback (most recent call last) /tmp/ipykernel_28/2363476019.py\
          \ in &lt;module&gt;\n      3 \n      4 inputs = tokenizer(question, context,\
          \ return_tensors='pt')\n----&gt; 5 outputs = model(**inputs)\n      6 \n\
          \      7 inputs, outputs\n\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\
          \ in\n_call_impl(self, *input, **kwargs)    1128         if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\
          \    1129              or _global_forward_hooks or _global_forward_pre_hooks):\n\
          -&gt; 1130             return forward_call(*input, **kwargs)    1131   \
          \      # Do not call functions when jit is used    1132         full_backward_hooks,\
          \ non_full_backward_hooks = [], []\n\n/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\
          \ in forward(self, input_ids, attention_mask, token_type_ids, position_ids,\
          \ head_mask, inputs_embeds, start_positions, end_positions, output_attentions,\
          \ output_hidden_states, return_dict)   1860     1861         logits = self.qa_outputs(sequence_output)\n\
          -&gt; 1862         start_logits, end_logits = logits.split(1, dim=-1)  \
          \  1863         start_logits = start_logits.squeeze(-1).contiguous()   \
          \ 1864         end_logits = end_logits.squeeze(-1).contiguous()\n\nValueError:\
          \ too many values to unpack (expected 2)\n</code></pre>\n<p>But when I initialize\
          \ the model with from a different repo i.e indolem/indobert-base-uncased,\
          \ it runs no problem. I only changed the from_pretrained value.</p>\n<p>Has\
          \ this ever happened to anyone? Is it a problem with the model I\u2019m\
          \ trying to load? Or is there a specific version of huggingface scripts\
          \ that I need to use? Any help would be appreciated, thanks!</p>\n"
        raw: "When I initialized AutoModelForQuestionAnswering from pretrained indobenchmark/indobert-base-p1\
          \ it won\u2019t produce any outputs and shows ValueError, below is the code:\r\
          \n```\r\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer\r\
          \n\r\nmodel = AutoModelForQuestionAnswering.from_pretrained('indobenchmark/indobert-base-p1')\r\
          \ntokenizer = AutoTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\r\
          \n\r\ncontext = 'Sindrom Bazex: acrokeratosis paraneoplastica.'\r\nquestion\
          \ = 'Nama sinonim dari Acrokeratosis paraneoplastica.'\r\n\r\ninputs = tokenizer(question,\
          \ context, return_tensors='pt')\r\noutputs = model(**inputs)\r\n\r\ninputs,\
          \ outputs\r\n```\r\n\r\nThe error shown is:\r\n```\r\nValueError       \
          \                         Traceback (most recent call last) /tmp/ipykernel_28/2363476019.py\
          \ in <module>\r\n      3 \r\n      4 inputs = tokenizer(question, context,\
          \ return_tensors='pt')\r\n----> 5 outputs = model(**inputs)\r\n      6 \r\
          \n      7 inputs, outputs\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\
          \ in\r\n_call_impl(self, *input, **kwargs)    1128         if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\
          \    1129              or _global_forward_hooks or _global_forward_pre_hooks):\r\
          \n-> 1130             return forward_call(*input, **kwargs)    1131    \
          \     # Do not call functions when jit is used    1132         full_backward_hooks,\
          \ non_full_backward_hooks = [], []\r\n\r\n/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\
          \ in forward(self, input_ids, attention_mask, token_type_ids, position_ids,\
          \ head_mask, inputs_embeds, start_positions, end_positions, output_attentions,\
          \ output_hidden_states, return_dict)   1860     1861         logits = self.qa_outputs(sequence_output)\r\
          \n-> 1862         start_logits, end_logits = logits.split(1, dim=-1)   \
          \ 1863         start_logits = start_logits.squeeze(-1).contiguous()    1864\
          \         end_logits = end_logits.squeeze(-1).contiguous()\r\n\r\nValueError:\
          \ too many values to unpack (expected 2)\r\n```\r\n\r\nBut when I initialize\
          \ the model with from a different repo i.e indolem/indobert-base-uncased,\
          \ it runs no problem. I only changed the from_pretrained value.\r\n\r\n\
          Has this ever happened to anyone? Is it a problem with the model I\u2019\
          m trying to load? Or is there a specific version of huggingface scripts\
          \ that I need to use? Any help would be appreciated, thanks!"
        updatedAt: '2023-02-26T04:44:57.465Z'
      numEdits: 0
      reactions: []
    id: 63fae3c94380ab0cb9599fa5
    type: comment
  author: BlueRey
  content: "When I initialized AutoModelForQuestionAnswering from pretrained indobenchmark/indobert-base-p1\
    \ it won\u2019t produce any outputs and shows ValueError, below is the code:\r\
    \n```\r\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer\r\
    \n\r\nmodel = AutoModelForQuestionAnswering.from_pretrained('indobenchmark/indobert-base-p1')\r\
    \ntokenizer = AutoTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\r\
    \n\r\ncontext = 'Sindrom Bazex: acrokeratosis paraneoplastica.'\r\nquestion =\
    \ 'Nama sinonim dari Acrokeratosis paraneoplastica.'\r\n\r\ninputs = tokenizer(question,\
    \ context, return_tensors='pt')\r\noutputs = model(**inputs)\r\n\r\ninputs, outputs\r\
    \n```\r\n\r\nThe error shown is:\r\n```\r\nValueError                        \
    \        Traceback (most recent call last) /tmp/ipykernel_28/2363476019.py in\
    \ <module>\r\n      3 \r\n      4 inputs = tokenizer(question, context, return_tensors='pt')\r\
    \n----> 5 outputs = model(**inputs)\r\n      6 \r\n      7 inputs, outputs\r\n\
    \r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in\r\n_call_impl(self,\
    \ *input, **kwargs)    1128         if not (self._backward_hooks or self._forward_hooks\
    \ or self._forward_pre_hooks or _global_backward_hooks    1129              or\
    \ _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1130            \
    \ return forward_call(*input, **kwargs)    1131         # Do not call functions\
    \ when jit is used    1132         full_backward_hooks, non_full_backward_hooks\
    \ = [], []\r\n\r\n/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\
    \ in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask,\
    \ inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states,\
    \ return_dict)   1860     1861         logits = self.qa_outputs(sequence_output)\r\
    \n-> 1862         start_logits, end_logits = logits.split(1, dim=-1)    1863 \
    \        start_logits = start_logits.squeeze(-1).contiguous()    1864        \
    \ end_logits = end_logits.squeeze(-1).contiguous()\r\n\r\nValueError: too many\
    \ values to unpack (expected 2)\r\n```\r\n\r\nBut when I initialize the model\
    \ with from a different repo i.e indolem/indobert-base-uncased, it runs no problem.\
    \ I only changed the from_pretrained value.\r\n\r\nHas this ever happened to anyone?\
    \ Is it a problem with the model I\u2019m trying to load? Or is there a specific\
    \ version of huggingface scripts that I need to use? Any help would be appreciated,\
    \ thanks!"
  created_at: 2023-02-26 04:44:57+00:00
  edited: false
  hidden: false
  id: 63fae3c94380ab0cb9599fa5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: indobenchmark/indobert-base-p1
repo_type: model
status: open
target_branch: null
title: 'AutoModelForQuestionAnswering: ValueError: too many values to unpack (expected
  2)'
