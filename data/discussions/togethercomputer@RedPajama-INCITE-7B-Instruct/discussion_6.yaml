!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Fionn
conflicting_files: null
created_at: 2023-07-04 07:38:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52e368c75a146f40ddf623d209a3f676.svg
      fullname: Fionn Delahunty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fionn
      type: user
    createdAt: '2023-07-04T08:38:46.000Z'
    data:
      edited: false
      editors:
      - Fionn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8471537828445435
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52e368c75a146f40ddf623d209a3f676.svg
          fullname: Fionn Delahunty
          isHf: false
          isPro: false
          name: Fionn
          type: user
        html: "<p>Saw some reports that this performs better than Falcon-7B. Was interested\
          \ to try that out! </p>\n<p>Unfortunately, using a handful of tests the\
          \ performance seems quite poor. For example, the prompt: <code>\"Label the\
          \ tweets as either 'positive', 'negative', 'mixed', or 'neutral': Tweet:\
          \ I can say that there isn't anything I would change. \" </code>  returns\
          \ </p>\n<pre><code>\"  Tweet: @jessicajayne haha yay for us!  Tweet: @gabrielladixon\
          \ I have no idea how you do it.  Tweet: @jessicajayne haha yay for us! \
          \ Tweet: @gabrielladixon I have no idea how you do it.  Tweet: @gabrielladixon\
          \ I have no idea how you do it.  Tweet: @gabrielladixon I have no idea how\
          \ you do it.  Tweet: @gabrielladixon I\"\n</code></pre>\n<p>Using the following\
          \ parameters </p>\n<pre><code>\"parameters\": {\n    \"max_new_tokens\"\
          : 128,\n\"temperature\": 0.7, \n\"top_p\": 0.7, \n\"top_k\":50\n  }\n</code></pre>\n\
          <p>Is this expected or unexpected? </p>\n"
        raw: "Saw some reports that this performs better than Falcon-7B. Was interested\
          \ to try that out! \r\n\r\nUnfortunately, using a handful of tests the performance\
          \ seems quite poor. For example, the prompt: `\"Label the tweets as either\
          \ 'positive', 'negative', 'mixed', or 'neutral': Tweet: I can say that there\
          \ isn't anything I would change. \" `  returns \r\n\r\n```\r\n\"  Tweet:\
          \ @jessicajayne haha yay for us!  Tweet: @gabrielladixon I have no idea\
          \ how you do it.  Tweet: @jessicajayne haha yay for us!  Tweet: @gabrielladixon\
          \ I have no idea how you do it.  Tweet: @gabrielladixon I have no idea how\
          \ you do it.  Tweet: @gabrielladixon I have no idea how you do it.  Tweet:\
          \ @gabrielladixon I\"\r\n```\r\n\r\nUsing the following parameters \r\n\
          ```\r\n\"parameters\": {\r\n    \"max_new_tokens\": 128,\r\n\"temperature\"\
          : 0.7, \r\n\"top_p\": 0.7, \r\n\"top_k\":50\r\n  }\r\n```\r\n\r\nIs this\
          \ expected or unexpected? \r\n"
        updatedAt: '2023-07-04T08:38:46.104Z'
      numEdits: 0
      reactions: []
    id: 64a3da96db05de144a9f422d
    type: comment
  author: Fionn
  content: "Saw some reports that this performs better than Falcon-7B. Was interested\
    \ to try that out! \r\n\r\nUnfortunately, using a handful of tests the performance\
    \ seems quite poor. For example, the prompt: `\"Label the tweets as either 'positive',\
    \ 'negative', 'mixed', or 'neutral': Tweet: I can say that there isn't anything\
    \ I would change. \" `  returns \r\n\r\n```\r\n\"  Tweet: @jessicajayne haha yay\
    \ for us!  Tweet: @gabrielladixon I have no idea how you do it.  Tweet: @jessicajayne\
    \ haha yay for us!  Tweet: @gabrielladixon I have no idea how you do it.  Tweet:\
    \ @gabrielladixon I have no idea how you do it.  Tweet: @gabrielladixon I have\
    \ no idea how you do it.  Tweet: @gabrielladixon I\"\r\n```\r\n\r\nUsing the following\
    \ parameters \r\n```\r\n\"parameters\": {\r\n    \"max_new_tokens\": 128,\r\n\"\
    temperature\": 0.7, \r\n\"top_p\": 0.7, \r\n\"top_k\":50\r\n  }\r\n```\r\n\r\n\
    Is this expected or unexpected? \r\n"
  created_at: 2023-07-04 07:38:46+00:00
  edited: false
  hidden: false
  id: 64a3da96db05de144a9f422d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-07-04T09:19:54.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.921831488609314
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Fionn&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Fionn\">@<span class=\"\
          underline\">Fionn</span></a></span>\n\n\t</span></span> Thank you for your\
          \ interests! In general it's not expected. I can offer some tips to help\
          \ improve the performance of the model:</p>\n<ol>\n<li>Always append \"\
          Label:\", \"Output:\", or \"Answer:\" at the end of the prompt. This helps\
          \ the model understand that it needs to provide the answer instead of completing\
          \ the input tweet.</li>\n<li>Feel free to use newlines to separate instructions,\
          \ input, and output for better organization.</li>\n</ol>\n<p>Based on these\
          \ tips, you can format your input as follows:</p>\n<pre><code>Label the\
          \ tweets as either 'positive', 'negative', 'mixed', or 'neutral'.\n\nTweet:\
          \ I can say that there isn't anything I would change.\nLabel:\n</code></pre>\n\
          <p>This formatting will prompt the model to provide a label for the given\
          \ tweet, which should be positive.</p>\n<p>Additionally, it would be very\
          \ helpful to include examples to help the model better understand what you're\
          \ looking for. For example:</p>\n<pre><code>Label the tweets as either 'positive',\
          \ 'negative', 'mixed', or 'neutral'.\n\nTweet: The weather is good.\nLabel:\
          \ positive\n\nTweet: I can say that there isn't anything I would change.\n\
          Label:\n</code></pre>\n"
        raw: '@Fionn Thank you for your interests! In general it''s not expected.
          I can offer some tips to help improve the performance of the model:


          1. Always append "Label:", "Output:", or "Answer:" at the end of the prompt.
          This helps the model understand that it needs to provide the answer instead
          of completing the input tweet.

          2. Feel free to use newlines to separate instructions, input, and output
          for better organization.


          Based on these tips, you can format your input as follows:


          ```

          Label the tweets as either ''positive'', ''negative'', ''mixed'', or ''neutral''.


          Tweet: I can say that there isn''t anything I would change.

          Label:

          ```

          This formatting will prompt the model to provide a label for the given tweet,
          which should be positive.


          Additionally, it would be very helpful to include examples to help the model
          better understand what you''re looking for. For example:

          ```

          Label the tweets as either ''positive'', ''negative'', ''mixed'', or ''neutral''.


          Tweet: The weather is good.

          Label: positive


          Tweet: I can say that there isn''t anything I would change.

          Label:

          ```

          '
        updatedAt: '2023-07-04T09:19:54.422Z'
      numEdits: 0
      reactions: []
    id: 64a3e43a838ef5bafeabdf06
    type: comment
  author: juewang
  content: '@Fionn Thank you for your interests! In general it''s not expected. I
    can offer some tips to help improve the performance of the model:


    1. Always append "Label:", "Output:", or "Answer:" at the end of the prompt. This
    helps the model understand that it needs to provide the answer instead of completing
    the input tweet.

    2. Feel free to use newlines to separate instructions, input, and output for better
    organization.


    Based on these tips, you can format your input as follows:


    ```

    Label the tweets as either ''positive'', ''negative'', ''mixed'', or ''neutral''.


    Tweet: I can say that there isn''t anything I would change.

    Label:

    ```

    This formatting will prompt the model to provide a label for the given tweet,
    which should be positive.


    Additionally, it would be very helpful to include examples to help the model better
    understand what you''re looking for. For example:

    ```

    Label the tweets as either ''positive'', ''negative'', ''mixed'', or ''neutral''.


    Tweet: The weather is good.

    Label: positive


    Tweet: I can say that there isn''t anything I would change.

    Label:

    ```

    '
  created_at: 2023-07-04 08:19:54+00:00
  edited: false
  hidden: false
  id: 64a3e43a838ef5bafeabdf06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52e368c75a146f40ddf623d209a3f676.svg
      fullname: Fionn Delahunty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fionn
      type: user
    createdAt: '2023-07-04T09:36:42.000Z'
    data:
      edited: false
      editors:
      - Fionn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9325990676879883
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52e368c75a146f40ddf623d209a3f676.svg
          fullname: Fionn Delahunty
          isHf: false
          isPro: false
          name: Fionn
          type: user
        html: "<p>Thanks for the quick and detailed response <span data-props=\"{&quot;user&quot;:&quot;juewang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/juewang\"\
          >@<span class=\"underline\">juewang</span></a></span>\n\n\t</span></span>\
          \ !</p>\n<p>I tested with your feedback, but unfortunately, it's still quite\
          \ poor. For me, inputting the second example you gave returns:</p>\n<pre><code>{'generated_text':\
          \ \"\\n    Tweet: <span data-props=\"{&quot;user&quot;:&quot;marcus&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/marcus\"\
          >@<span class=\"underline\">marcus</span></a></span>\n\n\t</span></span>\
          \ I'm glad you're feeling better!\\n    Label: positive\\n    \\n    Tweet:\
          \ @kylegriffin1  you're not going to be happy until you've turned the whole\
          \ world into a bunch of democrats.\\n    Label: negative\\n    \\n    Tweet:\
          \ @TheTweetOfGod i don't have twitter, but i did read the article and it\
          \ was very good!\\n    Label: positive\\n    \\n    Tweet: @DjThunderLips\
          \ I think you should try it. I've never been to one, but\"}\n</code></pre>\n\
          <p>For reference, I running this using the <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference\"\
          >HuggingFace text inference docker</a></p>\n"
        raw: 'Thanks for the quick and detailed response @juewang !


          I tested with your feedback, but unfortunately, it''s still quite poor.
          For me, inputting the second example you gave returns:



          ```

          {''generated_text'': "\n    Tweet: @marcus I''m glad you''re feeling better!\n    Label:
          positive\n    \n    Tweet: @kylegriffin1  you''re not going to be happy
          until you''ve turned the whole world into a bunch of democrats.\n    Label:
          negative\n    \n    Tweet: @TheTweetOfGod i don''t have twitter, but i did
          read the article and it was very good!\n    Label: positive\n    \n    Tweet:
          @DjThunderLips I think you should try it. I''ve never been to one, but"}

          ```


          For reference, I running this using the [HuggingFace text inference docker](https://github.com/huggingface/text-generation-inference)'
        updatedAt: '2023-07-04T09:36:42.078Z'
      numEdits: 0
      reactions: []
    id: 64a3e82a6866210ffc6e991b
    type: comment
  author: Fionn
  content: 'Thanks for the quick and detailed response @juewang !


    I tested with your feedback, but unfortunately, it''s still quite poor. For me,
    inputting the second example you gave returns:



    ```

    {''generated_text'': "\n    Tweet: @marcus I''m glad you''re feeling better!\n    Label:
    positive\n    \n    Tweet: @kylegriffin1  you''re not going to be happy until
    you''ve turned the whole world into a bunch of democrats.\n    Label: negative\n    \n    Tweet:
    @TheTweetOfGod i don''t have twitter, but i did read the article and it was very
    good!\n    Label: positive\n    \n    Tweet: @DjThunderLips I think you should
    try it. I''ve never been to one, but"}

    ```


    For reference, I running this using the [HuggingFace text inference docker](https://github.com/huggingface/text-generation-inference)'
  created_at: 2023-07-04 08:36:42+00:00
  edited: false
  hidden: false
  id: 64a3e82a6866210ffc6e991b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-07-04T10:28:29.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6790063977241516
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Fionn&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Fionn\">@<span class=\"\
          underline\">Fionn</span></a></span>\n\n\t</span></span><br>The code snippet\
          \ below works for me:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(<span class=\"\
          hljs-string\">'togethercomputer/RedPajama-INCITE-7B-Instruct'</span>, torch_dtype=torch.float16).to(<span\
          \ class=\"hljs-string\">'cuda:0'</span>)\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">'togethercomputer/RedPajama-INCITE-7B-Instruct'</span>)\n\
          \ninputs = tokenizer(<span class=\"hljs-string\">\"\"\"Label the tweets\
          \ as either 'positive', 'negative', 'mixed', or 'neutral'.</span>\n<span\
          \ class=\"hljs-string\"></span>\n<span class=\"hljs-string\">Tweet: The\
          \ weather is good.</span>\n<span class=\"hljs-string\">Label: positive</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">Tweet:\
          \ I can say that there isn't anything I would change.</span>\n<span class=\"\
          hljs-string\">Label:\"\"\"</span>, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).to(model.device)\n\noutput = model.generate(**inputs, max_new_tokens=<span\
          \ class=\"hljs-number\">32</span>)[<span class=\"hljs-number\">0</span>,\
          \ inputs.input_ids.size(<span class=\"hljs-number\">1</span>):]\n<span class=\"\
          hljs-built_in\">print</span>(tokenizer.decode(output))\n<span class=\"hljs-comment\"\
          ># ==&gt;</span>\n<span class=\"hljs-string\">'''</span>\n<span class=\"\
          hljs-string\"> positive</span>\n<span class=\"hljs-string\"></span>\n<span\
          \ class=\"hljs-string\">Tweet: @jennifer_truax I'm so sorry.  I hope you\
          \ feel better soon.  I'm glad you</span>\n<span class=\"hljs-string\">'''</span>\n\
          </code></pre>\n<p>Can you check that there are no extra spaces or \"\\n\"\
          \ at the end of the prompt? They are very harmful for BPE tokenizer-based\
          \ models.</p>\n"
        raw: "@Fionn \nThe code snippet below works for me:\n```python\nimport torch\n\
          from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel =\
          \ AutoModelForCausalLM.from_pretrained('togethercomputer/RedPajama-INCITE-7B-Instruct',\
          \ torch_dtype=torch.float16).to('cuda:0')\ntokenizer = AutoTokenizer.from_pretrained('togethercomputer/RedPajama-INCITE-7B-Instruct')\n\
          \ninputs = tokenizer(\"\"\"Label the tweets as either 'positive', 'negative',\
          \ 'mixed', or 'neutral'.\n\nTweet: The weather is good.\nLabel: positive\n\
          \nTweet: I can say that there isn't anything I would change.\nLabel:\"\"\
          \", return_tensors='pt').to(model.device)\n\noutput = model.generate(**inputs,\
          \ max_new_tokens=32)[0, inputs.input_ids.size(1):]\nprint(tokenizer.decode(output))\n\
          # ==>\n'''\n positive\n\nTweet: @jennifer_truax I'm so sorry.  I hope you\
          \ feel better soon.  I'm glad you\n'''\n```\n\nCan you check that there\
          \ are no extra spaces or \"\\n\" at the end of the prompt? They are very\
          \ harmful for BPE tokenizer-based models.\n"
        updatedAt: '2023-07-04T10:28:29.479Z'
      numEdits: 0
      reactions: []
    id: 64a3f44d1664581878a17b7d
    type: comment
  author: juewang
  content: "@Fionn \nThe code snippet below works for me:\n```python\nimport torch\n\
    from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained('togethercomputer/RedPajama-INCITE-7B-Instruct',\
    \ torch_dtype=torch.float16).to('cuda:0')\ntokenizer = AutoTokenizer.from_pretrained('togethercomputer/RedPajama-INCITE-7B-Instruct')\n\
    \ninputs = tokenizer(\"\"\"Label the tweets as either 'positive', 'negative',\
    \ 'mixed', or 'neutral'.\n\nTweet: The weather is good.\nLabel: positive\n\nTweet:\
    \ I can say that there isn't anything I would change.\nLabel:\"\"\", return_tensors='pt').to(model.device)\n\
    \noutput = model.generate(**inputs, max_new_tokens=32)[0, inputs.input_ids.size(1):]\n\
    print(tokenizer.decode(output))\n# ==>\n'''\n positive\n\nTweet: @jennifer_truax\
    \ I'm so sorry.  I hope you feel better soon.  I'm glad you\n'''\n```\n\nCan you\
    \ check that there are no extra spaces or \"\\n\" at the end of the prompt? They\
    \ are very harmful for BPE tokenizer-based models.\n"
  created_at: 2023-07-04 09:28:29+00:00
  edited: false
  hidden: false
  id: 64a3f44d1664581878a17b7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52e368c75a146f40ddf623d209a3f676.svg
      fullname: Fionn Delahunty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fionn
      type: user
    createdAt: '2023-07-04T13:23:15.000Z'
    data:
      edited: false
      editors:
      - Fionn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9839569926261902
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52e368c75a146f40ddf623d209a3f676.svg
          fullname: Fionn Delahunty
          isHf: false
          isPro: false
          name: Fionn
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;juewang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/juewang\"\
          >@<span class=\"underline\">juewang</span></a></span>\n\n\t</span></span>\
          \ , as I said I'm using the inference API. I haven't been able to reproduce\
          \ your results, but I'll keep trying. Thanks for the support!</p>\n"
        raw: Thanks @juewang , as I said I'm using the inference API. I haven't been
          able to reproduce your results, but I'll keep trying. Thanks for the support!
        updatedAt: '2023-07-04T13:23:15.486Z'
      numEdits: 0
      reactions: []
    id: 64a41d434a1f142c2b1328d2
    type: comment
  author: Fionn
  content: Thanks @juewang , as I said I'm using the inference API. I haven't been
    able to reproduce your results, but I'll keep trying. Thanks for the support!
  created_at: 2023-07-04 12:23:15+00:00
  edited: false
  hidden: false
  id: 64a41d434a1f142c2b1328d2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: togethercomputer/RedPajama-INCITE-7B-Instruct
repo_type: model
status: open
target_branch: null
title: 'Poor performance? '
