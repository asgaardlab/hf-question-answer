!!python/object:huggingface_hub.community.DiscussionWithDetails
author: markding
conflicting_files: null
created_at: 2023-07-27 12:12:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d1218684bfbee86b6ee521/BpXX_XUP80IfdGAvbs_VI.png?w=200&h=200&f=face
      fullname: MD
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: markding
      type: user
    createdAt: '2023-07-27T13:12:43.000Z'
    data:
      edited: false
      editors:
      - markding
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9706372022628784
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d1218684bfbee86b6ee521/BpXX_XUP80IfdGAvbs_VI.png?w=200&h=200&f=face
          fullname: MD
          isHf: false
          isPro: false
          name: markding
          type: user
        html: '<p>The model card doesn''t seem to offer details or info on how the
          Instruct and Chat versions were RLHF''d/instruction-tuned. This is what
          the release blog post says:</p>

          <blockquote>

          <p>RedPajama-INCITE-Chat-7B-v0.1 is its chat counterpart trained over Dolly
          2.0 and Open Assistant</p>

          </blockquote>

          <blockquote>

          <p>RedPajama-INCITE-Instruct-7B-v0.1 is instruction tuned for few-shot applications.
          We follow the recipe for GPT-JT but eliminate all datasets that overlap
          with the HELM benchmark. </p>

          </blockquote>

          <p>It would be useful to specify exactly which datasets were included /
          excluded, to spare interested users the trouble of figure out what the HELM
          benchmark includes and how it does or does not overlap with GPT-JT.</p>

          '
        raw: "The model card doesn't seem to offer details or info on how the Instruct\
          \ and Chat versions were RLHF'd/instruction-tuned. This is what the release\
          \ blog post says:\r\n\r\n>RedPajama-INCITE-Chat-7B-v0.1 is its chat counterpart\
          \ trained over Dolly 2.0 and Open Assistant\r\n\r\n>RedPajama-INCITE-Instruct-7B-v0.1\
          \ is instruction tuned for few-shot applications. We follow the recipe for\
          \ GPT-JT but eliminate all datasets that overlap with the HELM benchmark.\
          \ \r\n\r\nIt would be useful to specify exactly which datasets were included\
          \ / excluded, to spare interested users the trouble of figure out what the\
          \ HELM benchmark includes and how it does or does not overlap with GPT-JT."
        updatedAt: '2023-07-27T13:12:43.121Z'
      numEdits: 0
      reactions: []
    id: 64c26d4b6aa3a75f6319a4d7
    type: comment
  author: markding
  content: "The model card doesn't seem to offer details or info on how the Instruct\
    \ and Chat versions were RLHF'd/instruction-tuned. This is what the release blog\
    \ post says:\r\n\r\n>RedPajama-INCITE-Chat-7B-v0.1 is its chat counterpart trained\
    \ over Dolly 2.0 and Open Assistant\r\n\r\n>RedPajama-INCITE-Instruct-7B-v0.1\
    \ is instruction tuned for few-shot applications. We follow the recipe for GPT-JT\
    \ but eliminate all datasets that overlap with the HELM benchmark. \r\n\r\nIt\
    \ would be useful to specify exactly which datasets were included / excluded,\
    \ to spare interested users the trouble of figure out what the HELM benchmark\
    \ includes and how it does or does not overlap with GPT-JT."
  created_at: 2023-07-27 12:12:43+00:00
  edited: false
  hidden: false
  id: 64c26d4b6aa3a75f6319a4d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-07-31T14:19:54.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8410988450050354
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;markding&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/markding\"\
          >@<span class=\"underline\">markding</span></a></span>\n\n\t</span></span>\
          \ , thanks for your question!</p>\n<p>The instruction-tuned model was trained\
          \ on instructions from the <a href=\"https://huggingface.co/datasets/bigscience/P3\"\
          >P3</a> and <a rel=\"nofollow\" href=\"https://github.com/allenai/natural-instructions\"\
          >Natural Instructions</a> datasets, which were decontaminated against HELM\
          \ (you can find more details about the decontamination strategy in this\
          \ <a rel=\"nofollow\" href=\"https://together.ai/blog/redpajama-7b\">blog\
          \ bost</a>).</p>\n<p>You can find the resulting decontaminated dataset used\
          \ to train the instruction tuned model here: <a href=\"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct\"\
          >https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct</a>\
          \ -- the metadata here contains a <code>source</code> field pointing to\
          \ the task / dataset where the instance comes from.</p>\n"
        raw: 'Hi @markding , thanks for your question!


          The instruction-tuned model was trained on instructions from the [P3](https://huggingface.co/datasets/bigscience/P3)
          and [Natural Instructions](https://github.com/allenai/natural-instructions)
          datasets, which were decontaminated against HELM (you can find more details
          about the decontamination strategy in this [blog bost](https://together.ai/blog/redpajama-7b)).


          You can find the resulting decontaminated dataset used to train the instruction
          tuned model here: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct
          -- the metadata here contains a `source` field pointing to the task / dataset
          where the instance comes from.'
        updatedAt: '2023-07-31T14:19:54.881Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - markding
    id: 64c7c30a527d763655434618
    type: comment
  author: mauriceweber
  content: 'Hi @markding , thanks for your question!


    The instruction-tuned model was trained on instructions from the [P3](https://huggingface.co/datasets/bigscience/P3)
    and [Natural Instructions](https://github.com/allenai/natural-instructions) datasets,
    which were decontaminated against HELM (you can find more details about the decontamination
    strategy in this [blog bost](https://together.ai/blog/redpajama-7b)).


    You can find the resulting decontaminated dataset used to train the instruction
    tuned model here: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct
    -- the metadata here contains a `source` field pointing to the task / dataset
    where the instance comes from.'
  created_at: 2023-07-31 13:19:54+00:00
  edited: false
  hidden: false
  id: 64c7c30a527d763655434618
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d1218684bfbee86b6ee521/BpXX_XUP80IfdGAvbs_VI.png?w=200&h=200&f=face
      fullname: MD
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: markding
      type: user
    createdAt: '2023-10-08T18:55:30.000Z'
    data:
      edited: false
      editors:
      - markding
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.893356204032898
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d1218684bfbee86b6ee521/BpXX_XUP80IfdGAvbs_VI.png?w=200&h=200&f=face
          fullname: MD
          isHf: false
          isPro: false
          name: markding
          type: user
        html: '<p>Excellent! It probably won''t surprise you given the evident care
          taken in releasing and documenting this, but we find that RedPajama-INCITE
          makes it into the top 5 of our openness leaderboard <a rel="nofollow" href="https://opening-up-chatgpt.github.io/">https://opening-up-chatgpt.github.io/</a></p>

          '
        raw: 'Excellent! It probably won''t surprise you given the evident care taken
          in releasing and documenting this, but we find that RedPajama-INCITE makes
          it into the top 5 of our openness leaderboard https://opening-up-chatgpt.github.io/


          '
        updatedAt: '2023-10-08T18:55:30.888Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - mauriceweber
    id: 6522fb22b0e0d5745317d660
    type: comment
  author: markding
  content: 'Excellent! It probably won''t surprise you given the evident care taken
    in releasing and documenting this, but we find that RedPajama-INCITE makes it
    into the top 5 of our openness leaderboard https://opening-up-chatgpt.github.io/


    '
  created_at: 2023-10-08 17:55:30+00:00
  edited: false
  hidden: false
  id: 6522fb22b0e0d5745317d660
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/616745f893f6837511d9ae25/H0JIGDL5VOu29zVd4KiKB.jpeg?w=200&h=200&f=face
      fullname: Kaiser Sun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KaiserWhoLearns
      type: user
    createdAt: '2023-10-17T18:15:17.000Z'
    data:
      edited: false
      editors:
      - KaiserWhoLearns
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8138291835784912
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/616745f893f6837511d9ae25/H0JIGDL5VOu29zVd4KiKB.jpeg?w=200&h=200&f=face
          fullname: Kaiser Sun
          isHf: false
          isPro: false
          name: KaiserWhoLearns
          type: user
        html: "<p>Thanks for the information <span data-props=\"{&quot;user&quot;:&quot;mauriceweber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mauriceweber\"\
          >@<span class=\"underline\">mauriceweber</span></a></span>\n\n\t</span></span>\
          \ ! I just want to confirm that, <a rel=\"nofollow\" href=\"https://blog.research.google/2022/05/language-models-perform-reasoning-via.html\"\
          >Chain of Thought</a> and the <a href=\"https://huggingface.co/datasets/EleutherAI/pile\"\
          >Pile</a> are not used in the fine-tuning of INCITE-7B-Instruct?<br>Asking\
          \ this because <code>We follow the recipe for GPT-JT</code> and the GPT-JT\
          \ doc says that <code>Specifically, we first conduct training for 2.62 billion\
          \ tokens using the UL2 loss on the Pile, followed by 0.92 billion tokens\
          \ with a mixture of the above datasets: 5% of COT, 20% of P3, 20% of NI,\
          \ and 55% of the Pile.</code>, while I have only seen P3 and Natural Questions\
          \ in <a href=\"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct\"\
          >https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct</a></p>\n"
        raw: "Thanks for the information @mauriceweber ! I just want to confirm that,\
          \ [Chain of Thought](https://blog.research.google/2022/05/language-models-perform-reasoning-via.html)\
          \ and the [Pile](https://huggingface.co/datasets/EleutherAI/pile) are not\
          \ used in the fine-tuning of INCITE-7B-Instruct? \nAsking this because `We\
          \ follow the recipe for GPT-JT` and the GPT-JT doc says that `Specifically,\
          \ we first conduct training for 2.62 billion tokens using the UL2 loss on\
          \ the Pile, followed by 0.92 billion tokens with a mixture of the above\
          \ datasets: 5% of COT, 20% of P3, 20% of NI, and 55% of the Pile.`, while\
          \ I have only seen P3 and Natural Questions in https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct\n"
        updatedAt: '2023-10-17T18:15:17.984Z'
      numEdits: 0
      reactions: []
    id: 652ecf351c88b9e0fd1232b5
    type: comment
  author: KaiserWhoLearns
  content: "Thanks for the information @mauriceweber ! I just want to confirm that,\
    \ [Chain of Thought](https://blog.research.google/2022/05/language-models-perform-reasoning-via.html)\
    \ and the [Pile](https://huggingface.co/datasets/EleutherAI/pile) are not used\
    \ in the fine-tuning of INCITE-7B-Instruct? \nAsking this because `We follow the\
    \ recipe for GPT-JT` and the GPT-JT doc says that `Specifically, we first conduct\
    \ training for 2.62 billion tokens using the UL2 loss on the Pile, followed by\
    \ 0.92 billion tokens with a mixture of the above datasets: 5% of COT, 20% of\
    \ P3, 20% of NI, and 55% of the Pile.`, while I have only seen P3 and Natural\
    \ Questions in https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct\n"
  created_at: 2023-10-17 17:15:17+00:00
  edited: false
  hidden: false
  id: 652ecf351c88b9e0fd1232b5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: togethercomputer/RedPajama-INCITE-7B-Instruct
repo_type: model
status: open
target_branch: null
title: Specify RLHF data for the Instruct and Chat versions in model card
