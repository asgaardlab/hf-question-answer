!!python/object:huggingface_hub.community.DiscussionWithDetails
author: spanielrassler
conflicting_files: null
created_at: 2023-04-15 19:33:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0d146e4b922a325b69d3d509965ed60.svg
      fullname: Frankie G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spanielrassler
      type: user
    createdAt: '2023-04-15T20:33:01.000Z'
    data:
      edited: false
      editors:
      - spanielrassler
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0d146e4b922a325b69d3d509965ed60.svg
          fullname: Frankie G
          isHf: false
          isPro: false
          name: spanielrassler
          type: user
        html: '<p>Hi,</p>

          <p>I''m wondering if I''m on the wrong track by trying to use this with
          llama.cpp. I get the following error when trying:<br>error loading model:
          unexpectedly reached end of file<br>llama_init_from_file: failed to load
          model<br>main: error: failed to load model</p>

          <p>Any suggestions or is this model just not compatible? If not, do you
          know of a Pyg one that is? Thanks!</p>

          '
        raw: "Hi,\r\n\r\nI'm wondering if I'm on the wrong track by trying to use\
          \ this with llama.cpp. I get the following error when trying:\r\nerror loading\
          \ model: unexpectedly reached end of file\r\nllama_init_from_file: failed\
          \ to load model\r\nmain: error: failed to load model\r\n\r\nAny suggestions\
          \ or is this model just not compatible? If not, do you know of a Pyg one\
          \ that is? Thanks!"
        updatedAt: '2023-04-15T20:33:01.814Z'
      numEdits: 0
      reactions: []
    id: 643b09fd0e5495afdefc3594
    type: comment
  author: spanielrassler
  content: "Hi,\r\n\r\nI'm wondering if I'm on the wrong track by trying to use this\
    \ with llama.cpp. I get the following error when trying:\r\nerror loading model:\
    \ unexpectedly reached end of file\r\nllama_init_from_file: failed to load model\r\
    \nmain: error: failed to load model\r\n\r\nAny suggestions or is this model just\
    \ not compatible? If not, do you know of a Pyg one that is? Thanks!"
  created_at: 2023-04-15 19:33:01+00:00
  edited: false
  hidden: false
  id: 643b09fd0e5495afdefc3594
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a0d146e4b922a325b69d3d509965ed60.svg
      fullname: Frankie G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spanielrassler
      type: user
    createdAt: '2023-04-15T22:08:06.000Z'
    data:
      status: closed
    id: 643b204618e9973bc82278d7
    type: status-change
  author: spanielrassler
  created_at: 2023-04-15 21:08:06+00:00
  id: 643b204618e9973bc82278d7
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
      fullname: Alpin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: alpindale
      type: user
    createdAt: '2023-04-16T13:07:37.000Z'
    data:
      edited: false
      editors:
      - alpindale
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
          fullname: Alpin
          isHf: false
          isPro: false
          name: alpindale
          type: user
        html: '<p>Since you''ve closed the issue, I imagine you''ve figured it out,
          but anyway.</p>

          <p>No, you can only use this with my fork of ggml <a rel="nofollow" href="https://github.com/AlpinDale/pygmalion.cpp">here</a>
          or (preferably, as it''s more advanced) with <a rel="nofollow" href="https://github.com/LostRuins/koboldcpp">koboldcpp</a>.</p>

          '
        raw: 'Since you''ve closed the issue, I imagine you''ve figured it out, but
          anyway.


          No, you can only use this with my fork of ggml [here](https://github.com/AlpinDale/pygmalion.cpp)
          or (preferably, as it''s more advanced) with [koboldcpp](https://github.com/LostRuins/koboldcpp).'
        updatedAt: '2023-04-16T13:07:37.784Z'
      numEdits: 0
      reactions: []
      relatedEventId: 643bf31921686867003e92bf
    id: 643bf31921686867003e92be
    type: comment
  author: alpindale
  content: 'Since you''ve closed the issue, I imagine you''ve figured it out, but
    anyway.


    No, you can only use this with my fork of ggml [here](https://github.com/AlpinDale/pygmalion.cpp)
    or (preferably, as it''s more advanced) with [koboldcpp](https://github.com/LostRuins/koboldcpp).'
  created_at: 2023-04-16 12:07:37+00:00
  edited: false
  hidden: false
  id: 643bf31921686867003e92be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg?w=200&h=200&f=face
      fullname: Alpin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: alpindale
      type: user
    createdAt: '2023-04-16T13:07:37.000Z'
    data:
      status: open
    id: 643bf31921686867003e92bf
    type: status-change
  author: alpindale
  created_at: 2023-04-16 12:07:37+00:00
  id: 643bf31921686867003e92bf
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6425b391a5ec4a5cbc4f45e3/QwZPh9vJiFgmwzsAeHy4Z.jpeg?w=200&h=200&f=face
      fullname: Austin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aberrio
      type: user
    createdAt: '2023-04-30T23:31:50.000Z'
    data:
      edited: true
      editors:
      - aberrio
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6425b391a5ec4a5cbc4f45e3/QwZPh9vJiFgmwzsAeHy4Z.jpeg?w=200&h=200&f=face
          fullname: Austin
          isHf: false
          isPro: false
          name: aberrio
          type: user
        html: '<p>I was wondering why do the weights depend on koboldcpp? How does
          the conversion differ?</p>

          '
        raw: I was wondering why do the weights depend on koboldcpp? How does the
          conversion differ?
        updatedAt: '2023-04-30T23:32:58.832Z'
      numEdits: 1
      reactions: []
    id: 644efa66bf9683cba46b7d12
    type: comment
  author: aberrio
  content: I was wondering why do the weights depend on koboldcpp? How does the conversion
    differ?
  created_at: 2023-04-30 22:31:50+00:00
  edited: true
  hidden: false
  id: 644efa66bf9683cba46b7d12
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: alpindale/pygmalion-6b-ggml
repo_type: model
status: open
target_branch: null
title: Is this supposed to be usable with llama.cpp?
