!!python/object:huggingface_hub.community.DiscussionWithDetails
author: weissenbacherpwc
conflicting_files: null
created_at: 2023-11-07 15:04:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b97965055b57d2809870559a838fed8.svg
      fullname: Max Weissenbacher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: weissenbacherpwc
      type: user
    createdAt: '2023-11-07T15:04:26.000Z'
    data:
      edited: false
      editors:
      - weissenbacherpwc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8648139834403992
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b97965055b57d2809870559a838fed8.svg
          fullname: Max Weissenbacher
          isHf: false
          isPro: false
          name: weissenbacherpwc
          type: user
        html: '<p>Hi,</p>

          <p>I am wondering what the maximum context size of  the model is? Is is
          2048?</p>

          '
        raw: "Hi,\r\n\r\nI am wondering what the maximum context size of  the model\
          \ is? Is is 2048?"
        updatedAt: '2023-11-07T15:04:26.869Z'
      numEdits: 0
      reactions: []
    id: 654a51fa32d67f12f86ae9b5
    type: comment
  author: weissenbacherpwc
  content: "Hi,\r\n\r\nI am wondering what the maximum context size of  the model\
    \ is? Is is 2048?"
  created_at: 2023-11-07 15:04:26+00:00
  edited: false
  hidden: false
  id: 654a51fa32d67f12f86ae9b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
      fullname: Jan Philipp Harries
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jphme
      type: user
    createdAt: '2023-11-09T11:54:32.000Z'
    data:
      edited: false
      editors:
      - jphme
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6961384415626526
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
          fullname: Jan Philipp Harries
          isHf: false
          isPro: false
          name: jphme
          type: user
        html: "<p>It\xB4s trained at a max 4096 context window (next version will\
          \ probably be 8k+) and the max context size of the architecture is 32k token,\
          \ see details here: <a href=\"https://huggingface.co/jphme/em_german_mistral_v01/blob/main/config.json\"\
          >https://huggingface.co/jphme/em_german_mistral_v01/blob/main/config.json</a>\
          \ .  You should be able to scale the context window up to at least 8k using\
          \ rope scaling (see here: <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/llama#transformers.LlamaConfig.rope_scaling\"\
          >https://huggingface.co/docs/transformers/main/en/model_doc/llama#transformers.LlamaConfig.rope_scaling</a>\
          \ ).</p>\n"
        raw: "It\xB4s trained at a max 4096 context window (next version will probably\
          \ be 8k+) and the max context size of the architecture is 32k token, see\
          \ details here: https://huggingface.co/jphme/em_german_mistral_v01/blob/main/config.json\
          \ .  You should be able to scale the context window up to at least 8k using\
          \ rope scaling (see here: https://huggingface.co/docs/transformers/main/en/model_doc/llama#transformers.LlamaConfig.rope_scaling\
          \ )."
        updatedAt: '2023-11-09T11:54:32.208Z'
      numEdits: 0
      reactions: []
      relatedEventId: 654cc878732b9e3cf4d5fa51
    id: 654cc878732b9e3cf4d5fa4a
    type: comment
  author: jphme
  content: "It\xB4s trained at a max 4096 context window (next version will probably\
    \ be 8k+) and the max context size of the architecture is 32k token, see details\
    \ here: https://huggingface.co/jphme/em_german_mistral_v01/blob/main/config.json\
    \ .  You should be able to scale the context window up to at least 8k using rope\
    \ scaling (see here: https://huggingface.co/docs/transformers/main/en/model_doc/llama#transformers.LlamaConfig.rope_scaling\
    \ )."
  created_at: 2023-11-09 11:54:32+00:00
  edited: false
  hidden: false
  id: 654cc878732b9e3cf4d5fa4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1gCpz_Og6-LyCBXDmuOd1.jpeg?w=200&h=200&f=face
      fullname: Jan Philipp Harries
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: jphme
      type: user
    createdAt: '2023-11-09T11:54:32.000Z'
    data:
      status: closed
    id: 654cc878732b9e3cf4d5fa51
    type: status-change
  author: jphme
  created_at: 2023-11-09 11:54:32+00:00
  id: 654cc878732b9e3cf4d5fa51
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: jphme/em_german_mistral_v01
repo_type: model
status: closed
target_branch: null
title: Context Size of EM German Mistral
