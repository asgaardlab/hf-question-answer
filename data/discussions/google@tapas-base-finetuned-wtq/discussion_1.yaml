!!python/object:huggingface_hub.community.DiscussionWithDetails
author: moustafa123
conflicting_files: null
created_at: 2023-03-03 20:58:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c91eacbd674f0f74bf82e20662fe9d2f.svg
      fullname: Moustafa Banbouk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: moustafa123
      type: user
    createdAt: '2023-03-03T20:58:22.000Z'
    data:
      edited: false
      editors:
      - moustafa123
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c91eacbd674f0f74bf82e20662fe9d2f.svg
          fullname: Moustafa Banbouk
          isHf: false
          isPro: false
          name: moustafa123
          type: user
        html: '<p>Is it possible to use the model to answer questions on a huge table
          dataset (ex. having 1 million records)?</p>

          '
        raw: Is it possible to use the model to answer questions on a huge table dataset
          (ex. having 1 million records)?
        updatedAt: '2023-03-03T20:58:22.845Z'
      numEdits: 0
      reactions: []
    id: 64025f6ecad36eb2b0fb763c
    type: comment
  author: moustafa123
  content: Is it possible to use the model to answer questions on a huge table dataset
    (ex. having 1 million records)?
  created_at: 2023-03-03 20:58:22+00:00
  edited: false
  hidden: false
  id: 64025f6ecad36eb2b0fb763c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0f683ee16374ed5a73fdd38ae55c0310.svg
      fullname: Pranav Raikote
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pranavraikote
      type: user
    createdAt: '2023-03-16T07:51:30.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/0f683ee16374ed5a73fdd38ae55c0310.svg
          fullname: Pranav Raikote
          isHf: false
          isPro: false
          name: pranavraikote
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-03-16T07:51:46.938Z'
      numEdits: 0
      reactions: []
    id: 6412ca826b67359f43c8c918
    type: comment
  author: pranavraikote
  content: This comment has been hidden
  created_at: 2023-03-16 06:51:30+00:00
  edited: true
  hidden: true
  id: 6412ca826b67359f43c8c918
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0f683ee16374ed5a73fdd38ae55c0310.svg
      fullname: Pranav Raikote
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pranavraikote
      type: user
    createdAt: '2023-03-16T07:51:51.000Z'
    data:
      edited: false
      editors:
      - pranavraikote
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0f683ee16374ed5a73fdd38ae55c0310.svg
          fullname: Pranav Raikote
          isHf: false
          isPro: false
          name: pranavraikote
          type: user
        html: '<blockquote>

          <p>Is it possible to use the model to answer questions on a huge table dataset
          (ex. having 1 million records)?</p>

          </blockquote>

          <p>Hi, were you able to find a workaround this problem? Even I''m facing
          similar issue..</p>

          '
        raw: '> Is it possible to use the model to answer questions on a huge table
          dataset (ex. having 1 million records)?


          Hi, were you able to find a workaround this problem? Even I''m facing similar
          issue..'
        updatedAt: '2023-03-16T07:51:51.836Z'
      numEdits: 0
      reactions: []
    id: 6412ca97ddafa13852a9bb9d
    type: comment
  author: pranavraikote
  content: '> Is it possible to use the model to answer questions on a huge table
    dataset (ex. having 1 million records)?


    Hi, were you able to find a workaround this problem? Even I''m facing similar
    issue..'
  created_at: 2023-03-16 06:51:51+00:00
  edited: false
  hidden: false
  id: 6412ca97ddafa13852a9bb9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ddac1b8c234c237d97162b4aadb1258.svg
      fullname: Jose Mario Pareja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jmparejaz
      type: user
    createdAt: '2023-03-28T21:50:24.000Z'
    data:
      edited: false
      editors:
      - jmparejaz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ddac1b8c234c237d97162b4aadb1258.svg
          fullname: Jose Mario Pareja
          isHf: false
          isPro: false
          name: jmparejaz
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Is it possible to use the model to answer questions on a huge table dataset
          (ex. having 1 million records)?</p>

          </blockquote>

          <p>Hi, were you able to find a workaround this problem? Even I''m facing
          similar issue..</p>

          </blockquote>

          <p>Hi, I am facing the same issue, I want to query large tables too. Any
          update or advice would be awesome</p>

          '
        raw: "> > Is it possible to use the model to answer questions on a huge table\
          \ dataset (ex. having 1 million records)?\n> \n> Hi, were you able to find\
          \ a workaround this problem? Even I'm facing similar issue..\n\nHi, I am\
          \ facing the same issue, I want to query large tables too. Any update or\
          \ advice would be awesome"
        updatedAt: '2023-03-28T21:50:24.080Z'
      numEdits: 0
      reactions: []
    id: 642361205aed4fac78a11a01
    type: comment
  author: jmparejaz
  content: "> > Is it possible to use the model to answer questions on a huge table\
    \ dataset (ex. having 1 million records)?\n> \n> Hi, were you able to find a workaround\
    \ this problem? Even I'm facing similar issue..\n\nHi, I am facing the same issue,\
    \ I want to query large tables too. Any update or advice would be awesome"
  created_at: 2023-03-28 20:50:24+00:00
  edited: false
  hidden: false
  id: 642361205aed4fac78a11a01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
      fullname: Yusuf Kemal Demir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phdykd
      type: user
    createdAt: '2023-05-09T03:11:46.000Z'
    data:
      edited: false
      editors:
      - phdykd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
          fullname: Yusuf Kemal Demir
          isHf: false
          isPro: false
          name: phdykd
          type: user
        html: '<p>Hi,</p>

          <p>I have 4000 rows and 15 columns, and it is unable to read it. It gives,
          "too many rows" error. By any chance, have you find solution?<br>Thanks</p>

          '
        raw: 'Hi,


          I have 4000 rows and 15 columns, and it is unable to read it. It gives,
          "too many rows" error. By any chance, have you find solution?

          Thanks'
        updatedAt: '2023-05-09T03:11:46.907Z'
      numEdits: 0
      reactions: []
    id: 6459b9f239e6aea69cca9cbd
    type: comment
  author: phdykd
  content: 'Hi,


    I have 4000 rows and 15 columns, and it is unable to read it. It gives, "too many
    rows" error. By any chance, have you find solution?

    Thanks'
  created_at: 2023-05-09 02:11:46+00:00
  edited: false
  hidden: false
  id: 6459b9f239e6aea69cca9cbd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
      fullname: Yusuf Kemal Demir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phdykd
      type: user
    createdAt: '2023-05-09T03:16:24.000Z'
    data:
      edited: false
      editors:
      - phdykd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
          fullname: Yusuf Kemal Demir
          isHf: false
          isPro: false
          name: phdykd
          type: user
        html: '<p>ValueError: Too many rows is the error it gives</p>

          '
        raw: 'ValueError: Too many rows is the error it gives'
        updatedAt: '2023-05-09T03:16:24.505Z'
      numEdits: 0
      reactions: []
    id: 6459bb08232e5f0712c03edc
    type: comment
  author: phdykd
  content: 'ValueError: Too many rows is the error it gives'
  created_at: 2023-05-09 02:16:24+00:00
  edited: false
  hidden: false
  id: 6459bb08232e5f0712c03edc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf57e3c6d0a9fa281c4a982b0c93558a.svg
      fullname: Kamalraj M M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kamaljp
      type: user
    createdAt: '2023-05-09T04:47:08.000Z'
    data:
      edited: false
      editors:
      - Kamaljp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf57e3c6d0a9fa281c4a982b0c93558a.svg
          fullname: Kamalraj M M
          isHf: false
          isPro: false
          name: Kamaljp
          type: user
        html: '<p>The model is designed for 1024 tokens. Even if you feed more than
          that, model wont be able to process. I think the workaround will be to use
          some kind of vector store/ similarity search retriever which can feed max
          20 rows to the model. (Implementation will require vector store clients)</p>

          '
        raw: The model is designed for 1024 tokens. Even if you feed more than that,
          model wont be able to process. I think the workaround will be to use some
          kind of vector store/ similarity search retriever which can feed max 20
          rows to the model. (Implementation will require vector store clients)
        updatedAt: '2023-05-09T04:47:08.432Z'
      numEdits: 0
      reactions: []
    id: 6459d04c39e6aea69ccc2f64
    type: comment
  author: Kamaljp
  content: The model is designed for 1024 tokens. Even if you feed more than that,
    model wont be able to process. I think the workaround will be to use some kind
    of vector store/ similarity search retriever which can feed max 20 rows to the
    model. (Implementation will require vector store clients)
  created_at: 2023-05-09 03:47:08+00:00
  edited: false
  hidden: false
  id: 6459d04c39e6aea69ccc2f64
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: google/tapas-base-finetuned-wtq
repo_type: model
status: open
target_branch: null
title: Huge Table Q/A
