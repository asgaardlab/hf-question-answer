!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Fever8654
conflicting_files: null
created_at: 2023-05-18 06:58:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad8dc340e3050458213baac0a110976a.svg
      fullname: Anon Nona
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fever8654
      type: user
    createdAt: '2023-05-18T07:58:39.000Z'
    data:
      edited: false
      editors:
      - Fever8654
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad8dc340e3050458213baac0a110976a.svg
          fullname: Anon Nona
          isHf: false
          isPro: false
          name: Fever8654
          type: user
        html: '<p>I''m trying to load this model, but I get an ''Cannot Allocate Memory''
          error. ButI have 24GB of VRAM and 16GB of RAM (out of which about 13GB is
          available). Is this amount of RAM not sufficient for running this model?
          The model itself is only 8gb so I didn''t expect this to be an issue. Should
          I increase my RAM/VRAM?</p>

          '
        raw: I'm trying to load this model, but I get an 'Cannot Allocate Memory'
          error. ButI have 24GB of VRAM and 16GB of RAM (out of which about 13GB is
          available). Is this amount of RAM not sufficient for running this model?
          The model itself is only 8gb so I didn't expect this to be an issue. Should
          I increase my RAM/VRAM?
        updatedAt: '2023-05-18T07:58:39.881Z'
      numEdits: 0
      reactions: []
    id: 6465daafcf550af36eb198a2
    type: comment
  author: Fever8654
  content: I'm trying to load this model, but I get an 'Cannot Allocate Memory' error.
    ButI have 24GB of VRAM and 16GB of RAM (out of which about 13GB is available).
    Is this amount of RAM not sufficient for running this model? The model itself
    is only 8gb so I didn't expect this to be an issue. Should I increase my RAM/VRAM?
  created_at: 2023-05-18 06:58:39+00:00
  edited: false
  hidden: false
  id: 6465daafcf550af36eb198a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ad8dc340e3050458213baac0a110976a.svg
      fullname: Anon Nona
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fever8654
      type: user
    createdAt: '2023-06-07T14:08:39.000Z'
    data:
      status: closed
    id: 64808f67e1421e205fd8dd41
    type: status-change
  author: Fever8654
  created_at: 2023-06-07 13:08:39+00:00
  id: 64808f67e1421e205fd8dd41
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: tsumeone/wizard-vicuna-13b-4bit-128g-cuda
repo_type: model
status: closed
target_branch: null
title: Unable to mmap / Cannot Allocate Memory
