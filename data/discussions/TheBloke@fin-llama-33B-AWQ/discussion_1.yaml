!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brianhur
conflicting_files: null
created_at: 2023-11-15 23:40:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d2f8dc3c1fbcdaf9f1f3b277b299ad0e.svg
      fullname: Brian Hur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brianhur
      type: user
    createdAt: '2023-11-15T23:40:00.000Z'
    data:
      edited: false
      editors:
      - brianhur
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47020986676216125
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d2f8dc3c1fbcdaf9f1f3b277b299ad0e.svg
          fullname: Brian Hur
          isHf: false
          isPro: false
          name: brianhur
          type: user
        html: '<p>When running the demo code, the initial inference works, but then
          the pipeline gives the following error:</p>

          <hr>

          <p>AttributeError                            Traceback (most recent call
          last)<br>Input In [2], in &lt;cell line: 44&gt;()<br>     41 from transformers
          import pipeline<br>     43 print("*** Pipeline:")<br>---&gt; 44 pipe = pipeline(<br>     45     "text-generation",<br>     46     model=model,<br>     47     tokenizer=tokenizer,<br>     48     max_new_tokens=512,<br>     49     do_sample=True,<br>     50     temperature=0.7,<br>     51     top_p=0.95,<br>     52     top_k=40,<br>     53     repetition_penalty=1.1<br>     54
          )<br>     56 print(pipe(prompt_template)[0][''generated_text''])</p>

          <p>File /usr/local/lib/python3.9/dist-packages/transformers/pipelines/<strong>init</strong>.py:880,
          in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,
          framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code,
          model_kwargs, pipeline_class, **kwargs)<br>    869     model_classes = {"tf":
          targeted_task["tf"], "pt": targeted_task["pt"]}<br>    870     framework,
          model = infer_framework_load_model(<br>    871         model,<br>    872         model_classes=model_classes,<br>   (...)<br>    877         **model_kwargs,<br>    878     )<br>--&gt;
          880 model_config = model.config<br>    881 hub_kwargs["_commit_hash"] =
          model.config._commit_hash<br>    882 load_tokenizer = type(model_config)
          in TOKENIZER_MAPPING or model_config.tokenizer_class is not None</p>

          <p>File /usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1695,
          in Module.<strong>getattr</strong>(self, name)<br>   1693     if name in
          modules:<br>   1694         return modules[name]<br>-&gt; 1695 raise AttributeError(f"''{type(self).<strong>name</strong>}''
          object has no attribute ''{name}''")</p>

          <p>AttributeError: ''LlamaAWQForCausalLM'' object has no attribute ''config''</p>

          <p>Here''s the demo code for reference:</p>

          <p>from awq import AutoAWQForCausalLM<br>from transformers import AutoTokenizer</p>

          <p>model_name_or_path = "TheBloke/fin-llama-33B-AWQ"</p>

          <h1 id="load-model">Load model</h1>

          <p>model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,<br>                                          trust_remote_code=False,
          safetensors=True)<br>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,
          trust_remote_code=False)</p>

          <p>prompt = "Tell me about AI"<br>prompt_template=f''''''Below is an instruction
          that describes a task. Write a response that appropriately completes the
          request.</p>

          <h3 id="instruction">Instruction:</h3>

          <p>{prompt}</p>

          <h3 id="response">Response:</h3>

          <p>''''''</p>

          <p>print("\n\n*** Generate:")</p>

          <p>tokens = tokenizer(<br>    prompt_template,<br>    return_tensors=''pt''<br>).input_ids.cuda()</p>

          <h1 id="generate-output">Generate output</h1>

          <p>generation_output = model.generate(<br>    tokens,<br>    do_sample=True,<br>    temperature=0.7,<br>    top_p=0.95,<br>    top_k=40,<br>    max_new_tokens=512<br>)</p>

          <p>print("Output: ", tokenizer.decode(generation_output[0]))</p>

          <h1 id="inference-can-also-be-done-using-transformers-pipeline">Inference
          can also be done using transformers'' pipeline</h1>

          <p>from transformers import pipeline</p>

          <p>print("*** Pipeline:")<br>pipe = pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    max_new_tokens=512,<br>    do_sample=True,<br>    temperature=0.7,<br>    top_p=0.95,<br>    top_k=40,<br>    repetition_penalty=1.1<br>)</p>

          <p>print(pipe(prompt_template)[0][''generated_text''])</p>

          '
        raw: "When running the demo code, the initial inference works, but then the\
          \ pipeline gives the following error:\r\n\r\n\r\n---------------------------------------------------------------------------\r\
          \nAttributeError                            Traceback (most recent call\
          \ last)\r\nInput In [2], in <cell line: 44>()\r\n     41 from transformers\
          \ import pipeline\r\n     43 print(\"*** Pipeline:\")\r\n---> 44 pipe =\
          \ pipeline(\r\n     45     \"text-generation\",\r\n     46     model=model,\r\
          \n     47     tokenizer=tokenizer,\r\n     48     max_new_tokens=512,\r\n\
          \     49     do_sample=True,\r\n     50     temperature=0.7,\r\n     51\
          \     top_p=0.95,\r\n     52     top_k=40,\r\n     53     repetition_penalty=1.1\r\
          \n     54 )\r\n     56 print(pipe(prompt_template)[0]['generated_text'])\r\
          \n\r\nFile /usr/local/lib/python3.9/dist-packages/transformers/pipelines/__init__.py:880,\
          \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
          \ framework, revision, use_fast, token, device, device_map, torch_dtype,\
          \ trust_remote_code, model_kwargs, pipeline_class, **kwargs)\r\n    869\
          \     model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"\
          pt\"]}\r\n    870     framework, model = infer_framework_load_model(\r\n\
          \    871         model,\r\n    872         model_classes=model_classes,\r\
          \n   (...)\r\n    877         **model_kwargs,\r\n    878     )\r\n--> 880\
          \ model_config = model.config\r\n    881 hub_kwargs[\"_commit_hash\"] =\
          \ model.config._commit_hash\r\n    882 load_tokenizer = type(model_config)\
          \ in TOKENIZER_MAPPING or model_config.tokenizer_class is not None\r\n\r\
          \nFile /usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1695,\
          \ in Module.__getattr__(self, name)\r\n   1693     if name in modules:\r\
          \n   1694         return modules[name]\r\n-> 1695 raise AttributeError(f\"\
          '{type(self).__name__}' object has no attribute '{name}'\")\r\n\r\nAttributeError:\
          \ 'LlamaAWQForCausalLM' object has no attribute 'config'\r\n\r\n\r\nHere's\
          \ the demo code for reference:\r\n\r\nfrom awq import AutoAWQForCausalLM\r\
          \nfrom transformers import AutoTokenizer\r\n\r\nmodel_name_or_path = \"\
          TheBloke/fin-llama-33B-AWQ\"\r\n\r\n# Load model\r\nmodel = AutoAWQForCausalLM.from_quantized(model_name_or_path,\
          \ fuse_layers=True,\r\n                                          trust_remote_code=False,\
          \ safetensors=True)\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ trust_remote_code=False)\r\n\r\nprompt = \"Tell me about AI\"\r\nprompt_template=f'''Below\
          \ is an instruction that describes a task. Write a response that appropriately\
          \ completes the request.\r\n\r\n### Instruction:\r\n{prompt}\r\n\r\n###\
          \ Response:\r\n\r\n'''\r\n\r\nprint(\"\\n\\n*** Generate:\")\r\n\r\ntokens\
          \ = tokenizer(\r\n    prompt_template,\r\n    return_tensors='pt'\r\n).input_ids.cuda()\r\
          \n\r\n# Generate output\r\ngeneration_output = model.generate(\r\n    tokens,\r\
          \n    do_sample=True,\r\n    temperature=0.7,\r\n    top_p=0.95,\r\n   \
          \ top_k=40,\r\n    max_new_tokens=512\r\n)\r\n\r\nprint(\"Output: \", tokenizer.decode(generation_output[0]))\r\
          \n\r\n# Inference can also be done using transformers' pipeline\r\nfrom\
          \ transformers import pipeline\r\n\r\nprint(\"*** Pipeline:\")\r\npipe =\
          \ pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    max_new_tokens=512,\r\n    do_sample=True,\r\n    temperature=0.7,\r\
          \n    top_p=0.95,\r\n    top_k=40,\r\n    repetition_penalty=1.1\r\n)\r\n\
          \r\nprint(pipe(prompt_template)[0]['generated_text'])"
        updatedAt: '2023-11-15T23:40:00.455Z'
      numEdits: 0
      reactions: []
    id: 655556d0795cd3777f10532f
    type: comment
  author: brianhur
  content: "When running the demo code, the initial inference works, but then the\
    \ pipeline gives the following error:\r\n\r\n\r\n---------------------------------------------------------------------------\r\
    \nAttributeError                            Traceback (most recent call last)\r\
    \nInput In [2], in <cell line: 44>()\r\n     41 from transformers import pipeline\r\
    \n     43 print(\"*** Pipeline:\")\r\n---> 44 pipe = pipeline(\r\n     45    \
    \ \"text-generation\",\r\n     46     model=model,\r\n     47     tokenizer=tokenizer,\r\
    \n     48     max_new_tokens=512,\r\n     49     do_sample=True,\r\n     50  \
    \   temperature=0.7,\r\n     51     top_p=0.95,\r\n     52     top_k=40,\r\n \
    \    53     repetition_penalty=1.1\r\n     54 )\r\n     56 print(pipe(prompt_template)[0]['generated_text'])\r\
    \n\r\nFile /usr/local/lib/python3.9/dist-packages/transformers/pipelines/__init__.py:880,\
    \ in pipeline(task, model, config, tokenizer, feature_extractor, image_processor,\
    \ framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code,\
    \ model_kwargs, pipeline_class, **kwargs)\r\n    869     model_classes = {\"tf\"\
    : targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}\r\n    870     framework,\
    \ model = infer_framework_load_model(\r\n    871         model,\r\n    872   \
    \      model_classes=model_classes,\r\n   (...)\r\n    877         **model_kwargs,\r\
    \n    878     )\r\n--> 880 model_config = model.config\r\n    881 hub_kwargs[\"\
    _commit_hash\"] = model.config._commit_hash\r\n    882 load_tokenizer = type(model_config)\
    \ in TOKENIZER_MAPPING or model_config.tokenizer_class is not None\r\n\r\nFile\
    \ /usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1695, in Module.__getattr__(self,\
    \ name)\r\n   1693     if name in modules:\r\n   1694         return modules[name]\r\
    \n-> 1695 raise AttributeError(f\"'{type(self).__name__}' object has no attribute\
    \ '{name}'\")\r\n\r\nAttributeError: 'LlamaAWQForCausalLM' object has no attribute\
    \ 'config'\r\n\r\n\r\nHere's the demo code for reference:\r\n\r\nfrom awq import\
    \ AutoAWQForCausalLM\r\nfrom transformers import AutoTokenizer\r\n\r\nmodel_name_or_path\
    \ = \"TheBloke/fin-llama-33B-AWQ\"\r\n\r\n# Load model\r\nmodel = AutoAWQForCausalLM.from_quantized(model_name_or_path,\
    \ fuse_layers=True,\r\n                                          trust_remote_code=False,\
    \ safetensors=True)\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ trust_remote_code=False)\r\n\r\nprompt = \"Tell me about AI\"\r\nprompt_template=f'''Below\
    \ is an instruction that describes a task. Write a response that appropriately\
    \ completes the request.\r\n\r\n### Instruction:\r\n{prompt}\r\n\r\n### Response:\r\
    \n\r\n'''\r\n\r\nprint(\"\\n\\n*** Generate:\")\r\n\r\ntokens = tokenizer(\r\n\
    \    prompt_template,\r\n    return_tensors='pt'\r\n).input_ids.cuda()\r\n\r\n\
    # Generate output\r\ngeneration_output = model.generate(\r\n    tokens,\r\n  \
    \  do_sample=True,\r\n    temperature=0.7,\r\n    top_p=0.95,\r\n    top_k=40,\r\
    \n    max_new_tokens=512\r\n)\r\n\r\nprint(\"Output: \", tokenizer.decode(generation_output[0]))\r\
    \n\r\n# Inference can also be done using transformers' pipeline\r\nfrom transformers\
    \ import pipeline\r\n\r\nprint(\"*** Pipeline:\")\r\npipe = pipeline(\r\n    \"\
    text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n    max_new_tokens=512,\r\
    \n    do_sample=True,\r\n    temperature=0.7,\r\n    top_p=0.95,\r\n    top_k=40,\r\
    \n    repetition_penalty=1.1\r\n)\r\n\r\nprint(pipe(prompt_template)[0]['generated_text'])"
  created_at: 2023-11-15 23:40:00+00:00
  edited: false
  hidden: false
  id: 655556d0795cd3777f10532f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/fin-llama-33B-AWQ
repo_type: model
status: open
target_branch: null
title: '''LlamaAWQForCausalLM'' object has no attribute ''config'''
