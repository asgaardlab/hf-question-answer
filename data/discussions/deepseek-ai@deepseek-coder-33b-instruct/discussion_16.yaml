!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rjmehta
conflicting_files: null
created_at: 2023-12-18 16:18:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-12-18T16:18:29.000Z'
    data:
      edited: true
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5087124109268188
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: "<p>Model doesnt print anything. Just blank spaces. Using exllamav2.\
          \ <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n<h3 id=\"\
          input\">INPUT:</h3>\n<p>settings = ExLlamaV2Sampler.Settings()<br>settings.temperature\
          \ = 0.85<br>settings.top_k = 50<br>settings.top_p = 0.8<br>settings.token_repetition_penalty\
          \ = 1<br>#settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])<br>max_new_tokens\
          \ = 10</p>\n<h3 id=\"prompt\">Prompt</h3>\n<p>prompt = f\"\"\"Write a working\
          \ python code.<br>/#/#/# Instruction:<br>Write a working python code to\
          \ generate 100 random numbers.<br>/#/#/# Response:</p>\n<p>\"\"\"<br>input_ids\
          \ = tokenizer.encode(prompt)<br>prompt_tokens = input_ids.shape[-1]<br>generator.warmup()</p>\n\
          <p>time_begin_prompt = time.time()<br>print (prompt, end = \"\")<br>sys.stdout.flush()<br>generator.set_stop_conditions([])<br>generator.begin_stream(input_ids,\
          \ settings)<br>time_begin_stream = time.time()<br>generated_tokens = 0<br>while\
          \ True:<br>chunk, eos, _ = generator.stream()<br>generated_tokens += 1<br>print\
          \ (chunk, end = \"\")<br>sys.stdout.flush()<br>if eos or generated_tokens\
          \ == max_new_tokens: break<br>time_end = time.time()<br>time_prompt = time_begin_stream\
          \ - time_begin_prompt<br>time_tokens = time_end - time_begin_stream<br>print()<br>print()<br>print(f\"\
          Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens\
          \ / time_prompt:.2f} tokens/second\")<br>print(f\"Response generated in\
          \ {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens\
          \ / time_tokens:.2f} tokens/second\")</p>\n<h3 id=\"output\">OUTPUT:</h3>\n\
          <p>Write a working python code.<br>/#/#/# Instruction:<br>Write a working\
          \ python code to generate 100 random numbers.<br>/#/#/# Response:</p>\n\
          <p>Prompt processed in 0.00 seconds, 32 tokens, 27396.96 tokens/second<br>Response\
          \ generated in 0.43 seconds, 10 tokens, 23.49 tokens/second\"\"\"</p>\n"
        raw: "Model doesnt print anything. Just blank spaces. Using exllamav2. @TheBloke\
          \ \n\n### INPUT:\nsettings = ExLlamaV2Sampler.Settings()\nsettings.temperature\
          \ = 0.85\nsettings.top_k = 50\nsettings.top_p = 0.8\nsettings.token_repetition_penalty\
          \ = 1\n#settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n\
          max_new_tokens = 10\n\n### Prompt\nprompt = f\"\"\"Write a working python\
          \ code.\n/#/#/# Instruction:\nWrite a working python code to generate 100\
          \ random numbers.\n/#/#/# Response:\n\n\"\"\"\ninput_ids = tokenizer.encode(prompt)\n\
          prompt_tokens = input_ids.shape[-1]\ngenerator.warmup()\n\ntime_begin_prompt\
          \ = time.time()\nprint (prompt, end = \"\")\nsys.stdout.flush()\ngenerator.set_stop_conditions([])\n\
          generator.begin_stream(input_ids, settings)\ntime_begin_stream = time.time()\n\
          generated_tokens = 0\nwhile True:\nchunk, eos, _ = generator.stream()\n\
          generated_tokens += 1\nprint (chunk, end = \"\")\nsys.stdout.flush()\nif\
          \ eos or generated_tokens == max_new_tokens: break\ntime_end = time.time()\n\
          time_prompt = time_begin_stream - time_begin_prompt\ntime_tokens = time_end\
          \ - time_begin_stream\nprint()\nprint()\nprint(f\"Prompt processed in {time_prompt:.2f}\
          \ seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second\"\
          )\nprint(f\"Response generated in {time_tokens:.2f} seconds, {generated_tokens}\
          \ tokens, {generated_tokens / time_tokens:.2f} tokens/second\")\n\n### OUTPUT:\n\
          \nWrite a working python code.\n/#/#/# Instruction:\nWrite a working python\
          \ code to generate 100 random numbers.\n/#/#/# Response:\n\nPrompt processed\
          \ in 0.00 seconds, 32 tokens, 27396.96 tokens/second\nResponse generated\
          \ in 0.43 seconds, 10 tokens, 23.49 tokens/second\"\"\""
        updatedAt: '2023-12-18T16:20:08.461Z'
      numEdits: 2
      reactions: []
    id: 658070d57c71acb6464b6a6a
    type: comment
  author: rjmehta
  content: "Model doesnt print anything. Just blank spaces. Using exllamav2. @TheBloke\
    \ \n\n### INPUT:\nsettings = ExLlamaV2Sampler.Settings()\nsettings.temperature\
    \ = 0.85\nsettings.top_k = 50\nsettings.top_p = 0.8\nsettings.token_repetition_penalty\
    \ = 1\n#settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\nmax_new_tokens\
    \ = 10\n\n### Prompt\nprompt = f\"\"\"Write a working python code.\n/#/#/# Instruction:\n\
    Write a working python code to generate 100 random numbers.\n/#/#/# Response:\n\
    \n\"\"\"\ninput_ids = tokenizer.encode(prompt)\nprompt_tokens = input_ids.shape[-1]\n\
    generator.warmup()\n\ntime_begin_prompt = time.time()\nprint (prompt, end = \"\
    \")\nsys.stdout.flush()\ngenerator.set_stop_conditions([])\ngenerator.begin_stream(input_ids,\
    \ settings)\ntime_begin_stream = time.time()\ngenerated_tokens = 0\nwhile True:\n\
    chunk, eos, _ = generator.stream()\ngenerated_tokens += 1\nprint (chunk, end =\
    \ \"\")\nsys.stdout.flush()\nif eos or generated_tokens == max_new_tokens: break\n\
    time_end = time.time()\ntime_prompt = time_begin_stream - time_begin_prompt\n\
    time_tokens = time_end - time_begin_stream\nprint()\nprint()\nprint(f\"Prompt\
    \ processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens\
    \ / time_prompt:.2f} tokens/second\")\nprint(f\"Response generated in {time_tokens:.2f}\
    \ seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second\"\
    )\n\n### OUTPUT:\n\nWrite a working python code.\n/#/#/# Instruction:\nWrite a\
    \ working python code to generate 100 random numbers.\n/#/#/# Response:\n\nPrompt\
    \ processed in 0.00 seconds, 32 tokens, 27396.96 tokens/second\nResponse generated\
    \ in 0.43 seconds, 10 tokens, 23.49 tokens/second\"\"\""
  created_at: 2023-12-18 16:18:29+00:00
  edited: true
  hidden: false
  id: 658070d57c71acb6464b6a6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-12-18T18:31:32.000Z'
    data:
      edited: false
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9624691009521484
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: '<p>Okay. I had to manually set the rope_scale to 4.0. But gptq doesnt
          print EOS token</p>

          '
        raw: Okay. I had to manually set the rope_scale to 4.0. But gptq doesnt print
          EOS token
        updatedAt: '2023-12-18T18:31:32.688Z'
      numEdits: 0
      reactions: []
    id: 6580900416eb2b758e75751f
    type: comment
  author: rjmehta
  content: Okay. I had to manually set the rope_scale to 4.0. But gptq doesnt print
    EOS token
  created_at: 2023-12-18 18:31:32+00:00
  edited: false
  hidden: false
  id: 6580900416eb2b758e75751f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: deepseek-ai/deepseek-coder-33b-instruct
repo_type: model
status: open
target_branch: null
title: GPTQ Model doesnt work
