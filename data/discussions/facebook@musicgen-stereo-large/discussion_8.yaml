!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ziegabeez
conflicting_files: null
created_at: 2024-01-01 01:31:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e231c50fca1b4f14c0970db78cb60c74.svg
      fullname: Ryan O'Neill
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ziegabeez
      type: user
    createdAt: '2024-01-01T01:31:47.000Z'
    data:
      edited: false
      editors:
      - ziegabeez
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9639763236045837
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e231c50fca1b4f14c0970db78cb60c74.svg
          fullname: Ryan O'Neill
          isHf: false
          isPro: false
          name: ziegabeez
          type: user
        html: '<p>If I recall correctly, when MusicGen first came out there really
          was a 30 second limit to the generated clips. Basically, the clip loses
          coherence and became nonsense at exactly the 30 second mark. I think I encountered
          that issue using the original MusicGen-large (mono) model. At some point
          this limit seemed to disappear, and I have been able to generate 80 second
          clips that do not degenerate at any point using the MusicGen-Large (mono)
          model (I suspect I could go longer than 80 seconds, but haven''t tried).
          </p>

          <p>However, I recently was finally able to get the newer stereo-large model
          to function on a google colab using the A100 GPUs, but I am definitely hitting
          this limit as the clips are only coherent up to 30 seconds. In theory, that
          GPU should be able to generate a ~39 second clip based on the 40 GB of GPU
          RAM.</p>

          <p>I''m wondering if there is any further information about this issue.
          </p>

          <ul>

          <li>Am I correct that the 30 second limit on the original models was eventually
          bypassed somehow? </li>

          <li>If so, is there any plan to eventually do the same with the stereo models?</li>

          </ul>

          <p>I will personally opt to still use the newer Stereo clips with 30 second
          limit, but having the longer generations would be amazing because I would
          gain multiple additional full bars of music which is incredible for stitching
          clips together as a full-length song. </p>

          '
        raw: "If I recall correctly, when MusicGen first came out there really was\
          \ a 30 second limit to the generated clips. Basically, the clip loses coherence\
          \ and became nonsense at exactly the 30 second mark. I think I encountered\
          \ that issue using the original MusicGen-large (mono) model. At some point\
          \ this limit seemed to disappear, and I have been able to generate 80 second\
          \ clips that do not degenerate at any point using the MusicGen-Large (mono)\
          \ model (I suspect I could go longer than 80 seconds, but haven't tried).\
          \ \r\n\r\nHowever, I recently was finally able to get the newer stereo-large\
          \ model to function on a google colab using the A100 GPUs, but I am definitely\
          \ hitting this limit as the clips are only coherent up to 30 seconds. In\
          \ theory, that GPU should be able to generate a ~39 second clip based on\
          \ the 40 GB of GPU RAM.\r\n\r\nI'm wondering if there is any further information\
          \ about this issue. \r\n- Am I correct that the 30 second limit on the original\
          \ models was eventually bypassed somehow? \r\n- If so, is there any plan\
          \ to eventually do the same with the stereo models? \r\n\r\nI will personally\
          \ opt to still use the newer Stereo clips with 30 second limit, but having\
          \ the longer generations would be amazing because I would gain multiple\
          \ additional full bars of music which is incredible for stitching clips\
          \ together as a full-length song. "
        updatedAt: '2024-01-01T01:31:47.622Z'
      numEdits: 0
      reactions: []
    id: 6592160316227c7a2d695f95
    type: comment
  author: ziegabeez
  content: "If I recall correctly, when MusicGen first came out there really was a\
    \ 30 second limit to the generated clips. Basically, the clip loses coherence\
    \ and became nonsense at exactly the 30 second mark. I think I encountered that\
    \ issue using the original MusicGen-large (mono) model. At some point this limit\
    \ seemed to disappear, and I have been able to generate 80 second clips that do\
    \ not degenerate at any point using the MusicGen-Large (mono) model (I suspect\
    \ I could go longer than 80 seconds, but haven't tried). \r\n\r\nHowever, I recently\
    \ was finally able to get the newer stereo-large model to function on a google\
    \ colab using the A100 GPUs, but I am definitely hitting this limit as the clips\
    \ are only coherent up to 30 seconds. In theory, that GPU should be able to generate\
    \ a ~39 second clip based on the 40 GB of GPU RAM.\r\n\r\nI'm wondering if there\
    \ is any further information about this issue. \r\n- Am I correct that the 30\
    \ second limit on the original models was eventually bypassed somehow? \r\n- If\
    \ so, is there any plan to eventually do the same with the stereo models? \r\n\
    \r\nI will personally opt to still use the newer Stereo clips with 30 second limit,\
    \ but having the longer generations would be amazing because I would gain multiple\
    \ additional full bars of music which is incredible for stitching clips together\
    \ as a full-length song. "
  created_at: 2024-01-01 01:31:47+00:00
  edited: false
  hidden: false
  id: 6592160316227c7a2d695f95
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: facebook/musicgen-stereo-large
repo_type: model
status: open
target_branch: null
title: 30 second limit
