!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Marseus
conflicting_files: null
created_at: 2024-01-14 18:29:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/uJjV60dYSH5Xq9xyy9iqa.jpeg?w=200&h=200&f=face
      fullname: MarseusFu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marseus
      type: user
    createdAt: '2024-01-14T18:29:51.000Z'
    data:
      edited: false
      editors:
      - Marseus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7638190984725952
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/uJjV60dYSH5Xq9xyy9iqa.jpeg?w=200&h=200&f=face
          fullname: MarseusFu
          isHf: false
          isPro: false
          name: Marseus
          type: user
        html: '<p>Hello. I''m new here<br>I downloaded the original Mixtral in cloudyu/Mixtral_34Bx2_MoE_60B
          before, but my vram is not enough(4090 24G).<br>so i was trying to use this
          model but it pop out this error message.</p>

          <p>Here''s the code i use:</p>

          <p>Mixtral_model_name = "LoneStriker/Mixtral_34Bx2_MoE_60B-2.4bpw-h6-exl2"<br>model
          = AutoModelForCausalLM.from_pretrained(Mixtral_model_name,<br>                                                 torch_dtype=torch.float32,<br>                                                 device_map=''cuda'',<br>                                                 local_files_only=False,<br>                                                 #
          load_in_4bit=True<br>                                                )</p>

          <p>How can i fix this?<br>And how many vram will this model cost?<br>Thanks!</p>

          '
        raw: "Hello. I'm new here\r\nI downloaded the original Mixtral in cloudyu/Mixtral_34Bx2_MoE_60B\
          \ before, but my vram is not enough(4090 24G).\r\nso i was trying to use\
          \ this model but it pop out this error message.\r\n\r\nHere's the code i\
          \ use:\r\n\r\nMixtral_model_name = \"LoneStriker/Mixtral_34Bx2_MoE_60B-2.4bpw-h6-exl2\"\
          \r\nmodel = AutoModelForCausalLM.from_pretrained(Mixtral_model_name, \r\n\
          \                                                 torch_dtype=torch.float32,\
          \ \r\n                                                 device_map='cuda',\r\
          \n                                                 local_files_only=False,\
          \ \r\n                                                 # load_in_4bit=True\r\
          \n                                                )\r\n\r\nHow can i fix\
          \ this?\r\nAnd how many vram will this model cost?\r\nThanks!"
        updatedAt: '2024-01-14T18:29:51.144Z'
      numEdits: 0
      reactions: []
    id: 65a4281fd0e350dbc92d5414
    type: comment
  author: Marseus
  content: "Hello. I'm new here\r\nI downloaded the original Mixtral in cloudyu/Mixtral_34Bx2_MoE_60B\
    \ before, but my vram is not enough(4090 24G).\r\nso i was trying to use this\
    \ model but it pop out this error message.\r\n\r\nHere's the code i use:\r\n\r\
    \nMixtral_model_name = \"LoneStriker/Mixtral_34Bx2_MoE_60B-2.4bpw-h6-exl2\"\r\n\
    model = AutoModelForCausalLM.from_pretrained(Mixtral_model_name, \r\n        \
    \                                         torch_dtype=torch.float32, \r\n    \
    \                                             device_map='cuda',\r\n         \
    \                                        local_files_only=False, \r\n        \
    \                                         # load_in_4bit=True\r\n            \
    \                                    )\r\n\r\nHow can i fix this?\r\nAnd how many\
    \ vram will this model cost?\r\nThanks!"
  created_at: 2024-01-14 18:29:51+00:00
  edited: false
  hidden: false
  id: 65a4281fd0e350dbc92d5414
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2024-01-15T04:52:39.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4395515024662018
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Use either oobabooga''s text-generation-webui or exui from Github
          to load this model.  For ooba, use the <code>exllamav2</code> loader.</p>

          '
        raw: Use either oobabooga's text-generation-webui or exui from Github to load
          this model.  For ooba, use the `exllamav2` loader.
        updatedAt: '2024-01-15T04:52:39.355Z'
      numEdits: 0
      reactions: []
    id: 65a4ba179b0ac6aafc566901
    type: comment
  author: LoneStriker
  content: Use either oobabooga's text-generation-webui or exui from Github to load
    this model.  For ooba, use the `exllamav2` loader.
  created_at: 2024-01-15 04:52:39+00:00
  edited: false
  hidden: false
  id: 65a4ba179b0ac6aafc566901
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/uJjV60dYSH5Xq9xyy9iqa.jpeg?w=200&h=200&f=face
      fullname: MarseusFu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marseus
      type: user
    createdAt: '2024-01-15T08:13:44.000Z'
    data:
      edited: false
      editors:
      - Marseus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6185957789421082
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/uJjV60dYSH5Xq9xyy9iqa.jpeg?w=200&h=200&f=face
          fullname: MarseusFu
          isHf: false
          isPro: false
          name: Marseus
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;LoneStriker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/LoneStriker\"\
          >@<span class=\"underline\">LoneStriker</span></a></span>\n\n\t</span></span>\
          \ Thansk! oobabooga's text-generation-webui works, but still out of vram.<br>How\
          \ many vram does this model cost? It seems &gt;24G<br><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/64a568dc764b1dce366f9c40/0T3lEqrZARWpsOIhomK3D.jpeg\"\
          ><img alt=\"\u87A2\u5E55\u64F7\u53D6\u756B\u9762 2024-01-15 161323.jpg\"\
          \ src=\"https://cdn-uploads.huggingface.co/production/uploads/64a568dc764b1dce366f9c40/0T3lEqrZARWpsOIhomK3D.jpeg\"\
          ></a></p>\n"
        raw: "@LoneStriker Thansk! oobabooga's text-generation-webui works, but still\
          \ out of vram. \nHow many vram does this model cost? It seems >24G\n![\u87A2\
          \u5E55\u64F7\u53D6\u756B\u9762 2024-01-15 161323.jpg](https://cdn-uploads.huggingface.co/production/uploads/64a568dc764b1dce366f9c40/0T3lEqrZARWpsOIhomK3D.jpeg)\n"
        updatedAt: '2024-01-15T08:13:44.922Z'
      numEdits: 0
      reactions: []
    id: 65a4e9389f0a34c173889aba
    type: comment
  author: Marseus
  content: "@LoneStriker Thansk! oobabooga's text-generation-webui works, but still\
    \ out of vram. \nHow many vram does this model cost? It seems >24G\n![\u87A2\u5E55\
    \u64F7\u53D6\u756B\u9762 2024-01-15 161323.jpg](https://cdn-uploads.huggingface.co/production/uploads/64a568dc764b1dce366f9c40/0T3lEqrZARWpsOIhomK3D.jpeg)\n"
  created_at: 2024-01-15 08:13:44+00:00
  edited: false
  hidden: false
  id: 65a4e9389f0a34c173889aba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2024-01-15T15:57:33.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7350398898124695
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>It should fit in 24 GB VRAM, you probably need to reduce the max
          tokens in ooba, here is the model size for 2.4 and 2.65:</p>

          <pre><code>18G     Mixtral_34Bx2_MoE_60B-2.4bpw-h6-exl2

          20G     Mixtral_34Bx2_MoE_60B-2.65bpw-h6-exl2

          </code></pre>

          <p>Drop your max tokens to 2048 to see if it loads.</p>

          '
        raw: 'It should fit in 24 GB VRAM, you probably need to reduce the max tokens
          in ooba, here is the model size for 2.4 and 2.65:

          ```

          18G     Mixtral_34Bx2_MoE_60B-2.4bpw-h6-exl2

          20G     Mixtral_34Bx2_MoE_60B-2.65bpw-h6-exl2

          ```

          Drop your max tokens to 2048 to see if it loads.'
        updatedAt: '2024-01-15T15:57:33.681Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Marseus
    id: 65a555edb7897304be768392
    type: comment
  author: LoneStriker
  content: 'It should fit in 24 GB VRAM, you probably need to reduce the max tokens
    in ooba, here is the model size for 2.4 and 2.65:

    ```

    18G     Mixtral_34Bx2_MoE_60B-2.4bpw-h6-exl2

    20G     Mixtral_34Bx2_MoE_60B-2.65bpw-h6-exl2

    ```

    Drop your max tokens to 2048 to see if it loads.'
  created_at: 2024-01-15 15:57:33+00:00
  edited: false
  hidden: false
  id: 65a555edb7897304be768392
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: LoneStriker/Mixtral_34Bx2_MoE_60B-2.4bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: 'OSError: LoneStriker/Mixtral_34Bx2_MoE_60B-2.4bpw-h6-exl2 does not appear
  to have a file named model-00001-of-00013.safetensors.'
