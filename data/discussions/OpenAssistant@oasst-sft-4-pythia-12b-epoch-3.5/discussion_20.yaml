!!python/object:huggingface_hub.community.DiscussionWithDetails
author: felipemv
conflicting_files: null
created_at: 2023-06-07 11:55:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0ed9aacad3f230f1e68265d4a806bcc.svg
      fullname: felipe m. vieira
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: felipemv
      type: user
    createdAt: '2023-06-07T12:55:13.000Z'
    data:
      edited: false
      editors:
      - felipemv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5891434550285339
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0ed9aacad3f230f1e68265d4a806bcc.svg
          fullname: felipe m. vieira
          isHf: false
          isPro: false
          name: felipemv
          type: user
        html: "<p>Hi, I'm trying to reproduce your results, but at the early stages\
          \ there seems to be a stuck process.</p>\n<pre><code>echo '\n{\n  \"fp16\"\
          : {\n    \"enabled\": true,\n\u22EF (identical to yours)\n  \"train_micro_batch_size_per_gpu\"\
          : \"auto\",\n  \"wall_clock_breakdown\": false\n}\n' &gt; ./ds_config.json\n\
          deepspeed \\\n    ./trainer_sft.py \\\n    --configs defaults reference-data\
          \ reference-pythia-12b \\\n    --cache_dir /root/.cache/huggingface \\\n\
          \    --output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre \\\n   \
          \ --num_train_epochs 8 \\\n    --use_flash_attention false \\\n    --verbose\
          \ true \\\n    --logging_steps 1 \\\n    --dtype fp16 \\\n    --residual_dropout\
          \ 0.2 \\\n    --model_name andreaskoepf/pythia-12b-pre-2000\n</code></pre>\n\
          <p>So I get the following logs (abbreviated):</p>\n<pre><code>Evaluation\
          \ set sizes:\noasst_export: 2026 (16.55%)\nalpaca: 10212 (83.45%)\nTotal\
          \ eval: 12238\n--------------------------------------------------------------------------------\n\
          \u22EF\nNumber of trainable parameters: 11841M\nLoading checkpoint shards:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3\
          \ [00:17&lt;00:00,  5.83s/it]\nResizing embeddings to 50288\n\u22EF\n  warnings.warn(\n\
          /usr/local/lib/python3.10/site-packages/transformers/optimization.py:407:\
          \ FutureWarning: This implementation of AdamW is deprecated and will be\
          \ removed in a future version. Use the PyTorch implementation torch.optim.AdamW\
          \ instead, or set `no_deprecation_warning=True` to disable this warning\n\
          \  warnings.warn(\n</code></pre>\n<p>I get a burst of GPU activity some\
          \ 3 minutes after starting the process. It lasts for about 10 seconds, then\
          \ it halts completely and I get stuck with a single process using 100% of\
          \ a CPU:</p>\n<pre><code>    PID USER      PR  NI    VIRT    RES    SHR\
          \ S  %CPU  %MEM     TIME+ COMMAND\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \ \u2588\u2588\u2588\u2588      \u2588\u2588   \u2588   \u2588\u2588\u2588\
          \u2588\u2588  \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\
          \u2588 \u2588  \u2588\u2588\u2588\u2588   \u2588\u2588\u2588 \u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588 /usr/local/bin/python3 -u ./trainer_sft.py\
          \ --local_rank=6\n</code></pre>\n<p>Do you have any idea what might that\
          \ be? </p>\n<p>Let me know if more logs/info would help. I'm using 8 GPUs\
          \ which should fit this model comfortably in memory.</p>\n"
        raw: "Hi, I'm trying to reproduce your results, but at the early stages there\
          \ seems to be a stuck process.\r\n\r\n```\r\necho '\r\n{\r\n  \"fp16\":\
          \ {\r\n    \"enabled\": true,\r\n\u22EF (identical to yours)\r\n  \"train_micro_batch_size_per_gpu\"\
          : \"auto\",\r\n  \"wall_clock_breakdown\": false\r\n}\r\n' > ./ds_config.json\r\
          \ndeepspeed \\\r\n    ./trainer_sft.py \\\r\n    --configs defaults reference-data\
          \ reference-pythia-12b \\\r\n    --cache_dir /root/.cache/huggingface \\\
          \r\n    --output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre \\\r\n\
          \    --num_train_epochs 8 \\\r\n    --use_flash_attention false \\\r\n \
          \   --verbose true \\\r\n    --logging_steps 1 \\\r\n    --dtype fp16 \\\
          \r\n    --residual_dropout 0.2 \\\r\n    --model_name andreaskoepf/pythia-12b-pre-2000\r\
          \n```\r\n\r\nSo I get the following logs (abbreviated):\r\n\r\n```\r\nEvaluation\
          \ set sizes:\r\noasst_export: 2026 (16.55%)\r\nalpaca: 10212 (83.45%)\r\n\
          Total eval: 12238\r\n--------------------------------------------------------------------------------\r\
          \n\u22EF\r\nNumber of trainable parameters: 11841M\r\nLoading checkpoint\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 3/3 [00:17<00:00,  5.83s/it]\r\nResizing embeddings to 50288\r\n\u22EF\
          \r\n  warnings.warn(\r\n/usr/local/lib/python3.10/site-packages/transformers/optimization.py:407:\
          \ FutureWarning: This implementation of AdamW is deprecated and will be\
          \ removed in a future version. Use the PyTorch implementation torch.optim.AdamW\
          \ instead, or set `no_deprecation_warning=True` to disable this warning\r\
          \n  warnings.warn(\r\n```\r\n\r\nI get a burst of GPU activity some 3 minutes\
          \ after starting the process. It lasts for about 10 seconds, then it halts\
          \ completely and I get stuck with a single process using 100% of a CPU:\r\
          \n\r\n```\r\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM\
          \     TIME+ COMMAND\r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\
          \u2588\u2588\u2588      \u2588\u2588   \u2588   \u2588\u2588\u2588\u2588\
          \u2588  \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\
          \ \u2588  \u2588\u2588\u2588\u2588   \u2588\u2588\u2588 \u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588 /usr/local/bin/python3 -u ./trainer_sft.py\
          \ --local_rank=6\r\n```\r\n\r\nDo you have any idea what might that be?\
          \ \r\n\r\nLet me know if more logs/info would help. I'm using 8 GPUs which\
          \ should fit this model comfortably in memory.\r\n"
        updatedAt: '2023-06-07T12:55:13.259Z'
      numEdits: 0
      reactions: []
    id: 64807e31cacb1c4a06950ffe
    type: comment
  author: felipemv
  content: "Hi, I'm trying to reproduce your results, but at the early stages there\
    \ seems to be a stuck process.\r\n\r\n```\r\necho '\r\n{\r\n  \"fp16\": {\r\n\
    \    \"enabled\": true,\r\n\u22EF (identical to yours)\r\n  \"train_micro_batch_size_per_gpu\"\
    : \"auto\",\r\n  \"wall_clock_breakdown\": false\r\n}\r\n' > ./ds_config.json\r\
    \ndeepspeed \\\r\n    ./trainer_sft.py \\\r\n    --configs defaults reference-data\
    \ reference-pythia-12b \\\r\n    --cache_dir /root/.cache/huggingface \\\r\n \
    \   --output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre \\\r\n    --num_train_epochs\
    \ 8 \\\r\n    --use_flash_attention false \\\r\n    --verbose true \\\r\n    --logging_steps\
    \ 1 \\\r\n    --dtype fp16 \\\r\n    --residual_dropout 0.2 \\\r\n    --model_name\
    \ andreaskoepf/pythia-12b-pre-2000\r\n```\r\n\r\nSo I get the following logs (abbreviated):\r\
    \n\r\n```\r\nEvaluation set sizes:\r\noasst_export: 2026 (16.55%)\r\nalpaca: 10212\
    \ (83.45%)\r\nTotal eval: 12238\r\n--------------------------------------------------------------------------------\r\
    \n\u22EF\r\nNumber of trainable parameters: 11841M\r\nLoading checkpoint shards:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:17<00:00,\
    \  5.83s/it]\r\nResizing embeddings to 50288\r\n\u22EF\r\n  warnings.warn(\r\n\
    /usr/local/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning:\
    \ This implementation of AdamW is deprecated and will be removed in a future version.\
    \ Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True`\
    \ to disable this warning\r\n  warnings.warn(\r\n```\r\n\r\nI get a burst of GPU\
    \ activity some 3 minutes after starting the process. It lasts for about 10 seconds,\
    \ then it halts completely and I get stuck with a single process using 100% of\
    \ a CPU:\r\n\r\n```\r\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU\
    \  %MEM     TIME+ COMMAND\r\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\
    \u2588\u2588\u2588      \u2588\u2588   \u2588   \u2588\u2588\u2588\u2588\u2588\
    \  \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\
    \  \u2588\u2588\u2588\u2588   \u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588 /usr/local/bin/python3 -u ./trainer_sft.py --local_rank=6\r\
    \n```\r\n\r\nDo you have any idea what might that be? \r\n\r\nLet me know if more\
    \ logs/info would help. I'm using 8 GPUs which should fit this model comfortably\
    \ in memory.\r\n"
  created_at: 2023-06-07 11:55:13+00:00
  edited: false
  hidden: false
  id: 64807e31cacb1c4a06950ffe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5
repo_type: model
status: open
target_branch: null
title: Reproducing the fine tuning gets stuck with 100% CPU on one process
