!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sauravm8
conflicting_files: null
created_at: 2023-05-12 04:04:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/177baf68c9c43bb80f532c556f9457e8.svg
      fullname: Mukherjee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sauravm8
      type: user
    createdAt: '2023-05-12T05:04:02.000Z'
    data:
      edited: false
      editors:
      - sauravm8
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/177baf68c9c43bb80f532c556f9457e8.svg
          fullname: Mukherjee
          isHf: false
          isPro: false
          name: sauravm8
          type: user
        html: '<p>I am facing an inference error<br><code>The size of tensor a (2048)
          must match the size of tensor b (2049) at non-singleton dimension 3</code></p>

          <p>I am cutting off the input prompt way before 2048 tokens at around 1250
          words (~1500 tokens)</p>

          <pre><code>`final_message = f"&lt;|prompter|&gt;What do you think of the
          following and keep it under 100 words: \n {received_message}&lt;|endoftext|&gt;&lt;|assistant|&gt;"

          inputs = tokenizer(final_message, return_tensors="pt").to(model.device)

          print(f"Number of tokens --&gt; {inputs[''input_ids''].shape[1]}")


          tokens = model.generate(**inputs,  max_new_tokens=1000, do_sample=True,
          temperature=0.8)

          response = tokenizer.decode(tokens[0]).split("&lt;|assistant|&gt;")[1].strip("&lt;|endoftext|&gt;")

          print(f"Total time taken is {time.time()-start_query_time}")

          print(response)`

          </code></pre>

          <p>GPU is not an issue. What is going on here?</p>

          '
        raw: "I am facing an inference error\r\n`The size of tensor a (2048) must\
          \ match the size of tensor b (2049) at non-singleton dimension 3`\r\n\r\n\
          I am cutting off the input prompt way before 2048 tokens at around 1250\
          \ words (~1500 tokens)\r\n\r\n    `final_message = f\"<|prompter|>What do\
          \ you think of the following and keep it under 100 words: \\n {received_message}<|endoftext|><|assistant|>\"\
          \r\n    inputs = tokenizer(final_message, return_tensors=\"pt\").to(model.device)\r\
          \n    print(f\"Number of tokens --> {inputs['input_ids'].shape[1]}\")\r\n\
          \    \r\n    tokens = model.generate(**inputs,  max_new_tokens=1000, do_sample=True,\
          \ temperature=0.8)\r\n    response = tokenizer.decode(tokens[0]).split(\"\
          <|assistant|>\")[1].strip(\"<|endoftext|>\")\r\n    print(f\"Total time\
          \ taken is {time.time()-start_query_time}\")\r\n    print(response)`\r\n\
          \r\n\r\n\r\nGPU is not an issue. What is going on here?"
        updatedAt: '2023-05-12T05:04:02.444Z'
      numEdits: 0
      reactions: []
    id: 645dc8c201f4eaab2a0b4386
    type: comment
  author: sauravm8
  content: "I am facing an inference error\r\n`The size of tensor a (2048) must match\
    \ the size of tensor b (2049) at non-singleton dimension 3`\r\n\r\nI am cutting\
    \ off the input prompt way before 2048 tokens at around 1250 words (~1500 tokens)\r\
    \n\r\n    `final_message = f\"<|prompter|>What do you think of the following and\
    \ keep it under 100 words: \\n {received_message}<|endoftext|><|assistant|>\"\r\
    \n    inputs = tokenizer(final_message, return_tensors=\"pt\").to(model.device)\r\
    \n    print(f\"Number of tokens --> {inputs['input_ids'].shape[1]}\")\r\n    \r\
    \n    tokens = model.generate(**inputs,  max_new_tokens=1000, do_sample=True,\
    \ temperature=0.8)\r\n    response = tokenizer.decode(tokens[0]).split(\"<|assistant|>\"\
    )[1].strip(\"<|endoftext|>\")\r\n    print(f\"Total time taken is {time.time()-start_query_time}\"\
    )\r\n    print(response)`\r\n\r\n\r\n\r\nGPU is not an issue. What is going on\
    \ here?"
  created_at: 2023-05-12 04:04:02+00:00
  edited: false
  hidden: false
  id: 645dc8c201f4eaab2a0b4386
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/177baf68c9c43bb80f532c556f9457e8.svg
      fullname: Mukherjee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sauravm8
      type: user
    createdAt: '2023-05-12T05:09:26.000Z'
    data:
      edited: false
      editors:
      - sauravm8
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/177baf68c9c43bb80f532c556f9457e8.svg
          fullname: Mukherjee
          isHf: false
          isPro: false
          name: sauravm8
          type: user
        html: '<p>I think I figured it out, even the new tokens getting generated
          have to be within the 2048 token limit. Otherwise, it crashes. Can it not
          have a streaming window of memory? Am I missing something?</p>

          '
        raw: I think I figured it out, even the new tokens getting generated have
          to be within the 2048 token limit. Otherwise, it crashes. Can it not have
          a streaming window of memory? Am I missing something?
        updatedAt: '2023-05-12T05:09:26.682Z'
      numEdits: 0
      reactions: []
    id: 645dca06f1e3b219cb11b595
    type: comment
  author: sauravm8
  content: I think I figured it out, even the new tokens getting generated have to
    be within the 2048 token limit. Otherwise, it crashes. Can it not have a streaming
    window of memory? Am I missing something?
  created_at: 2023-05-12 04:09:26+00:00
  edited: false
  hidden: false
  id: 645dca06f1e3b219cb11b595
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/892903cba2a3d02afa44e3181e96bb9b.svg
      fullname: Vijay RPS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vijayrps
      type: user
    createdAt: '2023-06-20T13:51:17.000Z'
    data:
      edited: false
      editors:
      - vijayrps
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9002957940101624
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/892903cba2a3d02afa44e3181e96bb9b.svg
          fullname: Vijay RPS
          isHf: false
          isPro: false
          name: vijayrps
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sauravm8&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sauravm8\">@<span class=\"\
          underline\">sauravm8</span></a></span>\n\n\t</span></span> facing a similar\
          \ issue.whats the solution?</p>\n"
        raw: '@sauravm8 facing a similar issue.whats the solution?'
        updatedAt: '2023-06-20T13:51:17.288Z'
      numEdits: 0
      reactions: []
    id: 6491aed5c1b37bfee200b0a5
    type: comment
  author: vijayrps
  content: '@sauravm8 facing a similar issue.whats the solution?'
  created_at: 2023-06-20 12:51:17+00:00
  edited: false
  hidden: false
  id: 6491aed5c1b37bfee200b0a5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5
repo_type: model
status: open
target_branch: null
title: Facing error while inferencing
