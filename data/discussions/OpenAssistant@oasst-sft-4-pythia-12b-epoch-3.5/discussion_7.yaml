!!python/object:huggingface_hub.community.DiscussionWithDetails
author: russellsparadox
conflicting_files: null
created_at: 2023-04-18 15:37:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667582983668-63654b9f98da81987e215a5b.jpeg?w=200&h=200&f=face
      fullname: Sergey Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: russellsparadox
      type: user
    createdAt: '2023-04-18T16:37:25.000Z'
    data:
      edited: true
      editors:
      - russellsparadox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667582983668-63654b9f98da81987e215a5b.jpeg?w=200&h=200&f=face
          fullname: Sergey Ivanov
          isHf: false
          isPro: false
          name: russellsparadox
          type: user
        html: "<p>I found that '4.21.1' version of transformers give better inference\
          \ time than '4.28.1'. </p>\n<p>For 4.21.1 the downloaded models looks like\
          \ this </p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63654b9f98da81987e215a5b/Xwm1WV6k2KBr8BkKPEFly.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63654b9f98da81987e215a5b/Xwm1WV6k2KBr8BkKPEFly.png\"\
          ></a></p>\n<p>While for 4.28.1<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63654b9f98da81987e215a5b/wYu2mc8uFx9gsZCyMp0FY.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63654b9f98da81987e215a5b/wYu2mc8uFx9gsZCyMp0FY.png\"\
          ></a></p>\n<p>Also for 4.21.1 version gives <code>load_in_8bit</code> is\
          \ not implemented. </p>\n<p>Generated text is identical (do_sample=False).\
          \ </p>\n<p>Here are few comparisons I did. It seems the difference becomes\
          \ more pronounced for bigger texts. </p>\n<p><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/63654b9f98da81987e215a5b/14Q2UtW8Z97hA-5H3y59a.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63654b9f98da81987e215a5b/14Q2UtW8Z97hA-5H3y59a.png\"\
          ></a></p>\n<p>Here is the code to generate it: </p>\n<pre><code class=\"\
          language-python\">model_name = <span class=\"hljs-string\">\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          </span>\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_name, \n          \
          \                                   device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>,\n                                             <span class=\"\
          hljs-comment\"># load_in_8bit=True,</span>\n                           \
          \                  torch_dtype=<span class=\"hljs-string\">'auto'</span>,\n\
          \                                             cache_dir=cache_dir\n    \
          \                                        )\ninputs = tokenizer(query, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>).to(model.device)\ntokens = model.generate(**inputs,\
          \  max_new_tokens=<span class=\"hljs-number\">128</span>, do_sample=<span\
          \ class=\"hljs-literal\">False</span>, pad_token_id=tokenizer.eos_token_id)\n\
          output = tokenizer.decode(tokens[<span class=\"hljs-number\">0</span>])\n\
          </code></pre>\n"
        raw: "I found that '4.21.1' version of transformers give better inference\
          \ time than '4.28.1'. \n\nFor 4.21.1 the downloaded models looks like this\
          \ \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63654b9f98da81987e215a5b/Xwm1WV6k2KBr8BkKPEFly.png)\n\
          \nWhile for 4.28.1 \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63654b9f98da81987e215a5b/wYu2mc8uFx9gsZCyMp0FY.png)\n\
          \nAlso for 4.21.1 version gives `load_in_8bit` is not implemented. \n\n\
          Generated text is identical (do_sample=False). \n\nHere are few comparisons\
          \ I did. It seems the difference becomes more pronounced for bigger texts.\
          \ \n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63654b9f98da81987e215a5b/14Q2UtW8Z97hA-5H3y59a.png)\n\
          \nHere is the code to generate it: \n\n```python\nmodel_name = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          \n\ntokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_name, \n          \
          \                                   device_map=\"auto\",\n             \
          \                                # load_in_8bit=True,\n                \
          \                             torch_dtype='auto',\n                    \
          \                         cache_dir=cache_dir\n                        \
          \                    )\ninputs = tokenizer(query, return_tensors=\"pt\"\
          ).to(model.device)\ntokens = model.generate(**inputs,  max_new_tokens=128,\
          \ do_sample=False, pad_token_id=tokenizer.eos_token_id)\noutput = tokenizer.decode(tokens[0])\n\
          ```"
        updatedAt: '2023-04-19T07:56:46.903Z'
      numEdits: 1
      reactions: []
    id: 643ec745a16cd6d1f4c277bb
    type: comment
  author: russellsparadox
  content: "I found that '4.21.1' version of transformers give better inference time\
    \ than '4.28.1'. \n\nFor 4.21.1 the downloaded models looks like this \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63654b9f98da81987e215a5b/Xwm1WV6k2KBr8BkKPEFly.png)\n\
    \nWhile for 4.28.1 \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63654b9f98da81987e215a5b/wYu2mc8uFx9gsZCyMp0FY.png)\n\
    \nAlso for 4.21.1 version gives `load_in_8bit` is not implemented. \n\nGenerated\
    \ text is identical (do_sample=False). \n\nHere are few comparisons I did. It\
    \ seems the difference becomes more pronounced for bigger texts. \n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63654b9f98da81987e215a5b/14Q2UtW8Z97hA-5H3y59a.png)\n\
    \nHere is the code to generate it: \n\n```python\nmodel_name = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
    \n\ntokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n\
    \nmodel = AutoModelForCausalLM.from_pretrained(model_name, \n                \
    \                             device_map=\"auto\",\n                         \
    \                    # load_in_8bit=True,\n                                  \
    \           torch_dtype='auto',\n                                            \
    \ cache_dir=cache_dir\n                                            )\ninputs =\
    \ tokenizer(query, return_tensors=\"pt\").to(model.device)\ntokens = model.generate(**inputs,\
    \  max_new_tokens=128, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n\
    output = tokenizer.decode(tokens[0])\n```"
  created_at: 2023-04-18 15:37:25+00:00
  edited: true
  hidden: false
  id: 643ec745a16cd6d1f4c277bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-04-18T21:16:54.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>This is an interesting finding... I was wondering why the model
          feels slow even in 8bit while the llama 30b model is lightning quick at
          <a rel="nofollow" href="https://open-assistant.io/chat">https://open-assistant.io/chat</a>
          .</p>

          '
        raw: This is an interesting finding... I was wondering why the model feels
          slow even in 8bit while the llama 30b model is lightning quick at https://open-assistant.io/chat
          .
        updatedAt: '2023-04-18T21:16:54.887Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PierrePeng
    id: 643f08c6f2ed3bc5c065fd0f
    type: comment
  author: gsaivinay
  content: This is an interesting finding... I was wondering why the model feels slow
    even in 8bit while the llama 30b model is lightning quick at https://open-assistant.io/chat
    .
  created_at: 2023-04-18 20:16:54+00:00
  edited: false
  hidden: false
  id: 643f08c6f2ed3bc5c065fd0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
      fullname: qc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captainst
      type: user
    createdAt: '2023-04-19T11:08:33.000Z'
    data:
      edited: false
      editors:
      - captainst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
          fullname: qc
          isHf: false
          isPro: false
          name: captainst
          type: user
        html: '<p>Have you tried the output for multiple times ?</p>

          '
        raw: Have you tried the output for multiple times ?
        updatedAt: '2023-04-19T11:08:33.420Z'
      numEdits: 0
      reactions: []
    id: 643fcbb14164a65ca121bf85
    type: comment
  author: captainst
  content: Have you tried the output for multiple times ?
  created_at: 2023-04-19 10:08:33+00:00
  edited: false
  hidden: false
  id: 643fcbb14164a65ca121bf85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-04-19T11:12:19.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>I use huggingface pipeline with model loaded in 8bit and repeatedly
          calling it to generate, while first token takes some time, the subsequent
          tokens take around 0.5s each. Haven''t tried much to optimise TBH. Also
          there is generate_stream available now but haven''t explored it yet.</p>

          '
        raw: I use huggingface pipeline with model loaded in 8bit and repeatedly calling
          it to generate, while first token takes some time, the subsequent tokens
          take around 0.5s each. Haven't tried much to optimise TBH. Also there is
          generate_stream available now but haven't explored it yet.
        updatedAt: '2023-04-19T11:12:19.414Z'
      numEdits: 0
      reactions: []
    id: 643fcc932113f7dfcb49bb15
    type: comment
  author: gsaivinay
  content: I use huggingface pipeline with model loaded in 8bit and repeatedly calling
    it to generate, while first token takes some time, the subsequent tokens take
    around 0.5s each. Haven't tried much to optimise TBH. Also there is generate_stream
    available now but haven't explored it yet.
  created_at: 2023-04-19 10:12:19+00:00
  edited: false
  hidden: false
  id: 643fcc932113f7dfcb49bb15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
      fullname: qc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captainst
      type: user
    createdAt: '2023-04-19T12:12:19.000Z'
    data:
      edited: false
      editors:
      - captainst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
          fullname: qc
          isHf: false
          isPro: false
          name: captainst
          type: user
        html: '<p>I used 2 x 3060 to load the model in 8-bit (with load_in_ 8bit=True),
          where each GPU takes about 7GB of memory. The first time I called model.generate,
          it took more than 2 min to generate the output. However, the subsequntial
          call took merely 2~3 seconds to generate response.</p>

          <p>I think the speed of inference depends much on your hardware setup.</p>

          '
        raw: 'I used 2 x 3060 to load the model in 8-bit (with load_in_ 8bit=True),
          where each GPU takes about 7GB of memory. The first time I called model.generate,
          it took more than 2 min to generate the output. However, the subsequntial
          call took merely 2~3 seconds to generate response.


          I think the speed of inference depends much on your hardware setup.'
        updatedAt: '2023-04-19T12:12:19.147Z'
      numEdits: 0
      reactions: []
    id: 643fdaa32113f7dfcb4b24d2
    type: comment
  author: captainst
  content: 'I used 2 x 3060 to load the model in 8-bit (with load_in_ 8bit=True),
    where each GPU takes about 7GB of memory. The first time I called model.generate,
    it took more than 2 min to generate the output. However, the subsequntial call
    took merely 2~3 seconds to generate response.


    I think the speed of inference depends much on your hardware setup.'
  created_at: 2023-04-19 11:12:19+00:00
  edited: false
  hidden: false
  id: 643fdaa32113f7dfcb4b24d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-04-19T12:15:47.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>How many token are generated in those 2 to 3 seconds?</p>

          '
        raw: How many token are generated in those 2 to 3 seconds?
        updatedAt: '2023-04-19T12:15:47.140Z'
      numEdits: 0
      reactions: []
    id: 643fdb73dbd88206a833510c
    type: comment
  author: gsaivinay
  content: How many token are generated in those 2 to 3 seconds?
  created_at: 2023-04-19 11:15:47+00:00
  edited: false
  hidden: false
  id: 643fdb73dbd88206a833510c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
      fullname: qc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captainst
      type: user
    createdAt: '2023-04-19T12:44:00.000Z'
    data:
      edited: false
      editors:
      - captainst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
          fullname: qc
          isHf: false
          isPro: false
          name: captainst
          type: user
        html: '<p>I just made an experiment with different prompts. On my hardware
          setup, </p>

          <ol>

          <li>for longer output (&gt; 200 token), it''s about 0.22 sec for an output
          token.</li>

          <li>for shorter output (&lt; 20 token), it''s about 0.15 sec for an output
          token.</li>

          </ol>

          <p>The above is just roughly estimated</p>

          '
        raw: "I just made an experiment with different prompts. On my hardware setup,\
          \ \n1. for longer output (> 200 token), it's about 0.22 sec for an output\
          \ token.\n2. for shorter output (< 20 token), it's about 0.15 sec for an\
          \ output token.\n\nThe above is just roughly estimated"
        updatedAt: '2023-04-19T12:44:00.668Z'
      numEdits: 0
      reactions: []
    id: 643fe210dbd88206a833f2a9
    type: comment
  author: captainst
  content: "I just made an experiment with different prompts. On my hardware setup,\
    \ \n1. for longer output (> 200 token), it's about 0.22 sec for an output token.\n\
    2. for shorter output (< 20 token), it's about 0.15 sec for an output token.\n\
    \nThe above is just roughly estimated"
  created_at: 2023-04-19 11:44:00+00:00
  edited: false
  hidden: false
  id: 643fe210dbd88206a833f2a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-04-19T13:05:23.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>That is nice. I use Nvidia A10G single GPU. May be the <code>pipeline</code>
          wrapper issue? I''ll just use model.generate() and try... but the issue
          is I''ll lose <code>return_full_text</code> argument which is handy in <code>pipeline</code></p>

          '
        raw: That is nice. I use Nvidia A10G single GPU. May be the `pipeline` wrapper
          issue? I'll just use model.generate() and try... but the issue is I'll lose
          `return_full_text` argument which is handy in `pipeline`
        updatedAt: '2023-04-19T13:05:23.365Z'
      numEdits: 0
      reactions: []
    id: 643fe7132113f7dfcb4c5b41
    type: comment
  author: gsaivinay
  content: That is nice. I use Nvidia A10G single GPU. May be the `pipeline` wrapper
    issue? I'll just use model.generate() and try... but the issue is I'll lose `return_full_text`
    argument which is handy in `pipeline`
  created_at: 2023-04-19 12:05:23+00:00
  edited: false
  hidden: false
  id: 643fe7132113f7dfcb4c5b41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-04-19T19:58:28.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>I dropped transformers, and used <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a></p>

          <p>This is super fast and support streaming as well.</p>

          '
        raw: 'I dropped transformers, and used https://github.com/huggingface/text-generation-inference


          This is super fast and support streaming as well.'
        updatedAt: '2023-04-19T19:58:28.899Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PierrePeng
    id: 644047e42113f7dfcb566282
    type: comment
  author: gsaivinay
  content: 'I dropped transformers, and used https://github.com/huggingface/text-generation-inference


    This is super fast and support streaming as well.'
  created_at: 2023-04-19 18:58:28+00:00
  edited: false
  hidden: false
  id: 644047e42113f7dfcb566282
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
      fullname: qc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captainst
      type: user
    createdAt: '2023-04-20T01:22:20.000Z'
    data:
      edited: false
      editors:
      - captainst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
          fullname: qc
          isHf: false
          isPro: false
          name: captainst
          type: user
        html: '<p>@sonatasv Cool. I am not sure if text-generation-inference lib supports
          8-bit quantization. Are you using 8-bit ?</p>

          '
        raw: '@sonatasv Cool. I am not sure if text-generation-inference lib supports
          8-bit quantization. Are you using 8-bit ?'
        updatedAt: '2023-04-20T01:22:20.960Z'
      numEdits: 0
      reactions: []
    id: 644093ccd4229e14aea45146
    type: comment
  author: captainst
  content: '@sonatasv Cool. I am not sure if text-generation-inference lib supports
    8-bit quantization. Are you using 8-bit ?'
  created_at: 2023-04-20 00:22:20+00:00
  edited: false
  hidden: false
  id: 644093ccd4229e14aea45146
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-04-20T07:55:11.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>yes, can be enabled with  --quantize argument</p>

          '
        raw: yes, can be enabled with  --quantize argument
        updatedAt: '2023-04-20T07:55:11.307Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - captainst
    id: 6440efdfe46e14ed5580f784
    type: comment
  author: gsaivinay
  content: yes, can be enabled with  --quantize argument
  created_at: 2023-04-20 06:55:11+00:00
  edited: false
  hidden: false
  id: 6440efdfe46e14ed5580f784
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f76876fa8db2a29352b87dc0b1f85d81.svg
      fullname: Ankit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: banank1989
      type: user
    createdAt: '2023-06-27T04:12:12.000Z'
    data:
      edited: false
      editors:
      - banank1989
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8935103416442871
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f76876fa8db2a29352b87dc0b1f85d81.svg
          fullname: Ankit
          isHf: false
          isPro: false
          name: banank1989
          type: user
        html: '<p>Is this totally free or do we need huggingface key in this?</p>

          '
        raw: 'Is this totally free or do we need huggingface key in this?

          '
        updatedAt: '2023-06-27T04:12:12.056Z'
      numEdits: 0
      reactions: []
    id: 649a619cbd3275f90dfdc24d
    type: comment
  author: banank1989
  content: 'Is this totally free or do we need huggingface key in this?

    '
  created_at: 2023-06-27 03:12:12+00:00
  edited: false
  hidden: false
  id: 649a619cbd3275f90dfdc24d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5
repo_type: model
status: open
target_branch: null
title: Slow inference time for new version of transformers
