!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Guilherme34
conflicting_files: null
created_at: 2023-04-15 10:35:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/154dbc6372a4e87f69fcf78d46d6a9d2.svg
      fullname: Guilherme Keller2
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Guilherme34
      type: user
    createdAt: '2023-04-15T11:35:08.000Z'
    data:
      edited: false
      editors:
      - Guilherme34
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/154dbc6372a4e87f69fcf78d46d6a9d2.svg
          fullname: Guilherme Keller2
          isHf: false
          isPro: false
          name: Guilherme34
          type: user
        html: '<p>Just how to run</p>

          '
        raw: Just how to run
        updatedAt: '2023-04-15T11:35:08.048Z'
      numEdits: 0
      reactions: []
    id: 643a8bec0e5495afdef84aa6
    type: comment
  author: Guilherme34
  content: Just how to run
  created_at: 2023-04-15 10:35:08+00:00
  edited: false
  hidden: false
  id: 643a8bec0e5495afdef84aa6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11e8bf0c352f64c3e1e93c1f02c805c2.svg
      fullname: Emma Tew
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: EmmaTew
      type: user
    createdAt: '2023-04-15T20:27:48.000Z'
    data:
      edited: true
      editors:
      - EmmaTew
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11e8bf0c352f64c3e1e93c1f02c805c2.svg
          fullname: Emma Tew
          isHf: false
          isPro: false
          name: EmmaTew
          type: user
        html: '<p>you can use this <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a></p>

          <p>in the download part just use "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"
          (there is a spot in the UI now to download models, load/reload/unload and
          apply any LoRA etc.)</p>

          <p>on my pc it takes ~13220MiB VRAM, loaded in 8bit generates around 5-10
          tokens/sec on consumer GPU that has 16Gb VRAM.</p>

          '
        raw: 'you can use this https://github.com/oobabooga/text-generation-webui


          in the download part just use "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"
          (there is a spot in the UI now to download models, load/reload/unload and
          apply any LoRA etc.)


          on my pc it takes ~13220MiB VRAM, loaded in 8bit generates around 5-10 tokens/sec
          on consumer GPU that has 16Gb VRAM.'
        updatedAt: '2023-04-15T22:50:03.941Z'
      numEdits: 3
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - cojosef96
        - jeffwadsworth
        - jeunjetta
    id: 643b08c45a3b913ad46d62ce
    type: comment
  author: EmmaTew
  content: 'you can use this https://github.com/oobabooga/text-generation-webui


    in the download part just use "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"
    (there is a spot in the UI now to download models, load/reload/unload and apply
    any LoRA etc.)


    on my pc it takes ~13220MiB VRAM, loaded in 8bit generates around 5-10 tokens/sec
    on consumer GPU that has 16Gb VRAM.'
  created_at: 2023-04-15 19:27:48+00:00
  edited: true
  hidden: false
  id: 643b08c45a3b913ad46d62ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/db1b09e9d490556f7279ff0b3d8032a3.svg
      fullname: Yossi Cohen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cojosef96
      type: user
    createdAt: '2023-04-15T21:00:41.000Z'
    data:
      edited: true
      editors:
      - cojosef96
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/db1b09e9d490556f7279ff0b3d8032a3.svg
          fullname: Yossi Cohen
          isHf: false
          isPro: false
          name: cojosef96
          type: user
        html: "<p>Hey,<br>If you want to run this with hugfging face and transformers\
          \ api you can use the model this way:,</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> GPTNeoXForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          </span>)\n<span class=\"hljs-comment\"># Note: This model takes 15 GB of\
          \ Vram when loaded in 8bit</span>\nmodel = GPTNeoXForCausalLM.from_pretrained(\n\
          \  <span class=\"hljs-string\">\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          </span>,device_map=<span class=\"hljs-string\">\"auto\"</span>, load_in_8bit=<span\
          \ class=\"hljs-literal\">True</span>)\n<span class=\"hljs-comment\"># for\
          \ cpu ver</span>\n<span class=\"hljs-comment\"># model = AutoModelForCausalLM.from_pretrained(\"\
          OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\", torch_dtype=torch.bfloat16)</span>\n\
          message = <span class=\"hljs-string\">\"&lt;|prompter|&gt;What is a meme,\
          \ and what's the history behind this word?&lt;|endoftext|&gt;&lt;|assistant|&gt;\"\
          </span>\ninputs = tokenizer(message, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>).to(model.device)\ntokens = model.generate(**inputs,  max_new_tokens=<span\
          \ class=\"hljs-number\">1000</span>, do_sample=<span class=\"hljs-literal\"\
          >True</span>, temperature=<span class=\"hljs-number\">0.8</span>)\ntokenizer.decode(tokens[<span\
          \ class=\"hljs-number\">0</span>])\n</code></pre>\n<p>output:<br>'&lt;|prompter|&gt;What\
          \ is a meme, and what's the history behind this word?&lt;|endoftext|&gt;&lt;|assistant|&gt;A\
          \ meme is a cultural idea, behavior, or style that spreads from person to\
          \ person within a society. The word \"meme\" was first used by Richard Dawkins\
          \ in his 1976 book \"The Selfish Gene.\" He defined a meme as a unit of\
          \ cultural information that is transmitted from one individual to another\
          \ through language, gestures, or other means.\\n\\nThe history of the word\
          \ \"meme\" dates back to the late 1960s, when Richard Dawkins was working\
          \ on his book \"The Selfish Gene.\" He was fascinated by the way cultural\
          \ information spreads and evolved within a society, and he began to use\
          \ the term \"meme\" to describe these ideas.\\n\\nSince then, the word \"\
          meme\" has become widely used in the field of cultural studies and has been\
          \ adopted by many different academic fields and disciplines, including linguistics,\
          \ anthropology, and psychology. Today, the term \"meme\" is used to refer\
          \ to any cultural idea, behavior, or style that is spread from person to\
          \ person within a society.&lt;|endoftext|&gt;'</p>\n"
        raw: "Hey, \nIf you want to run this with hugfging face and transformers api\
          \ you can use the model this way:,\n```python\nfrom transformers import\
          \ GPTNeoXForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"\
          OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\")\n# Note: This model takes\
          \ 15 GB of Vram when loaded in 8bit\nmodel = GPTNeoXForCausalLM.from_pretrained(\n\
          \  \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\",device_map=\"auto\"\
          , load_in_8bit=True)\n# for cpu ver\n# model = AutoModelForCausalLM.from_pretrained(\"\
          OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\", torch_dtype=torch.bfloat16)\n\
          message = \"<|prompter|>What is a meme, and what's the history behind this\
          \ word?<|endoftext|><|assistant|>\"\ninputs = tokenizer(message, return_tensors=\"\
          pt\").to(model.device)\ntokens = model.generate(**inputs,  max_new_tokens=1000,\
          \ do_sample=True, temperature=0.8)\ntokenizer.decode(tokens[0])\n```\n\n\
          output:\n'<|prompter|>What is a meme, and what\\'s the history behind this\
          \ word?<|endoftext|><|assistant|>A meme is a cultural idea, behavior, or\
          \ style that spreads from person to person within a society. The word \"\
          meme\" was first used by Richard Dawkins in his 1976 book \"The Selfish\
          \ Gene.\" He defined a meme as a unit of cultural information that is transmitted\
          \ from one individual to another through language, gestures, or other means.\\\
          n\\nThe history of the word \"meme\" dates back to the late 1960s, when\
          \ Richard Dawkins was working on his book \"The Selfish Gene.\" He was fascinated\
          \ by the way cultural information spreads and evolved within a society,\
          \ and he began to use the term \"meme\" to describe these ideas.\\n\\nSince\
          \ then, the word \"meme\" has become widely used in the field of cultural\
          \ studies and has been adopted by many different academic fields and disciplines,\
          \ including linguistics, anthropology, and psychology. Today, the term \"\
          meme\" is used to refer to any cultural idea, behavior, or style that is\
          \ spread from person to person within a society.<|endoftext|>'"
        updatedAt: '2023-04-15T21:01:02.185Z'
      numEdits: 1
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - BrandonH
        - Gtomberlin
        - jeunjetta
        - Q4234
        - mvetter
        - fvancesco
        - jaded0
    id: 643b1079f6ef50c310d91bb7
    type: comment
  author: cojosef96
  content: "Hey, \nIf you want to run this with hugfging face and transformers api\
    \ you can use the model this way:,\n```python\nfrom transformers import GPTNeoXForCausalLM,\
    \ AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
    )\n# Note: This model takes 15 GB of Vram when loaded in 8bit\nmodel = GPTNeoXForCausalLM.from_pretrained(\n\
    \  \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\",device_map=\"auto\", load_in_8bit=True)\n\
    # for cpu ver\n# model = AutoModelForCausalLM.from_pretrained(\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
    , torch_dtype=torch.bfloat16)\nmessage = \"<|prompter|>What is a meme, and what's\
    \ the history behind this word?<|endoftext|><|assistant|>\"\ninputs = tokenizer(message,\
    \ return_tensors=\"pt\").to(model.device)\ntokens = model.generate(**inputs, \
    \ max_new_tokens=1000, do_sample=True, temperature=0.8)\ntokenizer.decode(tokens[0])\n\
    ```\n\noutput:\n'<|prompter|>What is a meme, and what\\'s the history behind this\
    \ word?<|endoftext|><|assistant|>A meme is a cultural idea, behavior, or style\
    \ that spreads from person to person within a society. The word \"meme\" was first\
    \ used by Richard Dawkins in his 1976 book \"The Selfish Gene.\" He defined a\
    \ meme as a unit of cultural information that is transmitted from one individual\
    \ to another through language, gestures, or other means.\\n\\nThe history of the\
    \ word \"meme\" dates back to the late 1960s, when Richard Dawkins was working\
    \ on his book \"The Selfish Gene.\" He was fascinated by the way cultural information\
    \ spreads and evolved within a society, and he began to use the term \"meme\"\
    \ to describe these ideas.\\n\\nSince then, the word \"meme\" has become widely\
    \ used in the field of cultural studies and has been adopted by many different\
    \ academic fields and disciplines, including linguistics, anthropology, and psychology.\
    \ Today, the term \"meme\" is used to refer to any cultural idea, behavior, or\
    \ style that is spread from person to person within a society.<|endoftext|>'"
  created_at: 2023-04-15 20:00:41+00:00
  edited: true
  hidden: false
  id: 643b1079f6ef50c310d91bb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
      fullname: Jeff Wadsworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeffwadsworth
      type: user
    createdAt: '2023-04-15T21:48:23.000Z'
    data:
      edited: true
      editors:
      - jeffwadsworth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
          fullname: Jeff Wadsworth
          isHf: false
          isPro: false
          name: jeffwadsworth
          type: user
        html: '<p>If using the oobabooga setup,  just go to the directory "oobabooga-windows\text-generation-webui\models"
          and create a folder named "open-assistant" or whatever.  Copy all the files
          from the files tab on this page to that folder.  3 large bin files and lots
          of jsons, etc.  When you run it, you should see it on the list of options.  Note,
          you need a GPU with a lot of VRAM.  I can run a few queries with 12 GB''s,
          but then it drops out due to memory.  So, CPU is probably the way to go
          if you want to play hardcore.</p>

          '
        raw: If using the oobabooga setup,  just go to the directory "oobabooga-windows\text-generation-webui\models"
          and create a folder named "open-assistant" or whatever.  Copy all the files
          from the files tab on this page to that folder.  3 large bin files and lots
          of jsons, etc.  When you run it, you should see it on the list of options.  Note,
          you need a GPU with a lot of VRAM.  I can run a few queries with 12 GB's,
          but then it drops out due to memory.  So, CPU is probably the way to go
          if you want to play hardcore.
        updatedAt: '2023-04-15T21:49:04.783Z'
      numEdits: 1
      reactions: []
    id: 643b1ba7101fbcb9402ffa39
    type: comment
  author: jeffwadsworth
  content: If using the oobabooga setup,  just go to the directory "oobabooga-windows\text-generation-webui\models"
    and create a folder named "open-assistant" or whatever.  Copy all the files from
    the files tab on this page to that folder.  3 large bin files and lots of jsons,
    etc.  When you run it, you should see it on the list of options.  Note, you need
    a GPU with a lot of VRAM.  I can run a few queries with 12 GB's, but then it drops
    out due to memory.  So, CPU is probably the way to go if you want to play hardcore.
  created_at: 2023-04-15 20:48:23+00:00
  edited: true
  hidden: false
  id: 643b1ba7101fbcb9402ffa39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670558389645-63058c7f99870e13d3e06170.png?w=200&h=200&f=face
      fullname: Kage Rage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kagerage
      type: user
    createdAt: '2023-04-15T22:20:47.000Z'
    data:
      edited: false
      editors:
      - Kagerage
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670558389645-63058c7f99870e13d3e06170.png?w=200&h=200&f=face
          fullname: Kage Rage
          isHf: false
          isPro: false
          name: Kagerage
          type: user
        html: '<p>Someone needs to make a 4-bit version of this model. I''ve been
          using various 4-bit 12B/13B models on Colab''s free tier with zero issues,
          and I''ve even gotten <a href="https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g">this
          model</a> to run just fine on there.</p>

          '
        raw: Someone needs to make a 4-bit version of this model. I've been using
          various 4-bit 12B/13B models on Colab's free tier with zero issues, and
          I've even gotten [this model](https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g)
          to run just fine on there.
        updatedAt: '2023-04-15T22:20:47.731Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Logophoman
        - jeunjetta
        - FgRegistr
    id: 643b233f1f162723357270bb
    type: comment
  author: Kagerage
  content: Someone needs to make a 4-bit version of this model. I've been using various
    4-bit 12B/13B models on Colab's free tier with zero issues, and I've even gotten
    [this model](https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g) to run just
    fine on there.
  created_at: 2023-04-15 21:20:47+00:00
  edited: false
  hidden: false
  id: 643b233f1f162723357270bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
      fullname: qc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captainst
      type: user
    createdAt: '2023-04-18T07:51:03.000Z'
    data:
      edited: false
      editors:
      - captainst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
          fullname: qc
          isHf: false
          isPro: false
          name: captainst
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cojosef96&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cojosef96\">@<span class=\"\
          underline\">cojosef96</span></a></span>\n\n\t</span></span> Thank you for\
          \ the sample code. I tried your code for CPU, but \"model.generate\" seems\
          \ to run endlessly. I have 6 core CPU, it seems that the process is using\
          \ only 1 of them. Do you have any clue?</p>\n"
        raw: '@cojosef96 Thank you for the sample code. I tried your code for CPU,
          but "model.generate" seems to run endlessly. I have 6 core CPU, it seems
          that the process is using only 1 of them. Do you have any clue?'
        updatedAt: '2023-04-18T07:51:03.455Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Neronuser
    id: 643e4be75fc641a0bfc53090
    type: comment
  author: captainst
  content: '@cojosef96 Thank you for the sample code. I tried your code for CPU, but
    "model.generate" seems to run endlessly. I have 6 core CPU, it seems that the
    process is using only 1 of them. Do you have any clue?'
  created_at: 2023-04-18 06:51:03+00:00
  edited: false
  hidden: false
  id: 643e4be75fc641a0bfc53090
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2023-04-18T12:12:08.000Z'
    data:
      edited: true
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: "<p>We turned the Inference API on for this model. You can now use the\
          \ <a rel=\"nofollow\" href=\"https://pypi.org/project/text-generation/\"\
          >text-generation</a> client to prompt this model:</p>\n<pre><code class=\"\
          language-shell\">pip install --upgrade text-generation==0.5.0\n</code></pre>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ text_generation <span class=\"hljs-keyword\">import</span> InferenceAPIClient\n\
          \nclient = InferenceAPIClient(<span class=\"hljs-string\">\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          </span>)\n\ncomplete_answer = <span class=\"hljs-string\">\"\"</span>\n\
          <span class=\"hljs-keyword\">for</span> response <span class=\"hljs-keyword\"\
          >in</span> client.generate_stream(<span class=\"hljs-string\">\"&lt;|prompter|&gt;What\
          \ is a meme, and what's the history behind this word?&lt;|endoftext|&gt;&lt;|assistant|&gt;\"\
          </span>):\n    <span class=\"hljs-built_in\">print</span>(response.token)\n\
          \    complete_answer += response.token.text\n\n<span class=\"hljs-built_in\"\
          >print</span>(complete_answer)\n</code></pre>\n<p>You can also run this\
          \ model locally with <a rel=\"nofollow\" href=\"https://github.com/huggingface/text-generation-inference\"\
          >text-generation-inference</a>.</p>\n<p>To run on a GPU with enough VRAM:</p>\n\
          <pre><code class=\"language-shell\"><span class=\"hljs-meta prompt_\">#\
          \ </span><span class=\"language-bash\">Use a volume to share weights between\
          \ independant docker runs</span>\ndocker run --gpus \"device=0\" -p 8080:80\
          \ -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:sha-7a1ba58\
          \  --model-id OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n</code></pre>\n\
          <p>The first run whill take a while as it needs to download the model.</p>\n\
          <p>or to run on two smaller GPUs: </p>\n<pre><code class=\"language-shell\"\
          ><span class=\"hljs-meta prompt_\">#</span><span class=\"language-bash\"\
          >&nbsp;Use a volume to share weights between independant docker runs</span>\n\
          docker run --gpus \"device=0,1\" -p 8080:80 -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:sha-7a1ba58\
          \  --model-id OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5 --num-shard\
          \ 2\n</code></pre>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> text_generation <span class=\"hljs-keyword\"\
          >import</span> Client\n\nclient = Client(<span class=\"hljs-string\">\"\
          http://localhost:8080\"</span>)\n\ncomplete_answer = <span class=\"hljs-string\"\
          >\"\"</span>\n<span class=\"hljs-keyword\">for</span> response <span class=\"\
          hljs-keyword\">in</span> client.generate_stream(<span class=\"hljs-string\"\
          >\"&lt;|prompter|&gt;What is a meme, and what's the history behind this\
          \ word?&lt;|endoftext|&gt;&lt;|assistant|&gt;\"</span>):\n    <span class=\"\
          hljs-built_in\">print</span>(response.token)\n    complete_answer += response.token.text\n\
          \n<span class=\"hljs-built_in\">print</span>(complete_answer)\n</code></pre>\n"
        raw: "We turned the Inference API on for this model. You can now use the [text-generation](https://pypi.org/project/text-generation/)\
          \ client to prompt this model:\n\n```shell\npip install --upgrade text-generation==0.5.0\n\
          ```\n\n```python\nfrom text_generation import InferenceAPIClient\n\nclient\
          \ = InferenceAPIClient(\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          )\n\ncomplete_answer = \"\"\nfor response in client.generate_stream(\"<|prompter|>What\
          \ is a meme, and what's the history behind this word?<|endoftext|><|assistant|>\"\
          ):\n    print(response.token)\n    complete_answer += response.token.text\n\
          \nprint(complete_answer)\n```\n\nYou can also run this model locally with\
          \ [text-generation-inference](https://github.com/huggingface/text-generation-inference).\n\
          \nTo run on a GPU with enough VRAM:\n\n```shell\n# Use a volume to share\
          \ weights between independant docker runs\ndocker run --gpus \"device=0\"\
          \ -p 8080:80 -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:sha-7a1ba58\
          \  --model-id OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n```\n\nThe\
          \ first run whill take a while as it needs to download the model.\n\nor\
          \ to run on two smaller GPUs: \n\n```shell\n#\_Use a volume to share weights\
          \ between independant docker runs\ndocker run --gpus \"device=0,1\" -p 8080:80\
          \ -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:sha-7a1ba58\
          \  --model-id OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5 --num-shard\
          \ 2\n```\n\n```python\nfrom text_generation import Client\n\nclient = Client(\"\
          http://localhost:8080\")\n\ncomplete_answer = \"\"\nfor response in client.generate_stream(\"\
          <|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>\"\
          ):\n    print(response.token)\n    complete_answer += response.token.text\n\
          \nprint(complete_answer)\n```"
        updatedAt: '2023-04-18T12:13:50.064Z'
      numEdits: 1
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - Guilherme34
        - FgRegistr
        - deepakkaura26
        - shaybc
      - count: 2
        reaction: "\U0001F44D"
        users:
        - captainst
        - shaybc
    id: 643e891852885f3d3f54743e
    type: comment
  author: olivierdehaene
  content: "We turned the Inference API on for this model. You can now use the [text-generation](https://pypi.org/project/text-generation/)\
    \ client to prompt this model:\n\n```shell\npip install --upgrade text-generation==0.5.0\n\
    ```\n\n```python\nfrom text_generation import InferenceAPIClient\n\nclient = InferenceAPIClient(\"\
    OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\")\n\ncomplete_answer = \"\"\n\
    for response in client.generate_stream(\"<|prompter|>What is a meme, and what's\
    \ the history behind this word?<|endoftext|><|assistant|>\"):\n    print(response.token)\n\
    \    complete_answer += response.token.text\n\nprint(complete_answer)\n```\n\n\
    You can also run this model locally with [text-generation-inference](https://github.com/huggingface/text-generation-inference).\n\
    \nTo run on a GPU with enough VRAM:\n\n```shell\n# Use a volume to share weights\
    \ between independant docker runs\ndocker run --gpus \"device=0\" -p 8080:80 -v\
    \ $PWD/data:/data ghcr.io/huggingface/text-generation-inference:sha-7a1ba58  --model-id\
    \ OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n```\n\nThe first run whill take\
    \ a while as it needs to download the model.\n\nor to run on two smaller GPUs:\
    \ \n\n```shell\n#\_Use a volume to share weights between independant docker runs\n\
    docker run --gpus \"device=0,1\" -p 8080:80 -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:sha-7a1ba58\
    \  --model-id OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5 --num-shard 2\n```\n\
    \n```python\nfrom text_generation import Client\n\nclient = Client(\"http://localhost:8080\"\
    )\n\ncomplete_answer = \"\"\nfor response in client.generate_stream(\"<|prompter|>What\
    \ is a meme, and what's the history behind this word?<|endoftext|><|assistant|>\"\
    ):\n    print(response.token)\n    complete_answer += response.token.text\n\n\
    print(complete_answer)\n```"
  created_at: 2023-04-18 11:12:08+00:00
  edited: true
  hidden: false
  id: 643e891852885f3d3f54743e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
      fullname: qc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captainst
      type: user
    createdAt: '2023-04-18T15:00:30.000Z'
    data:
      edited: true
      editors:
      - captainst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
          fullname: qc
          isHf: false
          isPro: false
          name: captainst
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>\
          \ Thank you very much. I have tried the API. It generate a reply but seems\
          \ to have a limit of number of output tokens:<br><b>A meme is a cultural\
          \ idea, behavior, or style that spreads from person to person within a</b><br>Is\
          \ that normal ?</p>\n"
        raw: '@olivierdehaene Thank you very much. I have tried the API. It generate
          a reply but seems to have a limit of number of output tokens:

          <b>A meme is a cultural idea, behavior, or style that spreads from person
          to person within a</b>

          Is that normal ?'
        updatedAt: '2023-04-18T15:00:41.198Z'
      numEdits: 1
      reactions: []
    id: 643eb08ee868b8d92824fa3b
    type: comment
  author: captainst
  content: '@olivierdehaene Thank you very much. I have tried the API. It generate
    a reply but seems to have a limit of number of output tokens:

    <b>A meme is a cultural idea, behavior, or style that spreads from person to person
    within a</b>

    Is that normal ?'
  created_at: 2023-04-18 14:00:30+00:00
  edited: true
  hidden: false
  id: 643eb08ee868b8d92824fa3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2023-04-18T17:56:14.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: '<p>You can modify the parameters easily (in this case, it''s the <code>max_new_tokens</code>
          parameter). Check the signature of the <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/client.py#L61-L76"><code>generate</code></a>
          or <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/client.py#L148-L162"><code>generate_stream</code></a>
          functions.</p>

          '
        raw: You can modify the parameters easily (in this case, it's the `max_new_tokens`
          parameter). Check the signature of the [`generate`](https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/client.py#L61-L76)
          or [`generate_stream`](https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/client.py#L148-L162)
          functions.
        updatedAt: '2023-04-18T17:56:14.716Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - captainst
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - captainst
    id: 643ed9be4c5309c24495ff61
    type: comment
  author: olivierdehaene
  content: You can modify the parameters easily (in this case, it's the `max_new_tokens`
    parameter). Check the signature of the [`generate`](https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/client.py#L61-L76)
    or [`generate_stream`](https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/client.py#L148-L162)
    functions.
  created_at: 2023-04-18 16:56:14+00:00
  edited: false
  hidden: false
  id: 643ed9be4c5309c24495ff61
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-04-19T03:38:34.000Z'
    data:
      edited: false
      editors:
      - pevogam
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
          fullname: Plamen Dimitrov
          isHf: false
          isPro: false
          name: pevogam
          type: user
        html: "<p>Very helpful, thanks a lot <span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>!</p>\n"
        raw: Very helpful, thanks a lot @olivierdehaene!
        updatedAt: '2023-04-19T03:38:34.324Z'
      numEdits: 0
      reactions: []
    id: 643f623a8d9a6e3bba3e539b
    type: comment
  author: pevogam
  content: Very helpful, thanks a lot @olivierdehaene!
  created_at: 2023-04-19 02:38:34+00:00
  edited: false
  hidden: false
  id: 643f623a8d9a6e3bba3e539b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
      fullname: qc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captainst
      type: user
    createdAt: '2023-04-19T06:21:32.000Z'
    data:
      edited: false
      editors:
      - captainst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
          fullname: qc
          isHf: false
          isPro: false
          name: captainst
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>\
          \ Consice &amp; Comprehensive, Thank you!</p>\n"
        raw: '@olivierdehaene Consice & Comprehensive, Thank you!'
        updatedAt: '2023-04-19T06:21:32.444Z'
      numEdits: 0
      reactions: []
    id: 643f886c2ab12e441fa65441
    type: comment
  author: captainst
  content: '@olivierdehaene Consice & Comprehensive, Thank you!'
  created_at: 2023-04-19 05:21:32+00:00
  edited: false
  hidden: false
  id: 643f886c2ab12e441fa65441
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
      fullname: qc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captainst
      type: user
    createdAt: '2023-04-19T08:18:10.000Z'
    data:
      edited: false
      editors:
      - captainst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
          fullname: qc
          isHf: false
          isPro: false
          name: captainst
          type: user
        html: "<p>I have setup a workstation with 2 x RTX 3060 (12GB for each). I\
          \ am using the latest version of transformers-4.28.1 from huggingface.<br>The\
          \ example from <span data-props=\"{&quot;user&quot;:&quot;cojosef96&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/cojosef96\"\
          >@<span class=\"underline\">cojosef96</span></a></span>\n\n\t</span></span>\
          \ works correctly, and with <b>device_map=\"auto\"</b> , the model is evenly\
          \ distributed to 2 GPUs.<br>The 1st call to <b>model.generate()</b> function\
          \ took over 2 minutes to finish. The successive calls took merely a couple\
          \ of seconds.</p>\n<p>(maybe I have some problems with the 2nd PCIE slot\
          \ on motherboard, since during the 1st run, the 2nd GPU utilization is only\
          \ around 15%~20%)</p>\n"
        raw: 'I have setup a workstation with 2 x RTX 3060 (12GB for each). I am using
          the latest version of transformers-4.28.1 from huggingface.

          The example from @cojosef96 works correctly, and with <b>device_map="auto"</b>
          , the model is evenly distributed to 2 GPUs.

          The 1st call to <b>model.generate()</b> function took over 2 minutes to
          finish. The successive calls took merely a couple of seconds.


          (maybe I have some problems with the 2nd PCIE slot on motherboard, since
          during the 1st run, the 2nd GPU utilization is only around 15%~20%)'
        updatedAt: '2023-04-19T08:18:10.930Z'
      numEdits: 0
      reactions: []
    id: 643fa3c2c54114df11c621b3
    type: comment
  author: captainst
  content: 'I have setup a workstation with 2 x RTX 3060 (12GB for each). I am using
    the latest version of transformers-4.28.1 from huggingface.

    The example from @cojosef96 works correctly, and with <b>device_map="auto"</b>
    , the model is evenly distributed to 2 GPUs.

    The 1st call to <b>model.generate()</b> function took over 2 minutes to finish.
    The successive calls took merely a couple of seconds.


    (maybe I have some problems with the 2nd PCIE slot on motherboard, since during
    the 1st run, the 2nd GPU utilization is only around 15%~20%)'
  created_at: 2023-04-19 07:18:10+00:00
  edited: false
  hidden: false
  id: 643fa3c2c54114df11c621b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/740054f4adeeeaa2d8df171829f8b4a2.svg
      fullname: Himanshu Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: himanshu3344
      type: user
    createdAt: '2023-04-24T19:24:51.000Z'
    data:
      edited: false
      editors:
      - himanshu3344
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/740054f4adeeeaa2d8df171829f8b4a2.svg
          fullname: Himanshu Sharma
          isHf: false
          isPro: false
          name: himanshu3344
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>\
          \ Thanks a lot for sharing. I am getting the following error while deploying\
          \ the model in a g5.4xlarge instance on aws. I am able to load the model\
          \ and run an inference in an jupyter notebook, but the inference server\
          \ is not starting.<br><code>torch.cuda.OutOfMemoryError: CUDA out of memory.\
          \ Tried to allocate 150.00 MiB (GPU 0; 22.04 GiB total capacity; 20.99 GiB\
          \ already allocated; 91.19 MiB free; 20.99 GiB reserved in total by PyTorch)\
          \ If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.</code><br>Can someone please guide me setting\
          \ this up? Do I need a bigger instance?</p>\n"
        raw: "@olivierdehaene Thanks a lot for sharing. I am getting the following\
          \ error while deploying the model in a g5.4xlarge instance on aws. I am\
          \ able to load the model and run an inference in an jupyter notebook, but\
          \ the inference server is not starting. \n```torch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 22.04 GiB total\
          \ capacity; 20.99 GiB already allocated; 91.19 MiB free; 20.99 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.```\nCan someone please guide\
          \ me setting this up? Do I need a bigger instance?"
        updatedAt: '2023-04-24T19:24:51.007Z'
      numEdits: 0
      reactions: []
    id: 6446d783f9dc06bea2a8284b
    type: comment
  author: himanshu3344
  content: "@olivierdehaene Thanks a lot for sharing. I am getting the following error\
    \ while deploying the model in a g5.4xlarge instance on aws. I am able to load\
    \ the model and run an inference in an jupyter notebook, but the inference server\
    \ is not starting. \n```torch.cuda.OutOfMemoryError: CUDA out of memory. Tried\
    \ to allocate 150.00 MiB (GPU 0; 22.04 GiB total capacity; 20.99 GiB already allocated;\
    \ 91.19 MiB free; 20.99 GiB reserved in total by PyTorch) If reserved memory is\
    \ >> allocated memory try setting max_split_size_mb to avoid fragmentation.```\n\
    Can someone please guide me setting this up? Do I need a bigger instance?"
  created_at: 2023-04-24 18:24:51+00:00
  edited: false
  hidden: false
  id: 6446d783f9dc06bea2a8284b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
      fullname: qc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captainst
      type: user
    createdAt: '2023-04-25T05:26:05.000Z'
    data:
      edited: false
      editors:
      - captainst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
          fullname: qc
          isHf: false
          isPro: false
          name: captainst
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;himanshu3344&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/himanshu3344\"\
          >@<span class=\"underline\">himanshu3344</span></a></span>\n\n\t</span></span>\
          \ 22.04GiB total is not enough for the FP16 version. You can try the 8bit\
          \ quantized version, which consumes &lt; 15GiB VRAM</p>\n"
        raw: '@himanshu3344 22.04GiB total is not enough for the FP16 version. You
          can try the 8bit quantized version, which consumes < 15GiB VRAM'
        updatedAt: '2023-04-25T05:26:05.831Z'
      numEdits: 0
      reactions: []
    id: 6447646da9f9c62e6cd85690
    type: comment
  author: captainst
  content: '@himanshu3344 22.04GiB total is not enough for the FP16 version. You can
    try the 8bit quantized version, which consumes < 15GiB VRAM'
  created_at: 2023-04-25 04:26:05+00:00
  edited: false
  hidden: false
  id: 6447646da9f9c62e6cd85690
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c19364aeab253825f215d05de25794f7.svg
      fullname: Shrayani Mondal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shrayani
      type: user
    createdAt: '2023-04-26T01:19:00.000Z'
    data:
      edited: false
      editors:
      - shrayani
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c19364aeab253825f215d05de25794f7.svg
          fullname: Shrayani Mondal
          isHf: false
          isPro: false
          name: shrayani
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>\
          \ Thanks a lot for sharing your code. I am trying to use the text_generation\
          \ InferenceAPIClient on a text column of a python dataframe with a suitable\
          \ prompt. But it seems to have a rate limit and i have to wait atleast an\
          \ hour or so before I can use the inference api again, once this rate limit\
          \ is surpassed. Is there a way to bypass it? The dataframe is quite large\
          \ but the task at hand is a one time operation as I will be writing the\
          \ generated texts in a separate file. All suggestions are welcome</p>\n"
        raw: '@olivierdehaene Thanks a lot for sharing your code. I am trying to use
          the text_generation InferenceAPIClient on a text column of a python dataframe
          with a suitable prompt. But it seems to have a rate limit and i have to
          wait atleast an hour or so before I can use the inference api again, once
          this rate limit is surpassed. Is there a way to bypass it? The dataframe
          is quite large but the task at hand is a one time operation as I will be
          writing the generated texts in a separate file. All suggestions are welcome'
        updatedAt: '2023-04-26T01:19:00.492Z'
      numEdits: 0
      reactions: []
    id: 64487c043411a0902bc75bdf
    type: comment
  author: shrayani
  content: '@olivierdehaene Thanks a lot for sharing your code. I am trying to use
    the text_generation InferenceAPIClient on a text column of a python dataframe
    with a suitable prompt. But it seems to have a rate limit and i have to wait atleast
    an hour or so before I can use the inference api again, once this rate limit is
    surpassed. Is there a way to bypass it? The dataframe is quite large but the task
    at hand is a one time operation as I will be writing the generated texts in a
    separate file. All suggestions are welcome'
  created_at: 2023-04-26 00:19:00+00:00
  edited: false
  hidden: false
  id: 64487c043411a0902bc75bdf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2023-04-26T07:43:57.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: '<p>You can pay a pro subscription to decrease the rate limit.</p>

          '
        raw: You can pay a pro subscription to decrease the rate limit.
        updatedAt: '2023-04-26T07:43:57.259Z'
      numEdits: 0
      reactions: []
    id: 6448d63dd5d86def91c7b2e1
    type: comment
  author: olivierdehaene
  content: You can pay a pro subscription to decrease the rate limit.
  created_at: 2023-04-26 06:43:57+00:00
  edited: false
  hidden: false
  id: 6448d63dd5d86def91c7b2e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/740054f4adeeeaa2d8df171829f8b4a2.svg
      fullname: Himanshu Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: himanshu3344
      type: user
    createdAt: '2023-04-28T22:38:19.000Z'
    data:
      edited: false
      editors:
      - himanshu3344
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/740054f4adeeeaa2d8df171829f8b4a2.svg
          fullname: Himanshu Sharma
          isHf: false
          isPro: false
          name: himanshu3344
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;captainst&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/captainst\"\
          >@<span class=\"underline\">captainst</span></a></span>\n\n\t</span></span>.\
          \ I was able to deploy the model on bigger instance.</p>\n"
        raw: Thanks @captainst. I was able to deploy the model on bigger instance.
        updatedAt: '2023-04-28T22:38:19.934Z'
      numEdits: 0
      reactions: []
    id: 644c4adbed08a4fdf4e5d87a
    type: comment
  author: himanshu3344
  content: Thanks @captainst. I was able to deploy the model on bigger instance.
  created_at: 2023-04-28 21:38:19+00:00
  edited: false
  hidden: false
  id: 644c4adbed08a4fdf4e5d87a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d830c33c744fcbf8eef250915d938223.svg
      fullname: David Hung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: davidhung
      type: user
    createdAt: '2023-05-15T15:02:38.000Z'
    data:
      edited: false
      editors:
      - davidhung
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d830c33c744fcbf8eef250915d938223.svg
          fullname: David Hung
          isHf: false
          isPro: false
          name: davidhung
          type: user
        html: '<p>I don''t get any results when running this model. My results are
          "&lt;|prompter|&gt;What is a meme, and what''s the history behind this word?&lt;|endoftext|&gt;&lt;|assistant|&gt;&lt;|endoftext|&gt;".
          Why is that?</p>

          '
        raw: I don't get any results when running this model. My results are "<|prompter|>What
          is a meme, and what's the history behind this word?<|endoftext|><|assistant|><|endoftext|>".
          Why is that?
        updatedAt: '2023-05-15T15:02:38.882Z'
      numEdits: 0
      reactions: []
    id: 6462498e02de01c83c0ae066
    type: comment
  author: davidhung
  content: I don't get any results when running this model. My results are "<|prompter|>What
    is a meme, and what's the history behind this word?<|endoftext|><|assistant|><|endoftext|>".
    Why is that?
  created_at: 2023-05-15 14:02:38+00:00
  edited: false
  hidden: false
  id: 6462498e02de01c83c0ae066
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84cf38f99c6808ba6b3a16c197491663.svg
      fullname: Tim Esler
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: timesler
      type: user
    createdAt: '2023-05-25T03:45:46.000Z'
    data:
      edited: false
      editors:
      - timesler
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84cf38f99c6808ba6b3a16c197491663.svg
          fullname: Tim Esler
          isHf: false
          isPro: false
          name: timesler
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;davidhung&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/davidhung\">@<span class=\"\
          underline\">davidhung</span></a></span>\n\n\t</span></span> it is giving\
          \ no output because the prompt you are using ends with <code>&lt;|endoftext|&gt;</code>,\
          \ which the model interprets as the end of its generation.<br>If you prompt\
          \ with <code>&lt;|prompter|&gt;What is a meme, and what's the history behind\
          \ this word?&lt;|endoftext|&gt;&lt;|assistant|&gt;</code>, you should get\
          \ some output.</p>\n"
        raw: '@davidhung it is giving no output because the prompt you are using ends
          with `<|endoftext|>`, which the model interprets as the end of its generation.

          If you prompt with `<|prompter|>What is a meme, and what''s the history
          behind this word?<|endoftext|><|assistant|>`, you should get some output.'
        updatedAt: '2023-05-25T03:45:46.133Z'
      numEdits: 0
      reactions: []
    id: 646ed9ea34fde71fdaa763b1
    type: comment
  author: timesler
  content: '@davidhung it is giving no output because the prompt you are using ends
    with `<|endoftext|>`, which the model interprets as the end of its generation.

    If you prompt with `<|prompter|>What is a meme, and what''s the history behind
    this word?<|endoftext|><|assistant|>`, you should get some output.'
  created_at: 2023-05-25 02:45:46+00:00
  edited: false
  hidden: false
  id: 646ed9ea34fde71fdaa763b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d830c33c744fcbf8eef250915d938223.svg
      fullname: David Hung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: davidhung
      type: user
    createdAt: '2023-05-25T18:46:51.000Z'
    data:
      edited: true
      editors:
      - davidhung
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d830c33c744fcbf8eef250915d938223.svg
          fullname: David Hung
          isHf: false
          isPro: false
          name: davidhung
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;timesler&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/timesler\">@<span class=\"\
          underline\">timesler</span></a></span>\n\n\t</span></span> I did use that\
          \ as the prompt. That was my reponse. It just added &lt;|endoftext|&gt;\
          \ at the end. It isn't working with load_in_8bit=True.</p>\n"
        raw: '@timesler I did use that as the prompt. That was my reponse. It just
          added <|endoftext|> at the end. It isn''t working with load_in_8bit=True.'
        updatedAt: '2023-05-25T19:13:55.893Z'
      numEdits: 4
      reactions: []
    id: 646fad1b150f4cab86310409
    type: comment
  author: davidhung
  content: '@timesler I did use that as the prompt. That was my reponse. It just added
    <|endoftext|> at the end. It isn''t working with load_in_8bit=True.'
  created_at: 2023-05-25 17:46:51+00:00
  edited: true
  hidden: false
  id: 646fad1b150f4cab86310409
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fe6a0fe2224c53da437d85a5f82496b8.svg
      fullname: Samy Rashed
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: samyar03
      type: user
    createdAt: '2023-05-26T20:46:58.000Z'
    data:
      edited: false
      editors:
      - samyar03
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fe6a0fe2224c53da437d85a5f82496b8.svg
          fullname: Samy Rashed
          isHf: false
          isPro: false
          name: samyar03
          type: user
        html: '<blockquote>

          <p>you can use this <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a></p>

          <p>in the download part just use "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"
          (there is a spot in the UI now to download models, load/reload/unload and
          apply any LoRA etc.)</p>

          <p>on my pc it takes ~13220MiB VRAM, loaded in 8bit generates around 5-10
          tokens/sec on consumer GPU that has 16Gb VRAM.</p>

          </blockquote>

          <p>For this one, let''s say I want to rent a GPU Cloud in order to run this,
          would you say something like a NVIDIA T4 (16 GiB in VRAM) along with a machine
          of 4 vCPUs and 26 GB memory would suffice?</p>

          '
        raw: "> you can use this https://github.com/oobabooga/text-generation-webui\n\
          > \n> in the download part just use \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          \ (there is a spot in the UI now to download models, load/reload/unload\
          \ and apply any LoRA etc.)\n> \n> on my pc it takes ~13220MiB VRAM, loaded\
          \ in 8bit generates around 5-10 tokens/sec on consumer GPU that has 16Gb\
          \ VRAM.\n\nFor this one, let's say I want to rent a GPU Cloud in order to\
          \ run this, would you say something like a NVIDIA T4 (16 GiB in VRAM) along\
          \ with a machine of 4 vCPUs and 26 GB memory would suffice?"
        updatedAt: '2023-05-26T20:46:58.822Z'
      numEdits: 0
      reactions: []
    id: 64711ac2a4fe3fa9f12d9d9f
    type: comment
  author: samyar03
  content: "> you can use this https://github.com/oobabooga/text-generation-webui\n\
    > \n> in the download part just use \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
    \ (there is a spot in the UI now to download models, load/reload/unload and apply\
    \ any LoRA etc.)\n> \n> on my pc it takes ~13220MiB VRAM, loaded in 8bit generates\
    \ around 5-10 tokens/sec on consumer GPU that has 16Gb VRAM.\n\nFor this one,\
    \ let's say I want to rent a GPU Cloud in order to run this, would you say something\
    \ like a NVIDIA T4 (16 GiB in VRAM) along with a machine of 4 vCPUs and 26 GB\
    \ memory would suffice?"
  created_at: 2023-05-26 19:46:58+00:00
  edited: false
  hidden: false
  id: 64711ac2a4fe3fa9f12d9d9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84a05ca2b27c59b32bc63e6838871242.svg
      fullname: balu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: balu548411
      type: user
    createdAt: '2023-06-04T03:14:19.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/84a05ca2b27c59b32bc63e6838871242.svg
          fullname: balu
          isHf: false
          isPro: false
          name: balu548411
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-06-04T03:14:35.557Z'
      numEdits: 0
      reactions: []
    id: 647c018b4d7c0c3fcce1917e
    type: comment
  author: balu548411
  content: This comment has been hidden
  created_at: 2023-06-04 02:14:19+00:00
  edited: true
  hidden: true
  id: 647c018b4d7c0c3fcce1917e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-06-29T19:03:14.000Z'
    data:
      edited: true
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6780248284339905
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: "<p>from text_generation import InferenceAPIClient</p>\n<p>client =\
          \ InferenceAPIClient(\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          )</p>\n<p>complete_answer = \"\"<br>for response in client.generate_stream(\"\
          &lt;|prompter|&gt;What is a meme, and what's the history behind this word?&lt;|endoftext|&gt;&lt;|assistant|&gt;\"\
          ):<br>    print(response.token)<br>    complete_answer += response.token.text</p>\n\
          <p>print(complete_answer)</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>\
          \ can you suggest me that in the above code which other models I can ? </p>\n"
        raw: "from text_generation import InferenceAPIClient\n\nclient = InferenceAPIClient(\"\
          OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\")\n\ncomplete_answer = \"\
          \"\nfor response in client.generate_stream(\"<|prompter|>What is a meme,\
          \ and what's the history behind this word?<|endoftext|><|assistant|>\"):\n\
          \    print(response.token)\n    complete_answer += response.token.text\n\
          \nprint(complete_answer)\n\n@olivierdehaene can you suggest me that in the\
          \ above code which other models I can ? "
        updatedAt: '2023-06-29T19:03:30.758Z'
      numEdits: 1
      reactions: []
    id: 649dd5723494e6e8a2b6fb0b
    type: comment
  author: deepakkaura26
  content: "from text_generation import InferenceAPIClient\n\nclient = InferenceAPIClient(\"\
    OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\")\n\ncomplete_answer = \"\"\n\
    for response in client.generate_stream(\"<|prompter|>What is a meme, and what's\
    \ the history behind this word?<|endoftext|><|assistant|>\"):\n    print(response.token)\n\
    \    complete_answer += response.token.text\n\nprint(complete_answer)\n\n@olivierdehaene\
    \ can you suggest me that in the above code which other models I can ? "
  created_at: 2023-06-29 18:03:14+00:00
  edited: true
  hidden: false
  id: 649dd5723494e6e8a2b6fb0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-06-29T19:11:00.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5349958539009094
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: "<p>from transformers import GPTNeoXForCausalLM, AutoTokenizer<br>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          )</p>\n<h1 id=\"note-this-model-takes-15-gb-of-vram-when-loaded-in-8bit\"\
          >Note: This model takes 15 GB of Vram when loaded in 8bit</h1>\n<p>model\
          \ = GPTNeoXForCausalLM.from_pretrained(<br>  \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          ,device_map=\"auto\", load_in_8bit=True)</p>\n<h1 id=\"for-cpu-ver\">for\
          \ cpu ver</h1>\n<h1 id=\"model--automodelforcausallmfrom_pretrainedopenassistantoasst-sft-4-pythia-12b-epoch-35-torch_dtypetorchbfloat16\"\
          >model = AutoModelForCausalLM.from_pretrained(\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          , torch_dtype=torch.bfloat16)</h1>\n<p>message = \"&lt;|prompter|&gt;What\
          \ is a meme, and what's the history behind this word?&lt;|endoftext|&gt;&lt;|assistant|&gt;\"\
          <br>inputs = tokenizer(message, return_tensors=\"pt\").to(model.device)<br>tokens\
          \ = model.generate(**inputs,  max_new_tokens=1000, do_sample=True, temperature=0.8)<br>tokenizer.decode(tokens[0])</p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;cojosef96&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cojosef96\">@<span class=\"\
          underline\">cojosef96</span></a></span>\n\n\t</span></span>  I have 3 questions\
          \ kindly guide me for their answers </p>\n<ol>\n<li><p>Can I run above codes\
          \ on colab's CPU ? </p>\n</li>\n<li><p>What's the difference between 4 bit\
          \ and 8 bit ?</p>\n</li>\n<li><p>Which other models I can use for above\
          \ codes ?</p>\n</li>\n</ol>\n"
        raw: "from transformers import GPTNeoXForCausalLM, AutoTokenizer\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          )\n# Note: This model takes 15 GB of Vram when loaded in 8bit\nmodel = GPTNeoXForCausalLM.from_pretrained(\n\
          \  \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\",device_map=\"auto\"\
          , load_in_8bit=True)\n# for cpu ver\n# model = AutoModelForCausalLM.from_pretrained(\"\
          OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\", torch_dtype=torch.bfloat16)\n\
          message = \"<|prompter|>What is a meme, and what's the history behind this\
          \ word?<|endoftext|><|assistant|>\"\ninputs = tokenizer(message, return_tensors=\"\
          pt\").to(model.device)\ntokens = model.generate(**inputs,  max_new_tokens=1000,\
          \ do_sample=True, temperature=0.8)\ntokenizer.decode(tokens[0])\n\n@cojosef96\
          \  I have 3 questions kindly guide me for their answers \n\n1) Can I run\
          \ above codes on colab's CPU ? \n2) What's the difference between 4 bit\
          \ and 8 bit ?\n\n3) Which other models I can use for above codes ? "
        updatedAt: '2023-06-29T19:11:00.888Z'
      numEdits: 0
      reactions: []
    id: 649dd7441e1eacd552d6caf3
    type: comment
  author: deepakkaura26
  content: "from transformers import GPTNeoXForCausalLM, AutoTokenizer\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
    )\n# Note: This model takes 15 GB of Vram when loaded in 8bit\nmodel = GPTNeoXForCausalLM.from_pretrained(\n\
    \  \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\",device_map=\"auto\", load_in_8bit=True)\n\
    # for cpu ver\n# model = AutoModelForCausalLM.from_pretrained(\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
    , torch_dtype=torch.bfloat16)\nmessage = \"<|prompter|>What is a meme, and what's\
    \ the history behind this word?<|endoftext|><|assistant|>\"\ninputs = tokenizer(message,\
    \ return_tensors=\"pt\").to(model.device)\ntokens = model.generate(**inputs, \
    \ max_new_tokens=1000, do_sample=True, temperature=0.8)\ntokenizer.decode(tokens[0])\n\
    \n@cojosef96  I have 3 questions kindly guide me for their answers \n\n1) Can\
    \ I run above codes on colab's CPU ? \n2) What's the difference between 4 bit\
    \ and 8 bit ?\n\n3) Which other models I can use for above codes ? "
  created_at: 2023-06-29 18:11:00+00:00
  edited: false
  hidden: false
  id: 649dd7441e1eacd552d6caf3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5
repo_type: model
status: open
target_branch: null
title: How to run that?
