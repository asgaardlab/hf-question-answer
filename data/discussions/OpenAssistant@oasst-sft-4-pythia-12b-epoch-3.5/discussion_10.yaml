!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Blue-Devil
conflicting_files: null
created_at: 2023-04-28 00:52:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Oqk6T_U52gaDWUQITg-jb.jpeg?w=200&h=200&f=face
      fullname: Hansen Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Blue-Devil
      type: user
    createdAt: '2023-04-28T01:52:33.000Z'
    data:
      edited: false
      editors:
      - Blue-Devil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Oqk6T_U52gaDWUQITg-jb.jpeg?w=200&h=200&f=face
          fullname: Hansen Zhang
          isHf: false
          isPro: false
          name: Blue-Devil
          type: user
        html: '<p>Hi, there. Do you have suggestions for the minimum GPU RAM size
          to run this? I am using a 12 G RAM NVIDIA GPU, but I could not run it on
          my machine. Thank you.</p>

          '
        raw: Hi, there. Do you have suggestions for the minimum GPU RAM size to run
          this? I am using a 12 G RAM NVIDIA GPU, but I could not run it on my machine.
          Thank you.
        updatedAt: '2023-04-28T01:52:33.380Z'
      numEdits: 0
      reactions: []
    id: 644b26e1b91ef1f7d135f22b
    type: comment
  author: Blue-Devil
  content: Hi, there. Do you have suggestions for the minimum GPU RAM size to run
    this? I am using a 12 G RAM NVIDIA GPU, but I could not run it on my machine.
    Thank you.
  created_at: 2023-04-28 00:52:33+00:00
  edited: false
  hidden: false
  id: 644b26e1b91ef1f7d135f22b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
      fullname: SVG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gsaivinay
      type: user
    createdAt: '2023-04-28T08:43:50.000Z'
    data:
      edited: false
      editors:
      - gsaivinay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad4bb6fe31efe3634e349f59d6d57b79.svg
          fullname: SVG
          isHf: false
          isPro: false
          name: gsaivinay
          type: user
        html: '<p>Each parameter occupies approximately 2bytes in fp16 mode, and 1byte
          in 8bit mode. </p>

          <p>So, just to load this model in 8bit, you need 12billion params * 1byte
          = 12GB approx.</p>

          <p>You need at-least 4GB more for inference, total 16GB.</p>

          '
        raw: "Each parameter occupies approximately 2bytes in fp16 mode, and 1byte\
          \ in 8bit mode. \n\nSo, just to load this model in 8bit, you need 12billion\
          \ params * 1byte = 12GB approx.\n\nYou need at-least 4GB more for inference,\
          \ total 16GB."
        updatedAt: '2023-04-28T08:43:50.048Z'
      numEdits: 0
      reactions: []
    id: 644b8746b64fb3f65f59f18a
    type: comment
  author: gsaivinay
  content: "Each parameter occupies approximately 2bytes in fp16 mode, and 1byte in\
    \ 8bit mode. \n\nSo, just to load this model in 8bit, you need 12billion params\
    \ * 1byte = 12GB approx.\n\nYou need at-least 4GB more for inference, total 16GB."
  created_at: 2023-04-28 07:43:50+00:00
  edited: false
  hidden: false
  id: 644b8746b64fb3f65f59f18a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Oqk6T_U52gaDWUQITg-jb.jpeg?w=200&h=200&f=face
      fullname: Hansen Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Blue-Devil
      type: user
    createdAt: '2023-04-28T23:27:32.000Z'
    data:
      edited: false
      editors:
      - Blue-Devil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Oqk6T_U52gaDWUQITg-jb.jpeg?w=200&h=200&f=face
          fullname: Hansen Zhang
          isHf: false
          isPro: false
          name: Blue-Devil
          type: user
        html: '<p>Thank you very much for your reply!</p>

          '
        raw: Thank you very much for your reply!
        updatedAt: '2023-04-28T23:27:32.121Z'
      numEdits: 0
      reactions: []
    id: 644c566445e79023c7e6d5ae
    type: comment
  author: Blue-Devil
  content: Thank you very much for your reply!
  created_at: 2023-04-28 22:27:32+00:00
  edited: false
  hidden: false
  id: 644c566445e79023c7e6d5ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ff5fc4fe6383d50b29052e/Vk9R5rKqG-Z_ou-55J9x-.jpeg?w=200&h=200&f=face
      fullname: AayushShah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AayushShah
      type: user
    createdAt: '2023-05-02T09:25:00.000Z'
    data:
      edited: true
      editors:
      - AayushShah
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ff5fc4fe6383d50b29052e/Vk9R5rKqG-Z_ou-55J9x-.jpeg?w=200&h=200&f=face
          fullname: AayushShah
          isHf: false
          isPro: false
          name: AayushShah
          type: user
        html: "<p>And also <span data-props=\"{&quot;user&quot;:&quot;Blue-Devil&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Blue-Devil\"\
          >@<span class=\"underline\">Blue-Devil</span></a></span>\n\n\t</span></span>\
          \ keep in mind that , while making generations you need to watch out the\
          \ \"parameters\" that you pass. </p>\n<p>In general, I when used:</p>\n\
          <pre><code class=\"language-python\">temperature=<span class=\"hljs-number\"\
          >0.9</span>, \nmin_length=<span class=\"hljs-number\">15</span>,\nearly_stopping=<span\
          \ class=\"hljs-literal\">True</span>,\nnum_beams=<span class=\"hljs-number\"\
          >8</span>,\nno_repeat_ngram_size=<span class=\"hljs-number\">2</span>,\n\
          top_k=<span class=\"hljs-number\">40</span>, \ntop_p=<span class=\"hljs-number\"\
          >0.7</span>,\nmax_new_tokens=<span class=\"hljs-number\">200</span>,\npenalty_alpha=<span\
          \ class=\"hljs-number\">0.6</span>,\nuse_cache=<span class=\"hljs-literal\"\
          >False</span>,\npad_token_id=tokenizer.eos_token_id)\n</code></pre>\n<p>I\
          \ get the CUDA OOM errors. There are a couple of reasons to this.</p>\n\
          <ol>\n<li>Using high <code>n</code> as <code>num_beams</code>: Here I used\
          \ 8 which internally keeps track of 8 different possible generations paths.\
          \ That indeed takesup the memory. For details you would need to refer to\
          \ some literature like <a href=\"https://huggingface.co/blog/how-to-generate\"\
          >this</a>.</li>\n<li><code>pentalty_alpha</code>: Surprisingly this generation\
          \ parameter also takes up memory and after passing through a hell of OOM\
          \ error, I have came to know that this parameter was the culprit. </li>\n\
          <li><code>max_new_tokens</code>: This one is obvious, the more you will\
          \ try to generate the more memory it will take (and obviously time)</li>\n\
          </ol>\n<p>So, my suggestion is to only play with a couple of generation\
          \ parameters if you have limited resources. The simple ones like:</p>\n\
          <ol>\n<li>Temperature</li>\n<li>Top k</li>\n<li>Top p</li>\n<li>no repeat\
          \ ngram size</li>\n<li>length penalty</li>\n<li>repetition penalty<br>etc.</li>\n\
          </ol>\n<p>Hope it helps \U0001F917</p>\n"
        raw: "And also @Blue-Devil keep in mind that , while making generations you\
          \ need to watch out the \"parameters\" that you pass. \n\nIn general, I\
          \ when used:\n```python\ntemperature=0.9, \nmin_length=15,\nearly_stopping=True,\n\
          num_beams=8,\nno_repeat_ngram_size=2,\ntop_k=40, \ntop_p=0.7,\nmax_new_tokens=200,\n\
          penalty_alpha=0.6,\nuse_cache=False,\npad_token_id=tokenizer.eos_token_id)\n\
          ```\n\nI get the CUDA OOM errors. There are a couple of reasons to this.\n\
          1. Using high `n` as `num_beams`: Here I used 8 which internally keeps track\
          \ of 8 different possible generations paths. That indeed takesup the memory.\
          \ For details you would need to refer to some literature like [this](https://huggingface.co/blog/how-to-generate).\n\
          2. `pentalty_alpha`: Surprisingly this generation parameter also takes up\
          \ memory and after passing through a hell of OOM error, I have came to know\
          \ that this parameter was the culprit. \n3. `max_new_tokens`: This one is\
          \ obvious, the more you will try to generate the more memory it will take\
          \ (and obviously time)\n\nSo, my suggestion is to only play with a couple\
          \ of generation parameters if you have limited resources. The simple ones\
          \ like:\n1. Temperature\n2. Top k\n3. Top p\n4. no repeat ngram size\n5.\
          \ length penalty\n6. repetition penalty\netc.\n\nHope it helps \U0001F917"
        updatedAt: '2023-05-02T09:26:36.638Z'
      numEdits: 2
      reactions: []
    id: 6450d6ec8f876fbfc5eb384f
    type: comment
  author: AayushShah
  content: "And also @Blue-Devil keep in mind that , while making generations you\
    \ need to watch out the \"parameters\" that you pass. \n\nIn general, I when used:\n\
    ```python\ntemperature=0.9, \nmin_length=15,\nearly_stopping=True,\nnum_beams=8,\n\
    no_repeat_ngram_size=2,\ntop_k=40, \ntop_p=0.7,\nmax_new_tokens=200,\npenalty_alpha=0.6,\n\
    use_cache=False,\npad_token_id=tokenizer.eos_token_id)\n```\n\nI get the CUDA\
    \ OOM errors. There are a couple of reasons to this.\n1. Using high `n` as `num_beams`:\
    \ Here I used 8 which internally keeps track of 8 different possible generations\
    \ paths. That indeed takesup the memory. For details you would need to refer to\
    \ some literature like [this](https://huggingface.co/blog/how-to-generate).\n\
    2. `pentalty_alpha`: Surprisingly this generation parameter also takes up memory\
    \ and after passing through a hell of OOM error, I have came to know that this\
    \ parameter was the culprit. \n3. `max_new_tokens`: This one is obvious, the more\
    \ you will try to generate the more memory it will take (and obviously time)\n\
    \nSo, my suggestion is to only play with a couple of generation parameters if\
    \ you have limited resources. The simple ones like:\n1. Temperature\n2. Top k\n\
    3. Top p\n4. no repeat ngram size\n5. length penalty\n6. repetition penalty\n\
    etc.\n\nHope it helps \U0001F917"
  created_at: 2023-05-02 08:25:00+00:00
  edited: true
  hidden: false
  id: 6450d6ec8f876fbfc5eb384f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5
repo_type: model
status: open
target_branch: null
title: CUDA out of memory
