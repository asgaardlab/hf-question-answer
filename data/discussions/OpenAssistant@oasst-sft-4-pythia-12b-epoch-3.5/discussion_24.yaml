!!python/object:huggingface_hub.community.DiscussionWithDetails
author: julianouxui
conflicting_files: null
created_at: 2023-07-02 13:06:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58eb5c99b53084db0e53679b4e55ccb3.svg
      fullname: juliano bresolin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: julianouxui
      type: user
    createdAt: '2023-07-02T14:06:48.000Z'
    data:
      edited: false
      editors:
      - julianouxui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9330093264579773
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58eb5c99b53084db0e53679b4e55ccb3.svg
          fullname: juliano bresolin
          isHf: false
          isPro: false
          name: julianouxui
          type: user
        html: '<p>I''m trying to use the vercel sdk but the application doesn''t work
          and doesn''t return an error.<br>Is anyone having this problem?<br>in addition
          to configuring the token in the .env, do you need to configure anything
          else here?</p>

          <p><a rel="nofollow" href="https://sdk.vercel.ai/docs/guides/huggingface">https://sdk.vercel.ai/docs/guides/huggingface</a></p>

          '
        raw: "I'm trying to use the vercel sdk but the application doesn't work and\
          \ doesn't return an error.\r\nIs anyone having this problem?\r\nin addition\
          \ to configuring the token in the .env, do you need to configure anything\
          \ else here?\r\n\r\nhttps://sdk.vercel.ai/docs/guides/huggingface"
        updatedAt: '2023-07-02T14:06:48.667Z'
      numEdits: 0
      reactions: []
    id: 64a18478f20126f9d0dad12e
    type: comment
  author: julianouxui
  content: "I'm trying to use the vercel sdk but the application doesn't work and\
    \ doesn't return an error.\r\nIs anyone having this problem?\r\nin addition to\
    \ configuring the token in the .env, do you need to configure anything else here?\r\
    \n\r\nhttps://sdk.vercel.ai/docs/guides/huggingface"
  created_at: 2023-07-02 13:06:48+00:00
  edited: false
  hidden: false
  id: 64a18478f20126f9d0dad12e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PIHS80ydeBIKgfM9mIM-N.png?w=200&h=200&f=face
      fullname: Walter Jenkins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AvidDabbler
      type: user
    createdAt: '2023-07-07T15:29:30.000Z'
    data:
      edited: false
      editors:
      - AvidDabbler
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4282703697681427
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PIHS80ydeBIKgfM9mIM-N.png?w=200&h=200&f=face
          fullname: Walter Jenkins
          isHf: false
          isPro: false
          name: AvidDabbler
          type: user
        html: '<p>same</p>

          '
        raw: same
        updatedAt: '2023-07-07T15:29:30.592Z'
      numEdits: 0
      reactions: []
    id: 64a82f5a4b01c33cf6913eba
    type: comment
  author: AvidDabbler
  content: same
  created_at: 2023-07-07 14:29:30+00:00
  edited: false
  hidden: false
  id: 64a82f5a4b01c33cf6913eba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6473e7ed7afa69c3c7a8b673/kOssQ_OK_U2SSjNa1O30c.png?w=200&h=200&f=face
      fullname: Esocoder
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: EsoCode
      type: user
    createdAt: '2023-07-10T14:09:12.000Z'
    data:
      edited: false
      editors:
      - EsoCode
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7819202542304993
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6473e7ed7afa69c3c7a8b673/kOssQ_OK_U2SSjNa1O30c.png?w=200&h=200&f=face
          fullname: Esocoder
          isHf: false
          isPro: true
          name: EsoCode
          type: user
        html: '<p>Not sure, but if the most basic way of loading the model with gradio
          already gives a error. Then i assum something is not right with it. </p>

          <p>import gradio as gr<br>gr.Interface.load("models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5").launch()</p>

          '
        raw: "Not sure, but if the most basic way of loading the model with gradio\
          \ already gives a error. Then i assum something is not right with it. \n\
          \nimport gradio as gr\ngr.Interface.load(\"models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          ).launch()\n\n"
        updatedAt: '2023-07-10T14:09:12.977Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - FelixMildon
    id: 64ac1108726a26fda447d24e
    type: comment
  author: EsoCode
  content: "Not sure, but if the most basic way of loading the model with gradio already\
    \ gives a error. Then i assum something is not right with it. \n\nimport gradio\
    \ as gr\ngr.Interface.load(\"models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
    ).launch()\n\n"
  created_at: 2023-07-10 13:09:12+00:00
  edited: false
  hidden: false
  id: 64ac1108726a26fda447d24e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58eb5c99b53084db0e53679b4e55ccb3.svg
      fullname: juliano bresolin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: julianouxui
      type: user
    createdAt: '2023-08-16T22:43:19.000Z'
    data:
      edited: false
      editors:
      - julianouxui
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3872499465942383
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58eb5c99b53084db0e53679b4e55ccb3.svg
          fullname: juliano bresolin
          isHf: false
          isPro: false
          name: julianouxui
          type: user
        html: '<p>i''m use the exemple from github : ai/examples/next-huggingface/app/api/chat<br>/route.ts   ,
          and sucess.<br>import { HfInference } from ''@huggingface/inference''<br>import
          { HuggingFaceStream, StreamingTextResponse } from ''ai''<br>import { experimental_buildOpenAssistantPrompt
          } from ''ai/prompts''</p>

          <p>// Create a new HuggingFace Inference instance<br>const Hf = new HfInference(process.env.HUGGINGFACE_API_KEY)</p>

          <p>// IMPORTANT! Set the runtime to edge<br>export const runtime = ''edge''</p>

          <p>export async function POST(req: Request) {<br>  // Extract the <code>messages</code>
          from the body of the request<br>  const { messages } = await req.json()</p>

          <p>  const response = Hf.textGenerationStream({<br>    model: ''OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5'',<br>    inputs:
          experimental_buildOpenAssistantPrompt(messages),<br>    parameters: {<br>      max_new_tokens:
          200,<br>      // @ts-ignore (this is a valid parameter specifically in OpenAssistant
          models)<br>      typical_p: 0.2,<br>      repetition_penalty: 1,<br>      truncate:
          1000,<br>      return_full_text: false<br>    }<br>  })</p>

          <p>  // Convert the response into a friendly text-stream<br>  const stream
          = HuggingFaceStream(response)</p>

          <p>  // Respond with the stream<br>  return new StreamingTextResponse(stream)<br>}</p>

          '
        raw: "i'm use the exemple from github : ai/examples/next-huggingface/app/api/chat\n\
          /route.ts   , and sucess.\nimport { HfInference } from '@huggingface/inference'\n\
          import { HuggingFaceStream, StreamingTextResponse } from 'ai'\nimport {\
          \ experimental_buildOpenAssistantPrompt } from 'ai/prompts'\n\n// Create\
          \ a new HuggingFace Inference instance\nconst Hf = new HfInference(process.env.HUGGINGFACE_API_KEY)\n\
          \n// IMPORTANT! Set the runtime to edge\nexport const runtime = 'edge'\n\
          \nexport async function POST(req: Request) {\n  // Extract the `messages`\
          \ from the body of the request\n  const { messages } = await req.json()\n\
          \n  const response = Hf.textGenerationStream({\n    model: 'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',\n\
          \    inputs: experimental_buildOpenAssistantPrompt(messages),\n    parameters:\
          \ {\n      max_new_tokens: 200,\n      // @ts-ignore (this is a valid parameter\
          \ specifically in OpenAssistant models)\n      typical_p: 0.2,\n      repetition_penalty:\
          \ 1,\n      truncate: 1000,\n      return_full_text: false\n    }\n  })\n\
          \n  // Convert the response into a friendly text-stream\n  const stream\
          \ = HuggingFaceStream(response)\n\n  // Respond with the stream\n  return\
          \ new StreamingTextResponse(stream)\n}"
        updatedAt: '2023-08-16T22:43:19.667Z'
      numEdits: 0
      reactions: []
    id: 64dd5107ea8c6869317b874d
    type: comment
  author: julianouxui
  content: "i'm use the exemple from github : ai/examples/next-huggingface/app/api/chat\n\
    /route.ts   , and sucess.\nimport { HfInference } from '@huggingface/inference'\n\
    import { HuggingFaceStream, StreamingTextResponse } from 'ai'\nimport { experimental_buildOpenAssistantPrompt\
    \ } from 'ai/prompts'\n\n// Create a new HuggingFace Inference instance\nconst\
    \ Hf = new HfInference(process.env.HUGGINGFACE_API_KEY)\n\n// IMPORTANT! Set the\
    \ runtime to edge\nexport const runtime = 'edge'\n\nexport async function POST(req:\
    \ Request) {\n  // Extract the `messages` from the body of the request\n  const\
    \ { messages } = await req.json()\n\n  const response = Hf.textGenerationStream({\n\
    \    model: 'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',\n    inputs: experimental_buildOpenAssistantPrompt(messages),\n\
    \    parameters: {\n      max_new_tokens: 200,\n      // @ts-ignore (this is a\
    \ valid parameter specifically in OpenAssistant models)\n      typical_p: 0.2,\n\
    \      repetition_penalty: 1,\n      truncate: 1000,\n      return_full_text:\
    \ false\n    }\n  })\n\n  // Convert the response into a friendly text-stream\n\
    \  const stream = HuggingFaceStream(response)\n\n  // Respond with the stream\n\
    \  return new StreamingTextResponse(stream)\n}"
  created_at: 2023-08-16 21:43:19+00:00
  edited: false
  hidden: false
  id: 64dd5107ea8c6869317b874d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5
repo_type: model
status: open
target_branch: null
title: Vercel AI SDK
