!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jbollenbacher
conflicting_files: null
created_at: 2023-04-17 01:47:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/442a61a9874eab73632bc407cd0e2b4e.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jbollenbacher
      type: user
    createdAt: '2023-04-17T02:47:17.000Z'
    data:
      edited: true
      editors:
      - jbollenbacher
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/442a61a9874eab73632bc407cd0e2b4e.svg
          fullname: John
          isHf: false
          isPro: false
          name: jbollenbacher
          type: user
        html: '<p>Hi team!  Love your work on OpenAssistant! </p>

          <p>Can we get these models ported to a 4bit gglm version? Especially the
          pythia-based models, but the llama ones would be nice too.  This would make
          them much more portable and easy to experiment with.</p>

          <p>Thanks!</p>

          '
        raw: "Hi team!  Love your work on OpenAssistant! \n\nCan we get these models\
          \ ported to a 4bit gglm version? Especially the pythia-based models, but\
          \ the llama ones would be nice too.  This would make them much more portable\
          \ and easy to experiment with.\n\nThanks!"
        updatedAt: '2023-04-17T02:49:05.793Z'
      numEdits: 1
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - Kagerage
        - Logophoman
        - chromesync
        - ndparisssos
        - ignorethis
        - TheHappyDrone
    id: 643cb3359f5d314db2dffd23
    type: comment
  author: jbollenbacher
  content: "Hi team!  Love your work on OpenAssistant! \n\nCan we get these models\
    \ ported to a 4bit gglm version? Especially the pythia-based models, but the llama\
    \ ones would be nice too.  This would make them much more portable and easy to\
    \ experiment with.\n\nThanks!"
  created_at: 2023-04-17 01:47:17+00:00
  edited: true
  hidden: false
  id: 643cb3359f5d314db2dffd23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6449d9c56fa5e1ba98a442c1/zl5fC_x1Rh_Df85zHU76A.jpeg?w=200&h=200&f=face
      fullname: Byron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: byroneverson
      type: user
    createdAt: '2023-04-29T08:54:34.000Z'
    data:
      edited: false
      editors:
      - byroneverson
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6449d9c56fa5e1ba98a442c1/zl5fC_x1Rh_Df85zHU76A.jpeg?w=200&h=200&f=face
          fullname: Byron
          isHf: false
          isPro: false
          name: byroneverson
          type: user
        html: '<p>Please see my repositories here and on github, GPT-NeoX based models
          (OpenAssistant StableLM and Pythia models) will not run in llama.cpp or
          ggml at the moment, but my fork of llama.cpp will. Best of luck!</p>

          <p><a rel="nofollow" href="https://github.com/byroneverson/gptneox.cpp">https://github.com/byroneverson/gptneox.cpp</a></p>

          '
        raw: 'Please see my repositories here and on github, GPT-NeoX based models
          (OpenAssistant StableLM and Pythia models) will not run in llama.cpp or
          ggml at the moment, but my fork of llama.cpp will. Best of luck!


          https://github.com/byroneverson/gptneox.cpp'
        updatedAt: '2023-04-29T08:54:34.543Z'
      numEdits: 0
      reactions: []
    id: 644cdb4afa94e93b0ebbe8a5
    type: comment
  author: byroneverson
  content: 'Please see my repositories here and on github, GPT-NeoX based models (OpenAssistant
    StableLM and Pythia models) will not run in llama.cpp or ggml at the moment, but
    my fork of llama.cpp will. Best of luck!


    https://github.com/byroneverson/gptneox.cpp'
  created_at: 2023-04-29 07:54:34+00:00
  edited: false
  hidden: false
  id: 644cdb4afa94e93b0ebbe8a5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5
repo_type: model
status: open
target_branch: null
title: 4bit gglm version?
