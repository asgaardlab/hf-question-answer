!!python/object:huggingface_hub.community.DiscussionWithDetails
author: swulling
conflicting_files: null
created_at: 2023-11-18 14:37:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cbc0ceb6b221b828a3a030ce02e46f26.svg
      fullname: Alex Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: swulling
      type: user
    createdAt: '2023-11-18T14:37:23.000Z'
    data:
      edited: false
      editors:
      - swulling
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5149450302124023
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cbc0ceb6b221b828a3a030ce02e46f26.svg
          fullname: Alex Yang
          isHf: false
          isPro: false
          name: swulling
          type: user
        html: "<p>Load:</p>\n<pre><code>import torch\nfrom transformers import AutoModelForSequenceClassification,\
          \ AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\n\
          model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\n\
          model.to(\"cuda\")\nmodel.eval()\nmodel.half()\n\npairs = [['what is panda?',\
          \ 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes\
          \ called a panda bear or simply panda, is a bear species endemic to China.']]\n\
          with torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True,\
          \ return_tensors='pt', max_length=512).to(\"cuda\")\n    scores = model(**inputs,\
          \ return_dict=True).logits.view(-1, ).float()\n    print(scores)\n</code></pre>\n\
          <p>If use ONNX GPU Runtime with O4, it will fast than ctranslate2.</p>\n"
        raw: "Load:\r\n\r\n```\r\nimport torch\r\nfrom transformers import AutoModelForSequenceClassification,\
          \ AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\r\
          \nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\r\
          \nmodel.to(\"cuda\")\r\nmodel.eval()\r\nmodel.half()\r\n\r\npairs = [['what\
          \ is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca),\
          \ sometimes called a panda bear or simply panda, is a bear species endemic\
          \ to China.']]\r\nwith torch.no_grad():\r\n    inputs = tokenizer(pairs,\
          \ padding=True, truncation=True, return_tensors='pt', max_length=512).to(\"\
          cuda\")\r\n    scores = model(**inputs, return_dict=True).logits.view(-1,\
          \ ).float()\r\n    print(scores)\r\n```\r\n\r\nIf use ONNX GPU Runtime with\
          \ O4, it will fast than ctranslate2.\r\n"
        updatedAt: '2023-11-18T14:37:23.555Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hooman650
    id: 6558cc2351ebd26dbc0ac714
    type: comment
  author: swulling
  content: "Load:\r\n\r\n```\r\nimport torch\r\nfrom transformers import AutoModelForSequenceClassification,\
    \ AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\r\
    \nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\r\
    \nmodel.to(\"cuda\")\r\nmodel.eval()\r\nmodel.half()\r\n\r\npairs = [['what is\
    \ panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca),\
    \ sometimes called a panda bear or simply panda, is a bear species endemic to\
    \ China.']]\r\nwith torch.no_grad():\r\n    inputs = tokenizer(pairs, padding=True,\
    \ truncation=True, return_tensors='pt', max_length=512).to(\"cuda\")\r\n    scores\
    \ = model(**inputs, return_dict=True).logits.view(-1, ).float()\r\n    print(scores)\r\
    \n```\r\n\r\nIf use ONNX GPU Runtime with O4, it will fast than ctranslate2.\r\
    \n"
  created_at: 2023-11-18 14:37:23+00:00
  edited: false
  hidden: false
  id: 6558cc2351ebd26dbc0ac714
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625156462799-6053bc853efc404ddc7c0c11.png?w=200&h=200&f=face
      fullname: Hooman Sedghamiz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: hooman650
      type: user
    createdAt: '2023-11-18T21:58:00.000Z'
    data:
      edited: false
      editors:
      - hooman650
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.896375834941864
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625156462799-6053bc853efc404ddc7c0c11.png?w=200&h=200&f=face
          fullname: Hooman Sedghamiz
          isHf: false
          isPro: false
          name: hooman650
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;swulling&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/swulling\"\
          >@<span class=\"underline\">swulling</span></a></span>\n\n\t</span></span>\
          \ thanks for your comment. I will do some extensive testing today to benchmark\
          \ against ONNX O4.</p>\n"
        raw: hi @swulling thanks for your comment. I will do some extensive testing
          today to benchmark against ONNX O4.
        updatedAt: '2023-11-18T21:58:00.864Z'
      numEdits: 0
      reactions: []
    id: 65593368e0a6202d3648e1fa
    type: comment
  author: hooman650
  content: hi @swulling thanks for your comment. I will do some extensive testing
    today to benchmark against ONNX O4.
  created_at: 2023-11-18 21:58:00+00:00
  edited: false
  hidden: false
  id: 65593368e0a6202d3648e1fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625156462799-6053bc853efc404ddc7c0c11.png?w=200&h=200&f=face
      fullname: Hooman Sedghamiz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: hooman650
      type: user
    createdAt: '2023-11-18T23:01:23.000Z'
    data:
      edited: false
      editors:
      - hooman650
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6851159930229187
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625156462799-6053bc853efc404ddc7c0c11.png?w=200&h=200&f=face
          fullname: Hooman Sedghamiz
          isHf: false
          isPro: false
          name: hooman650
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;swulling&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/swulling\">@<span class=\"\
          underline\">swulling</span></a></span>\n\n\t</span></span> </p>\n<p>Bellow\
          \ is a quick benchmark (on A10 GPU).</p>\n<pre><code>from transformers import\
          \ AutoTokenizer\nfrom optimum.onnxruntime import ORTModelForSequenceClassification\n\
          import time\nimport torch\n\ndevice_mapping=\"cuda\" if torch.cuda.is_available()\
          \ else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"./onnxO4_bge_reranker_large\"\
          )\nmodel = ORTModelForSequenceClassification.from_pretrained(\"./onnxO4_bge_reranker_large\"\
          ).to(device_mapping)\n\npairs = [['what is panda?', 'The giant panda (Ailuropoda\
          \ melanoleuca), sometimes called a panda bear or simply panda, is a bear\
          \ species endemic to China.']]*1024\nt0 = time.time()\nwith torch.no_grad():\n\
          \    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt',\
          \ max_length=512).to(device_mapping)\n    scores = model(**inputs, return_dict=True).logits.view(-1,\
          \ ).float()\nt1 = time.time()\nprint(f\"Seconds: {t1-t0}\")\n\n# Seconds:\
          \ 1.3976035118103027\n</code></pre>\n<p>I tried to convert the model weights\
          \ using both O3 and O4 (<code>--device cuda</code>), I encountered some\
          \ issues but anyway using both the average time for a batch of <code>1024</code>\
          \ was 1.39 seconds VS <code>0.8</code> for <code>ctranslate2</code> and\
          \ <code>0.9</code> for <code>fp16</code>. It seems like <code>fp16</code>\
          \ is definitely a good competitor! Have you tried converting the weights\
          \ to ONNX O4 and benchmark too?</p>\n"
        raw: "@swulling \n\nBellow is a quick benchmark (on A10 GPU).\n\n```\nfrom\
          \ transformers import AutoTokenizer\nfrom optimum.onnxruntime import ORTModelForSequenceClassification\n\
          import time\nimport torch\n\ndevice_mapping=\"cuda\" if torch.cuda.is_available()\
          \ else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"./onnxO4_bge_reranker_large\"\
          )\nmodel = ORTModelForSequenceClassification.from_pretrained(\"./onnxO4_bge_reranker_large\"\
          ).to(device_mapping)\n\npairs = [['what is panda?', 'The giant panda (Ailuropoda\
          \ melanoleuca), sometimes called a panda bear or simply panda, is a bear\
          \ species endemic to China.']]*1024\nt0 = time.time()\nwith torch.no_grad():\n\
          \    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt',\
          \ max_length=512).to(device_mapping)\n    scores = model(**inputs, return_dict=True).logits.view(-1,\
          \ ).float()\nt1 = time.time()\nprint(f\"Seconds: {t1-t0}\")\n\n# Seconds:\
          \ 1.3976035118103027\n```\n\nI tried to convert the model weights using\
          \ both O3 and O4 (`--device cuda`), I encountered some issues but anyway\
          \ using both the average time for a batch of `1024` was 1.39 seconds VS\
          \ `0.8` for `ctranslate2` and `0.9` for `fp16`. It seems like `fp16` is\
          \ definitely a good competitor! Have you tried converting the weights to\
          \ ONNX O4 and benchmark too?\n\n"
        updatedAt: '2023-11-18T23:01:23.448Z'
      numEdits: 0
      reactions: []
    id: 655942433aff9efaad3009aa
    type: comment
  author: hooman650
  content: "@swulling \n\nBellow is a quick benchmark (on A10 GPU).\n\n```\nfrom transformers\
    \ import AutoTokenizer\nfrom optimum.onnxruntime import ORTModelForSequenceClassification\n\
    import time\nimport torch\n\ndevice_mapping=\"cuda\" if torch.cuda.is_available()\
    \ else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"./onnxO4_bge_reranker_large\"\
    )\nmodel = ORTModelForSequenceClassification.from_pretrained(\"./onnxO4_bge_reranker_large\"\
    ).to(device_mapping)\n\npairs = [['what is panda?', 'The giant panda (Ailuropoda\
    \ melanoleuca), sometimes called a panda bear or simply panda, is a bear species\
    \ endemic to China.']]*1024\nt0 = time.time()\nwith torch.no_grad():\n    inputs\
    \ = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512).to(device_mapping)\n\
    \    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\nt1\
    \ = time.time()\nprint(f\"Seconds: {t1-t0}\")\n\n# Seconds: 1.3976035118103027\n\
    ```\n\nI tried to convert the model weights using both O3 and O4 (`--device cuda`),\
    \ I encountered some issues but anyway using both the average time for a batch\
    \ of `1024` was 1.39 seconds VS `0.8` for `ctranslate2` and `0.9` for `fp16`.\
    \ It seems like `fp16` is definitely a good competitor! Have you tried converting\
    \ the weights to ONNX O4 and benchmark too?\n\n"
  created_at: 2023-11-18 23:01:23+00:00
  edited: false
  hidden: false
  id: 655942433aff9efaad3009aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cbc0ceb6b221b828a3a030ce02e46f26.svg
      fullname: Alex Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: swulling
      type: user
    createdAt: '2023-11-21T08:18:13.000Z'
    data:
      edited: true
      editors:
      - swulling
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8780779838562012
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cbc0ceb6b221b828a3a030ce02e46f26.svg
          fullname: Alex Yang
          isHf: false
          isPro: false
          name: swulling
          type: user
        html: "<blockquote>\n<p>I tried to convert the model weights using both O3\
          \ and O4 (<code>--device cuda</code>), I encountered some issues but anyway\
          \ using both the average time for a batch of <code>1024</code> was 1.39\
          \ seconds VS <code>0.8</code> for <code>ctranslate2</code> and <code>0.9</code>\
          \ for <code>fp16</code>. It seems like <code>fp16</code> is definitely a\
          \ good competitor! Have you tried converting the weights to ONNX O4 and\
          \ benchmark too?</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;hooman650&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/hooman650\"\
          >@<span class=\"underline\">hooman650</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p><a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1HP9GQKdzYa6H9SJnAZoxJWq920gxwd2k?usp=sharing\"\
          >https://colab.research.google.com/drive/1HP9GQKdzYa6H9SJnAZoxJWq920gxwd2k?usp=sharing</a></p>\n\
          <p>bge-rerank-base: ONNX O4  is 2x fast than fp16.<br>bge-rerank-large:\
          \ same result</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64190f940e48a6689b2829fe/TVuc4cuRQFzxSK_b6zRdI.png\"\
          ><img alt=\"infoflow 2023-11-21 16-30-38.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64190f940e48a6689b2829fe/TVuc4cuRQFzxSK_b6zRdI.png\"\
          ></a></p>\n"
        raw: "\n> I tried to convert the model weights using both O3 and O4 (`--device\
          \ cuda`), I encountered some issues but anyway using both the average time\
          \ for a batch of `1024` was 1.39 seconds VS `0.8` for `ctranslate2` and\
          \ `0.9` for `fp16`. It seems like `fp16` is definitely a good competitor!\
          \ Have you tried converting the weights to ONNX O4 and benchmark too?\n\n\
          @hooman650 \n\n[https://colab.research.google.com/drive/1HP9GQKdzYa6H9SJnAZoxJWq920gxwd2k?usp=sharing](https://colab.research.google.com/drive/1HP9GQKdzYa6H9SJnAZoxJWq920gxwd2k?usp=sharing)\n\
          \nbge-rerank-base: ONNX O4  is 2x fast than fp16.\nbge-rerank-large: same\
          \ result\n\n\n![infoflow 2023-11-21 16-30-38.png](https://cdn-uploads.huggingface.co/production/uploads/64190f940e48a6689b2829fe/TVuc4cuRQFzxSK_b6zRdI.png)\n"
        updatedAt: '2023-11-21T08:32:42.242Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hooman650
    id: 655c67c56d02c2b1a93d83ca
    type: comment
  author: swulling
  content: "\n> I tried to convert the model weights using both O3 and O4 (`--device\
    \ cuda`), I encountered some issues but anyway using both the average time for\
    \ a batch of `1024` was 1.39 seconds VS `0.8` for `ctranslate2` and `0.9` for\
    \ `fp16`. It seems like `fp16` is definitely a good competitor! Have you tried\
    \ converting the weights to ONNX O4 and benchmark too?\n\n@hooman650 \n\n[https://colab.research.google.com/drive/1HP9GQKdzYa6H9SJnAZoxJWq920gxwd2k?usp=sharing](https://colab.research.google.com/drive/1HP9GQKdzYa6H9SJnAZoxJWq920gxwd2k?usp=sharing)\n\
    \nbge-rerank-base: ONNX O4  is 2x fast than fp16.\nbge-rerank-large: same result\n\
    \n\n![infoflow 2023-11-21 16-30-38.png](https://cdn-uploads.huggingface.co/production/uploads/64190f940e48a6689b2829fe/TVuc4cuRQFzxSK_b6zRdI.png)\n"
  created_at: 2023-11-21 08:18:13+00:00
  edited: true
  hidden: false
  id: 655c67c56d02c2b1a93d83ca
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: hooman650/ct2fast-bge-reranker
repo_type: model
status: open
target_branch: null
title: In actual testing, compared to using fp16,  it is only less than 10% faster
