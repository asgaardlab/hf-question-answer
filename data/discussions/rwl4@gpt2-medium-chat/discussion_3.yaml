!!python/object:huggingface_hub.community.DiscussionWithDetails
author: developerbayman
conflicting_files: null
created_at: 2023-11-09 21:50:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8d798337166c1864ade52160e716d81.svg
      fullname: jeremy vernotzy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: developerbayman
      type: user
    createdAt: '2023-11-09T21:50:00.000Z'
    data:
      edited: false
      editors:
      - developerbayman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.660834014415741
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8d798337166c1864ade52160e716d81.svg
          fullname: jeremy vernotzy
          isHf: false
          isPro: false
          name: developerbayman
          type: user
        html: "<p>here is my code im trying to get it to just reply but without repeating\
          \  example also im not sure about the warning? : </p>\n<p>%Run ai_test1.py<br>Listening...<br>You\
          \ said: hello how are you today<br>C:\\Users\\xanth\\AppData\\Roaming\\\
          Python\\Python310\\site-packages\\transformers\\generation\\utils.py:1421:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use and modify the model\
          \ generation configuration (see <a href=\"https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\"\
          >https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration</a>\
          \ )<br>  warnings.warn(<br>Chatbot: hello how are you today?</p>\n<p>A.\
          \ I'm doing well.<br>B. How about you?<br>C. Good morning.   How are things\
          \ going? How long have you been here? What do you think of the new<br>Listening...<br>You\
          \ said: can you tell me a joke<br>Chatbot: can you tell me a joke about\
          \ a cat?user<br>What is the most common way to describe a person who is\
          \ a genius?<br>A person with a brilliant mind.assistant<br>The most commonly\
          \ used way of describing a</p>\n<pre><code>#!/usr/bin/python3\n\nimport\
          \ sys\nimport pyttsx3\nimport speech_recognition as sr\n#from transformers\
          \ import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\n\n\n\n# Load the model and tokenizer\n\
          #tokenizer = AutoTokenizer.from_pretrained(\"liam168/chat-DialoGPT-small-en\"\
          )\n#model = AutoModelForSeq2SeqLM.from_pretrained(\"liam168/chat-DialoGPT-small-en\"\
          )\n\ntokenizer = AutoTokenizer.from_pretrained(\"rwl4/gpt2-medium-chat\"\
          )\nmodel = AutoModelForCausalLM.from_pretrained(\"rwl4/gpt2-medium-chat\"\
          , pad_token_id=50256)\n\n\ndef generate_response(user_input):\n    input_ids\
          \ = tokenizer.encode(user_input, return_tensors=\"pt\")\n    response_ids\
          \ = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\n\
          \    response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n\
          \    return response\n\ndef listen_to_command():\n    r = sr.Recognizer()\n\
          \n    while True:\n        with sr.Microphone() as source:\n           \
          \ r.adjust_for_ambient_noise(source)\n            print(\"Listening...\"\
          )\n            try:\n                audio = r.listen(source)\n        \
          \        command = r.recognize_google(audio)\n                print(\"You\
          \ said:\", command)\n                response = generate_response(command)\n\
          \                print(\"Chatbot:\", response)\n                speak(response)\n\
          \            except sr.UnknownValueError:\n                pass\n      \
          \      except sr.RequestError as e:\n                print(\"Could not request\
          \ results from Google Speech Recognition service:\", str(e))\n\ndef speak(text):\n\
          \    tts_engine = pyttsx3.init()\n    tts_engine.say(text)\n    tts_engine.runAndWait()\n\
          \nif __name__ == \"__main__\":\n    listen_to_command()\n</code></pre>\n"
        raw: "here is my code im trying to get it to just reply but without repeating\
          \  example also im not sure about the warning? : \r\n\r\n%Run ai_test1.py\r\
          \nListening...\r\nYou said: hello how are you today\r\nC:\\Users\\xanth\\\
          AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\generation\\\
          utils.py:1421: UserWarning: You have modified the pretrained model configuration\
          \ to control generation. This is a deprecated strategy to control generation\
          \ and will be removed soon, in a future version. Please use and modify the\
          \ model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\
          \ )\r\n  warnings.warn(\r\nChatbot: hello how are you today?\r\n\r\nA. I'm\
          \ doing well.\r\nB. How about you? \r\nC. Good morning.   How are things\
          \ going? How long have you been here? What do you think of the new\r\nListening...\r\
          \nYou said: can you tell me a joke\r\nChatbot: can you tell me a joke about\
          \ a cat?user\r\nWhat is the most common way to describe a person who is\
          \ a genius?\r\nA person with a brilliant mind.assistant\r\nThe most commonly\
          \ used way of describing a\r\n\r\n```\r\n#!/usr/bin/python3\r\n\r\nimport\
          \ sys\r\nimport pyttsx3\r\nimport speech_recognition as sr\r\n#from transformers\
          \ import AutoTokenizer, AutoModelForSeq2SeqLM\r\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\r\n\r\n\r\n\r\n# Load the model and\
          \ tokenizer\r\n#tokenizer = AutoTokenizer.from_pretrained(\"liam168/chat-DialoGPT-small-en\"\
          )\r\n#model = AutoModelForSeq2SeqLM.from_pretrained(\"liam168/chat-DialoGPT-small-en\"\
          )\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"rwl4/gpt2-medium-chat\"\
          )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"rwl4/gpt2-medium-chat\"\
          , pad_token_id=50256)\r\n\r\n\r\ndef generate_response(user_input):\r\n\
          \    input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\r\n\
          \    response_ids = model.generate(input_ids, max_length=50, num_return_sequences=1,\
          \ no_repeat_ngram_size=2)\r\n    response = tokenizer.decode(response_ids[0],\
          \ skip_special_tokens=True)\r\n    return response\r\n\r\ndef listen_to_command():\r\
          \n    r = sr.Recognizer()\r\n\r\n    while True:\r\n        with sr.Microphone()\
          \ as source:\r\n            r.adjust_for_ambient_noise(source)\r\n     \
          \       print(\"Listening...\")\r\n            try:\r\n                audio\
          \ = r.listen(source)\r\n                command = r.recognize_google(audio)\r\
          \n                print(\"You said:\", command)\r\n                response\
          \ = generate_response(command)\r\n                print(\"Chatbot:\", response)\r\
          \n                speak(response)\r\n            except sr.UnknownValueError:\r\
          \n                pass\r\n            except sr.RequestError as e:\r\n \
          \               print(\"Could not request results from Google Speech Recognition\
          \ service:\", str(e))\r\n\r\ndef speak(text):\r\n    tts_engine = pyttsx3.init()\r\
          \n    tts_engine.say(text)\r\n    tts_engine.runAndWait()\r\n\r\nif __name__\
          \ == \"__main__\":\r\n    listen_to_command()\r\n\r\n```"
        updatedAt: '2023-11-09T21:50:00.084Z'
      numEdits: 0
      reactions: []
    id: 654d54081e6216cb2d8c2020
    type: comment
  author: developerbayman
  content: "here is my code im trying to get it to just reply but without repeating\
    \  example also im not sure about the warning? : \r\n\r\n%Run ai_test1.py\r\n\
    Listening...\r\nYou said: hello how are you today\r\nC:\\Users\\xanth\\AppData\\\
    Roaming\\Python\\Python310\\site-packages\\transformers\\generation\\utils.py:1421:\
    \ UserWarning: You have modified the pretrained model configuration to control\
    \ generation. This is a deprecated strategy to control generation and will be\
    \ removed soon, in a future version. Please use and modify the model generation\
    \ configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\
    \ )\r\n  warnings.warn(\r\nChatbot: hello how are you today?\r\n\r\nA. I'm doing\
    \ well.\r\nB. How about you? \r\nC. Good morning.   How are things going? How\
    \ long have you been here? What do you think of the new\r\nListening...\r\nYou\
    \ said: can you tell me a joke\r\nChatbot: can you tell me a joke about a cat?user\r\
    \nWhat is the most common way to describe a person who is a genius?\r\nA person\
    \ with a brilliant mind.assistant\r\nThe most commonly used way of describing\
    \ a\r\n\r\n```\r\n#!/usr/bin/python3\r\n\r\nimport sys\r\nimport pyttsx3\r\nimport\
    \ speech_recognition as sr\r\n#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\r\
    \nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\n\r\n\r\n\r\n\
    # Load the model and tokenizer\r\n#tokenizer = AutoTokenizer.from_pretrained(\"\
    liam168/chat-DialoGPT-small-en\")\r\n#model = AutoModelForSeq2SeqLM.from_pretrained(\"\
    liam168/chat-DialoGPT-small-en\")\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    rwl4/gpt2-medium-chat\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"rwl4/gpt2-medium-chat\"\
    , pad_token_id=50256)\r\n\r\n\r\ndef generate_response(user_input):\r\n    input_ids\
    \ = tokenizer.encode(user_input, return_tensors=\"pt\")\r\n    response_ids =\
    \ model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\r\
    \n    response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\r\
    \n    return response\r\n\r\ndef listen_to_command():\r\n    r = sr.Recognizer()\r\
    \n\r\n    while True:\r\n        with sr.Microphone() as source:\r\n         \
    \   r.adjust_for_ambient_noise(source)\r\n            print(\"Listening...\")\r\
    \n            try:\r\n                audio = r.listen(source)\r\n           \
    \     command = r.recognize_google(audio)\r\n                print(\"You said:\"\
    , command)\r\n                response = generate_response(command)\r\n      \
    \          print(\"Chatbot:\", response)\r\n                speak(response)\r\n\
    \            except sr.UnknownValueError:\r\n                pass\r\n        \
    \    except sr.RequestError as e:\r\n                print(\"Could not request\
    \ results from Google Speech Recognition service:\", str(e))\r\n\r\ndef speak(text):\r\
    \n    tts_engine = pyttsx3.init()\r\n    tts_engine.say(text)\r\n    tts_engine.runAndWait()\r\
    \n\r\nif __name__ == \"__main__\":\r\n    listen_to_command()\r\n\r\n```"
  created_at: 2023-11-09 21:50:00+00:00
  edited: false
  hidden: false
  id: 654d54081e6216cb2d8c2020
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: rwl4/gpt2-medium-chat
repo_type: model
status: open
target_branch: null
title: how do i get it to not repeat what i said on answer?
