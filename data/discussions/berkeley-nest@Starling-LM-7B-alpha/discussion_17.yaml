!!python/object:huggingface_hub.community.DiscussionWithDetails
author: toranb
conflicting_files: null
created_at: 2023-12-02 16:39:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679252986201-noauth.png?w=200&h=200&f=face
      fullname: Toran Billups
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: toranb
      type: user
    createdAt: '2023-12-02T16:39:33.000Z'
    data:
      edited: true
      editors:
      - toranb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8912336826324463
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679252986201-noauth.png?w=200&h=200&f=face
          fullname: Toran Billups
          isHf: false
          isPro: false
          name: toranb
          type: user
        html: '<p>I''ve had luck fine tuning Zephyr and other fine tunes of Mistral
          with transformers 4.35.2 but Starling throws an error related to vocab mismatch
          (likely because I''m using Mistral)</p>

          <p><code>shape ''[-1, 32000]'' is invalid for input of size 19457216</code></p>

          <p>this originates from <code>transformers/models/mistral/modeling_mistral.py</code>
          line 1032 for those interested.<br><code>shift_logits.view(-1, self.config.vocab_size)</code></p>

          <p>Does anyone know of a workaround until we have 1st class support in transformers?</p>

          '
        raw: "I've had luck fine tuning Zephyr and other fine tunes of Mistral with\
          \ transformers 4.35.2 but Starling throws an error related to vocab mismatch\
          \ (likely because I'm using Mistral)\n\n```shape '[-1, 32000]' is invalid\
          \ for input of size 19457216```\n\nthis originates from `transformers/models/mistral/modeling_mistral.py`\
          \ line 1032 for those interested. \n```shift_logits.view(-1, self.config.vocab_size)```\n\
          \nDoes anyone know of a workaround until we have 1st class support in transformers?"
        updatedAt: '2023-12-02T19:43:43.853Z'
      numEdits: 2
      reactions: []
    id: 656b5dc5c56388a985c62108
    type: comment
  author: toranb
  content: "I've had luck fine tuning Zephyr and other fine tunes of Mistral with\
    \ transformers 4.35.2 but Starling throws an error related to vocab mismatch (likely\
    \ because I'm using Mistral)\n\n```shape '[-1, 32000]' is invalid for input of\
    \ size 19457216```\n\nthis originates from `transformers/models/mistral/modeling_mistral.py`\
    \ line 1032 for those interested. \n```shift_logits.view(-1, self.config.vocab_size)```\n\
    \nDoes anyone know of a workaround until we have 1st class support in transformers?"
  created_at: 2023-12-02 16:39:33+00:00
  edited: true
  hidden: false
  id: 656b5dc5c56388a985c62108
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8b1419f999f31ce3fdcb8ad994b5351.svg
      fullname: MB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: MB7977
      type: user
    createdAt: '2023-12-03T04:40:42.000Z'
    data:
      edited: false
      editors:
      - MB7977
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9450392723083496
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8b1419f999f31ce3fdcb8ad994b5351.svg
          fullname: MB
          isHf: false
          isPro: true
          name: MB7977
          type: user
        html: "<p>For what it\u2019s worth I\u2019ve successfully done a full fine\
          \ tune on Starling with Transformers 4.35.2 (with Axolotl). Are you perhaps\
          \ adding tokens and changing the vocab size? I trained with the OpenChat\
          \ prompt format and stuck with the default EOS, BOS tokens etc so no added\
          \ tokens were necessary. I think the openchat.json file may also be relevant?</p>\n"
        raw: "For what it\u2019s worth I\u2019ve successfully done a full fine tune\
          \ on Starling with Transformers 4.35.2 (with Axolotl). Are you perhaps adding\
          \ tokens and changing the vocab size? I trained with the OpenChat prompt\
          \ format and stuck with the default EOS, BOS tokens etc so no added tokens\
          \ were necessary. I think the openchat.json file may also be relevant?"
        updatedAt: '2023-12-03T04:40:42.598Z'
      numEdits: 0
      reactions: []
    id: 656c06cafe7fe0b1e9dd68b3
    type: comment
  author: MB7977
  content: "For what it\u2019s worth I\u2019ve successfully done a full fine tune\
    \ on Starling with Transformers 4.35.2 (with Axolotl). Are you perhaps adding\
    \ tokens and changing the vocab size? I trained with the OpenChat prompt format\
    \ and stuck with the default EOS, BOS tokens etc so no added tokens were necessary.\
    \ I think the openchat.json file may also be relevant?"
  created_at: 2023-12-03 04:40:42+00:00
  edited: false
  hidden: false
  id: 656c06cafe7fe0b1e9dd68b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679252986201-noauth.png?w=200&h=200&f=face
      fullname: Toran Billups
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: toranb
      type: user
    createdAt: '2023-12-03T19:32:53.000Z'
    data:
      edited: false
      editors:
      - toranb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8362633585929871
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679252986201-noauth.png?w=200&h=200&f=face
          fullname: Toran Billups
          isHf: false
          isPro: false
          name: toranb
          type: user
        html: '<p>When do you use the openchat.json file? I didn''t even pull that
          down ahead of converting weights/fine tuning so I''m curious to learn more</p>

          <p><a href="https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/blob/main/openchat.json">https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/blob/main/openchat.json</a></p>

          '
        raw: 'When do you use the openchat.json file? I didn''t even pull that down
          ahead of converting weights/fine tuning so I''m curious to learn more


          https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/blob/main/openchat.json'
        updatedAt: '2023-12-03T19:32:53.273Z'
      numEdits: 0
      reactions: []
    id: 656cd7e57825b310101ed290
    type: comment
  author: toranb
  content: 'When do you use the openchat.json file? I didn''t even pull that down
    ahead of converting weights/fine tuning so I''m curious to learn more


    https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/blob/main/openchat.json'
  created_at: 2023-12-03 19:32:53+00:00
  edited: false
  hidden: false
  id: 656cd7e57825b310101ed290
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8b1419f999f31ce3fdcb8ad994b5351.svg
      fullname: MB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: MB7977
      type: user
    createdAt: '2023-12-04T00:29:31.000Z'
    data:
      edited: false
      editors:
      - MB7977
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9769906997680664
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8b1419f999f31ce3fdcb8ad994b5351.svg
          fullname: MB
          isHf: false
          isPro: true
          name: MB7977
          type: user
        html: '<p>I may be wrong about that. It was one of a couple of files added
          after the fact that seemed to fix early training issues, but that one seems
          more related to compatibility with the OpenChat API. I''m working with Transformers
          indirectly, via Axolotl, so it''s difficult to tease out why it''s working
          in my instance versus yours.  The OpenChat 3.5 format used by Starling adds
          a couple of tokens to the vocabulary that I suspect are the source of your
          issues. Hopefully the devs can help.</p>

          '
        raw: I may be wrong about that. It was one of a couple of files added after
          the fact that seemed to fix early training issues, but that one seems more
          related to compatibility with the OpenChat API. I'm working with Transformers
          indirectly, via Axolotl, so it's difficult to tease out why it's working
          in my instance versus yours.  The OpenChat 3.5 format used by Starling adds
          a couple of tokens to the vocabulary that I suspect are the source of your
          issues. Hopefully the devs can help.
        updatedAt: '2023-12-04T00:29:31.266Z'
      numEdits: 0
      reactions: []
    id: 656d1d6befd0eea7c5c77f93
    type: comment
  author: MB7977
  content: I may be wrong about that. It was one of a couple of files added after
    the fact that seemed to fix early training issues, but that one seems more related
    to compatibility with the OpenChat API. I'm working with Transformers indirectly,
    via Axolotl, so it's difficult to tease out why it's working in my instance versus
    yours.  The OpenChat 3.5 format used by Starling adds a couple of tokens to the
    vocabulary that I suspect are the source of your issues. Hopefully the devs can
    help.
  created_at: 2023-12-04 00:29:31+00:00
  edited: false
  hidden: false
  id: 656d1d6befd0eea7c5c77f93
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: berkeley-nest/Starling-LM-7B-alpha
repo_type: model
status: open
target_branch: null
title: Fine tuning with transformers?
