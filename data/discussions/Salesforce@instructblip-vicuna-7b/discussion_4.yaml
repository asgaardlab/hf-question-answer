!!python/object:huggingface_hub.community.DiscussionWithDetails
author: handing2412
conflicting_files: null
created_at: 2023-06-28 22:07:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b50841f6ddd066ecec112ac09bca9602.svg
      fullname: Han Ding
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: handing2412
      type: user
    createdAt: '2023-06-28T23:07:56.000Z'
    data:
      edited: false
      editors:
      - handing2412
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.43782326579093933
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b50841f6ddd066ecec112ac09bca9602.svg
          fullname: Han Ding
          isHf: false
          isPro: false
          name: handing2412
          type: user
        html: '<h2 id="i-got-the-following-error-when-i-load-processor-i-assume-its-during-load-tokenizer-from-pretrain">I
          got the following error when i load processor, I assume it''s during load
          tokenizer from pretrain.</h2>

          <p>Exception                                 Traceback (most recent call
          last)<br>Cell In[2], line 7<br>      4 import requests<br>      6 model
          = InstructBlipForConditionalGeneration.from_pretrained("Salesforce/instructblip-vicuna-7b")<br>----&gt;
          7 processor = InstructBlipProcessor.from_pretrained("Salesforce/instructblip-vicuna-7b")<br>      9
          device = "cuda" if torch.cuda.is_available() else "cpu"<br>     10 model.to(device)</p>

          <p>File ~/.local/lib/python3.9/site-packages/transformers/models/instructblip/processing_instructblip.py:170,
          in InstructBlipProcessor.from_pretrained(cls, pretrained_model_name_or_path,
          **kwargs)<br>    167 @classmethod<br>    168 def from_pretrained(cls, pretrained_model_name_or_path,
          **kwargs):<br>    169     qformer_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,
          subfolder="qformer_tokenizer")<br>--&gt; 170     args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path,
          **kwargs)<br>    171     args.append(qformer_tokenizer)<br>    172     return
          cls(*args)</p>

          <p>File ~/.local/lib/python3.9/site-packages/transformers/processing_utils.py:259,
          in ProcessorMixin._get_arguments_from_pretrained(cls, pretrained_model_name_or_path,
          **kwargs)<br>    256     else:<br>    257         attribute_class = getattr(transformers_module,
          class_name)<br>--&gt; 259     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path,
          **kwargs))<br>    260 return args</p>

          <p>File ~/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:692,
          in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,
          **kwargs)<br>    688     if tokenizer_class is None:<br>    689         raise
          ValueError(<br>    690             f"Tokenizer class {tokenizer_class_candidate}
          does not exist or is not currently imported."<br>    691         )<br>--&gt;
          692     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,
          *inputs, **kwargs)<br>    694 # Otherwise we have to be creative.<br>    695
          # if model is an encoder decoder, the encoder tokenizer class is used by
          default<br>    696 if isinstance(config, EncoderDecoderConfig):</p>

          <p>File ~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1846,
          in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,
          cache_dir, force_download, local_files_only, token, revision, *init_inputs,
          **kwargs)<br>   1843     else:<br>   1844         logger.info(f"loading
          file {file_path} from cache at {resolved_vocab_files[file_id]}")<br>-&gt;
          1846 return cls._from_pretrained(<br>   1847     resolved_vocab_files,<br>   1848     pretrained_model_name_or_path,<br>   1849     init_configuration,<br>   1850     *init_inputs,<br>   1851     use_auth_token=token,<br>   1852     cache_dir=cache_dir,<br>   1853     local_files_only=local_files_only,<br>   1854     _commit_hash=commit_hash,<br>   1855     _is_local=is_local,<br>   1856     **kwargs,<br>   1857
          )</p>

          <p>File ~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2009,
          in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,
          init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,
          _is_local, *init_inputs, **kwargs)<br>   2007 # Instantiate tokenizer.<br>   2008
          try:<br>-&gt; 2009     tokenizer = cls(*init_inputs, **init_kwargs)<br>   2010
          except OSError:<br>   2011     raise OSError(<br>   2012         "Unable
          to load vocabulary from file. "<br>   2013         "Please check that the
          provided vocabulary is accessible and not corrupted."<br>   2014     )</p>

          <p>File ~/.local/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py:100,
          in LlamaTokenizerFast.<strong>init</strong>(self, vocab_file, tokenizer_file,
          clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token,
          add_eos_token, **kwargs)<br>     88 def <strong>init</strong>(<br>     89     self,<br>     90     vocab_file=None,<br>   (...)<br>     98     **kwargs,<br>     99
          ):<br>--&gt; 100     super().<strong>init</strong>(<br>    101         vocab_file=vocab_file,<br>    102         tokenizer_file=tokenizer_file,<br>    103         clean_up_tokenization_spaces=clean_up_tokenization_spaces,<br>    104         unk_token=unk_token,<br>    105         bos_token=bos_token,<br>    106         eos_token=eos_token,<br>    107         **kwargs,<br>    108     )<br>    109     self._add_bos_token
          = add_bos_token<br>    110     self._add_eos_token = add_eos_token</p>

          <p>File ~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:111,
          in PreTrainedTokenizerFast.<strong>init</strong>(self, *args, **kwargs)<br>    108     fast_tokenizer
          = copy.deepcopy(tokenizer_object)<br>    109 elif fast_tokenizer_file is
          not None and not from_slow:<br>    110     # We have a serialization from
          tokenizers which let us directly build the backend<br>--&gt; 111     fast_tokenizer
          = TokenizerFast.from_file(fast_tokenizer_file)<br>    112 elif slow_tokenizer
          is not None:<br>    113     # We need to convert a slow tokenizer to build
          the backend<br>    114     fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)</p>

          <p>Exception: data did not match any variant of untagged enum PyNormalizerTypeWrapper
          at line 58 column 3</p>

          '
        raw: "I got the following error when i load processor, I assume it's during\
          \ load tokenizer from pretrain.\r\n---------------------------------------------------------------------------\r\
          \nException                                 Traceback (most recent call\
          \ last)\r\nCell In[2], line 7\r\n      4 import requests\r\n      6 model\
          \ = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\"\
          )\r\n----> 7 processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\"\
          )\r\n      9 device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\
          \r\n     10 model.to(device)\r\n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/models/instructblip/processing_instructblip.py:170,\
          \ in InstructBlipProcessor.from_pretrained(cls, pretrained_model_name_or_path,\
          \ **kwargs)\r\n    167 @classmethod\r\n    168 def from_pretrained(cls,\
          \ pretrained_model_name_or_path, **kwargs):\r\n    169     qformer_tokenizer\
          \ = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"\
          qformer_tokenizer\")\r\n--> 170     args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\r\n    171     args.append(qformer_tokenizer)\r\n    172   \
          \  return cls(*args)\r\n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/processing_utils.py:259,\
          \ in ProcessorMixin._get_arguments_from_pretrained(cls, pretrained_model_name_or_path,\
          \ **kwargs)\r\n    256     else:\r\n    257         attribute_class = getattr(transformers_module,\
          \ class_name)\r\n--> 259     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs))\r\n    260 return args\r\n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:692,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\r\n    688     if tokenizer_class is None:\r\n    689      \
          \   raise ValueError(\r\n    690             f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\r\n    691         )\r\n\
          --> 692     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    694 # Otherwise we have to be creative.\r\n\
          \    695 # if model is an encoder decoder, the encoder tokenizer class is\
          \ used by default\r\n    696 if isinstance(config, EncoderDecoderConfig):\r\
          \n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1846,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\r\n   1843     else:\r\n   1844         logger.info(f\"loading\
          \ file {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n\
          -> 1846 return cls._from_pretrained(\r\n   1847     resolved_vocab_files,\r\
          \n   1848     pretrained_model_name_or_path,\r\n   1849     init_configuration,\r\
          \n   1850     *init_inputs,\r\n   1851     use_auth_token=token,\r\n   1852\
          \     cache_dir=cache_dir,\r\n   1853     local_files_only=local_files_only,\r\
          \n   1854     _commit_hash=commit_hash,\r\n   1855     _is_local=is_local,\r\
          \n   1856     **kwargs,\r\n   1857 )\r\n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2009,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir,\
          \ local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\r\n\
          \   2007 # Instantiate tokenizer.\r\n   2008 try:\r\n-> 2009     tokenizer\
          \ = cls(*init_inputs, **init_kwargs)\r\n   2010 except OSError:\r\n   2011\
          \     raise OSError(\r\n   2012         \"Unable to load vocabulary from\
          \ file. \"\r\n   2013         \"Please check that the provided vocabulary\
          \ is accessible and not corrupted.\"\r\n   2014     )\r\n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py:100,\
          \ in LlamaTokenizerFast.__init__(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces,\
          \ unk_token, bos_token, eos_token, add_bos_token, add_eos_token, **kwargs)\r\
          \n     88 def __init__(\r\n     89     self,\r\n     90     vocab_file=None,\r\
          \n   (...)\r\n     98     **kwargs,\r\n     99 ):\r\n--> 100     super().__init__(\r\
          \n    101         vocab_file=vocab_file,\r\n    102         tokenizer_file=tokenizer_file,\r\
          \n    103         clean_up_tokenization_spaces=clean_up_tokenization_spaces,\r\
          \n    104         unk_token=unk_token,\r\n    105         bos_token=bos_token,\r\
          \n    106         eos_token=eos_token,\r\n    107         **kwargs,\r\n\
          \    108     )\r\n    109     self._add_bos_token = add_bos_token\r\n  \
          \  110     self._add_eos_token = add_eos_token\r\n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:111,\
          \ in PreTrainedTokenizerFast.__init__(self, *args, **kwargs)\r\n    108\
          \     fast_tokenizer = copy.deepcopy(tokenizer_object)\r\n    109 elif fast_tokenizer_file\
          \ is not None and not from_slow:\r\n    110     # We have a serialization\
          \ from tokenizers which let us directly build the backend\r\n--> 111   \
          \  fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\r\n   \
          \ 112 elif slow_tokenizer is not None:\r\n    113     # We need to convert\
          \ a slow tokenizer to build the backend\r\n    114     fast_tokenizer =\
          \ convert_slow_tokenizer(slow_tokenizer)\r\n\r\nException: data did not\
          \ match any variant of untagged enum PyNormalizerTypeWrapper at line 58\
          \ column 3"
        updatedAt: '2023-06-28T23:07:56.759Z'
      numEdits: 0
      reactions: []
    id: 649cbd4c4cc1419329415763
    type: comment
  author: handing2412
  content: "I got the following error when i load processor, I assume it's during\
    \ load tokenizer from pretrain.\r\n---------------------------------------------------------------------------\r\
    \nException                                 Traceback (most recent call last)\r\
    \nCell In[2], line 7\r\n      4 import requests\r\n      6 model = InstructBlipForConditionalGeneration.from_pretrained(\"\
    Salesforce/instructblip-vicuna-7b\")\r\n----> 7 processor = InstructBlipProcessor.from_pretrained(\"\
    Salesforce/instructblip-vicuna-7b\")\r\n      9 device = \"cuda\" if torch.cuda.is_available()\
    \ else \"cpu\"\r\n     10 model.to(device)\r\n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/models/instructblip/processing_instructblip.py:170,\
    \ in InstructBlipProcessor.from_pretrained(cls, pretrained_model_name_or_path,\
    \ **kwargs)\r\n    167 @classmethod\r\n    168 def from_pretrained(cls, pretrained_model_name_or_path,\
    \ **kwargs):\r\n    169     qformer_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,\
    \ subfolder=\"qformer_tokenizer\")\r\n--> 170     args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs)\r\n    171     args.append(qformer_tokenizer)\r\n    172     return\
    \ cls(*args)\r\n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/processing_utils.py:259,\
    \ in ProcessorMixin._get_arguments_from_pretrained(cls, pretrained_model_name_or_path,\
    \ **kwargs)\r\n    256     else:\r\n    257         attribute_class = getattr(transformers_module,\
    \ class_name)\r\n--> 259     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs))\r\n    260 return args\r\n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:692,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    688     if tokenizer_class is None:\r\n    689         raise\
    \ ValueError(\r\n    690             f\"Tokenizer class {tokenizer_class_candidate}\
    \ does not exist or is not currently imported.\"\r\n    691         )\r\n--> 692\
    \     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    694 # Otherwise we have to be creative.\r\n    695 # if model\
    \ is an encoder decoder, the encoder tokenizer class is used by default\r\n  \
    \  696 if isinstance(config, EncoderDecoderConfig):\r\n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1846,\
    \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
    \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
    \ **kwargs)\r\n   1843     else:\r\n   1844         logger.info(f\"loading file\
    \ {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n-> 1846 return\
    \ cls._from_pretrained(\r\n   1847     resolved_vocab_files,\r\n   1848     pretrained_model_name_or_path,\r\
    \n   1849     init_configuration,\r\n   1850     *init_inputs,\r\n   1851    \
    \ use_auth_token=token,\r\n   1852     cache_dir=cache_dir,\r\n   1853     local_files_only=local_files_only,\r\
    \n   1854     _commit_hash=commit_hash,\r\n   1855     _is_local=is_local,\r\n\
    \   1856     **kwargs,\r\n   1857 )\r\n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2009,\
    \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
    \ _is_local, *init_inputs, **kwargs)\r\n   2007 # Instantiate tokenizer.\r\n \
    \  2008 try:\r\n-> 2009     tokenizer = cls(*init_inputs, **init_kwargs)\r\n \
    \  2010 except OSError:\r\n   2011     raise OSError(\r\n   2012         \"Unable\
    \ to load vocabulary from file. \"\r\n   2013         \"Please check that the\
    \ provided vocabulary is accessible and not corrupted.\"\r\n   2014     )\r\n\r\
    \nFile ~/.local/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py:100,\
    \ in LlamaTokenizerFast.__init__(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces,\
    \ unk_token, bos_token, eos_token, add_bos_token, add_eos_token, **kwargs)\r\n\
    \     88 def __init__(\r\n     89     self,\r\n     90     vocab_file=None,\r\n\
    \   (...)\r\n     98     **kwargs,\r\n     99 ):\r\n--> 100     super().__init__(\r\
    \n    101         vocab_file=vocab_file,\r\n    102         tokenizer_file=tokenizer_file,\r\
    \n    103         clean_up_tokenization_spaces=clean_up_tokenization_spaces,\r\
    \n    104         unk_token=unk_token,\r\n    105         bos_token=bos_token,\r\
    \n    106         eos_token=eos_token,\r\n    107         **kwargs,\r\n    108\
    \     )\r\n    109     self._add_bos_token = add_bos_token\r\n    110     self._add_eos_token\
    \ = add_eos_token\r\n\r\nFile ~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:111,\
    \ in PreTrainedTokenizerFast.__init__(self, *args, **kwargs)\r\n    108     fast_tokenizer\
    \ = copy.deepcopy(tokenizer_object)\r\n    109 elif fast_tokenizer_file is not\
    \ None and not from_slow:\r\n    110     # We have a serialization from tokenizers\
    \ which let us directly build the backend\r\n--> 111     fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\r\
    \n    112 elif slow_tokenizer is not None:\r\n    113     # We need to convert\
    \ a slow tokenizer to build the backend\r\n    114     fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\r\
    \n\r\nException: data did not match any variant of untagged enum PyNormalizerTypeWrapper\
    \ at line 58 column 3"
  created_at: 2023-06-28 22:07:56+00:00
  edited: false
  hidden: false
  id: 649cbd4c4cc1419329415763
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-09T16:16:44.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8353654742240906
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;handing2412&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/handing2412\"\
          >@<span class=\"underline\">handing2412</span></a></span>\n\n\t</span></span><br>Thanks\
          \ for the issue, can you try to re-run your snippet on the latest transformers\
          \ and tokenizers versions?</p>\n<pre><code class=\"language-bash\">pip install\
          \ -U transformers tokenizers\n</code></pre>\n"
        raw: "Hi @handing2412 \nThanks for the issue, can you try to re-run your snippet\
          \ on the latest transformers and tokenizers versions?\n```bash\npip install\
          \ -U transformers tokenizers\n```"
        updatedAt: '2023-10-09T16:16:44.178Z'
      numEdits: 0
      reactions: []
    id: 6524276ce6e5f6b10337ab30
    type: comment
  author: ybelkada
  content: "Hi @handing2412 \nThanks for the issue, can you try to re-run your snippet\
    \ on the latest transformers and tokenizers versions?\n```bash\npip install -U\
    \ transformers tokenizers\n```"
  created_at: 2023-10-09 15:16:44+00:00
  edited: false
  hidden: false
  id: 6524276ce6e5f6b10337ab30
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Salesforce/instructblip-vicuna-7b
repo_type: model
status: open
target_branch: null
title: 'processor = InstructBlipProcessor.from_pretrained("Salesforce/instructblip-vicuna-7b")
  got error '
