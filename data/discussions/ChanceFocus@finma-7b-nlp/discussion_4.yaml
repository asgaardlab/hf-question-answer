!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ghbacct
conflicting_files: null
created_at: 2023-07-06 21:50:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37f672ace2a9dd4fcadf46148861215d.svg
      fullname: ghbacct
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ghbacct
      type: user
    createdAt: '2023-07-06T22:50:49.000Z'
    data:
      edited: false
      editors:
      - ghbacct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3614073097705841
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37f672ace2a9dd4fcadf46148861215d.svg
          fullname: ghbacct
          isHf: false
          isPro: false
          name: ghbacct
          type: user
        html: "<p>Hi, </p>\n<p>When running the tokenizer loading code as per documentation\
          \ I run into a recursion error.</p>\n<pre><code class=\"language----------------------------------------------------------------------------\"\
          >RecursionError                            Traceback (most recent call last)\n\
          /tmp/ipykernel_510781/3877109626.py in ()\n      2 from transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\n      3 \n----&gt; 4 tokenizer =\
          \ AutoTokenizer.from_pretrained(\"ChanceFocus/finma-7b-nlp\")\n      5 model\
          \ = AutoModelForCausalLM.from_pretrained(\"ChanceFocus/finma-7b-nlp\")\n\
          ~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\n\
          \    689                     f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\n    690              \
          \   )\n--&gt; 691             return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n    692 \n    693         # Otherwise we have to be\
          \ creative.\n\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\n\
          \   1823                 logger.info(f\"loading file {file_path} from cache\
          \ at {resolved_vocab_files[file_id]}\")\n   1824 \n-&gt; 1825         return\
          \ cls._from_pretrained(\n   1826             resolved_vocab_files,\n   1827\
          \             pretrained_model_name_or_path,\n\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
          \ in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
          \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
          \ _is_local, *init_inputs, **kwargs)\n   1986         # Instantiate tokenizer.\n\
          \   1987         try:\n-&gt; 1988             tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\n   1989         except OSError:\n   1990             raise\
          \ OSError(\n\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama_fast.py\
          \ in __init__(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces,\
          \ unk_token, bos_token, eos_token, add_bos_token, add_eos_token, **kwargs)\n\
          \    102         self._add_bos_token = add_bos_token\n    103         self._add_eos_token\
          \ = add_eos_token\n--&gt; 104         self.update_post_processor()\n   \
          \ 105 \n    106         self.vocab_file = vocab_file\n\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama_fast.py\
          \ in update_post_processor(self)\n    109     def update_post_processor(self):\n\
          \    110         bos = self.bos_token\n--&gt; 111         bos_token_id =\
          \ self.bos_token_id\n    112 \n    113         eos = self.eos_token\n\n\
          ~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
          \ in bos_token_id(self)\n   1134         if self._bos_token is None:\n \
          \  1135             return None\n-&gt; 1136         return self.convert_tokens_to_ids(self.bos_token)\n\
          \   1137 \n   1138     @property\n\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\
          \ in convert_tokens_to_ids(self, tokens)\n    248 \n    249         if isinstance(tokens,\
          \ str):\n--&gt; 250             return self._convert_token_to_id_with_added_voc(tokens)\n\
          \    251 \n    252         return [self._convert_token_to_id_with_added_voc(token)\
          \ for token in tokens]\n\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\
          \ in _convert_token_to_id_with_added_voc(self, token)\n    255         index\
          \ = self._tokenizer.token_to_id(token)\n    256         if index is None:\n\
          --&gt; 257             return self.unk_token_id\n    258         return\
          \ index\n    259 \n\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
          \ in unk_token_id(self)\n   1153         if self._unk_token is None:\n \
          \  1154             return None\n-&gt; 1155         return self.convert_tokens_to_ids(self.unk_token)\n\
          \   1156 \n   1157     @property\n\n... last 3 frames repeated, from the\
          \ frame below ...\n\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\
          \ in convert_tokens_to_ids(self, tokens)\n    248 \n    249         if isinstance(tokens,\
          \ str):\n--&gt; 250             return self._convert_token_to_id_with_added_voc(tokens)\n\
          \    251 \n    252         return [self._convert_token_to_id_with_added_voc(token)\
          \ for token in tokens]\n\nRecursionError: maximum recursion depth exceeded\
          \ while getting the str of an object\n</code></pre>\n<p>Please help.</p>\n"
        raw: "Hi, \r\n\r\nWhen running the tokenizer loading code as per documentation\
          \ I run into a recursion error.\r\n\r\n```---------------------------------------------------------------------------\r\
          \nRecursionError                            Traceback (most recent call\
          \ last)\r\n/tmp/ipykernel_510781/3877109626.py in ()\r\n      2 from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\n      3 \r\n----> 4 tokenizer\
          \ = AutoTokenizer.from_pretrained(\"ChanceFocus/finma-7b-nlp\")\r\n    \
          \  5 model = AutoModelForCausalLM.from_pretrained(\"ChanceFocus/finma-7b-nlp\"\
          )\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\r\
          \n    689                     f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\r\n    690            \
          \     )\r\n--> 691             return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    692 \r\n    693         # Otherwise we have\
          \ to be creative.\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\r\
          \n   1823                 logger.info(f\"loading file {file_path} from cache\
          \ at {resolved_vocab_files[file_id]}\")\r\n   1824 \r\n-> 1825         return\
          \ cls._from_pretrained(\r\n   1826             resolved_vocab_files,\r\n\
          \   1827             pretrained_model_name_or_path,\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
          \ in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
          \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
          \ _is_local, *init_inputs, **kwargs)\r\n   1986         # Instantiate tokenizer.\r\
          \n   1987         try:\r\n-> 1988             tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\r\n   1989         except OSError:\r\n   1990         \
          \    raise OSError(\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama_fast.py\
          \ in __init__(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces,\
          \ unk_token, bos_token, eos_token, add_bos_token, add_eos_token, **kwargs)\r\
          \n    102         self._add_bos_token = add_bos_token\r\n    103       \
          \  self._add_eos_token = add_eos_token\r\n--> 104         self.update_post_processor()\r\
          \n    105 \r\n    106         self.vocab_file = vocab_file\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama_fast.py\
          \ in update_post_processor(self)\r\n    109     def update_post_processor(self):\r\
          \n    110         bos = self.bos_token\r\n--> 111         bos_token_id =\
          \ self.bos_token_id\r\n    112 \r\n    113         eos = self.eos_token\r\
          \n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
          \ in bos_token_id(self)\r\n   1134         if self._bos_token is None:\r\
          \n   1135             return None\r\n-> 1136         return self.convert_tokens_to_ids(self.bos_token)\r\
          \n   1137 \r\n   1138     @property\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\
          \ in convert_tokens_to_ids(self, tokens)\r\n    248 \r\n    249        \
          \ if isinstance(tokens, str):\r\n--> 250             return self._convert_token_to_id_with_added_voc(tokens)\r\
          \n    251 \r\n    252         return [self._convert_token_to_id_with_added_voc(token)\
          \ for token in tokens]\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\
          \ in _convert_token_to_id_with_added_voc(self, token)\r\n    255       \
          \  index = self._tokenizer.token_to_id(token)\r\n    256         if index\
          \ is None:\r\n--> 257             return self.unk_token_id\r\n    258  \
          \       return index\r\n    259 \r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
          \ in unk_token_id(self)\r\n   1153         if self._unk_token is None:\r\
          \n   1154             return None\r\n-> 1155         return self.convert_tokens_to_ids(self.unk_token)\r\
          \n   1156 \r\n   1157     @property\r\n\r\n... last 3 frames repeated, from\
          \ the frame below ...\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\
          \ in convert_tokens_to_ids(self, tokens)\r\n    248 \r\n    249        \
          \ if isinstance(tokens, str):\r\n--> 250             return self._convert_token_to_id_with_added_voc(tokens)\r\
          \n    251 \r\n    252         return [self._convert_token_to_id_with_added_voc(token)\
          \ for token in tokens]\r\n\r\nRecursionError: maximum recursion depth exceeded\
          \ while getting the str of an object\r\n```\r\n\r\nPlease help."
        updatedAt: '2023-07-06T22:50:49.179Z'
      numEdits: 0
      reactions: []
    id: 64a74549166b638e8ee75497
    type: comment
  author: ghbacct
  content: "Hi, \r\n\r\nWhen running the tokenizer loading code as per documentation\
    \ I run into a recursion error.\r\n\r\n```---------------------------------------------------------------------------\r\
    \nRecursionError                            Traceback (most recent call last)\r\
    \n/tmp/ipykernel_510781/3877109626.py in ()\r\n      2 from transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\r\n      3 \r\n----> 4 tokenizer = AutoTokenizer.from_pretrained(\"\
    ChanceFocus/finma-7b-nlp\")\r\n      5 model = AutoModelForCausalLM.from_pretrained(\"\
    ChanceFocus/finma-7b-nlp\")\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\r\n\
    \    689                     f\"Tokenizer class {tokenizer_class_candidate} does\
    \ not exist or is not currently imported.\"\r\n    690                 )\r\n-->\
    \ 691             return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n    692 \r\n    693         # Otherwise we have to be\
    \ creative.\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\r\
    \n   1823                 logger.info(f\"loading file {file_path} from cache at\
    \ {resolved_vocab_files[file_id]}\")\r\n   1824 \r\n-> 1825         return cls._from_pretrained(\r\
    \n   1826             resolved_vocab_files,\r\n   1827             pretrained_model_name_or_path,\r\
    \n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
    \ in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
    \ _is_local, *init_inputs, **kwargs)\r\n   1986         # Instantiate tokenizer.\r\
    \n   1987         try:\r\n-> 1988             tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n   1989         except OSError:\r\n   1990             raise OSError(\r\n\r\n\
    ~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama_fast.py\
    \ in __init__(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces,\
    \ unk_token, bos_token, eos_token, add_bos_token, add_eos_token, **kwargs)\r\n\
    \    102         self._add_bos_token = add_bos_token\r\n    103         self._add_eos_token\
    \ = add_eos_token\r\n--> 104         self.update_post_processor()\r\n    105 \r\
    \n    106         self.vocab_file = vocab_file\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/models/llama/tokenization_llama_fast.py\
    \ in update_post_processor(self)\r\n    109     def update_post_processor(self):\r\
    \n    110         bos = self.bos_token\r\n--> 111         bos_token_id = self.bos_token_id\r\
    \n    112 \r\n    113         eos = self.eos_token\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
    \ in bos_token_id(self)\r\n   1134         if self._bos_token is None:\r\n   1135\
    \             return None\r\n-> 1136         return self.convert_tokens_to_ids(self.bos_token)\r\
    \n   1137 \r\n   1138     @property\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\
    \ in convert_tokens_to_ids(self, tokens)\r\n    248 \r\n    249         if isinstance(tokens,\
    \ str):\r\n--> 250             return self._convert_token_to_id_with_added_voc(tokens)\r\
    \n    251 \r\n    252         return [self._convert_token_to_id_with_added_voc(token)\
    \ for token in tokens]\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\
    \ in _convert_token_to_id_with_added_voc(self, token)\r\n    255         index\
    \ = self._tokenizer.token_to_id(token)\r\n    256         if index is None:\r\n\
    --> 257             return self.unk_token_id\r\n    258         return index\r\
    \n    259 \r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\
    \ in unk_token_id(self)\r\n   1153         if self._unk_token is None:\r\n   1154\
    \             return None\r\n-> 1155         return self.convert_tokens_to_ids(self.unk_token)\r\
    \n   1156 \r\n   1157     @property\r\n\r\n... last 3 frames repeated, from the\
    \ frame below ...\r\n\r\n~/Dev/finma-experiments/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\
    \ in convert_tokens_to_ids(self, tokens)\r\n    248 \r\n    249         if isinstance(tokens,\
    \ str):\r\n--> 250             return self._convert_token_to_id_with_added_voc(tokens)\r\
    \n    251 \r\n    252         return [self._convert_token_to_id_with_added_voc(token)\
    \ for token in tokens]\r\n\r\nRecursionError: maximum recursion depth exceeded\
    \ while getting the str of an object\r\n```\r\n\r\nPlease help."
  created_at: 2023-07-06 21:50:49+00:00
  edited: false
  hidden: false
  id: 64a74549166b638e8ee75497
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672842917861-noauth.jpeg?w=200&h=200&f=face
      fullname: Jimin Huang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jiminHuang
      type: user
    createdAt: '2023-07-06T23:29:11.000Z'
    data:
      edited: false
      editors:
      - jiminHuang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8117968440055847
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672842917861-noauth.jpeg?w=200&h=200&f=face
          fullname: Jimin Huang
          isHf: false
          isPro: false
          name: jiminHuang
          type: user
        html: '<p>Hi,</p>

          <p>Firstly, I am grateful for your interest in our work.</p>

          <p>You can follow the guidelines provided on the model card to load the
          model. Here''s the Python code for your reference:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> LlamaTokenizer, LlamaForCausalLM


          tokenizer = LlamaTokenizer.from_pretrained(<span class="hljs-string">''ChanceFocus/finma-7b-nlp''</span>)

          model = LlamaForCausalLM.from_pretrained(<span class="hljs-string">''ChanceFocus/finma-7b-nlp''</span>,
          device_map=<span class="hljs-string">''auto''</span>)

          </code></pre>

          <p>If executing this code doesn''t resolve your issue, please don''t hesitate
          to comment!</p>

          '
        raw: 'Hi,


          Firstly, I am grateful for your interest in our work.


          You can follow the guidelines provided on the model card to load the model.
          Here''s the Python code for your reference:


          ```python

          from transformers import LlamaTokenizer, LlamaForCausalLM


          tokenizer = LlamaTokenizer.from_pretrained(''ChanceFocus/finma-7b-nlp'')

          model = LlamaForCausalLM.from_pretrained(''ChanceFocus/finma-7b-nlp'', device_map=''auto'')

          ```


          If executing this code doesn''t resolve your issue, please don''t hesitate
          to comment!'
        updatedAt: '2023-07-06T23:29:11.171Z'
      numEdits: 0
      reactions: []
    id: 64a74e47a194220d66c0f349
    type: comment
  author: jiminHuang
  content: 'Hi,


    Firstly, I am grateful for your interest in our work.


    You can follow the guidelines provided on the model card to load the model. Here''s
    the Python code for your reference:


    ```python

    from transformers import LlamaTokenizer, LlamaForCausalLM


    tokenizer = LlamaTokenizer.from_pretrained(''ChanceFocus/finma-7b-nlp'')

    model = LlamaForCausalLM.from_pretrained(''ChanceFocus/finma-7b-nlp'', device_map=''auto'')

    ```


    If executing this code doesn''t resolve your issue, please don''t hesitate to
    comment!'
  created_at: 2023-07-06 22:29:11+00:00
  edited: false
  hidden: false
  id: 64a74e47a194220d66c0f349
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1672842917861-noauth.jpeg?w=200&h=200&f=face
      fullname: Jimin Huang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jiminHuang
      type: user
    createdAt: '2023-07-17T12:26:10.000Z'
    data:
      status: closed
    id: 64b533629a88b423da96307d
    type: status-change
  author: jiminHuang
  created_at: 2023-07-17 11:26:10+00:00
  id: 64b533629a88b423da96307d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: ChanceFocus/finma-7b-nlp
repo_type: model
status: closed
target_branch: null
title: Loading the tokenizer fails due to infinite loop
