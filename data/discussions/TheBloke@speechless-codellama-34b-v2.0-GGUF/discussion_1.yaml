!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ramzeez88
conflicting_files: null
created_at: 2023-10-13 17:13:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5186befff3a95ba33f4900421ad23b4.svg
      fullname: Adam Filipkowski
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ramzeez88
      type: user
    createdAt: '2023-10-13T18:13:43.000Z'
    data:
      edited: false
      editors:
      - ramzeez88
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8605653643608093
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5186befff3a95ba33f4900421ad23b4.svg
          fullname: Adam Filipkowski
          isHf: false
          isPro: false
          name: ramzeez88
          type: user
        html: '<p>I am wondering what kind of speed can i expect from this model if
          i split the layers between cpu and gpu. i have 4 cores/8threads  cpu , 16gb
          ram , and nvidia 1070ti 8gb vram.</p>

          '
        raw: I am wondering what kind of speed can i expect from this model if i split
          the layers between cpu and gpu. i have 4 cores/8threads  cpu , 16gb ram
          , and nvidia 1070ti 8gb vram.
        updatedAt: '2023-10-13T18:13:43.528Z'
      numEdits: 0
      reactions: []
    id: 652988d73a416e1f217903ed
    type: comment
  author: ramzeez88
  content: I am wondering what kind of speed can i expect from this model if i split
    the layers between cpu and gpu. i have 4 cores/8threads  cpu , 16gb ram , and
    nvidia 1070ti 8gb vram.
  created_at: 2023-10-13 17:13:43+00:00
  edited: false
  hidden: false
  id: 652988d73a416e1f217903ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/25437db01744dfc4a7b707aaf5726d2b.svg
      fullname: 'Dmitriy '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dimanchkek
      type: user
    createdAt: '2023-10-13T21:42:39.000Z'
    data:
      edited: false
      editors:
      - dimanchkek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9256341457366943
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/25437db01744dfc4a7b707aaf5726d2b.svg
          fullname: 'Dmitriy '
          isHf: false
          isPro: false
          name: dimanchkek
          type: user
        html: '<blockquote>

          <p>I am wondering what kind of speed can i expect from this model if i split
          the layers between cpu and gpu. i have 4 cores/8threads  cpu , 16gb ram
          , and nvidia 1070ti 8gb vram.</p>

          </blockquote>

          <p>It would be amazing if 16GB of RAM is enough for this model. Even if
          your PC doesn''t hang, the output speed will still be very slow due to the
          use of swap file</p>

          '
        raw: '> I am wondering what kind of speed can i expect from this model if
          i split the layers between cpu and gpu. i have 4 cores/8threads  cpu , 16gb
          ram , and nvidia 1070ti 8gb vram.


          It would be amazing if 16GB of RAM is enough for this model. Even if your
          PC doesn''t hang, the output speed will still be very slow due to the use
          of swap file'
        updatedAt: '2023-10-13T21:42:39.748Z'
      numEdits: 0
      reactions: []
    id: 6529b9cff6390fe048bb11ba
    type: comment
  author: dimanchkek
  content: '> I am wondering what kind of speed can i expect from this model if i
    split the layers between cpu and gpu. i have 4 cores/8threads  cpu , 16gb ram
    , and nvidia 1070ti 8gb vram.


    It would be amazing if 16GB of RAM is enough for this model. Even if your PC doesn''t
    hang, the output speed will still be very slow due to the use of swap file'
  created_at: 2023-10-13 20:42:39+00:00
  edited: false
  hidden: false
  id: 6529b9cff6390fe048bb11ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-14T07:15:24.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9532755017280579
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah 16GB is going to be tight. But it should be possible - offload
          8GB of layers to GPU, and then the smaller quants, eg Q3_K_M, will use around
          10GB RAM, so it should just fit.</p>

          <p>Speed is not going to be great on account of your CPU and GPU both being
          weak and old (1070Ti is very old now) - expect it to be very slow.  But
          it hopefully won''t swap.</p>

          <p>Maybe 1 token a second, or 2 tokens a second? Something like that.</p>

          '
        raw: 'Yeah 16GB is going to be tight. But it should be possible - offload
          8GB of layers to GPU, and then the smaller quants, eg Q3_K_M, will use around
          10GB RAM, so it should just fit.


          Speed is not going to be great on account of your CPU and GPU both being
          weak and old (1070Ti is very old now) - expect it to be very slow.  But
          it hopefully won''t swap.


          Maybe 1 token a second, or 2 tokens a second? Something like that.'
        updatedAt: '2023-10-14T07:15:24.909Z'
      numEdits: 0
      reactions: []
    id: 652a400c66313ebb617caee2
    type: comment
  author: TheBloke
  content: 'Yeah 16GB is going to be tight. But it should be possible - offload 8GB
    of layers to GPU, and then the smaller quants, eg Q3_K_M, will use around 10GB
    RAM, so it should just fit.


    Speed is not going to be great on account of your CPU and GPU both being weak
    and old (1070Ti is very old now) - expect it to be very slow.  But it hopefully
    won''t swap.


    Maybe 1 token a second, or 2 tokens a second? Something like that.'
  created_at: 2023-10-14 06:15:24+00:00
  edited: false
  hidden: false
  id: 652a400c66313ebb617caee2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/speechless-codellama-34b-v2.0-GGUF
repo_type: model
status: open
target_branch: null
title: Speed
