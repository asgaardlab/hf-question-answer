!!python/object:huggingface_hub.community.DiscussionWithDetails
author: johnhk
conflicting_files: null
created_at: 2023-10-25 17:12:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f3e1f9c57d4d2df5f306ef8ce744699.svg
      fullname: pengfei ye
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johnhk
      type: user
    createdAt: '2023-10-25T18:12:23.000Z'
    data:
      edited: false
      editors:
      - johnhk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8745712041854858
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f3e1f9c57d4d2df5f306ef8ce744699.svg
          fullname: pengfei ye
          isHf: false
          isPro: false
          name: johnhk
          type: user
        html: '<p>is there a way to find code llama prompt template? I found one like<br>[INST]
          Write code to solve the following coding problem that obeys the constraints
          and passes the example test cases. Please wrap your code answer using ```:<br>{prompt}<br>[/INST],
          so I''m curious is there a set of prompt templates designed for codellama
          to carry out different tasks more efficiently</p>

          '
        raw: "is there a way to find code llama prompt template? I found one like\
          \ \r\n[INST] Write code to solve the following coding problem that obeys\
          \ the constraints and passes the example test cases. Please wrap your code\
          \ answer using ```:\r\n{prompt}\r\n[/INST], so I'm curious is there a set\
          \ of prompt templates designed for codellama to carry out different tasks\
          \ more efficiently"
        updatedAt: '2023-10-25T18:12:23.228Z'
      numEdits: 0
      reactions: []
    id: 65395a87fc8dea94ef5be522
    type: comment
  author: johnhk
  content: "is there a way to find code llama prompt template? I found one like \r\
    \n[INST] Write code to solve the following coding problem that obeys the constraints\
    \ and passes the example test cases. Please wrap your code answer using ```:\r\
    \n{prompt}\r\n[/INST], so I'm curious is there a set of prompt templates designed\
    \ for codellama to carry out different tasks more efficiently"
  created_at: 2023-10-25 17:12:23+00:00
  edited: false
  hidden: false
  id: 65395a87fc8dea94ef5be522
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f3e1f9c57d4d2df5f306ef8ce744699.svg
      fullname: pengfei ye
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johnhk
      type: user
    createdAt: '2023-10-25T18:17:47.000Z'
    data:
      edited: false
      editors:
      - johnhk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9281718134880066
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f3e1f9c57d4d2df5f306ef8ce744699.svg
          fullname: pengfei ye
          isHf: false
          isPro: false
          name: johnhk
          type: user
        html: '<p>I''m also curious about the specialized tag like[INST] that code
          llama use, how to find more such tags </p>

          '
        raw: 'I''m also curious about the specialized tag like[INST] that code llama
          use, how to find more such tags '
        updatedAt: '2023-10-25T18:17:47.987Z'
      numEdits: 0
      reactions: []
    id: 65395bcbf186c8b4a89faa91
    type: comment
  author: johnhk
  content: 'I''m also curious about the specialized tag like[INST] that code llama
    use, how to find more such tags '
  created_at: 2023-10-25 17:17:47+00:00
  edited: false
  hidden: false
  id: 65395bcbf186c8b4a89faa91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f43cc07fbd0e7da1ab83c1186f559c8a.svg
      fullname: Atahan Akyildiz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: atahanakyildiz
      type: user
    createdAt: '2023-10-28T14:58:18.000Z'
    data:
      edited: true
      editors:
      - atahanakyildiz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6773220300674438
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f43cc07fbd0e7da1ab83c1186f559c8a.svg
          fullname: Atahan Akyildiz
          isHf: false
          isPro: false
          name: atahanakyildiz
          type: user
        html: '<p>The following tags are used in the original repository.</p>

          <h3 id="codellamallamagenerationpy">codellama/llama/generation.py</h3>

          <pre><code>B_INST, E_INST = "[INST]", "[/INST]"

          B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"


          SPECIAL_TAGS = [B_INST, E_INST, "&lt;&lt;SYS&gt;&gt;", "&lt;&lt;/SYS&gt;&gt;"]

          </code></pre>

          '
        raw: 'The following tags are used in the original repository.


          ### codellama/llama/generation.py

          ```

          B_INST, E_INST = "[INST]", "[/INST]"

          B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"


          SPECIAL_TAGS = [B_INST, E_INST, "<<SYS>>", "<</SYS>>"]

          ```'
        updatedAt: '2023-10-28T14:59:05.071Z'
      numEdits: 1
      reactions: []
    id: 653d218ade20728b27c82560
    type: comment
  author: atahanakyildiz
  content: 'The following tags are used in the original repository.


    ### codellama/llama/generation.py

    ```

    B_INST, E_INST = "[INST]", "[/INST]"

    B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"


    SPECIAL_TAGS = [B_INST, E_INST, "<<SYS>>", "<</SYS>>"]

    ```'
  created_at: 2023-10-28 13:58:18+00:00
  edited: true
  hidden: false
  id: 653d218ade20728b27c82560
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
      fullname: Pedro Cuenca
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pcuenq
      type: user
    createdAt: '2023-10-28T17:49:38.000Z'
    data:
      edited: false
      editors:
      - pcuenq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8466984629631042
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
          fullname: Pedro Cuenca
          isHf: true
          isPro: false
          name: pcuenq
          type: user
        html: '<p>The conversational instructions follow the same format as Llama
          2. <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/9e87618f2be1401df55c36ad726629ae201e8e4d/src/transformers/models/code_llama/tokenization_code_llama.py#L465-L466">Here''s
          a template</a> that shows the structure when you use a system prompt (which
          is optional) followed by several rounds of user instructions and model answers.
          The easiest way to ensure you adhere to that format is by using the new
          "Chat Templates" feature in transformers, which will take care of the delimiters
          for you: <a href="https://huggingface.co/docs/transformers/chat_templating">https://huggingface.co/docs/transformers/chat_templating</a></p>

          <p>In addition to interfacing with the model using chat conversations, you
          can also ask it to fill a block of code with a suitable completion, which
          is what some code assistant tools do. To use this interaction mode, I''d
          recommend you follow <a href="https://huggingface.co/blog/codellama#code-infilling">the
          Code Infilling section of this blog post</a>. </p>

          '
        raw: 'The conversational instructions follow the same format as Llama 2. [Here''s
          a template](https://github.com/huggingface/transformers/blob/9e87618f2be1401df55c36ad726629ae201e8e4d/src/transformers/models/code_llama/tokenization_code_llama.py#L465-L466)
          that shows the structure when you use a system prompt (which is optional)
          followed by several rounds of user instructions and model answers. The easiest
          way to ensure you adhere to that format is by using the new "Chat Templates"
          feature in transformers, which will take care of the delimiters for you:
          https://huggingface.co/docs/transformers/chat_templating


          In addition to interfacing with the model using chat conversations, you
          can also ask it to fill a block of code with a suitable completion, which
          is what some code assistant tools do. To use this interaction mode, I''d
          recommend you follow [the Code Infilling section of this blog post](https://huggingface.co/blog/codellama#code-infilling). '
        updatedAt: '2023-10-28T17:49:38.095Z'
      numEdits: 0
      reactions: []
    id: 653d49b2cd52f28849cf39a5
    type: comment
  author: pcuenq
  content: 'The conversational instructions follow the same format as Llama 2. [Here''s
    a template](https://github.com/huggingface/transformers/blob/9e87618f2be1401df55c36ad726629ae201e8e4d/src/transformers/models/code_llama/tokenization_code_llama.py#L465-L466)
    that shows the structure when you use a system prompt (which is optional) followed
    by several rounds of user instructions and model answers. The easiest way to ensure
    you adhere to that format is by using the new "Chat Templates" feature in transformers,
    which will take care of the delimiters for you: https://huggingface.co/docs/transformers/chat_templating


    In addition to interfacing with the model using chat conversations, you can also
    ask it to fill a block of code with a suitable completion, which is what some
    code assistant tools do. To use this interaction mode, I''d recommend you follow
    [the Code Infilling section of this blog post](https://huggingface.co/blog/codellama#code-infilling). '
  created_at: 2023-10-28 16:49:38+00:00
  edited: false
  hidden: false
  id: 653d49b2cd52f28849cf39a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cfe3e58f5a4e5dbf0bf0d9c605753fff.svg
      fullname: wll
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wllvcxz
      type: user
    createdAt: '2023-11-03T04:04:22.000Z'
    data:
      edited: false
      editors:
      - wllvcxz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4643491208553314
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cfe3e58f5a4e5dbf0bf0d9c605753fff.svg
          fullname: wll
          isHf: false
          isPro: false
          name: wllvcxz
          type: user
        html: '<p>Here is a prompt template for 34b instruct model: <a href="https://huggingface.co/spaces/codellama/codellama-13b-chat/blob/main/model.py#L25-L36">https://huggingface.co/spaces/codellama/codellama-13b-chat/blob/main/model.py#L25-L36</a></p>

          '
        raw: 'Here is a prompt template for 34b instruct model: https://huggingface.co/spaces/codellama/codellama-13b-chat/blob/main/model.py#L25-L36'
        updatedAt: '2023-11-03T04:04:22.376Z'
      numEdits: 0
      reactions: []
    id: 654471468272e6b1f38fc7ed
    type: comment
  author: wllvcxz
  content: 'Here is a prompt template for 34b instruct model: https://huggingface.co/spaces/codellama/codellama-13b-chat/blob/main/model.py#L25-L36'
  created_at: 2023-11-03 03:04:22+00:00
  edited: false
  hidden: false
  id: 654471468272e6b1f38fc7ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633d6d4f48ab6a0add2ce1a3/qTO75kR0hk1Yn1SaP7ZPb.jpeg?w=200&h=200&f=face
      fullname: Humza Sami
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: humza-sami
      type: user
    createdAt: '2023-12-24T20:14:47.000Z'
    data:
      edited: false
      editors:
      - humza-sami
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5528088808059692
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633d6d4f48ab6a0add2ce1a3/qTO75kR0hk1Yn1SaP7ZPb.jpeg?w=200&h=200&f=face
          fullname: Humza Sami
          isHf: false
          isPro: false
          name: humza-sami
          type: user
        html: '<p>I use following template for finetuning and inference:</p>

          <p><code>&lt;s&gt;[INST] user_message_1 [/INST] response_1 &lt;/s&gt;&lt;s&gt;[INST]
          user_message_2 [/INST]  response_2 &lt;/s&gt;</code></p>

          '
        raw: 'I use following template for finetuning and inference:


          `<s>[INST] user_message_1 [/INST] response_1 </s><s>[INST] user_message_2
          [/INST]  response_2 </s>`'
        updatedAt: '2023-12-24T20:14:47.652Z'
      numEdits: 0
      reactions: []
    id: 65889137438d7b1ccf379b64
    type: comment
  author: humza-sami
  content: 'I use following template for finetuning and inference:


    `<s>[INST] user_message_1 [/INST] response_1 </s><s>[INST] user_message_2 [/INST]  response_2
    </s>`'
  created_at: 2023-12-24 20:14:47+00:00
  edited: false
  hidden: false
  id: 65889137438d7b1ccf379b64
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: codellama/CodeLlama-7b-Instruct-hf
repo_type: model
status: open
target_branch: null
title: code llama prompt template
