!!python/object:huggingface_hub.community.DiscussionWithDetails
author: oximi123
conflicting_files: null
created_at: 2023-12-26 07:58:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0843044e0a25975f121a6bd824fb9aed.svg
      fullname: mike
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oximi123
      type: user
    createdAt: '2023-12-26T07:58:51.000Z'
    data:
      edited: false
      editors:
      - oximi123
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6190637946128845
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0843044e0a25975f121a6bd824fb9aed.svg
          fullname: mike
          isHf: false
          isPro: false
          name: oximi123
          type: user
        html: '<p>I use FastChat to deploy CodeLlama-7b-Instruct-hf on a A800-80GB
          server. The inference speed is extremly slow (It runs more than ten minutes
          without producing the response for a request). Any suggestion on how to
          solve this problem?</p>

          <p>Here is how I deploy it with FastChat:</p>

          <p>python -m fastchat.serve.controller<br>python -m fastchat.serve.model_worker
          --model-path /home/user/botao/CodeLlama-7b-Instruct-hf<br>python -m fastchat.serve.openai_api_server
          --host localhost --port 8000</p>

          '
        raw: "I use FastChat to deploy CodeLlama-7b-Instruct-hf on a A800-80GB server.\
          \ The inference speed is extremly slow (It runs more than ten minutes without\
          \ producing the response for a request). Any suggestion on how to solve\
          \ this problem?\r\n\r\nHere is how I deploy it with FastChat:\r\n\r\npython\
          \ -m fastchat.serve.controller\r\npython -m fastchat.serve.model_worker\
          \ --model-path /home/user/botao/CodeLlama-7b-Instruct-hf\r\npython -m fastchat.serve.openai_api_server\
          \ --host localhost --port 8000"
        updatedAt: '2023-12-26T07:58:51.346Z'
      numEdits: 0
      reactions: []
    id: 658a87bb35f23c0f1c5de972
    type: comment
  author: oximi123
  content: "I use FastChat to deploy CodeLlama-7b-Instruct-hf on a A800-80GB server.\
    \ The inference speed is extremly slow (It runs more than ten minutes without\
    \ producing the response for a request). Any suggestion on how to solve this problem?\r\
    \n\r\nHere is how I deploy it with FastChat:\r\n\r\npython -m fastchat.serve.controller\r\
    \npython -m fastchat.serve.model_worker --model-path /home/user/botao/CodeLlama-7b-Instruct-hf\r\
    \npython -m fastchat.serve.openai_api_server --host localhost --port 8000"
  created_at: 2023-12-26 07:58:51+00:00
  edited: false
  hidden: false
  id: 658a87bb35f23c0f1c5de972
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: codellama/CodeLlama-7b-Instruct-hf
repo_type: model
status: open
target_branch: null
title: Inference speed is extremly slow with FastChat
