!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Jamil-Brian
conflicting_files: null
created_at: 2023-12-28 15:30:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ef89361092f8a385c142fd92a121404.svg
      fullname: Jamil Palma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jamil-Brian
      type: user
    createdAt: '2023-12-28T15:30:33.000Z'
    data:
      edited: false
      editors:
      - Jamil-Brian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9099352359771729
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ef89361092f8a385c142fd92a121404.svg
          fullname: Jamil Palma
          isHf: false
          isPro: false
          name: Jamil-Brian
          type: user
        html: '<p>Hello everyone,<br>I would like to know if someone could help me
          with this or provide some suggestions on what to do.<br>What I want is to
          use the theoretical 100K input for the codellama-instruct input prompt.<br>I
          am using 8 GPUs with 32GB each, but when I try to send a very long input
          prompt, I get this message.</p>

          <h1 id=""></h1>

          <p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 69.32
          GiB. GPU 0 has a total capacity of 31.74 GiB of which 24.29 GiB is free.
          Including non-PyTorch memory, this process has 7.44 GiB memory in use. Of
          the allocated memory 6.50 GiB is allocated by PyTorch, and 593.00 MiB is
          reserved by PyTorch but unallocated. If reserved but unallocated memory
          is large try setting PYTORCH_CUDA_ALLOC_CONF=disableable_segments:True to
          avoid fragmentation. See documentation for Memory Management (<a rel="nofollow"
          href="https://pytorch.org/docs/stable/notes/cuda.html#environment-variables">https://pytorch.org/docs/stable/notes/cuda.html#environment-variables</a>)<br>#<br>It''s
          as if it''s only using 1 GPU to process the input and ignoring the others.</p>

          <p>This issue doesn''t occur when generating a long output since all the
          GPUs work correctly. It only happens when I send a very long input prompt.</p>

          <p>I would appreciate any help or suggestions you may have.</p>

          '
        raw: "Hello everyone,\r\nI would like to know if someone could help me with\
          \ this or provide some suggestions on what to do.\r\nWhat I want is to use\
          \ the theoretical 100K input for the codellama-instruct input prompt.\r\n\
          I am using 8 GPUs with 32GB each, but when I try to send a very long input\
          \ prompt, I get this message.\r\n\r\n#\r\ntorch.cuda.OutOfMemoryError: CUDA\
          \ out of memory. Tried to allocate 69.32 GiB. GPU 0 has a total capacity\
          \ of 31.74 GiB of which 24.29 GiB is free. Including non-PyTorch memory,\
          \ this process has 7.44 GiB memory in use. Of the allocated memory 6.50\
          \ GiB is allocated by PyTorch, and 593.00 MiB is reserved by PyTorch but\
          \ unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=disableable_segments:True\
          \ to avoid fragmentation. See documentation for Memory Management (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\
          \n#\r\nIt's as if it's only using 1 GPU to process the input and ignoring\
          \ the others.\r\n\r\nThis issue doesn't occur when generating a long output\
          \ since all the GPUs work correctly. It only happens when I send a very\
          \ long input prompt.\r\n\r\nI would appreciate any help or suggestions you\
          \ may have.\r\n"
        updatedAt: '2023-12-28T15:30:33.705Z'
      numEdits: 0
      reactions: []
    id: 658d9499c461a3c43296fb3f
    type: comment
  author: Jamil-Brian
  content: "Hello everyone,\r\nI would like to know if someone could help me with\
    \ this or provide some suggestions on what to do.\r\nWhat I want is to use the\
    \ theoretical 100K input for the codellama-instruct input prompt.\r\nI am using\
    \ 8 GPUs with 32GB each, but when I try to send a very long input prompt, I get\
    \ this message.\r\n\r\n#\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried\
    \ to allocate 69.32 GiB. GPU 0 has a total capacity of 31.74 GiB of which 24.29\
    \ GiB is free. Including non-PyTorch memory, this process has 7.44 GiB memory\
    \ in use. Of the allocated memory 6.50 GiB is allocated by PyTorch, and 593.00\
    \ MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory\
    \ is large try setting PYTORCH_CUDA_ALLOC_CONF=disableable_segments:True to avoid\
    \ fragmentation. See documentation for Memory Management (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\
    \n#\r\nIt's as if it's only using 1 GPU to process the input and ignoring the\
    \ others.\r\n\r\nThis issue doesn't occur when generating a long output since\
    \ all the GPUs work correctly. It only happens when I send a very long input prompt.\r\
    \n\r\nI would appreciate any help or suggestions you may have.\r\n"
  created_at: 2023-12-28 15:30:33+00:00
  edited: false
  hidden: false
  id: 658d9499c461a3c43296fb3f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: codellama/CodeLlama-7b-Instruct-hf
repo_type: model
status: open
target_branch: null
title: how to pass a large entry, or split the entry, to get the use of 100K tokens
