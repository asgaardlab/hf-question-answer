!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cofade
conflicting_files: null
created_at: 2023-08-28 19:59:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b4f554cf1f40922c011a0fedcbfe6184.svg
      fullname: S W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cofade
      type: user
    createdAt: '2023-08-28T20:59:11.000Z'
    data:
      edited: true
      editors:
      - cofade
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.881521999835968
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b4f554cf1f40922c011a0fedcbfe6184.svg
          fullname: S W
          isHf: false
          isPro: false
          name: cofade
          type: user
        html: '<p>When trying to run the model, I get the error<br>"Tokenizer class
          CodeLlamaTokenizer does not exist or is not currently imported."</p>

          <p>It is raised by "Lib\site-packages\transformers\models\auto\tokenization_auto.py",
          line 724,  as it is indeed not contained in the TOKENIZER_MAPPING_NAMES
          OrderedDict.<br>The requested tokenizer "CodeLlamaTokenizer" is defined
          in "models\codellama_CodeLlama-7b-Instruct-hf\tokenizer_config.json".</p>

          <p>Can you please help me with this issue?</p>

          '
        raw: 'When trying to run the model, I get the error

          "Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported."


          It is raised by "Lib\site-packages\transformers\models\auto\tokenization_auto.py",
          line 724,  as it is indeed not contained in the TOKENIZER_MAPPING_NAMES
          OrderedDict.

          The requested tokenizer "CodeLlamaTokenizer" is defined in "models\codellama_CodeLlama-7b-Instruct-hf\tokenizer_config.json".


          Can you please help me with this issue?'
        updatedAt: '2023-08-28T21:01:02.047Z'
      numEdits: 1
      reactions: []
    id: 64ed0a9f69b203c202f07ae6
    type: comment
  author: cofade
  content: 'When trying to run the model, I get the error

    "Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported."


    It is raised by "Lib\site-packages\transformers\models\auto\tokenization_auto.py",
    line 724,  as it is indeed not contained in the TOKENIZER_MAPPING_NAMES OrderedDict.

    The requested tokenizer "CodeLlamaTokenizer" is defined in "models\codellama_CodeLlama-7b-Instruct-hf\tokenizer_config.json".


    Can you please help me with this issue?'
  created_at: 2023-08-28 19:59:11+00:00
  edited: true
  hidden: false
  id: 64ed0a9f69b203c202f07ae6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
      fullname: Pedro Cuenca
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pcuenq
      type: user
    createdAt: '2023-08-28T22:08:39.000Z'
    data:
      edited: false
      editors:
      - pcuenq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8945550918579102
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
          fullname: Pedro Cuenca
          isHf: true
          isPro: false
          name: pcuenq
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;cofade&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cofade\">@<span class=\"\
          underline\">cofade</span></a></span>\n\n\t</span></span>!</p>\n<p>You need\
          \ to install transformers from the <code>main</code> development branch,\
          \ because the Code Llama changes have not been released through PyPi yet.\
          \ This is how you'd do it:</p>\n<pre><code>pip install git+https://github.com/huggingface/transformers.git@main\n\
          </code></pre>\n<p>Hope that helps!</p>\n"
        raw: 'Hi @cofade!


          You need to install transformers from the `main` development branch, because
          the Code Llama changes have not been released through PyPi yet. This is
          how you''d do it:


          ```

          pip install git+https://github.com/huggingface/transformers.git@main

          ```


          Hope that helps!'
        updatedAt: '2023-08-28T22:08:39.918Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - cofade
    id: 64ed1ae7476cdc8502845760
    type: comment
  author: pcuenq
  content: 'Hi @cofade!


    You need to install transformers from the `main` development branch, because the
    Code Llama changes have not been released through PyPi yet. This is how you''d
    do it:


    ```

    pip install git+https://github.com/huggingface/transformers.git@main

    ```


    Hope that helps!'
  created_at: 2023-08-28 21:08:39+00:00
  edited: false
  hidden: false
  id: 64ed1ae7476cdc8502845760
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14c3251a71a2d6844f28195be4581001.svg
      fullname: Shend Ibrani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Emrys95
      type: user
    createdAt: '2023-08-30T14:05:09.000Z'
    data:
      edited: false
      editors:
      - Emrys95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9760358929634094
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14c3251a71a2d6844f28195be4581001.svg
          fullname: Shend Ibrani
          isHf: false
          isPro: false
          name: Emrys95
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;pcuenq&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pcuenq\">@<span class=\"\
          underline\">pcuenq</span></a></span>\n\n\t</span></span>, I did this, but\
          \ how do i use the downloaded repo? also it's not even telling me where\
          \ it downloaded it? </p>\n"
        raw: 'Hi @pcuenq, I did this, but how do i use the downloaded repo? also it''s
          not even telling me where it downloaded it? '
        updatedAt: '2023-08-30T14:05:09.628Z'
      numEdits: 0
      reactions: []
    id: 64ef4c950f5a7993222f4f59
    type: comment
  author: Emrys95
  content: 'Hi @pcuenq, I did this, but how do i use the downloaded repo? also it''s
    not even telling me where it downloaded it? '
  created_at: 2023-08-30 13:05:09+00:00
  edited: false
  hidden: false
  id: 64ef4c950f5a7993222f4f59
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/084ec577ebe53e735a74a0ceb6b17199.svg
      fullname: Saksham Consul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saksham-lamini
      type: user
    createdAt: '2023-08-30T17:28:42.000Z'
    data:
      edited: false
      editors:
      - saksham-lamini
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6367189884185791
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/084ec577ebe53e735a74a0ceb6b17199.svg
          fullname: Saksham Consul
          isHf: false
          isPro: false
          name: saksham-lamini
          type: user
        html: '<p>Post running the pip install, use it normally as you would have
          any python package. <code>from transformers import AutoModelForCausalLM,
          AutoTokenizer, AutoConfig</code>. Then tokenizer = <code>AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Instruct-hf")</code></p>

          '
        raw: Post running the pip install, use it normally as you would have any python
          package. `from transformers import AutoModelForCausalLM, AutoTokenizer,
          AutoConfig`. Then tokenizer = `AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Instruct-hf")`
        updatedAt: '2023-08-30T17:28:42.529Z'
      numEdits: 0
      reactions: []
    id: 64ef7c4a0bf5be9a4719fdc9
    type: comment
  author: saksham-lamini
  content: Post running the pip install, use it normally as you would have any python
    package. `from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig`.
    Then tokenizer = `AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-Instruct-hf")`
  created_at: 2023-08-30 16:28:42+00:00
  edited: false
  hidden: false
  id: 64ef7c4a0bf5be9a4719fdc9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14c3251a71a2d6844f28195be4581001.svg
      fullname: Shend Ibrani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Emrys95
      type: user
    createdAt: '2023-08-30T18:46:26.000Z'
    data:
      edited: false
      editors:
      - Emrys95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8771138787269592
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14c3251a71a2d6844f28195be4581001.svg
          fullname: Shend Ibrani
          isHf: false
          isPro: false
          name: Emrys95
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;saksham-lamini&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/saksham-lamini\"\
          >@<span class=\"underline\">saksham-lamini</span></a></span>\n\n\t</span></span>\
          \ doesn't from_pretrained automatically download the model from api? what\
          \ is the point of downloading the git repo also then?</p>\n"
        raw: '@saksham-lamini doesn''t from_pretrained automatically download the
          model from api? what is the point of downloading the git repo also then?'
        updatedAt: '2023-08-30T18:46:26.993Z'
      numEdits: 0
      reactions: []
    id: 64ef8e8250dae59768d61adf
    type: comment
  author: Emrys95
  content: '@saksham-lamini doesn''t from_pretrained automatically download the model
    from api? what is the point of downloading the git repo also then?'
  created_at: 2023-08-30 17:46:26+00:00
  edited: false
  hidden: false
  id: 64ef8e8250dae59768d61adf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/084ec577ebe53e735a74a0ceb6b17199.svg
      fullname: Saksham Consul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saksham-lamini
      type: user
    createdAt: '2023-08-30T19:14:29.000Z'
    data:
      edited: false
      editors:
      - saksham-lamini
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9318179488182068
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/084ec577ebe53e735a74a0ceb6b17199.svg
          fullname: Saksham Consul
          isHf: false
          isPro: false
          name: saksham-lamini
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Emrys95&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Emrys95\">@<span class=\"\
          underline\">Emrys95</span></a></span>\n\n\t</span></span> you're right that\
          \ <code>from_pretrained</code> will download the model or in this case the\
          \ token shards from the API,  but it would try to load it into a <code>CodeLlamaTokenizer</code>\
          \ class which does not exist if you did a normal pip install.</p>\n"
        raw: '@Emrys95 you''re right that `from_pretrained` will download the model
          or in this case the token shards from the API,  but it would try to load
          it into a `CodeLlamaTokenizer` class which does not exist if you did a normal
          pip install.

          '
        updatedAt: '2023-08-30T19:14:29.704Z'
      numEdits: 0
      reactions: []
    id: 64ef9515abd9fb19142a74f7
    type: comment
  author: saksham-lamini
  content: '@Emrys95 you''re right that `from_pretrained` will download the model
    or in this case the token shards from the API,  but it would try to load it into
    a `CodeLlamaTokenizer` class which does not exist if you did a normal pip install.

    '
  created_at: 2023-08-30 18:14:29+00:00
  edited: false
  hidden: false
  id: 64ef9515abd9fb19142a74f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b4f554cf1f40922c011a0fedcbfe6184.svg
      fullname: S W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cofade
      type: user
    createdAt: '2023-08-30T20:20:02.000Z'
    data:
      edited: true
      editors:
      - cofade
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8483113050460815
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b4f554cf1f40922c011a0fedcbfe6184.svg
          fullname: S W
          isHf: false
          isPro: false
          name: cofade
          type: user
        html: "<blockquote>\n<pre><code>pip install git+https://github.com/huggingface/transformers.git@main\n\
          </code></pre>\n</blockquote>\n<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;pcuenq&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pcuenq\"\
          >@<span class=\"underline\">pcuenq</span></a></span>\n\n\t</span></span>,\
          \ that worked perfectly! I am using this LLM with the oobabooga Web UI,\
          \ and the installer didn't provide the correct transformers version yet.\
          \ </p>\n"
        raw: '> ```

          > pip install git+https://github.com/huggingface/transformers.git@main

          > ```


          Thank you @pcuenq, that worked perfectly! I am using this LLM with the oobabooga
          Web UI, and the installer didn''t provide the correct transformers version
          yet. '
        updatedAt: '2023-08-30T20:20:33.948Z'
      numEdits: 1
      reactions: []
    id: 64efa472a02abfcfd75e8313
    type: comment
  author: cofade
  content: '> ```

    > pip install git+https://github.com/huggingface/transformers.git@main

    > ```


    Thank you @pcuenq, that worked perfectly! I am using this LLM with the oobabooga
    Web UI, and the installer didn''t provide the correct transformers version yet. '
  created_at: 2023-08-30 19:20:02+00:00
  edited: true
  hidden: false
  id: 64efa472a02abfcfd75e8313
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14c3251a71a2d6844f28195be4581001.svg
      fullname: Shend Ibrani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Emrys95
      type: user
    createdAt: '2023-08-30T21:45:31.000Z'
    data:
      edited: true
      editors:
      - Emrys95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9712253212928772
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14c3251a71a2d6844f28195be4581001.svg
          fullname: Shend Ibrani
          isHf: false
          isPro: false
          name: Emrys95
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;pcuenq&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/pcuenq\">@<span class=\"\
          underline\">pcuenq</span></a></span>\n\n\t</span></span> I've been trying\
          \ for days to get one of these models running, always running into one problem\
          \ or another, such as python package conflicts (im new at this, yes), could\
          \ you please give me some valid code i can just copy/paste and i can work\
          \ to get it running? So far only GPT2 has worked for me, the very old version,\
          \ but fine tuning it has resulted in catastrophic forgetting where it cant\
          \ answer anything except my own document which i fed to it. If you could\
          \ guide me in the right direction i'd appreciate it. </p>\n"
        raw: "@pcuenq I've been trying for days to get one of these models running,\
          \ always running into one problem or another, such as python package conflicts\
          \ (im new at this, yes), could you please give me some valid code i can\
          \ just copy/paste and i can work to get it running? So far only GPT2 has\
          \ worked for me, the very old version, but fine tuning it has resulted in\
          \ catastrophic forgetting where it cant answer anything except my own document\
          \ which i fed to it. If you could guide me in the right direction i'd appreciate\
          \ it. \n\n"
        updatedAt: '2023-08-30T22:03:02.548Z'
      numEdits: 2
      reactions: []
    id: 64efb87bc26b0228ec48822b
    type: comment
  author: Emrys95
  content: "@pcuenq I've been trying for days to get one of these models running,\
    \ always running into one problem or another, such as python package conflicts\
    \ (im new at this, yes), could you please give me some valid code i can just copy/paste\
    \ and i can work to get it running? So far only GPT2 has worked for me, the very\
    \ old version, but fine tuning it has resulted in catastrophic forgetting where\
    \ it cant answer anything except my own document which i fed to it. If you could\
    \ guide me in the right direction i'd appreciate it. \n\n"
  created_at: 2023-08-30 20:45:31+00:00
  edited: true
  hidden: false
  id: 64efb87bc26b0228ec48822b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0cb22bab1bb0d1f6596d2f52e0a0b08a.svg
      fullname: VENKATA SAI SRIHARSHA MEDURI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sai0720
      type: user
    createdAt: '2023-08-31T04:43:00.000Z'
    data:
      edited: true
      editors:
      - Sai0720
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9695815443992615
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0cb22bab1bb0d1f6596d2f52e0a0b08a.svg
          fullname: VENKATA SAI SRIHARSHA MEDURI
          isHf: false
          isPro: false
          name: Sai0720
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Emrys95&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Emrys95\">@<span class=\"\
          underline\">Emrys95</span></a></span>\n\n\t</span></span> yeah I too need\
          \ some valid full code as there have been a lot of dependancy issues coming</p>\n"
        raw: '@Emrys95 yeah I too need some valid full code as there have been a lot
          of dependancy issues coming'
        updatedAt: '2023-08-31T04:43:26.065Z'
      numEdits: 1
      reactions: []
    id: 64f01a5450dae59768e83321
    type: comment
  author: Sai0720
  content: '@Emrys95 yeah I too need some valid full code as there have been a lot
    of dependancy issues coming'
  created_at: 2023-08-31 03:43:00+00:00
  edited: true
  hidden: false
  id: 64f01a5450dae59768e83321
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663429462553-noauth.jpeg?w=200&h=200&f=face
      fullname: asd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hawei-z
      type: user
    createdAt: '2023-08-31T12:52:16.000Z'
    data:
      edited: false
      editors:
      - hawei-z
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4278163015842438
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663429462553-noauth.jpeg?w=200&h=200&f=face
          fullname: asd
          isHf: false
          isPro: false
          name: hawei-z
          type: user
        html: '<p>same issue. </p>

          <pre><code>---------------------------------------------------------------------------

          ImportError                               Traceback (most recent call last)

          Cell In[9], line 1

          ----&gt; 1 from transformers import CodeLlamaTokenizer


          ImportError: cannot import name ''CodeLlamaTokenizer'' from ''transformers''
          (/Users/hawei/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/__init__.py)

          </code></pre>

          '
        raw: "same issue. \n\n```\n---------------------------------------------------------------------------\n\
          ImportError                               Traceback (most recent call last)\n\
          Cell In[9], line 1\n----> 1 from transformers import CodeLlamaTokenizer\n\
          \nImportError: cannot import name 'CodeLlamaTokenizer' from 'transformers'\
          \ (/Users/hawei/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/__init__.py)\n\
          \n```"
        updatedAt: '2023-08-31T12:52:16.104Z'
      numEdits: 0
      reactions: []
    id: 64f08d00e5500d42ffbf6ae1
    type: comment
  author: hawei-z
  content: "same issue. \n\n```\n---------------------------------------------------------------------------\n\
    ImportError                               Traceback (most recent call last)\n\
    Cell In[9], line 1\n----> 1 from transformers import CodeLlamaTokenizer\n\nImportError:\
    \ cannot import name 'CodeLlamaTokenizer' from 'transformers' (/Users/hawei/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/__init__.py)\n\
    \n```"
  created_at: 2023-08-31 11:52:16+00:00
  edited: false
  hidden: false
  id: 64f08d00e5500d42ffbf6ae1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663429462553-noauth.jpeg?w=200&h=200&f=face
      fullname: asd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hawei-z
      type: user
    createdAt: '2023-08-31T12:54:49.000Z'
    data:
      edited: false
      editors:
      - hawei-z
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40314507484436035
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663429462553-noauth.jpeg?w=200&h=200&f=face
          fullname: asd
          isHf: false
          isPro: false
          name: hawei-z
          type: user
        html: "<blockquote>\n<p>same issue. </p>\n<pre><code>---------------------------------------------------------------------------\n\
          ImportError                               Traceback (most recent call last)\n\
          Cell In[9], line 1\n----&gt; 1 from transformers import CodeLlamaTokenizer\n\
          \nImportError: cannot import name 'CodeLlamaTokenizer' from 'transformers'\
          \ (/Users/hawei/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/__init__.py)\n\
          </code></pre>\n</blockquote>\n<p>This error fix after I re-install main\
          \ branch transformers. </p>\n<p>But I get a new error.</p>\n<pre><code>---------------------------------------------------------------------------\n\
          AttributeError                            Traceback (most recent call last)\n\
          Cell In[5], line 1\n----&gt; 1 tokenizer = AutoTokenizer.from_pretrained(model)\n\
          \nFile ~/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:735,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\n    731     if tokenizer_class is None:\n    732         raise\
          \ ValueError(\n    733             f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\n    734         )\n--&gt;\
          \ 735     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n    737 # Otherwise we have to be creative.\n    738\
          \ # if model is an encoder decoder, the encoder tokenizer class is used\
          \ by default\n    739 if isinstance(config, EncoderDecoderConfig):\n\nFile\
          \ ~/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\n   1851     else:\n   1852         logger.info(f\"loading file\
          \ {file_path} from cache at {resolved_vocab_files[file_id]}\")\n-&gt; 1854\
          \ return cls._from_pretrained(\n   1855     resolved_vocab_files,\n   1856\
          \     pretrained_model_name_or_path,\n   1857     init_configuration,\n\
          \   1858     *init_inputs,\n   1859     token=token,\n   1860     cache_dir=cache_dir,\n\
          \   1861     local_files_only=local_files_only,\n   1862     _commit_hash=commit_hash,\n\
          \   1863     _is_local=is_local,\n   1864     **kwargs,\n   1865 )\n\nFile\
          \ ~/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2017,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only,\
          \ _commit_hash, _is_local, *init_inputs, **kwargs)\n   2015 # Instantiate\
          \ tokenizer.\n   2016 try:\n-&gt; 2017     tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\n   2018 except OSError:\n   2019     raise OSError(\n\
          \   2020         \"Unable to load vocabulary from file. \"\n   2021    \
          \     \"Please check that the provided vocabulary is accessible and not\
          \ corrupted.\"\n   2022     )\n\nFile ~/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/models/code_llama/tokenization_code_llama_fast.py:154,\
          \ in CodeLlamaTokenizerFast.__init__(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces,\
          \ unk_token, bos_token, eos_token, prefix_token, middle_token, suffix_token,\
          \ eot_token, fill_token, add_bos_token, add_eos_token, **kwargs)\n    151\
          \ self.update_post_processor()\n    153 self.vocab_file = vocab_file\n--&gt;\
          \ 154 self.can_save_slow_tokenizer = False if not self.vocab_file else True\n\
          \    156 self._prefix_token = prefix_token\n    157 self._middle_token =\
          \ middle_token\n\nAttributeError: can't set attribute 'can_save_slow_tokenizer'\n\
          </code></pre>\n"
        raw: "> same issue. \n> \n> ```\n> ---------------------------------------------------------------------------\n\
          > ImportError                               Traceback (most recent call\
          \ last)\n> Cell In[9], line 1\n> ----> 1 from transformers import CodeLlamaTokenizer\n\
          > \n> ImportError: cannot import name 'CodeLlamaTokenizer' from 'transformers'\
          \ (/Users/hawei/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/__init__.py)\n\
          > \n> ```\n\nThis error fix after I re-install main branch transformers.\
          \ \n\nBut I get a new error.\n\n```\n---------------------------------------------------------------------------\n\
          AttributeError                            Traceback (most recent call last)\n\
          Cell In[5], line 1\n----> 1 tokenizer = AutoTokenizer.from_pretrained(model)\n\
          \nFile ~/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:735,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\n    731     if tokenizer_class is None:\n    732         raise\
          \ ValueError(\n    733             f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\n    734         )\n-->\
          \ 735     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n    737 # Otherwise we have to be creative.\n    738\
          \ # if model is an encoder decoder, the encoder tokenizer class is used\
          \ by default\n    739 if isinstance(config, EncoderDecoderConfig):\n\nFile\
          \ ~/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\n   1851     else:\n   1852         logger.info(f\"loading file\
          \ {file_path} from cache at {resolved_vocab_files[file_id]}\")\n-> 1854\
          \ return cls._from_pretrained(\n   1855     resolved_vocab_files,\n   1856\
          \     pretrained_model_name_or_path,\n   1857     init_configuration,\n\
          \   1858     *init_inputs,\n   1859     token=token,\n   1860     cache_dir=cache_dir,\n\
          \   1861     local_files_only=local_files_only,\n   1862     _commit_hash=commit_hash,\n\
          \   1863     _is_local=is_local,\n   1864     **kwargs,\n   1865 )\n\nFile\
          \ ~/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2017,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only,\
          \ _commit_hash, _is_local, *init_inputs, **kwargs)\n   2015 # Instantiate\
          \ tokenizer.\n   2016 try:\n-> 2017     tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \   2018 except OSError:\n   2019     raise OSError(\n   2020         \"\
          Unable to load vocabulary from file. \"\n   2021         \"Please check\
          \ that the provided vocabulary is accessible and not corrupted.\"\n   2022\
          \     )\n\nFile ~/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/models/code_llama/tokenization_code_llama_fast.py:154,\
          \ in CodeLlamaTokenizerFast.__init__(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces,\
          \ unk_token, bos_token, eos_token, prefix_token, middle_token, suffix_token,\
          \ eot_token, fill_token, add_bos_token, add_eos_token, **kwargs)\n    151\
          \ self.update_post_processor()\n    153 self.vocab_file = vocab_file\n-->\
          \ 154 self.can_save_slow_tokenizer = False if not self.vocab_file else True\n\
          \    156 self._prefix_token = prefix_token\n    157 self._middle_token =\
          \ middle_token\n\nAttributeError: can't set attribute 'can_save_slow_tokenizer'\n\
          ```"
        updatedAt: '2023-08-31T12:54:49.969Z'
      numEdits: 0
      reactions: []
    id: 64f08d99de8b758e1789e6ad
    type: comment
  author: hawei-z
  content: "> same issue. \n> \n> ```\n> ---------------------------------------------------------------------------\n\
    > ImportError                               Traceback (most recent call last)\n\
    > Cell In[9], line 1\n> ----> 1 from transformers import CodeLlamaTokenizer\n\
    > \n> ImportError: cannot import name 'CodeLlamaTokenizer' from 'transformers'\
    \ (/Users/hawei/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/__init__.py)\n\
    > \n> ```\n\nThis error fix after I re-install main branch transformers. \n\n\
    But I get a new error.\n\n```\n---------------------------------------------------------------------------\n\
    AttributeError                            Traceback (most recent call last)\n\
    Cell In[5], line 1\n----> 1 tokenizer = AutoTokenizer.from_pretrained(model)\n\
    \nFile ~/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:735,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\n    731     if tokenizer_class is None:\n    732         raise ValueError(\n\
    \    733             f\"Tokenizer class {tokenizer_class_candidate} does not exist\
    \ or is not currently imported.\"\n    734         )\n--> 735     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\n    737 # Otherwise we have to be creative.\n    738 # if\
    \ model is an encoder decoder, the encoder tokenizer class is used by default\n\
    \    739 if isinstance(config, EncoderDecoderConfig):\n\nFile ~/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854,\
    \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
    \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
    \ **kwargs)\n   1851     else:\n   1852         logger.info(f\"loading file {file_path}\
    \ from cache at {resolved_vocab_files[file_id]}\")\n-> 1854 return cls._from_pretrained(\n\
    \   1855     resolved_vocab_files,\n   1856     pretrained_model_name_or_path,\n\
    \   1857     init_configuration,\n   1858     *init_inputs,\n   1859     token=token,\n\
    \   1860     cache_dir=cache_dir,\n   1861     local_files_only=local_files_only,\n\
    \   1862     _commit_hash=commit_hash,\n   1863     _is_local=is_local,\n   1864\
    \     **kwargs,\n   1865 )\n\nFile ~/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2017,\
    \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local,\
    \ *init_inputs, **kwargs)\n   2015 # Instantiate tokenizer.\n   2016 try:\n->\
    \ 2017     tokenizer = cls(*init_inputs, **init_kwargs)\n   2018 except OSError:\n\
    \   2019     raise OSError(\n   2020         \"Unable to load vocabulary from\
    \ file. \"\n   2021         \"Please check that the provided vocabulary is accessible\
    \ and not corrupted.\"\n   2022     )\n\nFile ~/miniconda3/envs/lang/lib/python3.10/site-packages/transformers/models/code_llama/tokenization_code_llama_fast.py:154,\
    \ in CodeLlamaTokenizerFast.__init__(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces,\
    \ unk_token, bos_token, eos_token, prefix_token, middle_token, suffix_token, eot_token,\
    \ fill_token, add_bos_token, add_eos_token, **kwargs)\n    151 self.update_post_processor()\n\
    \    153 self.vocab_file = vocab_file\n--> 154 self.can_save_slow_tokenizer =\
    \ False if not self.vocab_file else True\n    156 self._prefix_token = prefix_token\n\
    \    157 self._middle_token = middle_token\n\nAttributeError: can't set attribute\
    \ 'can_save_slow_tokenizer'\n```"
  created_at: 2023-08-31 11:54:49+00:00
  edited: false
  hidden: false
  id: 64f08d99de8b758e1789e6ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-08-31T15:14:14.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9951106905937195
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>This was fixed on main! </p>

          '
        raw: 'This was fixed on main! '
        updatedAt: '2023-08-31T15:14:14.067Z'
      numEdits: 0
      reactions: []
    id: 64f0ae46a72b9182bb346bee
    type: comment
  author: ArthurZ
  content: 'This was fixed on main! '
  created_at: 2023-08-31 14:14:14+00:00
  edited: false
  hidden: false
  id: 64f0ae46a72b9182bb346bee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663429462553-noauth.jpeg?w=200&h=200&f=face
      fullname: asd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hawei-z
      type: user
    createdAt: '2023-09-01T05:59:49.000Z'
    data:
      edited: false
      editors:
      - hawei-z
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9847589135169983
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663429462553-noauth.jpeg?w=200&h=200&f=face
          fullname: asd
          isHf: false
          isPro: false
          name: hawei-z
          type: user
        html: '<blockquote>

          <p>This was fixed on main!</p>

          </blockquote>

          <p>Actully I still face this issue.</p>

          '
        raw: '> This was fixed on main!


          Actully I still face this issue.'
        updatedAt: '2023-09-01T05:59:49.490Z'
      numEdits: 0
      reactions: []
    id: 64f17dd5e024fd084742284e
    type: comment
  author: hawei-z
  content: '> This was fixed on main!


    Actully I still face this issue.'
  created_at: 2023-09-01 04:59:49+00:00
  edited: false
  hidden: false
  id: 64f17dd5e024fd084742284e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6f6e321b31075b9820c1d50373f62fcd.svg
      fullname: Miguel Acevedo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: opencode
      type: user
    createdAt: '2023-09-01T17:14:56.000Z'
    data:
      edited: true
      editors:
      - opencode
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9310848116874695
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6f6e321b31075b9820c1d50373f62fcd.svg
          fullname: Miguel Acevedo
          isHf: false
          isPro: false
          name: opencode
          type: user
        html: '<p>pip installing from the main branch fixes the issue, but installing
          from the main branch will also cause a latency bug that slows down inference
          speed when using 4bit.</p>

          <p>Edit: Fixed by pip installing directly from the branch which added CodeLlama
          support: <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/25740">https://github.com/huggingface/transformers/pull/25740</a></p>

          <p>Installed using: <code>pip install git+https://github.com/ArthurZucker/transformers.git@add-llama-code</code></p>

          '
        raw: 'pip installing from the main branch fixes the issue, but installing
          from the main branch will also cause a latency bug that slows down inference
          speed when using 4bit.


          Edit: Fixed by pip installing directly from the branch which added CodeLlama
          support: https://github.com/huggingface/transformers/pull/25740


          Installed using: `pip install git+https://github.com/ArthurZucker/transformers.git@add-llama-code`'
        updatedAt: '2023-09-01T17:28:05.755Z'
      numEdits: 3
      reactions: []
    id: 64f21c10a30b70b737be859e
    type: comment
  author: opencode
  content: 'pip installing from the main branch fixes the issue, but installing from
    the main branch will also cause a latency bug that slows down inference speed
    when using 4bit.


    Edit: Fixed by pip installing directly from the branch which added CodeLlama support:
    https://github.com/huggingface/transformers/pull/25740


    Installed using: `pip install git+https://github.com/ArthurZucker/transformers.git@add-llama-code`'
  created_at: 2023-09-01 16:14:56+00:00
  edited: true
  hidden: false
  id: 64f21c10a30b70b737be859e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
      fullname: Pedro Cuenca
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: pcuenq
      type: user
    createdAt: '2023-09-02T09:19:57.000Z'
    data:
      edited: false
      editors:
      - pcuenq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9502658247947693
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg?w=200&h=200&f=face
          fullname: Pedro Cuenca
          isHf: true
          isPro: false
          name: pcuenq
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;opencode&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/opencode\">@<span class=\"\
          underline\">opencode</span></a></span>\n\n\t</span></span> could you please\
          \ explain more about the latency bug you mentioned?</p>\n"
        raw: '@opencode could you please explain more about the latency bug you mentioned?'
        updatedAt: '2023-09-02T09:19:57.413Z'
      numEdits: 0
      reactions: []
    id: 64f2fe3dd65ea0340ad28f39
    type: comment
  author: pcuenq
  content: '@opencode could you please explain more about the latency bug you mentioned?'
  created_at: 2023-09-02 08:19:57+00:00
  edited: false
  hidden: false
  id: 64f2fe3dd65ea0340ad28f39
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: codellama/CodeLlama-7b-Instruct-hf
repo_type: model
status: open
target_branch: null
title: Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported.
