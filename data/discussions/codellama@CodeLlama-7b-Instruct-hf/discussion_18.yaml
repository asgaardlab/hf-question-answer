!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JingyaoLi
conflicting_files: null
created_at: 2023-10-11 02:59:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/741096302cbdeb3670a8f31cf2d31d5d.svg
      fullname: Jingyao Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JingyaoLi
      type: user
    createdAt: '2023-10-11T03:59:03.000Z'
    data:
      edited: false
      editors:
      - JingyaoLi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4978606402873993
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/741096302cbdeb3670a8f31cf2d31d5d.svg
          fullname: Jingyao Li
          isHf: false
          isPro: false
          name: JingyaoLi
          type: user
        html: "<p>Hello, may I ask how you conducted testing on Humaneval? I attempted\
          \ to test using the two methods you provided in your Hugging Face blog,\
          \ including code completion and code infilling on Humaneval. However, I\
          \ only achieved results of 22% and 25% on Instruct CodeLLama 7B, which is\
          \ far from the reported 35%.</p>\n<pre><code>args.max_length = 1024\n\n\
          if args.task == 'code-infilling':\n    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)\n\
          \    pipeline = transformers.pipeline(\n        \"text-generation\",\n \
          \       model=args.model_path,\n        torch_dtype=torch.float16,\n   \
          \     device_map=\"auto\",\n    )\n    for task_id in problems:\n      \
          \  prompt = problems[task_id]['question'] + '&lt;FILL_ME&gt;\\n    return\
          \ result'\n        input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"\
          input_ids\"].to(\"cuda\")\n        output = model.generate(input_ids, max_new_tokens=args.max_length,)\n\
          \        output = output[0].to(\"cpu\")\n        filling = tokenizer.decode(output[input_ids.shape[1]:],\
          \ skip_special_tokens=True)\n        completion = prompt.replace(\"&lt;FILL_ME&gt;\"\
          , filling)\n\nelif args.task == 'code-completion':\n    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)\n\
          \    model = transformers.AutoModelForCausalLM.from_pretrained(\n      \
          \  args.model_path,\n        torch_dtype=torch.float16\n    ).to(\"cuda\"\
          )\n    for task_id in problems:\n        completion = pipeline(\n      \
          \      prompt,\n            do_sample=True,\n            temperature=0.2,\n\
          \            top_p=0.9,\n            num_return_sequences=1,\n         \
          \   eos_token_id=tokenizer.eos_token_id,\n            max_length=args.max_length,\n\
          \        )[0]['generated_text'].strip()\n</code></pre>\n"
        raw: "Hello, may I ask how you conducted testing on Humaneval? I attempted\
          \ to test using the two methods you provided in your Hugging Face blog,\
          \ including code completion and code infilling on Humaneval. However, I\
          \ only achieved results of 22% and 25% on Instruct CodeLLama 7B, which is\
          \ far from the reported 35%.\r\n```\r\nargs.max_length = 1024\r\n\r\nif\
          \ args.task == 'code-infilling':\r\n    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)\r\
          \n    pipeline = transformers.pipeline(\r\n        \"text-generation\",\r\
          \n        model=args.model_path,\r\n        torch_dtype=torch.float16,\r\
          \n        device_map=\"auto\",\r\n    )\r\n    for task_id in problems:\r\
          \n        prompt = problems[task_id]['question'] + '<FILL_ME>\\n    return\
          \ result'\r\n        input_ids = tokenizer(prompt, return_tensors=\"pt\"\
          )[\"input_ids\"].to(\"cuda\")\r\n        output = model.generate(input_ids,\
          \ max_new_tokens=args.max_length,)\r\n        output = output[0].to(\"cpu\"\
          )\r\n        filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)\r\
          \n        completion = prompt.replace(\"<FILL_ME>\", filling)\r\n\r\nelif\
          \ args.task == 'code-completion':\r\n    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)\r\
          \n    model = transformers.AutoModelForCausalLM.from_pretrained(\r\n   \
          \     args.model_path,\r\n        torch_dtype=torch.float16\r\n    ).to(\"\
          cuda\")\r\n    for task_id in problems:\r\n        completion = pipeline(\r\
          \n            prompt,\r\n            do_sample=True,\r\n            temperature=0.2,\r\
          \n            top_p=0.9,\r\n            num_return_sequences=1,\r\n    \
          \        eos_token_id=tokenizer.eos_token_id,\r\n            max_length=args.max_length,\r\
          \n        )[0]['generated_text'].strip()\r\n```"
        updatedAt: '2023-10-11T03:59:03.226Z'
      numEdits: 0
      reactions: []
    id: 65261d870c3abf6c7b95059f
    type: comment
  author: JingyaoLi
  content: "Hello, may I ask how you conducted testing on Humaneval? I attempted to\
    \ test using the two methods you provided in your Hugging Face blog, including\
    \ code completion and code infilling on Humaneval. However, I only achieved results\
    \ of 22% and 25% on Instruct CodeLLama 7B, which is far from the reported 35%.\r\
    \n```\r\nargs.max_length = 1024\r\n\r\nif args.task == 'code-infilling':\r\n \
    \   tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)\r\n\
    \    pipeline = transformers.pipeline(\r\n        \"text-generation\",\r\n   \
    \     model=args.model_path,\r\n        torch_dtype=torch.float16,\r\n       \
    \ device_map=\"auto\",\r\n    )\r\n    for task_id in problems:\r\n        prompt\
    \ = problems[task_id]['question'] + '<FILL_ME>\\n    return result'\r\n      \
    \  input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\"\
    )\r\n        output = model.generate(input_ids, max_new_tokens=args.max_length,)\r\
    \n        output = output[0].to(\"cpu\")\r\n        filling = tokenizer.decode(output[input_ids.shape[1]:],\
    \ skip_special_tokens=True)\r\n        completion = prompt.replace(\"<FILL_ME>\"\
    , filling)\r\n\r\nelif args.task == 'code-completion':\r\n    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)\r\
    \n    model = transformers.AutoModelForCausalLM.from_pretrained(\r\n        args.model_path,\r\
    \n        torch_dtype=torch.float16\r\n    ).to(\"cuda\")\r\n    for task_id in\
    \ problems:\r\n        completion = pipeline(\r\n            prompt,\r\n     \
    \       do_sample=True,\r\n            temperature=0.2,\r\n            top_p=0.9,\r\
    \n            num_return_sequences=1,\r\n            eos_token_id=tokenizer.eos_token_id,\r\
    \n            max_length=args.max_length,\r\n        )[0]['generated_text'].strip()\r\
    \n```"
  created_at: 2023-10-11 02:59:03+00:00
  edited: false
  hidden: false
  id: 65261d870c3abf6c7b95059f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: codellama/CodeLlama-7b-Instruct-hf
repo_type: model
status: open
target_branch: null
title: Can't reproduce the results on Humaneval
