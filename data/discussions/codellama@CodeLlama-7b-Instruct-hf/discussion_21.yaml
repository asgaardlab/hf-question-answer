!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ratar37003
conflicting_files: null
created_at: 2023-12-11 16:19:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45789eff9cce621a686d7f7fe5386a0f.svg
      fullname: Riyaj Atar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ratar37003
      type: user
    createdAt: '2023-12-11T16:19:49.000Z'
    data:
      edited: false
      editors:
      - Ratar37003
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4292621314525604
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45789eff9cce621a686d7f7fe5386a0f.svg
          fullname: Riyaj Atar
          isHf: false
          isPro: false
          name: Ratar37003
          type: user
        html: '<p>!pip install git+<a rel="nofollow" href="https://github.com/huggingface/transformers.git@main">https://github.com/huggingface/transformers.git@main</a>
          accelerate</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig<br>import
          torch</p>

          <p>model_id = "codellama/CodeLlama-7b-Instruct-hf"<br>quantization_config
          = BitsAndBytesConfig(<br>   load_in_4bit=True,<br>   bnb_4bit_compute_dtype=torch.float16<br>)</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_id)<br>model = AutoModelForCausalLM.from_pretrained(<br>    model_id,<br>    quantization_config=quantization_config,<br>    device_map="auto")</p>

          <p>OSError: codellama/CodeLlama-7b-Instruct-hf does not appear to have a
          file named config.json. Checkout ''<a href="https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/None''">https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/None''</a>
          for available files.</p>

          '
        raw: "\r\n!pip install git+https://github.com/huggingface/transformers.git@main\
          \ accelerate\r\n\r\n\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
          \ BitsAndBytesConfig\r\nimport torch\r\n\r\n\r\n\r\nmodel_id = \"codellama/CodeLlama-7b-Instruct-hf\"\
          \r\nquantization_config = BitsAndBytesConfig(\r\n   load_in_4bit=True,\r\
          \n   bnb_4bit_compute_dtype=torch.float16\r\n)\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\
          \nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_id,\r\n   \
          \ quantization_config=quantization_config,\r\n    device_map=\"auto\")\r\
          \n\r\n\r\n\r\nOSError: codellama/CodeLlama-7b-Instruct-hf does not appear\
          \ to have a file named config.json. Checkout 'https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/None'\
          \ for available files."
        updatedAt: '2023-12-11T16:19:49.198Z'
      numEdits: 0
      reactions: []
    id: 657736a5ddfd20e3cc0c2e30
    type: comment
  author: Ratar37003
  content: "\r\n!pip install git+https://github.com/huggingface/transformers.git@main\
    \ accelerate\r\n\r\n\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
    \ BitsAndBytesConfig\r\nimport torch\r\n\r\n\r\n\r\nmodel_id = \"codellama/CodeLlama-7b-Instruct-hf\"\
    \r\nquantization_config = BitsAndBytesConfig(\r\n   load_in_4bit=True,\r\n   bnb_4bit_compute_dtype=torch.float16\r\
    \n)\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\
    \n    model_id,\r\n    quantization_config=quantization_config,\r\n    device_map=\"\
    auto\")\r\n\r\n\r\n\r\nOSError: codellama/CodeLlama-7b-Instruct-hf does not appear\
    \ to have a file named config.json. Checkout 'https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/None'\
    \ for available files."
  created_at: 2023-12-11 16:19:49+00:00
  edited: false
  hidden: false
  id: 657736a5ddfd20e3cc0c2e30
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633d6d4f48ab6a0add2ce1a3/qTO75kR0hk1Yn1SaP7ZPb.jpeg?w=200&h=200&f=face
      fullname: Humza Sami
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: humza-sami
      type: user
    createdAt: '2023-12-24T20:11:33.000Z'
    data:
      edited: false
      editors:
      - humza-sami
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5426453351974487
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633d6d4f48ab6a0add2ce1a3/qTO75kR0hk1Yn1SaP7ZPb.jpeg?w=200&h=200&f=face
          fullname: Humza Sami
          isHf: false
          isPro: false
          name: humza-sami
          type: user
        html: '<p>Install transformers from here <code>pip install transformers</code></p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

          MODEL_NAME = <span class="hljs-string">"codellama/CodeLlama-7b-Instruct-hf"</span>

          model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=<span
          class="hljs-string">"auto"</span>,  trust_remote_code=<span class="hljs-literal">True</span>,
          load_in_8bit=<span class="hljs-literal">True</span>)

          </code></pre>

          '
        raw: 'Install transformers from here `pip install transformers`


          ```python

          from transformers import AutoModelForCausalLM

          MODEL_NAME = "codellama/CodeLlama-7b-Instruct-hf"

          model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map="auto",  trust_remote_code=True,
          load_in_8bit=True)

          ```'
        updatedAt: '2023-12-24T20:11:33.209Z'
      numEdits: 0
      reactions: []
    id: 658890756cf9325ae6c5ee6c
    type: comment
  author: humza-sami
  content: 'Install transformers from here `pip install transformers`


    ```python

    from transformers import AutoModelForCausalLM

    MODEL_NAME = "codellama/CodeLlama-7b-Instruct-hf"

    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map="auto",  trust_remote_code=True,
    load_in_8bit=True)

    ```'
  created_at: 2023-12-24 20:11:33+00:00
  edited: false
  hidden: false
  id: 658890756cf9325ae6c5ee6c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: codellama/CodeLlama-7b-Instruct-hf
repo_type: model
status: open
target_branch: null
title: unable to load
