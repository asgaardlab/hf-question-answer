!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andreydmitr20
conflicting_files: null
created_at: 2023-11-21 16:03:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/095e9d84227834e372f84bf695a139f5.svg
      fullname: Andrei Osetrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andreydmitr20
      type: user
    createdAt: '2023-11-21T16:03:07.000Z'
    data:
      edited: false
      editors:
      - andreydmitr20
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6097589731216431
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/095e9d84227834e372f84bf695a139f5.svg
          fullname: Andrei Osetrov
          isHf: false
          isPro: false
          name: andreydmitr20
          type: user
        html: '<p>Hi,<br>I''ve got llama.cpp working with ggml-model-q4_k.gguf on
          my notebook.<br>Now, I''m trying to run:</p>

          <p>python3 -m llava.serve.controller --host 0.0.0.0 --port 10000<br>python3
          -m llava.serve.model_worker --host 0.0.0.0 --controller <a rel="nofollow"
          href="http://localhost:10000">http://localhost:10000</a> --port 40000 --worker
          <a rel="nofollow" href="http://localhost:40000">http://localhost:40000</a>
          --model-path liuhaotian/llava-v1.5-7b --load-4bit</p>

          <p>and getting error: </p>

          <p>modeling_utils.py", line 2842, in from_pretrained<br>2023-11-21 16:41:10
          | ERROR | stderr |     raise ValueError(<br>2023-11-21 16:41:10 | ERROR
          | stderr | ValueError:<br>2023-11-21 16:41:10 | ERROR | stderr |                         Some
          modules are dispatched on the CPU or the disk. Make sure you have enough
          GPU RAM to fit<br>2023-11-21 16:41:10 | ERROR | stderr |                         the
          quantized model. If you want to dispatch the model on the CPU or the disk
          while keeping<br>2023-11-21 16:41:10 | ERROR | stderr |                         these
          modules in 32-bit, you need to set <code>load_in_8bit_fp32_cpu_offload=True</code>
          and pass a custom<br>2023-11-21 16:41:10 | ERROR | stderr |                         <code>device_map</code>
          to <code>from_pretrained</code>. Check<br>2023-11-21 16:41:10 | ERROR |
          stderr |                         <a href="https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu">https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu</a><br>2023-11-21
          16:41:10 | ERROR | stderr |                         for more details.</p>

          <p>What should I do to run this model with CPU only?<br>Thanks.</p>

          '
        raw: "Hi,\r\nI've got llama.cpp working with ggml-model-q4_k.gguf on my notebook.\r\
          \nNow, I'm trying to run:\r\n\r\npython3 -m llava.serve.controller --host\
          \ 0.0.0.0 --port 10000\r\npython3 -m llava.serve.model_worker --host 0.0.0.0\
          \ --controller http://localhost:10000 --port 40000 --worker http://localhost:40000\
          \ --model-path liuhaotian/llava-v1.5-7b --load-4bit\r\n\r\nand getting error:\
          \ \r\n\r\nmodeling_utils.py\", line 2842, in from_pretrained\r\n2023-11-21\
          \ 16:41:10 | ERROR | stderr |     raise ValueError(\r\n2023-11-21 16:41:10\
          \ | ERROR | stderr | ValueError:\r\n2023-11-21 16:41:10 | ERROR | stderr\
          \ |                         Some modules are dispatched on the CPU or the\
          \ disk. Make sure you have enough GPU RAM to fit\r\n2023-11-21 16:41:10\
          \ | ERROR | stderr |                         the quantized model. If you\
          \ want to dispatch the model on the CPU or the disk while keeping\r\n2023-11-21\
          \ 16:41:10 | ERROR | stderr |                         these modules in 32-bit,\
          \ you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\r\
          \n2023-11-21 16:41:10 | ERROR | stderr |                         `device_map`\
          \ to `from_pretrained`. Check\r\n2023-11-21 16:41:10 | ERROR | stderr |\
          \                         https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\r\
          \n2023-11-21 16:41:10 | ERROR | stderr |                         for more\
          \ details.\r\n\r\nWhat should I do to run this model with CPU only?\r\n\
          Thanks."
        updatedAt: '2023-11-21T16:03:07.057Z'
      numEdits: 0
      reactions: []
    id: 655cd4bb3f68c41a5ba7cb08
    type: comment
  author: andreydmitr20
  content: "Hi,\r\nI've got llama.cpp working with ggml-model-q4_k.gguf on my notebook.\r\
    \nNow, I'm trying to run:\r\n\r\npython3 -m llava.serve.controller --host 0.0.0.0\
    \ --port 10000\r\npython3 -m llava.serve.model_worker --host 0.0.0.0 --controller\
    \ http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path\
    \ liuhaotian/llava-v1.5-7b --load-4bit\r\n\r\nand getting error: \r\n\r\nmodeling_utils.py\"\
    , line 2842, in from_pretrained\r\n2023-11-21 16:41:10 | ERROR | stderr |    \
    \ raise ValueError(\r\n2023-11-21 16:41:10 | ERROR | stderr | ValueError:\r\n\
    2023-11-21 16:41:10 | ERROR | stderr |                         Some modules are\
    \ dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\r\
    \n2023-11-21 16:41:10 | ERROR | stderr |                         the quantized\
    \ model. If you want to dispatch the model on the CPU or the disk while keeping\r\
    \n2023-11-21 16:41:10 | ERROR | stderr |                         these modules\
    \ in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\r\
    \n2023-11-21 16:41:10 | ERROR | stderr |                         `device_map`\
    \ to `from_pretrained`. Check\r\n2023-11-21 16:41:10 | ERROR | stderr |      \
    \                   https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\r\
    \n2023-11-21 16:41:10 | ERROR | stderr |                         for more details.\r\
    \n\r\nWhat should I do to run this model with CPU only?\r\nThanks."
  created_at: 2023-11-21 16:03:07+00:00
  edited: false
  hidden: false
  id: 655cd4bb3f68c41a5ba7cb08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/388be61db2d85bc4ffa98beaf933c3e2.svg
      fullname: mOHAMED Nayeem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KIK99
      type: user
    createdAt: '2024-01-15T21:12:45.000Z'
    data:
      edited: false
      editors:
      - KIK99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9600262641906738
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/388be61db2d85bc4ffa98beaf933c3e2.svg
          fullname: mOHAMED Nayeem
          isHf: false
          isPro: false
          name: KIK99
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;andreydmitr20&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/andreydmitr20\"\
          >@<span class=\"underline\">andreydmitr20</span></a></span>\n\n\t</span></span>\
          \ could you please help me out to fine tune this model. could you please\
          \ let me know when can we connect on this?</p>\n"
        raw: '@andreydmitr20 could you please help me out to fine tune this model.
          could you please let me know when can we connect on this?'
        updatedAt: '2024-01-15T21:12:45.581Z'
      numEdits: 0
      reactions: []
    id: 65a59fcdc980ec2296629c55
    type: comment
  author: KIK99
  content: '@andreydmitr20 could you please help me out to fine tune this model. could
    you please let me know when can we connect on this?'
  created_at: 2024-01-15 21:12:45+00:00
  edited: false
  hidden: false
  id: 65a59fcdc980ec2296629c55
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: liuhaotian/llava-v1.5-7b
repo_type: model
status: open
target_branch: null
title: How could I deploy liuhaotian/llava-v1.5-7b on a server?
