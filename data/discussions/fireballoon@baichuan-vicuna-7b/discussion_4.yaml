!!python/object:huggingface_hub.community.DiscussionWithDetails
author: starvin-lw
conflicting_files: null
created_at: 2023-06-19 00:41:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31de55325b1e098ba3411fdcfde5dec8.svg
      fullname: Wei Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: starvin-lw
      type: user
    createdAt: '2023-06-19T01:41:59.000Z'
    data:
      edited: false
      editors:
      - starvin-lw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.17696301639080048
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31de55325b1e098ba3411fdcfde5dec8.svg
          fullname: Wei Li
          isHf: false
          isPro: false
          name: starvin-lw
          type: user
        html: "<p>I have load Baichuan-7B and Baichuan-vicuna-7B in Fastchat code,\
          \ and the load model changed to this:</p>\n<p>tokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name_or_path,\
          \ use_fast=False,add_bos_token=False, model_max_length=4096,padding_side=\"\
          right\",trust_remote_code=True)<br>model = transformers.AutoModelForCausalLM.from_pretrained(<br>\
          \        model_args.model_name_or_path,<br>        torch_dtype=torch.float16,<br>\
          \        trust_remote_code=True,)</p>\n<p>But the error named setStorage\
          \ appeared:</p>\n<p>\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback\
          \ (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256E<br>\u2502 /home/ec2-user/FastChat/fastchat/train/train_mem.py:13\
          \ in                          \u2502<br>\u2502                         \
          \                                                                      \
          \   \u2502<br>\u2502   10 from fastchat.train.train import train       \
          \                                               \u2502<br>\u2502   11  \
          \                                                                      \
          \                     \u2502<br>\u2502   12 if <strong>name</strong> ==\
          \ \"<strong>main</strong>\":                                           \
          \                       \u2502<br>\u2502 \u2771 13 \u2502   train()    \
          \                                                                      \
          \       \u2502<br>\u2502   14                                          \
          \                                                   \u2502<br>\u2502   \
          \                                                                      \
          \                         \u2502<br>\u2502 /home/ec2-user/FastChat/fastchat/train/train.py:282\
          \ in train                               \u2502<br>\u2502              \
          \                                                                      \
          \              \u2502<br>\u2502   279 \u2502   if list(pathlib.Path(training_args.output_dir).glob(\"\
          checkpoint-*\")):                  \u2502<br>\u2502   280 \u2502   \u2502\
          \   trainer.train(resume_from_checkpoint=True)                         \
          \                \u2502<br>\u2502   281 \u2502   else:                 \
          \                                                                 \u2502\
          <br>\u2502 \u2771 282 \u2502   \u2502   trainer.train()                \
          \                                                    \u2502<br>\u2502  \
          \ 283 \u2502   trainer.save_state()                                    \
          \                               \u2502<br>\u2502   284 \u2502   safe_save_model_for_hf_trainer(trainer=trainer,\
          \ output_dir=training_args.output_dir)   \u2502<br>\u2502   285        \
          \                                                                      \
          \              \u2502<br>\u2502                                        \
          \                                                          \u2502<br>\u2502\
          \ /home/ec2-user/anaconda3/envs/liwei2/lib/python3.10/site-packages/transformers/trainer.py:1664\
          \   \u2502<br>\u2502 in train                                          \
          \                                               \u2502<br>\u2502       \
          \                                                                      \
          \                     \u2502<br>\u2502   1661 \u2502   \u2502   inner_training_loop\
          \ = find_executable_batch_size(                                 \u2502<br>\u2502\
          \   1662 \u2502   \u2502   \u2502   self._inner_training_loop, self._train_batch_size,\
          \ args.auto_find_batch_size  \u2502<br>\u2502   1663 \u2502   \u2502   )\
          \                                                                      \
          \           \u2502<br>\u2502 \u2771 1664 \u2502   \u2502   return inner_training_loop(\
          \                                                       \u2502<br>\u2502\
          \   1665 \u2502   \u2502   \u2502   args=args,                         \
          \                                           \u2502<br>\u2502   1666 \u2502\
          \   \u2502   \u2502   resume_from_checkpoint=resume_from_checkpoint,   \
          \                             \u2502<br>\u2502   1667 \u2502   \u2502  \
          \ \u2502   trial=trial,                                                \
          \                  \u2502<br>\u2502                                    \
          \                                                              \u2502<br>\u2502\
          \ /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/transformers/trainer.py:1938\
          \   \u2502<br>\u2502 in _inner_training_loop                           \
          \                                               \u2502<br>\u2502       \
          \                                                                      \
          \                     \u2502<br>\u2502   1935 \u2502   \u2502   \u2502 \
          \  \u2502   ):                                                         \
          \               \u2502<br>\u2502   1936 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   # Avoid unnecessary DDP synchronization since there will be\
          \ no backw  \u2502<br>\u2502   1937 \u2502   \u2502   \u2502   \u2502  \
          \ \u2502   with model.no_sync():                                       \
          \          \u2502<br>\u2502 \u2771 1938 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   tr_loss_step = self.training_step(model, inputs) \
          \                 \u2502<br>\u2502   1939 \u2502   \u2502   \u2502   \u2502\
          \   else:                                                              \
          \       \u2502<br>\u2502   1940 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   tr_loss_step = self.training_step(model, inputs)                   \
          \   \u2502<br>\u2502   1941                                            \
          \                                               \u2502<br>\u2502       \
          \                                                                      \
          \                     \u2502<br>\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/transformers/trainer.py:2753\
          \   \u2502<br>\u2502 in training_step                                  \
          \                                               \u2502<br>\u2502       \
          \                                                                      \
          \                     \u2502<br>\u2502   2750 \u2502   \u2502   \u2502 \
          \  # loss gets scaled under gradient_accumulation_steps in deepspeed   \
          \          \u2502<br>\u2502   2751 \u2502   \u2502   \u2502   loss = self.deepspeed.backward(loss)\
          \                                          \u2502<br>\u2502   2752 \u2502\
          \   \u2502   else:                                                     \
          \                        \u2502<br>\u2502 \u2771 2753 \u2502   \u2502  \
          \ \u2502   loss.backward()                                             \
          \                  \u2502<br>\u2502   2754 \u2502   \u2502             \
          \                                                                      \
          \  \u2502<br>\u2502   2755 \u2502   \u2502   return loss.detach()      \
          \                                                        \u2502<br>\u2502\
          \   2756                                                               \
          \                            \u2502<br>\u2502                          \
          \                                                                      \
          \  \u2502<br>\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/_tensor.py:487\
          \ in        \u2502<br>\u2502 backward                                  \
          \                                                       \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502    484 \u2502   \u2502   \u2502\
          \   \u2502   create_graph=create_graph,                                \
          \                \u2502<br>\u2502    485 \u2502   \u2502   \u2502   \u2502\
          \   inputs=inputs,                                                     \
          \       \u2502<br>\u2502    486 \u2502   \u2502   \u2502   )           \
          \                                                                  \u2502\
          <br>\u2502 \u2771  487 \u2502   \u2502   torch.autograd.backward(      \
          \                                                    \u2502<br>\u2502  \
          \  488 \u2502   \u2502   \u2502   self, gradient, retain_graph, create_graph,\
          \ inputs=inputs                     \u2502<br>\u2502    489 \u2502   \u2502\
          \   )                                                                  \
          \               \u2502<br>\u2502    490                                \
          \                                                           \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/autograd/<strong>init</strong>.py:200\
          \ \u2502<br>\u2502 in backward                                         \
          \                                             \u2502<br>\u2502         \
          \                                                                      \
          \                   \u2502<br>\u2502   197 \u2502   # The reason we repeat\
          \ same the comment below is that                                  \u2502\
          <br>\u2502   198 \u2502   # some Python versions print out the first line\
          \ of a multi-line function               \u2502<br>\u2502   199 \u2502 \
          \  # calls in the traceback and some print out the last line           \
          \                   \u2502<br>\u2502 \u2771 200 \u2502   Variable.<em>execution_engine.run_backward(\
          \  # Calls into the C++ engine to run the bac   \u2502<br>\u2502   201 \u2502\
          \   \u2502   tensors, grad_tensors</em>, retain_graph, create_graph, inputs,\
          \                        \u2502<br>\u2502   202 \u2502   \u2502   allow_unreachable=True,\
          \ accumulate_grad=True)  # Calls into the C++ engine to ru   \u2502<br>\u2502\
          \   203                                                                \
          \                            \u2502<br>\u2502                          \
          \                                                                      \
          \  \u2502<br>\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/autograd/function.py:274\
          \ \u2502<br>\u2502 in apply                                            \
          \                                             \u2502<br>\u2502         \
          \                                                                      \
          \                   \u2502<br>\u2502   271 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502      \"Function is not allowed. You should\
          \ only implement one \"   \u2502<br>\u2502   272 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502      \"of them.\")               \
          \                                  \u2502<br>\u2502   273 \u2502   \u2502\
          \   user_fn = vjp_fn if vjp_fn is not Function.vjp else backward_fn    \
          \                \u2502<br>\u2502 \u2771 274 \u2502   \u2502   return user_fn(self,\
          \ *args)                                                        \u2502<br>\u2502\
          \   275 \u2502                                                         \
          \                                 \u2502<br>\u2502   276 \u2502   def apply_jvp(self,\
          \ *args):                                                            \u2502\
          <br>\u2502   277 \u2502   \u2502   # _forward_cls is defined by derived\
          \ class                                         \u2502<br>\u2502       \
          \                                                                      \
          \                     \u2502<br>\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/utils/checkpoint.py:141\
          \  \u2502<br>\u2502 in backward                                        \
          \                                              \u2502<br>\u2502        \
          \                                                                      \
          \                    \u2502<br>\u2502   138 \u2502   \u2502   \u2502   with\
          \ torch.enable_grad(), \\                                              \
          \      \u2502<br>\u2502   139 \u2502   \u2502   \u2502   \u2502    torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs),\
          \ \\                     \u2502<br>\u2502   140 \u2502   \u2502   \u2502\
          \   \u2502    torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):       \
          \                 \u2502<br>\u2502 \u2771 141 \u2502   \u2502   \u2502 \
          \  \u2502   outputs = ctx.run_function(*detached_inputs)               \
          \                \u2502<br>\u2502   142 \u2502   \u2502                \
          \                                                                      \u2502\
          <br>\u2502   143 \u2502   \u2502   if isinstance(outputs, torch.Tensor):\
          \                                              \u2502<br>\u2502   144 \u2502\
          \   \u2502   \u2502   outputs = (outputs,)                             \
          \                              \u2502<br>\u2502                        \
          \                                                                      \
          \    \u2502<br>\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/transformers/models/llama/mode\
          \ \u2502<br>\u2502 ling_llama.py:566 in custom_forward                 \
          \                                             \u2502<br>\u2502         \
          \                                                                      \
          \                   \u2502<br>\u2502   563 \u2502   \u2502   \u2502   \u2502\
          \   def create_custom_forward(module):                                 \
          \        \u2502<br>\u2502   564 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   def custom_forward(*inputs):                                       \
          \    \u2502<br>\u2502   565 \u2502   \u2502   \u2502   \u2502   \u2502 \
          \  \u2502   # None for past_key_value                                  \
          \        \u2502<br>\u2502 \u2771 566 \u2502   \u2502   \u2502   \u2502 \
          \  \u2502   \u2502   return module(*inputs, output_attentions, None)   \
          \                 \u2502<br>\u2502   567 \u2502   \u2502   \u2502   \u2502\
          \   \u2502                                                             \
          \             \u2502<br>\u2502   568 \u2502   \u2502   \u2502   \u2502 \
          \  \u2502   return custom_forward                                      \
          \            \u2502<br>\u2502   569                                    \
          \                                                        \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/nn/modules/module.py:150\
          \ \u2502<br>\u2502 1 in _call_impl                                     \
          \                                             \u2502<br>\u2502         \
          \                                                                      \
          \                   \u2502<br>\u2502   1498 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._backward_pre_hooks or self._forward_hooks   \u2502<br>\u2502\
          \   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks\
          \ or _global_backward_hooks                   \u2502<br>\u2502   1500 \u2502\
          \   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):\
          \                   \u2502<br>\u2502 \u2771 1501 \u2502   \u2502   \u2502\
          \   return forward_call(*args, **kwargs)                               \
          \           \u2502<br>\u2502   1502 \u2502   \u2502   # Do not call functions\
          \ when jit is used                                          \u2502<br>\u2502\
          \   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []                             \u2502<br>\u2502   1504 \u2502  \
          \ \u2502   backward_pre_hooks = []                                     \
          \                      \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/transformers/models/llama/mode\
          \ \u2502<br>\u2502 ling_llama.py:293 in forward                        \
          \                                             \u2502<br>\u2502         \
          \                                                                      \
          \                   \u2502<br>\u2502   290 \u2502   \u2502   hidden_states\
          \ = self.input_layernorm(hidden_states)                                \u2502\
          <br>\u2502   291 \u2502   \u2502                                       \
          \                                               \u2502<br>\u2502   292 \u2502\
          \   \u2502   # Self Attention                                          \
          \                         \u2502<br>\u2502 \u2771 293 \u2502   \u2502  \
          \ hidden_states, self_attn_weights, present_key_value = self.self_attn(\
          \              \u2502<br>\u2502   294 \u2502   \u2502   \u2502   hidden_states=hidden_states,\
          \                                                   \u2502<br>\u2502   295\
          \ \u2502   \u2502   \u2502   attention_mask=attention_mask,            \
          \                                     \u2502<br>\u2502   296 \u2502   \u2502\
          \   \u2502   position_ids=position_ids,                                \
          \                     \u2502<br>\u2502                                 \
          \                                                                 \u2502\
          <br>\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/nn/modules/module.py:150\
          \ \u2502<br>\u2502 1 in _call_impl                                     \
          \                                             \u2502<br>\u2502         \
          \                                                                      \
          \                   \u2502<br>\u2502   1498 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._backward_pre_hooks or self._forward_hooks   \u2502<br>\u2502\
          \   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks\
          \ or _global_backward_hooks                   \u2502<br>\u2502   1500 \u2502\
          \   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):\
          \                   \u2502<br>\u2502 \u2771 1501 \u2502   \u2502   \u2502\
          \   return forward_call(*args, **kwargs)                               \
          \           \u2502<br>\u2502   1502 \u2502   \u2502   # Do not call functions\
          \ when jit is used                                          \u2502<br>\u2502\
          \   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []                             \u2502<br>\u2502   1504 \u2502  \
          \ \u2502   backward_pre_hooks = []                                     \
          \                      \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502 /home/ec2-user/FastChat/fastchat/train/llama_flash_attn_monkey_patch.py:32\
          \ in forward      \u2502<br>\u2502                                     \
          \                                                             \u2502<br>\u2502\
          \    29 \u2502   bsz, q_len, _ = hidden_states.size()                  \
          \                                 \u2502<br>\u2502    30 \u2502        \
          \                                                                      \
          \            \u2502<br>\u2502    31 \u2502   query_states = (          \
          \                                                             \u2502<br>\u2502\
          \ \u2771  32 \u2502   \u2502   self.q_proj(hidden_states)              \
          \                                           \u2502<br>\u2502    33 \u2502\
          \   \u2502   .view(bsz, q_len, self.num_heads, self.head_dim)          \
          \                         \u2502<br>\u2502    34 \u2502   \u2502   .transpose(1,\
          \ 2)                                                                   \u2502\
          <br>\u2502    35 \u2502   )                                            \
          \                                          \u2502<br>\u2502            \
          \                                                                      \
          \                \u2502<br>\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/nn/modules/module.py:150\
          \ \u2502<br>\u2502 1 in _call_impl                                     \
          \                                             \u2502<br>\u2502         \
          \                                                                      \
          \                   \u2502<br>\u2502   1498 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._backward_pre_hooks or self._forward_hooks   \u2502<br>\u2502\
          \   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks\
          \ or _global_backward_hooks                   \u2502<br>\u2502   1500 \u2502\
          \   \u2502   \u2502   \u2502   or _global_forward_hooks or <em>global_forward_pre_hooks):\
          \                   \u2502<br>\u2502 \u2771 1501 \u2502   \u2502   \u2502\
          \   return forward_call(*args, **kwargs)                               \
          \           \u2502<br>\u2502   1502 \u2502   \u2502   # Do not call functions\
          \ when jit is used                                          \u2502<br>\u2502\
          \   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []                             \u2502<br>\u2502   1504 \u2502  \
          \ \u2502   backward_pre_hooks = []                                     \
          \                      \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\
          \ \u2502<br>\u2502 in forward                                          \
          \                                             \u2502<br>\u2502         \
          \                                                                      \
          \                   \u2502<br>\u2502   111 \u2502   \u2502   \u2502   init.uniform</em>(self.bias,\
          \ -bound, bound)                                        \u2502<br>\u2502\
          \   112 \u2502                                                         \
          \                                 \u2502<br>\u2502   113 \u2502   def forward(self,\
          \ input: Tensor) -&gt; Tensor:                                         \
          \   \u2502<br>\u2502 \u2771 114 \u2502   \u2502   return F.linear(input,\
          \ self.weight, self.bias)                                     \u2502<br>\u2502\
          \   115 \u2502                                                         \
          \                                 \u2502<br>\u2502   116 \u2502   def extra_repr(self)\
          \ -&gt; str:                                                           \u2502\
          <br>\u2502   117 \u2502   \u2502   return 'in_features={}, out_features={},\
          \ bias={}'.format(                          \u2502<br>\u2570\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256F<br>RuntimeError: setStorage: sizes [4096, 4096], strides [1, 4096],\
          \ storage offset 6333644800, and itemsize 2 requiring<br>a storage size\
          \ of 12700844032 are out of bounds for storage of size 0</p>\n<p>And the\
          \ error in Baichuan-7B is</p>\n<p>RuntimeError: setStorage: sizes [4096,\
          \ 12288], strides [1, 4096], storage offset 6333644800, and itemsize 2 requiring<br>a\
          \ storage size of 12700844032 are out of bounds for storage of size 0</p>\n\
          <p>Have you ever meet this problem?</p>\n"
        raw: "I have load Baichuan-7B and Baichuan-vicuna-7B in Fastchat code, and\
          \ the load model changed to this:\r\n\r\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name_or_path,\
          \ use_fast=False,add_bos_token=False, model_max_length=4096,padding_side=\"\
          right\",trust_remote_code=True)\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\
          \n        model_args.model_name_or_path, \r\n        torch_dtype=torch.float16,\r\
          \n        trust_remote_code=True,)\r\n\r\nBut the error named setStorage\
          \ appeared:\r\n\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback\
          \ (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256E\r\n\u2502 /home/ec2-user/FastChat/fastchat/train/train_mem.py:13\
          \ in <module>                         \u2502\r\n\u2502                 \
          \                                                                      \
          \           \u2502\r\n\u2502   10 from fastchat.train.train import train\
          \                                                      \u2502\r\n\u2502\
          \   11                                                                 \
          \                            \u2502\r\n\u2502   12 if __name__ == \"__main__\"\
          :                                                                  \u2502\
          \r\n\u2502 \u2771 13 \u2502   train()                                  \
          \                                               \u2502\r\n\u2502   14  \
          \                                                                      \
          \                     \u2502\r\n\u2502                                 \
          \                                                                 \u2502\
          \r\n\u2502 /home/ec2-user/FastChat/fastchat/train/train.py:282 in train\
          \                               \u2502\r\n\u2502                       \
          \                                                                      \
          \     \u2502\r\n\u2502   279 \u2502   if list(pathlib.Path(training_args.output_dir).glob(\"\
          checkpoint-*\")):                  \u2502\r\n\u2502   280 \u2502   \u2502\
          \   trainer.train(resume_from_checkpoint=True)                         \
          \                \u2502\r\n\u2502   281 \u2502   else:                 \
          \                                                                 \u2502\
          \r\n\u2502 \u2771 282 \u2502   \u2502   trainer.train()                \
          \                                                    \u2502\r\n\u2502  \
          \ 283 \u2502   trainer.save_state()                                    \
          \                               \u2502\r\n\u2502   284 \u2502   safe_save_model_for_hf_trainer(trainer=trainer,\
          \ output_dir=training_args.output_dir)   \u2502\r\n\u2502   285        \
          \                                                                      \
          \              \u2502\r\n\u2502                                        \
          \                                                          \u2502\r\n\u2502\
          \ /home/ec2-user/anaconda3/envs/liwei2/lib/python3.10/site-packages/transformers/trainer.py:1664\
          \   \u2502\r\n\u2502 in train                                          \
          \                                               \u2502\r\n\u2502       \
          \                                                                      \
          \                     \u2502\r\n\u2502   1661 \u2502   \u2502   inner_training_loop\
          \ = find_executable_batch_size(                                 \u2502\r\
          \n\u2502   1662 \u2502   \u2502   \u2502   self._inner_training_loop, self._train_batch_size,\
          \ args.auto_find_batch_size  \u2502\r\n\u2502   1663 \u2502   \u2502   )\
          \                                                                      \
          \           \u2502\r\n\u2502 \u2771 1664 \u2502   \u2502   return inner_training_loop(\
          \                                                       \u2502\r\n\u2502\
          \   1665 \u2502   \u2502   \u2502   args=args,                         \
          \                                           \u2502\r\n\u2502   1666 \u2502\
          \   \u2502   \u2502   resume_from_checkpoint=resume_from_checkpoint,   \
          \                             \u2502\r\n\u2502   1667 \u2502   \u2502  \
          \ \u2502   trial=trial,                                                \
          \                  \u2502\r\n\u2502                                    \
          \                                                              \u2502\r\n\
          \u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/transformers/trainer.py:1938\
          \   \u2502\r\n\u2502 in _inner_training_loop                           \
          \                                               \u2502\r\n\u2502       \
          \                                                                      \
          \                     \u2502\r\n\u2502   1935 \u2502   \u2502   \u2502 \
          \  \u2502   ):                                                         \
          \               \u2502\r\n\u2502   1936 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   # Avoid unnecessary DDP synchronization since there will be\
          \ no backw  \u2502\r\n\u2502   1937 \u2502   \u2502   \u2502   \u2502  \
          \ \u2502   with model.no_sync():                                       \
          \          \u2502\r\n\u2502 \u2771 1938 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   tr_loss_step = self.training_step(model, inputs) \
          \                 \u2502\r\n\u2502   1939 \u2502   \u2502   \u2502   \u2502\
          \   else:                                                              \
          \       \u2502\r\n\u2502   1940 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   tr_loss_step = self.training_step(model, inputs)                   \
          \   \u2502\r\n\u2502   1941                                            \
          \                                               \u2502\r\n\u2502       \
          \                                                                      \
          \                     \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/transformers/trainer.py:2753\
          \   \u2502\r\n\u2502 in training_step                                  \
          \                                               \u2502\r\n\u2502       \
          \                                                                      \
          \                     \u2502\r\n\u2502   2750 \u2502   \u2502   \u2502 \
          \  # loss gets scaled under gradient_accumulation_steps in deepspeed   \
          \          \u2502\r\n\u2502   2751 \u2502   \u2502   \u2502   loss = self.deepspeed.backward(loss)\
          \                                          \u2502\r\n\u2502   2752 \u2502\
          \   \u2502   else:                                                     \
          \                        \u2502\r\n\u2502 \u2771 2753 \u2502   \u2502  \
          \ \u2502   loss.backward()                                             \
          \                  \u2502\r\n\u2502   2754 \u2502   \u2502             \
          \                                                                      \
          \  \u2502\r\n\u2502   2755 \u2502   \u2502   return loss.detach()      \
          \                                                        \u2502\r\n\u2502\
          \   2756                                                               \
          \                            \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/_tensor.py:487\
          \ in        \u2502\r\n\u2502 backward                                  \
          \                                                       \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502    484 \u2502   \u2502   \u2502\
          \   \u2502   create_graph=create_graph,                                \
          \                \u2502\r\n\u2502    485 \u2502   \u2502   \u2502   \u2502\
          \   inputs=inputs,                                                     \
          \       \u2502\r\n\u2502    486 \u2502   \u2502   \u2502   )           \
          \                                                                  \u2502\
          \r\n\u2502 \u2771  487 \u2502   \u2502   torch.autograd.backward(      \
          \                                                    \u2502\r\n\u2502  \
          \  488 \u2502   \u2502   \u2502   self, gradient, retain_graph, create_graph,\
          \ inputs=inputs                     \u2502\r\n\u2502    489 \u2502   \u2502\
          \   )                                                                  \
          \               \u2502\r\n\u2502    490                                \
          \                                                           \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/autograd/__init__.py:200\
          \ \u2502\r\n\u2502 in backward                                         \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   197 \u2502   # The reason we repeat\
          \ same the comment below is that                                  \u2502\
          \r\n\u2502   198 \u2502   # some Python versions print out the first line\
          \ of a multi-line function               \u2502\r\n\u2502   199 \u2502 \
          \  # calls in the traceback and some print out the last line           \
          \                   \u2502\r\n\u2502 \u2771 200 \u2502   Variable._execution_engine.run_backward(\
          \  # Calls into the C++ engine to run the bac   \u2502\r\n\u2502   201 \u2502\
          \   \u2502   tensors, grad_tensors_, retain_graph, create_graph, inputs,\
          \                        \u2502\r\n\u2502   202 \u2502   \u2502   allow_unreachable=True,\
          \ accumulate_grad=True)  # Calls into the C++ engine to ru   \u2502\r\n\u2502\
          \   203                                                                \
          \                            \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/autograd/function.py:274\
          \ \u2502\r\n\u2502 in apply                                            \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   271 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502      \"Function is not allowed. You should\
          \ only implement one \"   \u2502\r\n\u2502   272 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   \u2502   \u2502      \"of them.\")               \
          \                                  \u2502\r\n\u2502   273 \u2502   \u2502\
          \   user_fn = vjp_fn if vjp_fn is not Function.vjp else backward_fn    \
          \                \u2502\r\n\u2502 \u2771 274 \u2502   \u2502   return user_fn(self,\
          \ *args)                                                        \u2502\r\
          \n\u2502   275 \u2502                                                  \
          \                                        \u2502\r\n\u2502   276 \u2502 \
          \  def apply_jvp(self, *args):                                         \
          \                   \u2502\r\n\u2502   277 \u2502   \u2502   # _forward_cls\
          \ is defined by derived class                                         \u2502\
          \r\n\u2502                                                             \
          \                                     \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/utils/checkpoint.py:141\
          \  \u2502\r\n\u2502 in backward                                        \
          \                                              \u2502\r\n\u2502        \
          \                                                                      \
          \                    \u2502\r\n\u2502   138 \u2502   \u2502   \u2502   with\
          \ torch.enable_grad(), \\                                              \
          \      \u2502\r\n\u2502   139 \u2502   \u2502   \u2502   \u2502    torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs),\
          \ \\                     \u2502\r\n\u2502   140 \u2502   \u2502   \u2502\
          \   \u2502    torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):       \
          \                 \u2502\r\n\u2502 \u2771 141 \u2502   \u2502   \u2502 \
          \  \u2502   outputs = ctx.run_function(*detached_inputs)               \
          \                \u2502\r\n\u2502   142 \u2502   \u2502                \
          \                                                                      \u2502\
          \r\n\u2502   143 \u2502   \u2502   if isinstance(outputs, torch.Tensor):\
          \                                              \u2502\r\n\u2502   144 \u2502\
          \   \u2502   \u2502   outputs = (outputs,)                             \
          \                              \u2502\r\n\u2502                        \
          \                                                                      \
          \    \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/transformers/models/llama/mode\
          \ \u2502\r\n\u2502 ling_llama.py:566 in custom_forward                 \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   563 \u2502   \u2502   \u2502   \u2502\
          \   def create_custom_forward(module):                                 \
          \        \u2502\r\n\u2502   564 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   def custom_forward(*inputs):                                       \
          \    \u2502\r\n\u2502   565 \u2502   \u2502   \u2502   \u2502   \u2502 \
          \  \u2502   # None for past_key_value                                  \
          \        \u2502\r\n\u2502 \u2771 566 \u2502   \u2502   \u2502   \u2502 \
          \  \u2502   \u2502   return module(*inputs, output_attentions, None)   \
          \                 \u2502\r\n\u2502   567 \u2502   \u2502   \u2502   \u2502\
          \   \u2502                                                             \
          \             \u2502\r\n\u2502   568 \u2502   \u2502   \u2502   \u2502 \
          \  \u2502   return custom_forward                                      \
          \            \u2502\r\n\u2502   569                                    \
          \                                                        \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/nn/modules/module.py:150\
          \ \u2502\r\n\u2502 1 in _call_impl                                     \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502\
          \   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks\
          \ or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502\
          \   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):\
          \                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502\
          \   return forward_call(*args, **kwargs)                               \
          \           \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions\
          \ when jit is used                                          \u2502\r\n\u2502\
          \   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []                             \u2502\r\n\u2502   1504 \u2502  \
          \ \u2502   backward_pre_hooks = []                                     \
          \                      \u2502\r\n\u2502                                \
          \                                                                  \u2502\
          \r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/transformers/models/llama/mode\
          \ \u2502\r\n\u2502 ling_llama.py:293 in forward                        \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   290 \u2502   \u2502   hidden_states\
          \ = self.input_layernorm(hidden_states)                                \u2502\
          \r\n\u2502   291 \u2502   \u2502                                       \
          \                                               \u2502\r\n\u2502   292 \u2502\
          \   \u2502   # Self Attention                                          \
          \                         \u2502\r\n\u2502 \u2771 293 \u2502   \u2502  \
          \ hidden_states, self_attn_weights, present_key_value = self.self_attn(\
          \              \u2502\r\n\u2502   294 \u2502   \u2502   \u2502   hidden_states=hidden_states,\
          \                                                   \u2502\r\n\u2502   295\
          \ \u2502   \u2502   \u2502   attention_mask=attention_mask,            \
          \                                     \u2502\r\n\u2502   296 \u2502   \u2502\
          \   \u2502   position_ids=position_ids,                                \
          \                     \u2502\r\n\u2502                                 \
          \                                                                 \u2502\
          \r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/nn/modules/module.py:150\
          \ \u2502\r\n\u2502 1 in _call_impl                                     \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502\
          \   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks\
          \ or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502\
          \   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):\
          \                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502\
          \   return forward_call(*args, **kwargs)                               \
          \           \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions\
          \ when jit is used                                          \u2502\r\n\u2502\
          \   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []                             \u2502\r\n\u2502   1504 \u2502  \
          \ \u2502   backward_pre_hooks = []                                     \
          \                      \u2502\r\n\u2502                                \
          \                                                                  \u2502\
          \r\n\u2502 /home/ec2-user/FastChat/fastchat/train/llama_flash_attn_monkey_patch.py:32\
          \ in forward      \u2502\r\n\u2502                                     \
          \                                                             \u2502\r\n\
          \u2502    29 \u2502   bsz, q_len, _ = hidden_states.size()             \
          \                                      \u2502\r\n\u2502    30 \u2502   \
          \                                                                      \
          \                 \u2502\r\n\u2502    31 \u2502   query_states = (     \
          \                                                                  \u2502\
          \r\n\u2502 \u2771  32 \u2502   \u2502   self.q_proj(hidden_states)     \
          \                                                    \u2502\r\n\u2502  \
          \  33 \u2502   \u2502   .view(bsz, q_len, self.num_heads, self.head_dim)\
          \                                   \u2502\r\n\u2502    34 \u2502   \u2502\
          \   .transpose(1, 2)                                                   \
          \                \u2502\r\n\u2502    35 \u2502   )                     \
          \                                                                 \u2502\
          \r\n\u2502                                                             \
          \                                     \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/nn/modules/module.py:150\
          \ \u2502\r\n\u2502 1 in _call_impl                                     \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks\
          \ or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502\
          \   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks\
          \ or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502\
          \   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):\
          \                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502\
          \   return forward_call(*args, **kwargs)                               \
          \           \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions\
          \ when jit is used                                          \u2502\r\n\u2502\
          \   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
          \ = [], []                             \u2502\r\n\u2502   1504 \u2502  \
          \ \u2502   backward_pre_hooks = []                                     \
          \                      \u2502\r\n\u2502                                \
          \                                                                  \u2502\
          \r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\
          \ \u2502\r\n\u2502 in forward                                          \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   111 \u2502   \u2502   \u2502   init.uniform_(self.bias,\
          \ -bound, bound)                                        \u2502\r\n\u2502\
          \   112 \u2502                                                         \
          \                                 \u2502\r\n\u2502   113 \u2502   def forward(self,\
          \ input: Tensor) -> Tensor:                                            \u2502\
          \r\n\u2502 \u2771 114 \u2502   \u2502   return F.linear(input, self.weight,\
          \ self.bias)                                     \u2502\r\n\u2502   115\
          \ \u2502                                                               \
          \                           \u2502\r\n\u2502   116 \u2502   def extra_repr(self)\
          \ -> str:                                                           \u2502\
          \r\n\u2502   117 \u2502   \u2502   return 'in_features={}, out_features={},\
          \ bias={}'.format(                          \u2502\r\n\u2570\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u256F\r\nRuntimeError: setStorage: sizes [4096, 4096], strides [1, 4096],\
          \ storage offset 6333644800, and itemsize 2 requiring\r\na storage size\
          \ of 12700844032 are out of bounds for storage of size 0\r\n\r\nAnd the\
          \ error in Baichuan-7B is\r\n \r\nRuntimeError: setStorage: sizes [4096,\
          \ 12288], strides [1, 4096], storage offset 6333644800, and itemsize 2 requiring\r\
          \na storage size of 12700844032 are out of bounds for storage of size 0\r\
          \n\r\nHave you ever meet this problem?"
        updatedAt: '2023-06-19T01:41:59.782Z'
      numEdits: 0
      reactions: []
    id: 648fb267c124ea06b98cc8dd
    type: comment
  author: starvin-lw
  content: "I have load Baichuan-7B and Baichuan-vicuna-7B in Fastchat code, and the\
    \ load model changed to this:\r\n\r\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_args.model_name_or_path,\
    \ use_fast=False,add_bos_token=False, model_max_length=4096,padding_side=\"right\"\
    ,trust_remote_code=True)\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\
    \n        model_args.model_name_or_path, \r\n        torch_dtype=torch.float16,\r\
    \n        trust_remote_code=True,)\r\n\r\nBut the error named setStorage appeared:\r\
    \n\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502 /home/ec2-user/FastChat/fastchat/train/train_mem.py:13\
    \ in <module>                         \u2502\r\n\u2502                       \
    \                                                                           \u2502\
    \r\n\u2502   10 from fastchat.train.train import train                       \
    \                               \u2502\r\n\u2502   11                        \
    \                                                                     \u2502\r\
    \n\u2502   12 if __name__ == \"__main__\":                                   \
    \                               \u2502\r\n\u2502 \u2771 13 \u2502   train()  \
    \                                                                            \
    \   \u2502\r\n\u2502   14                                                    \
    \                                         \u2502\r\n\u2502                   \
    \                                                                            \
    \   \u2502\r\n\u2502 /home/ec2-user/FastChat/fastchat/train/train.py:282 in train\
    \                               \u2502\r\n\u2502                             \
    \                                                                     \u2502\r\
    \n\u2502   279 \u2502   if list(pathlib.Path(training_args.output_dir).glob(\"\
    checkpoint-*\")):                  \u2502\r\n\u2502   280 \u2502   \u2502   trainer.train(resume_from_checkpoint=True)\
    \                                         \u2502\r\n\u2502   281 \u2502   else:\
    \                                                                            \
    \      \u2502\r\n\u2502 \u2771 282 \u2502   \u2502   trainer.train()         \
    \                                                           \u2502\r\n\u2502 \
    \  283 \u2502   trainer.save_state()                                         \
    \                          \u2502\r\n\u2502   284 \u2502   safe_save_model_for_hf_trainer(trainer=trainer,\
    \ output_dir=training_args.output_dir)   \u2502\r\n\u2502   285              \
    \                                                                            \
    \  \u2502\r\n\u2502                                                          \
    \                                        \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/liwei2/lib/python3.10/site-packages/transformers/trainer.py:1664\
    \   \u2502\r\n\u2502 in train                                                \
    \                                         \u2502\r\n\u2502                   \
    \                                                                            \
    \   \u2502\r\n\u2502   1661 \u2502   \u2502   inner_training_loop = find_executable_batch_size(\
    \                                 \u2502\r\n\u2502   1662 \u2502   \u2502   \u2502\
    \   self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size\
    \  \u2502\r\n\u2502   1663 \u2502   \u2502   )                               \
    \                                                  \u2502\r\n\u2502 \u2771 1664\
    \ \u2502   \u2502   return inner_training_loop(                              \
    \                         \u2502\r\n\u2502   1665 \u2502   \u2502   \u2502   args=args,\
    \                                                                    \u2502\r\n\
    \u2502   1666 \u2502   \u2502   \u2502   resume_from_checkpoint=resume_from_checkpoint,\
    \                                \u2502\r\n\u2502   1667 \u2502   \u2502   \u2502\
    \   trial=trial,                                                             \
    \     \u2502\r\n\u2502                                                       \
    \                                           \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/transformers/trainer.py:1938\
    \   \u2502\r\n\u2502 in _inner_training_loop                                 \
    \                                         \u2502\r\n\u2502                   \
    \                                                                            \
    \   \u2502\r\n\u2502   1935 \u2502   \u2502   \u2502   \u2502   ):           \
    \                                                             \u2502\r\n\u2502\
    \   1936 \u2502   \u2502   \u2502   \u2502   \u2502   # Avoid unnecessary DDP\
    \ synchronization since there will be no backw  \u2502\r\n\u2502   1937 \u2502\
    \   \u2502   \u2502   \u2502   \u2502   with model.no_sync():                \
    \                                 \u2502\r\n\u2502 \u2771 1938 \u2502   \u2502\
    \   \u2502   \u2502   \u2502   \u2502   tr_loss_step = self.training_step(model,\
    \ inputs)                  \u2502\r\n\u2502   1939 \u2502   \u2502   \u2502  \
    \ \u2502   else:                                                             \
    \        \u2502\r\n\u2502   1940 \u2502   \u2502   \u2502   \u2502   \u2502  \
    \ tr_loss_step = self.training_step(model, inputs)                      \u2502\
    \r\n\u2502   1941                                                            \
    \                               \u2502\r\n\u2502                             \
    \                                                                     \u2502\r\
    \n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/transformers/trainer.py:2753\
    \   \u2502\r\n\u2502 in training_step                                        \
    \                                         \u2502\r\n\u2502                   \
    \                                                                            \
    \   \u2502\r\n\u2502   2750 \u2502   \u2502   \u2502   # loss gets scaled under\
    \ gradient_accumulation_steps in deepspeed             \u2502\r\n\u2502   2751\
    \ \u2502   \u2502   \u2502   loss = self.deepspeed.backward(loss)            \
    \                              \u2502\r\n\u2502   2752 \u2502   \u2502   else:\
    \                                                                            \
    \ \u2502\r\n\u2502 \u2771 2753 \u2502   \u2502   \u2502   loss.backward()    \
    \                                                           \u2502\r\n\u2502 \
    \  2754 \u2502   \u2502                                                      \
    \                               \u2502\r\n\u2502   2755 \u2502   \u2502   return\
    \ loss.detach()                                                              \u2502\
    \r\n\u2502   2756                                                            \
    \                               \u2502\r\n\u2502                             \
    \                                                                     \u2502\r\
    \n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/_tensor.py:487\
    \ in        \u2502\r\n\u2502 backward                                        \
    \                                                 \u2502\r\n\u2502           \
    \                                                                            \
    \           \u2502\r\n\u2502    484 \u2502   \u2502   \u2502   \u2502   create_graph=create_graph,\
    \                                                \u2502\r\n\u2502    485 \u2502\
    \   \u2502   \u2502   \u2502   inputs=inputs,                                \
    \                            \u2502\r\n\u2502    486 \u2502   \u2502   \u2502\
    \   )                                                                        \
    \     \u2502\r\n\u2502 \u2771  487 \u2502   \u2502   torch.autograd.backward(\
    \                                                          \u2502\r\n\u2502  \
    \  488 \u2502   \u2502   \u2502   self, gradient, retain_graph, create_graph,\
    \ inputs=inputs                     \u2502\r\n\u2502    489 \u2502   \u2502  \
    \ )                                                                          \
    \       \u2502\r\n\u2502    490                                              \
    \                                             \u2502\r\n\u2502               \
    \                                                                            \
    \       \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/autograd/__init__.py:200\
    \ \u2502\r\n\u2502 in backward                                               \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   197 \u2502   # The reason we repeat same the comment below\
    \ is that                                  \u2502\r\n\u2502   198 \u2502   # some\
    \ Python versions print out the first line of a multi-line function          \
    \     \u2502\r\n\u2502   199 \u2502   # calls in the traceback and some print\
    \ out the last line                              \u2502\r\n\u2502 \u2771 200 \u2502\
    \   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run\
    \ the bac   \u2502\r\n\u2502   201 \u2502   \u2502   tensors, grad_tensors_, retain_graph,\
    \ create_graph, inputs,                        \u2502\r\n\u2502   202 \u2502 \
    \  \u2502   allow_unreachable=True, accumulate_grad=True)  # Calls into the C++\
    \ engine to ru   \u2502\r\n\u2502   203                                      \
    \                                                      \u2502\r\n\u2502      \
    \                                                                            \
    \                \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/autograd/function.py:274\
    \ \u2502\r\n\u2502 in apply                                                  \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   271 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502 \
    \  \u2502      \"Function is not allowed. You should only implement one \"   \u2502\
    \r\n\u2502   272 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502\
    \      \"of them.\")                                                 \u2502\r\n\
    \u2502   273 \u2502   \u2502   user_fn = vjp_fn if vjp_fn is not Function.vjp\
    \ else backward_fn                    \u2502\r\n\u2502 \u2771 274 \u2502   \u2502\
    \   return user_fn(self, *args)                                              \
    \          \u2502\r\n\u2502   275 \u2502                                     \
    \                                                     \u2502\r\n\u2502   276 \u2502\
    \   def apply_jvp(self, *args):                                              \
    \              \u2502\r\n\u2502   277 \u2502   \u2502   # _forward_cls is defined\
    \ by derived class                                         \u2502\r\n\u2502  \
    \                                                                            \
    \                    \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/utils/checkpoint.py:141\
    \  \u2502\r\n\u2502 in backward                                              \
    \                                        \u2502\r\n\u2502                    \
    \                                                                            \
    \  \u2502\r\n\u2502   138 \u2502   \u2502   \u2502   with torch.enable_grad(),\
    \ \\                                                    \u2502\r\n\u2502   139\
    \ \u2502   \u2502   \u2502   \u2502    torch.cuda.amp.autocast(**ctx.gpu_autocast_kwargs),\
    \ \\                     \u2502\r\n\u2502   140 \u2502   \u2502   \u2502   \u2502\
    \    torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):                      \
    \  \u2502\r\n\u2502 \u2771 141 \u2502   \u2502   \u2502   \u2502   outputs = ctx.run_function(*detached_inputs)\
    \                               \u2502\r\n\u2502   142 \u2502   \u2502       \
    \                                                                            \
    \   \u2502\r\n\u2502   143 \u2502   \u2502   if isinstance(outputs, torch.Tensor):\
    \                                              \u2502\r\n\u2502   144 \u2502 \
    \  \u2502   \u2502   outputs = (outputs,)                                    \
    \                       \u2502\r\n\u2502                                     \
    \                                                             \u2502\r\n\u2502\
    \ /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/transformers/models/llama/mode\
    \ \u2502\r\n\u2502 ling_llama.py:566 in custom_forward                       \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   563 \u2502   \u2502   \u2502   \u2502   def create_custom_forward(module):\
    \                                         \u2502\r\n\u2502   564 \u2502   \u2502\
    \   \u2502   \u2502   \u2502   def custom_forward(*inputs):                  \
    \                         \u2502\r\n\u2502   565 \u2502   \u2502   \u2502   \u2502\
    \   \u2502   \u2502   # None for past_key_value                              \
    \            \u2502\r\n\u2502 \u2771 566 \u2502   \u2502   \u2502   \u2502   \u2502\
    \   \u2502   return module(*inputs, output_attentions, None)                 \
    \   \u2502\r\n\u2502   567 \u2502   \u2502   \u2502   \u2502   \u2502        \
    \                                                                  \u2502\r\n\u2502\
    \   568 \u2502   \u2502   \u2502   \u2502   \u2502   return custom_forward   \
    \                                               \u2502\r\n\u2502   569       \
    \                                                                            \
    \         \u2502\r\n\u2502                                                   \
    \                                               \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/nn/modules/module.py:150\
    \ \u2502\r\n\u2502 1 in _call_impl                                           \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502  \
    \ \u2502   or _global_backward_pre_hooks or _global_backward_hooks           \
    \        \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501\
    \ \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)            \
    \                              \u2502\r\n\u2502   1502 \u2502   \u2502   # Do\
    \ not call functions when jit is used                                        \
    \  \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
    \ = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502\
    \   backward_pre_hooks = []                                                  \
    \         \u2502\r\n\u2502                                                   \
    \                                               \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/transformers/models/llama/mode\
    \ \u2502\r\n\u2502 ling_llama.py:293 in forward                              \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   290 \u2502   \u2502   hidden_states = self.input_layernorm(hidden_states)\
    \                                \u2502\r\n\u2502   291 \u2502   \u2502      \
    \                                                                            \
    \    \u2502\r\n\u2502   292 \u2502   \u2502   # Self Attention               \
    \                                                    \u2502\r\n\u2502 \u2771 293\
    \ \u2502   \u2502   hidden_states, self_attn_weights, present_key_value = self.self_attn(\
    \              \u2502\r\n\u2502   294 \u2502   \u2502   \u2502   hidden_states=hidden_states,\
    \                                                   \u2502\r\n\u2502   295 \u2502\
    \   \u2502   \u2502   attention_mask=attention_mask,                         \
    \                        \u2502\r\n\u2502   296 \u2502   \u2502   \u2502   position_ids=position_ids,\
    \                                                     \u2502\r\n\u2502       \
    \                                                                            \
    \               \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/nn/modules/module.py:150\
    \ \u2502\r\n\u2502 1 in _call_impl                                           \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502  \
    \ \u2502   or _global_backward_pre_hooks or _global_backward_hooks           \
    \        \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501\
    \ \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)            \
    \                              \u2502\r\n\u2502   1502 \u2502   \u2502   # Do\
    \ not call functions when jit is used                                        \
    \  \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
    \ = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502\
    \   backward_pre_hooks = []                                                  \
    \         \u2502\r\n\u2502                                                   \
    \                                               \u2502\r\n\u2502 /home/ec2-user/FastChat/fastchat/train/llama_flash_attn_monkey_patch.py:32\
    \ in forward      \u2502\r\n\u2502                                           \
    \                                                       \u2502\r\n\u2502    29\
    \ \u2502   bsz, q_len, _ = hidden_states.size()                              \
    \                     \u2502\r\n\u2502    30 \u2502                          \
    \                                                                \u2502\r\n\u2502\
    \    31 \u2502   query_states = (                                            \
    \                           \u2502\r\n\u2502 \u2771  32 \u2502   \u2502   self.q_proj(hidden_states)\
    \                                                         \u2502\r\n\u2502   \
    \ 33 \u2502   \u2502   .view(bsz, q_len, self.num_heads, self.head_dim)      \
    \                             \u2502\r\n\u2502    34 \u2502   \u2502   .transpose(1,\
    \ 2)                                                                   \u2502\r\
    \n\u2502    35 \u2502   )                                                    \
    \                                  \u2502\r\n\u2502                          \
    \                                                                        \u2502\
    \r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/nn/modules/module.py:150\
    \ \u2502\r\n\u2502 1 in _call_impl                                           \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502  \
    \ \u2502   or _global_backward_pre_hooks or _global_backward_hooks           \
    \        \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501\
    \ \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)            \
    \                              \u2502\r\n\u2502   1502 \u2502   \u2502   # Do\
    \ not call functions when jit is used                                        \
    \  \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
    \ = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502\
    \   backward_pre_hooks = []                                                  \
    \         \u2502\r\n\u2502                                                   \
    \                                               \u2502\r\n\u2502 /home/ec2-user/anaconda3/envs/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\
    \ \u2502\r\n\u2502 in forward                                                \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   111 \u2502   \u2502   \u2502   init.uniform_(self.bias, -bound,\
    \ bound)                                        \u2502\r\n\u2502   112 \u2502\
    \                                                                            \
    \              \u2502\r\n\u2502   113 \u2502   def forward(self, input: Tensor)\
    \ -> Tensor:                                            \u2502\r\n\u2502 \u2771\
    \ 114 \u2502   \u2502   return F.linear(input, self.weight, self.bias)       \
    \                              \u2502\r\n\u2502   115 \u2502                 \
    \                                                                         \u2502\
    \r\n\u2502   116 \u2502   def extra_repr(self) -> str:                       \
    \                                    \u2502\r\n\u2502   117 \u2502   \u2502  \
    \ return 'in_features={}, out_features={}, bias={}'.format(                  \
    \        \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nRuntimeError:\
    \ setStorage: sizes [4096, 4096], strides [1, 4096], storage offset 6333644800,\
    \ and itemsize 2 requiring\r\na storage size of 12700844032 are out of bounds\
    \ for storage of size 0\r\n\r\nAnd the error in Baichuan-7B is\r\n \r\nRuntimeError:\
    \ setStorage: sizes [4096, 12288], strides [1, 4096], storage offset 6333644800,\
    \ and itemsize 2 requiring\r\na storage size of 12700844032 are out of bounds\
    \ for storage of size 0\r\n\r\nHave you ever meet this problem?"
  created_at: 2023-06-19 00:41:59+00:00
  edited: false
  hidden: false
  id: 648fb267c124ea06b98cc8dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648b7223355113a5fba3cba7/jDetKunlde-KPUv_YOToD.png?w=200&h=200&f=face
      fullname: fireballoon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: fireballoon
      type: user
    createdAt: '2023-06-19T04:04:43.000Z'
    data:
      edited: false
      editors:
      - fireballoon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8457668423652649
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648b7223355113a5fba3cba7/jDetKunlde-KPUv_YOToD.png?w=200&h=200&f=face
          fullname: fireballoon
          isHf: false
          isPro: false
          name: fireballoon
          type: user
        html: '<p>I did not encounter this problem. This problem appears to be from
          the flash attention implementation when calculating q_proj. This is probably
          because the original baichuan-inc/baichuan-7B implementation merged the
          qkv matrix. To avoid this you may try fireballoon/baichuan-llama-7b, a converted
          baichuan-inc/baichuan-7B in llama format, which is also the model I use
          for training.</p>

          '
        raw: I did not encounter this problem. This problem appears to be from the
          flash attention implementation when calculating q_proj. This is probably
          because the original baichuan-inc/baichuan-7B implementation merged the
          qkv matrix. To avoid this you may try fireballoon/baichuan-llama-7b, a converted
          baichuan-inc/baichuan-7B in llama format, which is also the model I use
          for training.
        updatedAt: '2023-06-19T04:04:43.983Z'
      numEdits: 0
      reactions: []
    id: 648fd3dbe9c88d273ca03ae5
    type: comment
  author: fireballoon
  content: I did not encounter this problem. This problem appears to be from the flash
    attention implementation when calculating q_proj. This is probably because the
    original baichuan-inc/baichuan-7B implementation merged the qkv matrix. To avoid
    this you may try fireballoon/baichuan-llama-7b, a converted baichuan-inc/baichuan-7B
    in llama format, which is also the model I use for training.
  created_at: 2023-06-19 03:04:43+00:00
  edited: false
  hidden: false
  id: 648fd3dbe9c88d273ca03ae5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: fireballoon/baichuan-vicuna-7b
repo_type: model
status: open
target_branch: null
title: A question of setstorage in Baichuan-7B and Baichuan-vicuna-7B
