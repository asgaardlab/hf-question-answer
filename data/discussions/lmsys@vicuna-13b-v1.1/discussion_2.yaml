!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Thireus
conflicting_files: null
created_at: 2023-04-15 11:32:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-04-15T12:32:52.000Z'
    data:
      edited: true
      editors:
      - Thireus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: '<p>I''m a bit puzzled about the size difference between eachadea/vicuna-13b-1.1
          and TheBloke/vicuna-13B-1.1-HF.</p>

          <p>Looking at the description of both models it would appear they are both
          transformation of lmsys/vicuna-13b-delta-v1.1 with delta files applied to
          Llama 13B. Is there anything else I''m missing? Why are the model files
          much larger in size on TheBloke/vicuna-13B-1.1-HF?</p>

          <p>I''ve also observed the tokenizer_config.json differ with the "content"
          lines not mentioning &lt;/s&gt;.</p>

          '
        raw: 'I''m a bit puzzled about the size difference between eachadea/vicuna-13b-1.1
          and TheBloke/vicuna-13B-1.1-HF.


          Looking at the description of both models it would appear they are both
          transformation of lmsys/vicuna-13b-delta-v1.1 with delta files applied to
          Llama 13B. Is there anything else I''m missing? Why are the model files
          much larger in size on TheBloke/vicuna-13B-1.1-HF?


          I''ve also observed the tokenizer_config.json differ with the "content"
          lines not mentioning <\/s>.'
        updatedAt: '2023-04-15T12:34:42.624Z'
      numEdits: 3
      reactions: []
    id: 643a9974065961b22524e3d2
    type: comment
  author: Thireus
  content: 'I''m a bit puzzled about the size difference between eachadea/vicuna-13b-1.1
    and TheBloke/vicuna-13B-1.1-HF.


    Looking at the description of both models it would appear they are both transformation
    of lmsys/vicuna-13b-delta-v1.1 with delta files applied to Llama 13B. Is there
    anything else I''m missing? Why are the model files much larger in size on TheBloke/vicuna-13B-1.1-HF?


    I''ve also observed the tokenizer_config.json differ with the "content" lines
    not mentioning <\/s>.'
  created_at: 2023-04-15 11:32:52+00:00
  edited: true
  hidden: false
  id: 643a9974065961b22524e3d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-15T14:19:36.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>EDIT: See explanation below</p>

          '
        raw: 'EDIT: See explanation below'
        updatedAt: '2023-04-15T15:12:19.547Z'
      numEdits: 2
      reactions: []
    id: 643ab278f6ef50c310d6e1ee
    type: comment
  author: TheBloke
  content: 'EDIT: See explanation below'
  created_at: 2023-04-15 13:19:36+00:00
  edited: true
  hidden: false
  id: 643ab278f6ef50c310d6e1ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-04-15T14:54:21.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span>, thank you for\
          \ looking into this. Could it be the llama-13b-hf that differs?</p>\n<p>When\
          \ I transform Llama-13b to Llama-13-hf it produces pytorch_model-00001-of-00003.bin\
          \ and pytorch_model-00002-of-00003.bin of 9.9GB each and pytorch_model-00003-of-00003.bin\
          \ of 6.5GB.</p>\n"
        raw: '@TheBloke, thank you for looking into this. Could it be the llama-13b-hf
          that differs?


          When I transform Llama-13b to Llama-13-hf it produces pytorch_model-00001-of-00003.bin
          and pytorch_model-00002-of-00003.bin of 9.9GB each and pytorch_model-00003-of-00003.bin
          of 6.5GB.'
        updatedAt: '2023-04-15T14:54:21.884Z'
      numEdits: 0
      reactions: []
    id: 643aba9df6ef50c310d71845
    type: comment
  author: Thireus
  content: '@TheBloke, thank you for looking into this. Could it be the llama-13b-hf
    that differs?


    When I transform Llama-13b to Llama-13-hf it produces pytorch_model-00001-of-00003.bin
    and pytorch_model-00002-of-00003.bin of 9.9GB each and pytorch_model-00003-of-00003.bin
    of 6.5GB.'
  created_at: 2023-04-15 13:54:21+00:00
  edited: false
  hidden: false
  id: 643aba9df6ef50c310d71845
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-15T14:58:59.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>EDIT: see explanation below</p>

          '
        raw: 'EDIT: see explanation below'
        updatedAt: '2023-04-15T15:12:32.731Z'
      numEdits: 1
      reactions: []
    id: 643abbb39f4e4abaf851dfcb
    type: comment
  author: TheBloke
  content: 'EDIT: see explanation below'
  created_at: 2023-04-15 13:58:59+00:00
  edited: true
  hidden: false
  id: 643abbb39f4e4abaf851dfcb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-15T15:05:23.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Yeah OK I see the same as you. I just did a fresh Llama 13B to HF\
          \ conversion and yeah it's 9.3 + 9.2 + 6.1G:</p>\n<pre><code>tomj@Eddie\
          \ ~/src/gpt4all-chat/build (master\u25CF\u25CF)$ ll /Volumes/EVO-2TB-A/Users/tomj/Llama-13B-HF\n\
          total 51528392\ndrwxr-xr-x   11 tomj  staff   352B 15 Apr 16:03 .\ndrwxr-xr-x+\
          \ 169 tomj  staff   5.3K 15 Apr 16:00 ..\n-rw-r--r--    1 tomj  staff  \
          \ 507B 15 Apr 16:02 config.json\n-rw-r--r--    1 tomj  staff   137B 15 Apr\
          \ 16:02 generation_config.json\n-rw-r--r--    1 tomj  staff   9.3G 15 Apr\
          \ 16:02 pytorch_model-00001-of-00003.bin\n-rw-r--r--    1 tomj  staff  \
          \ 9.2G 15 Apr 16:03 pytorch_model-00002-of-00003.bin\n-rw-r--r--    1 tomj\
          \  staff   6.1G 15 Apr 16:03 pytorch_model-00003-of-00003.bin\n-rw-r--r--\
          \    1 tomj  staff    33K 15 Apr 16:03 pytorch_model.bin.index.json\n-rw-r--r--\
          \    1 tomj  staff     2B 15 Apr 16:03 special_tokens_map.json\n-rw-r--r--\
          \    1 tomj  staff   488K 15 Apr 16:03 tokenizer.model\n-rw-r--r--    1\
          \ tomj  staff   141B 15 Apr 16:03 tokenizer_config.json\n</code></pre>\n\
          <p>I would have to guess that there's been some change in the HF format,\
          \ but I don't know what exactly.</p>\n"
        raw: "Yeah OK I see the same as you. I just did a fresh Llama 13B to HF conversion\
          \ and yeah it's 9.3 + 9.2 + 6.1G:\n```\ntomj@Eddie ~/src/gpt4all-chat/build\
          \ (master\u25CF\u25CF)$ ll /Volumes/EVO-2TB-A/Users/tomj/Llama-13B-HF\n\
          total 51528392\ndrwxr-xr-x   11 tomj  staff   352B 15 Apr 16:03 .\ndrwxr-xr-x+\
          \ 169 tomj  staff   5.3K 15 Apr 16:00 ..\n-rw-r--r--    1 tomj  staff  \
          \ 507B 15 Apr 16:02 config.json\n-rw-r--r--    1 tomj  staff   137B 15 Apr\
          \ 16:02 generation_config.json\n-rw-r--r--    1 tomj  staff   9.3G 15 Apr\
          \ 16:02 pytorch_model-00001-of-00003.bin\n-rw-r--r--    1 tomj  staff  \
          \ 9.2G 15 Apr 16:03 pytorch_model-00002-of-00003.bin\n-rw-r--r--    1 tomj\
          \  staff   6.1G 15 Apr 16:03 pytorch_model-00003-of-00003.bin\n-rw-r--r--\
          \    1 tomj  staff    33K 15 Apr 16:03 pytorch_model.bin.index.json\n-rw-r--r--\
          \    1 tomj  staff     2B 15 Apr 16:03 special_tokens_map.json\n-rw-r--r--\
          \    1 tomj  staff   488K 15 Apr 16:03 tokenizer.model\n-rw-r--r--    1\
          \ tomj  staff   141B 15 Apr 16:03 tokenizer_config.json\n```\n\nI would\
          \ have to guess that there's been some change in the HF format, but I don't\
          \ know what exactly."
        updatedAt: '2023-04-15T15:05:23.913Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Thireus
    id: 643abd33e756b67eee1d7411
    type: comment
  author: TheBloke
  content: "Yeah OK I see the same as you. I just did a fresh Llama 13B to HF conversion\
    \ and yeah it's 9.3 + 9.2 + 6.1G:\n```\ntomj@Eddie ~/src/gpt4all-chat/build (master\u25CF\
    \u25CF)$ ll /Volumes/EVO-2TB-A/Users/tomj/Llama-13B-HF\ntotal 51528392\ndrwxr-xr-x\
    \   11 tomj  staff   352B 15 Apr 16:03 .\ndrwxr-xr-x+ 169 tomj  staff   5.3K 15\
    \ Apr 16:00 ..\n-rw-r--r--    1 tomj  staff   507B 15 Apr 16:02 config.json\n\
    -rw-r--r--    1 tomj  staff   137B 15 Apr 16:02 generation_config.json\n-rw-r--r--\
    \    1 tomj  staff   9.3G 15 Apr 16:02 pytorch_model-00001-of-00003.bin\n-rw-r--r--\
    \    1 tomj  staff   9.2G 15 Apr 16:03 pytorch_model-00002-of-00003.bin\n-rw-r--r--\
    \    1 tomj  staff   6.1G 15 Apr 16:03 pytorch_model-00003-of-00003.bin\n-rw-r--r--\
    \    1 tomj  staff    33K 15 Apr 16:03 pytorch_model.bin.index.json\n-rw-r--r--\
    \    1 tomj  staff     2B 15 Apr 16:03 special_tokens_map.json\n-rw-r--r--   \
    \ 1 tomj  staff   488K 15 Apr 16:03 tokenizer.model\n-rw-r--r--    1 tomj  staff\
    \   141B 15 Apr 16:03 tokenizer_config.json\n```\n\nI would have to guess that\
    \ there's been some change in the HF format, but I don't know what exactly."
  created_at: 2023-04-15 14:05:23+00:00
  edited: false
  hidden: false
  id: 643abd33e756b67eee1d7411
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-15T15:10:42.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ahhh yeah I''ve got it! See this commit to the conversion script:
          <a rel="nofollow" href="https://github.com/huggingface/transformers/commit/786092a35e18154cacad62c30fe92bac2c27a1e1">https://github.com/huggingface/transformers/commit/786092a35e18154cacad62c30fe92bac2c27a1e1</a></p>

          <p>They resolved an issue unique to Llama 13B which causes "the checkpoint
          becoming 37GB instead of 26GB for some reason."</p>

          <p>So yeah my guess is that there is redundant or useless data in the older
          Llama 13B HF releases, and that''s been replicated in my release here as
          well.</p>

          <p>I''m going to re-do my conversion to fix it. Though you could equally
          just use eachadea''s instead as that''s already correct.</p>

          '
        raw: 'Ahhh yeah I''ve got it! See this commit to the conversion script: https://github.com/huggingface/transformers/commit/786092a35e18154cacad62c30fe92bac2c27a1e1


          They resolved an issue unique to Llama 13B which causes "the checkpoint
          becoming 37GB instead of 26GB for some reason."


          So yeah my guess is that there is redundant or useless data in the older
          Llama 13B HF releases, and that''s been replicated in my release here as
          well.


          I''m going to re-do my conversion to fix it. Though you could equally just
          use eachadea''s instead as that''s already correct.'
        updatedAt: '2023-04-15T15:10:42.358Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Thireus
    id: 643abe7221aad82a5315f83c
    type: comment
  author: TheBloke
  content: 'Ahhh yeah I''ve got it! See this commit to the conversion script: https://github.com/huggingface/transformers/commit/786092a35e18154cacad62c30fe92bac2c27a1e1


    They resolved an issue unique to Llama 13B which causes "the checkpoint becoming
    37GB instead of 26GB for some reason."


    So yeah my guess is that there is redundant or useless data in the older Llama
    13B HF releases, and that''s been replicated in my release here as well.


    I''m going to re-do my conversion to fix it. Though you could equally just use
    eachadea''s instead as that''s already correct.'
  created_at: 2023-04-15 14:10:42+00:00
  edited: false
  hidden: false
  id: 643abe7221aad82a5315f83c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-04-15T15:31:47.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: '<p>Excellent, I''m glad you figured out the issue. Does it mean TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g
          is also affected?</p>

          '
        raw: Excellent, I'm glad you figured out the issue. Does it mean TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g
          is also affected?
        updatedAt: '2023-04-15T15:31:47.218Z'
      numEdits: 0
      reactions: []
    id: 643ac363e756b67eee1d9de8
    type: comment
  author: Thireus
  content: Excellent, I'm glad you figured out the issue. Does it mean TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g
    is also affected?
  created_at: 2023-04-15 14:31:47+00:00
  edited: false
  hidden: false
  id: 643ac363e756b67eee1d9de8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-15T19:53:50.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I have replaced all the files in this repo, so they''re now the
          correct size.</p>

          <p>As regards the GPTQ, I don''t <em>think</em> there would be any issue.
          As I understand the issue, the reason the HF repo was too large was due
          to certain layers being duplicated. So GPTQ would read the first layer and
          I assume ignore the second. Certainly the GPTQ file size was correct for
          the size of model.</p>

          <p>But just to be safe, I re-ran the GPTQ on my new 1.1 HF repo to produce
          a new <code>safetensors</code> file and uploaded it to  TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g
          .  The file size was exactly the same as the one I generated before, but
          the SHA256SUM was different. My guess is that this is because I used a slightly
          new version of the GPTQ-for-LLaMa code, which is changing every day with
          various improvements and bug fixes all the time. So it may have done some
          calculations differently. (To be exact, when re-creating the file today
          I used the GPTQ-for-LLaMa code as of commit <a rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa/commit/58c8ab4c7aaccc50f507fd08cce941976affe5e0">https://github.com/qwopqwop200/GPTQ-for-LLaMa/commit/58c8ab4c7aaccc50f507fd08cce941976affe5e0</a>
          - the last commit of yesterday.  I didn''t use the code from today as they''ve
          just done a big refactor so I wanted to avoid any potential new issues in
          that. Especially as it seems today''s changes have meant that it''s no longer
          possible to link the latest GPTQ-for-LLaMa in to ooba''s UI, due to them
          changing some of the function signatures.)</p>

          <p>Anyway, whether or not there was an issue with the previous GPTQ I have
          done a new safetensors file based on the correctly sized HF repo, so if
          you have any concerns you can re-fetch the safetensors and use that instead
          of the old one.</p>

          <p>I''ve not yet done the no-act-order.pt but I will do that in a minute.</p>

          <p>Thanks for bringing this to my attention!</p>

          '
        raw: 'I have replaced all the files in this repo, so they''re now the correct
          size.


          As regards the GPTQ, I don''t *think* there would be any issue. As I understand
          the issue, the reason the HF repo was too large was due to certain layers
          being duplicated. So GPTQ would read the first layer and I assume ignore
          the second. Certainly the GPTQ file size was correct for the size of model.


          But just to be safe, I re-ran the GPTQ on my new 1.1 HF repo to produce
          a new `safetensors` file and uploaded it to  TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g
          .  The file size was exactly the same as the one I generated before, but
          the SHA256SUM was different. My guess is that this is because I used a slightly
          new version of the GPTQ-for-LLaMa code, which is changing every day with
          various improvements and bug fixes all the time. So it may have done some
          calculations differently. (To be exact, when re-creating the file today
          I used the GPTQ-for-LLaMa code as of commit https://github.com/qwopqwop200/GPTQ-for-LLaMa/commit/58c8ab4c7aaccc50f507fd08cce941976affe5e0
          - the last commit of yesterday.  I didn''t use the code from today as they''ve
          just done a big refactor so I wanted to avoid any potential new issues in
          that. Especially as it seems today''s changes have meant that it''s no longer
          possible to link the latest GPTQ-for-LLaMa in to ooba''s UI, due to them
          changing some of the function signatures.)


          Anyway, whether or not there was an issue with the previous GPTQ I have
          done a new safetensors file based on the correctly sized HF repo, so if
          you have any concerns you can re-fetch the safetensors and use that instead
          of the old one.


          I''ve not yet done the no-act-order.pt but I will do that in a minute.


          Thanks for bringing this to my attention!'
        updatedAt: '2023-04-15T19:58:48.257Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Thireus
      relatedEventId: 643b00ce45200ac3e706a620
    id: 643b00ce45200ac3e706a61f
    type: comment
  author: TheBloke
  content: 'I have replaced all the files in this repo, so they''re now the correct
    size.


    As regards the GPTQ, I don''t *think* there would be any issue. As I understand
    the issue, the reason the HF repo was too large was due to certain layers being
    duplicated. So GPTQ would read the first layer and I assume ignore the second.
    Certainly the GPTQ file size was correct for the size of model.


    But just to be safe, I re-ran the GPTQ on my new 1.1 HF repo to produce a new
    `safetensors` file and uploaded it to  TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g
    .  The file size was exactly the same as the one I generated before, but the SHA256SUM
    was different. My guess is that this is because I used a slightly new version
    of the GPTQ-for-LLaMa code, which is changing every day with various improvements
    and bug fixes all the time. So it may have done some calculations differently.
    (To be exact, when re-creating the file today I used the GPTQ-for-LLaMa code as
    of commit https://github.com/qwopqwop200/GPTQ-for-LLaMa/commit/58c8ab4c7aaccc50f507fd08cce941976affe5e0
    - the last commit of yesterday.  I didn''t use the code from today as they''ve
    just done a big refactor so I wanted to avoid any potential new issues in that.
    Especially as it seems today''s changes have meant that it''s no longer possible
    to link the latest GPTQ-for-LLaMa in to ooba''s UI, due to them changing some
    of the function signatures.)


    Anyway, whether or not there was an issue with the previous GPTQ I have done a
    new safetensors file based on the correctly sized HF repo, so if you have any
    concerns you can re-fetch the safetensors and use that instead of the old one.


    I''ve not yet done the no-act-order.pt but I will do that in a minute.


    Thanks for bringing this to my attention!'
  created_at: 2023-04-15 18:53:50+00:00
  edited: true
  hidden: false
  id: 643b00ce45200ac3e706a61f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-15T19:53:50.000Z'
    data:
      status: closed
    id: 643b00ce45200ac3e706a620
    type: status-change
  author: TheBloke
  created_at: 2023-04-15 18:53:50+00:00
  id: 643b00ce45200ac3e706a620
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: lmsys/vicuna-13b-v1.1
repo_type: model
status: closed
target_branch: null
title: eachadea/vicuna-13b-1.1 vs TheBloke/vicuna-13B-1.1-HF
