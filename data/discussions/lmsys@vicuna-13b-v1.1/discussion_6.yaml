!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MorphzZ
conflicting_files: null
created_at: 2023-06-21 21:52:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
      fullname: MORPHEUS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MorphzZ
      type: user
    createdAt: '2023-06-21T22:52:09.000Z'
    data:
      edited: false
      editors:
      - MorphzZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4768965244293213
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
          fullname: MORPHEUS
          isHf: false
          isPro: false
          name: MorphzZ
          type: user
        html: "<p>Hi Bloke, </p>\n<p>Hope you are well. I am trying to use this model\
          \ and get <a rel=\"nofollow\" href=\"https://github.com/artidoro/qlora/issues/82\"\
          >this</a> error:</p>\n<pre><code>&gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(model_name,\
          \ quantization_config=quant_config, device_map={\"\":0})\nLoading checkpoint\
          \ shards:   0%|                                 | 0/3 [00:06&lt;?, ?it/s]\n\
          Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in\
          \ &lt;module&gt;\n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 484, in from_pretrained\n    return model_class.from_pretrained(\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 2897, in from_pretrained\n    ) = cls._load_pretrained_model(\n \
          \       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3236, in _load_pretrained_model\n    new_error_msgs, offload_index,\
          \ state_dict_index = _load_state_dict_into_meta_model(\n               \
          \                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 718, in _load_state_dict_into_meta_model\n    set_module_quantized_tensor_to_device(\n\
          \  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/utils/bitsandbytes.py\"\
          , line 91, in set_module_quantized_tensor_to_device\n    new_value = bnb.nn.Params4bit(new_value,\
          \ requires_grad=False, **kwargs).to(device)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\"\
          , line 176, in to\n    return self.cuda(device)\n           ^^^^^^^^^^^^^^^^^\n\
          \  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\"\
          , line 153, in cuda\n    w = self.data.contiguous().half().cuda(device)\n\
          \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA\
          \ error: an illegal memory access was encountered\nCUDA kernel errors might\
          \ be asynchronously reported at some other API call, so the stacktrace below\
          \ might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\
          Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n</code></pre>\n\
          <p>I have</p>\n<pre><code>&gt;&gt;&gt; torch.__version__\n'2.0.1+cu117'\n\
          </code></pre>\n<p>could you please help? Is there another model I can use\
          \ which does not give this error? from <a rel=\"nofollow\" href=\"https://github.com/artidoro/qlora#known-issues-and-limitations\"\
          >here</a>:</p>\n<blockquote>\n<p>If you get an this issue (\"illegal memory\
          \ access\") then you should use a newer HF LLaMA conversion or downgrade\
          \ your PyTorch version.</p>\n</blockquote>\n"
        raw: "Hi Bloke, \r\n\r\nHope you are well. I am trying to use this model and\
          \ get [this](https://github.com/artidoro/qlora/issues/82) error:\r\n\r\n\
          ```\r\n>>> model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config,\
          \ device_map={\"\":0})\r\nLoading checkpoint shards:   0%|             \
          \                    | 0/3 [00:06<?, ?it/s]\r\nTraceback (most recent call\
          \ last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 484, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 2897, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
          \n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3236, in _load_pretrained_model\r\n    new_error_msgs, offload_index,\
          \ state_dict_index = _load_state_dict_into_meta_model(\r\n             \
          \                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 718, in _load_state_dict_into_meta_model\r\n    set_module_quantized_tensor_to_device(\r\
          \n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/utils/bitsandbytes.py\"\
          , line 91, in set_module_quantized_tensor_to_device\r\n    new_value = bnb.nn.Params4bit(new_value,\
          \ requires_grad=False, **kwargs).to(device)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\"\
          , line 176, in to\r\n    return self.cuda(device)\r\n           ^^^^^^^^^^^^^^^^^\r\
          \n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\"\
          , line 153, in cuda\r\n    w = self.data.contiguous().half().cuda(device)\r\
          \n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: CUDA\
          \ error: an illegal memory access was encountered\r\nCUDA kernel errors\
          \ might be asynchronously reported at some other API call, so the stacktrace\
          \ below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\
          \nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\
          ```\r\n\r\nI have\r\n\r\n```\r\n>>> torch.__version__\r\n'2.0.1+cu117'\r\
          \n```\r\n\r\ncould you please help? Is there another model I can use which\
          \ does not give this error? from [here](https://github.com/artidoro/qlora#known-issues-and-limitations):\r\
          \n\r\n>If you get an this issue (\"illegal memory access\") then you should\
          \ use a newer HF LLaMA conversion or downgrade your PyTorch version."
        updatedAt: '2023-06-21T22:52:09.464Z'
      numEdits: 0
      reactions: []
    id: 64937f1989fefd775ac37051
    type: comment
  author: MorphzZ
  content: "Hi Bloke, \r\n\r\nHope you are well. I am trying to use this model and\
    \ get [this](https://github.com/artidoro/qlora/issues/82) error:\r\n\r\n```\r\n\
    >>> model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config,\
    \ device_map={\"\":0})\r\nLoading checkpoint shards:   0%|                   \
    \              | 0/3 [00:06<?, ?it/s]\r\nTraceback (most recent call last):\r\n\
    \  File \"<stdin>\", line 1, in <module>\r\n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 484, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 2897, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n   \
    \     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 3236, in _load_pretrained_model\r\n    new_error_msgs, offload_index, state_dict_index\
    \ = _load_state_dict_into_meta_model(\r\n                                    \
    \                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 718, in _load_state_dict_into_meta_model\r\n    set_module_quantized_tensor_to_device(\r\
    \n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/transformers/utils/bitsandbytes.py\"\
    , line 91, in set_module_quantized_tensor_to_device\r\n    new_value = bnb.nn.Params4bit(new_value,\
    \ requires_grad=False, **kwargs).to(device)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\"\
    , line 176, in to\r\n    return self.cuda(device)\r\n           ^^^^^^^^^^^^^^^^^\r\
    \n  File \"/mnt/disks/sdb/finetuning-with-qlora/.env/lib/python3.11/site-packages/bitsandbytes/nn/modules.py\"\
    , line 153, in cuda\r\n    w = self.data.contiguous().half().cuda(device)\r\n\
    \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: CUDA error:\
    \ an illegal memory access was encountered\r\nCUDA kernel errors might be asynchronously\
    \ reported at some other API call, so the stacktrace below might be incorrect.\r\
    \nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA`\
    \ to enable device-side assertions.\r\n```\r\n\r\nI have\r\n\r\n```\r\n>>> torch.__version__\r\
    \n'2.0.1+cu117'\r\n```\r\n\r\ncould you please help? Is there another model I\
    \ can use which does not give this error? from [here](https://github.com/artidoro/qlora#known-issues-and-limitations):\r\
    \n\r\n>If you get an this issue (\"illegal memory access\") then you should use\
    \ a newer HF LLaMA conversion or downgrade your PyTorch version."
  created_at: 2023-06-21 21:52:09+00:00
  edited: false
  hidden: false
  id: 64937f1989fefd775ac37051
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-22T12:33:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9796010851860046
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I guess I need to re-convert the model weights.</p>

          <p>But in the meantime, why not use Vicuna 1.3 instead? It''s an upgrade
          over 1.1 and was made much more recently so hopefully won''t have this problem
          (which I believe is caused by models created with an older version of transformers).</p>

          <p>You can download the Vicuna 1.3 model here: <a href="https://huggingface.co/lmsys/vicuna-13b-v1.3">https://huggingface.co/lmsys/vicuna-13b-v1.3</a></p>

          '
        raw: 'I guess I need to re-convert the model weights.


          But in the meantime, why not use Vicuna 1.3 instead? It''s an upgrade over
          1.1 and was made much more recently so hopefully won''t have this problem
          (which I believe is caused by models created with an older version of transformers).


          You can download the Vicuna 1.3 model here: https://huggingface.co/lmsys/vicuna-13b-v1.3'
        updatedAt: '2023-06-22T12:33:38.832Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MorphzZ
    id: 64943fa2b620a52ed0557cc1
    type: comment
  author: TheBloke
  content: 'I guess I need to re-convert the model weights.


    But in the meantime, why not use Vicuna 1.3 instead? It''s an upgrade over 1.1
    and was made much more recently so hopefully won''t have this problem (which I
    believe is caused by models created with an older version of transformers).


    You can download the Vicuna 1.3 model here: https://huggingface.co/lmsys/vicuna-13b-v1.3'
  created_at: 2023-06-22 11:33:38+00:00
  edited: false
  hidden: false
  id: 64943fa2b620a52ed0557cc1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
      fullname: MORPHEUS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MorphzZ
      type: user
    createdAt: '2023-06-22T15:22:26.000Z'
    data:
      edited: false
      editors:
      - MorphzZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9676952362060547
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
          fullname: MORPHEUS
          isHf: false
          isPro: false
          name: MorphzZ
          type: user
        html: '<p>thanks Bloke. that works.</p>

          '
        raw: thanks Bloke. that works.
        updatedAt: '2023-06-22T15:22:26.559Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6494673277b7c245e11c2e93
    id: 6494673277b7c245e11c2e90
    type: comment
  author: MorphzZ
  content: thanks Bloke. that works.
  created_at: 2023-06-22 14:22:26+00:00
  edited: false
  hidden: false
  id: 6494673277b7c245e11c2e90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
      fullname: MORPHEUS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MorphzZ
      type: user
    createdAt: '2023-06-22T15:22:26.000Z'
    data:
      status: closed
    id: 6494673277b7c245e11c2e93
    type: status-change
  author: MorphzZ
  created_at: 2023-06-22 14:22:26+00:00
  id: 6494673277b7c245e11c2e93
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: lmsys/vicuna-13b-v1.1
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: CUDA error: an illegal memory access was encountered'
