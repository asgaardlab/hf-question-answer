!!python/object:huggingface_hub.community.DiscussionWithDetails
author: warhol-AC
conflicting_files: null
created_at: 2023-05-24 14:15:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665153501664-632b19d086d51a62b2393e7e.jpeg?w=200&h=200&f=face
      fullname: A
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: warhol-AC
      type: user
    createdAt: '2023-05-24T15:15:53.000Z'
    data:
      edited: false
      editors:
      - warhol-AC
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1665153501664-632b19d086d51a62b2393e7e.jpeg?w=200&h=200&f=face
          fullname: A
          isHf: false
          isPro: false
          name: warhol-AC
          type: user
        html: "<p>I am sure I am missing something obvious but I get the following\
          \ error when trying to load in oobabooga:</p>\n<p>Traceback (most recent\
          \ call last):<br>File \u201CD:\\new_ooba\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 102, in load_model_wrapper<br>shared.model, shared.tokenizer\
          \ = load_model(shared.model_name)<br>File \u201CD:\\new_ooba\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 217, in load_model<br>model\
          \ = LoaderClass.from_pretrained(checkpoint, **params)<br>File \u201CD:\\\
          new_ooba\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\u201D, line 471, in from_pretrained<br>return\
          \ model_class.from_pretrained(<br>File \u201CD:\\new_ooba\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 2405, in from_pretrained<br>raise EnvironmentError(<br>OSError: Error\
          \ no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models\\TehVenom_Pygmalion-7b-4bit-GPTQ-Safetensors.</p>\n"
        raw: "I am sure I am missing something obvious but I get the following error\
          \ when trying to load in oobabooga:\r\n \r\nTraceback (most recent call\
          \ last):\r\nFile \u201CD:\\new_ooba\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 102, in load_model_wrapper\r\nshared.model, shared.tokenizer\
          \ = load_model(shared.model_name)\r\nFile \u201CD:\\new_ooba\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 217, in load_model\r\
          \nmodel = LoaderClass.from_pretrained(checkpoint, **params)\r\nFile \u201C\
          D:\\new_ooba\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\u201D, line 471, in from_pretrained\r\
          \nreturn model_class.from_pretrained(\r\nFile \u201CD:\\new_ooba\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 2405, in from_pretrained\r\nraise EnvironmentError(\r\nOSError: Error\
          \ no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models\\TehVenom_Pygmalion-7b-4bit-GPTQ-Safetensors."
        updatedAt: '2023-05-24T15:15:53.253Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Defalt-404
    id: 646e2a29deb963805b336f2b
    type: comment
  author: warhol-AC
  content: "I am sure I am missing something obvious but I get the following error\
    \ when trying to load in oobabooga:\r\n \r\nTraceback (most recent call last):\r\
    \nFile \u201CD:\\new_ooba\\oobabooga_windows\\text-generation-webui\\server.py\u201D\
    , line 102, in load_model_wrapper\r\nshared.model, shared.tokenizer = load_model(shared.model_name)\r\
    \nFile \u201CD:\\new_ooba\\oobabooga_windows\\text-generation-webui\\modules\\\
    models.py\u201D, line 217, in load_model\r\nmodel = LoaderClass.from_pretrained(checkpoint,\
    \ **params)\r\nFile \u201CD:\\new_ooba\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D, line 471,\
    \ in from_pretrained\r\nreturn model_class.from_pretrained(\r\nFile \u201CD:\\\
    new_ooba\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    modeling_utils.py\u201D, line 2405, in from_pretrained\r\nraise EnvironmentError(\r\
    \nOSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index\
    \ or flax_model.msgpack found in directory models\\TehVenom_Pygmalion-7b-4bit-GPTQ-Safetensors."
  created_at: 2023-05-24 14:15:53+00:00
  edited: false
  hidden: false
  id: 646e2a29deb963805b336f2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
      fullname: TeH_Venom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TehVenom
      type: user
    createdAt: '2023-05-24T15:17:13.000Z'
    data:
      edited: true
      editors:
      - TehVenom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
          fullname: TeH_Venom
          isHf: false
          isPro: false
          name: TehVenom
          type: user
        html: '<p>Your ooba is not looking for a 4bit model, make sure to read the
          documentation to pass the proper arguments / know the proper file renaming
          scheme they follow so that it starts looking for the right files.</p>

          '
        raw: Your ooba is not looking for a 4bit model, make sure to read the documentation
          to pass the proper arguments / know the proper file renaming scheme they
          follow so that it starts looking for the right files.
        updatedAt: '2023-05-24T15:17:32.673Z'
      numEdits: 1
      reactions: []
    id: 646e2a794bbb6117ee370339
    type: comment
  author: TehVenom
  content: Your ooba is not looking for a 4bit model, make sure to read the documentation
    to pass the proper arguments / know the proper file renaming scheme they follow
    so that it starts looking for the right files.
  created_at: 2023-05-24 14:17:13+00:00
  edited: true
  hidden: false
  id: 646e2a794bbb6117ee370339
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TehVenom/Pygmalion-7b-4bit-GPTQ-Safetensors
repo_type: model
status: open
target_branch: null
title: Error on model load
