!!python/object:huggingface_hub.community.DiscussionWithDetails
author: arclight1981
conflicting_files: null
created_at: 2023-04-30 22:47:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3146d907e56922b117c567ba6b97e48.svg
      fullname: Sebastian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arclight1981
      type: user
    createdAt: '2023-04-30T23:47:07.000Z'
    data:
      edited: false
      editors:
      - arclight1981
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3146d907e56922b117c567ba6b97e48.svg
          fullname: Sebastian
          isHf: false
          isPro: false
          name: arclight1981
          type: user
        html: '<p>RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:<br>        size
          mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param with
          shape torch.Size([128, 512]) from checkpoint, the shape in current model
          is torch.Size([32, 512]).</p>

          <p>And a whole bunch more. Might be the model or might be my code (although
          loading the quantized 7B with 128 groupsize seems to work fine). But since
          I''m still learning, I can''t say for absolutely sure.</p>

          '
        raw: "RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\r\
          \n        size mismatch for model.layers.0.self_attn.k_proj.qzeros: copying\
          \ a param with shape torch.Size([128, 512]) from checkpoint, the shape in\
          \ current model is torch.Size([32, 512]).\r\n\r\nAnd a whole bunch more.\
          \ Might be the model or might be my code (although loading the quantized\
          \ 7B with 128 groupsize seems to work fine). But since I'm still learning,\
          \ I can't say for absolutely sure."
        updatedAt: '2023-04-30T23:47:07.338Z'
      numEdits: 0
      reactions: []
    id: 644efdfbf2ada99b2eb993fc
    type: comment
  author: arclight1981
  content: "RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\r\n\
    \        size mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param\
    \ with shape torch.Size([128, 512]) from checkpoint, the shape in current model\
    \ is torch.Size([32, 512]).\r\n\r\nAnd a whole bunch more. Might be the model\
    \ or might be my code (although loading the quantized 7B with 128 groupsize seems\
    \ to work fine). But since I'm still learning, I can't say for absolutely sure."
  created_at: 2023-04-30 22:47:07+00:00
  edited: false
  hidden: false
  id: 644efdfbf2ada99b2eb993fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
      fullname: TeH_Venom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TehVenom
      type: user
    createdAt: '2023-04-30T23:55:31.000Z'
    data:
      edited: true
      editors:
      - TehVenom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
          fullname: TeH_Venom
          isHf: false
          isPro: false
          name: TehVenom
          type: user
        html: '<p>Sounds like it''s trying to load an 128 group size model? This one
          is 32g.<br>How are you loading this? An UI or using the GPTQ library by
          itself? Try renaming the model file to "4bit-32g.safetensors" and trying
          again.</p>

          '
        raw: 'Sounds like it''s trying to load an 128 group size model? This one is
          32g.

          How are you loading this? An UI or using the GPTQ library by itself? Try
          renaming the model file to "4bit-32g.safetensors" and trying again.'
        updatedAt: '2023-04-30T23:57:21.299Z'
      numEdits: 1
      reactions: []
    id: 644efff3a00f4b11d39ddaa1
    type: comment
  author: TehVenom
  content: 'Sounds like it''s trying to load an 128 group size model? This one is
    32g.

    How are you loading this? An UI or using the GPTQ library by itself? Try renaming
    the model file to "4bit-32g.safetensors" and trying again.'
  created_at: 2023-04-30 22:55:31+00:00
  edited: true
  hidden: false
  id: 644efff3a00f4b11d39ddaa1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3146d907e56922b117c567ba6b97e48.svg
      fullname: Sebastian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arclight1981
      type: user
    createdAt: '2023-05-01T00:02:09.000Z'
    data:
      edited: false
      editors:
      - arclight1981
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3146d907e56922b117c567ba6b97e48.svg
          fullname: Sebastian
          isHf: false
          isPro: false
          name: arclight1981
          type: user
        html: '<p>And you are absolutely right, of course. I use a modified version
          of the _load_quant function from oobabooga and while I did set the groupsize
          to 32 in my startup parameters, it seems that somewhere along the way they
          don''t get passed. Sorry about that, completely my fault.</p>

          '
        raw: And you are absolutely right, of course. I use a modified version of
          the _load_quant function from oobabooga and while I did set the groupsize
          to 32 in my startup parameters, it seems that somewhere along the way they
          don't get passed. Sorry about that, completely my fault.
        updatedAt: '2023-05-01T00:02:09.075Z'
      numEdits: 0
      reactions: []
    id: 644f0181ddf20748b062a1fe
    type: comment
  author: arclight1981
  content: And you are absolutely right, of course. I use a modified version of the
    _load_quant function from oobabooga and while I did set the groupsize to 32 in
    my startup parameters, it seems that somewhere along the way they don't get passed.
    Sorry about that, completely my fault.
  created_at: 2023-04-30 23:02:09+00:00
  edited: false
  hidden: false
  id: 644f0181ddf20748b062a1fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d3146d907e56922b117c567ba6b97e48.svg
      fullname: Sebastian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arclight1981
      type: user
    createdAt: '2023-05-01T01:18:14.000Z'
    data:
      status: closed
    id: 644f1356bf9683cba46de9df
    type: status-change
  author: arclight1981
  created_at: 2023-05-01 00:18:14+00:00
  id: 644f1356bf9683cba46de9df
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TehVenom/Pygmalion-7b-4bit-GPTQ-Safetensors
repo_type: model
status: closed
target_branch: null
title: 'Unable to load: size mismatches'
