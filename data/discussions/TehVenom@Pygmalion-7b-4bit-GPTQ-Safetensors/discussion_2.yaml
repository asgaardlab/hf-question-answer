!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RoARene317
conflicting_files: null
created_at: 2023-05-01 10:57:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca3e713c445279604d9880a03de33056.svg
      fullname: RoARene317
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RoARene317
      type: user
    createdAt: '2023-05-01T11:57:55.000Z'
    data:
      edited: false
      editors:
      - RoARene317
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca3e713c445279604d9880a03de33056.svg
          fullname: RoARene317
          isHf: false
          isPro: false
          name: RoARene317
          type: user
        html: '<p> size mismatch for model.layers.29.self_attn.k_proj.scales: copying
          a param with shape torch.Size([128, 4096]) from checkpoint, the shape in
          current model is torch.Size([1, 4096]).<br>        size mismatch for model.layers.29.self_attn.o_proj.qzeros:
          copying a param with shape torch.Size([128, 512]) from checkpoint, the shape
          in current model is torch.Size([1, 512]).<br>        size mismatch for model.layers.29.self_attn.o_proj.scales:
          copying a param with shape torch.Size([128, 4096]) from checkpoint, the
          shape in current model is torch.Size([1, 4096]).<br>        size mismatch
          for model.layers.29.self_attn.q_proj.qzeros: copying a param with shape
          torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([1,
          512]).<br>        size mismatch for model.layers.29.self_attn.q_proj.scales:
          copying a param with shape torch.Size([128, 4096]) from checkpoint, the
          shape in current model is torch.Size([1, 4096]).<br>        size mismatch
          for model.layers.29.self_attn.v_proj.qzeros: copying a param with shape
          torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([1,
          512]).<br>        size mismatch for model.layers.29.self_attn.v_proj.scales:
          copying a param with shape torch.Size([128, 4096]) from checkpoint, the
          shape in current model is torch.Size([1, 4096]).<br>        size mismatch
          for model.layers.29.mlp.down_proj.qzeros: copying a param with shape torch.Size([344,
          512]) from checkpoint, the shape in current model is torch.Size([1, 512]).<br>        size
          mismatch for model.layers.29.mlp.down_proj.scales: copying a param with
          shape torch.Size([344, 4096]) from checkpoint, the shape in current model
          is torch.Size([1, 4096]).<br>        size mismatch for model.layers.29.mlp.gate_proj.qzeros:
          copying a param with shape torch.Size([128, 1376]) from checkpoint, the
          shape in current model is torch.Size([1, 1376]).<br>        size mismatch
          for model.layers.29.mlp.gate_proj.scales: copying a param with shape torch.Size([128,
          11008]) from checkpoint, the shape in current model is torch.Size([1, 11008]).<br>        size
          mismatch for model.layers.29.mlp.up_proj.qzeros: copying a param with shape
          torch.Size([128, 1376]) from checkpoint, the shape in current model is torch.Size([1,
          1376]).<br>        size mismatch for model.layers.29.mlp.up_proj.scales:
          copying a param with shape torch.Size([128, 11008]) from checkpoint, the
          shape in current model is torch.Size([1, 11008]).<br>        size mismatch
          for model.layers.30.self_attn.k_proj.qzeros: copying a param with shape
          torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([1,
          512]).<br>        size mismatch for model.layers.30.self_attn.k_proj.scales:
          copying a param with shape torch.Size([128, 4096]) from checkpoint, the
          shape in current model is torch.Size([1, 4096]).<br>        size mismatch
          for model.layers.30.self_attn.o_proj.qzeros: copying a param with shape
          torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([1,
          512]).<br>        size mismatch for model.layers.30.self_attn.o_proj.scales:
          copying a param with shape torch.Size([128, 4096]) from checkpoint, the
          shape in current model is torch.Size([1, 4096]).<br>        size mismatch
          for model.layers.30.self_attn.q_proj.qzeros: copying a param with shape
          torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([1,
          512]).<br>        size mismatch for model.layers.30.self_attn.q_proj.scales:
          copying a param with shape torch.Size([128, 4096]) from checkpoint, the
          shape in current model is torch.Size([1, 4096]).<br>        size mismatch
          for model.layers.30.self_attn.v_proj.qzeros: copying a param with shape
          torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([1,
          512]).<br>        size mismatch for model.layers.30.self_attn.v_proj.scales:
          copying a param with shape torch.Size([128, 4096]) from checkpoint, the
          shape in current model is torch.Size([1, 4096]).<br>        size mismatch
          for model.layers.30.mlp.down_proj.qzeros: copying a param with shape torch.Size([344,
          512]) from checkpoint, the shape in current model is torch.Size([1, 512]).<br>        size
          mismatch for model.layers.30.mlp.down_proj.scales: copying a param with
          shape torch.Size([344, 4096]) from checkpoint, the shape in current model
          is torch.Size([1, 4096]).<br>        size mismatch for model.layers.30.mlp.gate_proj.qzeros:
          copying a param with shape torch.Size([128, 1376]) from checkpoint, the
          shape in current model is torch.Size([1, 1376]).<br>        size mismatch
          for model.layers.30.mlp.gate_proj.scales: copying a param with shape torch.Size([128,
          11008]) from checkpoint, the shape in current model is torch.Size([1, 11008]).<br>        size
          mismatch for model.layers.30.mlp.up_proj.qzeros: copying a param with shape
          torch.Size([128, 1376]) from checkpoint, the shape in current model is torch.Size([1,
          1376]).<br>        size mismatch for model.layers.30.mlp.up_proj.scales:
          copying a param with shape torch.Size([128, 11008]) from checkpoint, the
          shape in current model is torch.Size([1, 11008]).<br>        size mismatch
          for model.layers.31.self_attn.k_proj.qzeros: copying a param with shape
          torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([1,
          512]).<br>        size mismatch for model.layers.31.self_attn.k_proj.scales:
          copying a param with shape torch.Size([128, 4096]) from checkpoint, the
          shape in current model is torch.Size([1, 4096]).<br>        size mismatch
          for model.layers.31.self_attn.o_proj.qzeros: copying a param with shape
          torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([1,
          512]).<br>        size mismatch for model.layers.31.self_attn.o_proj.scales:
          copying a param with shape torch.Size([128, 4096]) from checkpoint, the
          shape in current model is torch.Size([1, 4096]).<br>        size mismatch
          for model.layers.31.self_attn.q_proj.qzeros: copying a param with shape
          torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([1,
          512]).<br>        size mismatch for model.layers.31.self_attn.q_proj.scales:
          copying a param with shape torch.Size([128, 4096]) from checkpoint, the
          shape in current model is torch.Size([1, 4096]).<br>        size mismatch
          for model.layers.31.self_attn.v_proj.qzeros: copying a param with shape
          torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([1,
          512]).<br>        size mismatch for model.layers.31.self_attn.v_proj.scales:
          copying a param with shape torch.Size([128, 4096]) from checkpoint, the
          shape in current model is torch.Size([1, 4096]).<br>        size mismatch
          for model.layers.31.mlp.down_proj.qzeros: copying a param with shape torch.Size([344,
          512]) from checkpoint, the shape in current model is torch.Size([1, 512]).<br>        size
          mismatch for model.layers.31.mlp.down_proj.scales: copying a param with
          shape torch.Size([344, 4096]) from checkpoint, the shape in current model
          is torch.Size([1, 4096]).<br>        size mismatch for model.layers.31.mlp.gate_proj.qzeros:
          copying a param with shape torch.Size([128, 1376]) from checkpoint, the
          shape in current model is torch.Size([1, 1376]).<br>        size mismatch
          for model.layers.31.mlp.gate_proj.scales: copying a param with shape torch.Size([128,
          11008]) from checkpoint, the shape in current model is torch.Size([1, 11008]).<br>        size
          mismatch for model.layers.31.mlp.up_proj.qzeros: copying a param with shape
          torch.Size([128, 1376]) from checkpoint, the shape in current model is torch.Size([1,
          1376]).<br>        size mismatch for model.layers.31.mlp.up_proj.scales:
          copying a param with shape torch.Size([128, 11008]) from checkpoint, the
          shape in current model is torch.Size([1, 11008]).</p>

          <p>I got this error, any help?</p>

          '
        raw: " size mismatch for model.layers.29.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.29.self_attn.o_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.29.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.29.self_attn.q_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.29.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.29.self_attn.v_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.29.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.29.mlp.down_proj.qzeros: copying a param with shape torch.Size([344,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.29.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([344, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.29.mlp.gate_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 1376]) from checkpoint, the shape in current model is torch.Size([1, 1376]).\r\
          \n        size mismatch for model.layers.29.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([128, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 11008]).\r\n        size mismatch for\
          \ model.layers.29.mlp.up_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 1376]) from checkpoint, the shape in current model is torch.Size([1, 1376]).\r\
          \n        size mismatch for model.layers.29.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([128, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 11008]).\r\n        size mismatch for\
          \ model.layers.30.self_attn.k_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.30.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.30.self_attn.o_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.30.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.30.self_attn.q_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.30.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.30.self_attn.v_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.30.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.30.mlp.down_proj.qzeros: copying a param with shape torch.Size([344,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.30.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([344, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.30.mlp.gate_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 1376]) from checkpoint, the shape in current model is torch.Size([1, 1376]).\r\
          \n        size mismatch for model.layers.30.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([128, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 11008]).\r\n        size mismatch for\
          \ model.layers.30.mlp.up_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 1376]) from checkpoint, the shape in current model is torch.Size([1, 1376]).\r\
          \n        size mismatch for model.layers.30.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([128, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 11008]).\r\n        size mismatch for\
          \ model.layers.31.self_attn.k_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.31.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.31.self_attn.o_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.31.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.31.self_attn.q_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.31.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.31.self_attn.v_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.31.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.31.mlp.down_proj.qzeros: copying a param with shape torch.Size([344,\
          \ 512]) from checkpoint, the shape in current model is torch.Size([1, 512]).\r\
          \n        size mismatch for model.layers.31.mlp.down_proj.scales: copying\
          \ a param with shape torch.Size([344, 4096]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for\
          \ model.layers.31.mlp.gate_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 1376]) from checkpoint, the shape in current model is torch.Size([1, 1376]).\r\
          \n        size mismatch for model.layers.31.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([128, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 11008]).\r\n        size mismatch for\
          \ model.layers.31.mlp.up_proj.qzeros: copying a param with shape torch.Size([128,\
          \ 1376]) from checkpoint, the shape in current model is torch.Size([1, 1376]).\r\
          \n        size mismatch for model.layers.31.mlp.up_proj.scales: copying\
          \ a param with shape torch.Size([128, 11008]) from checkpoint, the shape\
          \ in current model is torch.Size([1, 11008]).\r\n\r\nI got this error, any\
          \ help?"
        updatedAt: '2023-05-01T11:57:55.675Z'
      numEdits: 0
      reactions: []
    id: 644fa943ccf704d60ffa51b6
    type: comment
  author: RoARene317
  content: " size mismatch for model.layers.29.self_attn.k_proj.scales: copying a\
    \ param with shape torch.Size([128, 4096]) from checkpoint, the shape in current\
    \ model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.29.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.29.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.29.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.29.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.29.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.29.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.29.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([344, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.29.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([344, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.29.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 1376]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 1376]).\r\n        size mismatch for model.layers.29.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([128, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 11008]).\r\n        size mismatch for model.layers.29.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 1376]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 1376]).\r\n        size mismatch for model.layers.29.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([128, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 11008]).\r\n        size mismatch for model.layers.30.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.30.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.30.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.30.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.30.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.30.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.30.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.30.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.30.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([344, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.30.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([344, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.30.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 1376]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 1376]).\r\n        size mismatch for model.layers.30.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([128, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 11008]).\r\n        size mismatch for model.layers.30.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 1376]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 1376]).\r\n        size mismatch for model.layers.30.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([128, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 11008]).\r\n        size mismatch for model.layers.31.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.31.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.31.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.31.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.31.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.31.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.31.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.31.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([128, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.31.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([344, 512]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 512]).\r\n        size mismatch for model.layers.31.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([344, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\r\n        size mismatch for model.layers.31.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 1376]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 1376]).\r\n        size mismatch for model.layers.31.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([128, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 11008]).\r\n        size mismatch for model.layers.31.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([128, 1376]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 1376]).\r\n        size mismatch for model.layers.31.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([128, 11008]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 11008]).\r\n\r\nI got this error, any help?"
  created_at: 2023-05-01 10:57:55+00:00
  edited: false
  hidden: false
  id: 644fa943ccf704d60ffa51b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
      fullname: TeH_Venom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TehVenom
      type: user
    createdAt: '2023-05-01T13:16:11.000Z'
    data:
      edited: false
      editors:
      - TehVenom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
          fullname: TeH_Venom
          isHf: false
          isPro: false
          name: TehVenom
          type: user
        html: '<p>make sure to load it as a 32 large group size model, you might''ve
          set whatever script you had there to load it as a 128g model.</p>

          '
        raw: make sure to load it as a 32 large group size model, you might've set
          whatever script you had there to load it as a 128g model.
        updatedAt: '2023-05-01T13:16:11.676Z'
      numEdits: 0
      reactions: []
    id: 644fbb9b577838187ef8c9d8
    type: comment
  author: TehVenom
  content: make sure to load it as a 32 large group size model, you might've set whatever
    script you had there to load it as a 128g model.
  created_at: 2023-05-01 12:16:11+00:00
  edited: false
  hidden: false
  id: 644fbb9b577838187ef8c9d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ca3e713c445279604d9880a03de33056.svg
      fullname: RoARene317
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RoARene317
      type: user
    createdAt: '2023-05-01T15:05:54.000Z'
    data:
      status: closed
    id: 644fd55220ba3e3e4bea4d30
    type: status-change
  author: RoARene317
  created_at: 2023-05-01 14:05:54+00:00
  id: 644fd55220ba3e3e4bea4d30
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TehVenom/Pygmalion-7b-4bit-GPTQ-Safetensors
repo_type: model
status: closed
target_branch: null
title: size mismatch for the model
