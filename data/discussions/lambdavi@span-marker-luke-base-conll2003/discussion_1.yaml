!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tomaarsen
conflicting_files: null
created_at: 2024-01-09 21:30:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
      fullname: Tom Aarsen
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomaarsen
      type: user
    createdAt: '2024-01-09T21:30:20.000Z'
    data:
      edited: false
      editors:
      - tomaarsen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9508345723152161
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png?w=200&h=200&f=face
          fullname: Tom Aarsen
          isHf: true
          isPro: false
          name: tomaarsen
          type: user
        html: '<p>Hello!</p>

          <p>This is very cool - I''d love to run it myself too, but I get an issue
          that <code>word_ids()</code> is not accessible on the non-fast <code>LukeTokenizer
          </code>. Did you get around this issue somehow?</p>

          <ul>

          <li>Tom Aarsen</li>

          </ul>

          '
        raw: "Hello!\r\n\r\nThis is very cool - I'd love to run it myself too, but\
          \ I get an issue that `word_ids()` is not accessible on the non-fast `LukeTokenizer\
          \ `. Did you get around this issue somehow?\r\n\r\n- Tom Aarsen"
        updatedAt: '2024-01-09T21:30:20.226Z'
      numEdits: 0
      reactions: []
    id: 659dbaece08ffb4dbe6745e0
    type: comment
  author: tomaarsen
  content: "Hello!\r\n\r\nThis is very cool - I'd love to run it myself too, but I\
    \ get an issue that `word_ids()` is not accessible on the non-fast `LukeTokenizer\
    \ `. Did you get around this issue somehow?\r\n\r\n- Tom Aarsen"
  created_at: 2024-01-09 21:30:20+00:00
  edited: false
  hidden: false
  id: 659dbaece08ffb4dbe6745e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8801c698434c332902ec180547f6692.svg
      fullname: Davide Buoso
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: lambdavi
      type: user
    createdAt: '2024-01-09T21:44:11.000Z'
    data:
      edited: false
      editors:
      - lambdavi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8718734979629517
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8801c698434c332902ec180547f6692.svg
          fullname: Davide Buoso
          isHf: false
          isPro: false
          name: lambdavi
          type: user
        html: '<p>Hello!</p>

          <p>First of all, thanks for your library, works great. Yeah I am going to
          update the model card, I encountered this problem as well, I used the RobertaTokenizer
          as alternative. I still have to figure out how to use the LukeTokenizer
          but I am working on it and I will release a v2 soon.</p>

          <pre><code>tokenizer = SpanMarkerTokenizer.from_pretrained("roberta-base",
          config=model.tokenizer.config)

          model.set_tokenizer(tokenizer)

          </code></pre>

          <p>Let me know if this solved the problem. </p>

          '
        raw: 'Hello!


          First of all, thanks for your library, works great. Yeah I am going to update
          the model card, I encountered this problem as well, I used the RobertaTokenizer
          as alternative. I still have to figure out how to use the LukeTokenizer
          but I am working on it and I will release a v2 soon.


          ```

          tokenizer = SpanMarkerTokenizer.from_pretrained("roberta-base", config=model.tokenizer.config)

          model.set_tokenizer(tokenizer)

          ```


          Let me know if this solved the problem. '
        updatedAt: '2024-01-09T21:44:11.268Z'
      numEdits: 0
      reactions: []
    id: 659dbe2b73410185e5381b27
    type: comment
  author: lambdavi
  content: 'Hello!


    First of all, thanks for your library, works great. Yeah I am going to update
    the model card, I encountered this problem as well, I used the RobertaTokenizer
    as alternative. I still have to figure out how to use the LukeTokenizer but I
    am working on it and I will release a v2 soon.


    ```

    tokenizer = SpanMarkerTokenizer.from_pretrained("roberta-base", config=model.tokenizer.config)

    model.set_tokenizer(tokenizer)

    ```


    Let me know if this solved the problem. '
  created_at: 2024-01-09 21:44:11+00:00
  edited: false
  hidden: false
  id: 659dbe2b73410185e5381b27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f8801c698434c332902ec180547f6692.svg
      fullname: Davide Buoso
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: lambdavi
      type: user
    createdAt: '2024-01-09T22:03:26.000Z'
    data:
      status: closed
    id: 659dc2aef4fb04c122032633
    type: status-change
  author: lambdavi
  created_at: 2024-01-09 22:03:26+00:00
  id: 659dc2aef4fb04c122032633
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: lambdavi/span-marker-luke-base-conll2003
repo_type: model
status: closed
target_branch: null
title: Tokenizer issue
