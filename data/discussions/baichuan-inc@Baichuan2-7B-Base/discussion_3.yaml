!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yiming1894
conflicting_files: null
created_at: 2023-11-24 09:42:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5ae6c59d507399569f62bd98d5f6347.svg
      fullname: Yiming
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yiming1894
      type: user
    createdAt: '2023-11-24T09:42:25.000Z'
    data:
      edited: false
      editors:
      - Yiming1894
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6862618923187256
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5ae6c59d507399569f62bd98d5f6347.svg
          fullname: Yiming
          isHf: false
          isPro: false
          name: Yiming1894
          type: user
        html: "<p>The code snippet below highlights a potential risk in mixed-precision\
          \ training/inference:</p>\n<pre><code>with torch.backends.cuda.sdp_kernel(enable_flash=True,\
          \ enable_math=True, enable_mem_efficient=True):\n    attn_output = F.scaled_dot_product_attention(query_states,\
          \ key_states, value_states, attn_mask = attention_mask)\n</code></pre>\n\
          <p>The qkv states may have mixed precision, depending on the configuration\
          \ of the accelerator or Seq2SeqTrainer. However, the attn_mask always has\
          \ a dtype of float32, which casts torch.finfo(torch.float32).min to '-inf'.\
          \ This can potentially cause the softmax result to become NaN.</p>\n"
        raw: "The code snippet below highlights a potential risk in mixed-precision\
          \ training/inference:\r\n```\r\nwith torch.backends.cuda.sdp_kernel(enable_flash=True,\
          \ enable_math=True, enable_mem_efficient=True):\r\n    attn_output = F.scaled_dot_product_attention(query_states,\
          \ key_states, value_states, attn_mask = attention_mask)\r\n```\r\nThe qkv\
          \ states may have mixed precision, depending on the configuration of the\
          \ accelerator or Seq2SeqTrainer. However, the attn_mask always has a dtype\
          \ of float32, which casts torch.finfo(torch.float32).min to '-inf'. This\
          \ can potentially cause the softmax result to become NaN."
        updatedAt: '2023-11-24T09:42:25.672Z'
      numEdits: 0
      reactions: []
    id: 6560700147d95544efa0751c
    type: comment
  author: Yiming1894
  content: "The code snippet below highlights a potential risk in mixed-precision\
    \ training/inference:\r\n```\r\nwith torch.backends.cuda.sdp_kernel(enable_flash=True,\
    \ enable_math=True, enable_mem_efficient=True):\r\n    attn_output = F.scaled_dot_product_attention(query_states,\
    \ key_states, value_states, attn_mask = attention_mask)\r\n```\r\nThe qkv states\
    \ may have mixed precision, depending on the configuration of the accelerator\
    \ or Seq2SeqTrainer. However, the attn_mask always has a dtype of float32, which\
    \ casts torch.finfo(torch.float32).min to '-inf'. This can potentially cause the\
    \ softmax result to become NaN."
  created_at: 2023-11-24 09:42:25+00:00
  edited: false
  hidden: false
  id: 6560700147d95544efa0751c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5ae6c59d507399569f62bd98d5f6347.svg
      fullname: Yiming
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yiming1894
      type: user
    createdAt: '2023-11-24T09:49:42.000Z'
    data:
      edited: false
      editors:
      - Yiming1894
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44902098178863525
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5ae6c59d507399569f62bd98d5f6347.svg
          fullname: Yiming
          isHf: false
          isPro: false
          name: Yiming1894
          type: user
        html: "<p>According to Baichuan1, <a rel=\"nofollow\" href=\"url\">https://huggingface.co/baichuan-inc/Baichuan-7B/blob/c1a5c7d5b7f50ecc51bb0e08150a9f12e5656756/modeling_baichuan.py#L229</a>\
          \ a feasible approach could be:</p>\n<pre><code>with torch.backends.cuda.sdp_kernel(enable_flash=True,\
          \ enable_math=True, enable_mem_efficient=True):\n    attention_mask = torch.max(attention_mask,\
          \ torch.tensor(torch.finfo(query_states.dtype).min))\n    attn_output =\
          \ F.scaled_dot_product_attention(query_states, key_states, value_states,\
          \ attn_mask = attention_mask)\n</code></pre>\n"
        raw: "According to Baichuan1, [https://huggingface.co/baichuan-inc/Baichuan-7B/blob/c1a5c7d5b7f50ecc51bb0e08150a9f12e5656756/modeling_baichuan.py#L229](url)\
          \ a feasible approach could be:\n```\nwith torch.backends.cuda.sdp_kernel(enable_flash=True,\
          \ enable_math=True, enable_mem_efficient=True):\n    attention_mask = torch.max(attention_mask,\
          \ torch.tensor(torch.finfo(query_states.dtype).min))\n    attn_output =\
          \ F.scaled_dot_product_attention(query_states, key_states, value_states,\
          \ attn_mask = attention_mask)\n\n```"
        updatedAt: '2023-11-24T09:49:42.035Z'
      numEdits: 0
      reactions: []
    id: 656071b65f80d0ddf4a61fdd
    type: comment
  author: Yiming1894
  content: "According to Baichuan1, [https://huggingface.co/baichuan-inc/Baichuan-7B/blob/c1a5c7d5b7f50ecc51bb0e08150a9f12e5656756/modeling_baichuan.py#L229](url)\
    \ a feasible approach could be:\n```\nwith torch.backends.cuda.sdp_kernel(enable_flash=True,\
    \ enable_math=True, enable_mem_efficient=True):\n    attention_mask = torch.max(attention_mask,\
    \ torch.tensor(torch.finfo(query_states.dtype).min))\n    attn_output = F.scaled_dot_product_attention(query_states,\
    \ key_states, value_states, attn_mask = attention_mask)\n\n```"
  created_at: 2023-11-24 09:49:42+00:00
  edited: false
  hidden: false
  id: 656071b65f80d0ddf4a61fdd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: baichuan-inc/Baichuan2-7B-Base
repo_type: model
status: open
target_branch: null
title: The dtype of attention mask should match the qkv states
