!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sd3ntato
conflicting_files: null
created_at: 2024-01-11 11:34:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/507797bbab731b009d2c236c9705bb36.svg
      fullname: Valerio Mariani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sd3ntato
      type: user
    createdAt: '2024-01-11T11:34:42.000Z'
    data:
      edited: false
      editors:
      - sd3ntato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47078225016593933
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/507797bbab731b009d2c236c9705bb36.svg
          fullname: Valerio Mariani
          isHf: false
          isPro: false
          name: sd3ntato
          type: user
        html: "<pre><code>$ docker run \\\n&gt;   --gpus all \\\n&gt;   --shm-size\
          \ 1g \\\n&gt;   -v ~/models:/data \\\n&gt;   ghcr.io/huggingface/text-generation-inference:1.3.0\
          \ \\\n&gt;   --model-id TheBloke/mixtral-8x7b-v0.1-AWQ --port 3000 --quantize\
          \ awq --max-input-length 3696 --max-total-tokens 4096 --max-batch-prefill-tokens\
          \ 4096\n2024-01-11T11:11:54.309311Z  INFO text_generation_launcher: Args\
          \ { model_id: \"TheBloke/mixtral-8x7b-v0.1-AWQ\", revision: None, validation_workers:\
          \ 2, sharded: None, num_shard: None, quantize: Some(Awq), speculate: None,\
          \ dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of:\
          \ 2, max_stop_sequences: 4, max_top_n_tokens: 5, max_input_length: 3696,\
          \ max_total_tokens: 4096, waiting_served_ratio: 1.2, max_batch_prefill_tokens:\
          \ 4096, max_batch_total_tokens: None, max_waiting_tokens: 20, hostname:\
          \ \"b1a6fd99478a\", port: 3000, shard_uds_path: \"/tmp/text-generation-server\"\
          , master_addr: \"localhost\", master_port: 29500, huggingface_hub_cache:\
          \ Some(\"/data\"), weights_cache_override: None, disable_custom_kernels:\
          \ false, cuda_memory_fraction: 1.0, rope_scaling: None, rope_factor: None,\
          \ json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma:\
          \ None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_edge:\
          \ None, env: false }\n2024-01-11T11:11:54.309424Z  INFO download: text_generation_launcher:\
          \ Starting download process.\n2024-01-11T11:11:57.699491Z  INFO text_generation_launcher:\
          \ Files are already present on the host. Skipping download.\n\n2024-01-11T11:11:58.313113Z\
          \  INFO download: text_generation_launcher: Successfully downloaded weights.\n\
          2024-01-11T11:11:58.313354Z  INFO shard-manager: text_generation_launcher:\
          \ Starting shard rank=0\n2024-01-11T11:12:02.347372Z ERROR text_generation_launcher:\
          \ Error when initializing model\nTraceback (most recent call last):\n  File\
          \ \"/opt/conda/bin/text-generation-server\", line 8, in &lt;module&gt;\n\
          \    sys.exit(app())\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 311, in __call__\n    return get_command(self)(*args, **kwargs)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 778, in main\n    return _main(\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 216, in _main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
          , line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 683, in wrapper\n    return callback(**use_params)  # type: ignore\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
          , line 89, in serve\n    server.serve(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 215, in serve\n    asyncio.run(\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
          , line 44, in run\n    return loop.run_until_complete(main)\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 636, in run_until_complete\n    self.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          &gt; File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 161, in serve_inner\n    model = get_model(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/__init__.py\"\
          , line 310, in get_model\n    return FlashMixtral(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mixtral.py\"\
          , line 21, in __init__\n    super(FlashMixtral, self).__init__(\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 318, in __init__\n    SLIDING_WINDOW_BLOCKS = math.ceil(config.sliding_window\
          \ / BLOCK_SIZE)\nTypeError: unsupported operand type(s) for /: 'NoneType'\
          \ and 'int'\n</code></pre>\n"
        raw: "\r\n```\r\n$ docker run \\\r\n>   --gpus all \\\r\n>   --shm-size 1g\
          \ \\\r\n>   -v ~/models:/data \\\r\n>   ghcr.io/huggingface/text-generation-inference:1.3.0\
          \ \\\r\n>   --model-id TheBloke/mixtral-8x7b-v0.1-AWQ --port 3000 --quantize\
          \ awq --max-input-length 3696 --max-total-tokens 4096 --max-batch-prefill-tokens\
          \ 4096\r\n2024-01-11T11:11:54.309311Z  INFO text_generation_launcher: Args\
          \ { model_id: \"TheBloke/mixtral-8x7b-v0.1-AWQ\", revision: None, validation_workers:\
          \ 2, sharded: None, num_shard: None, quantize: Some(Awq), speculate: None,\
          \ dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of:\
          \ 2, max_stop_sequences: 4, max_top_n_tokens: 5, max_input_length: 3696,\
          \ max_total_tokens: 4096, waiting_served_ratio: 1.2, max_batch_prefill_tokens:\
          \ 4096, max_batch_total_tokens: None, max_waiting_tokens: 20, hostname:\
          \ \"b1a6fd99478a\", port: 3000, shard_uds_path: \"/tmp/text-generation-server\"\
          , master_addr: \"localhost\", master_port: 29500, huggingface_hub_cache:\
          \ Some(\"/data\"), weights_cache_override: None, disable_custom_kernels:\
          \ false, cuda_memory_fraction: 1.0, rope_scaling: None, rope_factor: None,\
          \ json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma:\
          \ None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_edge:\
          \ None, env: false }\r\n2024-01-11T11:11:54.309424Z  INFO download: text_generation_launcher:\
          \ Starting download process.\r\n2024-01-11T11:11:57.699491Z  INFO text_generation_launcher:\
          \ Files are already present on the host. Skipping download.\r\n\r\n2024-01-11T11:11:58.313113Z\
          \  INFO download: text_generation_launcher: Successfully downloaded weights.\r\
          \n2024-01-11T11:11:58.313354Z  INFO shard-manager: text_generation_launcher:\
          \ Starting shard rank=0\r\n2024-01-11T11:12:02.347372Z ERROR text_generation_launcher:\
          \ Error when initializing model\r\nTraceback (most recent call last):\r\n\
          \  File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\
          \n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
          , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/typer/core.py\", line 778, in main\r\
          \n    return _main(\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
          , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
          , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line\
          \ 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/typer/main.py\", line 683, in wrapper\r\
          \n    return callback(**use_params)  # type: ignore\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
          , line 89, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 215, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\r\
          \n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 603, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
          , line 1909, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
          , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\
          \n> File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
          , line 161, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/__init__.py\"\
          , line 310, in get_model\r\n    return FlashMixtral(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mixtral.py\"\
          , line 21, in __init__\r\n    super(FlashMixtral, self).__init__(\r\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
          , line 318, in __init__\r\n    SLIDING_WINDOW_BLOCKS = math.ceil(config.sliding_window\
          \ / BLOCK_SIZE)\r\nTypeError: unsupported operand type(s) for /: 'NoneType'\
          \ and 'int'\r\n```"
        updatedAt: '2024-01-11T11:34:42.020Z'
      numEdits: 0
      reactions: []
    id: 659fd252874e583fed8260d6
    type: comment
  author: sd3ntato
  content: "\r\n```\r\n$ docker run \\\r\n>   --gpus all \\\r\n>   --shm-size 1g \\\
    \r\n>   -v ~/models:/data \\\r\n>   ghcr.io/huggingface/text-generation-inference:1.3.0\
    \ \\\r\n>   --model-id TheBloke/mixtral-8x7b-v0.1-AWQ --port 3000 --quantize awq\
    \ --max-input-length 3696 --max-total-tokens 4096 --max-batch-prefill-tokens 4096\r\
    \n2024-01-11T11:11:54.309311Z  INFO text_generation_launcher: Args { model_id:\
    \ \"TheBloke/mixtral-8x7b-v0.1-AWQ\", revision: None, validation_workers: 2, sharded:\
    \ None, num_shard: None, quantize: Some(Awq), speculate: None, dtype: None, trust_remote_code:\
    \ false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4,\
    \ max_top_n_tokens: 5, max_input_length: 3696, max_total_tokens: 4096, waiting_served_ratio:\
    \ 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: None, max_waiting_tokens:\
    \ 20, hostname: \"b1a6fd99478a\", port: 3000, shard_uds_path: \"/tmp/text-generation-server\"\
    , master_addr: \"localhost\", master_port: 29500, huggingface_hub_cache: Some(\"\
    /data\"), weights_cache_override: None, disable_custom_kernels: false, cuda_memory_fraction:\
    \ 1.0, rope_scaling: None, rope_factor: None, json_output: false, otlp_endpoint:\
    \ None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok:\
    \ false, ngrok_authtoken: None, ngrok_edge: None, env: false }\r\n2024-01-11T11:11:54.309424Z\
    \  INFO download: text_generation_launcher: Starting download process.\r\n2024-01-11T11:11:57.699491Z\
    \  INFO text_generation_launcher: Files are already present on the host. Skipping\
    \ download.\r\n\r\n2024-01-11T11:11:58.313113Z  INFO download: text_generation_launcher:\
    \ Successfully downloaded weights.\r\n2024-01-11T11:11:58.313354Z  INFO shard-manager:\
    \ text_generation_launcher: Starting shard rank=0\r\n2024-01-11T11:12:02.347372Z\
    \ ERROR text_generation_launcher: Error when initializing model\r\nTraceback (most\
    \ recent call last):\r\n  File \"/opt/conda/bin/text-generation-server\", line\
    \ 8, in <module>\r\n    sys.exit(app())\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
    , line 311, in __call__\r\n    return get_command(self)(*args, **kwargs)\r\n \
    \ File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line 1157, in\
    \ __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
    , line 778, in main\r\n    return _main(\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/core.py\"\
    , line 216, in _main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\"\
    , line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/click/core.py\", line 1434,\
    \ in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\
    /opt/conda/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\r\
    \n    return __callback(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/typer/main.py\"\
    , line 683, in wrapper\r\n    return callback(**use_params)  # type: ignore\r\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py\"\
    , line 89, in serve\r\n    server.serve(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
    , line 215, in serve\r\n    asyncio.run(\r\n  File \"/opt/conda/lib/python3.10/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 636, in run_until_complete\r\n    self.run_forever()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 603, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\"\
    , line 1909, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\"\
    , line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n>\
    \ File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py\"\
    , line 161, in serve_inner\r\n    model = get_model(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/__init__.py\"\
    , line 310, in get_model\r\n    return FlashMixtral(\r\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mixtral.py\"\
    , line 21, in __init__\r\n    super(FlashMixtral, self).__init__(\r\n  File \"\
    /opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py\"\
    , line 318, in __init__\r\n    SLIDING_WINDOW_BLOCKS = math.ceil(config.sliding_window\
    \ / BLOCK_SIZE)\r\nTypeError: unsupported operand type(s) for /: 'NoneType' and\
    \ 'int'\r\n```"
  created_at: 2024-01-11 11:34:42+00:00
  edited: false
  hidden: false
  id: 659fd252874e583fed8260d6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/mixtral-8x7b-v0.1-AWQ
repo_type: model
status: open
target_branch: null
title: TGI error sliding window null
