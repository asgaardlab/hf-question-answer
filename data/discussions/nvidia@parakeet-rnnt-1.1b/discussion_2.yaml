!!python/object:huggingface_hub.community.DiscussionWithDetails
author: qanastek
conflicting_files: null
created_at: 2024-01-02 20:51:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1632390916422-noauth.jpeg?w=200&h=200&f=face
      fullname: yanis labrak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: qanastek
      type: user
    createdAt: '2024-01-02T20:51:06.000Z'
    data:
      edited: false
      editors:
      - qanastek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8823875188827515
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1632390916422-noauth.jpeg?w=200&h=200&f=face
          fullname: yanis labrak
          isHf: false
          isPro: false
          name: qanastek
          type: user
        html: '<p>Thank you very much for your contribution to the community while
          sharing both models and training scripts.</p>

          <p>You have mentioned that the training dataset consists of private subset
          with 40K hours of English speech plus 25K hours from the following public
          datasets:</p>

          <ul>

          <li>Librispeech 960 hours of English speech</li>

          <li>Fisher Corpus</li>

          <li>Switchboard-1 Dataset</li>

          <li>WSJ-0 and WSJ-1</li>

          <li>National Speech Corpus (Part 1, Part 6)</li>

          <li>VCTK</li>

          <li>VoxPopuli (EN)</li>

          <li>Europarl-ASR (EN)</li>

          <li>Multilingual Librispeech (MLS EN) - 2,000 hour subset</li>

          <li>Mozilla Common Voice (v7.0)</li>

          <li>People''s Speech - 12,000 hour subset</li>

          </ul>

          <p>But you haven''t mentioned any of the normalization steps applied to
          the transcriptions, while each corpus have its own annotation protocol.
          Do you share these pre-processing steps anywhere ? I cannot find them on
          the GitHub repository of NeMo.</p>

          <p>Regards.</p>

          '
        raw: "Thank you very much for your contribution to the community while sharing\
          \ both models and training scripts.\r\n\r\nYou have mentioned that the training\
          \ dataset consists of private subset with 40K hours of English speech plus\
          \ 25K hours from the following public datasets:\r\n\r\n- Librispeech 960\
          \ hours of English speech\r\n- Fisher Corpus\r\n- Switchboard-1 Dataset\r\
          \n- WSJ-0 and WSJ-1\r\n- National Speech Corpus (Part 1, Part 6)\r\n- VCTK\r\
          \n- VoxPopuli (EN)\r\n- Europarl-ASR (EN)\r\n- Multilingual Librispeech\
          \ (MLS EN) - 2,000 hour subset\r\n- Mozilla Common Voice (v7.0)\r\n- People's\
          \ Speech - 12,000 hour subset\r\n\r\nBut you haven't mentioned any of the\
          \ normalization steps applied to the transcriptions, while each corpus have\
          \ its own annotation protocol. Do you share these pre-processing steps anywhere\
          \ ? I cannot find them on the GitHub repository of NeMo.\r\n\r\nRegards."
        updatedAt: '2024-01-02T20:51:06.923Z'
      numEdits: 0
      reactions: []
    id: 6594773a754092f6b140bcef
    type: comment
  author: qanastek
  content: "Thank you very much for your contribution to the community while sharing\
    \ both models and training scripts.\r\n\r\nYou have mentioned that the training\
    \ dataset consists of private subset with 40K hours of English speech plus 25K\
    \ hours from the following public datasets:\r\n\r\n- Librispeech 960 hours of\
    \ English speech\r\n- Fisher Corpus\r\n- Switchboard-1 Dataset\r\n- WSJ-0 and\
    \ WSJ-1\r\n- National Speech Corpus (Part 1, Part 6)\r\n- VCTK\r\n- VoxPopuli\
    \ (EN)\r\n- Europarl-ASR (EN)\r\n- Multilingual Librispeech (MLS EN) - 2,000 hour\
    \ subset\r\n- Mozilla Common Voice (v7.0)\r\n- People's Speech - 12,000 hour subset\r\
    \n\r\nBut you haven't mentioned any of the normalization steps applied to the\
    \ transcriptions, while each corpus have its own annotation protocol. Do you share\
    \ these pre-processing steps anywhere ? I cannot find them on the GitHub repository\
    \ of NeMo.\r\n\r\nRegards."
  created_at: 2024-01-02 20:51:06+00:00
  edited: false
  hidden: false
  id: 6594773a754092f6b140bcef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07b095d748ef7c24b06c1b2e222171bf.svg
      fullname: Nithin Rao Koluguri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nithinraok
      type: user
    createdAt: '2024-01-03T01:08:59.000Z'
    data:
      edited: true
      editors:
      - nithinraok
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.756671667098999
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07b095d748ef7c24b06c1b2e222171bf.svg
          fullname: Nithin Rao Koluguri
          isHf: false
          isPro: false
          name: nithinraok
          type: user
        html: '<p>Some of the dataset preprocessing scripts are made available here:
          <a rel="nofollow" href="https://github.com/NVIDIA/NeMo/tree/main/scripts/dataset_processing">https://github.com/NVIDIA/NeMo/tree/main/scripts/dataset_processing</a>
          </p>

          <p>Eventually we will make all public dataset pre processing scripts available.
          </p>

          '
        raw: "Some of the dataset preprocessing scripts are made available here: https://github.com/NVIDIA/NeMo/tree/main/scripts/dataset_processing\
          \ \n\nEventually we will make all public dataset pre processing scripts\
          \ available. "
        updatedAt: '2024-01-03T04:12:08.954Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - qanastek
        - smajumdar94
    id: 6594b3abf1aef46ec0e95699
    type: comment
  author: nithinraok
  content: "Some of the dataset preprocessing scripts are made available here: https://github.com/NVIDIA/NeMo/tree/main/scripts/dataset_processing\
    \ \n\nEventually we will make all public dataset pre processing scripts available. "
  created_at: 2024-01-03 01:08:59+00:00
  edited: true
  hidden: false
  id: 6594b3abf1aef46ec0e95699
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649899774659-6254f8e5d21e4cc386b881ad.jpeg?w=200&h=200&f=face
      fullname: Somshubra Majumdar
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: smajumdar94
      type: user
    createdAt: '2024-01-22T01:16:40.000Z'
    data:
      status: closed
    id: 65adc1f8d3eff8cf4ae06414
    type: status-change
  author: smajumdar94
  created_at: 2024-01-22 01:16:40+00:00
  id: 65adc1f8d3eff8cf4ae06414
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: nvidia/parakeet-rnnt-1.1b
repo_type: model
status: closed
target_branch: null
title: Transcription normalization
