!!python/object:huggingface_hub.community.DiscussionWithDetails
author: peshangjaafar
conflicting_files: null
created_at: 2022-09-30 20:26:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c17997a972a16b2a6cfbd29101891920.svg
      fullname: Peshang Jafaar Dhahir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: peshangjaafar
      type: user
    createdAt: '2022-09-30T21:26:52.000Z'
    data:
      edited: false
      editors:
      - peshangjaafar
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c17997a972a16b2a6cfbd29101891920.svg
          fullname: Peshang Jafaar Dhahir
          isHf: false
          isPro: false
          name: peshangjaafar
          type: user
        html: '<p>could you please provide the source code? I  tried and trained the
          model but the output was much worst.<br>I have used :<br>tokenizer =  XLMRobertaTokenizerFast.from_pretrained("xlm-roberta-base")<br>model
          = EncoderDecoderModel.from_encoder_decoder_pretrained("xlm-roberta-base",
          "xlm-roberta-base", tie_encoder_decoder=True)<br>hyperparameters: ( do_train=True,    do_eval=True,    eval_steps=20,    num_train_epochs=5,    dataloader_num_workers=2,    optim="adamw_torch",<br>    learning_rate=5e-5,    warmup_steps=500,    fp16=True)</p>

          '
        raw: "could you please provide the source code? I  tried and trained the model\
          \ but the output was much worst.\r\nI have used :\r\ntokenizer =  XLMRobertaTokenizerFast.from_pretrained(\"\
          xlm-roberta-base\")\r\nmodel = EncoderDecoderModel.from_encoder_decoder_pretrained(\"\
          xlm-roberta-base\", \"xlm-roberta-base\", tie_encoder_decoder=True)\r\n\
          hyperparameters: ( do_train=True,    do_eval=True,    eval_steps=20,   \
          \ num_train_epochs=5,    dataloader_num_workers=2,    optim=\"adamw_torch\"\
          ,\r\n    learning_rate=5e-5,    warmup_steps=500,    fp16=True)"
        updatedAt: '2022-09-30T21:26:52.911Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MehwishFatima
    id: 63375f1c9d05959872c56adb
    type: comment
  author: peshangjaafar
  content: "could you please provide the source code? I  tried and trained the model\
    \ but the output was much worst.\r\nI have used :\r\ntokenizer =  XLMRobertaTokenizerFast.from_pretrained(\"\
    xlm-roberta-base\")\r\nmodel = EncoderDecoderModel.from_encoder_decoder_pretrained(\"\
    xlm-roberta-base\", \"xlm-roberta-base\", tie_encoder_decoder=True)\r\nhyperparameters:\
    \ ( do_train=True,    do_eval=True,    eval_steps=20,    num_train_epochs=5, \
    \   dataloader_num_workers=2,    optim=\"adamw_torch\",\r\n    learning_rate=5e-5,\
    \    warmup_steps=500,    fp16=True)"
  created_at: 2022-09-30 20:26:52+00:00
  edited: false
  hidden: false
  id: 63375f1c9d05959872c56adb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650536577460-62612f81aee4bea0213ede6b.jpeg?w=200&h=200&f=face
      fullname: Mehwish Fatima
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MehwishFatima
      type: user
    createdAt: '2022-10-28T14:32:05.000Z'
    data:
      edited: false
      editors:
      - MehwishFatima
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650536577460-62612f81aee4bea0213ede6b.jpeg?w=200&h=200&f=face
          fullname: Mehwish Fatima
          isHf: false
          isPro: false
          name: MehwishFatima
          type: user
        html: '<blockquote>

          <p>could you please provide the source code? I  tried and trained the model
          but the output was much worst.<br>I have used :<br>tokenizer =  XLMRobertaTokenizerFast.from_pretrained("xlm-roberta-base")<br>model
          = EncoderDecoderModel.from_encoder_decoder_pretrained("xlm-roberta-base",
          "xlm-roberta-base", tie_encoder_decoder=True)<br>hyperparameters: ( do_train=True,    do_eval=True,    eval_steps=20,    num_train_epochs=5,    dataloader_num_workers=2,    optim="adamw_torch",<br>    learning_rate=5e-5,    warmup_steps=500,    fp16=True)</p>

          </blockquote>

          <p>Have you solved the issue? I am trying for a seq-2-seq model and also
          having some issues.</p>

          '
        raw: '> could you please provide the source code? I  tried and trained the
          model but the output was much worst.

          > I have used :

          > tokenizer =  XLMRobertaTokenizerFast.from_pretrained("xlm-roberta-base")

          > model = EncoderDecoderModel.from_encoder_decoder_pretrained("xlm-roberta-base",
          "xlm-roberta-base", tie_encoder_decoder=True)

          > hyperparameters: ( do_train=True,    do_eval=True,    eval_steps=20,    num_train_epochs=5,    dataloader_num_workers=2,    optim="adamw_torch",

          >     learning_rate=5e-5,    warmup_steps=500,    fp16=True)


          Have you solved the issue? I am trying for a seq-2-seq model and also having
          some issues.'
        updatedAt: '2022-10-28T14:32:05.944Z'
      numEdits: 0
      reactions: []
    id: 635be7e51a1cf10f9e59c540
    type: comment
  author: MehwishFatima
  content: '> could you please provide the source code? I  tried and trained the model
    but the output was much worst.

    > I have used :

    > tokenizer =  XLMRobertaTokenizerFast.from_pretrained("xlm-roberta-base")

    > model = EncoderDecoderModel.from_encoder_decoder_pretrained("xlm-roberta-base",
    "xlm-roberta-base", tie_encoder_decoder=True)

    > hyperparameters: ( do_train=True,    do_eval=True,    eval_steps=20,    num_train_epochs=5,    dataloader_num_workers=2,    optim="adamw_torch",

    >     learning_rate=5e-5,    warmup_steps=500,    fp16=True)


    Have you solved the issue? I am trying for a seq-2-seq model and also having some
    issues.'
  created_at: 2022-10-28 13:32:05+00:00
  edited: false
  hidden: false
  id: 635be7e51a1cf10f9e59c540
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ahmeddbahaa/xlmroberta2xlmroberta-finetune-summarization-ar
repo_type: model
status: open
target_branch: null
title: 'implementation issue '
