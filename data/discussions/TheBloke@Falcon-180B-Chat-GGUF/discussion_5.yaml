!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Flanua
conflicting_files: null
created_at: 2023-09-09 17:40:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-09T18:40:23.000Z'
    data:
      edited: true
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9290716648101807
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<p>It''s been said that this model requires 400GB of ram for inference
          to work swiftly (which might be the true to run it swiftly)<br>But upon
          loading the model I saw that this model as so many other large models requires
          just a bit more ram from it actual size.<br>This particular model with Q8
          requires: 181920.04 MB (+ 320.00 MB per state) of DDR ram.<br>Model falcon-180b-chat.Q8_0.gguf
          file size 177GB (186,288,380KB) and (190,759,300,512 bytes - on my HDD disk).<br>So
          just to run this model you just need around 181920.04 MB (+ 320.00 MB per
          state) of DDR ram.</p>

          '
        raw: "It's been said that this model requires 400GB of ram for inference to\
          \ work swiftly (which might be the true to run it swiftly) \nBut upon loading\
          \ the model I saw that this model as so many other large models requires\
          \ just a bit more ram from it actual size. \nThis particular model with\
          \ Q8 requires: 181920.04 MB (+ 320.00 MB per state) of DDR ram.\nModel falcon-180b-chat.Q8_0.gguf\
          \ file size 177GB (186,288,380KB) and (190,759,300,512 bytes - on my HDD\
          \ disk).\nSo just to run this model you just need around 181920.04 MB (+\
          \ 320.00 MB per state) of DDR ram."
        updatedAt: '2023-09-09T18:56:58.566Z'
      numEdits: 8
      reactions: []
    id: 64fcbc1752e82dd4329a2a78
    type: comment
  author: Flanua
  content: "It's been said that this model requires 400GB of ram for inference to\
    \ work swiftly (which might be the true to run it swiftly) \nBut upon loading\
    \ the model I saw that this model as so many other large models requires just\
    \ a bit more ram from it actual size. \nThis particular model with Q8 requires:\
    \ 181920.04 MB (+ 320.00 MB per state) of DDR ram.\nModel falcon-180b-chat.Q8_0.gguf\
    \ file size 177GB (186,288,380KB) and (190,759,300,512 bytes - on my HDD disk).\n\
    So just to run this model you just need around 181920.04 MB (+ 320.00 MB per state)\
    \ of DDR ram."
  created_at: 2023-09-09 17:40:23+00:00
  edited: true
  hidden: false
  id: 64fcbc1752e82dd4329a2a78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-09T21:40:44.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.978417694568634
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah that "400GB" is info from the source model README, ie for the
          unquantized model.  </p>

          <p>All these models I''ve uploaded are quantised so they will need a lot
          less, with Q8 being the largest of them, needing roughly half what the original
          needed - as you showed.</p>

          '
        raw: "Yeah that \"400GB\" is info from the source model README, ie for the\
          \ unquantized model.  \n\nAll these models I've uploaded are quantised so\
          \ they will need a lot less, with Q8 being the largest of them, needing\
          \ roughly half what the original needed - as you showed."
        updatedAt: '2023-09-09T21:40:44.693Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Flanua
    id: 64fce65c74574268a5825592
    type: comment
  author: TheBloke
  content: "Yeah that \"400GB\" is info from the source model README, ie for the unquantized\
    \ model.  \n\nAll these models I've uploaded are quantised so they will need a\
    \ lot less, with Q8 being the largest of them, needing roughly half what the original\
    \ needed - as you showed."
  created_at: 2023-09-09 20:40:44+00:00
  edited: false
  hidden: false
  id: 64fce65c74574268a5825592
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df7bc66364c7f95729e256f01c5abef0.svg
      fullname: Alexone
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krustik
      type: user
    createdAt: '2023-09-10T16:30:04.000Z'
    data:
      edited: true
      editors:
      - krustik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7832135558128357
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df7bc66364c7f95729e256f01c5abef0.svg
          fullname: Alexone
          isHf: false
          isPro: false
          name: krustik
          type: user
        html: '<p>Yeah, i was managed to run a Q5_Medium model, but barely on limits
          of my system (consumer motherboards can support max 128Gb RAM) only on CPU
          (GPU even offloading produce always crashes).<br>My system is not Windows,
          where max ram is max, in Linux there''s swap and etc helping running such
          even above available RAM (i recommend everyone Pika OS which is Ubuntu clone
          without Ubuntu headache).<br>Q4_Medium also tested and it''s fine. Q4_M
          working at 0.40 tokens/sec max, Q5_M at 0.36 tokens/sec max registered.
          (Intel Xeon 14 cores, 28 threads(only 23 used)<br>Q5_Medium produced some
          webbrowser resetting, when page becoming blank, but it''s not crashing all
          and can be restored with refreshing localhost page view with all chat history
          saved (maybe it''s problem with lack of RAM or browser). In one of such
          instances it produces error in the logs, this is it:<br>ERROR:    Exception
          in ASGI application</p>

          <p>Traceback (most recent call last):<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/uvicorn/protocols/websockets/websockets_impl.py",
          line 247, in run_asgi<br>    result = await self.app(self.scope, self.asgi_receive,
          self.asgi_send)<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py",
          line 84, in <strong>call</strong><br>    return await self.app(scope, receive,
          send)<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/fastapi/applications.py",
          line 276, in <strong>call</strong><br>    await super().<strong>call</strong>(scope,
          receive, send)<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/applications.py",
          line 122, in <strong>call</strong><br>    await self.middleware_stack(scope,
          receive, send)<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/middleware/errors.py",
          line 149, in <strong>call</strong><br>    await self.app(scope, receive,
          send)<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/middleware/cors.py",
          line 75, in <strong>call</strong><br>    await self.app(scope, receive,
          send)<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/middleware/exceptions.py",
          line 79, in <strong>call</strong><br>    raise exc<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/middleware/exceptions.py",
          line 68, in <strong>call</strong><br>    await self.app(scope, receive,
          sender)<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py",
          line 21, in <strong>call</strong><br>    raise e<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py",
          line 18, in <strong>call</strong><br>    await self.app(scope, receive,
          send)<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/routing.py",
          line 718, in <strong>call</strong><br>    await route.handle(scope, receive,
          send)<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/routing.py",
          line 341, in handle<br>    await self.app(scope, receive, send)<br>  File
          "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/routing.py",
          line 82, in app<br>    await func(session)<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/fastapi/routing.py",
          line 289, in app<br>    await dependant.call(**values)<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/gradio/routes.py",
          line 536, in join_queue<br>    session_info = await asyncio.wait_for(<br>  File
          "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/asyncio/tasks.py",
          line 445, in wait_for<br>    return fut.result()<br>  File "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/websockets.py",
          line 133, in receive_json<br>    self._raise_on_disconnect(message)<br>  File
          "/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/websockets.py",
          line 105, in _raise_on_disconnect<br>    raise WebSocketDisconnect(message["code"])<br>starlette.websockets.WebSocketDisconnect:
          1001</p>

          <p>but it continues to work by refreshing webpage.</p>

          <p>About the model itself, i can call it the most "autistic". Q4_M after
          first prompt starting to going into hallucinating with itself, predicting
          next user input, writing it and self answering to it, so it''s writing such
          self-dialogue long novels about any topic. In chat mode it''s kinda censored
          more than in instruct mode (decline of medical diagnosing and etc. but not
          in instruct). Q5_M not yet tested but less that self-dialogues, although
          it''s also starting that if you click Continue to any ended prompt answer.
          Leaving Q4_M working overnight with max tokens and instruction was a fail,
          it''s produced mostly textual garbage.</p>

          <p>It is the most easiest model which you can put into "stasis" or "cold
          processing" mode (i''ve seen such only with Llama 1 65B Q8 GGML once), it''s
          a phenomena, when neuronet using most system resources and alive but not
          producing high heat, running cold, i see some smartness in this because
          it''s avoiding to reach temp limits for turning on fans, so it''s very quiet.
          All such cases it''s never freezing whole operating system with allowing
          other processes in parallel (like watching 4K video). I''m writing this
          during such mode in Q5_M, it reserved all available RAM but working at 20%
          CPU (rarely crossing 50%) with writing answer to prompt. In Llama 1 65B
          during stasis it''s just thinking about for hours no one knows what, this
          Falcon at least writing something during that. (there''s no parameters to
          force that and there''s no user implemented limits, it''s putting itself
          into such mode naturally, also it resetting by next prompt/next tokens session)</p>

          '
        raw: "Yeah, i was managed to run a Q5_Medium model, but barely on limits of\
          \ my system (consumer motherboards can support max 128Gb RAM) only on CPU\
          \ (GPU even offloading produce always crashes).\nMy system is not Windows,\
          \ where max ram is max, in Linux there's swap and etc helping running such\
          \ even above available RAM (i recommend everyone Pika OS which is Ubuntu\
          \ clone without Ubuntu headache).\nQ4_Medium also tested and it's fine.\
          \ Q4_M working at 0.40 tokens/sec max, Q5_M at 0.36 tokens/sec max registered.\
          \ (Intel Xeon 14 cores, 28 threads(only 23 used)\nQ5_Medium produced some\
          \ webbrowser resetting, when page becoming blank, but it's not crashing\
          \ all and can be restored with refreshing localhost page view with all chat\
          \ history saved (maybe it's problem with lack of RAM or browser). In one\
          \ of such instances it produces error in the logs, this is it:\nERROR: \
          \   Exception in ASGI application\n\nTraceback (most recent call last):\n\
          \  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/uvicorn/protocols/websockets/websockets_impl.py\"\
          , line 247, in run_asgi\n    result = await self.app(self.scope, self.asgi_receive,\
          \ self.asgi_send)\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\"\
          , line 84, in __call__\n    return await self.app(scope, receive, send)\n\
          \  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/fastapi/applications.py\"\
          , line 276, in __call__\n    await super().__call__(scope, receive, send)\n\
          \  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/applications.py\"\
          , line 122, in __call__\n    await self.middleware_stack(scope, receive,\
          \ send)\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/middleware/errors.py\"\
          , line 149, in __call__\n    await self.app(scope, receive, send)\n  File\
          \ \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/middleware/cors.py\"\
          , line 75, in __call__\n    await self.app(scope, receive, send)\n  File\
          \ \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/middleware/exceptions.py\"\
          , line 79, in __call__\n    raise exc\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/middleware/exceptions.py\"\
          , line 68, in __call__\n    await self.app(scope, receive, sender)\n  File\
          \ \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\"\
          , line 21, in __call__\n    raise e\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\"\
          , line 18, in __call__\n    await self.app(scope, receive, send)\n  File\
          \ \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/routing.py\"\
          , line 718, in __call__\n    await route.handle(scope, receive, send)\n\
          \  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/routing.py\"\
          , line 341, in handle\n    await self.app(scope, receive, send)\n  File\
          \ \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/routing.py\"\
          , line 82, in app\n    await func(session)\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/fastapi/routing.py\"\
          , line 289, in app\n    await dependant.call(**values)\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/gradio/routes.py\"\
          , line 536, in join_queue\n    session_info = await asyncio.wait_for(\n\
          \  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/asyncio/tasks.py\"\
          , line 445, in wait_for\n    return fut.result()\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/websockets.py\"\
          , line 133, in receive_json\n    self._raise_on_disconnect(message)\n  File\
          \ \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/websockets.py\"\
          , line 105, in _raise_on_disconnect\n    raise WebSocketDisconnect(message[\"\
          code\"])\nstarlette.websockets.WebSocketDisconnect: 1001\n\nbut it continues\
          \ to work by refreshing webpage.\n\nAbout the model itself, i can call it\
          \ the most \"autistic\". Q4_M after first prompt starting to going into\
          \ hallucinating with itself, predicting next user input, writing it and\
          \ self answering to it, so it's writing such self-dialogue long novels about\
          \ any topic. In chat mode it's kinda censored more than in instruct mode\
          \ (decline of medical diagnosing and etc. but not in instruct). Q5_M not\
          \ yet tested but less that self-dialogues, although it's also starting that\
          \ if you click Continue to any ended prompt answer. Leaving Q4_M working\
          \ overnight with max tokens and instruction was a fail, it's produced mostly\
          \ textual garbage.\n\nIt is the most easiest model which you can put into\
          \ \"stasis\" or \"cold processing\" mode (i've seen such only with Llama\
          \ 1 65B Q8 GGML once), it's a phenomena, when neuronet using most system\
          \ resources and alive but not producing high heat, running cold, i see some\
          \ smartness in this because it's avoiding to reach temp limits for turning\
          \ on fans, so it's very quiet. All such cases it's never freezing whole\
          \ operating system with allowing other processes in parallel (like watching\
          \ 4K video). I'm writing this during such mode in Q5_M, it reserved all\
          \ available RAM but working at 20% CPU (rarely crossing 50%) with writing\
          \ answer to prompt. In Llama 1 65B during stasis it's just thinking about\
          \ for hours no one knows what, this Falcon at least writing something during\
          \ that. (there's no parameters to force that and there's no user implemented\
          \ limits, it's putting itself into such mode naturally, also it resetting\
          \ by next prompt/next tokens session)"
        updatedAt: '2023-09-10T16:37:03.885Z'
      numEdits: 1
      reactions: []
    id: 64fdef0cee7f318a25c880c2
    type: comment
  author: krustik
  content: "Yeah, i was managed to run a Q5_Medium model, but barely on limits of\
    \ my system (consumer motherboards can support max 128Gb RAM) only on CPU (GPU\
    \ even offloading produce always crashes).\nMy system is not Windows, where max\
    \ ram is max, in Linux there's swap and etc helping running such even above available\
    \ RAM (i recommend everyone Pika OS which is Ubuntu clone without Ubuntu headache).\n\
    Q4_Medium also tested and it's fine. Q4_M working at 0.40 tokens/sec max, Q5_M\
    \ at 0.36 tokens/sec max registered. (Intel Xeon 14 cores, 28 threads(only 23\
    \ used)\nQ5_Medium produced some webbrowser resetting, when page becoming blank,\
    \ but it's not crashing all and can be restored with refreshing localhost page\
    \ view with all chat history saved (maybe it's problem with lack of RAM or browser).\
    \ In one of such instances it produces error in the logs, this is it:\nERROR:\
    \    Exception in ASGI application\n\nTraceback (most recent call last):\n  File\
    \ \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/uvicorn/protocols/websockets/websockets_impl.py\"\
    , line 247, in run_asgi\n    result = await self.app(self.scope, self.asgi_receive,\
    \ self.asgi_send)\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\"\
    , line 84, in __call__\n    return await self.app(scope, receive, send)\n  File\
    \ \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/fastapi/applications.py\"\
    , line 276, in __call__\n    await super().__call__(scope, receive, send)\n  File\
    \ \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/applications.py\"\
    , line 122, in __call__\n    await self.middleware_stack(scope, receive, send)\n\
    \  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/middleware/errors.py\"\
    , line 149, in __call__\n    await self.app(scope, receive, send)\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/middleware/cors.py\"\
    , line 75, in __call__\n    await self.app(scope, receive, send)\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/middleware/exceptions.py\"\
    , line 79, in __call__\n    raise exc\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/middleware/exceptions.py\"\
    , line 68, in __call__\n    await self.app(scope, receive, sender)\n  File \"\
    /home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\"\
    , line 21, in __call__\n    raise e\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py\"\
    , line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/routing.py\"\
    , line 718, in __call__\n    await route.handle(scope, receive, send)\n  File\
    \ \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/routing.py\"\
    , line 341, in handle\n    await self.app(scope, receive, send)\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/routing.py\"\
    , line 82, in app\n    await func(session)\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/fastapi/routing.py\"\
    , line 289, in app\n    await dependant.call(**values)\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/gradio/routes.py\"\
    , line 536, in join_queue\n    session_info = await asyncio.wait_for(\n  File\
    \ \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/asyncio/tasks.py\"\
    , line 445, in wait_for\n    return fut.result()\n  File \"/home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/websockets.py\"\
    , line 133, in receive_json\n    self._raise_on_disconnect(message)\n  File \"\
    /home/yui/Downloads/oobabooga_linux/installer_files/env/lib/python3.10/site-packages/starlette/websockets.py\"\
    , line 105, in _raise_on_disconnect\n    raise WebSocketDisconnect(message[\"\
    code\"])\nstarlette.websockets.WebSocketDisconnect: 1001\n\nbut it continues to\
    \ work by refreshing webpage.\n\nAbout the model itself, i can call it the most\
    \ \"autistic\". Q4_M after first prompt starting to going into hallucinating with\
    \ itself, predicting next user input, writing it and self answering to it, so\
    \ it's writing such self-dialogue long novels about any topic. In chat mode it's\
    \ kinda censored more than in instruct mode (decline of medical diagnosing and\
    \ etc. but not in instruct). Q5_M not yet tested but less that self-dialogues,\
    \ although it's also starting that if you click Continue to any ended prompt answer.\
    \ Leaving Q4_M working overnight with max tokens and instruction was a fail, it's\
    \ produced mostly textual garbage.\n\nIt is the most easiest model which you can\
    \ put into \"stasis\" or \"cold processing\" mode (i've seen such only with Llama\
    \ 1 65B Q8 GGML once), it's a phenomena, when neuronet using most system resources\
    \ and alive but not producing high heat, running cold, i see some smartness in\
    \ this because it's avoiding to reach temp limits for turning on fans, so it's\
    \ very quiet. All such cases it's never freezing whole operating system with allowing\
    \ other processes in parallel (like watching 4K video). I'm writing this during\
    \ such mode in Q5_M, it reserved all available RAM but working at 20% CPU (rarely\
    \ crossing 50%) with writing answer to prompt. In Llama 1 65B during stasis it's\
    \ just thinking about for hours no one knows what, this Falcon at least writing\
    \ something during that. (there's no parameters to force that and there's no user\
    \ implemented limits, it's putting itself into such mode naturally, also it resetting\
    \ by next prompt/next tokens session)"
  created_at: 2023-09-10 15:30:04+00:00
  edited: true
  hidden: false
  id: 64fdef0cee7f318a25c880c2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/Falcon-180B-Chat-GGUF
repo_type: model
status: open
target_branch: null
title: Some info about RAM.
