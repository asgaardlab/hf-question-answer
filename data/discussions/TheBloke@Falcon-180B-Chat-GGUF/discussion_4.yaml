!!python/object:huggingface_hub.community.DiscussionWithDetails
author: huytungst
conflicting_files: null
created_at: 2023-09-08 03:14:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d64dee2ddaeed2babe88a6003382d068.svg
      fullname: Huy Tung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: huytungst
      type: user
    createdAt: '2023-09-08T04:14:11.000Z'
    data:
      edited: false
      editors:
      - huytungst
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8387553691864014
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d64dee2ddaeed2babe88a6003382d068.svg
          fullname: Huy Tung
          isHf: false
          isPro: false
          name: huytungst
          type: user
        html: '<p>I am planning to build a workstation as follows:</p>

          <ul>

          <li>CPU: AMD 3960x (24 core)</li>

          <li>GPUs: 2x (dual) RTX 3090</li>

          <li>RAM: 128GB</li>

          </ul>

          <p>Can I run inference (or fine-tuning) this model? </p>

          <ul>

          <li>falcon-180b-chat.Q6_K.gguf (Max RAM required 150.02 GB)</li>

          </ul>

          <p>If it''s not possible, I''ll probably have to adjust my budgets and plans.</p>

          <p>Thank you for your great work.</p>

          '
        raw: "I am planning to build a workstation as follows:\r\n- CPU: AMD 3960x\
          \ (24 core)\r\n- GPUs: 2x (dual) RTX 3090\r\n- RAM: 128GB\r\n\r\nCan I run\
          \ inference (or fine-tuning) this model? \r\n- falcon-180b-chat.Q6_K.gguf\
          \ (Max RAM required 150.02 GB)\r\n\r\nIf it's not possible, I'll probably\
          \ have to adjust my budgets and plans.\r\n\r\nThank you for your great work."
        updatedAt: '2023-09-08T04:14:11.544Z'
      numEdits: 0
      reactions: []
    id: 64fa9f93b60e2c9dddf6aa0f
    type: comment
  author: huytungst
  content: "I am planning to build a workstation as follows:\r\n- CPU: AMD 3960x (24\
    \ core)\r\n- GPUs: 2x (dual) RTX 3090\r\n- RAM: 128GB\r\n\r\nCan I run inference\
    \ (or fine-tuning) this model? \r\n- falcon-180b-chat.Q6_K.gguf (Max RAM required\
    \ 150.02 GB)\r\n\r\nIf it's not possible, I'll probably have to adjust my budgets\
    \ and plans.\r\n\r\nThank you for your great work."
  created_at: 2023-09-08 03:14:11+00:00
  edited: false
  hidden: false
  id: 64fa9f93b60e2c9dddf6aa0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-09T18:06:14.000Z'
    data:
      edited: false
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8174340128898621
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<p>Maybe you actually can (who knows) by using this:<br><a rel="nofollow"
          href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a></p>

          '
        raw: 'Maybe you actually can (who knows) by using this:

          https://github.com/huggingface/text-generation-inference'
        updatedAt: '2023-09-09T18:06:14.630Z'
      numEdits: 0
      reactions: []
    id: 64fcb416b3eee10ba5343023
    type: comment
  author: Flanua
  content: 'Maybe you actually can (who knows) by using this:

    https://github.com/huggingface/text-generation-inference'
  created_at: 2023-09-09 17:06:14+00:00
  edited: false
  hidden: false
  id: 64fcb416b3eee10ba5343023
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Falcon-180B-Chat-GGUF
repo_type: model
status: open
target_branch: null
title: Can I run this model with 2X RTX 3090
