!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KusaPinpon
conflicting_files: null
created_at: 2024-01-15 01:55:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d615a8807ef751b05def200f9ea543e.svg
      fullname: Kusa Pinpon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KusaPinpon
      type: user
    createdAt: '2024-01-15T01:55:48.000Z'
    data:
      edited: false
      editors:
      - KusaPinpon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.954355001449585
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d615a8807ef751b05def200f9ea543e.svg
          fullname: Kusa Pinpon
          isHf: false
          isPro: false
          name: KusaPinpon
          type: user
        html: '<p>I used TheBloke_OpenHermes-2.5-Mistral-7B-16k-GPTQ for long texts
          writing before, but it starts hallucinate heavily after around 6k token.<br>Then
          I decided to check all 32k mistral models, they all produced weird results,
          except this one.<br>SciPhi worked for me in several cases with up to 10k
          tokens. I don''t say it''s perfect, but it worth trying. I surprised that
          the less popular model can perform actually better.</p>

          '
        raw: "I used TheBloke_OpenHermes-2.5-Mistral-7B-16k-GPTQ for long texts writing\
          \ before, but it starts hallucinate heavily after around 6k token.\r\nThen\
          \ I decided to check all 32k mistral models, they all produced weird results,\
          \ except this one.\r\nSciPhi worked for me in several cases with up to 10k\
          \ tokens. I don't say it's perfect, but it worth trying. I surprised that\
          \ the less popular model can perform actually better."
        updatedAt: '2024-01-15T01:55:48.030Z'
      numEdits: 0
      reactions: []
    id: 65a490a4c7e6b607c25d0d61
    type: comment
  author: KusaPinpon
  content: "I used TheBloke_OpenHermes-2.5-Mistral-7B-16k-GPTQ for long texts writing\
    \ before, but it starts hallucinate heavily after around 6k token.\r\nThen I decided\
    \ to check all 32k mistral models, they all produced weird results, except this\
    \ one.\r\nSciPhi worked for me in several cases with up to 10k tokens. I don't\
    \ say it's perfect, but it worth trying. I surprised that the less popular model\
    \ can perform actually better."
  created_at: 2024-01-15 01:55:48+00:00
  edited: false
  hidden: false
  id: 65a490a4c7e6b607c25d0d61
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/SciPhi-Mistral-7B-32k-GPTQ
repo_type: model
status: open
target_branch: null
title: It's not perfect but truly capable 10k tokens in my tests
