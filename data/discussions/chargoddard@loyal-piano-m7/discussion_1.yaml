!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil337
conflicting_files: null
created_at: 2023-12-03 03:05:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-03T03:05:34.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9714958667755127
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>I never chat or role play so I have no idea how well this LLM is
          at what it was primarily designed for.</p>

          <p>But at tasks like Q&amp;A, problem solving and story telling it performed
          as well for me as the leading Mistrals, including Zephyr Beta, Open Hermes
          2.5, Dolphin 2.1 and Intel Neural v 3.2.</p>

          <p>And it seemed to process my deliberately long and convoluted story prompts
          better than any other Mistral I tested.</p>

          '
        raw: "I never chat or role play so I have no idea how well this LLM is at\
          \ what it was primarily designed for.\r\n\r\nBut at tasks like Q&A, problem\
          \ solving and story telling it performed as well for me as the leading Mistrals,\
          \ including Zephyr Beta, Open Hermes 2.5, Dolphin 2.1 and Intel Neural v\
          \ 3.2.\r\n\r\nAnd it seemed to process my deliberately long and convoluted\
          \ story prompts better than any other Mistral I tested."
        updatedAt: '2023-12-03T03:05:34.987Z'
      numEdits: 0
      reactions: []
    id: 656bf07e801ed9952f95d453
    type: comment
  author: Phil337
  content: "I never chat or role play so I have no idea how well this LLM is at what\
    \ it was primarily designed for.\r\n\r\nBut at tasks like Q&A, problem solving\
    \ and story telling it performed as well for me as the leading Mistrals, including\
    \ Zephyr Beta, Open Hermes 2.5, Dolphin 2.1 and Intel Neural v 3.2.\r\n\r\nAnd\
    \ it seemed to process my deliberately long and convoluted story prompts better\
    \ than any other Mistral I tested."
  created_at: 2023-12-03 03:05:34+00:00
  edited: false
  hidden: false
  id: 656bf07e801ed9952f95d453
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png?w=200&h=200&f=face
      fullname: Charles Goddard
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: chargoddard
      type: user
    createdAt: '2023-12-03T20:45:10.000Z'
    data:
      edited: false
      editors:
      - chargoddard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9668915271759033
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png?w=200&h=200&f=face
          fullname: Charles Goddard
          isHf: false
          isPro: false
          name: chargoddard
          type: user
        html: '<p>Glad to hear it''s working well for you! I''m surprised at how well
          it turned out, particularly given the amount of PIPPA in there vs. actual
          instruction examples.</p>

          <p>Thanks for giving it a shot.</p>

          '
        raw: 'Glad to hear it''s working well for you! I''m surprised at how well
          it turned out, particularly given the amount of PIPPA in there vs. actual
          instruction examples.


          Thanks for giving it a shot.'
        updatedAt: '2023-12-03T20:45:10.267Z'
      numEdits: 0
      reactions: []
    id: 656ce8d6d848a6683af10de1
    type: comment
  author: chargoddard
  content: 'Glad to hear it''s working well for you! I''m surprised at how well it
    turned out, particularly given the amount of PIPPA in there vs. actual instruction
    examples.


    Thanks for giving it a shot.'
  created_at: 2023-12-03 20:45:10+00:00
  edited: false
  hidden: false
  id: 656ce8d6d848a6683af10de1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb985528d485627eb20844721fc3effb.svg
      fullname: Kara
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nailcan
      type: user
    createdAt: '2023-12-03T21:05:55.000Z'
    data:
      edited: true
      editors:
      - Nailcan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6952585577964783
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb985528d485627eb20844721fc3effb.svg
          fullname: Kara
          isHf: false
          isPro: false
          name: Nailcan
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;chargoddard&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/chargoddard\"\
          >@<span class=\"underline\">chargoddard</span></a></span>\n\n\t</span></span>,<br>Could\
          \ you provide prompt template for chat?</p>\n<p>Also DROP score was removed\
          \ by hf : <a href=\"https://huggingface.co/blog/leaderboard-drop-dive\"\
          >https://huggingface.co/blog/leaderboard-drop-dive</a><br>Your model now\
          \ has the highest score in 7B models!</p>\n"
        raw: 'Hi @chargoddard,

          Could you provide prompt template for chat?


          Also DROP score was removed by hf : https://huggingface.co/blog/leaderboard-drop-dive

          Your model now has the highest score in 7B models!'
        updatedAt: '2023-12-03T21:08:51.431Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pribadihcr
    id: 656cedb36f39f156583fc448
    type: comment
  author: Nailcan
  content: 'Hi @chargoddard,

    Could you provide prompt template for chat?


    Also DROP score was removed by hf : https://huggingface.co/blog/leaderboard-drop-dive

    Your model now has the highest score in 7B models!'
  created_at: 2023-12-03 21:05:55+00:00
  edited: true
  hidden: false
  id: 656cedb36f39f156583fc448
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-03T21:15:43.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9589943885803223
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>I''m just a user who tests LLMs as a hobby and know very little
          about fine-tuning or how LLMs work.</p>

          <p>But I am starting to pick up on a pattern that when people try to use
          too many step-by-step instruction examples and prompt pairs from teachers
          like GPT4 the student LLMs become frustratingly "stubborn". That is, they
          start ignoring details in the user''s prompt to say something it thinks
          is best, but in most cases isn''t true or reasonable.</p>

          <p>For example, in the case of story telling, it will make a character use
          a camera phone even though I explicitly said camcorder for a reason (it
          was the early 90s and there were no camera phones). Or if I said he caught
          stealing something like money from a counter the LLM (programmed to ad suspense)
          would start saying things like ''he heard footsteps coming down the hall''
          before being caught grabbing the money red handed (people obviously don''t
          grab the money and act surprised about being caught if they hear footsteps
          first). In short, the training data starts forcing absurd mistakes that
          blatantly contradict the user prompt, or even what the LLM already wrote.</p>

          <p>You may be on to something about finding a balance, adding just enough
          diverse step-by-step and prompt pairs from teacher to student in order to
          make smaller LLMs more "intelligent", but not enough to make them stubborn
          and ignore details in the user prompt and what was already stated in the
          response in favor of pre-packaged elements like always using camera phones,
          hearing footsteps to build suspense...</p>

          '
        raw: 'I''m just a user who tests LLMs as a hobby and know very little about
          fine-tuning or how LLMs work.


          But I am starting to pick up on a pattern that when people try to use too
          many step-by-step instruction examples and prompt pairs from teachers like
          GPT4 the student LLMs become frustratingly "stubborn". That is, they start
          ignoring details in the user''s prompt to say something it thinks is best,
          but in most cases isn''t true or reasonable.


          For example, in the case of story telling, it will make a character use
          a camera phone even though I explicitly said camcorder for a reason (it
          was the early 90s and there were no camera phones). Or if I said he caught
          stealing something like money from a counter the LLM (programmed to ad suspense)
          would start saying things like ''he heard footsteps coming down the hall''
          before being caught grabbing the money red handed (people obviously don''t
          grab the money and act surprised about being caught if they hear footsteps
          first). In short, the training data starts forcing absurd mistakes that
          blatantly contradict the user prompt, or even what the LLM already wrote.


          You may be on to something about finding a balance, adding just enough diverse
          step-by-step and prompt pairs from teacher to student in order to make smaller
          LLMs more "intelligent", but not enough to make them stubborn and ignore
          details in the user prompt and what was already stated in the response in
          favor of pre-packaged elements like always using camera phones, hearing
          footsteps to build suspense...


          '
        updatedAt: '2023-12-03T21:15:43.588Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - chargoddard
    id: 656cefffe0ff1cebe97ee622
    type: comment
  author: Phil337
  content: 'I''m just a user who tests LLMs as a hobby and know very little about
    fine-tuning or how LLMs work.


    But I am starting to pick up on a pattern that when people try to use too many
    step-by-step instruction examples and prompt pairs from teachers like GPT4 the
    student LLMs become frustratingly "stubborn". That is, they start ignoring details
    in the user''s prompt to say something it thinks is best, but in most cases isn''t
    true or reasonable.


    For example, in the case of story telling, it will make a character use a camera
    phone even though I explicitly said camcorder for a reason (it was the early 90s
    and there were no camera phones). Or if I said he caught stealing something like
    money from a counter the LLM (programmed to ad suspense) would start saying things
    like ''he heard footsteps coming down the hall'' before being caught grabbing
    the money red handed (people obviously don''t grab the money and act surprised
    about being caught if they hear footsteps first). In short, the training data
    starts forcing absurd mistakes that blatantly contradict the user prompt, or even
    what the LLM already wrote.


    You may be on to something about finding a balance, adding just enough diverse
    step-by-step and prompt pairs from teacher to student in order to make smaller
    LLMs more "intelligent", but not enough to make them stubborn and ignore details
    in the user prompt and what was already stated in the response in favor of pre-packaged
    elements like always using camera phones, hearing footsteps to build suspense...


    '
  created_at: 2023-12-03 21:15:43+00:00
  edited: false
  hidden: false
  id: 656cefffe0ff1cebe97ee622
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png?w=200&h=200&f=face
      fullname: Charles Goddard
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: chargoddard
      type: user
    createdAt: '2023-12-04T20:39:31.000Z'
    data:
      edited: false
      editors:
      - chargoddard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6809443831443787
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png?w=200&h=200&f=face
          fullname: Charles Goddard
          isHf: false
          isPro: false
          name: chargoddard
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Nailcan&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Nailcan\">@<span class=\"\
          underline\">Nailcan</span></a></span>\n\n\t</span></span><br>Here's the\
          \ format I used for the roleplay training data:</p>\n<pre><code>### Instruction:\n\
          \nEnter roleplay mode. You are {character name}.\n\n{character bio goes\
          \ here}\n\nExample session <a href=\"/chargoddard/loyal-piano-m7/discussions/1\"\
          >#1</a>:\n```\n{character name}: words words ...\nUser: a reply\n```\n\n\
          Example session <a href=\"/chargoddard/loyal-piano-m7/discussions/2\">#2</a>:\n\
          ```\n...\n```\n\n### Input:\n{character name}: words words words\n{user\
          \ name}: words words more words\n{character name}: ...\n{user name}: ...\n\
          \n### Response:\n{character name}:\n</code></pre>\n"
        raw: "@Nailcan \nHere's the format I used for the roleplay training data:\n\
          \n````\n### Instruction:\n\nEnter roleplay mode. You are {character name}.\n\
          \n{character bio goes here}\n\nExample session #1:\n```\n{character name}:\
          \ words words ...\nUser: a reply\n```\n\nExample session #2:\n```\n...\n\
          ```\n\n### Input:\n{character name}: words words words\n{user name}: words\
          \ words more words\n{character name}: ...\n{user name}: ...\n\n### Response:\n\
          {character name}:\n````\n"
        updatedAt: '2023-12-04T20:39:31.834Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Wisdawn
    id: 656e390375d92cf286d8ee29
    type: comment
  author: chargoddard
  content: "@Nailcan \nHere's the format I used for the roleplay training data:\n\n\
    ````\n### Instruction:\n\nEnter roleplay mode. You are {character name}.\n\n{character\
    \ bio goes here}\n\nExample session #1:\n```\n{character name}: words words ...\n\
    User: a reply\n```\n\nExample session #2:\n```\n...\n```\n\n### Input:\n{character\
    \ name}: words words words\n{user name}: words words more words\n{character name}:\
    \ ...\n{user name}: ...\n\n### Response:\n{character name}:\n````\n"
  created_at: 2023-12-04 20:39:31+00:00
  edited: false
  hidden: false
  id: 656e390375d92cf286d8ee29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/wVUbcSysTa9KRoheKNWR4.jpeg?w=200&h=200&f=face
      fullname: Thilo E.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThiloteE
      type: user
    createdAt: '2024-01-07T02:01:07.000Z'
    data:
      edited: false
      editors:
      - ThiloteE
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9400070309638977
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/wVUbcSysTa9KRoheKNWR4.jpeg?w=200&h=200&f=face
          fullname: Thilo E.
          isHf: false
          isPro: false
          name: ThiloteE
          type: user
        html: '<p>This model is a romantic :D</p>

          '
        raw: This model is a romantic :D
        updatedAt: '2024-01-07T02:01:07.482Z'
      numEdits: 0
      reactions: []
    id: 659a05e3b2ec894c51c32974
    type: comment
  author: ThiloteE
  content: This model is a romantic :D
  created_at: 2024-01-07 02:01:07+00:00
  edited: false
  hidden: false
  id: 659a05e3b2ec894c51c32974
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: chargoddard/loyal-piano-m7
repo_type: model
status: open
target_branch: null
title: This LLM is good at a lot of things.
