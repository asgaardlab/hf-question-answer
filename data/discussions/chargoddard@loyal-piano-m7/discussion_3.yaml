!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Overbite1741
conflicting_files: null
created_at: 2023-12-09 21:25:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c75d4f4e9a44b544c7807fc8fa78f62.svg
      fullname: Vish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Overbite1741
      type: user
    createdAt: '2023-12-09T21:25:16.000Z'
    data:
      edited: false
      editors:
      - Overbite1741
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9220989346504211
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c75d4f4e9a44b544c7807fc8fa78f62.svg
          fullname: Vish
          isHf: false
          isPro: false
          name: Overbite1741
          type: user
        html: '<p>It''s truly unbelievable how training on a such a small and diverse
          dataset could give such a good model. I think this deserves a deeper look
          on why this dataset mixture surpassed 100s of other finetunes.</p>

          <p>I am working on reproducing this model and then do some ablative experiment.
          Is it possible for you to share the axolotl config or more details about
          the training? Also did you start from base mistral or some other finetune?</p>

          '
        raw: "It's truly unbelievable how training on a such a small and diverse dataset\
          \ could give such a good model. I think this deserves a deeper look on why\
          \ this dataset mixture surpassed 100s of other finetunes.\r\n\r\nI am working\
          \ on reproducing this model and then do some ablative experiment. Is it\
          \ possible for you to share the axolotl config or more details about the\
          \ training? Also did you start from base mistral or some other finetune?"
        updatedAt: '2023-12-09T21:25:16.451Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Phil337
        - TravelingMan
    id: 6574db3c1488186315ac811c
    type: comment
  author: Overbite1741
  content: "It's truly unbelievable how training on a such a small and diverse dataset\
    \ could give such a good model. I think this deserves a deeper look on why this\
    \ dataset mixture surpassed 100s of other finetunes.\r\n\r\nI am working on reproducing\
    \ this model and then do some ablative experiment. Is it possible for you to share\
    \ the axolotl config or more details about the training? Also did you start from\
    \ base mistral or some other finetune?"
  created_at: 2023-12-09 21:25:16+00:00
  edited: false
  hidden: false
  id: 6574db3c1488186315ac811c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png?w=200&h=200&f=face
      fullname: Charles Goddard
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: chargoddard
      type: user
    createdAt: '2023-12-10T01:52:46.000Z'
    data:
      edited: false
      editors:
      - chargoddard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.60565584897995
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png?w=200&h=200&f=face
          fullname: Charles Goddard
          isHf: false
          isPro: false
          name: chargoddard
          type: user
        html: "<p>Thanks for your interest! I'm happy to share how I produced it -\
          \ I'd love to get to the bottom of what made it work so well.</p>\n<p>Here's\
          \ the axolotl config I used:</p>\n<pre><code class=\"language-yml\"><span\
          \ class=\"hljs-attr\">base_model:</span> <span class=\"hljs-string\">mistralai/Mistral-7B-v0.1</span>\n\
          <span class=\"hljs-attr\">model_type:</span> <span class=\"hljs-string\"\
          >MistralForCausalLM</span>\n<span class=\"hljs-attr\">tokenizer_type:</span>\
          \ <span class=\"hljs-string\">LlamaTokenizer</span>\n<span class=\"hljs-attr\"\
          >is_mistral_derived_model:</span> <span class=\"hljs-literal\">true</span>\n\
          \n<span class=\"hljs-attr\">load_in_8bit:</span> <span class=\"hljs-literal\"\
          >false</span>\n<span class=\"hljs-attr\">load_in_4bit:</span> <span class=\"\
          hljs-literal\">false</span>\n<span class=\"hljs-attr\">strict:</span> <span\
          \ class=\"hljs-literal\">false</span>\n\n<span class=\"hljs-attr\">datasets:</span>\n\
          \  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">path:</span>\
          \ <span class=\"hljs-string\">chargoddard/PIPPA-Judged</span>\n    <span\
          \ class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">adequately_rated</span>\n\
          \    <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\"\
          >pippa</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\"\
          >path:</span> <span class=\"hljs-string\">chargoddard/rpguild</span>\n \
          \   <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">pruned</span>\n\
          \    <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\"\
          >rp_forum</span>\n    <span class=\"hljs-attr\">shards:</span> <span class=\"\
          hljs-number\">20</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"\
          hljs-attr\">path:</span> <span class=\"hljs-string\">pankajmathur/orca_mini_v1_dataset</span>\n\
          \    <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\"\
          >orca_mini</span>\n    <span class=\"hljs-attr\">shards:</span> <span class=\"\
          hljs-number\">10</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"\
          hljs-attr\">path:</span> <span class=\"hljs-string\">chargoddard/summarize_from_feedback_alpaca</span>\n\
          \    <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\"\
          >alpaca</span>\n    <span class=\"hljs-attr\">shards:</span> <span class=\"\
          hljs-number\">20</span>\n  <span class=\"hljs-bullet\">-</span> <span class=\"\
          hljs-attr\">path:</span> <span class=\"hljs-string\">json</span>\n    <span\
          \ class=\"hljs-attr\">data_files:</span> <span class=\"hljs-string\">/workspace/limaerp-8192.jsonl</span>\n\
          \    <span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\"\
          >rp_forum</span>\n<span class=\"hljs-attr\">prompt_format:</span> <span\
          \ class=\"hljs-string\">rpinstruct</span>\n<span class=\"hljs-attr\">dataset_prepared_path:</span>\
          \ <span class=\"hljs-string\">last_run_prepared</span>\n<span class=\"hljs-attr\"\
          >val_set_size:</span> <span class=\"hljs-number\">0.05</span>\n<span class=\"\
          hljs-attr\">output_dir:</span> <span class=\"hljs-string\">./mistral-rp-out</span>\n\
          <span class=\"hljs-attr\">save_safetensors:</span> <span class=\"hljs-literal\"\
          >true</span>\n\n<span class=\"hljs-attr\">adapter:</span> <span class=\"\
          hljs-string\">lora</span>\n<span class=\"hljs-attr\">lora_model_dir:</span>\n\
          \n<span class=\"hljs-attr\">sequence_len:</span> <span class=\"hljs-number\"\
          >8192</span>\n<span class=\"hljs-attr\">sample_packing:</span> <span class=\"\
          hljs-literal\">true</span>\n<span class=\"hljs-attr\">pad_to_sequence_len:</span>\
          \ <span class=\"hljs-literal\">true</span>\n\n<span class=\"hljs-attr\"\
          >total_num_tokens:</span> <span class=\"hljs-number\">30637024</span>\n\
          <span class=\"hljs-attr\">sample_packing_eff_est:</span> <span class=\"\
          hljs-number\">0.98</span>\n\n<span class=\"hljs-attr\">lora_r:</span> <span\
          \ class=\"hljs-number\">64</span>\n<span class=\"hljs-attr\">lora_alpha:</span>\
          \ <span class=\"hljs-number\">128</span>\n<span class=\"hljs-attr\">lora_dropout:</span>\
          \ <span class=\"hljs-number\">0.05</span>\n<span class=\"hljs-attr\">lora_target_linear:</span>\
          \ <span class=\"hljs-literal\">true</span>\n<span class=\"hljs-attr\">lora_fan_in_fan_out:</span>\n\
          <span class=\"hljs-attr\">lora_target_modules:</span>\n  <span class=\"\
          hljs-bullet\">-</span> <span class=\"hljs-string\">gate_proj</span>\n  <span\
          \ class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">down_proj</span>\n\
          \  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">up_proj</span>\n\
          \  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">q_proj</span>\n\
          \  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">v_proj</span>\n\
          \  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">k_proj</span>\n\
          \  <span class=\"hljs-bullet\">-</span> <span class=\"hljs-string\">o_proj</span>\n\
          \n<span class=\"hljs-attr\">wandb_project:</span> <span class=\"hljs-string\"\
          >mistral-rp</span>\n<span class=\"hljs-attr\">wandb_entity:</span>\n<span\
          \ class=\"hljs-attr\">wandb_watch:</span>\n<span class=\"hljs-attr\">wandb_run_id:</span>\n\
          <span class=\"hljs-attr\">wandb_log_model:</span>\n\n<span class=\"hljs-attr\"\
          >gradient_accumulation_steps:</span> <span class=\"hljs-number\">4</span>\n\
          <span class=\"hljs-attr\">micro_batch_size:</span> <span class=\"hljs-number\"\
          >4</span>\n<span class=\"hljs-attr\">eval_batch_size:</span> <span class=\"\
          hljs-number\">4</span>\n<span class=\"hljs-attr\">num_epochs:</span> <span\
          \ class=\"hljs-number\">1</span>\n<span class=\"hljs-attr\">optimizer:</span>\
          \ <span class=\"hljs-string\">adamw_bnb_8bit</span>\n<span class=\"hljs-attr\"\
          >lr_scheduler:</span> <span class=\"hljs-string\">cosine</span>\n<span class=\"\
          hljs-attr\">learning_rate:</span> <span class=\"hljs-number\">0.0002</span>\n\
          \n<span class=\"hljs-attr\">train_on_inputs:</span> <span class=\"hljs-literal\"\
          >false</span>\n<span class=\"hljs-attr\">group_by_length:</span> <span class=\"\
          hljs-literal\">false</span>\n<span class=\"hljs-attr\">bf16:</span> <span\
          \ class=\"hljs-literal\">true</span>\n<span class=\"hljs-attr\">fp16:</span>\
          \ <span class=\"hljs-literal\">false</span>\n<span class=\"hljs-attr\">tf32:</span>\
          \ <span class=\"hljs-literal\">false</span>\n\n<span class=\"hljs-attr\"\
          >gradient_checkpointing:</span> <span class=\"hljs-literal\">true</span>\n\
          <span class=\"hljs-attr\">logging_steps:</span> <span class=\"hljs-number\"\
          >1</span>\n<span class=\"hljs-attr\">flash_attention:</span> <span class=\"\
          hljs-literal\">true</span>\n\n<span class=\"hljs-attr\">warmup_steps:</span>\
          \ <span class=\"hljs-number\">10</span>\n<span class=\"hljs-attr\">eval_steps:</span>\
          \ <span class=\"hljs-number\">0.05</span>\n<span class=\"hljs-attr\">save_steps:</span>\
          \ <span class=\"hljs-number\">0.05</span>\n<span class=\"hljs-attr\">weight_decay:</span>\
          \ <span class=\"hljs-number\">0.0</span>\n<span class=\"hljs-attr\">special_tokens:</span>\n\
          \  <span class=\"hljs-attr\">bos_token:</span> <span class=\"hljs-string\"\
          >\"&lt;s&gt;\"</span>\n  <span class=\"hljs-attr\">eos_token:</span> <span\
          \ class=\"hljs-string\">\"&lt;/s&gt;\"</span>\n  <span class=\"hljs-attr\"\
          >unk_token:</span> <span class=\"hljs-string\">\"&lt;unk&gt;\"</span>\n\
          </code></pre>\n<p>It does look like I goofed a bit on the dataset split\
          \ table - the proportion of summarize_from_feedback used is even lower than\
          \ listed.</p>\n<p>The <code>limaerp-8192.jsonl</code> mentioned is just\
          \ a lightly preprocessed version of lemonilia's LimaRP dataset. I would\
          \ upload it to huggingface but it's, uh, way too spicy for my tastes. You\
          \ can download it here: <a rel=\"nofollow\" href=\"https://files.catbox.moe/jj9srp.jsonl\"\
          >https://files.catbox.moe/jj9srp.jsonl</a></p>\n<p>I used my fork of axolotl\
          \ with custom prompt handling. Specifically <a rel=\"nofollow\" href=\"\
          https://github.com/cg123/axolotl/tree/84e2a389ccd230620f56747675a3097cd6b79143\"\
          >this commit</a> was used to train the model. The way <code>train_on_inputs</code>,\
          \ labels, and EOS tokkens are handled is different so it won't reproduce\
          \ exactly on mainline axolotl. I could probably throw a pre-tokenized version\
          \ of the dataset up if that's useful though.</p>\n"
        raw: "Thanks for your interest! I'm happy to share how I produced it - I'd\
          \ love to get to the bottom of what made it work so well.\n\nHere's the\
          \ axolotl config I used:\n```yml\nbase_model: mistralai/Mistral-7B-v0.1\n\
          model_type: MistralForCausalLM\ntokenizer_type: LlamaTokenizer\nis_mistral_derived_model:\
          \ true\n\nload_in_8bit: false\nload_in_4bit: false\nstrict: false\n\ndatasets:\n\
          \  - path: chargoddard/PIPPA-Judged\n    name: adequately_rated\n    type:\
          \ pippa\n  - path: chargoddard/rpguild\n    name: pruned\n    type: rp_forum\n\
          \    shards: 20\n  - path: pankajmathur/orca_mini_v1_dataset\n    type:\
          \ orca_mini\n    shards: 10\n  - path: chargoddard/summarize_from_feedback_alpaca\n\
          \    type: alpaca\n    shards: 20\n  - path: json\n    data_files: /workspace/limaerp-8192.jsonl\n\
          \    type: rp_forum\nprompt_format: rpinstruct\ndataset_prepared_path: last_run_prepared\n\
          val_set_size: 0.05\noutput_dir: ./mistral-rp-out\nsave_safetensors: true\n\
          \nadapter: lora\nlora_model_dir:\n\nsequence_len: 8192\nsample_packing:\
          \ true\npad_to_sequence_len: true\n\ntotal_num_tokens: 30637024\nsample_packing_eff_est:\
          \ 0.98\n\nlora_r: 64\nlora_alpha: 128\nlora_dropout: 0.05\nlora_target_linear:\
          \ true\nlora_fan_in_fan_out:\nlora_target_modules:\n  - gate_proj\n  - down_proj\n\
          \  - up_proj\n  - q_proj\n  - v_proj\n  - k_proj\n  - o_proj\n\nwandb_project:\
          \ mistral-rp\nwandb_entity:\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\n\
          \ngradient_accumulation_steps: 4\nmicro_batch_size: 4\neval_batch_size:\
          \ 4\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate:\
          \ 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: true\n\
          fp16: false\ntf32: false\n\ngradient_checkpointing: true\nlogging_steps:\
          \ 1\nflash_attention: true\n\nwarmup_steps: 10\neval_steps: 0.05\nsave_steps:\
          \ 0.05\nweight_decay: 0.0\nspecial_tokens:\n  bos_token: \"<s>\"\n  eos_token:\
          \ \"</s>\"\n  unk_token: \"<unk>\"\n```\n\nIt does look like I goofed a\
          \ bit on the dataset split table - the proportion of summarize_from_feedback\
          \ used is even lower than listed.\n\nThe `limaerp-8192.jsonl` mentioned\
          \ is just a lightly preprocessed version of lemonilia's LimaRP dataset.\
          \ I would upload it to huggingface but it's, uh, way too spicy for my tastes.\
          \ You can download it here: https://files.catbox.moe/jj9srp.jsonl\n\nI used\
          \ my fork of axolotl with custom prompt handling. Specifically [this commit](https://github.com/cg123/axolotl/tree/84e2a389ccd230620f56747675a3097cd6b79143)\
          \ was used to train the model. The way `train_on_inputs`, labels, and EOS\
          \ tokkens are handled is different so it won't reproduce exactly on mainline\
          \ axolotl. I could probably throw a pre-tokenized version of the dataset\
          \ up if that's useful though."
        updatedAt: '2023-12-10T01:52:46.685Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - Phil337
        - Overbite1741
        - TravelingMan
        - sevenreasons
    id: 657519ee5df6f1fec1b25c8e
    type: comment
  author: chargoddard
  content: "Thanks for your interest! I'm happy to share how I produced it - I'd love\
    \ to get to the bottom of what made it work so well.\n\nHere's the axolotl config\
    \ I used:\n```yml\nbase_model: mistralai/Mistral-7B-v0.1\nmodel_type: MistralForCausalLM\n\
    tokenizer_type: LlamaTokenizer\nis_mistral_derived_model: true\n\nload_in_8bit:\
    \ false\nload_in_4bit: false\nstrict: false\n\ndatasets:\n  - path: chargoddard/PIPPA-Judged\n\
    \    name: adequately_rated\n    type: pippa\n  - path: chargoddard/rpguild\n\
    \    name: pruned\n    type: rp_forum\n    shards: 20\n  - path: pankajmathur/orca_mini_v1_dataset\n\
    \    type: orca_mini\n    shards: 10\n  - path: chargoddard/summarize_from_feedback_alpaca\n\
    \    type: alpaca\n    shards: 20\n  - path: json\n    data_files: /workspace/limaerp-8192.jsonl\n\
    \    type: rp_forum\nprompt_format: rpinstruct\ndataset_prepared_path: last_run_prepared\n\
    val_set_size: 0.05\noutput_dir: ./mistral-rp-out\nsave_safetensors: true\n\nadapter:\
    \ lora\nlora_model_dir:\n\nsequence_len: 8192\nsample_packing: true\npad_to_sequence_len:\
    \ true\n\ntotal_num_tokens: 30637024\nsample_packing_eff_est: 0.98\n\nlora_r:\
    \ 64\nlora_alpha: 128\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\n\
    lora_target_modules:\n  - gate_proj\n  - down_proj\n  - up_proj\n  - q_proj\n\
    \  - v_proj\n  - k_proj\n  - o_proj\n\nwandb_project: mistral-rp\nwandb_entity:\n\
    wandb_watch:\nwandb_run_id:\nwandb_log_model:\n\ngradient_accumulation_steps:\
    \ 4\nmicro_batch_size: 4\neval_batch_size: 4\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\n\
    lr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length:\
    \ false\nbf16: true\nfp16: false\ntf32: false\n\ngradient_checkpointing: true\n\
    logging_steps: 1\nflash_attention: true\n\nwarmup_steps: 10\neval_steps: 0.05\n\
    save_steps: 0.05\nweight_decay: 0.0\nspecial_tokens:\n  bos_token: \"<s>\"\n \
    \ eos_token: \"</s>\"\n  unk_token: \"<unk>\"\n```\n\nIt does look like I goofed\
    \ a bit on the dataset split table - the proportion of summarize_from_feedback\
    \ used is even lower than listed.\n\nThe `limaerp-8192.jsonl` mentioned is just\
    \ a lightly preprocessed version of lemonilia's LimaRP dataset. I would upload\
    \ it to huggingface but it's, uh, way too spicy for my tastes. You can download\
    \ it here: https://files.catbox.moe/jj9srp.jsonl\n\nI used my fork of axolotl\
    \ with custom prompt handling. Specifically [this commit](https://github.com/cg123/axolotl/tree/84e2a389ccd230620f56747675a3097cd6b79143)\
    \ was used to train the model. The way `train_on_inputs`, labels, and EOS tokkens\
    \ are handled is different so it won't reproduce exactly on mainline axolotl.\
    \ I could probably throw a pre-tokenized version of the dataset up if that's useful\
    \ though."
  created_at: 2023-12-10 01:52:46+00:00
  edited: false
  hidden: false
  id: 657519ee5df6f1fec1b25c8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c75d4f4e9a44b544c7807fc8fa78f62.svg
      fullname: Vish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Overbite1741
      type: user
    createdAt: '2023-12-10T10:47:23.000Z'
    data:
      edited: false
      editors:
      - Overbite1741
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8983421325683594
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c75d4f4e9a44b544c7807fc8fa78f62.svg
          fullname: Vish
          isHf: false
          isPro: false
          name: Overbite1741
          type: user
        html: '<p>Thanks. Will post the results of my ablative experiments here once
          I run them. Also I am using <code>feature/rp</code> branch of <code>https://github.com/cg123/rathe/</code>.
          Is that correct?</p>

          '
        raw: Thanks. Will post the results of my ablative experiments here once I
          run them. Also I am using `feature/rp` branch of `https://github.com/cg123/rathe/`.
          Is that correct?
        updatedAt: '2023-12-10T10:47:23.975Z'
      numEdits: 0
      reactions: []
    id: 6575973bd7f487de5ff27ce2
    type: comment
  author: Overbite1741
  content: Thanks. Will post the results of my ablative experiments here once I run
    them. Also I am using `feature/rp` branch of `https://github.com/cg123/rathe/`.
    Is that correct?
  created_at: 2023-12-10 10:47:23+00:00
  edited: false
  hidden: false
  id: 6575973bd7f487de5ff27ce2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c75d4f4e9a44b544c7807fc8fa78f62.svg
      fullname: Vish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Overbite1741
      type: user
    createdAt: '2023-12-10T16:05:28.000Z'
    data:
      edited: false
      editors:
      - Overbite1741
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8232056498527527
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c75d4f4e9a44b544c7807fc8fa78f62.svg
          fullname: Vish
          isHf: false
          isPro: false
          name: Overbite1741
          type: user
        html: '<p>I am not able to reproduce the results. I checked out the commit
          ID you mentioned and installed <code>rathe</code> from the given branch
          and used the same axolotl config. Evaluation code is same for both your
          and reproduced model. </p>

          <p>Your model benchmarks:</p>

          <p>ARC: 66.72<br>Truthful: 59.86<br>winogrande: 79.16</p>

          <p>Reproduced model benchmarks:</p>

          <p>ARC: 61.00<br>truthful: 43.00<br>winogrande: 78.53</p>

          '
        raw: "I am not able to reproduce the results. I checked out the commit ID\
          \ you mentioned and installed `rathe` from the given branch and used the\
          \ same axolotl config. Evaluation code is same for both your and reproduced\
          \ model. \n\nYour model benchmarks:\n\nARC: 66.72\nTruthful: 59.86\nwinogrande:\
          \ 79.16\n\nReproduced model benchmarks:\n\nARC: 61.00\ntruthful: 43.00\n\
          winogrande: 78.53"
        updatedAt: '2023-12-10T16:05:28.464Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6575e1c8fc88f842e30fc326
    id: 6575e1c8fc88f842e30fc322
    type: comment
  author: Overbite1741
  content: "I am not able to reproduce the results. I checked out the commit ID you\
    \ mentioned and installed `rathe` from the given branch and used the same axolotl\
    \ config. Evaluation code is same for both your and reproduced model. \n\nYour\
    \ model benchmarks:\n\nARC: 66.72\nTruthful: 59.86\nwinogrande: 79.16\n\nReproduced\
    \ model benchmarks:\n\nARC: 61.00\ntruthful: 43.00\nwinogrande: 78.53"
  created_at: 2023-12-10 16:05:28+00:00
  edited: false
  hidden: false
  id: 6575e1c8fc88f842e30fc322
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7c75d4f4e9a44b544c7807fc8fa78f62.svg
      fullname: Vish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Overbite1741
      type: user
    createdAt: '2023-12-10T16:05:28.000Z'
    data:
      status: closed
    id: 6575e1c8fc88f842e30fc326
    type: status-change
  author: Overbite1741
  created_at: 2023-12-10 16:05:28+00:00
  id: 6575e1c8fc88f842e30fc326
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7c75d4f4e9a44b544c7807fc8fa78f62.svg
      fullname: Vish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Overbite1741
      type: user
    createdAt: '2023-12-10T16:08:44.000Z'
    data:
      status: open
    id: 6575e28c839aa08899331b0f
    type: status-change
  author: Overbite1741
  created_at: 2023-12-10 16:08:44+00:00
  id: 6575e28c839aa08899331b0f
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: chargoddard/loyal-piano-m7
repo_type: model
status: open
target_branch: null
title: This is incredible
