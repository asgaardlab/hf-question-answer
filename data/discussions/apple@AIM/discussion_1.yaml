!!python/object:huggingface_hub.community.DiscussionWithDetails
author: PapersAnon
conflicting_files: null
created_at: 2024-01-18 07:26:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/baf5ed9f881f58fc3ee584f5f6091b35.svg
      fullname: nanashi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PapersAnon
      type: user
    createdAt: '2024-01-18T07:26:31.000Z'
    data:
      edited: false
      editors:
      - PapersAnon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8832620978355408
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/baf5ed9f881f58fc3ee584f5f6091b35.svg
          fullname: nanashi
          isHf: false
          isPro: false
          name: PapersAnon
          type: user
        html: '<p>Scalable Pre-training of Large Autoregressive Image Models<br><a
          rel="nofollow" href="https://arxiv.org/abs/2401.08541">https://arxiv.org/abs/2401.08541</a></p>

          <blockquote>

          <p>This paper introduces AIM, a collection of vision models pre-trained
          with an autoregressive objective. These models are inspired by their textual
          counterparts, i.e., Large Language Models (LLMs), and exhibit similar scaling
          properties. Specifically, we highlight two key findings: (1) the performance
          of the visual features scale with both the model capacity and the quantity
          of data, (2) the value of the objective function correlates with the performance
          of the model on downstream tasks. We illustrate the practical implication
          of these findings by pre-training a 7 billion parameter AIM on 2 billion
          images, that achieves 84.0% on ImageNet-1k with a frozen trunk. Interestingly,
          even at this scale, we observe no sign of saturation in performance, suggesting
          that AIM potentially represents a new frontier for training large-scale
          vision models. The pre-training of AIM is similar to the pre-training of
          LLMs, and does not require any image-specific strategy to stabilize the
          training at scale.</p>

          </blockquote>

          '
        raw: "Scalable Pre-training of Large Autoregressive Image Models\r\nhttps://arxiv.org/abs/2401.08541\r\
          \n>This paper introduces AIM, a collection of vision models pre-trained\
          \ with an autoregressive objective. These models are inspired by their textual\
          \ counterparts, i.e., Large Language Models (LLMs), and exhibit similar\
          \ scaling properties. Specifically, we highlight two key findings: (1) the\
          \ performance of the visual features scale with both the model capacity\
          \ and the quantity of data, (2) the value of the objective function correlates\
          \ with the performance of the model on downstream tasks. We illustrate the\
          \ practical implication of these findings by pre-training a 7 billion parameter\
          \ AIM on 2 billion images, that achieves 84.0% on ImageNet-1k with a frozen\
          \ trunk. Interestingly, even at this scale, we observe no sign of saturation\
          \ in performance, suggesting that AIM potentially represents a new frontier\
          \ for training large-scale vision models. The pre-training of AIM is similar\
          \ to the pre-training of LLMs, and does not require any image-specific strategy\
          \ to stabilize the training at scale.\r\n"
        updatedAt: '2024-01-18T07:26:31.803Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - clem
    id: 65a8d2a79ea7bfebefcd783a
    type: comment
  author: PapersAnon
  content: "Scalable Pre-training of Large Autoregressive Image Models\r\nhttps://arxiv.org/abs/2401.08541\r\
    \n>This paper introduces AIM, a collection of vision models pre-trained with an\
    \ autoregressive objective. These models are inspired by their textual counterparts,\
    \ i.e., Large Language Models (LLMs), and exhibit similar scaling properties.\
    \ Specifically, we highlight two key findings: (1) the performance of the visual\
    \ features scale with both the model capacity and the quantity of data, (2) the\
    \ value of the objective function correlates with the performance of the model\
    \ on downstream tasks. We illustrate the practical implication of these findings\
    \ by pre-training a 7 billion parameter AIM on 2 billion images, that achieves\
    \ 84.0% on ImageNet-1k with a frozen trunk. Interestingly, even at this scale,\
    \ we observe no sign of saturation in performance, suggesting that AIM potentially\
    \ represents a new frontier for training large-scale vision models. The pre-training\
    \ of AIM is similar to the pre-training of LLMs, and does not require any image-specific\
    \ strategy to stabilize the training at scale.\r\n"
  created_at: 2024-01-18 07:26:31+00:00
  edited: false
  hidden: false
  id: 65a8d2a79ea7bfebefcd783a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2024-01-18T16:48:22.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48582255840301514
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: '<p><a href="https://huggingface.co/papers/2401.08541">or on hf here
          =)</a></p>

          '
        raw: '[or on hf here =)](https://huggingface.co/papers/2401.08541)'
        updatedAt: '2024-01-18T16:48:22.727Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PapersAnon
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - clem
    id: 65a95656d6fc9616a6307d28
    type: comment
  author: julien-c
  content: '[or on hf here =)](https://huggingface.co/papers/2401.08541)'
  created_at: 2024-01-18 16:48:22+00:00
  edited: false
  hidden: false
  id: 65a95656d6fc9616a6307d28
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: apple/AIM
repo_type: model
status: open
target_branch: null
title: Paper link for anyone interested
