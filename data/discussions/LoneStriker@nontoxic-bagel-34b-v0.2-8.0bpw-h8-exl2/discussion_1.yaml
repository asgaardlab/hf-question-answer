!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bdambrosio
conflicting_files: null
created_at: 2024-01-09 20:02:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5cbe88512c48efa55279c262e84c396.svg
      fullname: Bruce D'Ambrosio
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bdambrosio
      type: user
    createdAt: '2024-01-09T20:02:02.000Z'
    data:
      edited: false
      editors:
      - bdambrosio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9626550674438477
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5cbe88512c48efa55279c262e84c396.svg
          fullname: Bruce D'Ambrosio
          isHf: false
          isPro: false
          name: bdambrosio
          type: user
        html: '<p>I would love to run this model in 8bit, and I don''t need more than
          16k ''real'' context.<br>but exllamav2 says not enough memory (2x4090 =
          48GB).<br>Is there a mod I can make to config.json, or to exllamav2 config,
          that would allow me to load this?<br>6 bit, or even 6.5, seems to have lots
          of vram left over, so not sure why 8 bit won''t load.<br>Any ideas? Tnx!</p>

          '
        raw: "I would love to run this model in 8bit, and I don't need more than 16k\
          \ 'real' context.\r\nbut exllamav2 says not enough memory (2x4090 = 48GB).\r\
          \nIs there a mod I can make to config.json, or to exllamav2 config, that\
          \ would allow me to load this?\r\n6 bit, or even 6.5, seems to have lots\
          \ of vram left over, so not sure why 8 bit won't load.\r\nAny ideas? Tnx!"
        updatedAt: '2024-01-09T20:02:02.647Z'
      numEdits: 0
      reactions: []
    id: 659da63ac474a955d4cfea8e
    type: comment
  author: bdambrosio
  content: "I would love to run this model in 8bit, and I don't need more than 16k\
    \ 'real' context.\r\nbut exllamav2 says not enough memory (2x4090 = 48GB).\r\n\
    Is there a mod I can make to config.json, or to exllamav2 config, that would allow\
    \ me to load this?\r\n6 bit, or even 6.5, seems to have lots of vram left over,\
    \ so not sure why 8 bit won't load.\r\nAny ideas? Tnx!"
  created_at: 2024-01-09 20:02:02+00:00
  edited: false
  hidden: false
  id: 659da63ac474a955d4cfea8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2024-01-10T09:31:57.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8897297978401184
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Just set your context in ooba to reduce the max tokens. Or you can
          edit cnofig.yaml and change this line:<br><code>"max_position_embeddings":
          200000,</code></p>

          <p>I don''t think there is much difference in quality going to 6.0bpw. But,
          if you don''t need full context, 8.0bpw and dropping max tokens should be
          possible.</p>

          '
        raw: 'Just set your context in ooba to reduce the max tokens. Or you can edit
          cnofig.yaml and change this line:

          `"max_position_embeddings": 200000,`


          I don''t think there is much difference in quality going to 6.0bpw. But,
          if you don''t need full context, 8.0bpw and dropping max tokens should be
          possible.'
        updatedAt: '2024-01-10T09:31:57.428Z'
      numEdits: 0
      reactions: []
    id: 659e640dda61c174cc2a0207
    type: comment
  author: LoneStriker
  content: 'Just set your context in ooba to reduce the max tokens. Or you can edit
    cnofig.yaml and change this line:

    `"max_position_embeddings": 200000,`


    I don''t think there is much difference in quality going to 6.0bpw. But, if you
    don''t need full context, 8.0bpw and dropping max tokens should be possible.'
  created_at: 2024-01-10 09:31:57+00:00
  edited: false
  hidden: false
  id: 659e640dda61c174cc2a0207
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/nontoxic-bagel-34b-v0.2-8.0bpw-h8-exl2
repo_type: model
status: open
target_branch: null
title: Don't need 200k?
