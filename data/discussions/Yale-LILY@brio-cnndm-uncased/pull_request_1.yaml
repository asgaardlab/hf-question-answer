!!python/object:huggingface_hub.community.DiscussionWithDetails
author: duke2502
conflicting_files:
- README.md
created_at: 2022-08-27 10:56:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb189fec6000c6ec3997ae92c105f3a8.svg
      fullname: Duc M. H. Le
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: duke2502
      type: user
    createdAt: '2022-08-27T11:56:17.000Z'
    data:
      edited: false
      editors:
      - duke2502
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb189fec6000c6ec3997ae92c105f3a8.svg
          fullname: Duc M. H. Le
          isHf: false
          isPro: false
          name: duke2502
          type: user
        html: '<p>Abstractive summarization models are commonly trained using maximum
          likelihood estimation, which assumes a deterministic (one-point) target
          distribution in which an ideal model will assign all the probability mass
          to the reference summary. This assumption may lead to performance degradation
          during inference, where the model needs to compare several system-generated
          (candidate) summaries that have deviated from the reference summary. To
          address this problem, we propose a novel training paradigm which assumes
          a non-deterministic distribution so that different candidate summaries are
          assigned probability mass according to their quality. Our method achieves
          a new state-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum
          (49.07 ROUGE-1) datasets. Further analysis also shows that our model can
          estimate probabilities of candidate summaries that are more correlated with
          their level of quality.</p>

          '
        raw: Abstractive summarization models are commonly trained using maximum likelihood
          estimation, which assumes a deterministic (one-point) target distribution
          in which an ideal model will assign all the probability mass to the reference
          summary. This assumption may lead to performance degradation during inference,
          where the model needs to compare several system-generated (candidate) summaries
          that have deviated from the reference summary. To address this problem,
          we propose a novel training paradigm which assumes a non-deterministic distribution
          so that different candidate summaries are assigned probability mass according
          to their quality. Our method achieves a new state-of-the-art result on the
          CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07 ROUGE-1) datasets. Further
          analysis also shows that our model can estimate probabilities of candidate
          summaries that are more correlated with their level of quality.
        updatedAt: '2022-08-27T11:56:17.713Z'
      numEdits: 0
      reactions: []
    id: 630a066111f10445166a8295
    type: comment
  author: duke2502
  content: Abstractive summarization models are commonly trained using maximum likelihood
    estimation, which assumes a deterministic (one-point) target distribution in which
    an ideal model will assign all the probability mass to the reference summary.
    This assumption may lead to performance degradation during inference, where the
    model needs to compare several system-generated (candidate) summaries that have
    deviated from the reference summary. To address this problem, we propose a novel
    training paradigm which assumes a non-deterministic distribution so that different
    candidate summaries are assigned probability mass according to their quality.
    Our method achieves a new state-of-the-art result on the CNN/DailyMail (47.78
    ROUGE-1) and XSum (49.07 ROUGE-1) datasets. Further analysis also shows that our
    model can estimate probabilities of candidate summaries that are more correlated
    with their level of quality.
  created_at: 2022-08-27 10:56:17+00:00
  edited: false
  hidden: false
  id: 630a066111f10445166a8295
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/bb189fec6000c6ec3997ae92c105f3a8.svg
      fullname: Duc M. H. Le
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: duke2502
      type: user
    createdAt: '2022-08-27T11:56:18.000Z'
    data:
      oid: 769c3aafea8d3ddf81f462868a79692206f0af3e
      parents:
      - b3f3618fca366b8d70b460f8c32d65dab8e9322e
      subject: Create README.md
    id: 630a06620000000000000000
    type: commit
  author: duke2502
  created_at: 2022-08-27 10:56:18+00:00
  id: 630a06620000000000000000
  oid: 769c3aafea8d3ddf81f462868a79692206f0af3e
  summary: Create README.md
  type: commit
is_pull_request: true
merge_commit_oid: null
num: 1
repo_id: Yale-LILY/brio-cnndm-uncased
repo_type: model
status: open
target_branch: refs/heads/main
title: Create README.md
