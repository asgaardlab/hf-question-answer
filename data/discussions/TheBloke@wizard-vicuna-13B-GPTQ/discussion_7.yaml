!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cbiggerdev
conflicting_files: null
created_at: 2023-06-11 23:49:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45ab86d7ffef107c2e787773862ad874.svg
      fullname: Cameron Bigger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cbiggerdev
      type: user
    createdAt: '2023-06-12T00:49:28.000Z'
    data:
      edited: false
      editors:
      - cbiggerdev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5734850764274597
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45ab86d7ffef107c2e787773862ad874.svg
          fullname: Cameron Bigger
          isHf: false
          isPro: false
          name: cbiggerdev
          type: user
        html: '<p>I''ve been trying to get Auto-GPTQ to work in a jupyter notebook
          with a bunch of different quantized models. I end up with this error every
          time. </p>

          <p>Here is my basic code:<br>`<br>from transformers import AutoTokenizer,
          pipeline, logging<br>from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig<br>import
          torch</p>

          <p>quantized_model_dir = "TheBloke/stable-vicuna-13B-GPTQ"</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)</p>

          <p>AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=True)<br>`</p>

          <p>And here is my error:</p>

          <h2 id="">`</h2>

          <p>FileNotFoundError                         Traceback (most recent call
          last)<br>Cell In[9], line 9<br>      5 quantized_model_dir = "TheBloke/stable-vicuna-13B-GPTQ"<br>      7
          tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)<br>----&gt;
          9 AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=True)</p>

          <p>File /opt/conda/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:82,
          in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir,
          device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention,
          inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,
          trust_remote_code, warmup_triton, **kwargs)<br>     80 quant_func = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized<br>     81
          keywords = {key: kwargs[key] for key in signature(quant_func).parameters
          if key in kwargs}<br>---&gt; 82 return quant_func(<br>     83     model_name_or_path=model_name_or_path,<br>     84     save_dir=save_dir,<br>     85     device_map=device_map,<br>     86     max_memory=max_memory,<br>     87     device=device,<br>     88     low_cpu_mem_usage=low_cpu_mem_usage,<br>     89     use_triton=use_triton,<br>     90     inject_fused_attention=inject_fused_attention,<br>     91     inject_fused_mlp=inject_fused_mlp,<br>     92     use_cuda_fp16=use_cuda_fp16,<br>     93     quantize_config=quantize_config,<br>     94     model_basename=model_basename,<br>     95     use_safetensors=use_safetensors,<br>     96     trust_remote_code=trust_remote_code,<br>     97     warmup_triton=warmup_triton,<br>     98     **keywords<br>     99
          )</p>

          <p>File /opt/conda/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:698,
          in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir,
          device_map, max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype,
          inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config,
          model_basename, use_safetensors, trust_remote_code, warmup_triton, **kwargs)<br>    695             break<br>    697
          if resolved_archive_file is None: # Could not find a model file to use<br>--&gt;
          698     raise FileNotFoundError(f"Could not find model in {model_name_or_path}")<br>    700
          model_save_name = resolved_archive_file<br>    702 # == step2: convert model
          to gptq-model (replace Linear with QuantLinear) == #</p>

          <p>FileNotFoundError: Could not find model in TheBloke/stable-vicuna-13B-GPTQ<br>`</p>

          <p>Any help would be greatly appreciated!</p>

          '
        raw: "I've been trying to get Auto-GPTQ to work in a jupyter notebook with\
          \ a bunch of different quantized models. I end up with this error every\
          \ time. \r\n\r\n\r\nHere is my basic code:\r\n`\r\nfrom transformers import\
          \ AutoTokenizer, pipeline, logging\r\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\r\nimport torch\r\n\r\nquantized_model_dir = \"TheBloke/stable-vicuna-13B-GPTQ\"\
          \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)\r\
          \n\r\nAutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=True)\r\
          \n`\r\n\r\nAnd here is my error:\r\n\r\n`\r\n---------------------------------------------------------------------------\r\
          \nFileNotFoundError                         Traceback (most recent call\
          \ last)\r\nCell In[9], line 9\r\n      5 quantized_model_dir = \"TheBloke/stable-vicuna-13B-GPTQ\"\
          \r\n      7 tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)\r\
          \n----> 9 AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=True)\r\
          \n\r\nFile /opt/conda/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:82,\
          \ in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir,\
          \ device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention,\
          \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
          \ trust_remote_code, warmup_triton, **kwargs)\r\n     80 quant_func = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized\r\
          \n     81 keywords = {key: kwargs[key] for key in signature(quant_func).parameters\
          \ if key in kwargs}\r\n---> 82 return quant_func(\r\n     83     model_name_or_path=model_name_or_path,\r\
          \n     84     save_dir=save_dir,\r\n     85     device_map=device_map,\r\
          \n     86     max_memory=max_memory,\r\n     87     device=device,\r\n \
          \    88     low_cpu_mem_usage=low_cpu_mem_usage,\r\n     89     use_triton=use_triton,\r\
          \n     90     inject_fused_attention=inject_fused_attention,\r\n     91\
          \     inject_fused_mlp=inject_fused_mlp,\r\n     92     use_cuda_fp16=use_cuda_fp16,\r\
          \n     93     quantize_config=quantize_config,\r\n     94     model_basename=model_basename,\r\
          \n     95     use_safetensors=use_safetensors,\r\n     96     trust_remote_code=trust_remote_code,\r\
          \n     97     warmup_triton=warmup_triton,\r\n     98     **keywords\r\n\
          \     99 )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:698,\
          \ in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir,\
          \ device_map, max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype,\
          \ inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config,\
          \ model_basename, use_safetensors, trust_remote_code, warmup_triton, **kwargs)\r\
          \n    695             break\r\n    697 if resolved_archive_file is None:\
          \ # Could not find a model file to use\r\n--> 698     raise FileNotFoundError(f\"\
          Could not find model in {model_name_or_path}\")\r\n    700 model_save_name\
          \ = resolved_archive_file\r\n    702 # == step2: convert model to gptq-model\
          \ (replace Linear with QuantLinear) == #\r\n\r\nFileNotFoundError: Could\
          \ not find model in TheBloke/stable-vicuna-13B-GPTQ\r\n`\r\n\r\nAny help\
          \ would be greatly appreciated!"
        updatedAt: '2023-06-12T00:49:28.133Z'
      numEdits: 0
      reactions: []
    id: 64866b98548eeb715294681a
    type: comment
  author: cbiggerdev
  content: "I've been trying to get Auto-GPTQ to work in a jupyter notebook with a\
    \ bunch of different quantized models. I end up with this error every time. \r\
    \n\r\n\r\nHere is my basic code:\r\n`\r\nfrom transformers import AutoTokenizer,\
    \ pipeline, logging\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\
    \nimport torch\r\n\r\nquantized_model_dir = \"TheBloke/stable-vicuna-13B-GPTQ\"\
    \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)\r\n\r\n\
    AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=True)\r\
    \n`\r\n\r\nAnd here is my error:\r\n\r\n`\r\n---------------------------------------------------------------------------\r\
    \nFileNotFoundError                         Traceback (most recent call last)\r\
    \nCell In[9], line 9\r\n      5 quantized_model_dir = \"TheBloke/stable-vicuna-13B-GPTQ\"\
    \r\n      7 tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)\r\n\
    ----> 9 AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=True)\r\
    \n\r\nFile /opt/conda/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:82,\
    \ in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir, device_map,\
    \ max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention, inject_fused_mlp,\
    \ use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code,\
    \ warmup_triton, **kwargs)\r\n     80 quant_func = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized\r\
    \n     81 keywords = {key: kwargs[key] for key in signature(quant_func).parameters\
    \ if key in kwargs}\r\n---> 82 return quant_func(\r\n     83     model_name_or_path=model_name_or_path,\r\
    \n     84     save_dir=save_dir,\r\n     85     device_map=device_map,\r\n   \
    \  86     max_memory=max_memory,\r\n     87     device=device,\r\n     88    \
    \ low_cpu_mem_usage=low_cpu_mem_usage,\r\n     89     use_triton=use_triton,\r\
    \n     90     inject_fused_attention=inject_fused_attention,\r\n     91     inject_fused_mlp=inject_fused_mlp,\r\
    \n     92     use_cuda_fp16=use_cuda_fp16,\r\n     93     quantize_config=quantize_config,\r\
    \n     94     model_basename=model_basename,\r\n     95     use_safetensors=use_safetensors,\r\
    \n     96     trust_remote_code=trust_remote_code,\r\n     97     warmup_triton=warmup_triton,\r\
    \n     98     **keywords\r\n     99 )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:698,\
    \ in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir, device_map,\
    \ max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention,\
    \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
    \ trust_remote_code, warmup_triton, **kwargs)\r\n    695             break\r\n\
    \    697 if resolved_archive_file is None: # Could not find a model file to use\r\
    \n--> 698     raise FileNotFoundError(f\"Could not find model in {model_name_or_path}\"\
    )\r\n    700 model_save_name = resolved_archive_file\r\n    702 # == step2: convert\
    \ model to gptq-model (replace Linear with QuantLinear) == #\r\n\r\nFileNotFoundError:\
    \ Could not find model in TheBloke/stable-vicuna-13B-GPTQ\r\n`\r\n\r\nAny help\
    \ would be greatly appreciated!"
  created_at: 2023-06-11 23:49:28+00:00
  edited: false
  hidden: false
  id: 64866b98548eeb715294681a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-12T06:49:55.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5137585997581482
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You need to add model_basename to tell it the name of the model
          file</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline,
          logging

          <span class="hljs-keyword">from</span> auto_gptq <span class="hljs-keyword">import</span>
          AutoGPTQForCausalLM, BaseQuantizeConfig

          <span class="hljs-keyword">import</span> torch


          quantized_model_dir = <span class="hljs-string">"TheBloke/stable-vicuna-13B-GPTQ"</span>

          model_basename = <span class="hljs-string">"wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order"</span>


          tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)


          AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=<span
          class="hljs-literal">True</span>, model_basename=model_basename)

          </code></pre>

          '
        raw: 'You need to add model_basename to tell it the name of the model file

          ```python

          from transformers import AutoTokenizer, pipeline, logging

          from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

          import torch


          quantized_model_dir = "TheBloke/stable-vicuna-13B-GPTQ"

          model_basename = "wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order"


          tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)


          AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=True,
          model_basename=model_basename)

          ```'
        updatedAt: '2023-06-12T06:49:55.340Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Calivito
        - noobmaster29
    id: 6486c013cc2ce11bbf81cc4d
    type: comment
  author: TheBloke
  content: 'You need to add model_basename to tell it the name of the model file

    ```python

    from transformers import AutoTokenizer, pipeline, logging

    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

    import torch


    quantized_model_dir = "TheBloke/stable-vicuna-13B-GPTQ"

    model_basename = "wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order"


    tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)


    AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=True,
    model_basename=model_basename)

    ```'
  created_at: 2023-06-12 05:49:55+00:00
  edited: false
  hidden: false
  id: 6486c013cc2ce11bbf81cc4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45ab86d7ffef107c2e787773862ad874.svg
      fullname: Cameron Bigger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cbiggerdev
      type: user
    createdAt: '2023-06-15T23:19:19.000Z'
    data:
      edited: false
      editors:
      - cbiggerdev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9390435814857483
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45ab86d7ffef107c2e787773862ad874.svg
          fullname: Cameron Bigger
          isHf: false
          isPro: false
          name: cbiggerdev
          type: user
        html: '<p>Hey, thank you so much. I will use this tonight. </p>

          '
        raw: 'Hey, thank you so much. I will use this tonight. '
        updatedAt: '2023-06-15T23:19:19.013Z'
      numEdits: 0
      reactions: []
    id: 648b9c778ad06d2962c1e7b3
    type: comment
  author: cbiggerdev
  content: 'Hey, thank you so much. I will use this tonight. '
  created_at: 2023-06-15 22:19:19+00:00
  edited: false
  hidden: false
  id: 648b9c778ad06d2962c1e7b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8606a113b3b75430fdc43d95e7d2f7e7.svg
      fullname: Wong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Garmisch
      type: user
    createdAt: '2023-07-05T07:52:30.000Z'
    data:
      edited: true
      editors:
      - Garmisch
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9098430275917053
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8606a113b3b75430fdc43d95e7d2f7e7.svg
          fullname: Wong
          isHf: false
          isPro: false
          name: Garmisch
          type: user
        html: '<p>It didn''t work for me, the program is able to download the toknizers,
          but when it tries to download the model, i got the following error:<br>raise
          FileNotFoundError(f"Could not find model in {model_name_or_path}")<br>FileNotFoundError:
          Could not find model in TheBloke/stable-vicuna-13B-GPTQ</p>

          <p>Do you know what is wrong?<br>If i want to download the model manually,
          should i put the safetensor file together with the tokenizer.model file?
          Since the program created some folders like blobs, refs, i am not sure where
          i should put the safetensor file into.</p>

          <h2 id="solved">Solved</h2>

          <p>i typed the wrong name. "wizard-vicuna", not "stable-vicuna"</p>

          '
        raw: "It didn't work for me, the program is able to download the toknizers,\
          \ but when it tries to download the model, i got the following error: \n\
          raise FileNotFoundError(f\"Could not find model in {model_name_or_path}\"\
          )\nFileNotFoundError: Could not find model in TheBloke/stable-vicuna-13B-GPTQ\n\
          \nDo you know what is wrong? \nIf i want to download the model manually,\
          \ should i put the safetensor file together with the tokenizer.model file?\
          \ Since the program created some folders like blobs, refs, i am not sure\
          \ where i should put the safetensor file into.\n\n\n## Solved ##\ni typed\
          \ the wrong name. \"wizard-vicuna\", not \"stable-vicuna\"\n"
        updatedAt: '2023-07-05T08:05:09.965Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 64a5213e219d9495f791b942
    type: comment
  author: Garmisch
  content: "It didn't work for me, the program is able to download the toknizers,\
    \ but when it tries to download the model, i got the following error: \nraise\
    \ FileNotFoundError(f\"Could not find model in {model_name_or_path}\")\nFileNotFoundError:\
    \ Could not find model in TheBloke/stable-vicuna-13B-GPTQ\n\nDo you know what\
    \ is wrong? \nIf i want to download the model manually, should i put the safetensor\
    \ file together with the tokenizer.model file? Since the program created some\
    \ folders like blobs, refs, i am not sure where i should put the safetensor file\
    \ into.\n\n\n## Solved ##\ni typed the wrong name. \"wizard-vicuna\", not \"stable-vicuna\"\
    \n"
  created_at: 2023-07-05 06:52:30+00:00
  edited: true
  hidden: false
  id: 64a5213e219d9495f791b942
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/13dcd8eb2884ca43d726c788d31ccabb.svg
      fullname: Ivan Homoliak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ihomoliak
      type: user
    createdAt: '2023-10-16T19:52:40.000Z'
    data:
      edited: true
      editors:
      - ihomoliak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5816052556037903
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/13dcd8eb2884ca43d726c788d31ccabb.svg
          fullname: Ivan Homoliak
          isHf: false
          isPro: false
          name: ihomoliak
          type: user
        html: '<blockquote>

          <p>You need to add model_basename to tell it the name of the model file</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline,
          logging

          <span class="hljs-keyword">from</span> auto_gptq <span class="hljs-keyword">import</span>
          AutoGPTQForCausalLM, BaseQuantizeConfig

          <span class="hljs-keyword">import</span> torch


          quantized_model_dir = <span class="hljs-string">"TheBloke/stable-vicuna-13B-GPTQ"</span>

          model_basename = <span class="hljs-string">"wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order"</span>


          tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)


          AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=<span
          class="hljs-literal">True</span>, model_basename=model_basename)

          </code></pre>

          </blockquote>

          <p>May I ask you where you got the <em>model_basename</em> from, please?
          </p>

          '
        raw: "> You need to add model_basename to tell it the name of the model file\n\
          > ```python\n> from transformers import AutoTokenizer, pipeline, logging\n\
          > from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n> import\
          \ torch\n> \n> quantized_model_dir = \"TheBloke/stable-vicuna-13B-GPTQ\"\
          \n> model_basename = \"wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order\"\
          \n> \n> tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)\n\
          > \n> AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=True,\
          \ model_basename=model_basename)\n> ```\n\nMay I ask you where you got the\
          \ *model_basename* from, please? "
        updatedAt: '2023-10-16T19:53:07.194Z'
      numEdits: 1
      reactions: []
    id: 652d94881d12768fffeebb8a
    type: comment
  author: ihomoliak
  content: "> You need to add model_basename to tell it the name of the model file\n\
    > ```python\n> from transformers import AutoTokenizer, pipeline, logging\n> from\
    \ auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n> import torch\n>\
    \ \n> quantized_model_dir = \"TheBloke/stable-vicuna-13B-GPTQ\"\n> model_basename\
    \ = \"wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order\"\n> \n> tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)\n\
    > \n> AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=True,\
    \ model_basename=model_basename)\n> ```\n\nMay I ask you where you got the *model_basename*\
    \ from, please? "
  created_at: 2023-10-16 18:52:40+00:00
  edited: true
  hidden: false
  id: 652d94881d12768fffeebb8a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/wizard-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Could not find model in TheBloke/wizard-vicuna-13B-GPTQ
