!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MorphzZ
conflicting_files: null
created_at: 2023-06-14 22:53:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
      fullname: MORPHEUS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MorphzZ
      type: user
    createdAt: '2023-06-14T23:53:06.000Z'
    data:
      edited: true
      editors:
      - MorphzZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6195707321166992
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
          fullname: MORPHEUS
          isHf: false
          isPro: false
          name: MorphzZ
          type: user
        html: '<p>I try to load it like this:</p>

          <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM


          model = AutoModelForCausalLM.from_pretrained("TheBloke/wizard-vicuna-13B-GPTQ")

          </code></pre>

          <p>and it gives this error:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6477d713f32a4117fd16da2c/y4uO_nejQ1tNUbChrLeUO.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6477d713f32a4117fd16da2c/y4uO_nejQ1tNUbChrLeUO.png"></a></p>

          <pre><code>OSError: TheBloke/wizard-vicuna-13B-GPTQ does not appear to have
          a file named

          pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

          </code></pre>

          <p>How can I fix this? This is what I am following:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6477d713f32a4117fd16da2c/XlrcPtY7yCyejoExFPAg7.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6477d713f32a4117fd16da2c/XlrcPtY7yCyejoExFPAg7.png"></a></p>

          '
        raw: 'I try to load it like this:


          ```

          from transformers import AutoTokenizer, AutoModelForCausalLM


          model = AutoModelForCausalLM.from_pretrained("TheBloke/wizard-vicuna-13B-GPTQ")

          ```


          and it gives this error:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6477d713f32a4117fd16da2c/y4uO_nejQ1tNUbChrLeUO.png)


          ```

          OSError: TheBloke/wizard-vicuna-13B-GPTQ does not appear to have a file
          named

          pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

          ```


          How can I fix this? This is what I am following:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6477d713f32a4117fd16da2c/XlrcPtY7yCyejoExFPAg7.png)

          '
        updatedAt: '2023-06-14T23:59:26.141Z'
      numEdits: 2
      reactions: []
    id: 648a52e27de18d75a24373c1
    type: comment
  author: MorphzZ
  content: 'I try to load it like this:


    ```

    from transformers import AutoTokenizer, AutoModelForCausalLM


    model = AutoModelForCausalLM.from_pretrained("TheBloke/wizard-vicuna-13B-GPTQ")

    ```


    and it gives this error:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6477d713f32a4117fd16da2c/y4uO_nejQ1tNUbChrLeUO.png)


    ```

    OSError: TheBloke/wizard-vicuna-13B-GPTQ does not appear to have a file named

    pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.

    ```


    How can I fix this? This is what I am following:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6477d713f32a4117fd16da2c/XlrcPtY7yCyejoExFPAg7.png)

    '
  created_at: 2023-06-14 22:53:06+00:00
  edited: true
  hidden: false
  id: 648a52e27de18d75a24373c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-16T11:36:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5598204731941223
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>You can't load GPTQ models with bare transformers.  You need to\
          \ install <a rel=\"nofollow\" href=\"https://github.com/PanQiWei/AutoGPTQ\"\
          >AutoGPTQ</a>.</p>\n<p>Here is sample code using AutoGPTQ:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer, pipeline, logging\n\
          <span class=\"hljs-keyword\">from</span> auto_gptq <span class=\"hljs-keyword\"\
          >import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n<span class=\"hljs-keyword\"\
          >import</span> argparse\n\nmodel_name_or_path = <span class=\"hljs-string\"\
          >\"TheBloke/wizard-vicuna-13B-GPTQ\"</span>\n<span class=\"hljs-comment\"\
          ># You could also download the model locally, and access it there</span>\n\
          <span class=\"hljs-comment\"># model_name_or_path = \"/path/to/TheBloke_wizard-vicuna-13B-GPTQ\"\
          </span>\n\nmodel_basename = <span class=\"hljs-string\">\"wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order\"\
          </span>\n\nuse_triton = <span class=\"hljs-literal\">False</span>\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=<span class=\"\
          hljs-literal\">True</span>)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=<span class=\"\
          hljs-literal\">True</span>,\n        trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,\n        device=<span class=\"hljs-string\">\"cuda:0\"</span>,\n\
          \        use_triton=use_triton,\n        quantize_config=<span class=\"\
          hljs-literal\">None</span>)\n\nprompt = <span class=\"hljs-string\">\"Tell\
          \ me about AI\"</span>\nprompt_template=<span class=\"hljs-string\">f'''###\
          \ Human: <span class=\"hljs-subst\">{prompt}</span></span>\n<span class=\"\
          hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n\n<span class=\"hljs-comment\"># Inference\
          \ can also be done using transformers' pipeline</span>\n<span class=\"hljs-comment\"\
          ># Note that if you use pipeline, you will see a spurious error message\
          \ saying the model type is not supported</span>\n<span class=\"hljs-comment\"\
          ># This can be ignored!  Or you can hide it with the following logging line:</span>\n\
          <span class=\"hljs-comment\"># Prevent printing spurious transformers error\
          \ when using pipeline with AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\
          \n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"*** Pipeline:\"</span>)\npipe = pipeline(\n    <span class=\"hljs-string\"\
          >\"text-generation\"</span>,\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=<span class=\"hljs-number\">512</span>,\n    temperature=<span\
          \ class=\"hljs-number\">0.7</span>,\n    top_p=<span class=\"hljs-number\"\
          >0.95</span>,\n    repetition_penalty=<span class=\"hljs-number\">1.15</span>\n\
          )\n\n<span class=\"hljs-built_in\">print</span>(pipe(prompt_template)[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'generated_text'</span>])\n\
          </code></pre>\n"
        raw: "You can't load GPTQ models with bare transformers.  You need to install\
          \ [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ).\n\nHere is sample code\
          \ using AutoGPTQ:\n```python\nfrom transformers import AutoTokenizer, pipeline,\
          \ logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          import argparse\n\nmodel_name_or_path = \"TheBloke/wizard-vicuna-13B-GPTQ\"\
          \n# You could also download the model locally, and access it there\n# model_name_or_path\
          \ = \"/path/to/TheBloke_wizard-vicuna-13B-GPTQ\"\n\nmodel_basename = \"\
          wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order\"\n\nuse_triton = False\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n     \
          \   model_basename=model_basename,\n        use_safetensors=True,\n    \
          \    trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''###\
          \ Human: {prompt}\n### Assistant:'''\n\nprint(\"\\n\\n*** Generate:\")\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n\n# Inference can also be done using\
          \ transformers' pipeline\n# Note that if you use pipeline, you will see\
          \ a spurious error message saying the model type is not supported\n# This\
          \ can be ignored!  Or you can hide it with the following logging line:\n\
          # Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprint(\"*** Pipeline:\"\
          )\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n\
          )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n```"
        updatedAt: '2023-06-16T11:36:50.793Z'
      numEdits: 0
      reactions: []
    id: 648c495219bb04c064668033
    type: comment
  author: TheBloke
  content: "You can't load GPTQ models with bare transformers.  You need to install\
    \ [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ).\n\nHere is sample code using\
    \ AutoGPTQ:\n```python\nfrom transformers import AutoTokenizer, pipeline, logging\n\
    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\
    \nmodel_name_or_path = \"TheBloke/wizard-vicuna-13B-GPTQ\"\n# You could also download\
    \ the model locally, and access it there\n# model_name_or_path = \"/path/to/TheBloke_wizard-vicuna-13B-GPTQ\"\
    \n\nmodel_basename = \"wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order\"\n\nuse_triton\
    \ = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
    \nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n        model_basename=model_basename,\n\
    \        use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
    cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\nprompt\
    \ = \"Tell me about AI\"\nprompt_template=f'''### Human: {prompt}\n### Assistant:'''\n\
    \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers'\
    \ pipeline\n# Note that if you use pipeline, you will see a spurious error message\
    \ saying the model type is not supported\n# This can be ignored!  Or you can hide\
    \ it with the following logging line:\n# Prevent printing spurious transformers\
    \ error when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
    \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.7,\n   \
    \ top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    ```"
  created_at: 2023-06-16 10:36:50+00:00
  edited: false
  hidden: false
  id: 648c495219bb04c064668033
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
      fullname: MORPHEUS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MorphzZ
      type: user
    createdAt: '2023-06-16T15:18:36.000Z'
    data:
      edited: true
      editors:
      - MorphzZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9540657997131348
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
          fullname: MORPHEUS
          isHf: false
          isPro: false
          name: MorphzZ
          type: user
        html: '<p>Thanks for responding to me. I appreciate it. I did try installing
          AutoGPT. It gives me <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ/issues/160">this</a>
          error:</p>

          <p>The detected CUDA version (12.1) mismatches the version that was used
          to compile PyTorch (11.7)</p>

          <p>is this known to you and do you know how to fix this? I am able to run
          torch w/ GPU on my computer using transformers. why does AutoGPT complain
          then? </p>

          '
        raw: 'Thanks for responding to me. I appreciate it. I did try installing AutoGPT.
          It gives me [this](https://github.com/PanQiWei/AutoGPTQ/issues/160) error:


          The detected CUDA version (12.1) mismatches the version that was used to
          compile PyTorch (11.7)


          is this known to you and do you know how to fix this? I am able to run torch
          w/ GPU on my computer using transformers. why does AutoGPT complain then? '
        updatedAt: '2023-06-16T15:21:12.086Z'
      numEdits: 1
      reactions: []
    id: 648c7d4cbf31bc968292f317
    type: comment
  author: MorphzZ
  content: 'Thanks for responding to me. I appreciate it. I did try installing AutoGPT.
    It gives me [this](https://github.com/PanQiWei/AutoGPTQ/issues/160) error:


    The detected CUDA version (12.1) mismatches the version that was used to compile
    PyTorch (11.7)


    is this known to you and do you know how to fix this? I am able to run torch w/
    GPU on my computer using transformers. why does AutoGPT complain then? '
  created_at: 2023-06-16 14:18:36+00:00
  edited: true
  hidden: false
  id: 648c7d4cbf31bc968292f317
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
      fullname: MORPHEUS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MorphzZ
      type: user
    createdAt: '2023-06-16T21:25:41.000Z'
    data:
      edited: false
      editors:
      - MorphzZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8599246144294739
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
          fullname: MORPHEUS
          isHf: false
          isPro: false
          name: MorphzZ
          type: user
        html: "<p>for anyone who runs into this, the issue that puzzled me is that\
          \ I am able to run FastChat w/ CUDA, then why does AutoGPT complain? I think\
          \ <a rel=\"nofollow\" href=\"https://discuss.pytorch.org/t/pytorch-for-cuda-12/169447/20?u=siddhsql\"\
          >this</a> explains it:</p>\n<blockquote>\n<p>Your locally installed CUDA\
          \ toolkit won\u2019t be used as the PyTorch binaries ship with their own\
          \ CUDA dependencies unless you build PyTorch from source or a custom extension.<br>Check\
          \ the output of python -m torch.utils.collect_env and make sure a PyTorch\
          \ version with a CUDA runtime is installed.</p>\n</blockquote>\n"
        raw: "for anyone who runs into this, the issue that puzzled me is that I am\
          \ able to run FastChat w/ CUDA, then why does AutoGPT complain? I think\
          \ [this](https://discuss.pytorch.org/t/pytorch-for-cuda-12/169447/20?u=siddhsql)\
          \ explains it:\n\n>Your locally installed CUDA toolkit won\u2019t be used\
          \ as the PyTorch binaries ship with their own CUDA dependencies unless you\
          \ build PyTorch from source or a custom extension.\nCheck the output of\
          \ python -m torch.utils.collect_env and make sure a PyTorch version with\
          \ a CUDA runtime is installed."
        updatedAt: '2023-06-16T21:25:41.504Z'
      numEdits: 0
      reactions: []
    id: 648cd3553e8772095e0c8beb
    type: comment
  author: MorphzZ
  content: "for anyone who runs into this, the issue that puzzled me is that I am\
    \ able to run FastChat w/ CUDA, then why does AutoGPT complain? I think [this](https://discuss.pytorch.org/t/pytorch-for-cuda-12/169447/20?u=siddhsql)\
    \ explains it:\n\n>Your locally installed CUDA toolkit won\u2019t be used as the\
    \ PyTorch binaries ship with their own CUDA dependencies unless you build PyTorch\
    \ from source or a custom extension.\nCheck the output of python -m torch.utils.collect_env\
    \ and make sure a PyTorch version with a CUDA runtime is installed."
  created_at: 2023-06-16 20:25:41+00:00
  edited: false
  hidden: false
  id: 648cd3553e8772095e0c8beb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: TheBloke/wizard-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'OSError: TheBloke/wizard-vicuna-13B-GPTQ does not appear to have a file named
  pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.'
