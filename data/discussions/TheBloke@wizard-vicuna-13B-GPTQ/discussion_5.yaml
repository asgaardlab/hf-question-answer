!!python/object:huggingface_hub.community.DiscussionWithDetails
author: axeljeremy7
conflicting_files: null
created_at: 2023-05-22 07:57:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0PMmF9Gmcag8boB3zpIJy.jpeg?w=200&h=200&f=face
      fullname: 'Axl Jeremy '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: axeljeremy7
      type: user
    createdAt: '2023-05-22T08:57:00.000Z'
    data:
      edited: false
      editors:
      - axeljeremy7
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0PMmF9Gmcag8boB3zpIJy.jpeg?w=200&h=200&f=face
          fullname: 'Axl Jeremy '
          isHf: false
          isPro: false
          name: axeljeremy7
          type: user
        html: '<p>Running locally instead hub api </p>

          '
        raw: 'Running locally instead hub api '
        updatedAt: '2023-05-22T08:57:00.727Z'
      numEdits: 0
      reactions: []
    id: 646b2e5cc7f672003c828cf5
    type: comment
  author: axeljeremy7
  content: 'Running locally instead hub api '
  created_at: 2023-05-22 07:57:00+00:00
  edited: false
  hidden: false
  id: 646b2e5cc7f672003c828cf5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645d0281680734460f9cb8bf/K_0AtvDqpxfxKS540qCQL.png?w=200&h=200&f=face
      fullname: "Johan \xD6hman Saldes"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jsaldes
      type: user
    createdAt: '2023-05-22T09:45:35.000Z'
    data:
      edited: false
      editors:
      - jsaldes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645d0281680734460f9cb8bf/K_0AtvDqpxfxKS540qCQL.png?w=200&h=200&f=face
          fullname: "Johan \xD6hman Saldes"
          isHf: false
          isPro: false
          name: jsaldes
          type: user
        html: '<p>There''s a GGML version of Wizard-vicuna with LlamaCpp support that''s
          13B as well. Maybe try that instead or is it any particular reason you want
          THIS model? If you get the GGML version you can simply load it with the
          <code>LlamaCpp</code> interface in langchain!</p>

          '
        raw: There's a GGML version of Wizard-vicuna with LlamaCpp support that's
          13B as well. Maybe try that instead or is it any particular reason you want
          THIS model? If you get the GGML version you can simply load it with the
          `LlamaCpp` interface in langchain!
        updatedAt: '2023-05-22T09:45:35.633Z'
      numEdits: 0
      reactions: []
    id: 646b39bfdb697c798a3285a8
    type: comment
  author: jsaldes
  content: There's a GGML version of Wizard-vicuna with LlamaCpp support that's 13B
    as well. Maybe try that instead or is it any particular reason you want THIS model?
    If you get the GGML version you can simply load it with the `LlamaCpp` interface
    in langchain!
  created_at: 2023-05-22 08:45:35+00:00
  edited: false
  hidden: false
  id: 646b39bfdb697c798a3285a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df932694e10a6ef0e8d78aebc2ce253c.svg
      fullname: Peter Beks
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kwissbeats
      type: user
    createdAt: '2023-05-27T11:55:17.000Z'
    data:
      edited: false
      editors:
      - Kwissbeats
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df932694e10a6ef0e8d78aebc2ce253c.svg
          fullname: Peter Beks
          isHf: false
          isPro: false
          name: Kwissbeats
          type: user
        html: '<blockquote>

          <p>There''s a GGML version of Wizard-vicuna with LlamaCpp support that''s
          13B as well. Maybe try that instead or is it any particular reason you want
          THIS model? If you get the GGML version you can simply load it with the
          <code>LlamaCpp</code> interface in langchain!</p>

          </blockquote>

          <p>Is this true? do you now what version of ggml is supported right now
          in their llaamacpp? are they at v3 too?<br>?</p>

          '
        raw: '> There''s a GGML version of Wizard-vicuna with LlamaCpp support that''s
          13B as well. Maybe try that instead or is it any particular reason you want
          THIS model? If you get the GGML version you can simply load it with the
          `LlamaCpp` interface in langchain!


          Is this true? do you now what version of ggml is supported right now in
          their llaamacpp? are they at v3 too?

          ?'
        updatedAt: '2023-05-27T11:55:17.632Z'
      numEdits: 0
      reactions: []
    id: 6471efa50211f85270fcf6c4
    type: comment
  author: Kwissbeats
  content: '> There''s a GGML version of Wizard-vicuna with LlamaCpp support that''s
    13B as well. Maybe try that instead or is it any particular reason you want THIS
    model? If you get the GGML version you can simply load it with the `LlamaCpp`
    interface in langchain!


    Is this true? do you now what version of ggml is supported right now in their
    llaamacpp? are they at v3 too?

    ?'
  created_at: 2023-05-27 10:55:17+00:00
  edited: false
  hidden: false
  id: 6471efa50211f85270fcf6c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/305ffe97659628082e04ca17949cba89.svg
      fullname: Wali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hussainwali1
      type: user
    createdAt: '2023-05-30T10:53:12.000Z'
    data:
      edited: false
      editors:
      - hussainwali1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/305ffe97659628082e04ca17949cba89.svg
          fullname: Wali
          isHf: false
          isPro: false
          name: hussainwali1
          type: user
        html: "<p>assuming is webui is running and api is enabled on port 8080:</p>\n\
          <p>create this file, </p>\n<h2 id=\"llm_clientpy\">llm_client.py:</h2>\n\
          <p>from langchain.llms.base import LLM<br>from typing import Optional, List,\
          \ Mapping, Any</p>\n<p>import requests</p>\n<p>HOST = 'localhost:8080'<br>URI\
          \ = f'http://{HOST}/api/v1/generate'</p>\n<p>class AlpacaLLM(LLM):<br> \
          \   @property<br>    def _llm_type(self) -&gt; str:<br>        return \"\
          custom\"</p>\n<pre><code>def _call(self, prompt: str, stop: Optional[List[str]]\
          \ = None) -&gt; str:\n    if isinstance(stop, list):\n        stop = stop\
          \ + [\"\\n###\",\"\\nObservation:\", \"\\nObservations:\"]\n\n    response\
          \ = requests.post(\n        URI,\n        json={\n            \"prompt\"\
          : prompt,\n            \"temperature\": 0.7,\n            \"max_new_tokens\"\
          : 500,\n            \"early_stopping\": True,\n            \"stopping_strings\"\
          : stop,\n            'do_sample': True,\n            'top_p': 0.1,\n   \
          \         'typical_p': 1,\n            'repetition_penalty': 1.18,\n   \
          \         'top_k': 40,\n            'min_length': 0,\n            'no_repeat_ngram_size':\
          \ 0,\n            'num_beams': 1,\n            'penalty_alpha': 0,\n   \
          \         'length_penalty': 1,\n            'seed': -1,\n            'add_bos_token':\
          \ True,\n            'truncation_length': 2048,\n            'ban_eos_token':\
          \ False,\n            'skip_special_tokens': True,\n        },\n    )\n\
          \    response.raise_for_status()\n    return response.json()['results'][0]['text']\n\
          \n@property\ndef _identifying_params(self) -&gt; Mapping[str, Any]:\n  \
          \  \"\"\"Get the identifying parameters.\"\"\"\n    return {}\n</code></pre>\n\
          <hr>\n<p>then use it like this:<br>from langchain.chains.conversation.memory\
          \ import ConversationBufferMemory<br>from langchain.chains import ConversationChain</p>\n\
          <p>from llm_client import AlpacaLLM<br>llm = AlpacaLLM()<br>memory = ConversationBufferMemory()<br>conversation\
          \ = ConversationChain(<br>    llm = llm, verbose=True, memory=memory<br>)</p>\n\
          <p>conversation.predict(input=\"hi there, i am you doom\")</p>\n"
        raw: "assuming is webui is running and api is enabled on port 8080:\n\ncreate\
          \ this file, \n\nllm_client.py:\n---------------------------------------------------------------------------------------\n\
          from langchain.llms.base import LLM\nfrom typing import Optional, List,\
          \ Mapping, Any\n\nimport requests\n\nHOST = 'localhost:8080'\nURI = f'http://{HOST}/api/v1/generate'\n\
          \nclass AlpacaLLM(LLM):\n    @property\n    def _llm_type(self) -> str:\n\
          \        return \"custom\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]]\
          \ = None) -> str:\n        if isinstance(stop, list):\n            stop\
          \ = stop + [\"\\n###\",\"\\nObservation:\", \"\\nObservations:\"]\n\n  \
          \      response = requests.post(\n            URI,\n            json={\n\
          \                \"prompt\": prompt,\n                \"temperature\": 0.7,\n\
          \                \"max_new_tokens\": 500,\n                \"early_stopping\"\
          : True,\n                \"stopping_strings\": stop,\n                'do_sample':\
          \ True,\n                'top_p': 0.1,\n                'typical_p': 1,\n\
          \                'repetition_penalty': 1.18,\n                'top_k': 40,\n\
          \                'min_length': 0,\n                'no_repeat_ngram_size':\
          \ 0,\n                'num_beams': 1,\n                'penalty_alpha':\
          \ 0,\n                'length_penalty': 1,\n                'seed': -1,\n\
          \                'add_bos_token': True,\n                'truncation_length':\
          \ 2048,\n                'ban_eos_token': False,\n                'skip_special_tokens':\
          \ True,\n            },\n        )\n        response.raise_for_status()\n\
          \        return response.json()['results'][0]['text']\n\n    @property\n\
          \    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"\
          Get the identifying parameters.\"\"\"\n        return {}\n------------------------------------------------------------------------------------------\n\
          then use it like this:\nfrom langchain.chains.conversation.memory import\
          \ ConversationBufferMemory\nfrom langchain.chains import ConversationChain\n\
          \nfrom llm_client import AlpacaLLM\nllm = AlpacaLLM()\nmemory = ConversationBufferMemory()\n\
          conversation = ConversationChain(\n    llm = llm, verbose=True, memory=memory\n\
          )\n\nconversation.predict(input=\"hi there, i am you doom\")"
        updatedAt: '2023-05-30T10:53:12.237Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Kwissbeats
    id: 6475d598c894b5c9cf73a573
    type: comment
  author: hussainwali1
  content: "assuming is webui is running and api is enabled on port 8080:\n\ncreate\
    \ this file, \n\nllm_client.py:\n---------------------------------------------------------------------------------------\n\
    from langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping,\
    \ Any\n\nimport requests\n\nHOST = 'localhost:8080'\nURI = f'http://{HOST}/api/v1/generate'\n\
    \nclass AlpacaLLM(LLM):\n    @property\n    def _llm_type(self) -> str:\n    \
    \    return \"custom\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]]\
    \ = None) -> str:\n        if isinstance(stop, list):\n            stop = stop\
    \ + [\"\\n###\",\"\\nObservation:\", \"\\nObservations:\"]\n\n        response\
    \ = requests.post(\n            URI,\n            json={\n                \"prompt\"\
    : prompt,\n                \"temperature\": 0.7,\n                \"max_new_tokens\"\
    : 500,\n                \"early_stopping\": True,\n                \"stopping_strings\"\
    : stop,\n                'do_sample': True,\n                'top_p': 0.1,\n \
    \               'typical_p': 1,\n                'repetition_penalty': 1.18,\n\
    \                'top_k': 40,\n                'min_length': 0,\n            \
    \    'no_repeat_ngram_size': 0,\n                'num_beams': 1,\n           \
    \     'penalty_alpha': 0,\n                'length_penalty': 1,\n            \
    \    'seed': -1,\n                'add_bos_token': True,\n                'truncation_length':\
    \ 2048,\n                'ban_eos_token': False,\n                'skip_special_tokens':\
    \ True,\n            },\n        )\n        response.raise_for_status()\n    \
    \    return response.json()['results'][0]['text']\n\n    @property\n    def _identifying_params(self)\
    \ -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n\
    \        return {}\n------------------------------------------------------------------------------------------\n\
    then use it like this:\nfrom langchain.chains.conversation.memory import ConversationBufferMemory\n\
    from langchain.chains import ConversationChain\n\nfrom llm_client import AlpacaLLM\n\
    llm = AlpacaLLM()\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(\n\
    \    llm = llm, verbose=True, memory=memory\n)\n\nconversation.predict(input=\"\
    hi there, i am you doom\")"
  created_at: 2023-05-30 09:53:12+00:00
  edited: false
  hidden: false
  id: 6475d598c894b5c9cf73a573
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/wizard-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Anyone knows how to run it locally and with langchain
