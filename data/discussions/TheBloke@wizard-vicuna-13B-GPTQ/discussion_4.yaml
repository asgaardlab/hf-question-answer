!!python/object:huggingface_hub.community.DiscussionWithDetails
author: maddiemii
conflicting_files: null
created_at: 2023-05-15 04:52:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb5857de619ad34effd8138e15e5cbb9.svg
      fullname: Madelyn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maddiemii
      type: user
    createdAt: '2023-05-15T05:52:42.000Z'
    data:
      edited: false
      editors:
      - maddiemii
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb5857de619ad34effd8138e15e5cbb9.svg
          fullname: Madelyn
          isHf: false
          isPro: false
          name: maddiemii
          type: user
        html: '<p>I''m loving this model, it''s the best that I''ve been working with
          so far. I guess one thing that I don''t yet understand is how to increase
          the context window so that it remembers the conversation for longer. I''d
          love to have 4096 or higher context window, but I don''t yet understand
          where the limitation is. Is it in the base model itself and the way it''s
          trained, or something else? Not a setting I can change? Thank you!</p>

          '
        raw: I'm loving this model, it's the best that I've been working with so far.
          I guess one thing that I don't yet understand is how to increase the context
          window so that it remembers the conversation for longer. I'd love to have
          4096 or higher context window, but I don't yet understand where the limitation
          is. Is it in the base model itself and the way it's trained, or something
          else? Not a setting I can change? Thank you!
        updatedAt: '2023-05-15T05:52:42.099Z'
      numEdits: 0
      reactions: []
    id: 6461c8aa96259bec21d56f2a
    type: comment
  author: maddiemii
  content: I'm loving this model, it's the best that I've been working with so far.
    I guess one thing that I don't yet understand is how to increase the context window
    so that it remembers the conversation for longer. I'd love to have 4096 or higher
    context window, but I don't yet understand where the limitation is. Is it in the
    base model itself and the way it's trained, or something else? Not a setting I
    can change? Thank you!
  created_at: 2023-05-15 04:52:42+00:00
  edited: false
  hidden: false
  id: 6461c8aa96259bec21d56f2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-15T07:37:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great, glad to hear it.</p>

          <p>I''m afraid that context window is baked into the model and cannot be
          increased.  This applies to nearly all models available at the moment, including
          the major ones. For example, ChatGPT 3.5 has a limit of 4096 tokens, and
          ChatGPT 4 has two versions, one with 8k and one with 32k (not many people
          have access to 32k yet though).</p>

          <p>There are some new models coming out that have much longer context lengths
          or methods to increase context length, like MPT which can be increased up
          to 65K (though I believe it then has massive VRAM requirements.)  But generally
          speaking, existing models have a pre-defined context length which can''t
          be increased.  LLaMA released with a 2k context limit, and all models based
          on it therefore inherit that.</p>

          <p>For existing models there are some techniques that can sometimes help.
          For example LangChain has a summarisation feature whereby in a chat situation
          where you''re asking follow up questions it can automatically summarise
          past interactions to get the most out of your limited context window.</p>

          <p>But other than that there''s not much you can do right now I believe.</p>

          '
        raw: 'Great, glad to hear it.


          I''m afraid that context window is baked into the model and cannot be increased.  This
          applies to nearly all models available at the moment, including the major
          ones. For example, ChatGPT 3.5 has a limit of 4096 tokens, and ChatGPT 4
          has two versions, one with 8k and one with 32k (not many people have access
          to 32k yet though).


          There are some new models coming out that have much longer context lengths
          or methods to increase context length, like MPT which can be increased up
          to 65K (though I believe it then has massive VRAM requirements.)  But generally
          speaking, existing models have a pre-defined context length which can''t
          be increased.  LLaMA released with a 2k context limit, and all models based
          on it therefore inherit that.


          For existing models there are some techniques that can sometimes help. For
          example LangChain has a summarisation feature whereby in a chat situation
          where you''re asking follow up questions it can automatically summarise
          past interactions to get the most out of your limited context window.


          But other than that there''s not much you can do right now I believe.'
        updatedAt: '2023-05-15T07:37:06.401Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - hussainwali1
        - Notel
        - mikeyang01
        - Saugatkafley
    id: 6461e122cf4bf38e231c222e
    type: comment
  author: TheBloke
  content: 'Great, glad to hear it.


    I''m afraid that context window is baked into the model and cannot be increased.  This
    applies to nearly all models available at the moment, including the major ones.
    For example, ChatGPT 3.5 has a limit of 4096 tokens, and ChatGPT 4 has two versions,
    one with 8k and one with 32k (not many people have access to 32k yet though).


    There are some new models coming out that have much longer context lengths or
    methods to increase context length, like MPT which can be increased up to 65K
    (though I believe it then has massive VRAM requirements.)  But generally speaking,
    existing models have a pre-defined context length which can''t be increased.  LLaMA
    released with a 2k context limit, and all models based on it therefore inherit
    that.


    For existing models there are some techniques that can sometimes help. For example
    LangChain has a summarisation feature whereby in a chat situation where you''re
    asking follow up questions it can automatically summarise past interactions to
    get the most out of your limited context window.


    But other than that there''s not much you can do right now I believe.'
  created_at: 2023-05-15 06:37:06+00:00
  edited: false
  hidden: false
  id: 6461e122cf4bf38e231c222e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/wizard-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Increasing the context window
