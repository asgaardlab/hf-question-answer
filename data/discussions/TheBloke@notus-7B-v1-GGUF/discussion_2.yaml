!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alvarobartt
conflicting_files: null
created_at: 2023-12-04 13:09:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
      fullname: Alvaro Bartolome
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvarobartt
      type: user
    createdAt: '2023-12-04T13:09:13.000Z'
    data:
      edited: false
      editors:
      - alvarobartt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8960638642311096
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
          fullname: Alvaro Bartolome
          isHf: false
          isPro: false
          name: alvarobartt
          type: user
        html: "<p>Hi here <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>,\
          \ first of all thanks a lot again for investing the time, effort and compute\
          \ on quantizing our <code>notus-7b-v1</code> models \U0001FAF6\U0001F3FB\
          </p>\n<p>We just wanted to report that it seems that the GGUF variants via\
          \ <code>llama.cpp</code> are not properly encoding the <code>\\n</code>\
          \ probably due to the missing <code>tokenizer.model</code> file, so we've\
          \ ported it from Zephyr (as we share the same tokenizer) and it's already\
          \ available at <a href=\"https://huggingface.co/argilla/notus-7b-v1/blob/main/tokenizer.model\"\
          >https://huggingface.co/argilla/notus-7b-v1/blob/main/tokenizer.model</a>,\
          \ in case you'd like to re-run the GGUF quantization.</p>\n<p>We've tried\
          \ to quantize it using GGUF and Q4 K M, and it worked fine, see <a href=\"\
          https://huggingface.co/alvarobartt/notus-7b-v1-GGUF\">https://huggingface.co/alvarobartt/notus-7b-v1-GGUF</a>.\
          \ If you are not able to re-run we are happy to do so on our compute and\
          \ then share the different GGUF files with you, thanks in advance \U0001F917\
          </p>\n"
        raw: "Hi here @TheBloke, first of all thanks a lot again for investing the\
          \ time, effort and compute on quantizing our `notus-7b-v1` models \U0001FAF6\
          \U0001F3FB\r\n\r\nWe just wanted to report that it seems that the GGUF variants\
          \ via `llama.cpp` are not properly encoding the `\\n` probably due to the\
          \ missing `tokenizer.model` file, so we've ported it from Zephyr (as we\
          \ share the same tokenizer) and it's already available at https://huggingface.co/argilla/notus-7b-v1/blob/main/tokenizer.model,\
          \ in case you'd like to re-run the GGUF quantization.\r\n\r\nWe've tried\
          \ to quantize it using GGUF and Q4 K M, and it worked fine, see https://huggingface.co/alvarobartt/notus-7b-v1-GGUF.\
          \ If you are not able to re-run we are happy to do so on our compute and\
          \ then share the different GGUF files with you, thanks in advance \U0001F917"
        updatedAt: '2023-12-04T13:09:13.839Z'
      numEdits: 0
      reactions: []
    id: 656dcf79f7be0986b4d50ab0
    type: comment
  author: alvarobartt
  content: "Hi here @TheBloke, first of all thanks a lot again for investing the time,\
    \ effort and compute on quantizing our `notus-7b-v1` models \U0001FAF6\U0001F3FB\
    \r\n\r\nWe just wanted to report that it seems that the GGUF variants via `llama.cpp`\
    \ are not properly encoding the `\\n` probably due to the missing `tokenizer.model`\
    \ file, so we've ported it from Zephyr (as we share the same tokenizer) and it's\
    \ already available at https://huggingface.co/argilla/notus-7b-v1/blob/main/tokenizer.model,\
    \ in case you'd like to re-run the GGUF quantization.\r\n\r\nWe've tried to quantize\
    \ it using GGUF and Q4 K M, and it worked fine, see https://huggingface.co/alvarobartt/notus-7b-v1-GGUF.\
    \ If you are not able to re-run we are happy to do so on our compute and then\
    \ share the different GGUF files with you, thanks in advance \U0001F917"
  created_at: 2023-12-04 13:09:13+00:00
  edited: false
  hidden: false
  id: 656dcf79f7be0986b4d50ab0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-04T13:27:12.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9980928301811218
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: "<p>I was going to report his issue.<br>Thanks <span data-props=\"{&quot;user&quot;:&quot;alvarobartt&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/alvarobartt\"\
          >@<span class=\"underline\">alvarobartt</span></a></span>\n\n\t</span></span>\
          \  for reporting.</p>\n"
        raw: "I was going to report his issue. \nThanks @alvarobartt  for reporting.\n"
        updatedAt: '2023-12-04T13:27:12.235Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - alvarobartt
    id: 656dd3b06f39f1565867b2d6
    type: comment
  author: eramax
  content: "I was going to report his issue. \nThanks @alvarobartt  for reporting.\n"
  created_at: 2023-12-04 13:27:12+00:00
  edited: false
  hidden: false
  id: 656dd3b06f39f1565867b2d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-12-04T15:31:43.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9592383503913879
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK I will re-do it now</p>

          '
        raw: OK I will re-do it now
        updatedAt: '2023-12-04T15:31:43.975Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - rkfg
    id: 656df0df9c8778992fe90187
    type: comment
  author: TheBloke
  content: OK I will re-do it now
  created_at: 2023-12-04 15:31:43+00:00
  edited: false
  hidden: false
  id: 656df0df9c8778992fe90187
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
      fullname: Alvaro Bartolome
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvarobartt
      type: user
    createdAt: '2023-12-04T15:33:22.000Z'
    data:
      edited: false
      editors:
      - alvarobartt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8018363118171692
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
          fullname: Alvaro Bartolome
          isHf: false
          isPro: false
          name: alvarobartt
          type: user
        html: '<p>Thanks and sorry for the inconveniences! I could quantize some at
          <a href="https://huggingface.co/alvarobartt/notus-7b-v1-GGUF">https://huggingface.co/alvarobartt/notus-7b-v1-GGUF</a>,
          in case you want to reuse some </p>

          '
        raw: 'Thanks and sorry for the inconveniences! I could quantize some at https://huggingface.co/alvarobartt/notus-7b-v1-GGUF,
          in case you want to reuse some '
        updatedAt: '2023-12-04T15:33:22.161Z'
      numEdits: 0
      reactions: []
    id: 656df142903e16e62b282024
    type: comment
  author: alvarobartt
  content: 'Thanks and sorry for the inconveniences! I could quantize some at https://huggingface.co/alvarobartt/notus-7b-v1-GGUF,
    in case you want to reuse some '
  created_at: 2023-12-04 15:33:22+00:00
  edited: false
  hidden: false
  id: 656df142903e16e62b282024
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-12-04T15:39:52.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9790840148925781
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>GGUFs have been re-made from the updated source repo and now appear
          fine:</p>

          <pre><code> &lt;|system|&gt;

          You are a story writing assistant

          &lt;|user|&gt;

          Write a story about llamas

          &lt;|assistant|&gt;

          Once upon a time in the high Andes Mountains of South America, there lived
          a herd of llamas. These majestic creatures were known for their strong and
          sturdy bodies, long necks, and fluffy brown coats. They spent their days
          grazing on the lush grassy fields and roaming around the rocky terrain.


          Amongst this herd was a young female llama named Luna. She had just turned
          one year old and was eager to explore the world beyond her home. With the
          guidance of her mother, Luna learned how to navigate through the mountain
          paths, recognize different plants for food and water sources.


          As Luna grew older, she became more independent and started venturing out
          further each day. One afternoon, she came across a group of llamas from
          a nearby village who were traveling back home after trading goods. The leader
          of the group was an experienced llama named Tariq, who warmly welcomed Luna
          into their travel party.


          Together, they traversed through rugged mountain passes and crossed rushing
          rivers. Luna learned valuable lessons about trust, respect, and teamwork
          from her new friends. She also discovered that llamas were not just for
          transportation, but were skilled in carrying heavy loads and weaving woolen
          fabrics.


          As the days turned into weeks, Luna became more attached to Tariq''s herd.
          However, she knew deep down that it was time for her to return home to her
          mother and siblings. With a heavy heart, Luna said goodbye to her friends
          and set off on her journey back home.


          On her way back, Luna encountered some unexpected challenges, such as strong
          winds, treacherous cliffs, and wild animals. But with the determination
          she had learned from Tariq''s herd, Luna overcame these obstacles and found
          her way back home safely.


          From that day on, Luna never forgot the valuable lessons she had learned
          about friendship, trust, and adventure. And as she grew older and became
          a mother herself, she passed down these stories and traditions to her own
          offspring, ensuring that the legacy of llamas would continue for generations
          to come. [end of text]

          </code></pre>

          <p>Apologies for not spotting this and thanks for updating your repo.  </p>

          <p>I''ve raised an issue with llama.cpp regarding this <code>&lt;0x0A&gt;</code>
          issue.  It''s an issue caused by the llama.cpp PR for making GGUFs from
          <code>tokenizer.json</code>, when no <code>tokenizer.model</code> is provided.</p>

          '
        raw: "GGUFs have been re-made from the updated source repo and now appear\
          \ fine:\n```\n <|system|>\nYou are a story writing assistant\n<|user|>\n\
          Write a story about llamas\n<|assistant|>\nOnce upon a time in the high\
          \ Andes Mountains of South America, there lived a herd of llamas. These\
          \ majestic creatures were known for their strong and sturdy bodies, long\
          \ necks, and fluffy brown coats. They spent their days grazing on the lush\
          \ grassy fields and roaming around the rocky terrain.\n\nAmongst this herd\
          \ was a young female llama named Luna. She had just turned one year old\
          \ and was eager to explore the world beyond her home. With the guidance\
          \ of her mother, Luna learned how to navigate through the mountain paths,\
          \ recognize different plants for food and water sources.\n\nAs Luna grew\
          \ older, she became more independent and started venturing out further each\
          \ day. One afternoon, she came across a group of llamas from a nearby village\
          \ who were traveling back home after trading goods. The leader of the group\
          \ was an experienced llama named Tariq, who warmly welcomed Luna into their\
          \ travel party.\n\nTogether, they traversed through rugged mountain passes\
          \ and crossed rushing rivers. Luna learned valuable lessons about trust,\
          \ respect, and teamwork from her new friends. She also discovered that llamas\
          \ were not just for transportation, but were skilled in carrying heavy loads\
          \ and weaving woolen fabrics.\n\nAs the days turned into weeks, Luna became\
          \ more attached to Tariq's herd. However, she knew deep down that it was\
          \ time for her to return home to her mother and siblings. With a heavy heart,\
          \ Luna said goodbye to her friends and set off on her journey back home.\n\
          \nOn her way back, Luna encountered some unexpected challenges, such as\
          \ strong winds, treacherous cliffs, and wild animals. But with the determination\
          \ she had learned from Tariq's herd, Luna overcame these obstacles and found\
          \ her way back home safely.\n\nFrom that day on, Luna never forgot the valuable\
          \ lessons she had learned about friendship, trust, and adventure. And as\
          \ she grew older and became a mother herself, she passed down these stories\
          \ and traditions to her own offspring, ensuring that the legacy of llamas\
          \ would continue for generations to come. [end of text]\n```\n\nApologies\
          \ for not spotting this and thanks for updating your repo.  \n\nI've raised\
          \ an issue with llama.cpp regarding this `<0x0A>` issue.  It's an issue\
          \ caused by the llama.cpp PR for making GGUFs from `tokenizer.json`, when\
          \ no `tokenizer.model` is provided."
        updatedAt: '2023-12-04T15:40:02.794Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - alvarobartt
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Phil337
    id: 656df2c8801ed9952ff41301
    type: comment
  author: TheBloke
  content: "GGUFs have been re-made from the updated source repo and now appear fine:\n\
    ```\n <|system|>\nYou are a story writing assistant\n<|user|>\nWrite a story about\
    \ llamas\n<|assistant|>\nOnce upon a time in the high Andes Mountains of South\
    \ America, there lived a herd of llamas. These majestic creatures were known for\
    \ their strong and sturdy bodies, long necks, and fluffy brown coats. They spent\
    \ their days grazing on the lush grassy fields and roaming around the rocky terrain.\n\
    \nAmongst this herd was a young female llama named Luna. She had just turned one\
    \ year old and was eager to explore the world beyond her home. With the guidance\
    \ of her mother, Luna learned how to navigate through the mountain paths, recognize\
    \ different plants for food and water sources.\n\nAs Luna grew older, she became\
    \ more independent and started venturing out further each day. One afternoon,\
    \ she came across a group of llamas from a nearby village who were traveling back\
    \ home after trading goods. The leader of the group was an experienced llama named\
    \ Tariq, who warmly welcomed Luna into their travel party.\n\nTogether, they traversed\
    \ through rugged mountain passes and crossed rushing rivers. Luna learned valuable\
    \ lessons about trust, respect, and teamwork from her new friends. She also discovered\
    \ that llamas were not just for transportation, but were skilled in carrying heavy\
    \ loads and weaving woolen fabrics.\n\nAs the days turned into weeks, Luna became\
    \ more attached to Tariq's herd. However, she knew deep down that it was time\
    \ for her to return home to her mother and siblings. With a heavy heart, Luna\
    \ said goodbye to her friends and set off on her journey back home.\n\nOn her\
    \ way back, Luna encountered some unexpected challenges, such as strong winds,\
    \ treacherous cliffs, and wild animals. But with the determination she had learned\
    \ from Tariq's herd, Luna overcame these obstacles and found her way back home\
    \ safely.\n\nFrom that day on, Luna never forgot the valuable lessons she had\
    \ learned about friendship, trust, and adventure. And as she grew older and became\
    \ a mother herself, she passed down these stories and traditions to her own offspring,\
    \ ensuring that the legacy of llamas would continue for generations to come. [end\
    \ of text]\n```\n\nApologies for not spotting this and thanks for updating your\
    \ repo.  \n\nI've raised an issue with llama.cpp regarding this `<0x0A>` issue.\
    \  It's an issue caused by the llama.cpp PR for making GGUFs from `tokenizer.json`,\
    \ when no `tokenizer.model` is provided."
  created_at: 2023-12-04 15:39:52+00:00
  edited: true
  hidden: false
  id: 656df2c8801ed9952ff41301
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
      fullname: Alvaro Bartolome
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvarobartt
      type: user
    createdAt: '2023-12-04T15:43:12.000Z'
    data:
      edited: false
      editors:
      - alvarobartt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9659097790718079
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
          fullname: Alvaro Bartolome
          isHf: false
          isPro: false
          name: alvarobartt
          type: user
        html: "<p>No worries at all <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ it was on us that didn't realise! Thanks a ton for fixing it straight\
          \ away, it's also not super easy to get the <code>tokenizer.model</code>\
          \ unless it's available as part of the base model, because then the default\
          \ is the fast, Rust-based, tokenizer and there's not a snippet to go from\
          \ fast to slow, while the other can easily be done. I think it has some\
          \ issues attached to it, but maybe worth investigating on <code>llama.cpp</code>\
          \ to tra to use the Rust-based one instead from <code>tokenizer_config.json</code>,\
          \ anyway, thanks a ton \U0001F389</p>\n"
        raw: "No worries at all @TheBloke it was on us that didn't realise! Thanks\
          \ a ton for fixing it straight away, it's also not super easy to get the\
          \ `tokenizer.model` unless it's available as part of the base model, because\
          \ then the default is the fast, Rust-based, tokenizer and there's not a\
          \ snippet to go from fast to slow, while the other can easily be done. I\
          \ think it has some issues attached to it, but maybe worth investigating\
          \ on `llama.cpp` to tra to use the Rust-based one instead from `tokenizer_config.json`,\
          \ anyway, thanks a ton \U0001F389"
        updatedAt: '2023-12-04T15:43:12.262Z'
      numEdits: 0
      reactions: []
    id: 656df3907069c2390a83f120
    type: comment
  author: alvarobartt
  content: "No worries at all @TheBloke it was on us that didn't realise! Thanks a\
    \ ton for fixing it straight away, it's also not super easy to get the `tokenizer.model`\
    \ unless it's available as part of the base model, because then the default is\
    \ the fast, Rust-based, tokenizer and there's not a snippet to go from fast to\
    \ slow, while the other can easily be done. I think it has some issues attached\
    \ to it, but maybe worth investigating on `llama.cpp` to tra to use the Rust-based\
    \ one instead from `tokenizer_config.json`, anyway, thanks a ton \U0001F389"
  created_at: 2023-12-04 15:43:12+00:00
  edited: false
  hidden: false
  id: 656df3907069c2390a83f120
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/60f0608166e5701b80ed3f02/ZSIRRZgthYnTinV1wGE1N.jpeg?w=200&h=200&f=face
      fullname: Alvaro Bartolome
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvarobartt
      type: user
    createdAt: '2023-12-04T15:43:17.000Z'
    data:
      status: closed
    id: 656df395efd0eea7c5ed0874
    type: status-change
  author: alvarobartt
  created_at: 2023-12-04 15:43:17+00:00
  id: 656df395efd0eea7c5ed0874
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-04T15:46:51.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-04T15:47:13.409Z'
      numEdits: 0
      reactions: []
    id: 656df46b9dcedd16d5735e43
    type: comment
  author: Phil337
  content: This comment has been hidden
  created_at: 2023-12-04 15:46:51+00:00
  edited: true
  hidden: true
  id: 656df46b9dcedd16d5735e43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-04T15:48:35.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8402456045150757
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: "<p>Thanks for figuring this out <span data-props=\"{&quot;user&quot;:&quot;alvarobartt&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/alvarobartt\"\
          >@<span class=\"underline\">alvarobartt</span></a></span>\n\n\t</span></span>\
          \ and for the quick fix <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>.\
          \ Just tested the updated notus GGUF and it works great.</p>\n<p>Just in\
          \ case you aren't aware this issue is also impacting.</p>\n<p><a href=\"\
          https://huggingface.co/TheBloke/OpenHermes-2.5-neural-chat-7B-v3-2-7B-GGUF\"\
          >https://huggingface.co/TheBloke/OpenHermes-2.5-neural-chat-7B-v3-2-7B-GGUF</a></p>\n"
        raw: 'Thanks for figuring this out @alvarobartt and for the quick fix @TheBloke.
          Just tested the updated notus GGUF and it works great.


          Just in case you aren''t aware this issue is also impacting.


          https://huggingface.co/TheBloke/OpenHermes-2.5-neural-chat-7B-v3-2-7B-GGUF

          '
        updatedAt: '2023-12-04T15:48:35.337Z'
      numEdits: 0
      reactions: []
    id: 656df4d38dffbab5af861aa8
    type: comment
  author: Phil337
  content: 'Thanks for figuring this out @alvarobartt and for the quick fix @TheBloke.
    Just tested the updated notus GGUF and it works great.


    Just in case you aren''t aware this issue is also impacting.


    https://huggingface.co/TheBloke/OpenHermes-2.5-neural-chat-7B-v3-2-7B-GGUF

    '
  created_at: 2023-12-04 15:48:35+00:00
  edited: false
  hidden: false
  id: 656df4d38dffbab5af861aa8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/notus-7B-v1-GGUF
repo_type: model
status: closed
target_branch: null
title: GGUF adds `<0x0A>` during tokenization due to missing `tokenizer.model`
