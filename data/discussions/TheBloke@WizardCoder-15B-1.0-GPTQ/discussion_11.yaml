!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kshitijyad
conflicting_files: null
created_at: 2023-07-15 22:01:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f58ca5762e0ac06a6d5580b34ca32f1f.svg
      fullname: Kshitij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kshitijyad
      type: user
    createdAt: '2023-07-15T23:01:41.000Z'
    data:
      edited: false
      editors:
      - kshitijyad
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4121798276901245
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f58ca5762e0ac06a6d5580b34ca32f1f.svg
          fullname: Kshitij
          isHf: false
          isPro: false
          name: kshitijyad
          type: user
        html: '<p>Hi, can someone help me debug this?<br>I am trying to deploy wizard
          coder on Sagemaker using the recommended deploy code, I am getting the following
          error - </p>

          <p>[RuntimeError: found uninitialized parameters in model : [''transformer.h.0.attn.c_attn.weight'',
          ''transformer.h.0.attn.c_proj.weight'', ''transformer.h.0.mlp.c_fc.weight'',
          ''transformer.h.0.mlp.c_proj.weight'', ''transformer.h.1.attn.c_attn.weight'',
          ](RuntimeError: found uninitialized parameters in model : [''transformer.h.0.attn.c_attn.weight'',
          ''transformer.h.0.attn.c_proj.weight'', ''transformer.h.0.mlp.c_fc.weight'',
          ''transformer.h.0.mlp.c_proj.weight'', ''transformer.h.1.attn.c_attn.weight'',
          ''transformer.h.1.attn.c_proj.weight'', ''transformer.h.1.mlp.c_fc.weight'',
          ''transformer.h.1.mlp.c_proj.weight'', ''transformer.h.2.attn.c_attn.weight'',
          ''transformer.h.2.attn.c_proj.weight'', ''transformer.h.2.mlp.c_fc.weight'',
          ''transformer.h.2.mlp.c_proj.weight'', ''transformer.h.3.attn.c_attn.weight'',
          ''transformer.h.3.attn.c_proj.weight'', ''transformer.h.3.mlp.c_fc.weight'',
          ''transformer.h.3.mlp.c_proj.weight''........</p>

          <p>Recommended Deploy code - </p>

          <p>import json<br>import sagemaker<br>import boto3<br>from sagemaker.huggingface
          import HuggingFaceModel, get_huggingface_llm_image_uri</p>

          <p>try:<br>    role = sagemaker.get_execution_role()<br>except ValueError:<br>    iam
          = boto3.client(''iam'')<br>    role = iam.get_role(RoleName=''sagemaker_execution_role'')[''Role''][''Arn'']</p>

          <h1 id="hub-model-configuration-httpshuggingfacecomodels">Hub Model configuration.
          <a href="https://huggingface.co/models">https://huggingface.co/models</a></h1>

          <p>hub = {<br>    ''HF_MODEL_ID'':''TheBloke/WizardCoder-15B-1.0-GPTQ'',<br>    ''SM_NUM_GPUS'':
          json.dumps(1)<br>}</p>

          <h1 id="create-hugging-face-model-class">create Hugging Face Model Class</h1>

          <p>huggingface_model = HuggingFaceModel(<br>    image_uri=get_huggingface_llm_image_uri("huggingface",version="0.8.2"),<br>    env=hub,<br>    role=role,<br>)</p>

          <h1 id="deploy-model-to-sagemaker-inference">deploy model to SageMaker Inference</h1>

          <p>predictor = huggingface_model.deploy(<br>    initial_instance_count=1,<br>    instance_type="ml.g5.2xlarge",<br>    container_startup_health_check_timeout=300,<br>  )</p>

          <h1 id="send-request">send request</h1>

          <p>predictor.predict({<br>    "inputs": "My name is Julien and I like to",<br>})</p>

          '
        raw: "Hi, can someone help me debug this? \r\nI am trying to deploy wizard\
          \ coder on Sagemaker using the recommended deploy code, I am getting the\
          \ following error - \r\n\r\n\r\n[RuntimeError: found uninitialized parameters\
          \ in model : ['transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.weight',\
          \ 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.weight',\
          \ 'transformer.h.1.attn.c_attn.weight', ](RuntimeError: found uninitialized\
          \ parameters in model : ['transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.weight',\
          \ 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.weight',\
          \ 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.weight',\
          \ 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.weight',\
          \ 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.weight',\
          \ 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.weight',\
          \ 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.weight',\
          \ 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.weight'........\r\
          \n\r\n\r\n\r\nRecommended Deploy code - \r\n\r\nimport json\r\nimport sagemaker\r\
          \nimport boto3\r\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\r\
          \n\r\ntry:\r\n\trole = sagemaker.get_execution_role()\r\nexcept ValueError:\r\
          \n\tiam = boto3.client('iam')\r\n\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\r\
          \n\r\n# Hub Model configuration. https://huggingface.co/models\r\nhub =\
          \ {\r\n\t'HF_MODEL_ID':'TheBloke/WizardCoder-15B-1.0-GPTQ',\r\n\t'SM_NUM_GPUS':\
          \ json.dumps(1)\r\n}\r\n\r\n# create Hugging Face Model Class\r\nhuggingface_model\
          \ = HuggingFaceModel(\r\n\timage_uri=get_huggingface_llm_image_uri(\"huggingface\"\
          ,version=\"0.8.2\"),\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n\r\n# deploy\
          \ model to SageMaker Inference\r\npredictor = huggingface_model.deploy(\r\
          \n\tinitial_instance_count=1,\r\n\tinstance_type=\"ml.g5.2xlarge\",\r\n\t\
          container_startup_health_check_timeout=300,\r\n  )\r\n  \r\n# send request\r\
          \npredictor.predict({\r\n\t\"inputs\": \"My name is Julien and I like to\"\
          ,\r\n})"
        updatedAt: '2023-07-15T23:01:41.628Z'
      numEdits: 0
      reactions: []
    id: 64b32555a17e4a0519c14d01
    type: comment
  author: kshitijyad
  content: "Hi, can someone help me debug this? \r\nI am trying to deploy wizard coder\
    \ on Sagemaker using the recommended deploy code, I am getting the following error\
    \ - \r\n\r\n\r\n[RuntimeError: found uninitialized parameters in model : ['transformer.h.0.attn.c_attn.weight',\
    \ 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.weight',\
    \ 'transformer.h.1.attn.c_attn.weight', ](RuntimeError: found uninitialized parameters\
    \ in model : ['transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.weight',\
    \ 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.weight',\
    \ 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.weight',\
    \ 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.weight',\
    \ 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.weight',\
    \ 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.weight'........\r\
    \n\r\n\r\n\r\nRecommended Deploy code - \r\n\r\nimport json\r\nimport sagemaker\r\
    \nimport boto3\r\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\r\
    \n\r\ntry:\r\n\trole = sagemaker.get_execution_role()\r\nexcept ValueError:\r\n\
    \tiam = boto3.client('iam')\r\n\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\r\
    \n\r\n# Hub Model configuration. https://huggingface.co/models\r\nhub = {\r\n\t\
    'HF_MODEL_ID':'TheBloke/WizardCoder-15B-1.0-GPTQ',\r\n\t'SM_NUM_GPUS': json.dumps(1)\r\
    \n}\r\n\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\
    \n\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"0.8.2\"\
    ),\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n\r\n# deploy model to SageMaker Inference\r\
    \npredictor = huggingface_model.deploy(\r\n\tinitial_instance_count=1,\r\n\tinstance_type=\"\
    ml.g5.2xlarge\",\r\n\tcontainer_startup_health_check_timeout=300,\r\n  )\r\n \
    \ \r\n# send request\r\npredictor.predict({\r\n\t\"inputs\": \"My name is Julien\
    \ and I like to\",\r\n})"
  created_at: 2023-07-15 22:01:41+00:00
  edited: false
  hidden: false
  id: 64b32555a17e4a0519c14d01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-16T08:38:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9648850560188293
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I have no experience of Sagemaker at all I''m afraid.  But I''m
          not sure this model is going to work.  Firstly it''s a GPTQ model, and I''m
          not sure if the Sagemaker code supports those?</p>

          <p>Text Generation Inference, the Hugging Face inference library did recently
          add support for GPTQ.  And maybe Sagemaker is using that?  But I don''t
          believe TGI supports Starcoder models (in GPTQ anyway), and this is a Starcoder
          model.</p>

          <p>This Github Issue describes how to get my GPTQs working with Text Generation
          Inference - currently environment variables are required: <a rel="nofollow"
          href="https://github.com/huggingface/text-generation-inference/issues/601">https://github.com/huggingface/text-generation-inference/issues/601</a></p>

          <p>But I''ve already had it reported to me that TGI doesn''t work with Starcoder
          so even if Sagemaker is using TGI, I wouldn''t expect it to work with this
          specific model.  Try one of my Llama GPTQs instead.</p>

          '
        raw: 'I have no experience of Sagemaker at all I''m afraid.  But I''m not
          sure this model is going to work.  Firstly it''s a GPTQ model, and I''m
          not sure if the Sagemaker code supports those?


          Text Generation Inference, the Hugging Face inference library did recently
          add support for GPTQ.  And maybe Sagemaker is using that?  But I don''t
          believe TGI supports Starcoder models (in GPTQ anyway), and this is a Starcoder
          model.


          This Github Issue describes how to get my GPTQs working with Text Generation
          Inference - currently environment variables are required: https://github.com/huggingface/text-generation-inference/issues/601


          But I''ve already had it reported to me that TGI doesn''t work with Starcoder
          so even if Sagemaker is using TGI, I wouldn''t expect it to work with this
          specific model.  Try one of my Llama GPTQs instead.'
        updatedAt: '2023-07-16T08:38:21.782Z'
      numEdits: 0
      reactions: []
    id: 64b3ac7dd52d67c01ce54d02
    type: comment
  author: TheBloke
  content: 'I have no experience of Sagemaker at all I''m afraid.  But I''m not sure
    this model is going to work.  Firstly it''s a GPTQ model, and I''m not sure if
    the Sagemaker code supports those?


    Text Generation Inference, the Hugging Face inference library did recently add
    support for GPTQ.  And maybe Sagemaker is using that?  But I don''t believe TGI
    supports Starcoder models (in GPTQ anyway), and this is a Starcoder model.


    This Github Issue describes how to get my GPTQs working with Text Generation Inference
    - currently environment variables are required: https://github.com/huggingface/text-generation-inference/issues/601


    But I''ve already had it reported to me that TGI doesn''t work with Starcoder
    so even if Sagemaker is using TGI, I wouldn''t expect it to work with this specific
    model.  Try one of my Llama GPTQs instead.'
  created_at: 2023-07-16 07:38:21+00:00
  edited: false
  hidden: false
  id: 64b3ac7dd52d67c01ce54d02
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/WizardCoder-15B-1.0-GPTQ
repo_type: model
status: open
target_branch: null
title: Runtime error from Sagemaker deploy
