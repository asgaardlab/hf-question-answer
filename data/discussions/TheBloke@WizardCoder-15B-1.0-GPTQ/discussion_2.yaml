!!python/object:huggingface_hub.community.DiscussionWithDetails
author: freakontrol
conflicting_files: null
created_at: 2023-06-15 17:05:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/478156c2f7d3972f2a0cf8bd2dc00ffb.svg
      fullname: thomas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: freakontrol
      type: user
    createdAt: '2023-06-15T18:05:57.000Z'
    data:
      edited: false
      editors:
      - freakontrol
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5807123780250549
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/478156c2f7d3972f2a0cf8bd2dc00ffb.svg
          fullname: thomas
          isHf: false
          isPro: false
          name: freakontrol
          type: user
        html: "<p>Hi, first of all thank you for your work, it really helps people\
          \ experimenting with llm's...<br>I get error when trying to run this model\
          \ in text-generation-webui:  </p>\n<pre><code>Traceback (most recent call\
          \ last): File \u201C/app/server.py\u201D, line 70, in load_model_wrapper\
          \ shared.model, shared.tokenizer = load_model(shared.model_name) File \u201C\
          /app/modules/models.py\u201D, line 94, in load_model output = load_func(model_name)\
          \ File \u201C/app/modules/models.py\u201D, line 296, in AutoGPTQ_loader\
          \ return modules.AutoGPTQ_loader.load_quantized(model_name) File \u201C\
          /app/modules/AutoGPTQ_loader.py\u201D, line 60, in load_quantized model.embed_tokens\
          \ = model.model.model.embed_tokens File \u201C/app/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u201D\
          , line 1614, in getattr raise AttributeError(\u201C\u2018{}\u2019 object\
          \ has no attribute \u2018{}\u2019\u201D.format( AttributeError: \u2018GPTBigCodeForCausalLM\u2019\
          \ object has no attribute \u2018model\u2019\n</code></pre>\n<p>Other gptq\
          \ models load fine.<br>Am I missing something?  Thankyou again</p>\n"
        raw: "Hi, first of all thank you for your work, it really helps people experimenting\
          \ with llm's...\r\nI get error when trying to run this model in text-generation-webui:\
          \  \r\n``` \r\nTraceback (most recent call last): File \u201C/app/server.py\u201D\
          , line 70, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201C/app/modules/models.py\u201D, line 94, in load_model output\
          \ = load_func(model_name) File \u201C/app/modules/models.py\u201D, line\
          \ 296, in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
          \ File \u201C/app/modules/AutoGPTQ_loader.py\u201D, line 60, in load_quantized\
          \ model.embed_tokens = model.model.model.embed_tokens File \u201C/app/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u201D\
          , line 1614, in getattr raise AttributeError(\u201C\u2018{}\u2019 object\
          \ has no attribute \u2018{}\u2019\u201D.format( AttributeError: \u2018GPTBigCodeForCausalLM\u2019\
          \ object has no attribute \u2018model\u2019\r\n```\r\nOther gptq models\
          \ load fine.\r\nAm I missing something?  Thankyou again"
        updatedAt: '2023-06-15T18:05:57.967Z'
      numEdits: 0
      reactions: []
    id: 648b5305c1a3f9817fd2e99c
    type: comment
  author: freakontrol
  content: "Hi, first of all thank you for your work, it really helps people experimenting\
    \ with llm's...\r\nI get error when trying to run this model in text-generation-webui:\
    \  \r\n``` \r\nTraceback (most recent call last): File \u201C/app/server.py\u201D\
    , line 70, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
    \ File \u201C/app/modules/models.py\u201D, line 94, in load_model output = load_func(model_name)\
    \ File \u201C/app/modules/models.py\u201D, line 296, in AutoGPTQ_loader return\
    \ modules.AutoGPTQ_loader.load_quantized(model_name) File \u201C/app/modules/AutoGPTQ_loader.py\u201D\
    , line 60, in load_quantized model.embed_tokens = model.model.model.embed_tokens\
    \ File \u201C/app/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u201D\
    , line 1614, in getattr raise AttributeError(\u201C\u2018{}\u2019 object has no\
    \ attribute \u2018{}\u2019\u201D.format( AttributeError: \u2018GPTBigCodeForCausalLM\u2019\
    \ object has no attribute \u2018model\u2019\r\n```\r\nOther gptq models load fine.\r\
    \nAm I missing something?  Thankyou again"
  created_at: 2023-06-15 17:05:57+00:00
  edited: false
  hidden: false
  id: 648b5305c1a3f9817fd2e99c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/478156c2f7d3972f2a0cf8bd2dc00ffb.svg
      fullname: thomas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: freakontrol
      type: user
    createdAt: '2023-06-15T18:27:13.000Z'
    data:
      edited: false
      editors:
      - freakontrol
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6828091144561768
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/478156c2f7d3972f2a0cf8bd2dc00ffb.svg
          fullname: thomas
          isHf: false
          isPro: false
          name: freakontrol
          type: user
        html: '<p>Never mind I think it is related to this <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/issues/2655">issue</a>
          of oobabooga webui.</p>

          '
        raw: Never mind I think it is related to this [issue](https://github.com/oobabooga/text-generation-webui/issues/2655)
          of oobabooga webui.
        updatedAt: '2023-06-15T18:27:13.684Z'
      numEdits: 0
      reactions: []
    id: 648b5801412f81cc0c1dd043
    type: comment
  author: freakontrol
  content: Never mind I think it is related to this [issue](https://github.com/oobabooga/text-generation-webui/issues/2655)
    of oobabooga webui.
  created_at: 2023-06-15 17:27:13+00:00
  edited: false
  hidden: false
  id: 648b5801412f81cc0c1dd043
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-15T18:29:02.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8884462714195251
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes it is - update to latest text-gen-ui and it should work now</p>

          '
        raw: Yes it is - update to latest text-gen-ui and it should work now
        updatedAt: '2023-06-15T18:29:02.526Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nurb432
    id: 648b586edf4710674c614035
    type: comment
  author: TheBloke
  content: Yes it is - update to latest text-gen-ui and it should work now
  created_at: 2023-06-15 17:29:02+00:00
  edited: false
  hidden: false
  id: 648b586edf4710674c614035
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea0b136d6a26b744b4e9ebbd59eecc.svg
      fullname: Red Leader
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedLeader721
      type: user
    createdAt: '2023-06-16T15:43:34.000Z'
    data:
      edited: false
      editors:
      - RedLeader721
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.992364764213562
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea0b136d6a26b744b4e9ebbd59eecc.svg
          fullname: Red Leader
          isHf: false
          isPro: false
          name: RedLeader721
          type: user
        html: '<p>It works but I had to set wbits to 4 and groupsize to 128.</p>

          '
        raw: It works but I had to set wbits to 4 and groupsize to 128.
        updatedAt: '2023-06-16T15:43:34.736Z'
      numEdits: 0
      reactions: []
    id: 648c8326aeff93472199ff4b
    type: comment
  author: RedLeader721
  content: It works but I had to set wbits to 4 and groupsize to 128.
  created_at: 2023-06-16 14:43:34+00:00
  edited: false
  hidden: false
  id: 648c8326aeff93472199ff4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea0b136d6a26b744b4e9ebbd59eecc.svg
      fullname: Red Leader
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedLeader721
      type: user
    createdAt: '2023-06-16T15:48:41.000Z'
    data:
      edited: false
      editors:
      - RedLeader721
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9750847816467285
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea0b136d6a26b744b4e9ebbd59eecc.svg
          fullname: Red Leader
          isHf: false
          isPro: false
          name: RedLeader721
          type: user
        html: '<p>now I''m getting errors.  What setting should I use? Nothing was
          set by default as described in the instructions.<br>RuntimeError: expected
          scalar type Float but found Half</p>

          '
        raw: 'now I''m getting errors.  What setting should I use? Nothing was set
          by default as described in the instructions.

          RuntimeError: expected scalar type Float but found Half'
        updatedAt: '2023-06-16T15:48:41.722Z'
      numEdits: 0
      reactions: []
    id: 648c845921a4e18834e08f1d
    type: comment
  author: RedLeader721
  content: 'now I''m getting errors.  What setting should I use? Nothing was set by
    default as described in the instructions.

    RuntimeError: expected scalar type Float but found Half'
  created_at: 2023-06-16 14:48:41+00:00
  edited: false
  hidden: false
  id: 648c845921a4e18834e08f1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea0b136d6a26b744b4e9ebbd59eecc.svg
      fullname: Red Leader
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RedLeader721
      type: user
    createdAt: '2023-06-16T18:32:20.000Z'
    data:
      edited: false
      editors:
      - RedLeader721
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9240800142288208
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea0b136d6a26b744b4e9ebbd59eecc.svg
          fullname: Red Leader
          isHf: false
          isPro: false
          name: RedLeader721
          type: user
        html: '<p>Found my problem.  gptq-for-llama was checked for some reason.  Unchecked
          that and everything works now.  Be sure to set the Instruction Template
          in the Chat tab to "Alpaca", and on the Parameters tab, set temperature
          to 1 and top_p to 0.95</p>

          '
        raw: Found my problem.  gptq-for-llama was checked for some reason.  Unchecked
          that and everything works now.  Be sure to set the Instruction Template
          in the Chat tab to "Alpaca", and on the Parameters tab, set temperature
          to 1 and top_p to 0.95
        updatedAt: '2023-06-16T18:32:20.824Z'
      numEdits: 0
      reactions: []
    id: 648caab44c07e373a8d211ae
    type: comment
  author: RedLeader721
  content: Found my problem.  gptq-for-llama was checked for some reason.  Unchecked
    that and everything works now.  Be sure to set the Instruction Template in the
    Chat tab to "Alpaca", and on the Parameters tab, set temperature to 1 and top_p
    to 0.95
  created_at: 2023-06-16 17:32:20+00:00
  edited: false
  hidden: false
  id: 648caab44c07e373a8d211ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
      fullname: Jan Badertscher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: underlines
      type: user
    createdAt: '2023-06-17T20:08:23.000Z'
    data:
      edited: false
      editors:
      - underlines
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8828442692756653
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9b00218abc1e8d613db2ca2674a57f7.svg
          fullname: Jan Badertscher
          isHf: false
          isPro: false
          name: underlines
          type: user
        html: '<p>The instruction template mentioned by the original hugging face
          repo is :</p>

          <pre><code>Below is an instruction that describes a task. Write a response
          that appropriately completes the request.


          ### Instruction:

          {instruction}


          ### Response:

          </code></pre>

          <p>In oobabooga, that''s almost equal to <code>Wizard-Mega WizardLM</code>
          except for it should have no \n between bot and bot-message:</p>

          <pre><code>"user: ""### Instruction:""

          bot: ""### Response:""

          turn_template: ""&lt;|user|&gt;\n&lt;|user-message|&gt;\n\n&lt;|bot|&gt;&lt;|bot-message|&gt;\n\n""

          context: ""Below is an instruction that describes a task. Write a response
          that appropriately completes the request.\n\n"""

          </code></pre>

          <p>Did you face problems with this?</p>

          '
        raw: 'The instruction template mentioned by the original hugging face repo
          is :


          ```

          Below is an instruction that describes a task. Write a response that appropriately
          completes the request.


          ### Instruction:

          {instruction}


          ### Response:

          ```


          In oobabooga, that''s almost equal to `Wizard-Mega WizardLM` except for
          it should have no \n between bot and bot-message:

          ```

          "user: ""### Instruction:""

          bot: ""### Response:""

          turn_template: ""<|user|>\n<|user-message|>\n\n<|bot|><|bot-message|>\n\n""

          context: ""Below is an instruction that describes a task. Write a response
          that appropriately completes the request.\n\n"""

          ```


          Did you face problems with this?'
        updatedAt: '2023-06-17T20:08:23.709Z'
      numEdits: 0
      reactions: []
    id: 648e12b724746db241f2846d
    type: comment
  author: underlines
  content: 'The instruction template mentioned by the original hugging face repo is
    :


    ```

    Below is an instruction that describes a task. Write a response that appropriately
    completes the request.


    ### Instruction:

    {instruction}


    ### Response:

    ```


    In oobabooga, that''s almost equal to `Wizard-Mega WizardLM` except for it should
    have no \n between bot and bot-message:

    ```

    "user: ""### Instruction:""

    bot: ""### Response:""

    turn_template: ""<|user|>\n<|user-message|>\n\n<|bot|><|bot-message|>\n\n""

    context: ""Below is an instruction that describes a task. Write a response that
    appropriately completes the request.\n\n"""

    ```


    Did you face problems with this?'
  created_at: 2023-06-17 19:08:23+00:00
  edited: false
  hidden: false
  id: 648e12b724746db241f2846d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-17T20:28:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9609350562095642
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''ve always written Alpaca style prompts as:</p>

          <pre><code>### Instruction: prompt

          </code></pre>

          <p>and not </p>

          <pre><code>### Instruction:

          prompt

          </code></pre>

          <p>And haven''t noticed any problems.  So I''m pretty sure it''s fine.  However
          I haven''t specifically tested between the two.  If you want to be 100%
          accurate to their suggestion, you should be able to edit the turn template
          in ooba to exactly match theirs?</p>

          '
        raw: "I've always written Alpaca style prompts as:\n```\n### Instruction:\
          \ prompt\n```\nand not \n```\n### Instruction:\nprompt\n```\n\nAnd haven't\
          \ noticed any problems.  So I'm pretty sure it's fine.  However I haven't\
          \ specifically tested between the two.  If you want to be 100% accurate\
          \ to their suggestion, you should be able to edit the turn template in ooba\
          \ to exactly match theirs?"
        updatedAt: '2023-06-17T20:28:50.820Z'
      numEdits: 0
      reactions: []
    id: 648e1782a00aa3b29c56c34f
    type: comment
  author: TheBloke
  content: "I've always written Alpaca style prompts as:\n```\n### Instruction: prompt\n\
    ```\nand not \n```\n### Instruction:\nprompt\n```\n\nAnd haven't noticed any problems.\
    \  So I'm pretty sure it's fine.  However I haven't specifically tested between\
    \ the two.  If you want to be 100% accurate to their suggestion, you should be\
    \ able to edit the turn template in ooba to exactly match theirs?"
  created_at: 2023-06-17 19:28:50+00:00
  edited: false
  hidden: false
  id: 648e1782a00aa3b29c56c34f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637f218bd8a62e315590dadc/Yzi4rRgrPok8Hf0SfymA4.jpeg?w=200&h=200&f=face
      fullname: zuriher mallick
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zuriher786
      type: user
    createdAt: '2023-11-01T19:23:58.000Z'
    data:
      edited: false
      editors:
      - zuriher786
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.30692654848098755
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637f218bd8a62e315590dadc/Yzi4rRgrPok8Hf0SfymA4.jpeg?w=200&h=200&f=face
          fullname: zuriher mallick
          isHf: false
          isPro: false
          name: zuriher786
          type: user
        html: '<p>Traceback (most recent call last):<br>  File "E:\AI\TextGen\text-generation-webui\installer_files\env\Lib\site-packages\transformers\modeling_utils.py",
          line 484, in load_state_dict<br>    return torch.load(checkpoint_file, map_location=map_location)<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "E:\AI\TextGen\text-generation-webui\installer_files\env\Lib\site-packages\torch\serialization.py",
          line 993, in load<br>    with _open_zipfile_reader(opened_file) as opened_zipfile:<br>         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "E:\AI\TextGen\text-generation-webui\installer_files\env\Lib\site-packages\torch\serialization.py",
          line 447, in <strong>init</strong><br>    super().<strong>init</strong>(torch._C.PyTorchFileReader(name_or_buffer))<br>                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>RuntimeError:
          PytorchStreamReader failed reading zip archive: failed finding central directory</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "E:\AI\TextGen\text-generation-webui\installer_files\env\Lib\site-packages\transformers\modeling_utils.py",
          line 488, in load_state_dict<br>    if f.read(7) == "version":<br>       ^^^^^^^^^<br>  File
          "E:\AI\TextGen\text-generation-webui\installer_files\env\Lib\encodings\cp1252.py",
          line 23, in decode<br>    return codecs.charmap_decode(input,self.errors,decoding_table)[0]<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>UnicodeDecodeError:
          ''charmap'' codec can''t decode byte 0x90 in position 1284: character maps
          to </p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "E:\AI\TextGen\text-generation-webui\modules\ui_model_menu.py",
          line 206, in load_model_wrapper<br>    shared.model, shared.tokenizer =
          load_model(shared.model_name, loader)<br>                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "E:\AI\TextGen\text-generation-webui\modules\models.py", line 84, in load_model<br>    output
          = load_func_map<a rel="nofollow" href="model_name">loader</a><br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "E:\AI\TextGen\text-generation-webui\modules\models.py", line 141, in huggingface_loader<br>    model
          = LoaderClass.from_pretrained(path_to_model, **params)<br>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "E:\AI\TextGen\text-generation-webui\installer_files\env\Lib\site-packages\transformers\models\auto\auto_factory.py",
          line 565, in from_pretrained<br>    return model_class.from_pretrained(<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "E:\AI\TextGen\text-generation-webui\installer_files\env\Lib\site-packages\transformers\modeling_utils.py",
          line 3019, in from_pretrained<br>    state_dict = load_state_dict(resolved_archive_file)<br>                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "E:\AI\TextGen\text-generation-webui\installer_files\env\Lib\site-packages\transformers\modeling_utils.py",
          line 500, in load_state_dict<br>    raise OSError(<br>OSError: Unable to
          load weights from pytorch checkpoint file for ''models\databricks_dolly-v2-12b\pytorch_model.bin''
          at ''models\databricks_dolly-v2-12b\pytorch_model.bin''. If you tried to
          load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.</p>

          '
        raw: "Traceback (most recent call last):\n  File \"E:\\AI\\TextGen\\text-generation-webui\\\
          installer_files\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\"\
          , line 484, in load_state_dict\n    return torch.load(checkpoint_file, map_location=map_location)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \"E:\\AI\\TextGen\\text-generation-webui\\installer_files\\env\\Lib\\\
          site-packages\\torch\\serialization.py\", line 993, in load\n    with _open_zipfile_reader(opened_file)\
          \ as opened_zipfile:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \"E:\\AI\\TextGen\\text-generation-webui\\installer_files\\env\\Lib\\\
          site-packages\\torch\\serialization.py\", line 447, in __init__\n    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\n\
          \                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError:\
          \ PytorchStreamReader failed reading zip archive: failed finding central\
          \ directory\n\nDuring handling of the above exception, another exception\
          \ occurred:\n\nTraceback (most recent call last):\n  File \"E:\\AI\\TextGen\\\
          text-generation-webui\\installer_files\\env\\Lib\\site-packages\\transformers\\\
          modeling_utils.py\", line 488, in load_state_dict\n    if f.read(7) == \"\
          version\":\n       ^^^^^^^^^\n  File \"E:\\AI\\TextGen\\text-generation-webui\\\
          installer_files\\env\\Lib\\encodings\\cp1252.py\", line 23, in decode\n\
          \    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nUnicodeDecodeError:\
          \ 'charmap' codec can't decode byte 0x90 in position 1284: character maps\
          \ to <undefined>\n\nDuring handling of the above exception, another exception\
          \ occurred:\n\nTraceback (most recent call last):\n  File \"E:\\AI\\TextGen\\\
          text-generation-webui\\modules\\ui_model_menu.py\", line 206, in load_model_wrapper\n\
          \    shared.model, shared.tokenizer = load_model(shared.model_name, loader)\n\
          \                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"E:\\AI\\TextGen\\text-generation-webui\\modules\\models.py\",\
          \ line 84, in load_model\n    output = load_func_map[loader](model_name)\n\
          \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\AI\\TextGen\\\
          text-generation-webui\\modules\\models.py\", line 141, in huggingface_loader\n\
          \    model = LoaderClass.from_pretrained(path_to_model, **params)\n    \
          \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
          E:\\AI\\TextGen\\text-generation-webui\\installer_files\\env\\Lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\", line 565, in from_pretrained\n\
          \    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"E:\\AI\\TextGen\\text-generation-webui\\installer_files\\env\\\
          Lib\\site-packages\\transformers\\modeling_utils.py\", line 3019, in from_pretrained\n\
          \    state_dict = load_state_dict(resolved_archive_file)\n             \
          \    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\AI\\TextGen\\\
          text-generation-webui\\installer_files\\env\\Lib\\site-packages\\transformers\\\
          modeling_utils.py\", line 500, in load_state_dict\n    raise OSError(\n\
          OSError: Unable to load weights from pytorch checkpoint file for 'models\\\
          databricks_dolly-v2-12b\\pytorch_model.bin' at 'models\\databricks_dolly-v2-12b\\\
          pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint,\
          \ please set from_tf=True."
        updatedAt: '2023-11-01T19:23:58.925Z'
      numEdits: 0
      reactions: []
    id: 6542a5ced1386fcfca38a029
    type: comment
  author: zuriher786
  content: "Traceback (most recent call last):\n  File \"E:\\AI\\TextGen\\text-generation-webui\\\
    installer_files\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line\
    \ 484, in load_state_dict\n    return torch.load(checkpoint_file, map_location=map_location)\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
    E:\\AI\\TextGen\\text-generation-webui\\installer_files\\env\\Lib\\site-packages\\\
    torch\\serialization.py\", line 993, in load\n    with _open_zipfile_reader(opened_file)\
    \ as opened_zipfile:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\\
    AI\\TextGen\\text-generation-webui\\installer_files\\env\\Lib\\site-packages\\\
    torch\\serialization.py\", line 447, in __init__\n    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\n\
    \                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError:\
    \ PytorchStreamReader failed reading zip archive: failed finding central directory\n\
    \nDuring handling of the above exception, another exception occurred:\n\nTraceback\
    \ (most recent call last):\n  File \"E:\\AI\\TextGen\\text-generation-webui\\\
    installer_files\\env\\Lib\\site-packages\\transformers\\modeling_utils.py\", line\
    \ 488, in load_state_dict\n    if f.read(7) == \"version\":\n       ^^^^^^^^^\n\
    \  File \"E:\\AI\\TextGen\\text-generation-webui\\installer_files\\env\\Lib\\\
    encodings\\cp1252.py\", line 23, in decode\n    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nUnicodeDecodeError:\
    \ 'charmap' codec can't decode byte 0x90 in position 1284: character maps to <undefined>\n\
    \nDuring handling of the above exception, another exception occurred:\n\nTraceback\
    \ (most recent call last):\n  File \"E:\\AI\\TextGen\\text-generation-webui\\\
    modules\\ui_model_menu.py\", line 206, in load_model_wrapper\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name, loader)\n                 \
    \                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\AI\\\
    TextGen\\text-generation-webui\\modules\\models.py\", line 84, in load_model\n\
    \    output = load_func_map[loader](model_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"E:\\AI\\TextGen\\text-generation-webui\\modules\\models.py\", line 141,\
    \ in huggingface_loader\n    model = LoaderClass.from_pretrained(path_to_model,\
    \ **params)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"E:\\AI\\TextGen\\text-generation-webui\\installer_files\\env\\Lib\\\
    site-packages\\transformers\\models\\auto\\auto_factory.py\", line 565, in from_pretrained\n\
    \    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"E:\\AI\\TextGen\\text-generation-webui\\installer_files\\env\\Lib\\\
    site-packages\\transformers\\modeling_utils.py\", line 3019, in from_pretrained\n\
    \    state_dict = load_state_dict(resolved_archive_file)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"E:\\AI\\TextGen\\text-generation-webui\\installer_files\\env\\Lib\\\
    site-packages\\transformers\\modeling_utils.py\", line 500, in load_state_dict\n\
    \    raise OSError(\nOSError: Unable to load weights from pytorch checkpoint file\
    \ for 'models\\databricks_dolly-v2-12b\\pytorch_model.bin' at 'models\\databricks_dolly-v2-12b\\\
    pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint,\
    \ please set from_tf=True."
  created_at: 2023-11-01 18:23:58+00:00
  edited: false
  hidden: false
  id: 6542a5ced1386fcfca38a029
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-01T20:38:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9048039317131042
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Looks like a partially incomplete download, please try downloading
          again</p>

          '
        raw: Looks like a partially incomplete download, please try downloading again
        updatedAt: '2023-11-01T20:38:20.712Z'
      numEdits: 0
      reactions: []
    id: 6542b73c72defaf6593c2a63
    type: comment
  author: TheBloke
  content: Looks like a partially incomplete download, please try downloading again
  created_at: 2023-11-01 19:38:20+00:00
  edited: false
  hidden: false
  id: 6542b73c72defaf6593c2a63
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/WizardCoder-15B-1.0-GPTQ
repo_type: model
status: open
target_branch: null
title: Error when loading with text gen webui
