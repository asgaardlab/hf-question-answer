!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Vinitrajputt
conflicting_files: null
created_at: 2023-06-15 07:51:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64353f8ff81a16e74360481a/FxtZJ9Ejl77uNkJjOO3GV.jpeg?w=200&h=200&f=face
      fullname: Vinit rajput
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vinitrajputt
      type: user
    createdAt: '2023-06-15T08:51:04.000Z'
    data:
      edited: false
      editors:
      - Vinitrajputt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9776371717453003
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64353f8ff81a16e74360481a/FxtZJ9Ejl77uNkJjOO3GV.jpeg?w=200&h=200&f=face
          fullname: Vinit rajput
          isHf: false
          isPro: false
          name: Vinitrajputt
          type: user
        html: '<p>hey i liked your work to much, i am having to much fun while testing
          your diffrent models. you have done a great work. but i have a doubt, you
          curruntly have 200+ models, can you help me to select a best model, the
          work is simple. the model have to do role play and it should be great knowladge,
          also i have 15gb gpu so which is the best model that i can use</p>

          '
        raw: hey i liked your work to much, i am having to much fun while testing
          your diffrent models. you have done a great work. but i have a doubt, you
          curruntly have 200+ models, can you help me to select a best model, the
          work is simple. the model have to do role play and it should be great knowladge,
          also i have 15gb gpu so which is the best model that i can use
        updatedAt: '2023-06-15T08:51:04.881Z'
      numEdits: 0
      reactions: []
    id: 648ad0f85b9ec249b8840aa0
    type: comment
  author: Vinitrajputt
  content: hey i liked your work to much, i am having to much fun while testing your
    diffrent models. you have done a great work. but i have a doubt, you curruntly
    have 200+ models, can you help me to select a best model, the work is simple.
    the model have to do role play and it should be great knowladge, also i have 15gb
    gpu so which is the best model that i can use
  created_at: 2023-06-15 07:51:04+00:00
  edited: false
  hidden: false
  id: 648ad0f85b9ec249b8840aa0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-15T14:28:23.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7562575340270996
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>With 15GB VRAM you will want to limit yourself to 13B models.</p>

          <p>I don''t really look into role play myself, but all of the following
          models are good for chat, so may do what you want:</p>

          <ul>

          <li><a href="https://huggingface.co/TheBloke/manticore-13b-chat-pyg-GPTQ">https://huggingface.co/TheBloke/manticore-13b-chat-pyg-GPTQ</a></li>

          <li><a href="https://huggingface.co/TheBloke/samantha-1.1-llama-13B-GPTQ">https://huggingface.co/TheBloke/samantha-1.1-llama-13B-GPTQ</a></li>

          <li><a href="https://huggingface.co/TheBloke/based-13b-GPTQ">https://huggingface.co/TheBloke/based-13b-GPTQ</a></li>

          <li><a href="https://huggingface.co/TheBloke/minotaur-13B-fixed-GPTQ">https://huggingface.co/TheBloke/minotaur-13B-fixed-GPTQ</a></li>

          <li><a href="https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ">https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ</a></li>

          <li><a href="https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ">https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ</a></li>

          <li><a href="https://huggingface.co/TheBloke/chronos-wizardlm-uc-scot-st-13B-GPTQ">https://huggingface.co/TheBloke/chronos-wizardlm-uc-scot-st-13B-GPTQ</a></li>

          </ul>

          '
        raw: 'With 15GB VRAM you will want to limit yourself to 13B models.


          I don''t really look into role play myself, but all of the following models
          are good for chat, so may do what you want:

          - https://huggingface.co/TheBloke/manticore-13b-chat-pyg-GPTQ

          - https://huggingface.co/TheBloke/samantha-1.1-llama-13B-GPTQ

          - https://huggingface.co/TheBloke/based-13b-GPTQ

          - https://huggingface.co/TheBloke/minotaur-13B-fixed-GPTQ

          - https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ

          - https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ

          - https://huggingface.co/TheBloke/chronos-wizardlm-uc-scot-st-13B-GPTQ'
        updatedAt: '2023-06-15T14:28:23.183Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - whalewallet
        - Vinitrajputt
    id: 648b20078e81c2b53f2e1058
    type: comment
  author: TheBloke
  content: 'With 15GB VRAM you will want to limit yourself to 13B models.


    I don''t really look into role play myself, but all of the following models are
    good for chat, so may do what you want:

    - https://huggingface.co/TheBloke/manticore-13b-chat-pyg-GPTQ

    - https://huggingface.co/TheBloke/samantha-1.1-llama-13B-GPTQ

    - https://huggingface.co/TheBloke/based-13b-GPTQ

    - https://huggingface.co/TheBloke/minotaur-13B-fixed-GPTQ

    - https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ

    - https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ

    - https://huggingface.co/TheBloke/chronos-wizardlm-uc-scot-st-13B-GPTQ'
  created_at: 2023-06-15 13:28:23+00:00
  edited: false
  hidden: false
  id: 648b20078e81c2b53f2e1058
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64353f8ff81a16e74360481a/FxtZJ9Ejl77uNkJjOO3GV.jpeg?w=200&h=200&f=face
      fullname: Vinit rajput
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vinitrajputt
      type: user
    createdAt: '2023-06-15T14:41:29.000Z'
    data:
      edited: false
      editors:
      - Vinitrajputt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9760441184043884
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64353f8ff81a16e74360481a/FxtZJ9Ejl77uNkJjOO3GV.jpeg?w=200&h=200&f=face
          fullname: Vinit rajput
          isHf: false
          isPro: false
          name: Vinitrajputt
          type: user
        html: '<p>thanks for assistance, i belive they all works quite well. but can
          you do one of like in which we can give it instructions as well for chat.
          its not a priotized request, when you got a little time, can you that as
          well?<br>also i want to know 1 more thing, what thing you use for training
          and fine-tuning. like in my system i can run 13b model but i can only able
          to train 2-3b models. is there any prefered cloud-computing service there
          or you use your local system?</p>

          '
        raw: 'thanks for assistance, i belive they all works quite well. but can you
          do one of like in which we can give it instructions as well for chat. its
          not a priotized request, when you got a little time, can you that as well?

          also i want to know 1 more thing, what thing you use for training and fine-tuning.
          like in my system i can run 13b model but i can only able to train 2-3b
          models. is there any prefered cloud-computing service there or you use your
          local system?'
        updatedAt: '2023-06-15T14:41:29.960Z'
      numEdits: 0
      reactions: []
    id: 648b2319a39e3846e0590438
    type: comment
  author: Vinitrajputt
  content: 'thanks for assistance, i belive they all works quite well. but can you
    do one of like in which we can give it instructions as well for chat. its not
    a priotized request, when you got a little time, can you that as well?

    also i want to know 1 more thing, what thing you use for training and fine-tuning.
    like in my system i can run 13b model but i can only able to train 2-3b models.
    is there any prefered cloud-computing service there or you use your local system?'
  created_at: 2023-06-15 13:41:29+00:00
  edited: false
  hidden: false
  id: 648b2319a39e3846e0590438
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-16T11:33:05.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5298075079917908
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>For instructions:</p>

          <ul>

          <li><a href="https://huggingface.co/4bit/WizardLM-13B-Uncensored-4bit-128g">https://huggingface.co/4bit/WizardLM-13B-Uncensored-4bit-128g</a></li>

          <li><a href="https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ">https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ</a></li>

          <li><a href="https://huggingface.co/TheBloke/wizardLM-13B-1.0-GPTQ">https://huggingface.co/TheBloke/wizardLM-13B-1.0-GPTQ</a></li>

          <li><a href="https://huggingface.co/TheBloke/chronos-hermes-13B-GPTQ">https://huggingface.co/TheBloke/chronos-hermes-13B-GPTQ</a></li>

          <li><a href="https://huggingface.co/TheBloke/airoboros-13B-1.1-GPTQ">https://huggingface.co/TheBloke/airoboros-13B-1.1-GPTQ</a></li>

          </ul>

          '
        raw: 'For instructions:

          - https://huggingface.co/4bit/WizardLM-13B-Uncensored-4bit-128g

          - https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ

          - https://huggingface.co/TheBloke/wizardLM-13B-1.0-GPTQ

          - https://huggingface.co/TheBloke/chronos-hermes-13B-GPTQ

          - https://huggingface.co/TheBloke/airoboros-13B-1.1-GPTQ'
        updatedAt: '2023-06-16T11:33:05.553Z'
      numEdits: 0
      reactions: []
    id: 648c48719c935db2b52081d1
    type: comment
  author: TheBloke
  content: 'For instructions:

    - https://huggingface.co/4bit/WizardLM-13B-Uncensored-4bit-128g

    - https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ

    - https://huggingface.co/TheBloke/wizardLM-13B-1.0-GPTQ

    - https://huggingface.co/TheBloke/chronos-hermes-13B-GPTQ

    - https://huggingface.co/TheBloke/airoboros-13B-1.1-GPTQ'
  created_at: 2023-06-16 10:33:05+00:00
  edited: false
  hidden: false
  id: 648c48719c935db2b52081d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/qMF4wWDizHZFYcH1ft4XJ.png?w=200&h=200&f=face
      fullname: Paulo Henrique
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pauloheli
      type: user
    createdAt: '2023-08-27T14:46:49.000Z'
    data:
      edited: false
      editors:
      - pauloheli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9374147653579712
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/qMF4wWDizHZFYcH1ft4XJ.png?w=200&h=200&f=face
          fullname: Paulo Henrique
          isHf: false
          isPro: false
          name: pauloheli
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> For the purpose\
          \ of code generation, which model do you recommend for a 6GB (2060) graphics\
          \ card? I see many models, but it's hard for me to choose the best for this\
          \ purpose and limitation.</p>\n"
        raw: '@TheBloke For the purpose of code generation, which model do you recommend
          for a 6GB (2060) graphics card? I see many models, but it''s hard for me
          to choose the best for this purpose and limitation.'
        updatedAt: '2023-08-27T14:46:49.676Z'
      numEdits: 0
      reactions: []
    id: 64eb61d9240d8343f01bb4bc
    type: comment
  author: pauloheli
  content: '@TheBloke For the purpose of code generation, which model do you recommend
    for a 6GB (2060) graphics card? I see many models, but it''s hard for me to choose
    the best for this purpose and limitation.'
  created_at: 2023-08-27 13:46:49+00:00
  edited: false
  hidden: false
  id: 64eb61d9240d8343f01bb4bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64353f8ff81a16e74360481a/FxtZJ9Ejl77uNkJjOO3GV.jpeg?w=200&h=200&f=face
      fullname: Vinit rajput
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Vinitrajputt
      type: user
    createdAt: '2023-08-27T15:09:35.000Z'
    data:
      edited: true
      editors:
      - Vinitrajputt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9551501274108887
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64353f8ff81a16e74360481a/FxtZJ9Ejl77uNkJjOO3GV.jpeg?w=200&h=200&f=face
          fullname: Vinit rajput
          isHf: false
          isPro: false
          name: Vinitrajputt
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ For the purpose of code generation, which model do you recommend for a\
          \ 6GB (2060) graphics card? I see many models, but it's hard for me to choose\
          \ the best for this purpose and limitation.</p>\n</blockquote>\n<p>I guess\
          \ in 6gb you only have a few options, you can try using a 7b model at 4bit\
          \ quant. There are few models like starcoder etc. You can check according\
          \ to your usecase or use can use an cloud GPU service if you want better\
          \ results by loading bigger models </p>\n"
        raw: '> @TheBloke For the purpose of code generation, which model do you recommend
          for a 6GB (2060) graphics card? I see many models, but it''s hard for me
          to choose the best for this purpose and limitation.


          I guess in 6gb you only have a few options, you can try using a 7b model
          at 4bit quant. There are few models like starcoder etc. You can check according
          to your usecase or use can use an cloud GPU service if you want better results
          by loading bigger models '
        updatedAt: '2023-08-27T15:13:48.651Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pauloheli
    id: 64eb672fd848efd8a6762b45
    type: comment
  author: Vinitrajputt
  content: '> @TheBloke For the purpose of code generation, which model do you recommend
    for a 6GB (2060) graphics card? I see many models, but it''s hard for me to choose
    the best for this purpose and limitation.


    I guess in 6gb you only have a few options, you can try using a 7b model at 4bit
    quant. There are few models like starcoder etc. You can check according to your
    usecase or use can use an cloud GPU service if you want better results by loading
    bigger models '
  created_at: 2023-08-27 14:09:35+00:00
  edited: true
  hidden: false
  id: 64eb672fd848efd8a6762b45
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardCoder-15B-1.0-GPTQ
repo_type: model
status: open
target_branch: null
title: need assistance
