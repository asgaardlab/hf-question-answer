!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sunzx0810
conflicting_files: null
created_at: 2023-06-26 07:13:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41c81bd99888569d44c23a2cfb2b57a0.svg
      fullname: shawn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sunzx0810
      type: user
    createdAt: '2023-06-26T08:13:21.000Z'
    data:
      edited: false
      editors:
      - sunzx0810
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.942423403263092
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41c81bd99888569d44c23a2cfb2b57a0.svg
          fullname: shawn
          isHf: false
          isPro: false
          name: sunzx0810
          type: user
        html: '<p>when testing the model in a local environment by directly running
          the code, I input a 567 tokens'' question and raise error ''list index out
          of range'' which seems to tell me that the output exceeds in 2000 tokens.
          However, the question works well both on demo and text generation webui.
          why webui can generate answers word by word? And I also find that the response
          from the local model will try to edit the instructions but the webui and
          demo will not. It''s very confusing.</p>

          '
        raw: when testing the model in a local environment by directly running the
          code, I input a 567 tokens' question and raise error 'list index out of
          range' which seems to tell me that the output exceeds in 2000 tokens. However,
          the question works well both on demo and text generation webui. why webui
          can generate answers word by word? And I also find that the response from
          the local model will try to edit the instructions but the webui and demo
          will not. It's very confusing.
        updatedAt: '2023-06-26T08:13:21.614Z'
      numEdits: 0
      reactions: []
    id: 649948a19b50ea5cfafed9bf
    type: comment
  author: sunzx0810
  content: when testing the model in a local environment by directly running the code,
    I input a 567 tokens' question and raise error 'list index out of range' which
    seems to tell me that the output exceeds in 2000 tokens. However, the question
    works well both on demo and text generation webui. why webui can generate answers
    word by word? And I also find that the response from the local model will try
    to edit the instructions but the webui and demo will not. It's very confusing.
  created_at: 2023-06-26 07:13:21+00:00
  edited: false
  hidden: false
  id: 649948a19b50ea5cfafed9bf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/WizardCoder-15B-1.0-GPTQ
repo_type: model
status: open
target_branch: null
title: question about tokens limit
