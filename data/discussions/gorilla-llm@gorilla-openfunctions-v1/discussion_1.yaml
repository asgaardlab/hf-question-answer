!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tjtanaa
conflicting_files: null
created_at: 2023-11-17 10:03:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/51174cbf023128babbcc24c17ee0d21e.svg
      fullname: Tan Tun Jian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tjtanaa
      type: user
    createdAt: '2023-11-17T10:03:46.000Z'
    data:
      edited: false
      editors:
      - tjtanaa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9935939908027649
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/51174cbf023128babbcc24c17ee0d21e.svg
          fullname: Tan Tun Jian
          isHf: false
          isPro: false
          name: tjtanaa
          type: user
        html: '<p>I would like to host the model myself. May I know what is the prompt
          template?</p>

          '
        raw: "I would like to host the model myself. May I know what is the prompt\
          \ template?\r\n"
        updatedAt: '2023-11-17T10:03:46.191Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - limcheekin
        - fernandofernandes
        - layoric
        - victor
    id: 65573a82c4865c852d5b9f91
    type: comment
  author: tjtanaa
  content: "I would like to host the model myself. May I know what is the prompt template?\r\
    \n"
  created_at: 2023-11-17 10:03:46+00:00
  edited: false
  hidden: false
  id: 65573a82c4865c852d5b9f91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a65eb593f7e383740d9d2/Kh6Tf2pvIRQi0Rrx1LiAL.png?w=200&h=200&f=face
      fullname: Darren Reid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: layoric
      type: user
    createdAt: '2023-11-18T06:19:00.000Z'
    data:
      edited: false
      editors:
      - layoric
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7527915239334106
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a65eb593f7e383740d9d2/Kh6Tf2pvIRQi0Rrx1LiAL.png?w=200&h=200&f=face
          fullname: Darren Reid
          isHf: false
          isPro: true
          name: layoric
          type: user
        html: "<p>I don't know if this is correct, but I am getting decent results\
          \ with:</p>\n<pre><code>###USER: {prompt}\n###FUNCTIONS: {func_spec}\n###ASSISTANT:\n\
          </code></pre>\n<p>Here is what I am doing in Python (it's a bit rough):</p>\n\
          <pre><code>    functions = request.functions\n    prompt = \" \".join([msg.content\
          \ for msg in request.messages])\n    # Add the functions definitions to\
          \ the prompt, serialize them to JSON on a new line\n    func_json = \"\\\
          n\" + \"\\n\".join([function.json() for function in functions])\n    prompt\
          \ = \"###USER: \" + prompt + \"\\n\" + \"###FUNCTIONS: \" + func_json +\
          \ \"\\n\" + \"###ASSISTANT: \\n\"\n    response = get_response(prompt, model,\
          \ tokenizer, device='cuda')\n</code></pre>\n"
        raw: "I don't know if this is correct, but I am getting decent results with:\n\
          \n```\n###USER: {prompt}\n###FUNCTIONS: {func_spec}\n###ASSISTANT:\n```\n\
          \nHere is what I am doing in Python (it's a bit rough):\n```\n    functions\
          \ = request.functions\n    prompt = \" \".join([msg.content for msg in request.messages])\n\
          \    # Add the functions definitions to the prompt, serialize them to JSON\
          \ on a new line\n    func_json = \"\\n\" + \"\\n\".join([function.json()\
          \ for function in functions])\n    prompt = \"###USER: \" + prompt + \"\\\
          n\" + \"###FUNCTIONS: \" + func_json + \"\\n\" + \"###ASSISTANT: \\n\"\n\
          \    response = get_response(prompt, model, tokenizer, device='cuda')\n\
          ```"
        updatedAt: '2023-11-18T06:19:00.258Z'
      numEdits: 0
      reactions: []
    id: 655857547d187b097eb441ce
    type: comment
  author: layoric
  content: "I don't know if this is correct, but I am getting decent results with:\n\
    \n```\n###USER: {prompt}\n###FUNCTIONS: {func_spec}\n###ASSISTANT:\n```\n\nHere\
    \ is what I am doing in Python (it's a bit rough):\n```\n    functions = request.functions\n\
    \    prompt = \" \".join([msg.content for msg in request.messages])\n    # Add\
    \ the functions definitions to the prompt, serialize them to JSON on a new line\n\
    \    func_json = \"\\n\" + \"\\n\".join([function.json() for function in functions])\n\
    \    prompt = \"###USER: \" + prompt + \"\\n\" + \"###FUNCTIONS: \" + func_json\
    \ + \"\\n\" + \"###ASSISTANT: \\n\"\n    response = get_response(prompt, model,\
    \ tokenizer, device='cuda')\n```"
  created_at: 2023-11-18 06:19:00+00:00
  edited: false
  hidden: false
  id: 655857547d187b097eb441ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/51174cbf023128babbcc24c17ee0d21e.svg
      fullname: Tan Tun Jian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tjtanaa
      type: user
    createdAt: '2023-11-18T06:32:20.000Z'
    data:
      edited: false
      editors:
      - tjtanaa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.791724681854248
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/51174cbf023128babbcc24c17ee0d21e.svg
          fullname: Tan Tun Jian
          isHf: false
          isPro: false
          name: tjtanaa
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6463074f514ee1645bd3a89d/uBJAVDObNGbG8WYNDLfWV.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6463074f514ee1645bd3a89d/uBJAVDObNGbG8WYNDLfWV.png"></a><br>I
          saw some clue about the possible prompt, but it is not enought to make the
          model to continue the answer.</p>

          '
        raw: '

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6463074f514ee1645bd3a89d/uBJAVDObNGbG8WYNDLfWV.png)

          I saw some clue about the possible prompt, but it is not enought to make
          the model to continue the answer.'
        updatedAt: '2023-11-18T06:32:20.241Z'
      numEdits: 0
      reactions: []
    id: 65585a74cafc48de360680a1
    type: comment
  author: tjtanaa
  content: '

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6463074f514ee1645bd3a89d/uBJAVDObNGbG8WYNDLfWV.png)

    I saw some clue about the possible prompt, but it is not enought to make the model
    to continue the answer.'
  created_at: 2023-11-18 06:32:20+00:00
  edited: false
  hidden: false
  id: 65585a74cafc48de360680a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/51174cbf023128babbcc24c17ee0d21e.svg
      fullname: Tan Tun Jian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tjtanaa
      type: user
    createdAt: '2023-11-18T06:33:19.000Z'
    data:
      edited: true
      editors:
      - tjtanaa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7583276033401489
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/51174cbf023128babbcc24c17ee0d21e.svg
          fullname: Tan Tun Jian
          isHf: false
          isPro: false
          name: tjtanaa
          type: user
        html: "<blockquote>\n<p>I don't know if this is correct, but I am getting\
          \ decent results with:</p>\n<pre><code>###USER: {prompt}\n###FUNCTIONS:\
          \ {func_spec}\n###ASSISTANT:\n</code></pre>\n<p>Here is what I am doing\
          \ in Python (it's a bit rough):</p>\n<pre><code>    functions = request.functions\n\
          \    prompt = \" \".join([msg.content for msg in request.messages])\n  \
          \  # Add the functions definitions to the prompt, serialize them to JSON\
          \ on a new line\n    func_json = \"\\n\" + \"\\n\".join([function.json()\
          \ for function in functions])\n    prompt = \"###USER: \" + prompt + \"\\\
          n\" + \"###FUNCTIONS: \" + func_json + \"\\n\" + \"###ASSISTANT: \\n\"\n\
          \    response = get_response(prompt, model, tokenizer, device='cuda')\n\
          </code></pre>\n<p>Are you able to reproduce the expected results of the\
          \ example case in the README?</p>\n</blockquote>\n<pre><code>query = \"\
          Call me an Uber ride type \\\"Plus\\\" in Berkeley at zipcode 94704 in 10\
          \ minutes\"\nfunctions = [\n    {\n        \"name\": \"Uber Carpool\",\n\
          \        \"api_name\": \"uber.ride\",\n        \"description\": \"Find suitable\
          \ ride for customers given the location, type of ride, and the amount of\
          \ time the customer is willing to wait as parameters\",\n        \"parameters\"\
          :  [{\"name\": \"loc\", \"description\": \"location of the starting place\
          \ of the uber ride\"}, {\"name\":\"type\", \"enum\": [\"plus\", \"comfort\"\
          , \"black\"], \"description\": \"types of uber ride user is ordering\"},\
          \ {\"name\": \"time\", \"description\": \"the amount of time in minutes\
          \ the customer is willing to wait\"}]\n    }\n]\nget_gorilla_response(query,\
          \ functions=functions)\n\nExpected output:\nuber.ride(loc=\"berkeley\",\
          \ type=\"plus\", time=10)\n</code></pre>\n<p>I always get </p>\n<p>uber.ride(loc=\"\
          94704\", type=\"plus\", time=10)</p>\n"
        raw: "> I don't know if this is correct, but I am getting decent results with:\n\
          > \n> ```\n> ###USER: {prompt}\n> ###FUNCTIONS: {func_spec}\n> ###ASSISTANT:\n\
          > ```\n> \n> Here is what I am doing in Python (it's a bit rough):\n> ```\n\
          >     functions = request.functions\n>     prompt = \" \".join([msg.content\
          \ for msg in request.messages])\n>     # Add the functions definitions to\
          \ the prompt, serialize them to JSON on a new line\n>     func_json = \"\
          \\n\" + \"\\n\".join([function.json() for function in functions])\n>   \
          \  prompt = \"###USER: \" + prompt + \"\\n\" + \"###FUNCTIONS: \" + func_json\
          \ + \"\\n\" + \"###ASSISTANT: \\n\"\n>     response = get_response(prompt,\
          \ model, tokenizer, device='cuda')\n> ```\nAre you able to reproduce the\
          \ expected results of the example case in the README?\n```\nquery = \"Call\
          \ me an Uber ride type \\\"Plus\\\" in Berkeley at zipcode 94704 in 10 minutes\"\
          \nfunctions = [\n    {\n        \"name\": \"Uber Carpool\",\n        \"\
          api_name\": \"uber.ride\",\n        \"description\": \"Find suitable ride\
          \ for customers given the location, type of ride, and the amount of time\
          \ the customer is willing to wait as parameters\",\n        \"parameters\"\
          :  [{\"name\": \"loc\", \"description\": \"location of the starting place\
          \ of the uber ride\"}, {\"name\":\"type\", \"enum\": [\"plus\", \"comfort\"\
          , \"black\"], \"description\": \"types of uber ride user is ordering\"},\
          \ {\"name\": \"time\", \"description\": \"the amount of time in minutes\
          \ the customer is willing to wait\"}]\n    }\n]\nget_gorilla_response(query,\
          \ functions=functions)\n\nExpected output:\nuber.ride(loc=\"berkeley\",\
          \ type=\"plus\", time=10)\n```\n\nI always get \n\nuber.ride(loc=\"94704\"\
          , type=\"plus\", time=10)\n"
        updatedAt: '2023-11-18T06:35:13.627Z'
      numEdits: 2
      reactions: []
    id: 65585aafa75de9cef17183d2
    type: comment
  author: tjtanaa
  content: "> I don't know if this is correct, but I am getting decent results with:\n\
    > \n> ```\n> ###USER: {prompt}\n> ###FUNCTIONS: {func_spec}\n> ###ASSISTANT:\n\
    > ```\n> \n> Here is what I am doing in Python (it's a bit rough):\n> ```\n> \
    \    functions = request.functions\n>     prompt = \" \".join([msg.content for\
    \ msg in request.messages])\n>     # Add the functions definitions to the prompt,\
    \ serialize them to JSON on a new line\n>     func_json = \"\\n\" + \"\\n\".join([function.json()\
    \ for function in functions])\n>     prompt = \"###USER: \" + prompt + \"\\n\"\
    \ + \"###FUNCTIONS: \" + func_json + \"\\n\" + \"###ASSISTANT: \\n\"\n>     response\
    \ = get_response(prompt, model, tokenizer, device='cuda')\n> ```\nAre you able\
    \ to reproduce the expected results of the example case in the README?\n```\n\
    query = \"Call me an Uber ride type \\\"Plus\\\" in Berkeley at zipcode 94704\
    \ in 10 minutes\"\nfunctions = [\n    {\n        \"name\": \"Uber Carpool\",\n\
    \        \"api_name\": \"uber.ride\",\n        \"description\": \"Find suitable\
    \ ride for customers given the location, type of ride, and the amount of time\
    \ the customer is willing to wait as parameters\",\n        \"parameters\":  [{\"\
    name\": \"loc\", \"description\": \"location of the starting place of the uber\
    \ ride\"}, {\"name\":\"type\", \"enum\": [\"plus\", \"comfort\", \"black\"], \"\
    description\": \"types of uber ride user is ordering\"}, {\"name\": \"time\",\
    \ \"description\": \"the amount of time in minutes the customer is willing to\
    \ wait\"}]\n    }\n]\nget_gorilla_response(query, functions=functions)\n\nExpected\
    \ output:\nuber.ride(loc=\"berkeley\", type=\"plus\", time=10)\n```\n\nI always\
    \ get \n\nuber.ride(loc=\"94704\", type=\"plus\", time=10)\n"
  created_at: 2023-11-18 06:33:19+00:00
  edited: true
  hidden: false
  id: 65585aafa75de9cef17183d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a65eb593f7e383740d9d2/Kh6Tf2pvIRQi0Rrx1LiAL.png?w=200&h=200&f=face
      fullname: Darren Reid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: layoric
      type: user
    createdAt: '2023-11-18T07:09:20.000Z'
    data:
      edited: false
      editors:
      - layoric
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8436087965965271
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a65eb593f7e383740d9d2/Kh6Tf2pvIRQi0Rrx1LiAL.png?w=200&h=200&f=face
          fullname: Darren Reid
          isHf: false
          isPro: true
          name: layoric
          type: user
        html: '<p>I get <code>uber.ride(loc="Berkeley", type="plus", time=10)</code>,
          though I have seen that response when testing. Hopefully the actual prompt
          format will resolve some of the inconsistencies.</p>

          '
        raw: I get `uber.ride(loc="Berkeley", type="plus", time=10)`, though I have
          seen that response when testing. Hopefully the actual prompt format will
          resolve some of the inconsistencies.
        updatedAt: '2023-11-18T07:09:20.826Z'
      numEdits: 0
      reactions: []
    id: 6558632004a63a0dfbc5b407
    type: comment
  author: layoric
  content: I get `uber.ride(loc="Berkeley", type="plus", time=10)`, though I have
    seen that response when testing. Hopefully the actual prompt format will resolve
    some of the inconsistencies.
  created_at: 2023-11-18 07:09:20+00:00
  edited: false
  hidden: false
  id: 6558632004a63a0dfbc5b407
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80147200fd8355db9f7c9845096d4c2e.svg
      fullname: Manoranjan Rajguru
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: monuminu
      type: user
    createdAt: '2023-11-19T06:05:45.000Z'
    data:
      edited: false
      editors:
      - monuminu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42577341198921204
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80147200fd8355db9f7c9845096d4c2e.svg
          fullname: Manoranjan Rajguru
          isHf: false
          isPro: false
          name: monuminu
          type: user
        html: "<p>You can below :</p>\n<ol>\n<li>Deploy using vllm OpenAI-Compatible\
          \ Server.</li>\n</ol>\n<pre><code class=\"language-python\">    VLLM_USE_MODELSCOPE=<span\
          \ class=\"hljs-literal\">True</span> python -m vllm.entrypoints.openai.api_server\
          \  model=<span class=\"hljs-string\">\"gorilla-llm/gorilla-openfunctions-v1\"\
          </span> --revision=<span class=\"hljs-string\">\"v1.1.8\"</span> --trust-remote-code\n\
          </code></pre>\n<ol start=\"2\">\n<li>and then just use the same</li>\n</ol>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ openai\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\
          \ function_\">get_gorilla_response</span>(<span class=\"hljs-params\">prompt=<span\
          \ class=\"hljs-string\">\"Call me an Uber ride type \\\"Plus\\\" in Berkeley\
          \ at zipcode 94704 in 10 minutes\"</span>, model=<span class=\"hljs-string\"\
          >\"gorilla-openfunctions-v0\"</span>, functions=[]</span>):\n  openai.api_key\
          \ = <span class=\"hljs-string\">\"EMPTY\"</span>\n  openai.api_base = <span\
          \ class=\"hljs-string\">\"http://luigi.millennium.berkeley.edu:8000/v1\"\
          </span>\n  <span class=\"hljs-keyword\">try</span>:\n    completion = openai.ChatCompletion.create(\n\
          \      model=<span class=\"hljs-string\">\"gorilla-openfunctions-v1\"</span>,\n\
          \      temperature=<span class=\"hljs-number\">0.0</span>,\n      messages=[{<span\
          \ class=\"hljs-string\">\"role\"</span>: <span class=\"hljs-string\">\"\
          user\"</span>, <span class=\"hljs-string\">\"content\"</span>: prompt}],\n\
          \      functions=functions,\n    )\n    <span class=\"hljs-keyword\">return</span>\
          \ completion.choices[<span class=\"hljs-number\">0</span>].message.content\n\
          \  <span class=\"hljs-keyword\">except</span> Exception <span class=\"hljs-keyword\"\
          >as</span> e:\n    <span class=\"hljs-built_in\">print</span>(e, model,\
          \ prompt)\n\n</code></pre>\n<p>Then just the call the same way</p>\n"
        raw: "You can below :\n\n1. Deploy using vllm OpenAI-Compatible Server.\n\n\
          ```python\n    VLLM_USE_MODELSCOPE=True python -m vllm.entrypoints.openai.api_server\
          \  model=\"gorilla-llm/gorilla-openfunctions-v1\" --revision=\"v1.1.8\"\
          \ --trust-remote-code\n```\n\n2. and then just use the same \n\n```python\n\
          import openai\n\ndef get_gorilla_response(prompt=\"Call me an Uber ride\
          \ type \\\"Plus\\\" in Berkeley at zipcode 94704 in 10 minutes\", model=\"\
          gorilla-openfunctions-v0\", functions=[]):\n  openai.api_key = \"EMPTY\"\
          \n  openai.api_base = \"http://luigi.millennium.berkeley.edu:8000/v1\"\n\
          \  try:\n    completion = openai.ChatCompletion.create(\n      model=\"\
          gorilla-openfunctions-v1\",\n      temperature=0.0,\n      messages=[{\"\
          role\": \"user\", \"content\": prompt}],\n      functions=functions,\n \
          \   )\n    return completion.choices[0].message.content\n  except Exception\
          \ as e:\n    print(e, model, prompt)\n\n\n```\n\nThen just the call the\
          \ same way"
        updatedAt: '2023-11-19T06:05:45.840Z'
      numEdits: 0
      reactions: []
    id: 6559a5b9c619b4853158600e
    type: comment
  author: monuminu
  content: "You can below :\n\n1. Deploy using vllm OpenAI-Compatible Server.\n\n\
    ```python\n    VLLM_USE_MODELSCOPE=True python -m vllm.entrypoints.openai.api_server\
    \  model=\"gorilla-llm/gorilla-openfunctions-v1\" --revision=\"v1.1.8\" --trust-remote-code\n\
    ```\n\n2. and then just use the same \n\n```python\nimport openai\n\ndef get_gorilla_response(prompt=\"\
    Call me an Uber ride type \\\"Plus\\\" in Berkeley at zipcode 94704 in 10 minutes\"\
    , model=\"gorilla-openfunctions-v0\", functions=[]):\n  openai.api_key = \"EMPTY\"\
    \n  openai.api_base = \"http://luigi.millennium.berkeley.edu:8000/v1\"\n  try:\n\
    \    completion = openai.ChatCompletion.create(\n      model=\"gorilla-openfunctions-v1\"\
    ,\n      temperature=0.0,\n      messages=[{\"role\": \"user\", \"content\": prompt}],\n\
    \      functions=functions,\n    )\n    return completion.choices[0].message.content\n\
    \  except Exception as e:\n    print(e, model, prompt)\n\n\n```\n\nThen just the\
    \ call the same way"
  created_at: 2023-11-19 06:05:45+00:00
  edited: false
  hidden: false
  id: 6559a5b9c619b4853158600e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471b9f6094820190c324eec/yb62BiXzRFY1_pS0GAMGq.jpeg?w=200&h=200&f=face
      fullname: Shishir Patil
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: shishirpatil
      type: user
    createdAt: '2023-11-19T10:50:36.000Z'
    data:
      edited: false
      editors:
      - shishirpatil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7685114145278931
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471b9f6094820190c324eec/yb62BiXzRFY1_pS0GAMGq.jpeg?w=200&h=200&f=face
          fullname: Shishir Patil
          isHf: false
          isPro: false
          name: shishirpatil
          type: user
        html: '<p>Thanks for trying it! Just updated it here: <a rel="nofollow" href="https://github.com/ShishirPatil/gorilla/tree/main/openfunctions#running-openfunctions-locally">https://github.com/ShishirPatil/gorilla/tree/main/openfunctions#running-openfunctions-locally</a><br>Let
          me know if you run into any other issues! </p>

          '
        raw: 'Thanks for trying it! Just updated it here: https://github.com/ShishirPatil/gorilla/tree/main/openfunctions#running-openfunctions-locally

          Let me know if you run into any other issues! '
        updatedAt: '2023-11-19T10:50:36.713Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - layoric
    id: 6559e87cda4acab10a85942a
    type: comment
  author: shishirpatil
  content: 'Thanks for trying it! Just updated it here: https://github.com/ShishirPatil/gorilla/tree/main/openfunctions#running-openfunctions-locally

    Let me know if you run into any other issues! '
  created_at: 2023-11-19 10:50:36+00:00
  edited: false
  hidden: false
  id: 6559e87cda4acab10a85942a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: gorilla-llm/gorilla-openfunctions-v1
repo_type: model
status: open
target_branch: null
title: What is the prompt template?
