!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mnwato
conflicting_files: null
created_at: 2023-09-08 07:48:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/449d0c4879ff617fea41e1e8128d3f2f.svg
      fullname: Mostafa Najmi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mnwato
      type: user
    createdAt: '2023-09-08T08:48:07.000Z'
    data:
      edited: true
      editors:
      - mnwato
      hidden: false
      identifiedLanguage:
        language: fa
        probability: 0.7152916193008423
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/449d0c4879ff617fea41e1e8128d3f2f.svg
          fullname: Mostafa Najmi
          isHf: false
          isPro: false
          name: mnwato
          type: user
        html: "<p>I used pipeline based inference and it was ok. But when generate\
          \ manually, output have lots of  tokens:</p>\n<pre><code>input: \u062A\u0648\
          \u0627\u0646\u0627 \u0628\u0648\u062F \u0647\u0631 \u06A9\u0647\ngenerated:\
          \ \u062A\u0648\u0627\u0646\u0627 \u0628\u0648\u062F \u0647\u0631 \u06A9\u0647\
          \ \u0631\u0627 \u06A9\u0647 \u062A\u0648\u0627\u0646\u0627 \u0628\u0648\u062F\
          \u0647 \u062F\u0631 \u0627\u06CC\u0646 \u0631\u0648\u0632 \u06CC\u0627\u0631\
          \u06CC \u062F\u0627\u062F \u0648 \u062F\u0631 \u0631\u0627\u0647 \u062E\u062F\
          \u0627 \u062C\u0647\u0627\u062F \u06A9\u0631\u062F \u0648 \u0634\u0647\u06CC\
          \u062F \u06A9\u0631\u062F\n</code></pre>\n<pre><code class=\"language-sh\"\
          >    model_inputs = tokenizer(sent, padding=True, return_tensors=<span class=\"\
          hljs-string\">'pt'</span>).to(<span class=\"hljs-string\">'cuda'</span>)\n\
          \    generated_ids = model.generate(**model_inputs, max_length=100)\n  \
          \  generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\
          </code></pre>\n<pre><code>generated: \u062A\u0648\u0627\u0646\u0627 \u0628\
          \u0648\u062F \u0647\u0631 \u06A9\u0647 \u0628\u0647\u06A9\u0633\u062A\u0627\
          \u0631\u0628\u064F\u0646 \u0627\u0645\u0646\u062A&lt;unk&gt;\u0646\u067E\
          \ \u0627\u0632\u0631\u0641\u0627\u0647\u0623\u0628\u062A&lt;unk&gt;\u0647\
          &lt;unk&gt;\u0646\u0646\u0627\u0631\u06CC&lt;unk&gt;\u06A9&lt;unk&gt;\u062A\
          &lt;unk&gt;\u0632&lt;unk&gt;\u062A&lt;unk&gt;\u0646\u064F&lt;unk&gt;\u062E\
          \u062C\u064E\u062B\u0651\u0647\u062A\u0646\u0648\u0634\u06A9\u0628&lt;unk&gt;\u062A\
          &lt;unk&gt;\u0628\u0627\u0628\u0645&lt;unk&gt;\u0647\u0627\u0646&lt;unk&gt;\u0647\
          \ \u06A9\u064F\u06A9&lt;unk&gt;\u0646\u06CC&lt;unk&gt;\u06A9\u0646&lt;unk&gt;&lt;unk&gt;\u06A9\
          &lt;unk&gt; &lt;unk&gt;\u0628&lt;unk&gt;\u0644\u0651\u062A \u0646&lt;unk&gt;\u06CC\
          &lt;unk&gt; \u0628&lt;unk&gt;&lt;unk&gt;\n</code></pre>\n<p>But generating\
          \ with this script will generate output without  token but different from\
          \ pipeline based inference.</p>\n<pre><code class=\"language-sh\">    generated_ids\
          \ = model.generate(input_ids=model_inputs[<span class=\"hljs-string\">'input_ids'</span>],\
          \ max_length=100)\n</code></pre>\n<pre><code>generated: \u062A\u0648\u0627\
          \u0646\u0627 \u0628\u0648\u062F \u0647\u0631 \u06A9\u0647 \u0628\u0647 \u06AF\
          \u0632\u0627\u0631\u0634 \u062E\u0628\u0631\u0646\u06AF\u0627\u0631 \u06AF\
          \u0631\u0648\u0647 \u0627\u0633\u062A\u0627\u0646\u200C\u0647\u0627\u06CC\
          \ \u0628\u0627\u0634\u06AF\u0627\u0647 \u062E\u0628\u0631\u0646\u06AF\u0627\
          \u0631\u0627\u0646 \u062C\u0648\u0627\u0646 \u0627\u0632 \u0633\u0627\u0631\
          \u06CC \u060C \u0631\u0648\u0627\u0628\u0637 \u0639\u0645\u0648\u0645\u06CC\
          \ \u0627\u062F\u0627\u0631\u0647 \u06A9\u0644 \u062D\u0641\u0627\u0638\u062A\
          \ \u0645\u062D\u06CC\u0637 \u0632\u06CC\u0633\u062A \u0645\u0627\u0632\u0646\
          \u062F\u0631\u0627\u0646 \u0627\u0639\u0644\u0627\u0645 \u06A9\u0631\u062F\
          \ : \u0645\u0627\u0645\u0648\u0631\u0627\u0646 \u06CC\u06AF\u0627\u0646\
          \ \u062D\u0641\u0627\u0638\u062A \u0645\u062D\u06CC\u0637 \u0632\u06CC\u0633\
          \u062A \u0622\u0645\u0644 \u062F\u0631 \u062D\u06CC\u0646 \u06AF\u0634\u062A\
          \ \u0648 \u06A9\u0646\u062A\u0631\u0644 \u062F\u0631 \u0645\u0646\u0637\u0642\
          \u0647 \u062D\u0641\u0627\u0638\u062A \u0634\u062F\u0647 \u0645\u06CC\u0627\
          \u0646\u06A9\u0627\u0644\u0647 \u0628\u0647 \u06CC\u06A9 \u0634\u06A9\u0627\
          \u0631\u0686\u06CC \u063A\u06CC\u0631\u0645\u062C\u0627\u0632 \u06A9\u0647\
          \ \u062F\u0631 \u062D\u0627\u0644 \u0634\u06A9\u0627\u0631 \u06CC\u06A9\
          \ \u0631\u0627\u0633 \u06A9\u0644 \u0648\u062D\u0634\u06CC \u0628\u0648\u062F\
          \ \u060C \u0645\u0634\u06A9\u0648\u06A9 \u0634\u062F\u0646\u062F.\n</code></pre>\n\
          <p>Any help is appreciated</p>\n"
        raw: "I used pipeline based inference and it was ok. But when generate manually,\
          \ output have lots of <unk> tokens:\n```\ninput: \u062A\u0648\u0627\u0646\
          \u0627 \u0628\u0648\u062F \u0647\u0631 \u06A9\u0647\ngenerated: \u062A\u0648\
          \u0627\u0646\u0627 \u0628\u0648\u062F \u0647\u0631 \u06A9\u0647 \u0631\u0627\
          \ \u06A9\u0647 \u062A\u0648\u0627\u0646\u0627 \u0628\u0648\u062F\u0647 \u062F\
          \u0631 \u0627\u06CC\u0646 \u0631\u0648\u0632 \u06CC\u0627\u0631\u06CC \u062F\
          \u0627\u062F \u0648 \u062F\u0631 \u0631\u0627\u0647 \u062E\u062F\u0627 \u062C\
          \u0647\u0627\u062F \u06A9\u0631\u062F \u0648 \u0634\u0647\u06CC\u062F \u06A9\
          \u0631\u062F\n```\n\n```sh\n    model_inputs = tokenizer(sent, padding=True,\
          \ return_tensors='pt').to('cuda')\n    generated_ids = model.generate(**model_inputs,\
          \ max_length=100)\n    generated_text = tokenizer.batch_decode(generated_ids,\
          \ skip_special_tokens=True)[0]\n```\n```\ngenerated: \u062A\u0648\u0627\u0646\
          \u0627 \u0628\u0648\u062F \u0647\u0631 \u06A9\u0647 \u0628\u0647\u06A9\u0633\
          \u062A\u0627\u0631\u0628\u064F\u0646 \u0627\u0645\u0646\u062A<unk>\u0646\
          \u067E \u0627\u0632\u0631\u0641\u0627\u0647\u0623\u0628\u062A<unk>\u0647\
          <unk>\u0646\u0646\u0627\u0631\u06CC<unk>\u06A9<unk>\u062A<unk>\u0632<unk>\u062A\
          <unk>\u0646\u064F<unk>\u062E\u062C\u064E\u062B\u0651\u0647\u062A\u0646\u0648\
          \u0634\u06A9\u0628<unk>\u062A<unk>\u0628\u0627\u0628\u0645<unk>\u0647\u0627\
          \u0646<unk>\u0647 \u06A9\u064F\u06A9<unk>\u0646\u06CC<unk>\u06A9\u0646<unk><unk>\u06A9\
          <unk> <unk>\u0628<unk>\u0644\u0651\u062A \u0646<unk>\u06CC<unk> \u0628<unk><unk>\n\
          ```\n\nBut generating with this script will generate output without <unk>\
          \ token but different from pipeline based inference.\n```sh\n    generated_ids\
          \ = model.generate(input_ids=model_inputs['input_ids'], max_length=100)\n\
          ```\n```\ngenerated: \u062A\u0648\u0627\u0646\u0627 \u0628\u0648\u062F \u0647\
          \u0631 \u06A9\u0647 \u0628\u0647 \u06AF\u0632\u0627\u0631\u0634 \u062E\u0628\
          \u0631\u0646\u06AF\u0627\u0631 \u06AF\u0631\u0648\u0647 \u0627\u0633\u062A\
          \u0627\u0646\u200C\u0647\u0627\u06CC \u0628\u0627\u0634\u06AF\u0627\u0647\
          \ \u062E\u0628\u0631\u0646\u06AF\u0627\u0631\u0627\u0646 \u062C\u0648\u0627\
          \u0646 \u0627\u0632 \u0633\u0627\u0631\u06CC \u060C \u0631\u0648\u0627\u0628\
          \u0637 \u0639\u0645\u0648\u0645\u06CC \u0627\u062F\u0627\u0631\u0647 \u06A9\
          \u0644 \u062D\u0641\u0627\u0638\u062A \u0645\u062D\u06CC\u0637 \u0632\u06CC\
          \u0633\u062A \u0645\u0627\u0632\u0646\u062F\u0631\u0627\u0646 \u0627\u0639\
          \u0644\u0627\u0645 \u06A9\u0631\u062F : \u0645\u0627\u0645\u0648\u0631\u0627\
          \u0646 \u06CC\u06AF\u0627\u0646 \u062D\u0641\u0627\u0638\u062A \u0645\u062D\
          \u06CC\u0637 \u0632\u06CC\u0633\u062A \u0622\u0645\u0644 \u062F\u0631 \u062D\
          \u06CC\u0646 \u06AF\u0634\u062A \u0648 \u06A9\u0646\u062A\u0631\u0644 \u062F\
          \u0631 \u0645\u0646\u0637\u0642\u0647 \u062D\u0641\u0627\u0638\u062A \u0634\
          \u062F\u0647 \u0645\u06CC\u0627\u0646\u06A9\u0627\u0644\u0647 \u0628\u0647\
          \ \u06CC\u06A9 \u0634\u06A9\u0627\u0631\u0686\u06CC \u063A\u06CC\u0631\u0645\
          \u062C\u0627\u0632 \u06A9\u0647 \u062F\u0631 \u062D\u0627\u0644 \u0634\u06A9\
          \u0627\u0631 \u06CC\u06A9 \u0631\u0627\u0633 \u06A9\u0644 \u0648\u062D\u0634\
          \u06CC \u0628\u0648\u062F \u060C \u0645\u0634\u06A9\u0648\u06A9 \u0634\u062F\
          \u0646\u062F.\n```\nAny help is appreciated"
        updatedAt: '2023-09-08T08:48:58.294Z'
      numEdits: 1
      reactions: []
    id: 64fadfc7304b8cb412a9139b
    type: comment
  author: mnwato
  content: "I used pipeline based inference and it was ok. But when generate manually,\
    \ output have lots of <unk> tokens:\n```\ninput: \u062A\u0648\u0627\u0646\u0627\
    \ \u0628\u0648\u062F \u0647\u0631 \u06A9\u0647\ngenerated: \u062A\u0648\u0627\u0646\
    \u0627 \u0628\u0648\u062F \u0647\u0631 \u06A9\u0647 \u0631\u0627 \u06A9\u0647\
    \ \u062A\u0648\u0627\u0646\u0627 \u0628\u0648\u062F\u0647 \u062F\u0631 \u0627\u06CC\
    \u0646 \u0631\u0648\u0632 \u06CC\u0627\u0631\u06CC \u062F\u0627\u062F \u0648 \u062F\
    \u0631 \u0631\u0627\u0647 \u062E\u062F\u0627 \u062C\u0647\u0627\u062F \u06A9\u0631\
    \u062F \u0648 \u0634\u0647\u06CC\u062F \u06A9\u0631\u062F\n```\n\n```sh\n    model_inputs\
    \ = tokenizer(sent, padding=True, return_tensors='pt').to('cuda')\n    generated_ids\
    \ = model.generate(**model_inputs, max_length=100)\n    generated_text = tokenizer.batch_decode(generated_ids,\
    \ skip_special_tokens=True)[0]\n```\n```\ngenerated: \u062A\u0648\u0627\u0646\u0627\
    \ \u0628\u0648\u062F \u0647\u0631 \u06A9\u0647 \u0628\u0647\u06A9\u0633\u062A\u0627\
    \u0631\u0628\u064F\u0646 \u0627\u0645\u0646\u062A<unk>\u0646\u067E \u0627\u0632\
    \u0631\u0641\u0627\u0647\u0623\u0628\u062A<unk>\u0647<unk>\u0646\u0646\u0627\u0631\
    \u06CC<unk>\u06A9<unk>\u062A<unk>\u0632<unk>\u062A<unk>\u0646\u064F<unk>\u062E\
    \u062C\u064E\u062B\u0651\u0647\u062A\u0646\u0648\u0634\u06A9\u0628<unk>\u062A\
    <unk>\u0628\u0627\u0628\u0645<unk>\u0647\u0627\u0646<unk>\u0647 \u06A9\u064F\u06A9\
    <unk>\u0646\u06CC<unk>\u06A9\u0646<unk><unk>\u06A9<unk> <unk>\u0628<unk>\u0644\
    \u0651\u062A \u0646<unk>\u06CC<unk> \u0628<unk><unk>\n```\n\nBut generating with\
    \ this script will generate output without <unk> token but different from pipeline\
    \ based inference.\n```sh\n    generated_ids = model.generate(input_ids=model_inputs['input_ids'],\
    \ max_length=100)\n```\n```\ngenerated: \u062A\u0648\u0627\u0646\u0627 \u0628\u0648\
    \u062F \u0647\u0631 \u06A9\u0647 \u0628\u0647 \u06AF\u0632\u0627\u0631\u0634 \u062E\
    \u0628\u0631\u0646\u06AF\u0627\u0631 \u06AF\u0631\u0648\u0647 \u0627\u0633\u062A\
    \u0627\u0646\u200C\u0647\u0627\u06CC \u0628\u0627\u0634\u06AF\u0627\u0647 \u062E\
    \u0628\u0631\u0646\u06AF\u0627\u0631\u0627\u0646 \u062C\u0648\u0627\u0646 \u0627\
    \u0632 \u0633\u0627\u0631\u06CC \u060C \u0631\u0648\u0627\u0628\u0637 \u0639\u0645\
    \u0648\u0645\u06CC \u0627\u062F\u0627\u0631\u0647 \u06A9\u0644 \u062D\u0641\u0627\
    \u0638\u062A \u0645\u062D\u06CC\u0637 \u0632\u06CC\u0633\u062A \u0645\u0627\u0632\
    \u0646\u062F\u0631\u0627\u0646 \u0627\u0639\u0644\u0627\u0645 \u06A9\u0631\u062F\
    \ : \u0645\u0627\u0645\u0648\u0631\u0627\u0646 \u06CC\u06AF\u0627\u0646 \u062D\
    \u0641\u0627\u0638\u062A \u0645\u062D\u06CC\u0637 \u0632\u06CC\u0633\u062A \u0622\
    \u0645\u0644 \u062F\u0631 \u062D\u06CC\u0646 \u06AF\u0634\u062A \u0648 \u06A9\u0646\
    \u062A\u0631\u0644 \u062F\u0631 \u0645\u0646\u0637\u0642\u0647 \u062D\u0641\u0627\
    \u0638\u062A \u0634\u062F\u0647 \u0645\u06CC\u0627\u0646\u06A9\u0627\u0644\u0647\
    \ \u0628\u0647 \u06CC\u06A9 \u0634\u06A9\u0627\u0631\u0686\u06CC \u063A\u06CC\u0631\
    \u0645\u062C\u0627\u0632 \u06A9\u0647 \u062F\u0631 \u062D\u0627\u0644 \u0634\u06A9\
    \u0627\u0631 \u06CC\u06A9 \u0631\u0627\u0633 \u06A9\u0644 \u0648\u062D\u0634\u06CC\
    \ \u0628\u0648\u062F \u060C \u0645\u0634\u06A9\u0648\u06A9 \u0634\u062F\u0646\u062F\
    .\n```\nAny help is appreciated"
  created_at: 2023-09-08 07:48:07+00:00
  edited: true
  hidden: false
  id: 64fadfc7304b8cb412a9139b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/71a6489e70f5c199a87ce817c196d5d6.svg
      fullname: Afshin Khashei
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: khashei
      type: user
    createdAt: '2023-09-09T13:56:31.000Z'
    data:
      edited: true
      editors:
      - khashei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9288963675498962
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/71a6489e70f5c199a87ce817c196d5d6.svg
          fullname: Afshin Khashei
          isHf: false
          isPro: false
          name: khashei
          type: user
        html: "<p>Generally, don\u2019t expect too much from this model, the model\
          \ is small and stupid. You can get the best response considering following:\
          \ 1) lower the temperature 2) give a longer input that is a correct in terms\
          \ of grammar and semantic 3) don\u2019t use any non persian characters in\
          \ your input, as mentioned in the doc this model only training on text with\
          \ persian characters. 4) note that this is not a poetry model that is used\
          \ in bolbolzaban.com so don\u2019t expect it to continue persian poetry.\
          \ 5) when you get  simply retry programmatically.</p>\n"
        raw: "Generally, don\u2019t expect too much from this model, the model is\
          \ small and stupid. You can get the best response considering following:\
          \ 1) lower the temperature 2) give a longer input that is a correct in terms\
          \ of grammar and semantic 3) don\u2019t use any non persian characters in\
          \ your input, as mentioned in the doc this model only training on text with\
          \ persian characters. 4) note that this is not a poetry model that is used\
          \ in bolbolzaban.com so don\u2019t expect it to continue persian poetry.\
          \ 5) when you get <unk> simply retry programmatically."
        updatedAt: '2023-09-09T14:00:01.858Z'
      numEdits: 1
      reactions: []
    id: 64fc798f365e3069d73fdc0d
    type: comment
  author: khashei
  content: "Generally, don\u2019t expect too much from this model, the model is small\
    \ and stupid. You can get the best response considering following: 1) lower the\
    \ temperature 2) give a longer input that is a correct in terms of grammar and\
    \ semantic 3) don\u2019t use any non persian characters in your input, as mentioned\
    \ in the doc this model only training on text with persian characters. 4) note\
    \ that this is not a poetry model that is used in bolbolzaban.com so don\u2019\
    t expect it to continue persian poetry. 5) when you get <unk> simply retry programmatically."
  created_at: 2023-09-09 12:56:31+00:00
  edited: true
  hidden: false
  id: 64fc798f365e3069d73fdc0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/449d0c4879ff617fea41e1e8128d3f2f.svg
      fullname: Mostafa Najmi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mnwato
      type: user
    createdAt: '2023-09-10T05:30:50.000Z'
    data:
      edited: false
      editors:
      - mnwato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9369791746139526
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/449d0c4879ff617fea41e1e8128d3f2f.svg
          fullname: Mostafa Najmi
          isHf: false
          isPro: false
          name: mnwato
          type: user
        html: "<blockquote>\n<p>Generally, don\u2019t expect too much from this model,\
          \ the model is small and stupid. You can get the best response considering\
          \ following: 1) lower the temperature 2) give a longer input that is a correct\
          \ in terms of grammar and semantic 3) don\u2019t use any non persian characters\
          \ in your input, as mentioned in the doc this model only training on text\
          \ with persian characters. 4) note that this is not a poetry model that\
          \ is used in bolbolzaban.com so don\u2019t expect it to continue persian\
          \ poetry. 5) when you get  simply retry programmatically.</p>\n</blockquote>\n\
          <p>Thanks for your response. You are right, but I think the problem I faced\
          \ is a mistake in the inference method. Although the model is small, it\
          \ generates the correct text (the one which is generated with the pipeline).\
          \ It seems to relate to tokenization.</p>\n<p>Actually, I plan to train\
          \ (or maybe fine-tune Open LLM models like Llama2) a task-specific GPT model\
          \ in Persian. What's your idea about fine-tuning or retraining 'bolbolzaban/gpt2-persian'\
          \ model to achieve better accuracy in a specific task?</p>\n"
        raw: "> Generally, don\u2019t expect too much from this model, the model is\
          \ small and stupid. You can get the best response considering following:\
          \ 1) lower the temperature 2) give a longer input that is a correct in terms\
          \ of grammar and semantic 3) don\u2019t use any non persian characters in\
          \ your input, as mentioned in the doc this model only training on text with\
          \ persian characters. 4) note that this is not a poetry model that is used\
          \ in bolbolzaban.com so don\u2019t expect it to continue persian poetry.\
          \ 5) when you get <unk> simply retry programmatically.\n\nThanks for your\
          \ response. You are right, but I think the problem I faced is a mistake\
          \ in the inference method. Although the model is small, it generates the\
          \ correct text (the one which is generated with the pipeline). It seems\
          \ to relate to tokenization.\n\nActually, I plan to train (or maybe fine-tune\
          \ Open LLM models like Llama2) a task-specific GPT model in Persian. What's\
          \ your idea about fine-tuning or retraining 'bolbolzaban/gpt2-persian' model\
          \ to achieve better accuracy in a specific task?"
        updatedAt: '2023-09-10T05:30:50.558Z'
      numEdits: 0
      reactions: []
    id: 64fd548a365e3069d756c2cd
    type: comment
  author: mnwato
  content: "> Generally, don\u2019t expect too much from this model, the model is\
    \ small and stupid. You can get the best response considering following: 1) lower\
    \ the temperature 2) give a longer input that is a correct in terms of grammar\
    \ and semantic 3) don\u2019t use any non persian characters in your input, as\
    \ mentioned in the doc this model only training on text with persian characters.\
    \ 4) note that this is not a poetry model that is used in bolbolzaban.com so don\u2019\
    t expect it to continue persian poetry. 5) when you get <unk> simply retry programmatically.\n\
    \nThanks for your response. You are right, but I think the problem I faced is\
    \ a mistake in the inference method. Although the model is small, it generates\
    \ the correct text (the one which is generated with the pipeline). It seems to\
    \ relate to tokenization.\n\nActually, I plan to train (or maybe fine-tune Open\
    \ LLM models like Llama2) a task-specific GPT model in Persian. What's your idea\
    \ about fine-tuning or retraining 'bolbolzaban/gpt2-persian' model to achieve\
    \ better accuracy in a specific task?"
  created_at: 2023-09-10 04:30:50+00:00
  edited: false
  hidden: false
  id: 64fd548a365e3069d756c2cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/71a6489e70f5c199a87ce817c196d5d6.svg
      fullname: Afshin Khashei
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: khashei
      type: user
    createdAt: '2023-09-10T13:55:43.000Z'
    data:
      edited: true
      editors:
      - khashei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8942457437515259
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/71a6489e70f5c199a87ce817c196d5d6.svg
          fullname: Afshin Khashei
          isHf: false
          isPro: false
          name: khashei
          type: user
        html: '<p>You are right the tokenizer could be the issue, I assume you noticed
          in the example that the tokenizer can be created with AutoTokenizer.from_pretrained(''bolbolzaban/gpt2-persian'').
          </p>

          <p>Regarding fine tuning of this model it depends on your task. In bolbolzaban
          model as mentioned in the post and the medium articles English characters
          are replaced with [LAT]. So if your usecase have mixed language text this
          is not a good one to fine tune. </p>

          '
        raw: "You are right the tokenizer could be the issue, I assume you noticed\
          \ in the example that the tokenizer can be created with AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian').\
          \ \n\nRegarding fine tuning of this model it depends on your task. In bolbolzaban\
          \ model as mentioned in the post and the medium articles English characters\
          \ are replaced with [LAT]. So if your usecase have mixed language text this\
          \ is not a good one to fine tune. "
        updatedAt: '2023-09-10T13:57:29.444Z'
      numEdits: 1
      reactions: []
    id: 64fdcadf7904ea30e6afbf85
    type: comment
  author: khashei
  content: "You are right the tokenizer could be the issue, I assume you noticed in\
    \ the example that the tokenizer can be created with AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian').\
    \ \n\nRegarding fine tuning of this model it depends on your task. In bolbolzaban\
    \ model as mentioned in the post and the medium articles English characters are\
    \ replaced with [LAT]. So if your usecase have mixed language text this is not\
    \ a good one to fine tune. "
  created_at: 2023-09-10 12:55:43+00:00
  edited: true
  hidden: false
  id: 64fdcadf7904ea30e6afbf85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/449d0c4879ff617fea41e1e8128d3f2f.svg
      fullname: Mostafa Najmi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mnwato
      type: user
    createdAt: '2023-09-10T19:48:20.000Z'
    data:
      edited: false
      editors:
      - mnwato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8520857691764832
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/449d0c4879ff617fea41e1e8128d3f2f.svg
          fullname: Mostafa Najmi
          isHf: false
          isPro: false
          name: mnwato
          type: user
        html: '<blockquote>

          <p>You are right the tokenizer could be the issue, I assume you noticed
          in the example that the tokenizer can be created with AutoTokenizer.from_pretrained(''bolbolzaban/gpt2-persian'').
          </p>

          <p>Regarding fine tuning of this model it depends on your task. In bolbolzaban
          model as mentioned in the post and the medium articles English characters
          are replaced with [LAT]. So if your usecase have mixed language text this
          is not a good one to fine tune.</p>

          </blockquote>

          <p>It is so weird because I have used AutoTokenizer.from_pretrained(''bolbolzaban/gpt2-persian'').</p>

          '
        raw: "> You are right the tokenizer could be the issue, I assume you noticed\
          \ in the example that the tokenizer can be created with AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian').\
          \ \n> \n> Regarding fine tuning of this model it depends on your task. In\
          \ bolbolzaban model as mentioned in the post and the medium articles English\
          \ characters are replaced with [LAT]. So if your usecase have mixed language\
          \ text this is not a good one to fine tune.\n\nIt is so weird because I\
          \ have used AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')."
        updatedAt: '2023-09-10T19:48:20.008Z'
      numEdits: 0
      reactions: []
    id: 64fe1d845ca946a010e6a379
    type: comment
  author: mnwato
  content: "> You are right the tokenizer could be the issue, I assume you noticed\
    \ in the example that the tokenizer can be created with AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian').\
    \ \n> \n> Regarding fine tuning of this model it depends on your task. In bolbolzaban\
    \ model as mentioned in the post and the medium articles English characters are\
    \ replaced with [LAT]. So if your usecase have mixed language text this is not\
    \ a good one to fine tune.\n\nIt is so weird because I have used AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')."
  created_at: 2023-09-10 18:48:20+00:00
  edited: false
  hidden: false
  id: 64fe1d845ca946a010e6a379
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: bolbolzaban/gpt2-persian
repo_type: model
status: open
target_branch: null
title: outputs with lots of <unk> tokens
