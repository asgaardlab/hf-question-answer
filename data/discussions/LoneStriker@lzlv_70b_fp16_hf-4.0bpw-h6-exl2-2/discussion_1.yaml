!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nexesenex
conflicting_files: null
created_at: 2023-12-28 08:14:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-12-28T08:14:22.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7657089233398438
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Hey LoneStriker,<br>I run a heterogeneous GPU setup with 36GB VRAM
          (3090+3060), and I wondered if you could provide a 3.50bpw exl2-2 quant
          of this LZLV model at least, and if possible, add it to the other 70b models
          that you quantize in exl2-2.<br>It would be optimal for my setting at 8k
          context in fp8 (and at least for a few others folks I spotted on Reddit
          with similar setup to mine), and spare me the purchase of a second 3090!
          ^^<br>Thanks for providing all these exl2 quants in any case!</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/-LqSVwzSVsOjrup6_J6KG.png"><img
          alt="Screenshot 2023-12-28 at 08-48-30 https __preview.redd.it_thjkpeb78g5c1.png
          width 664&amp;format png&amp;auto webp&amp;s 111c217d35d208c653ab719b3fb7b74405fa30dd.png"
          src="https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/-LqSVwzSVsOjrup6_J6KG.png"></a></p>

          '
        raw: "Hey LoneStriker,\r\nI run a heterogeneous GPU setup with 36GB VRAM (3090+3060),\
          \ and I wondered if you could provide a 3.50bpw exl2-2 quant of this LZLV\
          \ model at least, and if possible, add it to the other 70b models that you\
          \ quantize in exl2-2.\r\nIt would be optimal for my setting at 8k context\
          \ in fp8 (and at least for a few others folks I spotted on Reddit with similar\
          \ setup to mine), and spare me the purchase of a second 3090! ^^\r\nThanks\
          \ for providing all these exl2 quants in any case!\r\n\r\n![Screenshot 2023-12-28\
          \ at 08-48-30 https __preview.redd.it_thjkpeb78g5c1.png width 664&format\
          \ png&auto webp&s 111c217d35d208c653ab719b3fb7b74405fa30dd.png](https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/-LqSVwzSVsOjrup6_J6KG.png)"
        updatedAt: '2023-12-28T08:14:22.084Z'
      numEdits: 0
      reactions: []
    id: 658d2e5ed92f514e67134b0f
    type: comment
  author: Nexesenex
  content: "Hey LoneStriker,\r\nI run a heterogeneous GPU setup with 36GB VRAM (3090+3060),\
    \ and I wondered if you could provide a 3.50bpw exl2-2 quant of this LZLV model\
    \ at least, and if possible, add it to the other 70b models that you quantize\
    \ in exl2-2.\r\nIt would be optimal for my setting at 8k context in fp8 (and at\
    \ least for a few others folks I spotted on Reddit with similar setup to mine),\
    \ and spare me the purchase of a second 3090! ^^\r\nThanks for providing all these\
    \ exl2 quants in any case!\r\n\r\n![Screenshot 2023-12-28 at 08-48-30 https __preview.redd.it_thjkpeb78g5c1.png\
    \ width 664&format png&auto webp&s 111c217d35d208c653ab719b3fb7b74405fa30dd.png](https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/-LqSVwzSVsOjrup6_J6KG.png)"
  created_at: 2023-12-28 08:14:22+00:00
  edited: false
  hidden: false
  id: 658d2e5ed92f514e67134b0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-29T04:32:54.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9884098768234253
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I''ll add it to the list of bpw. I can''t go back and add this to
          all of the quants, though, as that would take ages to churn through. If
          you have list of favorite models, let me know though.</p>

          '
        raw: I'll add it to the list of bpw. I can't go back and add this to all of
          the quants, though, as that would take ages to churn through. If you have
          list of favorite models, let me know though.
        updatedAt: '2023-12-29T04:32:54.150Z'
      numEdits: 0
      reactions: []
    id: 658e4bf6f0152a21fc975c31
    type: comment
  author: LoneStriker
  content: I'll add it to the list of bpw. I can't go back and add this to all of
    the quants, though, as that would take ages to churn through. If you have list
    of favorite models, let me know though.
  created_at: 2023-12-29 04:32:54+00:00
  edited: false
  hidden: false
  id: 658e4bf6f0152a21fc975c31
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-29T09:13:26.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4784332513809204
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>3.5bpw up: <a href="https://huggingface.co/LoneStriker/lzlv_70b_fp16_hf-3.5bpw-h6-exl2">https://huggingface.co/LoneStriker/lzlv_70b_fp16_hf-3.5bpw-h6-exl2</a></p>

          '
        raw: '3.5bpw up: https://huggingface.co/LoneStriker/lzlv_70b_fp16_hf-3.5bpw-h6-exl2'
        updatedAt: '2023-12-29T09:13:26.881Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 658e8db677105e6e40ae1112
    type: comment
  author: LoneStriker
  content: '3.5bpw up: https://huggingface.co/LoneStriker/lzlv_70b_fp16_hf-3.5bpw-h6-exl2'
  created_at: 2023-12-29 09:13:26+00:00
  edited: false
  hidden: false
  id: 658e8db677105e6e40ae1112
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-31T07:20:41.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9755738377571106
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I reran a bunch of the requested 3.5bpw quants as well.</p>

          '
        raw: I reran a bunch of the requested 3.5bpw quants as well.
        updatedAt: '2023-12-31T07:20:41.250Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 6591164950d39af7f420132d
    type: comment
  author: LoneStriker
  content: I reran a bunch of the requested 3.5bpw quants as well.
  created_at: 2023-12-31 07:20:41+00:00
  edited: false
  hidden: false
  id: 6591164950d39af7f420132d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-12-31T14:22:36.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9684398770332336
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Thank you very much, Lonestriker.<br>I''m testing them all, plus
          many older models in Oobabooga.<br>I will report a big table of my perplexity
          results (for all of them, it''s still a pertinent baseline indicator) and
          usage impression (for some of them) next week-end, including the quants
          that you made at my request and several others models you quantized.<br>For
          now, I can tell that Dophin 2.2 3.5bpw has a problem (low quality feeling
          and ppl of 6.6 vs 4.8 for others at 512ctx), while XWin 70b 3.5bpw seems
          to be broken (ppl 76000 at 512ctx and gibberish output, maybe there''s a
          parameter I don''t know about to set?), while the rest behaves as expected.<br>I
          also learned to install and use exllamav2 on my Windows setup and started
          to make my own quants, but on 1 to 14b models only because my hardware is
          still short for longer quants to realize. I''m hasty to see the wheel 0.0.12
          out, because I''m unable to create the dev wheels by myself.<br>Also, I
          know it''s some work to ask, but for the sake of testing, could you eventually
          provide a exl2-2 3.75bpw quant for lzlv 70b for my tests? I like very much
          that model due to the XWin base.</p>

          '
        raw: 'Thank you very much, Lonestriker.

          I''m testing them all, plus many older models in Oobabooga.

          I will report a big table of my perplexity results (for all of them, it''s
          still a pertinent baseline indicator) and usage impression (for some of
          them) next week-end, including the quants that you made at my request and
          several others models you quantized.

          For now, I can tell that Dophin 2.2 3.5bpw has a problem (low quality feeling
          and ppl of 6.6 vs 4.8 for others at 512ctx), while XWin 70b 3.5bpw seems
          to be broken (ppl 76000 at 512ctx and gibberish output, maybe there''s a
          parameter I don''t know about to set?), while the rest behaves as expected.

          I also learned to install and use exllamav2 on my Windows setup and started
          to make my own quants, but on 1 to 14b models only because my hardware is
          still short for longer quants to realize. I''m hasty to see the wheel 0.0.12
          out, because I''m unable to create the dev wheels by myself.

          Also, I know it''s some work to ask, but for the sake of testing, could
          you eventually provide a exl2-2 3.75bpw quant for lzlv 70b for my tests?
          I like very much that model due to the XWin base.'
        updatedAt: '2023-12-31T14:42:57.037Z'
      numEdits: 1
      reactions: []
    id: 6591792c89145cbc7cf6c55d
    type: comment
  author: Nexesenex
  content: 'Thank you very much, Lonestriker.

    I''m testing them all, plus many older models in Oobabooga.

    I will report a big table of my perplexity results (for all of them, it''s still
    a pertinent baseline indicator) and usage impression (for some of them) next week-end,
    including the quants that you made at my request and several others models you
    quantized.

    For now, I can tell that Dophin 2.2 3.5bpw has a problem (low quality feeling
    and ppl of 6.6 vs 4.8 for others at 512ctx), while XWin 70b 3.5bpw seems to be
    broken (ppl 76000 at 512ctx and gibberish output, maybe there''s a parameter I
    don''t know about to set?), while the rest behaves as expected.

    I also learned to install and use exllamav2 on my Windows setup and started to
    make my own quants, but on 1 to 14b models only because my hardware is still short
    for longer quants to realize. I''m hasty to see the wheel 0.0.12 out, because
    I''m unable to create the dev wheels by myself.

    Also, I know it''s some work to ask, but for the sake of testing, could you eventually
    provide a exl2-2 3.75bpw quant for lzlv 70b for my tests? I like very much that
    model due to the XWin base.'
  created_at: 2023-12-31 14:22:36+00:00
  edited: true
  hidden: false
  id: 6591792c89145cbc7cf6c55d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2024-01-01T05:17:46.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46062394976615906
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p><a href="https://huggingface.co/LoneStriker/lzlv_70b_fp16_hf-3.75bpw-h6-exl2">https://huggingface.co/LoneStriker/lzlv_70b_fp16_hf-3.75bpw-h6-exl2</a></p>

          '
        raw: https://huggingface.co/LoneStriker/lzlv_70b_fp16_hf-3.75bpw-h6-exl2
        updatedAt: '2024-01-01T05:17:46.992Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65924afa89f1ff0463babaa2
    type: comment
  author: LoneStriker
  content: https://huggingface.co/LoneStriker/lzlv_70b_fp16_hf-3.75bpw-h6-exl2
  created_at: 2024-01-01 05:17:46+00:00
  edited: false
  hidden: false
  id: 65924afa89f1ff0463babaa2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/lzlv_70b_fp16_hf-4.0bpw-h6-exl2-2
repo_type: model
status: open
target_branch: null
title: 3.5bpw quant of 70b model
