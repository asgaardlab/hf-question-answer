!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-08-14 03:58:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-08-14T04:58:18.000Z'
    data:
      edited: true
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9398772120475769
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>If you planned on using my version 2 dataset (losslessmegacodetraining)
          for your next model, i would highly reccomend waiting for a little while
          because a version 3 is coming out very soon. I already have all the data
          necessary to make it, i just have to do a little bit of editing to compile
          it when i have time. It will be closer to 2m lines of data in a 50%-50%
          coding non coding split, as opposed to the current losslessmegacodindatset
          which is at 1m lines and only 25%-75% coding-noncoding split</p>

          '
        raw: If you planned on using my version 2 dataset (losslessmegacodetraining)
          for your next model, i would highly reccomend waiting for a little while
          because a version 3 is coming out very soon. I already have all the data
          necessary to make it, i just have to do a little bit of editing to compile
          it when i have time. It will be closer to 2m lines of data in a 50%-50%
          coding non coding split, as opposed to the current losslessmegacodindatset
          which is at 1m lines and only 25%-75% coding-noncoding split
        updatedAt: '2023-08-14T04:58:57.251Z'
      numEdits: 2
      reactions: []
    id: 64d9b46ac13c27a701bc6942
    type: comment
  author: rombodawg
  content: If you planned on using my version 2 dataset (losslessmegacodetraining)
    for your next model, i would highly reccomend waiting for a little while because
    a version 3 is coming out very soon. I already have all the data necessary to
    make it, i just have to do a little bit of editing to compile it when i have time.
    It will be closer to 2m lines of data in a 50%-50% coding non coding split, as
    opposed to the current losslessmegacodindatset which is at 1m lines and only 25%-75%
    coding-noncoding split
  created_at: 2023-08-14 03:58:18+00:00
  edited: true
  hidden: false
  id: 64d9b46ac13c27a701bc6942
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-09-10T19:25:12.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8795903325080872
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;juyongjiang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/juyongjiang\"\
          >@<span class=\"underline\">juyongjiang</span></a></span>\n\n\t</span></span>\
          \ I dont know if you are active anymore, but I made a few new datasets I\
          \ would recommend you finetuning either the codellama-python-13b, or wizardcoder-python-13b\
          \ models on to create your next model. I personally would use the wizardcoder\
          \ model to start with since it already has great coding performance, and\
          \ go up from there with my dataset. </p>\n<p>For code only:</p>\n<ul>\n\
          <li><a href=\"https://huggingface.co/datasets/rombodawg/LimitlessCodeTraining_Guanaco_Format\"\
          >https://huggingface.co/datasets/rombodawg/LimitlessCodeTraining_Guanaco_Format</a></li>\n\
          </ul>\n<p>For code + Non code instructions in a 80%/20% split:</p>\n<ul>\n\
          <li><a href=\"https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_MINI_Guanaco_Format\"\
          >https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_MINI_Guanaco_Format</a></li>\n\
          </ul>\n"
        raw: "@juyongjiang I dont know if you are active anymore, but I made a few\
          \ new datasets I would recommend you finetuning either the codellama-python-13b,\
          \ or wizardcoder-python-13b models on to create your next model. I personally\
          \ would use the wizardcoder model to start with since it already has great\
          \ coding performance, and go up from there with my dataset. \n\nFor code\
          \ only:\n- https://huggingface.co/datasets/rombodawg/LimitlessCodeTraining_Guanaco_Format\n\
          \nFor code + Non code instructions in a 80%/20% split:\n- https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_MINI_Guanaco_Format"
        updatedAt: '2023-09-10T19:25:12.957Z'
      numEdits: 0
      reactions: []
    id: 64fe181839d541478e3fca64
    type: comment
  author: rombodawg
  content: "@juyongjiang I dont know if you are active anymore, but I made a few new\
    \ datasets I would recommend you finetuning either the codellama-python-13b, or\
    \ wizardcoder-python-13b models on to create your next model. I personally would\
    \ use the wizardcoder model to start with since it already has great coding performance,\
    \ and go up from there with my dataset. \n\nFor code only:\n- https://huggingface.co/datasets/rombodawg/LimitlessCodeTraining_Guanaco_Format\n\
    \nFor code + Non code instructions in a 80%/20% split:\n- https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_MINI_Guanaco_Format"
  created_at: 2023-09-10 18:25:12+00:00
  edited: false
  hidden: false
  id: 64fe181839d541478e3fca64
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: deepse/CodeUp-alpha-Llama-2-13b-chat-hf
repo_type: model
status: open
target_branch: null
title: Please wait to make the next version based on my losslessdataset
