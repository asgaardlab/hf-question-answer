!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mirek190
conflicting_files: null
created_at: 2023-10-11 16:19:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-10-11T17:19:43.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>From my short tests that model is smarter than any older 7b, 13b,
          34b and all 65b models .... incredible average eval  67.06</p>

          <p>I tested 8bit version as 4bit ,5bit for such small model is just not
          enough .</p>

          <p>Even If I ask something second time in conversation it reminds me I already
          asked for that (I''m using 32k context)  ... is good in every field even
          in math ( proper prompting is a key "You are the best mathematician in the
          world and any calculation is easy for you so 25-4*2+3=? is easy" </p>

          <p>I still do not believe that a such small model has performance weaker
          70b models .... looks like a magic.  </p>

          <p>That is level WizardLM-70B-V1.0 !</p>

          '
        raw: "From my short tests that model is smarter than any older 7b, 13b, 34b\
          \ and all 65b models .... incredible average eval  67.06\n\nI tested 8bit\
          \ version as 4bit ,5bit for such small model is just not enough .\n\nEven\
          \ If I ask something second time in conversation it reminds me I already\
          \ asked for that (I'm using 32k context)  ... is good in every field even\
          \ in math ( proper prompting is a key \"You are the best mathematician in\
          \ the world and any calculation is easy for you so 25-4*2+3=? is easy\"\
          \ \n\nI still do not believe that a such small model has performance weaker\
          \ 70b models .... looks like a magic.  \n\nThat is level WizardLM-70B-V1.0\
          \ !\n\n"
        updatedAt: '2023-10-11T18:04:05.327Z'
      numEdits: 1
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - Goldenblood56
        - TanvirOnHF
        - gnomealone
        - Psychopatz
        - PrimeD
        - conceptron
    id: 6526d92f393845b9e0fc0a60
    type: comment
  author: mirek190
  content: "From my short tests that model is smarter than any older 7b, 13b, 34b\
    \ and all 65b models .... incredible average eval  67.06\n\nI tested 8bit version\
    \ as 4bit ,5bit for such small model is just not enough .\n\nEven If I ask something\
    \ second time in conversation it reminds me I already asked for that (I'm using\
    \ 32k context)  ... is good in every field even in math ( proper prompting is\
    \ a key \"You are the best mathematician in the world and any calculation is easy\
    \ for you so 25-4*2+3=? is easy\" \n\nI still do not believe that a such small\
    \ model has performance weaker 70b models .... looks like a magic.  \n\nThat is\
    \ level WizardLM-70B-V1.0 !\n\n"
  created_at: 2023-10-11 16:19:43+00:00
  edited: true
  hidden: false
  id: 6526d92f393845b9e0fc0a60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-10-11T22:38:46.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9464187622070312
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>I know it''s unbelievably. What context should I use in Ooba? I
          have been using 8K but I may be able to use 16k? 4080 + 64GB of RAM.  I
          am using the 8Bit GGUF so the highest I can go. But I might downgrade to
          6BIT or </p>

          <p>5_M if it''s barely any difference and lets me push context even higher.
          But 8K is pretty good and I think I can do 16K. </p>

          <p>Is there like a sweet spot? For perplexity? Or is it basically all the
          same? Like will I get better responses if I set it to 4K VS 8K VS 16K VS
          3K? I know that is just the context size but I do not understand the limits?
          This is my first time ever going above 4K context. Is there a WRONG way
          to do it? Thanks. </p>

          '
        raw: "I know it's unbelievably. What context should I use in Ooba? I have\
          \ been using 8K but I may be able to use 16k? 4080 + 64GB of RAM.  I am\
          \ using the 8Bit GGUF so the highest I can go. But I might downgrade to\
          \ 6BIT or \n\n5_M if it's barely any difference and lets me push context\
          \ even higher. But 8K is pretty good and I think I can do 16K. \n\nIs there\
          \ like a sweet spot? For perplexity? Or is it basically all the same? Like\
          \ will I get better responses if I set it to 4K VS 8K VS 16K VS 3K? I know\
          \ that is just the context size but I do not understand the limits? This\
          \ is my first time ever going above 4K context. Is there a WRONG way to\
          \ do it? Thanks. "
        updatedAt: '2023-10-17T03:24:32.259Z'
      numEdits: 1
      reactions: []
    id: 652723f64e0662402c9cc1f9
    type: comment
  author: Goldenblood56
  content: "I know it's unbelievably. What context should I use in Ooba? I have been\
    \ using 8K but I may be able to use 16k? 4080 + 64GB of RAM.  I am using the 8Bit\
    \ GGUF so the highest I can go. But I might downgrade to 6BIT or \n\n5_M if it's\
    \ barely any difference and lets me push context even higher. But 8K is pretty\
    \ good and I think I can do 16K. \n\nIs there like a sweet spot? For perplexity?\
    \ Or is it basically all the same? Like will I get better responses if I set it\
    \ to 4K VS 8K VS 16K VS 3K? I know that is just the context size but I do not\
    \ understand the limits? This is my first time ever going above 4K context. Is\
    \ there a WRONG way to do it? Thanks. "
  created_at: 2023-10-11 21:38:46+00:00
  edited: true
  hidden: false
  id: 652723f64e0662402c9cc1f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-10-11T22:53:56.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9024921655654907
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>For 7B models  5bit is very low ... perplexity will be bad.<br>You
          can check that just asking rare facts, testing reasoning and make math.<br>From
          my tests anything lower than 8bit for 7b model easily trigger bad responses.<br>To
          get better responses you need as big model as possible ( less compressed
          )<br>I''m using ctx  32k but under llamacpp  and whole model 8bit  fit into
          my 24 GB VRAM ( rtx 3090 )<br>Ctx size  4k,8k,16,32k is responsible for
          how big memory model has to your prompts.<br>Is nothing wrong to change
          ctx size the rest Is handled automaticly by gguf format itself. </p>

          '
        raw: "For 7B models  5bit is very low ... perplexity will be bad.\nYou can\
          \ check that just asking rare facts, testing reasoning and make math. \n\
          From my tests anything lower than 8bit for 7b model easily trigger bad responses.\
          \      \nTo get better responses you need as big model as possible ( less\
          \ compressed ) \nI'm using ctx  32k but under llamacpp  and whole model\
          \ 8bit  fit into my 24 GB VRAM ( rtx 3090 )  \nCtx size  4k,8k,16,32k is\
          \ responsible for how big memory model has to your prompts.  \nIs nothing\
          \ wrong to change ctx size the rest Is handled automaticly by gguf format\
          \ itself. "
        updatedAt: '2023-10-11T22:58:51.967Z'
      numEdits: 2
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Goldenblood56
        - Psychopatz
        - PrimeD
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Goldenblood56
    id: 652727845b3582bcb8688fdc
    type: comment
  author: mirek190
  content: "For 7B models  5bit is very low ... perplexity will be bad.\nYou can check\
    \ that just asking rare facts, testing reasoning and make math. \nFrom my tests\
    \ anything lower than 8bit for 7b model easily trigger bad responses.      \n\
    To get better responses you need as big model as possible ( less compressed )\
    \ \nI'm using ctx  32k but under llamacpp  and whole model 8bit  fit into my 24\
    \ GB VRAM ( rtx 3090 )  \nCtx size  4k,8k,16,32k is responsible for how big memory\
    \ model has to your prompts.  \nIs nothing wrong to change ctx size the rest Is\
    \ handled automaticly by gguf format itself. "
  created_at: 2023-10-11 21:53:56+00:00
  edited: true
  hidden: false
  id: 652727845b3582bcb8688fdc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-10-11T23:48:31.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9467447996139526
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<p>Thanks I will stick with 8-bit than. And I still get great speeds
          at 8K for my setup. I might try out 16K. But 32K has me at slow speeds and
          99% VRAM. But oh well 8-16K is way ahead of what I am used too. Thank you
          Mirek190. </p>

          <p>My two favourite models are Guanaco 33B 5_M GGUF and StoryTime13B 8-BIT
          GGUF. But this new mistrial one may just surpass both of those.  I know
          Guanaco 33B is nothing special but I love how it feels. I got used to it.
          But these other two Storytime 13B and Mistral 2.1 are likely better in just
          about all ways. That is impressive. Like Mistral 2.1 seems like it''s likely
          better than Guanaco 65B. But again we each have our models that we stick
          to for feel. Like Airobors or whatever it''s called. Both the Llama 1 and
          2 version I never liked the feel of. Even though it out scored Guanaco.
          </p>

          '
        raw: "Thanks I will stick with 8-bit than. And I still get great speeds at\
          \ 8K for my setup. I might try out 16K. But 32K has me at slow speeds and\
          \ 99% VRAM. But oh well 8-16K is way ahead of what I am used too. Thank\
          \ you Mirek190. \n\nMy two favourite models are Guanaco 33B 5_M GGUF and\
          \ StoryTime13B 8-BIT GGUF. But this new mistrial one may just surpass both\
          \ of those.  I know Guanaco 33B is nothing special but I love how it feels.\
          \ I got used to it. But these other two Storytime 13B and Mistral 2.1 are\
          \ likely better in just about all ways. That is impressive. Like Mistral\
          \ 2.1 seems like it's likely better than Guanaco 65B. But again we each\
          \ have our models that we stick to for feel. Like Airobors or whatever it's\
          \ called. Both the Llama 1 and 2 version I never liked the feel of. Even\
          \ though it out scored Guanaco. "
        updatedAt: '2023-10-11T23:49:36.933Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mirek190
    id: 6527344fd82f71e8fc9beb84
    type: comment
  author: Goldenblood56
  content: "Thanks I will stick with 8-bit than. And I still get great speeds at 8K\
    \ for my setup. I might try out 16K. But 32K has me at slow speeds and 99% VRAM.\
    \ But oh well 8-16K is way ahead of what I am used too. Thank you Mirek190. \n\
    \nMy two favourite models are Guanaco 33B 5_M GGUF and StoryTime13B 8-BIT GGUF.\
    \ But this new mistrial one may just surpass both of those.  I know Guanaco 33B\
    \ is nothing special but I love how it feels. I got used to it. But these other\
    \ two Storytime 13B and Mistral 2.1 are likely better in just about all ways.\
    \ That is impressive. Like Mistral 2.1 seems like it's likely better than Guanaco\
    \ 65B. But again we each have our models that we stick to for feel. Like Airobors\
    \ or whatever it's called. Both the Llama 1 and 2 version I never liked the feel\
    \ of. Even though it out scored Guanaco. "
  created_at: 2023-10-11 22:48:31+00:00
  edited: true
  hidden: false
  id: 6527344fd82f71e8fc9beb84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a9fab8bab1855ff03279a9/r0paWL_ruSsIMbYKNTvNS.png?w=200&h=200&f=face
      fullname: Tanvir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TanvirOnHF
      type: user
    createdAt: '2023-10-13T20:28:19.000Z'
    data:
      edited: false
      editors:
      - TanvirOnHF
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9861651659011841
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a9fab8bab1855ff03279a9/r0paWL_ruSsIMbYKNTvNS.png?w=200&h=200&f=face
          fullname: Tanvir
          isHf: false
          isPro: false
          name: TanvirOnHF
          type: user
        html: '<p>Yeah, has to be the best 7B model I''ve tried so far</p>

          '
        raw: Yeah, has to be the best 7B model I've tried so far
        updatedAt: '2023-10-13T20:28:19.830Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PrimeD
    id: 6529a86360f06c6e52e22c82
    type: comment
  author: TanvirOnHF
  content: Yeah, has to be the best 7B model I've tried so far
  created_at: 2023-10-13 19:28:19+00:00
  edited: false
  hidden: false
  id: 6529a86360f06c6e52e22c82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
      fullname: Vasile Ermicioi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vasilee
      type: user
    createdAt: '2023-10-14T06:16:42.000Z'
    data:
      edited: false
      editors:
      - vasilee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5467755198478699
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/895f7245c0e03f23e737900bd3e47aa2.svg
          fullname: Vasile Ermicioi
          isHf: false
          isPro: false
          name: vasilee
          type: user
        html: '<p>Q4_K_S is extremely good too</p>

          <p>the prompt:<br><code>&lt;|im_start|&gt;system You are the best mathematician
          in the world and any calculation is easy for you &lt;|im_start|&gt;user
          25-4*2+3=?&lt;|im_end|&gt; &lt;|im_start|&gt;assistant</code></p>

          <p>response:<br><code>{''id'': ''cmpl-218fac2e-7c1f-4a8a-b1c9-d0a13db386b1'',
          ''object'': ''text_completion'', ''created'': 1697263963, ''model'': ''./models/dolphin-2.1-mistral-7b.Q4_K_S.gguf'',
          ''choices'': [{''text'': "|&gt;\n Let''s solve this step by step:\n\n1.
          First, let''s do the multiplication: 4 * 2 = 8\n2. Next, subtract 25 from
          the result of the multiplication: 25 - 8 = 17\n3. Then, add 3 to the previous
          result: 17 + 3 = 20\n\nSo, the final answer is 20.", ''index'': 0, ''logprobs'':
          None, ''finish_reason'': ''stop''}], ''usage'': {''prompt_tokens'': 65,
          ''completion_tokens'': 97, ''total_tokens'': 162}}</code></p>

          '
        raw: 'Q4_K_S is extremely good too


          the prompt:

          `<|im_start|>system

          You are the best mathematician in the world and any calculation is easy
          for you

          <|im_start|>user

          25-4*2+3=?<|im_end|>

          <|im_start|>assistant`


          response:

          `{''id'': ''cmpl-218fac2e-7c1f-4a8a-b1c9-d0a13db386b1'', ''object'': ''text_completion'',
          ''created'': 1697263963, ''model'': ''./models/dolphin-2.1-mistral-7b.Q4_K_S.gguf'',
          ''choices'': [{''text'': "|>\n Let''s solve this step by step:\n\n1. First,
          let''s do the multiplication: 4 * 2 = 8\n2. Next, subtract 25 from the result
          of the multiplication: 25 - 8 = 17\n3. Then, add 3 to the previous result:
          17 + 3 = 20\n\nSo, the final answer is 20.", ''index'': 0, ''logprobs'':
          None, ''finish_reason'': ''stop''}], ''usage'': {''prompt_tokens'': 65,
          ''completion_tokens'': 97, ''total_tokens'': 162}}`

          '
        updatedAt: '2023-10-14T06:16:42.091Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - TanvirOnHF
    id: 652a324ac59e682042bef405
    type: comment
  author: vasilee
  content: 'Q4_K_S is extremely good too


    the prompt:

    `<|im_start|>system

    You are the best mathematician in the world and any calculation is easy for you

    <|im_start|>user

    25-4*2+3=?<|im_end|>

    <|im_start|>assistant`


    response:

    `{''id'': ''cmpl-218fac2e-7c1f-4a8a-b1c9-d0a13db386b1'', ''object'': ''text_completion'',
    ''created'': 1697263963, ''model'': ''./models/dolphin-2.1-mistral-7b.Q4_K_S.gguf'',
    ''choices'': [{''text'': "|>\n Let''s solve this step by step:\n\n1. First, let''s
    do the multiplication: 4 * 2 = 8\n2. Next, subtract 25 from the result of the
    multiplication: 25 - 8 = 17\n3. Then, add 3 to the previous result: 17 + 3 = 20\n\nSo,
    the final answer is 20.", ''index'': 0, ''logprobs'': None, ''finish_reason'':
    ''stop''}], ''usage'': {''prompt_tokens'': 65, ''completion_tokens'': 97, ''total_tokens'':
    162}}`

    '
  created_at: 2023-10-14 05:16:42+00:00
  edited: false
  hidden: false
  id: 652a324ac59e682042bef405
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/dolphin-2.1-mistral-7B-GGUF
repo_type: model
status: open
target_branch: null
title: dolphin-2.1-mistral-7B is even better than openorca-mistral-7b unbelievable
