!!python/object:huggingface_hub.community.DiscussionWithDetails
author: syddharth
conflicting_files: null
created_at: 2023-05-17 09:22:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14401ff770324e5795e23b89bc5236a6.svg
      fullname: S M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: syddharth
      type: user
    createdAt: '2023-05-17T10:22:30.000Z'
    data:
      edited: false
      editors:
      - syddharth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14401ff770324e5795e23b89bc5236a6.svg
          fullname: S M
          isHf: false
          isPro: false
          name: syddharth
          type: user
        html: '<p>One gets the following error when trying to use the LoRA with Vicuna-7B
          model.<br>RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:<br>        size
          mismatch for base_model.model.lm_head.lora_B.llama-deus-7b-v3-lora.weight:
          copying a param with shape torch.Size([32000, 128]) from checkpoint, the
          shape in current model is torch.Size([32001, 128]).</p>

          <p>Can the model only be used with Llama models?</p>

          '
        raw: "One gets the following error when trying to use the LoRA with Vicuna-7B\
          \ model.\r\nRuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:\r\
          \n        size mismatch for base_model.model.lm_head.lora_B.llama-deus-7b-v3-lora.weight:\
          \ copying a param with shape torch.Size([32000, 128]) from checkpoint, the\
          \ shape in current model is torch.Size([32001, 128]).\r\n\r\nCan the model\
          \ only be used with Llama models?"
        updatedAt: '2023-05-17T10:22:30.777Z'
      numEdits: 0
      reactions: []
    id: 6464aae602f8858f21297446
    type: comment
  author: syddharth
  content: "One gets the following error when trying to use the LoRA with Vicuna-7B\
    \ model.\r\nRuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:\r\
    \n        size mismatch for base_model.model.lm_head.lora_B.llama-deus-7b-v3-lora.weight:\
    \ copying a param with shape torch.Size([32000, 128]) from checkpoint, the shape\
    \ in current model is torch.Size([32001, 128]).\r\n\r\nCan the model only be used\
    \ with Llama models?"
  created_at: 2023-05-17 09:22:30+00:00
  edited: false
  hidden: false
  id: 6464aae602f8858f21297446
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14401ff770324e5795e23b89bc5236a6.svg
      fullname: S M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: syddharth
      type: user
    createdAt: '2023-05-17T11:35:03.000Z'
    data:
      edited: false
      editors:
      - syddharth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14401ff770324e5795e23b89bc5236a6.svg
          fullname: S M
          isHf: false
          isPro: false
          name: syddharth
          type: user
        html: '<p>Got it running with llama-7b-hf.</p>

          '
        raw: Got it running with llama-7b-hf.
        updatedAt: '2023-05-17T11:35:03.717Z'
      numEdits: 0
      reactions: []
    id: 6464bbe706cd98685aa0abe9
    type: comment
  author: syddharth
  content: Got it running with llama-7b-hf.
  created_at: 2023-05-17 10:35:03+00:00
  edited: false
  hidden: false
  id: 6464bbe706cd98685aa0abe9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-05-18T01:27:35.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>Sorry didnt see til just now, yeah I believe it would only work
          with the base model, but I''ve never actually tried merging a lora with
          a different fine tune :o</p>

          '
        raw: Sorry didnt see til just now, yeah I believe it would only work with
          the base model, but I've never actually tried merging a lora with a different
          fine tune :o
        updatedAt: '2023-05-18T01:27:35.416Z'
      numEdits: 0
      reactions: []
    id: 64657f0786e668ad22eae328
    type: comment
  author: teknium
  content: Sorry didnt see til just now, yeah I believe it would only work with the
    base model, but I've never actually tried merging a lora with a different fine
    tune :o
  created_at: 2023-05-18 00:27:35+00:00
  edited: false
  hidden: false
  id: 64657f0786e668ad22eae328
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: teknium/llama-deus-7b-v3-lora
repo_type: model
status: open
target_branch: null
title: RuntimeError with Vicuna-7B, using with oobabooga text-generation-webui
