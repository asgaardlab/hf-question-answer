!!python/object:huggingface_hub.community.DiscussionWithDetails
author: flyingfishinwater
conflicting_files: null
created_at: 2023-11-20 16:04:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6439ec168e2145846439bd31/6A521n1sACt1jP4BlL-VT.jpeg?w=200&h=200&f=face
      fullname: Flying Fish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: flyingfishinwater
      type: user
    createdAt: '2023-11-20T16:04:35.000Z'
    data:
      edited: false
      editors:
      - flyingfishinwater
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.941982626914978
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6439ec168e2145846439bd31/6A521n1sACt1jP4BlL-VT.jpeg?w=200&h=200&f=face
          fullname: Flying Fish
          isHf: false
          isPro: true
          name: flyingfishinwater
          type: user
        html: "<p>Thanks for your excellent work <span data-props=\"{&quot;user&quot;:&quot;ehartford&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ehartford\"\
          >@<span class=\"underline\">ehartford</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Last week I read an article on fine-tuning: <a rel=\"nofollow\"\
          \ href=\"https://lightning.ai/pages/community/lora-insights/\">Finetuning\
          \ LLMs with LoRA and QLoRA: Insights from Hundreds of Experim</a>, In it:<br>\
          \  \"As a rule of thumb, it\u2019s usually common to choose an alpha that\
          \ is twice as large as the rank when finetuning LLMs (note that this is\
          \ different when working with diffusion models). Let\u2019s try this out\
          \ and see what happens when we increase alpha two-fold\"<br>  <a rel=\"\
          nofollow\" href=\"https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage8.jpg\"\
          ><img alt=\"lora-expimage8\" src=\"https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage8.jpg\"\
          ></a></p>\n<p>I noticed you set the lora_r=32 and lora_alpha=16 in the axolotl\
          \ yml file, which didn't comply with the common rule. May I know the reason\
          \ behind it? I'm just curious about it.</p>\n<p>Thanks and have a nice day!</p>\n"
        raw: "Thanks for your excellent work @ehartford \r\n\r\nLast week I read an\
          \ article on fine-tuning: [Finetuning LLMs with LoRA and QLoRA: Insights\
          \ from Hundreds of Experim](https://lightning.ai/pages/community/lora-insights/),\
          \ In it:\r\n  \"As a rule of thumb, it\u2019s usually common to choose an\
          \ alpha that is twice as large as the rank when finetuning LLMs (note that\
          \ this is different when working with diffusion models). Let\u2019s try\
          \ this out and see what happens when we increase alpha two-fold\"\r\n  ![lora-expimage8](https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage8.jpg)\r\
          \n\r\nI noticed you set the lora_r=32 and lora_alpha=16 in the axolotl yml\
          \ file, which didn't comply with the common rule. May I know the reason\
          \ behind it? I'm just curious about it.\r\n\r\nThanks and have a nice day!"
        updatedAt: '2023-11-20T16:04:35.546Z'
      numEdits: 0
      reactions: []
    id: 655b8393b27f103d7b70f93f
    type: comment
  author: flyingfishinwater
  content: "Thanks for your excellent work @ehartford \r\n\r\nLast week I read an\
    \ article on fine-tuning: [Finetuning LLMs with LoRA and QLoRA: Insights from\
    \ Hundreds of Experim](https://lightning.ai/pages/community/lora-insights/), In\
    \ it:\r\n  \"As a rule of thumb, it\u2019s usually common to choose an alpha that\
    \ is twice as large as the rank when finetuning LLMs (note that this is different\
    \ when working with diffusion models). Let\u2019s try this out and see what happens\
    \ when we increase alpha two-fold\"\r\n  ![lora-expimage8](https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage8.jpg)\r\
    \n\r\nI noticed you set the lora_r=32 and lora_alpha=16 in the axolotl yml file,\
    \ which didn't comply with the common rule. May I know the reason behind it? I'm\
    \ just curious about it.\r\n\r\nThanks and have a nice day!"
  created_at: 2023-11-20 16:04:35+00:00
  edited: false
  hidden: false
  id: 655b8393b27f103d7b70f93f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-11-20T23:06:41.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9937001466751099
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>I actually don''t know!</p>

          <p>I could train it again with another setting if you think it would give
          better results.</p>

          '
        raw: 'I actually don''t know!


          I could train it again with another setting if you think it would give better
          results.'
        updatedAt: '2023-11-20T23:06:41.636Z'
      numEdits: 0
      reactions: []
    id: 655be681c4bb0926a677e0be
    type: comment
  author: ehartford
  content: 'I actually don''t know!


    I could train it again with another setting if you think it would give better
    results.'
  created_at: 2023-11-20 23:06:41+00:00
  edited: false
  hidden: false
  id: 655be681c4bb0926a677e0be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6439ec168e2145846439bd31/6A521n1sACt1jP4BlL-VT.jpeg?w=200&h=200&f=face
      fullname: Flying Fish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: flyingfishinwater
      type: user
    createdAt: '2023-11-21T16:14:19.000Z'
    data:
      edited: false
      editors:
      - flyingfishinwater
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9275065660476685
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6439ec168e2145846439bd31/6A521n1sACt1jP4BlL-VT.jpeg?w=200&h=200&f=face
          fullname: Flying Fish
          isHf: false
          isPro: true
          name: flyingfishinwater
          type: user
        html: '<p>I saw the <a rel="nofollow" href="https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/examples/llama-2/qlora.yml">llama2
          example in Axolotl</a> has the same settings.<br>The article I mentioned
          compares two settings: "lora_r=32, lora_alpha=64" with "lora_r=32, lora_alpha=1".
          The result of the first one is better than previous one.<br><a rel="nofollow"
          href="https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage14.jpg"><img
          alt="lora_r &amp; lora_alpha" src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage14.jpg"></a><br>But
          the article didn''t mention detail procedures among tests,  it''s hard to
          say the results are statistically significant. </p>

          <p>I tested smaller models under 7B, the results are not significant. However,
          since the models themself are weak, it can not prove if those settings are
          useful.<br>I didn''t have enough resources to test them on 34B or bigger
          ones. If you have time and resources, it''s appreciated if you can test
          the "lora_r=32, lora_alpha=64" on new trainings to check if they can get
          better scores on leaderboard. </p>

          <p>Thanks and have a nice day!</p>

          '
        raw: "I saw the [llama2 example in Axolotl](https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/examples/llama-2/qlora.yml)\
          \ has the same settings. \nThe article I mentioned compares two settings:\
          \ \"lora_r=32, lora_alpha=64\" with \"lora_r=32, lora_alpha=1\". The result\
          \ of the first one is better than previous one. \n![lora_r & lora_alpha](https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage14.jpg)\n\
          But the article didn't mention detail procedures among tests,  it's hard\
          \ to say the results are statistically significant. \n\nI tested smaller\
          \ models under 7B, the results are not significant. However, since the models\
          \ themself are weak, it can not prove if those settings are useful.\nI didn't\
          \ have enough resources to test them on 34B or bigger ones. If you have\
          \ time and resources, it's appreciated if you can test the \"lora_r=32,\
          \ lora_alpha=64\" on new trainings to check if they can get better scores\
          \ on leaderboard. \n\nThanks and have a nice day!"
        updatedAt: '2023-11-21T16:14:19.666Z'
      numEdits: 0
      reactions: []
    id: 655cd75b8b77e2c30b28cf80
    type: comment
  author: flyingfishinwater
  content: "I saw the [llama2 example in Axolotl](https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/examples/llama-2/qlora.yml)\
    \ has the same settings. \nThe article I mentioned compares two settings: \"lora_r=32,\
    \ lora_alpha=64\" with \"lora_r=32, lora_alpha=1\". The result of the first one\
    \ is better than previous one. \n![lora_r & lora_alpha](https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage14.jpg)\n\
    But the article didn't mention detail procedures among tests,  it's hard to say\
    \ the results are statistically significant. \n\nI tested smaller models under\
    \ 7B, the results are not significant. However, since the models themself are\
    \ weak, it can not prove if those settings are useful.\nI didn't have enough resources\
    \ to test them on 34B or bigger ones. If you have time and resources, it's appreciated\
    \ if you can test the \"lora_r=32, lora_alpha=64\" on new trainings to check if\
    \ they can get better scores on leaderboard. \n\nThanks and have a nice day!"
  created_at: 2023-11-21 16:14:19+00:00
  edited: false
  hidden: false
  id: 655cd75b8b77e2c30b28cf80
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: cognitivecomputations/samantha-yi-34b
repo_type: model
status: open
target_branch: null
title: A question about lora_r and lora_alpha configuration
