!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joujiboi
conflicting_files: null
created_at: 2023-12-17 14:20:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671130976305-631608b395b55e2621d033a8.png?w=200&h=200&f=face
      fullname: JawGBoi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joujiboi
      type: user
    createdAt: '2023-12-17T14:20:37.000Z'
    data:
      edited: false
      editors:
      - joujiboi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9823638200759888
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671130976305-631608b395b55e2621d033a8.png?w=200&h=200&f=face
          fullname: JawGBoi
          isHf: false
          isPro: false
          name: joujiboi
          type: user
        html: '<p>How much vram does/will each bpw require?</p>

          '
        raw: How much vram does/will each bpw require?
        updatedAt: '2023-12-17T14:20:37.012Z'
      numEdits: 0
      reactions:
      - count: 9
        reaction: "\U0001F44D"
        users:
        - kkosm
        - WhiteMemory99
        - eramax
        - CamiloMM
        - Palemaster
        - kimhyeongjun
        - Steelclaw
        - marcasmed
        - Incomple
    id: 657f03b51e2087032453cb6d
    type: comment
  author: joujiboi
  content: How much vram does/will each bpw require?
  created_at: 2023-12-17 14:20:37+00:00
  edited: false
  hidden: false
  id: 657f03b51e2087032453cb6d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-17T22:02:15.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9856830835342407
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: '<p>following</p>

          '
        raw: following
        updatedAt: '2023-12-17T22:02:15.261Z'
      numEdits: 0
      reactions: []
    id: 657f6fe7e37618d867c9bf65
    type: comment
  author: eramax
  content: following
  created_at: 2023-12-17 22:02:15+00:00
  edited: false
  hidden: false
  id: 657f6fe7e37618d867c9bf65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14570e9514625800a440f40fec36ed6e.svg
      fullname: Camilo Martin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CamiloMM
      type: user
    createdAt: '2023-12-18T01:28:04.000Z'
    data:
      edited: true
      editors:
      - CamiloMM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8477914929389954
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14570e9514625800a440f40fec36ed6e.svg
          fullname: Camilo Martin
          isHf: false
          isPro: false
          name: CamiloMM
          type: user
        html: '<p>Out of an abundance of caution I downloaded the 2.4bpw for my 24GB
          card (because 2.4bpw fits a 70b model) but, 2.4bpw should fit in a 16GB
          card instead. Which is awesome.</p>

          <p>Edit: Nevermind, 3.5bpw is quite clever. 2.4bpw is too dumb, don''t use
          that one. If you need to use 2.4bpw, use a 13B model instead, unless you
          really need the context or it works for you.</p>

          '
        raw: 'Out of an abundance of caution I downloaded the 2.4bpw for my 24GB card
          (because 2.4bpw fits a 70b model) but, 2.4bpw should fit in a 16GB card
          instead. Which is awesome.


          Edit: Nevermind, 3.5bpw is quite clever. 2.4bpw is too dumb, don''t use
          that one. If you need to use 2.4bpw, use a 13B model instead, unless you
          really need the context or it works for you.'
        updatedAt: '2023-12-18T14:04:13.836Z'
      numEdits: 3
      reactions: []
    id: 657fa024112a9ca545841394
    type: comment
  author: CamiloMM
  content: 'Out of an abundance of caution I downloaded the 2.4bpw for my 24GB card
    (because 2.4bpw fits a 70b model) but, 2.4bpw should fit in a 16GB card instead.
    Which is awesome.


    Edit: Nevermind, 3.5bpw is quite clever. 2.4bpw is too dumb, don''t use that one.
    If you need to use 2.4bpw, use a 13B model instead, unless you really need the
    context or it works for you.'
  created_at: 2023-12-18 01:28:04+00:00
  edited: true
  hidden: false
  id: 657fa024112a9ca545841394
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/26bcafddbba9cb9db010a3ad0591e102.svg
      fullname: Jonas D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jonwondo
      type: user
    createdAt: '2023-12-18T05:55:18.000Z'
    data:
      edited: false
      editors:
      - jonwondo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9911664724349976
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/26bcafddbba9cb9db010a3ad0591e102.svg
          fullname: Jonas D
          isHf: false
          isPro: false
          name: jonwondo
          type: user
        html: '<p>3.5bpw uses 22.2GB of VRAM. It looks like if a 3.7bpw were created
          it would still fit on a 24GB VRAM card and maybe perform slightly better.
          </p>

          '
        raw: '3.5bpw uses 22.2GB of VRAM. It looks like if a 3.7bpw were created it
          would still fit on a 24GB VRAM card and maybe perform slightly better. '
        updatedAt: '2023-12-18T05:55:18.645Z'
      numEdits: 0
      reactions: []
    id: 657fdec683543a061b97fb88
    type: comment
  author: jonwondo
  content: '3.5bpw uses 22.2GB of VRAM. It looks like if a 3.7bpw were created it
    would still fit on a 24GB VRAM card and maybe perform slightly better. '
  created_at: 2023-12-18 05:55:18+00:00
  edited: false
  hidden: false
  id: 657fdec683543a061b97fb88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-18T06:51:24.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9884207248687744
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<blockquote>

          <p>3.5bpw uses 22.2GB of VRAM. It looks like if a 3.7bpw were created it
          would still fit on a 24GB VRAM card and maybe perform slightly better.</p>

          </blockquote>

          <p>Is that with the full 32K context?</p>

          '
        raw: '> 3.5bpw uses 22.2GB of VRAM. It looks like if a 3.7bpw were created
          it would still fit on a 24GB VRAM card and maybe perform slightly better.


          Is that with the full 32K context?'
        updatedAt: '2023-12-18T06:51:24.516Z'
      numEdits: 0
      reactions: []
    id: 657febec6dc01435cdea0fdd
    type: comment
  author: brucethemoose
  content: '> 3.5bpw uses 22.2GB of VRAM. It looks like if a 3.7bpw were created it
    would still fit on a 24GB VRAM card and maybe perform slightly better.


    Is that with the full 32K context?'
  created_at: 2023-12-18 06:51:24+00:00
  edited: false
  hidden: false
  id: 657febec6dc01435cdea0fdd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-18T07:21:00.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8965558409690857
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>The answer is "not quite"</p>

          <p>Looks like you need 3.3bpw-3.4bpw to fit 32K on a completely empty 3090.
          </p>

          '
        raw: 'The answer is "not quite"


          Looks like you need 3.3bpw-3.4bpw to fit 32K on a completely empty 3090. '
        updatedAt: '2023-12-18T07:21:00.398Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - CamiloMM
        - the-qa-company
    id: 657ff2dc90d65df03d0c1f61
    type: comment
  author: brucethemoose
  content: 'The answer is "not quite"


    Looks like you need 3.3bpw-3.4bpw to fit 32K on a completely empty 3090. '
  created_at: 2023-12-18 07:21:00+00:00
  edited: false
  hidden: false
  id: 657ff2dc90d65df03d0c1f61
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-18T07:52:13.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8767392039299011
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Yeah, 28500 context is precisely what my GPU can fit before OOM.</p>

          '
        raw: Yeah, 28500 context is precisely what my GPU can fit before OOM.
        updatedAt: '2023-12-18T07:52:13.698Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - CamiloMM
        - the-qa-company
    id: 657ffa2d90d65df03d0d93cc
    type: comment
  author: brucethemoose
  content: Yeah, 28500 context is precisely what my GPU can fit before OOM.
  created_at: 2023-12-18 07:52:13+00:00
  edited: false
  hidden: false
  id: 657ffa2d90d65df03d0d93cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03e141d9f57876e34a1e45ce261da7e.svg
      fullname: Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AliCat2
      type: user
    createdAt: '2023-12-18T13:57:49.000Z'
    data:
      edited: true
      editors:
      - AliCat2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8665630221366882
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03e141d9f57876e34a1e45ce261da7e.svg
          fullname: Ali
          isHf: false
          isPro: false
          name: AliCat2
          type: user
        html: '<p>You can do the full 32K completely on a 3090 with 3.5bpw, fp16 cache,
          with about 1GB VRAM Windows usage, and without CUDA - Sysmen Fallback Policy
          while using TabbyAPI w/ CUDA 12.x, Python 3.11, and Flash Attention 2.</p>

          <p>The usage is about 22.6 GB~ VRAM at 32k (or about 23.6 including the
          1GB VRAM Windows usage)</p>

          <p>Is it worth it or practical? Not on my machine or with the current version
          as the T/s was about 0.13.<br><code>Metrics: 78 tokens generated in 585.05
          seconds (0.13 T/s, context 32353 tokens)</code></p>

          <p>Speed around 4k is fast:<br><code>Metrics: 90 tokens generated in 2.17
          seconds (41.43 T/s, context 4219 tokens) &lt;-- Not accurate (truncated)</code></p>

          '
        raw: 'You can do the full 32K completely on a 3090 with 3.5bpw, fp16 cache,
          with about 1GB VRAM Windows usage, and without CUDA - Sysmen Fallback Policy
          while using TabbyAPI w/ CUDA 12.x, Python 3.11, and Flash Attention 2.


          The usage is about 22.6 GB~ VRAM at 32k (or about 23.6 including the 1GB
          VRAM Windows usage)


          Is it worth it or practical? Not on my machine or with the current version
          as the T/s was about 0.13.

          `Metrics: 78 tokens generated in 585.05 seconds (0.13 T/s, context 32353
          tokens)`


          Speed around 4k is fast:

          `Metrics: 90 tokens generated in 2.17 seconds (41.43 T/s, context 4219 tokens)
          <-- Not accurate (truncated)`'
        updatedAt: '2023-12-18T14:31:28.593Z'
      numEdits: 2
      reactions: []
    id: 65804fddbc91d2ab5d6d0c24
    type: comment
  author: AliCat2
  content: 'You can do the full 32K completely on a 3090 with 3.5bpw, fp16 cache,
    with about 1GB VRAM Windows usage, and without CUDA - Sysmen Fallback Policy while
    using TabbyAPI w/ CUDA 12.x, Python 3.11, and Flash Attention 2.


    The usage is about 22.6 GB~ VRAM at 32k (or about 23.6 including the 1GB VRAM
    Windows usage)


    Is it worth it or practical? Not on my machine or with the current version as
    the T/s was about 0.13.

    `Metrics: 78 tokens generated in 585.05 seconds (0.13 T/s, context 32353 tokens)`


    Speed around 4k is fast:

    `Metrics: 90 tokens generated in 2.17 seconds (41.43 T/s, context 4219 tokens)
    <-- Not accurate (truncated)`'
  created_at: 2023-12-18 13:57:49+00:00
  edited: true
  hidden: false
  id: 65804fddbc91d2ab5d6d0c24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-18T15:10:40.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8993778824806213
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<blockquote>

          <p>You can do the full 32K completely on a 3090 with 3.5bpw, fp16 cache,
          with about 1GB VRAM Windows usage, and without CUDA - Sysmen Fallback Policy
          while using TabbyAPI w/ CUDA 12.x, Python 3.11, and Flash Attention 2.</p>

          <p>The usage is about 22.6 GB~ VRAM at 32k (or about 23.6 including the
          1GB VRAM Windows usage)</p>

          <p>Is it worth it or practical? Not on my machine or with the current version
          as the T/s was about 0.13.<br><code>Metrics: 78 tokens generated in 585.05
          seconds (0.13 T/s, context 32353 tokens)</code></p>

          <p>Speed around 4k is fast:<br><code>Metrics: 90 tokens generated in 2.17
          seconds (41.43 T/s, context 4219 tokens) &lt;-- Not accurate (truncated)</code></p>

          </blockquote>

          <p>I think that means you are actually OOMing, even if the monitor doesn''t
          show it? On linux I OOM hard at 28K, and its crazy fast up to then.</p>

          <p>Actually I can probably get it up a bit more by changing the chunk size...
          Exui defaults to 2048, other UIs/APIs default to less I think.</p>

          '
        raw: "> You can do the full 32K completely on a 3090 with 3.5bpw, fp16 cache,\
          \ with about 1GB VRAM Windows usage, and without CUDA - Sysmen Fallback\
          \ Policy while using TabbyAPI w/ CUDA 12.x, Python 3.11, and Flash Attention\
          \ 2.\n> \n> The usage is about 22.6 GB~ VRAM at 32k (or about 23.6 including\
          \ the 1GB VRAM Windows usage)\n> \n> Is it worth it or practical? Not on\
          \ my machine or with the current version as the T/s was about 0.13.\n> `Metrics:\
          \ 78 tokens generated in 585.05 seconds (0.13 T/s, context 32353 tokens)`\n\
          > \n> Speed around 4k is fast:\n> `Metrics: 90 tokens generated in 2.17\
          \ seconds (41.43 T/s, context 4219 tokens) <-- Not accurate (truncated)`\n\
          \nI think that means you are actually OOMing, even if the monitor doesn't\
          \ show it? On linux I OOM hard at 28K, and its crazy fast up to then.\n\n\
          Actually I can probably get it up a bit more by changing the chunk size...\
          \ Exui defaults to 2048, other UIs/APIs default to less I think."
        updatedAt: '2023-12-18T15:11:20.623Z'
      numEdits: 1
      reactions: []
    id: 658060f06fecfe89360f6a36
    type: comment
  author: brucethemoose
  content: "> You can do the full 32K completely on a 3090 with 3.5bpw, fp16 cache,\
    \ with about 1GB VRAM Windows usage, and without CUDA - Sysmen Fallback Policy\
    \ while using TabbyAPI w/ CUDA 12.x, Python 3.11, and Flash Attention 2.\n> \n\
    > The usage is about 22.6 GB~ VRAM at 32k (or about 23.6 including the 1GB VRAM\
    \ Windows usage)\n> \n> Is it worth it or practical? Not on my machine or with\
    \ the current version as the T/s was about 0.13.\n> `Metrics: 78 tokens generated\
    \ in 585.05 seconds (0.13 T/s, context 32353 tokens)`\n> \n> Speed around 4k is\
    \ fast:\n> `Metrics: 90 tokens generated in 2.17 seconds (41.43 T/s, context 4219\
    \ tokens) <-- Not accurate (truncated)`\n\nI think that means you are actually\
    \ OOMing, even if the monitor doesn't show it? On linux I OOM hard at 28K, and\
    \ its crazy fast up to then.\n\nActually I can probably get it up a bit more by\
    \ changing the chunk size... Exui defaults to 2048, other UIs/APIs default to\
    \ less I think."
  created_at: 2023-12-18 15:10:40+00:00
  edited: true
  hidden: false
  id: 658060f06fecfe89360f6a36
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03e141d9f57876e34a1e45ce261da7e.svg
      fullname: Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AliCat2
      type: user
    createdAt: '2023-12-18T18:27:37.000Z'
    data:
      edited: false
      editors:
      - AliCat2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9689527153968811
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03e141d9f57876e34a1e45ce261da7e.svg
          fullname: Ali
          isHf: false
          isPro: false
          name: AliCat2
          type: user
        html: '<blockquote>

          <p>I think that means you are actually OOMing, even if the monitor doesn''t
          show it? On linux I OOM hard at 28K, and its crazy fast up to then.</p>

          <p>Actually I can probably get it up a bit more by changing the chunk size...
          Exui defaults to 2048, other UIs/APIs default to less I think.</p>

          </blockquote>

          <p>I think you''re right. When double-checking the the Sysmen Fallback Policy,
          it appears it wasn''t on for this instance of python, which explains the
          slowdown, because it was likely splitting the GPU with CPU.</p>

          '
        raw: "> \n> I think that means you are actually OOMing, even if the monitor\
          \ doesn't show it? On linux I OOM hard at 28K, and its crazy fast up to\
          \ then.\n> \n> Actually I can probably get it up a bit more by changing\
          \ the chunk size... Exui defaults to 2048, other UIs/APIs default to less\
          \ I think.\n\nI think you're right. When double-checking the the Sysmen\
          \ Fallback Policy, it appears it wasn't on for this instance of python,\
          \ which explains the slowdown, because it was likely splitting the GPU with\
          \ CPU."
        updatedAt: '2023-12-18T18:27:37.172Z'
      numEdits: 0
      reactions: []
    id: 65808f19560c1557cfbf2c9c
    type: comment
  author: AliCat2
  content: "> \n> I think that means you are actually OOMing, even if the monitor\
    \ doesn't show it? On linux I OOM hard at 28K, and its crazy fast up to then.\n\
    > \n> Actually I can probably get it up a bit more by changing the chunk size...\
    \ Exui defaults to 2048, other UIs/APIs default to less I think.\n\nI think you're\
    \ right. When double-checking the the Sysmen Fallback Policy, it appears it wasn't\
    \ on for this instance of python, which explains the slowdown, because it was\
    \ likely splitting the GPU with CPU."
  created_at: 2023-12-18 18:27:37+00:00
  edited: false
  hidden: false
  id: 65808f19560c1557cfbf2c9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad9c50879fe45f61272f57c5a0c55800.svg
      fullname: Jason Baller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: goldrushgames
      type: user
    createdAt: '2023-12-19T21:52:59.000Z'
    data:
      edited: false
      editors:
      - goldrushgames
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8460156321525574
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad9c50879fe45f61272f57c5a0c55800.svg
          fullname: Jason Baller
          isHf: false
          isPro: false
          name: goldrushgames
          type: user
        html: '<p>This model is amazing, on par with GPT 3.5 or better...so far better
          than dolphin or other fine tuned</p>

          '
        raw: This model is amazing, on par with GPT 3.5 or better...so far better
          than dolphin or other fine tuned
        updatedAt: '2023-12-19T21:52:59.957Z'
      numEdits: 0
      reactions: []
    id: 658210bb5f6d83438253ec4f
    type: comment
  author: goldrushgames
  content: This model is amazing, on par with GPT 3.5 or better...so far better than
    dolphin or other fine tuned
  created_at: 2023-12-19 21:52:59+00:00
  edited: false
  hidden: false
  id: 658210bb5f6d83438253ec4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2023-12-20T10:24:18.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9787366986274719
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: '<p>fyi. I was able to load this model (2.4bpw) on colab T4 instance
          with 15GB Vram</p>

          '
        raw: fyi. I was able to load this model (2.4bpw) on colab T4 instance with
          15GB Vram
        updatedAt: '2023-12-20T10:24:18.565Z'
      numEdits: 0
      reactions: []
    id: 6582c0d2d73d6402f7b875f2
    type: comment
  author: eramax
  content: fyi. I was able to load this model (2.4bpw) on colab T4 instance with 15GB
    Vram
  created_at: 2023-12-20 10:24:18+00:00
  edited: false
  hidden: false
  id: 6582c0d2d73d6402f7b875f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/48f5d78bb7aedba2b9739b13454be4dd.svg
      fullname: 'Navarro '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jostack
      type: user
    createdAt: '2023-12-20T23:26:01.000Z'
    data:
      edited: false
      editors:
      - Jostack
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.924360454082489
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/48f5d78bb7aedba2b9739b13454be4dd.svg
          fullname: 'Navarro '
          isHf: false
          isPro: false
          name: Jostack
          type: user
        html: "<p>Running inference on model: /home/neuron/exllamav2/models/Mixtral-8x7B-instruct-exl2_2.4bpw<br>\
          \ -- Model: /home/neuron/exllamav2/models/Mixtral-8x7B-instruct-exl2_2.4bpw<br>\
          \ -- Options: ['gpu_split: 23,23', 'rope_scale: 1.0', 'rope_alpha: 1.0']<br>\
          \ -- Loading model...<br> -- Loading tokenizer...<br> -- Warmup...<br> --\
          \ Generating...</p>\n<p>Once upon a time,TDM was the largest and most popular\
          \ game mode in CS:GO. Today, it is still one of the most played modes, but\
          \ it has been overtaken by Valorant\u2019s \u201CSolo queue\u201D mode.</p>\n\
          <p>The reason for this change is that a lot of gamers don\u2019t like playing\
          \ with strangers. They prefer to play with friends or known teammates. This\
          \ makes TDM less attractive for many players.</p>\n<p>However, there are\
          \ still plenty of reasons to play TDM in CS:GO. Here are some of them:</p>\n\
          <ol>\n<li>Teamwork: TDM is all about teamwork. You need to work together\
          \ with your teammates to eliminate the enemy team. This makes it a great\
          \ mode for practicing teamwork skills.</li>\n<li>Fun: TDM is a lot of fun.\
          \ It\u2019s fast-paced and action-packed. You never know what\u2019s going\
          \ to happen next.</li>\n<li>Competition: TDM is a competitive mode. You\
          \ can earn points for each kill, and the team with the most points wins.\
          \ This makes it a great mode for practicing your competitive skills.</li>\n\
          <li>Variety: TDM offers a lot of variety. There are different maps, weapons,</li>\n\
          </ol>\n<p> -- Response generated in 5.91 seconds, 256 tokens, 43.33 tokens/second\
          \ (includes prompt eval.)</p>\n<p>2 rtx 3090 nvlink</p>\n"
        raw: "Running inference on model: /home/neuron/exllamav2/models/Mixtral-8x7B-instruct-exl2_2.4bpw\n\
          \ -- Model: /home/neuron/exllamav2/models/Mixtral-8x7B-instruct-exl2_2.4bpw\n\
          \ -- Options: ['gpu_split: 23,23', 'rope_scale: 1.0', 'rope_alpha: 1.0']\n\
          \ -- Loading model...\n -- Loading tokenizer...\n -- Warmup...\n -- Generating...\n\
          \nOnce upon a time,TDM was the largest and most popular game mode in CS:GO.\
          \ Today, it is still one of the most played modes, but it has been overtaken\
          \ by Valorant\u2019s \u201CSolo queue\u201D mode.\n\nThe reason for this\
          \ change is that a lot of gamers don\u2019t like playing with strangers.\
          \ They prefer to play with friends or known teammates. This makes TDM less\
          \ attractive for many players.\n\nHowever, there are still plenty of reasons\
          \ to play TDM in CS:GO. Here are some of them:\n\n1. Teamwork: TDM is all\
          \ about teamwork. You need to work together with your teammates to eliminate\
          \ the enemy team. This makes it a great mode for practicing teamwork skills.\n\
          2. Fun: TDM is a lot of fun. It\u2019s fast-paced and action-packed. You\
          \ never know what\u2019s going to happen next.\n3. Competition: TDM is a\
          \ competitive mode. You can earn points for each kill, and the team with\
          \ the most points wins. This makes it a great mode for practicing your competitive\
          \ skills.\n4. Variety: TDM offers a lot of variety. There are different\
          \ maps, weapons,\n\n -- Response generated in 5.91 seconds, 256 tokens,\
          \ 43.33 tokens/second (includes prompt eval.)\n\n2 rtx 3090 nvlink"
        updatedAt: '2023-12-20T23:26:01.511Z'
      numEdits: 0
      reactions: []
    id: 658378098d32c17600374a76
    type: comment
  author: Jostack
  content: "Running inference on model: /home/neuron/exllamav2/models/Mixtral-8x7B-instruct-exl2_2.4bpw\n\
    \ -- Model: /home/neuron/exllamav2/models/Mixtral-8x7B-instruct-exl2_2.4bpw\n\
    \ -- Options: ['gpu_split: 23,23', 'rope_scale: 1.0', 'rope_alpha: 1.0']\n --\
    \ Loading model...\n -- Loading tokenizer...\n -- Warmup...\n -- Generating...\n\
    \nOnce upon a time,TDM was the largest and most popular game mode in CS:GO. Today,\
    \ it is still one of the most played modes, but it has been overtaken by Valorant\u2019\
    s \u201CSolo queue\u201D mode.\n\nThe reason for this change is that a lot of\
    \ gamers don\u2019t like playing with strangers. They prefer to play with friends\
    \ or known teammates. This makes TDM less attractive for many players.\n\nHowever,\
    \ there are still plenty of reasons to play TDM in CS:GO. Here are some of them:\n\
    \n1. Teamwork: TDM is all about teamwork. You need to work together with your\
    \ teammates to eliminate the enemy team. This makes it a great mode for practicing\
    \ teamwork skills.\n2. Fun: TDM is a lot of fun. It\u2019s fast-paced and action-packed.\
    \ You never know what\u2019s going to happen next.\n3. Competition: TDM is a competitive\
    \ mode. You can earn points for each kill, and the team with the most points wins.\
    \ This makes it a great mode for practicing your competitive skills.\n4. Variety:\
    \ TDM offers a lot of variety. There are different maps, weapons,\n\n -- Response\
    \ generated in 5.91 seconds, 256 tokens, 43.33 tokens/second (includes prompt\
    \ eval.)\n\n2 rtx 3090 nvlink"
  created_at: 2023-12-20 23:26:01+00:00
  edited: false
  hidden: false
  id: 658378098d32c17600374a76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad9c50879fe45f61272f57c5a0c55800.svg
      fullname: Jason Baller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: goldrushgames
      type: user
    createdAt: '2023-12-21T09:46:07.000Z'
    data:
      edited: false
      editors:
      - goldrushgames
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9531837701797485
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad9c50879fe45f61272f57c5a0c55800.svg
          fullname: Jason Baller
          isHf: false
          isPro: false
          name: goldrushgames
          type: user
        html: '<p>With 2x3090, you can go a lot more than 2.4 bpw. I''m able to load
          3.5bpw on a single 3090</p>

          '
        raw: With 2x3090, you can go a lot more than 2.4 bpw. I'm able to load 3.5bpw
          on a single 3090
        updatedAt: '2023-12-21T09:46:07.986Z'
      numEdits: 0
      reactions: []
    id: 6584095f4b2f0e9f3409eafd
    type: comment
  author: goldrushgames
  content: With 2x3090, you can go a lot more than 2.4 bpw. I'm able to load 3.5bpw
    on a single 3090
  created_at: 2023-12-21 09:46:07+00:00
  edited: false
  hidden: false
  id: 6584095f4b2f0e9f3409eafd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/48f5d78bb7aedba2b9739b13454be4dd.svg
      fullname: 'Navarro '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jostack
      type: user
    createdAt: '2023-12-21T14:03:33.000Z'
    data:
      edited: false
      editors:
      - Jostack
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7785620093345642
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/48f5d78bb7aedba2b9739b13454be4dd.svg
          fullname: 'Navarro '
          isHf: false
          isPro: false
          name: Jostack
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;goldrushgames&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/goldrushgames\"\
          >@<span class=\"underline\">goldrushgames</span></a></span>\n\n\t</span></span>\
          \  </p>\n<p>Yes, I am trying different combinations and the 8.0bpw model\
          \ is the one that I cannot start.</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6412495a227028b63e3128fd/6X-I9C1mA2qkfKRgMlj7c.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6412495a227028b63e3128fd/6X-I9C1mA2qkfKRgMlj7c.png\"\
          ></a></p>\n"
        raw: "@goldrushgames  \n\nYes, I am trying different combinations and the\
          \ 8.0bpw model is the one that I cannot start.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6412495a227028b63e3128fd/6X-I9C1mA2qkfKRgMlj7c.png)\n"
        updatedAt: '2023-12-21T14:03:33.645Z'
      numEdits: 0
      reactions: []
    id: 658445b5039e9e2f0ef63eb6
    type: comment
  author: Jostack
  content: "@goldrushgames  \n\nYes, I am trying different combinations and the 8.0bpw\
    \ model is the one that I cannot start.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6412495a227028b63e3128fd/6X-I9C1mA2qkfKRgMlj7c.png)\n"
  created_at: 2023-12-21 14:03:33+00:00
  edited: false
  hidden: false
  id: 658445b5039e9e2f0ef63eb6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca667a4022296eedbc8836fbb6e3b936.svg
      fullname: Prasanthi Nayunipati
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Prasanthin
      type: user
    createdAt: '2024-01-08T10:31:56.000Z'
    data:
      edited: true
      editors:
      - Prasanthin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9113929867744446
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca667a4022296eedbc8836fbb6e3b936.svg
          fullname: Prasanthi Nayunipati
          isHf: false
          isPro: false
          name: Prasanthin
          type: user
        html: '<p>Hi,<br>I want to download 3.0 bpw. Do I need to do manually? I am
          using git clone but no getting the config files and all and trying to use
          AutoModelForCausalLM but failed. Could you please guide me how to download
          and deploy using exllamav2</p>

          '
        raw: 'Hi,

          I want to download 3.0 bpw. Do I need to do manually? I am using git clone
          but no getting the config files and all and trying to use AutoModelForCausalLM
          but failed. Could you please guide me how to download and deploy using exllamav2

          '
        updatedAt: '2024-01-08T10:32:53.165Z'
      numEdits: 1
      reactions: []
    id: 659bcf1cc1144540fd21b6ed
    type: comment
  author: Prasanthin
  content: 'Hi,

    I want to download 3.0 bpw. Do I need to do manually? I am using git clone but
    no getting the config files and all and trying to use AutoModelForCausalLM but
    failed. Could you please guide me how to download and deploy using exllamav2

    '
  created_at: 2024-01-08 10:31:56+00:00
  edited: true
  hidden: false
  id: 659bcf1cc1144540fd21b6ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
      fullname: Ahmed Morsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramax
      type: user
    createdAt: '2024-01-08T13:57:43.000Z'
    data:
      edited: false
      editors:
      - eramax
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7886683344841003
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64fdfaeb01aedd0e86014de9/UliF1du7InfuCs7RHLiA5.png?w=200&h=200&f=face
          fullname: Ahmed Morsi
          isHf: false
          isPro: false
          name: eramax
          type: user
        html: '<blockquote>

          <p>Hi,<br>I want to download 3.0 bpw. Do I need to do manually? I am using
          git clone but no getting the config files and all and trying to use AutoModelForCausalLM
          but failed. Could you please guide me how to download and deploy using exllamav2</p>

          </blockquote>

          <p>use huggingface-cl,install it in python env and activate the env, git
          clone has many issues</p>

          <pre><code>huggingface-cli download "$model_path" --revision $branch --local-dir
          /home/user/dev/models/"$model_name" --local-dir-use-symlinks False

          </code></pre>

          '
        raw: '> Hi,

          > I want to download 3.0 bpw. Do I need to do manually? I am using git clone
          but no getting the config files and all and trying to use AutoModelForCausalLM
          but failed. Could you please guide me how to download and deploy using exllamav2


          use huggingface-cl,install it in python env and activate the env, git clone
          has many issues

          ```

          huggingface-cli download "$model_path" --revision $branch --local-dir /home/user/dev/models/"$model_name"
          --local-dir-use-symlinks False

          ```

          '
        updatedAt: '2024-01-08T13:57:43.382Z'
      numEdits: 0
      reactions: []
    id: 659bff5708a5095b07fb2bc3
    type: comment
  author: eramax
  content: '> Hi,

    > I want to download 3.0 bpw. Do I need to do manually? I am using git clone but
    no getting the config files and all and trying to use AutoModelForCausalLM but
    failed. Could you please guide me how to download and deploy using exllamav2


    use huggingface-cl,install it in python env and activate the env, git clone has
    many issues

    ```

    huggingface-cli download "$model_path" --revision $branch --local-dir /home/user/dev/models/"$model_name"
    --local-dir-use-symlinks False

    ```

    '
  created_at: 2024-01-08 13:57:43+00:00
  edited: false
  hidden: false
  id: 659bff5708a5095b07fb2bc3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca667a4022296eedbc8836fbb6e3b936.svg
      fullname: Prasanthi Nayunipati
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Prasanthin
      type: user
    createdAt: '2024-01-09T05:56:31.000Z'
    data:
      edited: true
      editors:
      - Prasanthin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.992087185382843
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca667a4022296eedbc8836fbb6e3b936.svg
          fullname: Prasanthi Nayunipati
          isHf: false
          isPro: false
          name: Prasanthin
          type: user
        html: "<p>It worked <span data-props=\"{&quot;user&quot;:&quot;Ahmed&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Ahmed\"\
          >@<span class=\"underline\">Ahmed</span></a></span>\n\n\t</span></span>\
          \ Morsi...thank you so much</p>\n"
        raw: It worked @Ahmed Morsi...thank you so much
        updatedAt: '2024-01-09T05:57:05.134Z'
      numEdits: 1
      reactions: []
    id: 659ce00ff9782802e9a4440f
    type: comment
  author: Prasanthin
  content: It worked @Ahmed Morsi...thank you so much
  created_at: 2024-01-09 05:56:31+00:00
  edited: true
  hidden: false
  id: 659ce00ff9782802e9a4440f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: turboderp/Mixtral-8x7B-instruct-exl2
repo_type: model
status: open
target_branch: null
title: bpw and corresponding vram usage
