!!python/object:huggingface_hub.community.DiscussionWithDetails
author: leoperelli
conflicting_files: null
created_at: 2022-09-14 16:30:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657707445281-noauth.jpeg?w=200&h=200&f=face
      fullname: Leonardo Perelli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leoperelli
      type: user
    createdAt: '2022-09-14T17:30:12.000Z'
    data:
      edited: true
      editors:
      - leoperelli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657707445281-noauth.jpeg?w=200&h=200&f=face
          fullname: Leonardo Perelli
          isHf: false
          isPro: false
          name: leoperelli
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;nielsr&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nielsr\">@<span class=\"\
          underline\">nielsr</span></a></span>\n\n\t</span></span> , thanks for another\
          \ great model. I was wondering on what type of data the model was trained,\
          \ the model seems to have a different dictionary from the Naver Clova one\
          \ (which has lots of Chinese characters).</p>\n<p>I was also wondering what\
          \ you believe should be changed to include a Token Classification head as\
          \ for example the LayoutLM family.<br>Thanks!</p>\n"
        raw: 'Hi @nielsr , thanks for another great model. I was wondering on what
          type of data the model was trained, the model seems to have a different
          dictionary from the Naver Clova one (which has lots of Chinese characters).


          I was also wondering what you believe should be changed to include a Token
          Classification head as for example the LayoutLM family.

          Thanks!'
        updatedAt: '2022-09-15T12:48:15.273Z'
      numEdits: 1
      reactions: []
    id: 63220fa4909ac44b572827a6
    type: comment
  author: leoperelli
  content: 'Hi @nielsr , thanks for another great model. I was wondering on what type
    of data the model was trained, the model seems to have a different dictionary
    from the Naver Clova one (which has lots of Chinese characters).


    I was also wondering what you believe should be changed to include a Token Classification
    head as for example the LayoutLM family.

    Thanks!'
  created_at: 2022-09-14 16:30:12+00:00
  edited: true
  hidden: false
  id: 63220fa4909ac44b572827a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nielsr
      type: user
    createdAt: '2022-09-15T17:33:30.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Hi,</p>

          <p>This model is exactly the same as <a href="https://huggingface.co/naver-clova-ix/donut-base">https://huggingface.co/naver-clova-ix/donut-base</a>.
          I just ported this one over to the Naver Clova organization.</p>

          <blockquote>

          <p>I was also wondering what you believe should be changed to include a
          Token Classification head as for example the LayoutLM family.</p>

          </blockquote>

          <p>Donut treats all tasks as a language modeling problem, hence it uses
          the same head (a language modeling) head for all tasks. No need to change
          the head. The model is just <code>VisionEncoderDecoderModel</code>, which
          includes a language modeling head on top of the decoder.</p>

          '
        raw: 'Hi,


          This model is exactly the same as https://huggingface.co/naver-clova-ix/donut-base.
          I just ported this one over to the Naver Clova organization.


          > I was also wondering what you believe should be changed to include a Token
          Classification head as for example the LayoutLM family.


          Donut treats all tasks as a language modeling problem, hence it uses the
          same head (a language modeling) head for all tasks. No need to change the
          head. The model is just `VisionEncoderDecoderModel`, which includes a language
          modeling head on top of the decoder.'
        updatedAt: '2022-09-15T17:33:30.325Z'
      numEdits: 0
      reactions: []
    id: 632361eac2d8cfcd80938839
    type: comment
  author: nielsr
  content: 'Hi,


    This model is exactly the same as https://huggingface.co/naver-clova-ix/donut-base.
    I just ported this one over to the Naver Clova organization.


    > I was also wondering what you believe should be changed to include a Token Classification
    head as for example the LayoutLM family.


    Donut treats all tasks as a language modeling problem, hence it uses the same
    head (a language modeling) head for all tasks. No need to change the head. The
    model is just `VisionEncoderDecoderModel`, which includes a language modeling
    head on top of the decoder.'
  created_at: 2022-09-15 16:33:30+00:00
  edited: false
  hidden: false
  id: 632361eac2d8cfcd80938839
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657707445281-noauth.jpeg?w=200&h=200&f=face
      fullname: Leonardo Perelli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leoperelli
      type: user
    createdAt: '2022-09-15T17:58:34.000Z'
    data:
      edited: false
      editors:
      - leoperelli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657707445281-noauth.jpeg?w=200&h=200&f=face
          fullname: Leonardo Perelli
          isHf: false
          isPro: false
          name: leoperelli
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;nielsr&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nielsr\"\
          >@<span class=\"underline\">nielsr</span></a></span>\n\n\t</span></span>\
          \ !</p>\n<p>Just a clarification on the head;<br>The current output has\
          \ the dimension of the dictionary, since it predicts a sequence as output.\
          \ To obtain a classification at token level should I change the output dimension\
          \ to the number of classes? Or you mean to manage this with a different\
          \ prompt?</p>\n"
        raw: 'Thanks @nielsr !


          Just a clarification on the head;

          The current output has the dimension of the dictionary, since it predicts
          a sequence as output. To obtain a classification at token level should I
          change the output dimension to the number of classes? Or you mean to manage
          this with a different prompt?'
        updatedAt: '2022-09-15T17:58:34.992Z'
      numEdits: 0
      reactions: []
    id: 632367ca75bf010a73db61f6
    type: comment
  author: leoperelli
  content: 'Thanks @nielsr !


    Just a clarification on the head;

    The current output has the dimension of the dictionary, since it predicts a sequence
    as output. To obtain a classification at token level should I change the output
    dimension to the number of classes? Or you mean to manage this with a different
    prompt?'
  created_at: 2022-09-15 16:58:34+00:00
  edited: false
  hidden: false
  id: 632367ca75bf010a73db61f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nielsr
      type: user
    createdAt: '2022-09-16T07:25:16.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>The model predicts a sequence, which you can turn back into JSON
          using the <code>token2json</code> method of <code>DonutProcessor</code>.
          </p>

          <p>Note that this model is entirely different compared to LayoutLM(v1/v2/v3).
          It doesn''t output a classification at the token level, it just outputs
          a sequence which can be turned into JSON.</p>

          '
        raw: "The model predicts a sequence, which you can turn back into JSON using\
          \ the `token2json` method of `DonutProcessor`. \n\nNote that this model\
          \ is entirely different compared to LayoutLM(v1/v2/v3). It doesn't output\
          \ a classification at the token level, it just outputs a sequence which\
          \ can be turned into JSON."
        updatedAt: '2022-09-16T07:25:16.336Z'
      numEdits: 0
      reactions: []
    id: 632424dce2623eb98d2509a0
    type: comment
  author: nielsr
  content: "The model predicts a sequence, which you can turn back into JSON using\
    \ the `token2json` method of `DonutProcessor`. \n\nNote that this model is entirely\
    \ different compared to LayoutLM(v1/v2/v3). It doesn't output a classification\
    \ at the token level, it just outputs a sequence which can be turned into JSON."
  created_at: 2022-09-16 06:25:16+00:00
  edited: false
  hidden: false
  id: 632424dce2623eb98d2509a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657707445281-noauth.jpeg?w=200&h=200&f=face
      fullname: Leonardo Perelli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leoperelli
      type: user
    createdAt: '2022-09-16T10:07:46.000Z'
    data:
      edited: true
      editors:
      - leoperelli
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657707445281-noauth.jpeg?w=200&h=200&f=face
          fullname: Leonardo Perelli
          isHf: false
          isPro: false
          name: leoperelli
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;nielsr&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nielsr\">@<span class=\"\
          underline\">nielsr</span></a></span>\n\n\t</span></span> , I reviewed the\
          \ architecture the Donut model and the Layout family.<br>I had the following\
          \ questions:</p>\n<ol>\n<li>Donut last hidden states of the encoder have\
          \ shape (batch_size, 1200, 768). What is the axis with 1200 dimension? How\
          \ can I know which tokens they are? Do you believe it makes sense/ it is\
          \ possible to use these encoder output to perform a token classification?\
          \ This would help me to better understand performance of the model, which\
          \ is lower than expected. I wanted to understand if something was wrong\
          \ with the decoder or its something else.</li>\n<li>For the task token,\
          \ I see some people fine tuning keeping the s token for their task. From\
          \ my understanding, the sequence should be: \"s\"\"s_start_token\" ... \"\
          /s_task_token\"\"/s\". Is this correct?</li>\n<li>The Donut dictionary seems\
          \ to be quite limited, a lot of common words are split in several subtokens.\
          \ Do you believe this is a big deal in the fine tuning performance? My model\
          \ seems to overfit very quickly and not learn much about the OOV tokens.</li>\n\
          </ol>\n<p>Thanks a lot!</p>\n"
        raw: "Hi @nielsr , I reviewed the architecture the Donut model and the Layout\
          \ family. \nI had the following questions:\n\n1) Donut last hidden states\
          \ of the encoder have shape (batch_size, 1200, 768). What is the axis with\
          \ 1200 dimension? How can I know which tokens they are? Do you believe it\
          \ makes sense/ it is possible to use these encoder output to perform a token\
          \ classification? This would help me to better understand performance of\
          \ the model, which is lower than expected. I wanted to understand if something\
          \ was wrong with the decoder or its something else.\n2) For the task token,\
          \ I see some people fine tuning keeping the s token for their task. From\
          \ my understanding, the sequence should be: \"s\"\"s_start_token\" ... \"\
          /s_task_token\"\"/s\". Is this correct?\n3) The Donut dictionary seems to\
          \ be quite limited, a lot of common words are split in several subtokens.\
          \ Do you believe this is a big deal in the fine tuning performance? My model\
          \ seems to overfit very quickly and not learn much about the OOV tokens.\
          \ \n\nThanks a lot!"
        updatedAt: '2022-09-22T19:37:59.511Z'
      numEdits: 5
      reactions: []
    id: 63244af2371b3b625ed6d8c9
    type: comment
  author: leoperelli
  content: "Hi @nielsr , I reviewed the architecture the Donut model and the Layout\
    \ family. \nI had the following questions:\n\n1) Donut last hidden states of the\
    \ encoder have shape (batch_size, 1200, 768). What is the axis with 1200 dimension?\
    \ How can I know which tokens they are? Do you believe it makes sense/ it is possible\
    \ to use these encoder output to perform a token classification? This would help\
    \ me to better understand performance of the model, which is lower than expected.\
    \ I wanted to understand if something was wrong with the decoder or its something\
    \ else.\n2) For the task token, I see some people fine tuning keeping the s token\
    \ for their task. From my understanding, the sequence should be: \"s\"\"s_start_token\"\
    \ ... \"/s_task_token\"\"/s\". Is this correct?\n3) The Donut dictionary seems\
    \ to be quite limited, a lot of common words are split in several subtokens. Do\
    \ you believe this is a big deal in the fine tuning performance? My model seems\
    \ to overfit very quickly and not learn much about the OOV tokens. \n\nThanks\
    \ a lot!"
  created_at: 2022-09-16 09:07:46+00:00
  edited: true
  hidden: false
  id: 63244af2371b3b625ed6d8c9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: nielsr/donut-base
repo_type: model
status: open
target_branch: null
title: Train dataset and different head
