!!python/object:huggingface_hub.community.DiscussionWithDetails
author: giuliogalvan
conflicting_files: null
created_at: 2023-07-07 10:02:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/614352744c551f7a6d7d5fccab3771c9.svg
      fullname: GIulioGalvan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: giuliogalvan
      type: user
    createdAt: '2023-07-07T11:02:34.000Z'
    data:
      edited: false
      editors:
      - giuliogalvan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6221255660057068
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/614352744c551f7a6d7d5fccab3771c9.svg
          fullname: GIulioGalvan
          isHf: false
          isPro: false
          name: giuliogalvan
          type: user
        html: "<p>I am trying to deploy (either through sagemaker or managed endpoints)\
          \ this model to make extensive tests but I ran in the following problem.</p>\n\
          <p>This is a log extract from AWS sagemaker after the invocation of huggingface_model.deploy()</p>\n\
          <pre><code>The tokenizer class you load from this checkpoint is not the\
          \ same type as the class this function is called from. It may result in\
          \ unexpected tokenization.\n\nThe tokenizer class you load from this checkpoint\
          \ is 'XgenTokenizer'. \n\nThe class this function is called from is 'LlamaTokenizer'.\n\
          \nTraceback (most recent call last):\nFile \"/opt/conda/bin/text-generation-server\"\
          , line 8, in &lt;module&gt;\n  sys.exit(app())\nFile \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 67, in serve\n  server.serve(model_id, revision, sharded, quantize,\
          \ trust_remote_code, uds_path)\nFile \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 155, in serve\n  asyncio.run(serve_inner(model_id, revision, sharded,\
          \ quantize, trust_remote_code))\nFile \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\n  return loop.run_until_complete(main)\nFile \"/opt/conda/lib/python3.9/asyncio/base_events.py\"\
          , line 647, in run_until_complete\n  return future.result()\n\n\nError:\
          \ ShardCannotStart\nFile \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\n  model = get_model(model_id, revision, sharded,\
          \ quantize, trust_remote_code)\nFile \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 246, in get_model\n  return llama_cls(\nFile \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 44, in __init__\n  tokenizer = LlamaTokenizer.from_pretrained(\n\
          File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1812, in from_pretrained\n  return cls._from_pretrained(\nFile \"\
          /usr/src/transformers/src/transformers/tokenization_utils_base.py\", line\
          \ 1975, in _from_pretrained\n  tokenizer = cls(*init_inputs, **init_kwargs)\n\
          File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\n  self.sp_model.Load(vocab_file)\nFile \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\"\
          , line 905, in Load\n  return self.LoadFromFile(model_file)\nFile \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\"\
          , line 310, in LoadFromFile\n  return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\n</code></pre>\n<p>Any help will be very appreciated :)</p>\n"
        raw: "I am trying to deploy (either through sagemaker or managed endpoints)\
          \ this model to make extensive tests but I ran in the following problem.\r\
          \n\r\nThis is a log extract from AWS sagemaker after the invocation of huggingface_model.deploy()\r\
          \n\r\n    The tokenizer class you load from this checkpoint is not the same\
          \ type as the class this function is called from. It may result in unexpected\
          \ tokenization.\r\n    \r\n    The tokenizer class you load from this checkpoint\
          \ is 'XgenTokenizer'. \r\n\r\n    The class this function is called from\
          \ is 'LlamaTokenizer'.\r\n\r\n    Traceback (most recent call last):\r\n\
          \    File \"/opt/conda/bin/text-generation-server\", line 8, in <module>\r\
          \n      sys.exit(app())\r\n    File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
          , line 67, in serve\r\n      server.serve(model_id, revision, sharded, quantize,\
          \ trust_remote_code, uds_path)\r\n    File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 155, in serve\r\n      asyncio.run(serve_inner(model_id, revision,\
          \ sharded, quantize, trust_remote_code))\r\n    File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
          , line 44, in run\r\n      return loop.run_until_complete(main)\r\n    File\
          \ \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
          \n      return future.result()\r\n\r\n\r\n    Error: ShardCannotStart\r\n\
          \    File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\r\n      model = get_model(model_id, revision,\
          \ sharded, quantize, trust_remote_code)\r\n    File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 246, in get_model\r\n      return llama_cls(\r\n    File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 44, in __init__\r\n      tokenizer = LlamaTokenizer.from_pretrained(\r\
          \n    File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1812, in from_pretrained\r\n      return cls._from_pretrained(\r\n\
          \    File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1975, in _from_pretrained\r\n      tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\r\n    File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\r\n      self.sp_model.Load(vocab_file)\r\n    File\
          \ \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\",\
          \ line 905, in Load\r\n      return self.LoadFromFile(model_file)\r\n  \
          \  File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\"\
          , line 310, in LoadFromFile\r\n      return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\n  \r\nAny help will be very appreciated :)\r\n\r\n\r\n"
        updatedAt: '2023-07-07T11:02:34.691Z'
      numEdits: 0
      reactions: []
    id: 64a7f0ca48c7c8ab9c6de69e
    type: comment
  author: giuliogalvan
  content: "I am trying to deploy (either through sagemaker or managed endpoints)\
    \ this model to make extensive tests but I ran in the following problem.\r\n\r\
    \nThis is a log extract from AWS sagemaker after the invocation of huggingface_model.deploy()\r\
    \n\r\n    The tokenizer class you load from this checkpoint is not the same type\
    \ as the class this function is called from. It may result in unexpected tokenization.\r\
    \n    \r\n    The tokenizer class you load from this checkpoint is 'XgenTokenizer'.\
    \ \r\n\r\n    The class this function is called from is 'LlamaTokenizer'.\r\n\r\
    \n    Traceback (most recent call last):\r\n    File \"/opt/conda/bin/text-generation-server\"\
    , line 8, in <module>\r\n      sys.exit(app())\r\n    File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py\"\
    , line 67, in serve\r\n      server.serve(model_id, revision, sharded, quantize,\
    \ trust_remote_code, uds_path)\r\n    File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 155, in serve\r\n      asyncio.run(serve_inner(model_id, revision, sharded,\
    \ quantize, trust_remote_code))\r\n    File \"/opt/conda/lib/python3.9/asyncio/runners.py\"\
    , line 44, in run\r\n      return loop.run_until_complete(main)\r\n    File \"\
    /opt/conda/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\r\
    \n      return future.result()\r\n\r\n\r\n    Error: ShardCannotStart\r\n    File\
    \ \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 124, in serve_inner\r\n      model = get_model(model_id, revision, sharded,\
    \ quantize, trust_remote_code)\r\n    File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 246, in get_model\r\n      return llama_cls(\r\n    File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
    , line 44, in __init__\r\n      tokenizer = LlamaTokenizer.from_pretrained(\r\n\
    \    File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
    , line 1812, in from_pretrained\r\n      return cls._from_pretrained(\r\n    File\
    \ \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\", line\
    \ 1975, in _from_pretrained\r\n      tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n    File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\"\
    , line 96, in __init__\r\n      self.sp_model.Load(vocab_file)\r\n    File \"\
    /opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\", line 905,\
    \ in Load\r\n      return self.LoadFromFile(model_file)\r\n    File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\"\
    , line 310, in LoadFromFile\r\n      return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\r\n  \r\nAny help will be very appreciated :)\r\n\r\n\r\n"
  created_at: 2023-07-07 10:02:34+00:00
  edited: false
  hidden: false
  id: 64a7f0ca48c7c8ab9c6de69e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: VMware/xgen-7b-8k-open-instruct
repo_type: model
status: open
target_branch: null
title: Error in deploy
