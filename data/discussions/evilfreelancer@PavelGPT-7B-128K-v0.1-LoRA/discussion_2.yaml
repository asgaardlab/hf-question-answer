!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Epimachok
conflicting_files: null
created_at: 2024-01-16 17:31:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b156ac3b7b59d2d30660875964b14a44.svg
      fullname: Dmitry Stukun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Epimachok
      type: user
    createdAt: '2024-01-16T17:31:18.000Z'
    data:
      edited: false
      editors:
      - Epimachok
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.18515591323375702
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b156ac3b7b59d2d30660875964b14a44.svg
          fullname: Dmitry Stukun
          isHf: false
          isPro: false
          name: Epimachok
          type: user
        html: "<p>\u041F\u043E\u043F\u044B\u0442\u0430\u043B\u0441\u044F \u043F\u0435\
          \u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u043C\u043E\u0434\u0435\u043B\
          \u044C \u0432 \u0444\u043E\u0440\u043C\u0430\u0442 awq, \u043D\u043E \u0432\
          \u0441\u0442\u0440\u0435\u0442\u0438\u043B \u043E\u0448\u0438\u0431\u043A\
          \u0443 KeyError: 'self_attn.q_proj' \u0432 \u043F\u0440\u043E\u0446\u0435\
          \u0441\u0441\u0435 \u0440\u0430\u0431\u043E\u0442\u044B.<br>\u041E\u0434\
          \u043D\u0430\u043A\u043E \u0432\u0438\u0436\u0443, \u0447\u0442\u043E \u0432\
          \ \u043A\u043E\u043D\u0444\u0438\u0433\u0435 \u0434\u043B\u044F peft \u0434\
          \u0430\u043D\u043D\u044B\u0439 \u0441\u043B\u043E\u0439 \u043F\u0440\u0438\
          \u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442. \u0431\u0430\u0437\
          \u043E\u0432\u044B\u0439 yarn \u043F\u0435\u0440\u0435\u0432\u043E\u0434\
          \u0438\u0442\u0441\u044F \u0432 \u044D\u0442\u043E\u0442 \u0444\u043E\u0440\
          \u043C\u0430\u0442 \u0431\u0435\u0437 \u043F\u0440\u043E\u0431\u043B\u0435\
          \u043C. \u041F\u043E\u0434\u0441\u043A\u0430\u0436\u0438\u0442\u0435, \u0432\
          \ \u0447\u0435\u043C \u043C\u043E\u0436\u0435\u0442 \u0431\u044B\u0442\u044C\
          \ \u043F\u0440\u043E\u0431\u043B\u0435\u043C\u0430?<br>\u0414\u0430\u043B\
          \u0435\u0435 \u043F\u0440\u0438\u043B\u0430\u0433\u0430\u044E \u043A\u043E\
          \u0434 \u043F\u0435\u0440\u0435\u0432\u043E\u0434\u0430 \u0438 \u043F\u043E\
          \u043B\u043D\u044B\u0439 \u0442\u0435\u043A\u0441\u0442 \u043E\u0448\u0438\
          \u0431\u043A\u0438</p>\n<p>from awq import AutoAWQForCausalLM<br>from transformers\
          \ import AutoTokenizer</p>\n<p>model_path = './PavelGPT-7B-128K-v0.1-LoRA/'<br>quant_path\
          \ = 'PavelGPT-7B-128K-v0.1-awq'<br>quant_config = { \"zero_point\": True,\
          \ \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }</p>\n<h1\
          \ id=\"load-model\">Load model</h1>\n<p>model = AutoAWQForCausalLM.from_pretrained(model_path)<br>tokenizer\
          \ = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)</p>\n\
          <h1 id=\"quantize\">Quantize</h1>\n<p>model.quantize(tokenizer, quant_config=quant_config)</p>\n\
          <h1 id=\"save-quantized-model\">Save quantized model</h1>\n<p>model.save_quantized(quant_path)<br>tokenizer.save_pretrained(quant_path)</p>\n\
          <p>Token indices sequence length is longer than the specified maximum sequence\
          \ length for this model (77696 &gt; 32768). Running this sequence through\
          \ the model will result in indexing errors<br>AWQ:   0%|               \
          \                                                                      \
          \                                                                      \
          \                                                                  | 0/32\
          \ [00:02&lt;?, ?it/s]<br>Traceback (most recent call last):<br>  File \"\
          /home/epimachok/awq/qawq.py\", line 13, in <br>    model.quantize(tokenizer,\
          \ quant_config=quant_config)<br>  File \"/home/epimachok/anaconda3/envs/autoawq/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context<br>    return func(*args, **kwargs)<br>\
          \  File \"/home/epimachok/anaconda3/envs/autoawq/lib/python3.10/site-packages/awq/models/base.py\"\
          , line 93, in quantize<br>    quantizer.quantize()<br>  File \"/home/epimachok/anaconda3/envs/autoawq/lib/python3.10/site-packages/awq/quantize/quantizer.py\"\
          , line 99, in quantize<br>    module_config: List[Dict] = self.awq_model.get_layers_for_scaling(<br>\
          \  File \"/home/epimachok/anaconda3/envs/autoawq/lib/python3.10/site-packages/awq/models/mistral.py\"\
          , line 46, in get_layers_for_scaling<br>    inp=input_feat['self_attn.q_proj'],<br>KeyError:\
          \ 'self_attn.q_proj'</p>\n"
        raw: "\u041F\u043E\u043F\u044B\u0442\u0430\u043B\u0441\u044F \u043F\u0435\u0440\
          \u0435\u0432\u0435\u0441\u0442\u0438 \u043C\u043E\u0434\u0435\u043B\u044C\
          \ \u0432 \u0444\u043E\u0440\u043C\u0430\u0442 awq, \u043D\u043E \u0432\u0441\
          \u0442\u0440\u0435\u0442\u0438\u043B \u043E\u0448\u0438\u0431\u043A\u0443\
          \ KeyError: 'self_attn.q_proj' \u0432 \u043F\u0440\u043E\u0446\u0435\u0441\
          \u0441\u0435 \u0440\u0430\u0431\u043E\u0442\u044B. \r\n\u041E\u0434\u043D\
          \u0430\u043A\u043E \u0432\u0438\u0436\u0443, \u0447\u0442\u043E \u0432 \u043A\
          \u043E\u043D\u0444\u0438\u0433\u0435 \u0434\u043B\u044F peft \u0434\u0430\
          \u043D\u043D\u044B\u0439 \u0441\u043B\u043E\u0439 \u043F\u0440\u0438\u0441\
          \u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442. \u0431\u0430\u0437\u043E\
          \u0432\u044B\u0439 yarn \u043F\u0435\u0440\u0435\u0432\u043E\u0434\u0438\
          \u0442\u0441\u044F \u0432 \u044D\u0442\u043E\u0442 \u0444\u043E\u0440\u043C\
          \u0430\u0442 \u0431\u0435\u0437 \u043F\u0440\u043E\u0431\u043B\u0435\u043C\
          . \u041F\u043E\u0434\u0441\u043A\u0430\u0436\u0438\u0442\u0435, \u0432 \u0447\
          \u0435\u043C \u043C\u043E\u0436\u0435\u0442 \u0431\u044B\u0442\u044C \u043F\
          \u0440\u043E\u0431\u043B\u0435\u043C\u0430?\r\n\u0414\u0430\u043B\u0435\u0435\
          \ \u043F\u0440\u0438\u043B\u0430\u0433\u0430\u044E \u043A\u043E\u0434 \u043F\
          \u0435\u0440\u0435\u0432\u043E\u0434\u0430 \u0438 \u043F\u043E\u043B\u043D\
          \u044B\u0439 \u0442\u0435\u043A\u0441\u0442 \u043E\u0448\u0438\u0431\u043A\
          \u0438\r\n\r\nfrom awq import AutoAWQForCausalLM\r\nfrom transformers import\
          \ AutoTokenizer\r\n\r\nmodel_path = './PavelGPT-7B-128K-v0.1-LoRA/'\r\n\
          quant_path = 'PavelGPT-7B-128K-v0.1-awq'\r\nquant_config = { \"zero_point\"\
          : True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\r\n\
          \r\n# Load model\r\nmodel = AutoAWQForCausalLM.from_pretrained(model_path)\r\
          \ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\
          \n\r\n# Quantize\r\nmodel.quantize(tokenizer, quant_config=quant_config)\r\
          \n\r\n# Save quantized model\r\nmodel.save_quantized(quant_path)\r\ntokenizer.save_pretrained(quant_path)\r\
          \n\r\n\r\nToken indices sequence length is longer than the specified maximum\
          \ sequence length for this model (77696 > 32768). Running this sequence\
          \ through the model will result in indexing errors\r\nAWQ:   0%|       \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \    | 0/32 [00:02<?, ?it/s]\r\nTraceback (most recent call last):\r\n \
          \ File \"/home/epimachok/awq/qawq.py\", line 13, in <module>\r\n    model.quantize(tokenizer,\
          \ quant_config=quant_config)\r\n  File \"/home/epimachok/anaconda3/envs/autoawq/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/home/epimachok/anaconda3/envs/autoawq/lib/python3.10/site-packages/awq/models/base.py\"\
          , line 93, in quantize\r\n    quantizer.quantize()\r\n  File \"/home/epimachok/anaconda3/envs/autoawq/lib/python3.10/site-packages/awq/quantize/quantizer.py\"\
          , line 99, in quantize\r\n    module_config: List[Dict] = self.awq_model.get_layers_for_scaling(\r\
          \n  File \"/home/epimachok/anaconda3/envs/autoawq/lib/python3.10/site-packages/awq/models/mistral.py\"\
          , line 46, in get_layers_for_scaling\r\n    inp=input_feat['self_attn.q_proj'],\r\
          \nKeyError: 'self_attn.q_proj'\r\n"
        updatedAt: '2024-01-16T17:31:18.986Z'
      numEdits: 0
      reactions: []
    id: 65a6bd668ae5a595370764a9
    type: comment
  author: Epimachok
  content: "\u041F\u043E\u043F\u044B\u0442\u0430\u043B\u0441\u044F \u043F\u0435\u0440\
    \u0435\u0432\u0435\u0441\u0442\u0438 \u043C\u043E\u0434\u0435\u043B\u044C \u0432\
    \ \u0444\u043E\u0440\u043C\u0430\u0442 awq, \u043D\u043E \u0432\u0441\u0442\u0440\
    \u0435\u0442\u0438\u043B \u043E\u0448\u0438\u0431\u043A\u0443 KeyError: 'self_attn.q_proj'\
    \ \u0432 \u043F\u0440\u043E\u0446\u0435\u0441\u0441\u0435 \u0440\u0430\u0431\u043E\
    \u0442\u044B. \r\n\u041E\u0434\u043D\u0430\u043A\u043E \u0432\u0438\u0436\u0443\
    , \u0447\u0442\u043E \u0432 \u043A\u043E\u043D\u0444\u0438\u0433\u0435 \u0434\u043B\
    \u044F peft \u0434\u0430\u043D\u043D\u044B\u0439 \u0441\u043B\u043E\u0439 \u043F\
    \u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442. \u0431\u0430\
    \u0437\u043E\u0432\u044B\u0439 yarn \u043F\u0435\u0440\u0435\u0432\u043E\u0434\
    \u0438\u0442\u0441\u044F \u0432 \u044D\u0442\u043E\u0442 \u0444\u043E\u0440\u043C\
    \u0430\u0442 \u0431\u0435\u0437 \u043F\u0440\u043E\u0431\u043B\u0435\u043C. \u041F\
    \u043E\u0434\u0441\u043A\u0430\u0436\u0438\u0442\u0435, \u0432 \u0447\u0435\u043C\
    \ \u043C\u043E\u0436\u0435\u0442 \u0431\u044B\u0442\u044C \u043F\u0440\u043E\u0431\
    \u043B\u0435\u043C\u0430?\r\n\u0414\u0430\u043B\u0435\u0435 \u043F\u0440\u0438\
    \u043B\u0430\u0433\u0430\u044E \u043A\u043E\u0434 \u043F\u0435\u0440\u0435\u0432\
    \u043E\u0434\u0430 \u0438 \u043F\u043E\u043B\u043D\u044B\u0439 \u0442\u0435\u043A\
    \u0441\u0442 \u043E\u0448\u0438\u0431\u043A\u0438\r\n\r\nfrom awq import AutoAWQForCausalLM\r\
    \nfrom transformers import AutoTokenizer\r\n\r\nmodel_path = './PavelGPT-7B-128K-v0.1-LoRA/'\r\
    \nquant_path = 'PavelGPT-7B-128K-v0.1-awq'\r\nquant_config = { \"zero_point\"\
    : True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\r\n\r\n#\
    \ Load model\r\nmodel = AutoAWQForCausalLM.from_pretrained(model_path)\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n\r\n\
    # Quantize\r\nmodel.quantize(tokenizer, quant_config=quant_config)\r\n\r\n# Save\
    \ quantized model\r\nmodel.save_quantized(quant_path)\r\ntokenizer.save_pretrained(quant_path)\r\
    \n\r\n\r\nToken indices sequence length is longer than the specified maximum sequence\
    \ length for this model (77696 > 32768). Running this sequence through the model\
    \ will result in indexing errors\r\nAWQ:   0%|                               \
    \                                                                            \
    \                                                                            \
    \                                      | 0/32 [00:02<?, ?it/s]\r\nTraceback (most\
    \ recent call last):\r\n  File \"/home/epimachok/awq/qawq.py\", line 13, in <module>\r\
    \n    model.quantize(tokenizer, quant_config=quant_config)\r\n  File \"/home/epimachok/anaconda3/envs/autoawq/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/home/epimachok/anaconda3/envs/autoawq/lib/python3.10/site-packages/awq/models/base.py\"\
    , line 93, in quantize\r\n    quantizer.quantize()\r\n  File \"/home/epimachok/anaconda3/envs/autoawq/lib/python3.10/site-packages/awq/quantize/quantizer.py\"\
    , line 99, in quantize\r\n    module_config: List[Dict] = self.awq_model.get_layers_for_scaling(\r\
    \n  File \"/home/epimachok/anaconda3/envs/autoawq/lib/python3.10/site-packages/awq/models/mistral.py\"\
    , line 46, in get_layers_for_scaling\r\n    inp=input_feat['self_attn.q_proj'],\r\
    \nKeyError: 'self_attn.q_proj'\r\n"
  created_at: 2024-01-16 17:31:18+00:00
  edited: false
  hidden: false
  id: 65a6bd668ae5a595370764a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676723695399-63ac8a10073766d08506ea74.png?w=200&h=200&f=face
      fullname: Paul Rock
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: evilfreelancer
      type: user
    createdAt: '2024-01-18T07:02:37.000Z'
    data:
      edited: false
      editors:
      - evilfreelancer
      hidden: false
      identifiedLanguage:
        language: ru
        probability: 0.9709116816520691
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676723695399-63ac8a10073766d08506ea74.png?w=200&h=200&f=face
          fullname: Paul Rock
          isHf: false
          isPro: false
          name: evilfreelancer
          type: user
        html: "<p>\u041F\u0440\u043E\u0431\u043B\u0435\u043C\u0430 \u0440\u0435\u0448\
          \u0435\u043D\u0430 \u0432 \u0447\u0430\u0442\u0435 <a rel=\"nofollow\" href=\"\
          https://t.me/evilfreelancer_chat\">https://t.me/evilfreelancer_chat</a></p>\n\
          <p>\u041E\u043A\u0430\u0437\u0430\u043B\u043E\u0441\u044C \u0447\u0442\u043E\
          \ \u0442\u043E\u043F\u0438\u043A-\u0441\u0442\u0430\u0440\u0442\u0435\u0440\
          \ \u0437\u0430\u0431\u044B\u043B \u0443\u0434\u0430\u043B\u0438\u0442\u044C\
          \ \u0438\u0437 \u0434\u0438\u0440\u0435\u043A\u0442\u043E\u0440\u0438 <code>model_path\
          \ = './PavelGPT-7B-128K-v0.1-LoRA/'</code> \u0444\u0430\u0439\u043B\u044B\
          \ <code>adapter_config.json</code> \u0438 <code>adapter_mode.bin</code>,\
          \ \u0438\u0437-\u0437\u0430 \u043D\u0438\u0445 \u043A\u043E\u043D\u0440\u0432\
          \u0435\u0440\u0442\u0435\u043D \u0432 AWQ \u043D\u0435 \u043C\u043E\u0433\
          \ \u043F\u0440\u0430\u0432\u0438\u043B\u044C\u043D\u043E \u043E\u043F\u0440\
          \u0435\u0434\u0435\u043B\u0438\u0442\u044C, \u0447\u0442\u043E \u0437\u0430\
          \ \u043C\u043E\u0434\u0435\u043B\u044C \u043D\u0443\u0436\u043D\u043E \u043A\
          \u043E\u043D\u0432\u0435\u0440\u0442\u0438\u0440\u043E\u0432\u0430\u0442\
          \u044C. \u041F\u043E\u0441\u043B\u0435 \u0443\u0434\u0430\u043B\u0435\u043D\
          \u0438\u044F AWQ \u043D\u0430\u0448\u0451\u043B \u0444\u0430\u0439\u043B\
          \ pytorch_model.bin \u0438 \u043A\u043E\u043D\u0432\u0435\u0440\u0442\u0438\
          \u0440\u043E\u0432\u0430\u043B \u0435\u0451.</p>\n"
        raw: "\u041F\u0440\u043E\u0431\u043B\u0435\u043C\u0430 \u0440\u0435\u0448\u0435\
          \u043D\u0430 \u0432 \u0447\u0430\u0442\u0435 https://t.me/evilfreelancer_chat\n\
          \n\u041E\u043A\u0430\u0437\u0430\u043B\u043E\u0441\u044C \u0447\u0442\u043E\
          \ \u0442\u043E\u043F\u0438\u043A-\u0441\u0442\u0430\u0440\u0442\u0435\u0440\
          \ \u0437\u0430\u0431\u044B\u043B \u0443\u0434\u0430\u043B\u0438\u0442\u044C\
          \ \u0438\u0437 \u0434\u0438\u0440\u0435\u043A\u0442\u043E\u0440\u0438 `model_path\
          \ = './PavelGPT-7B-128K-v0.1-LoRA/'` \u0444\u0430\u0439\u043B\u044B `adapter_config.json`\
          \ \u0438 `adapter_mode.bin`, \u0438\u0437-\u0437\u0430 \u043D\u0438\u0445\
          \ \u043A\u043E\u043D\u0440\u0432\u0435\u0440\u0442\u0435\u043D \u0432 AWQ\
          \ \u043D\u0435 \u043C\u043E\u0433 \u043F\u0440\u0430\u0432\u0438\u043B\u044C\
          \u043D\u043E \u043E\u043F\u0440\u0435\u0434\u0435\u043B\u0438\u0442\u044C\
          , \u0447\u0442\u043E \u0437\u0430 \u043C\u043E\u0434\u0435\u043B\u044C \u043D\
          \u0443\u0436\u043D\u043E \u043A\u043E\u043D\u0432\u0435\u0440\u0442\u0438\
          \u0440\u043E\u0432\u0430\u0442\u044C. \u041F\u043E\u0441\u043B\u0435 \u0443\
          \u0434\u0430\u043B\u0435\u043D\u0438\u044F AWQ \u043D\u0430\u0448\u0451\u043B\
          \ \u0444\u0430\u0439\u043B pytorch_model.bin \u0438 \u043A\u043E\u043D\u0432\
          \u0435\u0440\u0442\u0438\u0440\u043E\u0432\u0430\u043B \u0435\u0451."
        updatedAt: '2024-01-18T07:02:37.315Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65a8cd0dfee7ba6063d7206b
    id: 65a8cd0dfee7ba6063d72065
    type: comment
  author: evilfreelancer
  content: "\u041F\u0440\u043E\u0431\u043B\u0435\u043C\u0430 \u0440\u0435\u0448\u0435\
    \u043D\u0430 \u0432 \u0447\u0430\u0442\u0435 https://t.me/evilfreelancer_chat\n\
    \n\u041E\u043A\u0430\u0437\u0430\u043B\u043E\u0441\u044C \u0447\u0442\u043E \u0442\
    \u043E\u043F\u0438\u043A-\u0441\u0442\u0430\u0440\u0442\u0435\u0440 \u0437\u0430\
    \u0431\u044B\u043B \u0443\u0434\u0430\u043B\u0438\u0442\u044C \u0438\u0437 \u0434\
    \u0438\u0440\u0435\u043A\u0442\u043E\u0440\u0438 `model_path = './PavelGPT-7B-128K-v0.1-LoRA/'`\
    \ \u0444\u0430\u0439\u043B\u044B `adapter_config.json` \u0438 `adapter_mode.bin`,\
    \ \u0438\u0437-\u0437\u0430 \u043D\u0438\u0445 \u043A\u043E\u043D\u0440\u0432\u0435\
    \u0440\u0442\u0435\u043D \u0432 AWQ \u043D\u0435 \u043C\u043E\u0433 \u043F\u0440\
    \u0430\u0432\u0438\u043B\u044C\u043D\u043E \u043E\u043F\u0440\u0435\u0434\u0435\
    \u043B\u0438\u0442\u044C, \u0447\u0442\u043E \u0437\u0430 \u043C\u043E\u0434\u0435\
    \u043B\u044C \u043D\u0443\u0436\u043D\u043E \u043A\u043E\u043D\u0432\u0435\u0440\
    \u0442\u0438\u0440\u043E\u0432\u0430\u0442\u044C. \u041F\u043E\u0441\u043B\u0435\
    \ \u0443\u0434\u0430\u043B\u0435\u043D\u0438\u044F AWQ \u043D\u0430\u0448\u0451\
    \u043B \u0444\u0430\u0439\u043B pytorch_model.bin \u0438 \u043A\u043E\u043D\u0432\
    \u0435\u0440\u0442\u0438\u0440\u043E\u0432\u0430\u043B \u0435\u0451."
  created_at: 2024-01-18 07:02:37+00:00
  edited: false
  hidden: false
  id: 65a8cd0dfee7ba6063d72065
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676723695399-63ac8a10073766d08506ea74.png?w=200&h=200&f=face
      fullname: Paul Rock
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: evilfreelancer
      type: user
    createdAt: '2024-01-18T07:02:37.000Z'
    data:
      status: closed
    id: 65a8cd0dfee7ba6063d7206b
    type: status-change
  author: evilfreelancer
  created_at: 2024-01-18 07:02:37+00:00
  id: 65a8cd0dfee7ba6063d7206b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: evilfreelancer/PavelGPT-7B-128K-v0.1-LoRA
repo_type: model
status: closed
target_branch: null
title: AWQ Quantization
