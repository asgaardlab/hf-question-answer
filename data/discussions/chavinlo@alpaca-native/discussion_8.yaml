!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheAIGuyz
conflicting_files: null
created_at: 2023-03-22 21:26:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/518424d6c3703fffdf1a409cf69120c1.svg
      fullname: Justin H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheAIGuyz
      type: user
    createdAt: '2023-03-22T22:26:18.000Z'
    data:
      edited: false
      editors:
      - TheAIGuyz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/518424d6c3703fffdf1a409cf69120c1.svg
          fullname: Justin H
          isHf: false
          isPro: false
          name: TheAIGuyz
          type: user
        html: '<p>I am new to running AI on local machines</p>

          <p>I have the 10gb model of the RTX 3080</p>

          <p>I see the .bin files add up to around 30gb</p>

          <p>Will I still be able to use this model?</p>

          '
        raw: "I am new to running AI on local machines\r\n\r\nI have the 10gb model\
          \ of the RTX 3080\r\n\r\nI see the .bin files add up to around 30gb\r\n\r\
          \nWill I still be able to use this model?"
        updatedAt: '2023-03-22T22:26:18.958Z'
      numEdits: 0
      reactions: []
    id: 641b808aaebaa27e07526724
    type: comment
  author: TheAIGuyz
  content: "I am new to running AI on local machines\r\n\r\nI have the 10gb model\
    \ of the RTX 3080\r\n\r\nI see the .bin files add up to around 30gb\r\n\r\nWill\
    \ I still be able to use this model?"
  created_at: 2023-03-22 21:26:18+00:00
  edited: false
  hidden: false
  id: 641b808aaebaa27e07526724
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d6860a551de0d4912e08e64589921dc.svg
      fullname: John Steward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HDiffusion
      type: user
    createdAt: '2023-03-23T02:09:48.000Z'
    data:
      edited: false
      editors:
      - HDiffusion
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d6860a551de0d4912e08e64589921dc.svg
          fullname: John Steward
          isHf: false
          isPro: false
          name: HDiffusion
          type: user
        html: '<p>With cpu offloading in the textgen webui it should work fine. If
          you quantize it to 4bit you should be able to fit the whole thing on your
          gpu. The reason the model is so big is because it''s saved in 32bit, it
          will only be run in 16bit at most for inference.</p>

          '
        raw: With cpu offloading in the textgen webui it should work fine. If you
          quantize it to 4bit you should be able to fit the whole thing on your gpu.
          The reason the model is so big is because it's saved in 32bit, it will only
          be run in 16bit at most for inference.
        updatedAt: '2023-03-23T02:09:48.714Z'
      numEdits: 0
      reactions: []
    id: 641bb4ec5d0c7772c60f8047
    type: comment
  author: HDiffusion
  content: With cpu offloading in the textgen webui it should work fine. If you quantize
    it to 4bit you should be able to fit the whole thing on your gpu. The reason the
    model is so big is because it's saved in 32bit, it will only be run in 16bit at
    most for inference.
  created_at: 2023-03-23 01:09:48+00:00
  edited: false
  hidden: false
  id: 641bb4ec5d0c7772c60f8047
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62140dcdcf7928035e8135ad/D9AF5Fh43weMqnG6-H39r.png?w=200&h=200&f=face
      fullname: Kaizhao Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kz919
      type: user
    createdAt: '2023-04-10T01:02:37.000Z'
    data:
      edited: true
      editors:
      - kz919
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62140dcdcf7928035e8135ad/D9AF5Fh43weMqnG6-H39r.png?w=200&h=200&f=face
          fullname: Kaizhao Liang
          isHf: false
          isPro: false
          name: kz919
          type: user
        html: '<p>I tested load_in_8bit=True, but it seems it spells out only nonsenses.
          It would be great if we could figure out how to do int8 quantization on
          this, it will make things even faster.<br>But it will fit on 10GB 3080,
          once you use that flag. The current memory consumption is around 14GB with
          fp16/bf16, and with int8 it will be cut in half.</p>

          '
        raw: 'I tested load_in_8bit=True, but it seems it spells out only nonsenses.
          It would be great if we could figure out how to do int8 quantization on
          this, it will make things even faster.

          But it will fit on 10GB 3080, once you use that flag. The current memory
          consumption is around 14GB with fp16/bf16, and with int8 it will be cut
          in half.'
        updatedAt: '2023-04-10T01:05:05.729Z'
      numEdits: 2
      reactions: []
    id: 6433602d37d643c2691248c2
    type: comment
  author: kz919
  content: 'I tested load_in_8bit=True, but it seems it spells out only nonsenses.
    It would be great if we could figure out how to do int8 quantization on this,
    it will make things even faster.

    But it will fit on 10GB 3080, once you use that flag. The current memory consumption
    is around 14GB with fp16/bf16, and with int8 it will be cut in half.'
  created_at: 2023-04-10 00:02:37+00:00
  edited: true
  hidden: false
  id: 6433602d37d643c2691248c2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: chavinlo/alpaca-native
repo_type: model
status: open
target_branch: null
title: Will this work on an RTX 3080 10gb?
