!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yehiaserag
conflicting_files: null
created_at: 2023-03-18 23:45:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-03-19T00:45:26.000Z'
    data:
      edited: false
      editors:
      - yehiaserag
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: '<p>It would help a lot if we can get a 4bit version, since all the
          4bit versions out there are either based on the lora or are not working
          as expected</p>

          '
        raw: It would help a lot if we can get a 4bit version, since all the 4bit
          versions out there are either based on the lora or are not working as expected
        updatedAt: '2023-03-19T00:45:26.160Z'
      numEdits: 0
      reactions: []
    id: 64165b26486c7c9a5d189a71
    type: comment
  author: yehiaserag
  content: It would help a lot if we can get a 4bit version, since all the 4bit versions
    out there are either based on the lora or are not working as expected
  created_at: 2023-03-18 23:45:26+00:00
  edited: false
  hidden: false
  id: 64165b26486c7c9a5d189a71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400a068f4ff62c2616e81c8/Bu0oTEYKcYe6OzQiHdp_s.jpeg?w=200&h=200&f=face
      fullname: TaiyouIllusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TaiyouIllusion
      type: user
    createdAt: '2023-03-20T06:07:48.000Z'
    data:
      edited: false
      editors:
      - TaiyouIllusion
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400a068f4ff62c2616e81c8/Bu0oTEYKcYe6OzQiHdp_s.jpeg?w=200&h=200&f=face
          fullname: TaiyouIllusion
          isHf: false
          isPro: false
          name: TaiyouIllusion
          type: user
        html: "<p>I quantized the model in google colab and tested it with alpaca.cpp.\
          \ The quality is a bit improved compared to lora merged version.<br>I made\
          \ a magnet link for the quantized version of this (file type is .bin). \
          \ <span data-props=\"{&quot;user&quot;:&quot;chavinlo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/chavinlo\">@<span class=\"\
          underline\">chavinlo</span></a></span>\n\n\t</span></span> Can I share the\
          \ link on github?</p>\n"
        raw: 'I quantized the model in google colab and tested it with alpaca.cpp.
          The quality is a bit improved compared to lora merged version.

          I made a magnet link for the quantized version of this (file type is .bin).  @chavinlo
          Can I share the link on github?'
        updatedAt: '2023-03-20T06:07:48.566Z'
      numEdits: 0
      reactions: []
    id: 6417f834b8f44a03729bf83f
    type: comment
  author: TaiyouIllusion
  content: 'I quantized the model in google colab and tested it with alpaca.cpp. The
    quality is a bit improved compared to lora merged version.

    I made a magnet link for the quantized version of this (file type is .bin).  @chavinlo
    Can I share the link on github?'
  created_at: 2023-03-20 05:07:48+00:00
  edited: false
  hidden: false
  id: 6417f834b8f44a03729bf83f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400a068f4ff62c2616e81c8/Bu0oTEYKcYe6OzQiHdp_s.jpeg?w=200&h=200&f=face
      fullname: TaiyouIllusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TaiyouIllusion
      type: user
    createdAt: '2023-03-20T06:08:44.000Z'
    data:
      edited: false
      editors:
      - TaiyouIllusion
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400a068f4ff62c2616e81c8/Bu0oTEYKcYe6OzQiHdp_s.jpeg?w=200&h=200&f=face
          fullname: TaiyouIllusion
          isHf: false
          isPro: false
          name: TaiyouIllusion
          type: user
        html: '<p>The format is ggml.</p>

          '
        raw: The format is ggml.
        updatedAt: '2023-03-20T06:08:44.783Z'
      numEdits: 0
      reactions: []
    id: 6417f86c699419b1d37d85c9
    type: comment
  author: TaiyouIllusion
  content: The format is ggml.
  created_at: 2023-03-20 05:08:44+00:00
  edited: false
  hidden: false
  id: 6417f86c699419b1d37d85c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632eed9e04b24dbdb9eaa6d4/YEwpOqcVuEMN6FJ8NOwhK.gif?.png?w=200&h=200&f=face
      fullname: Chavez
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: chavinlo
      type: user
    createdAt: '2023-03-20T16:53:09.000Z'
    data:
      edited: false
      editors:
      - chavinlo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632eed9e04b24dbdb9eaa6d4/YEwpOqcVuEMN6FJ8NOwhK.gif?.png?w=200&h=200&f=face
          fullname: Chavez
          isHf: false
          isPro: false
          name: chavinlo
          type: user
        html: "<blockquote>\n<p>I quantized the model in google colab and tested it\
          \ with alpaca.cpp. The quality is a bit improved compared to lora merged\
          \ version.<br>I made a magnet link for the quantized version of this (file\
          \ type is .bin).  <span data-props=\"{&quot;user&quot;:&quot;chavinlo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/chavinlo\"\
          >@<span class=\"underline\">chavinlo</span></a></span>\n\n\t</span></span>\
          \ Can I share the link on github?</p>\n</blockquote>\n<p>sure or you can\
          \ do it here and I can link it on the readme</p>\n"
        raw: '> I quantized the model in google colab and tested it with alpaca.cpp.
          The quality is a bit improved compared to lora merged version.

          > I made a magnet link for the quantized version of this (file type is .bin).  @chavinlo
          Can I share the link on github?


          sure or you can do it here and I can link it on the readme'
        updatedAt: '2023-03-20T16:53:09.816Z'
      numEdits: 0
      reactions: []
    id: 64188f7509150c1101ff8426
    type: comment
  author: chavinlo
  content: '> I quantized the model in google colab and tested it with alpaca.cpp.
    The quality is a bit improved compared to lora merged version.

    > I made a magnet link for the quantized version of this (file type is .bin).  @chavinlo
    Can I share the link on github?


    sure or you can do it here and I can link it on the readme'
  created_at: 2023-03-20 15:53:09+00:00
  edited: false
  hidden: false
  id: 64188f7509150c1101ff8426
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400a068f4ff62c2616e81c8/Bu0oTEYKcYe6OzQiHdp_s.jpeg?w=200&h=200&f=face
      fullname: TaiyouIllusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TaiyouIllusion
      type: user
    createdAt: '2023-03-21T01:04:56.000Z'
    data:
      edited: false
      editors:
      - TaiyouIllusion
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400a068f4ff62c2616e81c8/Bu0oTEYKcYe6OzQiHdp_s.jpeg?w=200&h=200&f=face
          fullname: TaiyouIllusion
          isHf: false
          isPro: false
          name: TaiyouIllusion
          type: user
        html: '<p>Thanks! Here''s the link(sorry if it was too long, I used online
          generator):</p>

          <p>magnet:?xt=urn:btih:69fb9b4c1e0888336f5253ae75d3e10a9299ab7d&amp;dn=ggml-alpaca-7b-native-q4.bin&amp;tr=http%3A%2F%2F125.227.35.196%3A6969%2Fannounce&amp;tr=http%3A%2F%2F210.244.71.25%3A6969%2Fannounce&amp;tr=http%3A%2F%2F210.244.71.26%3A6969%2Fannounce&amp;tr=http%3A%2F%2F213.159.215.198%3A6970%2Fannounce&amp;tr=http%3A%2F%2F37.19.5.139%3A6969%2Fannounce&amp;tr=http%3A%2F%2F37.19.5.155%3A6881%2Fannounce&amp;tr=http%3A%2F%2F46.4.109.148%3A6969%2Fannounce&amp;tr=http%3A%2F%2F87.248.186.252%3A8080%2Fannounce&amp;tr=http%3A%2F%2Fasmlocator.ru%3A34000%2F1hfZS1k4jh%2Fannounce&amp;tr=http%3A%2F%2Fbt.evrl.to%2Fannounce&amp;tr=http%3A%2F%2Fbt.rutracker.org%2Fann&amp;tr=https%3A%2F%2F<a
          rel="nofollow" href="http://www.artikelplanet.nl&amp;tr=http%3A%2F%2Fmgtracker.org%3A6969%2Fannounce&amp;tr=http%3A%2F%2Fpubt.net%3A2710%2Fannounce&amp;tr=http%3A%2F%2Ftracker.baravik.org%3A6970%2Fannounce&amp;tr=http%3A%2F%2Ftracker.dler.org%3A6969%2Fannounce&amp;tr=http%3A%2F%2Ftracker.filetracker.pl%3A8089%2Fannounce&amp;tr=http%3A%2F%2Ftracker.grepler.com%3A6969%2Fannounce&amp;tr=http%3A%2F%2Ftracker.mg64.net%3A6881%2Fannounce&amp;tr=http%3A%2F%2Ftracker.tiny-vps.com%3A6969%2Fannounce&amp;tr=http%3A%2F%2Ftracker.torrentyorg.pl%2Fannounce&amp;tr=https%3A%2F%2Finternet.sitelio.me%2F&amp;tr=https%3A%2F%2Fcomputer1.sitelio.me%2F&amp;tr=udp%3A%2F%2F168.235.67.63%3A6969&amp;tr=udp%3A%2F%2F182.176.139.129%3A6969&amp;tr=udp%3A%2F%2F37.19.5.155%3A2710&amp;tr=udp%3A%2F%2F46.148.18.250%3A2710&amp;tr=udp%3A%2F%2F46.4.109.148%3A6969&amp;tr=udp%3A%2F%2Fcomputerbedrijven.bestelinks.nl%2F&amp;tr=udp%3A%2F%2Fcomputerbedrijven.startsuper.nl%2F&amp;tr=udp%3A%2F%2Fcomputershop.goedbegin.nl%2F&amp;tr=udp%3A%2F%2Fc3t.org&amp;tr=udp%3A%2F%2Fallerhandelenlaag.nl&amp;tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80&amp;tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337&amp;tr=udp%3A%2F%2Ftracker.publicbt.com%3A80&amp;tr=udp%3A%2F%2Ftracker.tiny-vps.com%3A6969">www.artikelplanet.nl&amp;tr=http%3A%2F%2Fmgtracker.org%3A6969%2Fannounce&amp;tr=http%3A%2F%2Fpubt.net%3A2710%2Fannounce&amp;tr=http%3A%2F%2Ftracker.baravik.org%3A6970%2Fannounce&amp;tr=http%3A%2F%2Ftracker.dler.org%3A6969%2Fannounce&amp;tr=http%3A%2F%2Ftracker.filetracker.pl%3A8089%2Fannounce&amp;tr=http%3A%2F%2Ftracker.grepler.com%3A6969%2Fannounce&amp;tr=http%3A%2F%2Ftracker.mg64.net%3A6881%2Fannounce&amp;tr=http%3A%2F%2Ftracker.tiny-vps.com%3A6969%2Fannounce&amp;tr=http%3A%2F%2Ftracker.torrentyorg.pl%2Fannounce&amp;tr=https%3A%2F%2Finternet.sitelio.me%2F&amp;tr=https%3A%2F%2Fcomputer1.sitelio.me%2F&amp;tr=udp%3A%2F%2F168.235.67.63%3A6969&amp;tr=udp%3A%2F%2F182.176.139.129%3A6969&amp;tr=udp%3A%2F%2F37.19.5.155%3A2710&amp;tr=udp%3A%2F%2F46.148.18.250%3A2710&amp;tr=udp%3A%2F%2F46.4.109.148%3A6969&amp;tr=udp%3A%2F%2Fcomputerbedrijven.bestelinks.nl%2F&amp;tr=udp%3A%2F%2Fcomputerbedrijven.startsuper.nl%2F&amp;tr=udp%3A%2F%2Fcomputershop.goedbegin.nl%2F&amp;tr=udp%3A%2F%2Fc3t.org&amp;tr=udp%3A%2F%2Fallerhandelenlaag.nl&amp;tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80&amp;tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337&amp;tr=udp%3A%2F%2Ftracker.publicbt.com%3A80&amp;tr=udp%3A%2F%2Ftracker.tiny-vps.com%3A6969</a></p>

          '
        raw: 'Thanks! Here''s the link(sorry if it was too long, I used online generator):


          magnet:?xt=urn:btih:69fb9b4c1e0888336f5253ae75d3e10a9299ab7d&dn=ggml-alpaca-7b-native-q4.bin&tr=http%3A%2F%2F125.227.35.196%3A6969%2Fannounce&tr=http%3A%2F%2F210.244.71.25%3A6969%2Fannounce&tr=http%3A%2F%2F210.244.71.26%3A6969%2Fannounce&tr=http%3A%2F%2F213.159.215.198%3A6970%2Fannounce&tr=http%3A%2F%2F37.19.5.139%3A6969%2Fannounce&tr=http%3A%2F%2F37.19.5.155%3A6881%2Fannounce&tr=http%3A%2F%2F46.4.109.148%3A6969%2Fannounce&tr=http%3A%2F%2F87.248.186.252%3A8080%2Fannounce&tr=http%3A%2F%2Fasmlocator.ru%3A34000%2F1hfZS1k4jh%2Fannounce&tr=http%3A%2F%2Fbt.evrl.to%2Fannounce&tr=http%3A%2F%2Fbt.rutracker.org%2Fann&tr=https%3A%2F%2Fwww.artikelplanet.nl&tr=http%3A%2F%2Fmgtracker.org%3A6969%2Fannounce&tr=http%3A%2F%2Fpubt.net%3A2710%2Fannounce&tr=http%3A%2F%2Ftracker.baravik.org%3A6970%2Fannounce&tr=http%3A%2F%2Ftracker.dler.org%3A6969%2Fannounce&tr=http%3A%2F%2Ftracker.filetracker.pl%3A8089%2Fannounce&tr=http%3A%2F%2Ftracker.grepler.com%3A6969%2Fannounce&tr=http%3A%2F%2Ftracker.mg64.net%3A6881%2Fannounce&tr=http%3A%2F%2Ftracker.tiny-vps.com%3A6969%2Fannounce&tr=http%3A%2F%2Ftracker.torrentyorg.pl%2Fannounce&tr=https%3A%2F%2Finternet.sitelio.me%2F&tr=https%3A%2F%2Fcomputer1.sitelio.me%2F&tr=udp%3A%2F%2F168.235.67.63%3A6969&tr=udp%3A%2F%2F182.176.139.129%3A6969&tr=udp%3A%2F%2F37.19.5.155%3A2710&tr=udp%3A%2F%2F46.148.18.250%3A2710&tr=udp%3A%2F%2F46.4.109.148%3A6969&tr=udp%3A%2F%2Fcomputerbedrijven.bestelinks.nl%2F&tr=udp%3A%2F%2Fcomputerbedrijven.startsuper.nl%2F&tr=udp%3A%2F%2Fcomputershop.goedbegin.nl%2F&tr=udp%3A%2F%2Fc3t.org&tr=udp%3A%2F%2Fallerhandelenlaag.nl&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80&tr=udp%3A%2F%2Ftracker.tiny-vps.com%3A6969'
        updatedAt: '2023-03-21T01:04:56.750Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - HenkPoley
        - futureagi
        - corentinm7
        - lamrin
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - futureagi
        - corentinm7
    id: 641902b82fcc7664a1fd28da
    type: comment
  author: TaiyouIllusion
  content: 'Thanks! Here''s the link(sorry if it was too long, I used online generator):


    magnet:?xt=urn:btih:69fb9b4c1e0888336f5253ae75d3e10a9299ab7d&dn=ggml-alpaca-7b-native-q4.bin&tr=http%3A%2F%2F125.227.35.196%3A6969%2Fannounce&tr=http%3A%2F%2F210.244.71.25%3A6969%2Fannounce&tr=http%3A%2F%2F210.244.71.26%3A6969%2Fannounce&tr=http%3A%2F%2F213.159.215.198%3A6970%2Fannounce&tr=http%3A%2F%2F37.19.5.139%3A6969%2Fannounce&tr=http%3A%2F%2F37.19.5.155%3A6881%2Fannounce&tr=http%3A%2F%2F46.4.109.148%3A6969%2Fannounce&tr=http%3A%2F%2F87.248.186.252%3A8080%2Fannounce&tr=http%3A%2F%2Fasmlocator.ru%3A34000%2F1hfZS1k4jh%2Fannounce&tr=http%3A%2F%2Fbt.evrl.to%2Fannounce&tr=http%3A%2F%2Fbt.rutracker.org%2Fann&tr=https%3A%2F%2Fwww.artikelplanet.nl&tr=http%3A%2F%2Fmgtracker.org%3A6969%2Fannounce&tr=http%3A%2F%2Fpubt.net%3A2710%2Fannounce&tr=http%3A%2F%2Ftracker.baravik.org%3A6970%2Fannounce&tr=http%3A%2F%2Ftracker.dler.org%3A6969%2Fannounce&tr=http%3A%2F%2Ftracker.filetracker.pl%3A8089%2Fannounce&tr=http%3A%2F%2Ftracker.grepler.com%3A6969%2Fannounce&tr=http%3A%2F%2Ftracker.mg64.net%3A6881%2Fannounce&tr=http%3A%2F%2Ftracker.tiny-vps.com%3A6969%2Fannounce&tr=http%3A%2F%2Ftracker.torrentyorg.pl%2Fannounce&tr=https%3A%2F%2Finternet.sitelio.me%2F&tr=https%3A%2F%2Fcomputer1.sitelio.me%2F&tr=udp%3A%2F%2F168.235.67.63%3A6969&tr=udp%3A%2F%2F182.176.139.129%3A6969&tr=udp%3A%2F%2F37.19.5.155%3A2710&tr=udp%3A%2F%2F46.148.18.250%3A2710&tr=udp%3A%2F%2F46.4.109.148%3A6969&tr=udp%3A%2F%2Fcomputerbedrijven.bestelinks.nl%2F&tr=udp%3A%2F%2Fcomputerbedrijven.startsuper.nl%2F&tr=udp%3A%2F%2Fcomputershop.goedbegin.nl%2F&tr=udp%3A%2F%2Fc3t.org&tr=udp%3A%2F%2Fallerhandelenlaag.nl&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80&tr=udp%3A%2F%2Ftracker.tiny-vps.com%3A6969'
  created_at: 2023-03-21 00:04:56+00:00
  edited: false
  hidden: false
  id: 641902b82fcc7664a1fd28da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5eab086489ffe8890f0e8ec32652f80e.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: heartyhotdish
      type: user
    createdAt: '2023-03-21T01:38:35.000Z'
    data:
      edited: true
      editors:
      - heartyhotdish
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5eab086489ffe8890f0e8ec32652f80e.svg
          fullname: ko
          isHf: false
          isPro: false
          name: heartyhotdish
          type: user
        html: '<p>Thank you so much for this.  I can confirm that the quantized native
          model from Taiyouillusion''s magnet link is legit.  Running on alpaca.cpp,
          it''s a big leap forward in response quality compared to the 7B or 13B alpaca-lora
          models.  What a time to be alive!</p>

          '
        raw: Thank you so much for this.  I can confirm that the quantized native
          model from Taiyouillusion's magnet link is legit.  Running on alpaca.cpp,
          it's a big leap forward in response quality compared to the 7B or 13B alpaca-lora
          models.  What a time to be alive!
        updatedAt: '2023-03-21T01:42:28.914Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - HenkPoley
        - futureagi
        - Jimmie
    id: 64190a9b7818018e3e4ddd8c
    type: comment
  author: heartyhotdish
  content: Thank you so much for this.  I can confirm that the quantized native model
    from Taiyouillusion's magnet link is legit.  Running on alpaca.cpp, it's a big
    leap forward in response quality compared to the 7B or 13B alpaca-lora models.  What
    a time to be alive!
  created_at: 2023-03-21 00:38:35+00:00
  edited: true
  hidden: false
  id: 64190a9b7818018e3e4ddd8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d025d0e937737a7e9f895e3d10aaba5e.svg
      fullname: Mike Hamilton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gorbypark
      type: user
    createdAt: '2023-03-21T15:07:48.000Z'
    data:
      edited: false
      editors:
      - gorbypark
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d025d0e937737a7e9f895e3d10aaba5e.svg
          fullname: Mike Hamilton
          isHf: false
          isPro: false
          name: gorbypark
          type: user
        html: '<blockquote>

          <p>The format is ggml.</p>

          </blockquote>

          <p>Can you share how you converted the post-trained HF weights back into
          the standard llama format, for conversion to ggml? Or did you go direct
          from HF to ggml somehow?  I got hung up on a few things, one being that
          convert-pth-to-ggml.py (from llama.cpp) calls numpy().squeeze() on the data
          which does not support bfloat16, which alpaca uses.  That was a quick fix
          (not sure if my hack would affect anything, but anyways), but quantize step
          then fails.  From some sleuthing around, it seems like there needs to be
          a conversion step after the fine tuning to get the weights back into the
          standard llama format.</p>

          '
        raw: '> The format is ggml.


          Can you share how you converted the post-trained HF weights back into the
          standard llama format, for conversion to ggml? Or did you go direct from
          HF to ggml somehow?  I got hung up on a few things, one being that convert-pth-to-ggml.py
          (from llama.cpp) calls numpy().squeeze() on the data which does not support
          bfloat16, which alpaca uses.  That was a quick fix (not sure if my hack
          would affect anything, but anyways), but quantize step then fails.  From
          some sleuthing around, it seems like there needs to be a conversion step
          after the fine tuning to get the weights back into the standard llama format.'
        updatedAt: '2023-03-21T15:07:48.552Z'
      numEdits: 0
      reactions: []
    id: 6419c84459097c6418ebef29
    type: comment
  author: gorbypark
  content: '> The format is ggml.


    Can you share how you converted the post-trained HF weights back into the standard
    llama format, for conversion to ggml? Or did you go direct from HF to ggml somehow?  I
    got hung up on a few things, one being that convert-pth-to-ggml.py (from llama.cpp)
    calls numpy().squeeze() on the data which does not support bfloat16, which alpaca
    uses.  That was a quick fix (not sure if my hack would affect anything, but anyways),
    but quantize step then fails.  From some sleuthing around, it seems like there
    needs to be a conversion step after the fine tuning to get the weights back into
    the standard llama format.'
  created_at: 2023-03-21 14:07:48+00:00
  edited: false
  hidden: false
  id: 6419c84459097c6418ebef29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400a068f4ff62c2616e81c8/Bu0oTEYKcYe6OzQiHdp_s.jpeg?w=200&h=200&f=face
      fullname: TaiyouIllusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TaiyouIllusion
      type: user
    createdAt: '2023-03-21T16:47:56.000Z'
    data:
      edited: false
      editors:
      - TaiyouIllusion
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6400a068f4ff62c2616e81c8/Bu0oTEYKcYe6OzQiHdp_s.jpeg?w=200&h=200&f=face
          fullname: TaiyouIllusion
          isHf: false
          isPro: false
          name: TaiyouIllusion
          type: user
        html: '<p>I uploaded the script I used in colab to convert the HF model on
          github: <a rel="nofollow" href="https://github.com/taiyou2000/alpaca-convert-colab/blob/main/alpaca-convert-colab-fixed.ipynb">https://github.com/taiyou2000/alpaca-convert-colab/blob/main/alpaca-convert-colab-fixed.ipynb</a>.</p>

          '
        raw: 'I uploaded the script I used in colab to convert the HF model on github:
          https://github.com/taiyou2000/alpaca-convert-colab/blob/main/alpaca-convert-colab-fixed.ipynb.'
        updatedAt: '2023-03-21T16:47:56.109Z'
      numEdits: 0
      reactions: []
    id: 6419dfbc0eca818e164ac86d
    type: comment
  author: TaiyouIllusion
  content: 'I uploaded the script I used in colab to convert the HF model on github:
    https://github.com/taiyou2000/alpaca-convert-colab/blob/main/alpaca-convert-colab-fixed.ipynb.'
  created_at: 2023-03-21 15:47:56+00:00
  edited: false
  hidden: false
  id: 6419dfbc0eca818e164ac86d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/08c8acc9d4d1942b8f22d37d18d15583.svg
      fullname: "Marko Sch\xFCtz-Schmuck"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: okram
      type: user
    createdAt: '2023-03-21T18:16:54.000Z'
    data:
      edited: false
      editors:
      - okram
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/08c8acc9d4d1942b8f22d37d18d15583.svg
          fullname: "Marko Sch\xFCtz-Schmuck"
          isHf: false
          isPro: false
          name: okram
          type: user
        html: "<blockquote>\n<p>I uploaded the script I used in colab to convert the\
          \ HF model on github: <a rel=\"nofollow\" href=\"https://github.com/taiyou2000/alpaca-convert-colab/blob/main/alpaca-convert-colab-fixed.ipynb\"\
          >https://github.com/taiyou2000/alpaca-convert-colab/blob/main/alpaca-convert-colab-fixed.ipynb</a>.</p>\n\
          </blockquote>\n<p>when I try running this script, I get first an error about\
          \ accelerate missing and after installing that, I  get:</p>\n<pre><code>NameError\
          \                                 Traceback (most recent call last)\n\n\
          &lt;ipython-input-4-bd7436545f55&gt; in &lt;module&gt;\n      8 tokenizer\
          \ = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n\
          \      9 \n---&gt; 10 base_model = LLaMAForCausalLM.from_pretrained(\n \
          \    11     \"chavinlo/alpaca-native\",\n     12     load_in_8bit=False,\n\
          \n/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n\
          \   2488             init_contexts = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config())]\
          \ + init_contexts\n   2489         elif load_in_8bit or low_cpu_mem_usage:\n\
          -&gt; 2490             init_contexts.append(init_empty_weights())\n   2491\
          \ \n   2492         with ContextManagers(init_contexts):\n\nNameError: name\
          \ 'init_empty_weights' is not defined\n</code></pre>\n<p>Any hints on fixing\
          \ this?</p>\n"
        raw: "> I uploaded the script I used in colab to convert the HF model on github:\
          \ https://github.com/taiyou2000/alpaca-convert-colab/blob/main/alpaca-convert-colab-fixed.ipynb.\n\
          \nwhen I try running this script, I get first an error about accelerate\
          \ missing and after installing that, I  get:\n```\nNameError           \
          \                      Traceback (most recent call last)\n\n<ipython-input-4-bd7436545f55>\
          \ in <module>\n      8 tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\"\
          )\n      9 \n---> 10 base_model = LLaMAForCausalLM.from_pretrained(\n  \
          \   11     \"chavinlo/alpaca-native\",\n     12     load_in_8bit=False,\n\
          \n/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n\
          \   2488             init_contexts = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config())]\
          \ + init_contexts\n   2489         elif load_in_8bit or low_cpu_mem_usage:\n\
          -> 2490             init_contexts.append(init_empty_weights())\n   2491\
          \ \n   2492         with ContextManagers(init_contexts):\n\nNameError: name\
          \ 'init_empty_weights' is not defined\n```\nAny hints on fixing this?"
        updatedAt: '2023-03-21T18:16:54.051Z'
      numEdits: 0
      reactions: []
    id: 6419f4960eca818e164c301b
    type: comment
  author: okram
  content: "> I uploaded the script I used in colab to convert the HF model on github:\
    \ https://github.com/taiyou2000/alpaca-convert-colab/blob/main/alpaca-convert-colab-fixed.ipynb.\n\
    \nwhen I try running this script, I get first an error about accelerate missing\
    \ and after installing that, I  get:\n```\nNameError                         \
    \        Traceback (most recent call last)\n\n<ipython-input-4-bd7436545f55> in\
    \ <module>\n      8 tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\"\
    )\n      9 \n---> 10 base_model = LLaMAForCausalLM.from_pretrained(\n     11 \
    \    \"chavinlo/alpaca-native\",\n     12     load_in_8bit=False,\n\n/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n\
    \   2488             init_contexts = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config())]\
    \ + init_contexts\n   2489         elif load_in_8bit or low_cpu_mem_usage:\n->\
    \ 2490             init_contexts.append(init_empty_weights())\n   2491 \n   2492\
    \         with ContextManagers(init_contexts):\n\nNameError: name 'init_empty_weights'\
    \ is not defined\n```\nAny hints on fixing this?"
  created_at: 2023-03-21 17:16:54+00:00
  edited: false
  hidden: false
  id: 6419f4960eca818e164c301b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-XFUv_yu4v_0OoR5Wg7zD.jpeg?w=200&h=200&f=face
      fullname: Blackle Mori
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: blackle
      type: user
    createdAt: '2023-03-24T02:34:59.000Z'
    data:
      edited: false
      editors:
      - blackle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-XFUv_yu4v_0OoR5Wg7zD.jpeg?w=200&h=200&f=face
          fullname: Blackle Mori
          isHf: false
          isPro: false
          name: blackle
          type: user
        html: '<p>Because the upstream llama.cpp repository recently changed the quantized
          ggml format, any old q4.bin files will stop working, so I had to requantize
          this. I did manage to get it working. I had to remove the "accelerate" pip3
          package, and use a colab runtime with a lot of ram. I was constantly almost
          running out of disk space while doing the conversion, I <em>just</em> managed
          to convert it.</p>

          <p>here''s the magnet link: magnet:?xt=urn:btih:0e51003c8a5610aa713f675891f0a7f87051be1a&amp;dn=ggml-alpaca-7b-native-q4.bin&amp;tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce</p>

          <p>sometimes I find that a magnet link won''t work unless a few people have
          downloaded thru the actual torrent file. you can find it at "suricrasia
          dot online slash stuff slash ggml-alpaca-7b-native-q4 dot bin dot torrent
          dot txt" just replace "dot" with "." and "slash" with "/"</p>

          '
        raw: 'Because the upstream llama.cpp repository recently changed the quantized
          ggml format, any old q4.bin files will stop working, so I had to requantize
          this. I did manage to get it working. I had to remove the "accelerate" pip3
          package, and use a colab runtime with a lot of ram. I was constantly almost
          running out of disk space while doing the conversion, I *just* managed to
          convert it.


          here''s the magnet link: magnet:?xt=urn:btih:0e51003c8a5610aa713f675891f0a7f87051be1a&dn=ggml-alpaca-7b-native-q4.bin&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce


          sometimes I find that a magnet link won''t work unless a few people have
          downloaded thru the actual torrent file. you can find it at "suricrasia
          dot online slash stuff slash ggml-alpaca-7b-native-q4 dot bin dot torrent
          dot txt" just replace "dot" with "." and "slash" with "/"'
        updatedAt: '2023-03-24T02:34:59.500Z'
      numEdits: 0
      reactions: []
    id: 641d0c53aa941743c6cbd770
    type: comment
  author: blackle
  content: 'Because the upstream llama.cpp repository recently changed the quantized
    ggml format, any old q4.bin files will stop working, so I had to requantize this.
    I did manage to get it working. I had to remove the "accelerate" pip3 package,
    and use a colab runtime with a lot of ram. I was constantly almost running out
    of disk space while doing the conversion, I *just* managed to convert it.


    here''s the magnet link: magnet:?xt=urn:btih:0e51003c8a5610aa713f675891f0a7f87051be1a&dn=ggml-alpaca-7b-native-q4.bin&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce


    sometimes I find that a magnet link won''t work unless a few people have downloaded
    thru the actual torrent file. you can find it at "suricrasia dot online slash
    stuff slash ggml-alpaca-7b-native-q4 dot bin dot torrent dot txt" just replace
    "dot" with "." and "slash" with "/"'
  created_at: 2023-03-24 01:34:59+00:00
  edited: false
  hidden: false
  id: 641d0c53aa941743c6cbd770
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8caaeab7c23abc7452780c0b07610fbe.svg
      fullname: Black_Engineer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Black-Engineer
      type: user
    createdAt: '2023-03-24T12:10:08.000Z'
    data:
      edited: false
      editors:
      - Black-Engineer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8caaeab7c23abc7452780c0b07610fbe.svg
          fullname: Black_Engineer
          isHf: false
          isPro: false
          name: Black-Engineer
          type: user
        html: '<blockquote>

          <p>Because the upstream llama.cpp repository recently changed the quantized
          ggml format, any old q4.bin files will stop working, so I had to requantize
          this. I did manage to get it working. I had to remove the "accelerate" pip3
          package, and use a colab runtime with a lot of ram. I was constantly almost
          running out of disk space while doing the conversion, I <em>just</em> managed
          to convert it.</p>

          <p>here''s the magnet link: magnet:?xt=urn:btih:0e51003c8a5610aa713f675891f0a7f87051be1a&amp;dn=ggml-alpaca-7b-native-q4.bin&amp;tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce</p>

          <p>sometimes I find that a magnet link won''t work unless a few people have
          downloaded thru the actual torrent file. you can find it at "suricrasia
          dot online slash stuff slash ggml-alpaca-7b-native-q4 dot bin dot torrent
          dot txt" just replace "dot" with "." and "slash" with "/"</p>

          </blockquote>

          <p>Can u post what you changed in google colab?</p>

          '
        raw: "> Because the upstream llama.cpp repository recently changed the quantized\
          \ ggml format, any old q4.bin files will stop working, so I had to requantize\
          \ this. I did manage to get it working. I had to remove the \"accelerate\"\
          \ pip3 package, and use a colab runtime with a lot of ram. I was constantly\
          \ almost running out of disk space while doing the conversion, I *just*\
          \ managed to convert it.\n> \n> here's the magnet link: magnet:?xt=urn:btih:0e51003c8a5610aa713f675891f0a7f87051be1a&dn=ggml-alpaca-7b-native-q4.bin&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce\n\
          > \n> sometimes I find that a magnet link won't work unless a few people\
          \ have downloaded thru the actual torrent file. you can find it at \"suricrasia\
          \ dot online slash stuff slash ggml-alpaca-7b-native-q4 dot bin dot torrent\
          \ dot txt\" just replace \"dot\" with \".\" and \"slash\" with \"/\"\n\n\
          Can u post what you changed in google colab?"
        updatedAt: '2023-03-24T12:10:08.091Z'
      numEdits: 0
      reactions: []
    id: 641d932067960a732edfd697
    type: comment
  author: Black-Engineer
  content: "> Because the upstream llama.cpp repository recently changed the quantized\
    \ ggml format, any old q4.bin files will stop working, so I had to requantize\
    \ this. I did manage to get it working. I had to remove the \"accelerate\" pip3\
    \ package, and use a colab runtime with a lot of ram. I was constantly almost\
    \ running out of disk space while doing the conversion, I *just* managed to convert\
    \ it.\n> \n> here's the magnet link: magnet:?xt=urn:btih:0e51003c8a5610aa713f675891f0a7f87051be1a&dn=ggml-alpaca-7b-native-q4.bin&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce\n\
    > \n> sometimes I find that a magnet link won't work unless a few people have\
    \ downloaded thru the actual torrent file. you can find it at \"suricrasia dot\
    \ online slash stuff slash ggml-alpaca-7b-native-q4 dot bin dot torrent dot txt\"\
    \ just replace \"dot\" with \".\" and \"slash\" with \"/\"\n\nCan u post what\
    \ you changed in google colab?"
  created_at: 2023-03-24 11:10:08+00:00
  edited: false
  hidden: false
  id: 641d932067960a732edfd697
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-XFUv_yu4v_0OoR5Wg7zD.jpeg?w=200&h=200&f=face
      fullname: Blackle Mori
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: blackle
      type: user
    createdAt: '2023-03-24T21:00:06.000Z'
    data:
      edited: false
      editors:
      - blackle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-XFUv_yu4v_0OoR5Wg7zD.jpeg?w=200&h=200&f=face
          fullname: Blackle Mori
          isHf: false
          isPro: false
          name: blackle
          type: user
        html: '<p>I actually didn''t need to change anything, I just had to run with
          google colab pro. if you don''t, it will ask you to install the "accelerate"
          package, and that''s where the error comes from.</p>

          '
        raw: I actually didn't need to change anything, I just had to run with google
          colab pro. if you don't, it will ask you to install the "accelerate" package,
          and that's where the error comes from.
        updatedAt: '2023-03-24T21:00:06.246Z'
      numEdits: 0
      reactions: []
    id: 641e0f565d820e1adc697554
    type: comment
  author: blackle
  content: I actually didn't need to change anything, I just had to run with google
    colab pro. if you don't, it will ask you to install the "accelerate" package,
    and that's where the error comes from.
  created_at: 2023-03-24 20:00:06+00:00
  edited: false
  hidden: false
  id: 641e0f565d820e1adc697554
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: chavinlo/alpaca-native
repo_type: model
status: open
target_branch: null
title: Can we get a 4bit quantized version?
