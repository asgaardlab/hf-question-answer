!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DongfuTingle
conflicting_files: null
created_at: 2023-04-10 15:26:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/FOdpXWhe4vZbLHM3TQnQU.jpeg?w=200&h=200&f=face
      fullname: Dongfu Jiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DongfuTingle
      type: user
    createdAt: '2023-04-10T16:26:46.000Z'
    data:
      edited: true
      editors:
      - DongfuTingle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/FOdpXWhe4vZbLHM3TQnQU.jpeg?w=200&h=200&f=face
          fullname: Dongfu Jiang
          isHf: false
          isPro: false
          name: DongfuTingle
          type: user
        html: '<p>I do a top p sampling on this model, and I first run it on pure
          cpu. However, I get an [error](IndexError: index out of range in self) for
          the Llama embedtokens. I  check token that cause this index error and found
          that, if you use hugging face <code>generate()</code> function to do the
          generation, it will automatically read the <code>pad_token_id</code> from
          <code>config.json</code>. And in that file, <code>pad_token_id</code> is
          set to -1, which causes this index error for the embeding.<br>I again checked
          the tokenizer <code>pad_token_id</code> from the tokenizer and and found
          that it''s actually 0 instead of -1. So I guess this must be the error in
          the <code>config.json</code> file.<br>May managers take a look at this file
          and fix this?<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/xEun0fX7QBri8UWeA5wqj.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/xEun0fX7QBri8UWeA5wqj.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/urG4DsevGCDluEZn1KyKh.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/urG4DsevGCDluEZn1KyKh.png"></a></p>

          '
        raw: "I do a top p sampling on this model, and I first run it on pure cpu.\
          \ However, I get an [error](IndexError: index out of range in self) for\
          \ the Llama embedtokens. I  check token that cause this index error and\
          \ found that, if you use hugging face `generate()` function to do the generation,\
          \ it will automatically read the `pad_token_id` from `config.json`. And\
          \ in that file, `pad_token_id` is set to -1, which causes this index error\
          \ for the embeding. \nI again checked the tokenizer `pad_token_id` from\
          \ the tokenizer and and found that it's actually 0 instead of -1. So I guess\
          \ this must be the error in the `config.json` file.\nMay managers take a\
          \ look at this file and fix this?\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/xEun0fX7QBri8UWeA5wqj.png)\n\
          \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/urG4DsevGCDluEZn1KyKh.png)"
        updatedAt: '2023-04-10T16:41:32.774Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - lover99
    id: 643438c6a5aed21dd11517f2
    type: comment
  author: DongfuTingle
  content: "I do a top p sampling on this model, and I first run it on pure cpu. However,\
    \ I get an [error](IndexError: index out of range in self) for the Llama embedtokens.\
    \ I  check token that cause this index error and found that, if you use hugging\
    \ face `generate()` function to do the generation, it will automatically read\
    \ the `pad_token_id` from `config.json`. And in that file, `pad_token_id` is set\
    \ to -1, which causes this index error for the embeding. \nI again checked the\
    \ tokenizer `pad_token_id` from the tokenizer and and found that it's actually\
    \ 0 instead of -1. So I guess this must be the error in the `config.json` file.\n\
    May managers take a look at this file and fix this?\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/xEun0fX7QBri8UWeA5wqj.png)\n\
    \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62567c86d444a9b5a0ec51c1/urG4DsevGCDluEZn1KyKh.png)"
  created_at: 2023-04-10 15:26:46+00:00
  edited: true
  hidden: false
  id: 643438c6a5aed21dd11517f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9346127ef2f641ce05923760e5ee19d5.svg
      fullname: keren
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lover99
      type: user
    createdAt: '2023-04-12T18:49:46.000Z'
    data:
      edited: false
      editors:
      - lover99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9346127ef2f641ce05923760e5ee19d5.svg
          fullname: keren
          isHf: false
          isPro: false
          name: lover99
          type: user
        html: '<p>I also find this batch generation problem and have no idea how to
          handle it, your solution works for me, thanks a lot!</p>

          '
        raw: I also find this batch generation problem and have no idea how to handle
          it, your solution works for me, thanks a lot!
        updatedAt: '2023-04-12T18:49:46.592Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - christangttt
    id: 6436fd4a1ed4fe4c0e94e4f2
    type: comment
  author: lover99
  content: I also find this batch generation problem and have no idea how to handle
    it, your solution works for me, thanks a lot!
  created_at: 2023-04-12 17:49:46+00:00
  edited: false
  hidden: false
  id: 6436fd4a1ed4fe4c0e94e4f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7431e4f55b9f1ea6d8647237ae65bcc2.svg
      fullname: Tang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: christangttt
      type: user
    createdAt: '2023-08-08T06:03:37.000Z'
    data:
      edited: false
      editors:
      - christangttt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.90147465467453
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7431e4f55b9f1ea6d8647237ae65bcc2.svg
          fullname: Tang
          isHf: false
          isPro: false
          name: christangttt
          type: user
        html: '<p>Thank you! This also fixes my bug on LLaMa. </p>

          '
        raw: 'Thank you! This also fixes my bug on LLaMa. '
        updatedAt: '2023-08-08T06:03:37.783Z'
      numEdits: 0
      reactions: []
    id: 64d1dab9228324a28b1901e3
    type: comment
  author: christangttt
  content: 'Thank you! This also fixes my bug on LLaMa. '
  created_at: 2023-08-08 05:03:37+00:00
  edited: false
  hidden: false
  id: 64d1dab9228324a28b1901e3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: chavinlo/alpaca-native
repo_type: model
status: open
target_branch: null
title: Error when generating multiple outputs using hugging face generation
