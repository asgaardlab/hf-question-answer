!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DanielTTY
conflicting_files: null
created_at: 2023-07-02 12:58:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/481394619236c114ade0003e6e424dec.svg
      fullname: TTY
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DanielTTY
      type: user
    createdAt: '2023-07-02T13:58:14.000Z'
    data:
      edited: false
      editors:
      - DanielTTY
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9594522714614868
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/481394619236c114ade0003e6e424dec.svg
          fullname: TTY
          isHf: false
          isPro: false
          name: DanielTTY
          type: user
        html: '<p>just trying to understand the qlora.py in your github, how many
          steps are there in an num_train_epochs ?</p>

          '
        raw: just trying to understand the qlora.py in your github, how many steps
          are there in an num_train_epochs ?
        updatedAt: '2023-07-02T13:58:14.752Z'
      numEdits: 0
      reactions: []
    id: 64a18276f7fb8232467f612d
    type: comment
  author: DanielTTY
  content: just trying to understand the qlora.py in your github, how many steps are
    there in an num_train_epochs ?
  created_at: 2023-07-02 12:58:14+00:00
  edited: false
  hidden: false
  id: 64a18276f7fb8232467f612d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-07-02T14:37:25.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9611921906471252
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>Depends on the number of elements in your training data, batch size,
          gradient accumulation steps, etc.  Personally I think it''s easier to go
          by epochs, and in this case I actually ended up using 5 (although with a
          higher learning rate perhaps 3 would be fine).</p>

          <p>Without gradient accumulation, and batch size of 6, on ~30k training
          rows, it''s about 5k steps per epoch, so I''m saving roughly every 5% through
          the training data.  With a gradient accumulation steps 16, the steps per
          epoch is closer to 325.</p>

          <p>1 epoch = once through the data, but steps is highly variable based on
          config, so I just updated the code to allow consistency in passes through
          training data without having to calculate steps.</p>

          <p>The save steps isn''t as important IMO, but in the event of a crash,
          or if you want to be able to test earlier checkpoints, it can be useful.</p>

          '
        raw: 'Depends on the number of elements in your training data, batch size,
          gradient accumulation steps, etc.  Personally I think it''s easier to go
          by epochs, and in this case I actually ended up using 5 (although with a
          higher learning rate perhaps 3 would be fine).


          Without gradient accumulation, and batch size of 6, on ~30k training rows,
          it''s about 5k steps per epoch, so I''m saving roughly every 5% through
          the training data.  With a gradient accumulation steps 16, the steps per
          epoch is closer to 325.


          1 epoch = once through the data, but steps is highly variable based on config,
          so I just updated the code to allow consistency in passes through training
          data without having to calculate steps.


          The save steps isn''t as important IMO, but in the event of a crash, or
          if you want to be able to test earlier checkpoints, it can be useful.'
        updatedAt: '2023-07-02T14:37:25.084Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - DanielTTY
    id: 64a18ba5ddcdc3438edd9380
    type: comment
  author: jondurbin
  content: 'Depends on the number of elements in your training data, batch size, gradient
    accumulation steps, etc.  Personally I think it''s easier to go by epochs, and
    in this case I actually ended up using 5 (although with a higher learning rate
    perhaps 3 would be fine).


    Without gradient accumulation, and batch size of 6, on ~30k training rows, it''s
    about 5k steps per epoch, so I''m saving roughly every 5% through the training
    data.  With a gradient accumulation steps 16, the steps per epoch is closer to
    325.


    1 epoch = once through the data, but steps is highly variable based on config,
    so I just updated the code to allow consistency in passes through training data
    without having to calculate steps.


    The save steps isn''t as important IMO, but in the event of a crash, or if you
    want to be able to test earlier checkpoints, it can be useful.'
  created_at: 2023-07-02 13:37:25+00:00
  edited: false
  hidden: false
  id: 64a18ba5ddcdc3438edd9380
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jondurbin/mpt-30b-qlora-compatible
repo_type: model
status: open
target_branch: null
title: num_train_epochs = 3 but save_steps = 250?
