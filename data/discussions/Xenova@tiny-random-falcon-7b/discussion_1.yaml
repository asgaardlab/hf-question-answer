!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dmmagdal
conflicting_files: null
created_at: 2023-11-19 01:34:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663131755848-noauth.jpeg?w=200&h=200&f=face
      fullname: Diego Magdaleno
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dmmagdal
      type: user
    createdAt: '2023-11-19T01:34:36.000Z'
    data:
      edited: false
      editors:
      - dmmagdal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5019703507423401
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663131755848-noauth.jpeg?w=200&h=200&f=face
          fullname: Diego Magdaleno
          isHf: false
          isPro: false
          name: dmmagdal
          type: user
        html: '<p>I''ve gone through the optimum documentation trying to export my
          falcon-7b model to onnx with no success. </p>

          <p>Ive tried all the following methods explored here (<a href="https://huggingface.co/blog/convert-transformers-to-onnx">https://huggingface.co/blog/convert-transformers-to-onnx</a>)
          and got quite a number of errors with each of them (see below). Is there
          any more specific guidance you can give for how to export these models (I
          saw you also added support for Mistral as well)? </p>

          <p>"low-level" error message:</p>

          <pre><code>batch_size, num_heads, kv_length, head_dim = past_key_value[0][0].shape

          ValueError: not enough values to unpack (expected 4, got 0)

          </code></pre>

          <p>"mid-level" error message:</p>

          <pre><code>KeyError: "falcon is not supported yet. Only [''albert'', ''bart'',
          ''beit'', ''bert'', ''big-bird'', ''bigbird-pegasus'', ''blenderbot'', ''blenderbot-small'',
          ''bloom'', ''camembert'', ''clip'', ''codegen'', ''convbert'', ''convnext'',
          ''data2vec-text'', ''data2vec-vision'', ''deberta'', ''deberta-v2'', ''deit'',
          ''detr'', ''distilbert'', ''electra'', ''flaubert'', ''gpt2'', ''gptj'',
          ''gpt-neo'', ''groupvit'', ''ibert'', ''imagegpt'', ''layoutlm'', ''layoutlmv3'',
          ''levit'', ''longt5'', ''longformer'', ''marian'', ''mbart'', ''mobilebert'',
          ''mobilenet-v1'', ''mobilenet-v2'', ''mobilevit'', ''mt5'', ''m2m-100'',
          ''owlvit'', ''perceiver'', ''poolformer'', ''rembert'', ''resnet'', ''roberta'',
          ''roformer'', ''segformer'', ''squeezebert'', ''swin'', ''t5'', ''vision-encoder-decoder'',
          ''vit'', ''whisper'', ''xlm'', ''xlm-roberta'', ''yolos''] are supported.
          If you want to support falcon please propose a PR or open up an issue."

          </code></pre>

          <p>"high-level" error message:</p>

          <pre><code>ValueError: Trying to export a falcon model, that is a custom
          or unsupported architecture for the task text-generation-with-past, but
          no custom onnx configuration was passed as `custom_onnx_configs`. Please
          refer to https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model#custom-export-of-transformers-models
          for an example on how to export custom models. For the task text-generation-with-past,
          the Optimum ONNX exporter supports natively the architectures: [''bart'',
          ''blenderbot'', ''blenderbot_small'', ''bloom'', ''codegen'', ''gpt2'',
          ''gpt_bigcode'', ''gptj'', ''gpt_neo'', ''gpt_neox'', ''marian'', ''mbart'',
          ''mpt'', ''opt'', ''llama'', ''pegasus''].

          </code></pre>

          '
        raw: "I've gone through the optimum documentation trying to export my falcon-7b\
          \ model to onnx with no success. \r\n\r\nIve tried all the following methods\
          \ explored here (https://huggingface.co/blog/convert-transformers-to-onnx)\
          \ and got quite a number of errors with each of them (see below). Is there\
          \ any more specific guidance you can give for how to export these models\
          \ (I saw you also added support for Mistral as well)? \r\n\r\n\"low-level\"\
          \ error message:\r\n```\r\nbatch_size, num_heads, kv_length, head_dim =\
          \ past_key_value[0][0].shape\r\nValueError: not enough values to unpack\
          \ (expected 4, got 0)\r\n```\r\n\r\n\"mid-level\" error message:\r\n```\r\
          \nKeyError: \"falcon is not supported yet. Only ['albert', 'bart', 'beit',\
          \ 'bert', 'big-bird', 'bigbird-pegasus', 'blenderbot', 'blenderbot-small',\
          \ 'bloom', 'camembert', 'clip', 'codegen', 'convbert', 'convnext', 'data2vec-text',\
          \ 'data2vec-vision', 'deberta', 'deberta-v2', 'deit', 'detr', 'distilbert',\
          \ 'electra', 'flaubert', 'gpt2', 'gptj', 'gpt-neo', 'groupvit', 'ibert',\
          \ 'imagegpt', 'layoutlm', 'layoutlmv3', 'levit', 'longt5', 'longformer',\
          \ 'marian', 'mbart', 'mobilebert', 'mobilenet-v1', 'mobilenet-v2', 'mobilevit',\
          \ 'mt5', 'm2m-100', 'owlvit', 'perceiver', 'poolformer', 'rembert', 'resnet',\
          \ 'roberta', 'roformer', 'segformer', 'squeezebert', 'swin', 't5', 'vision-encoder-decoder',\
          \ 'vit', 'whisper', 'xlm', 'xlm-roberta', 'yolos'] are supported. If you\
          \ want to support falcon please propose a PR or open up an issue.\"\r\n\
          ```\r\n\r\n\"high-level\" error message:\r\n```\r\nValueError: Trying to\
          \ export a falcon model, that is a custom or unsupported architecture for\
          \ the task text-generation-with-past, but no custom onnx configuration was\
          \ passed as `custom_onnx_configs`. Please refer to https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model#custom-export-of-transformers-models\
          \ for an example on how to export custom models. For the task text-generation-with-past,\
          \ the Optimum ONNX exporter supports natively the architectures: ['bart',\
          \ 'blenderbot', 'blenderbot_small', 'bloom', 'codegen', 'gpt2', 'gpt_bigcode',\
          \ 'gptj', 'gpt_neo', 'gpt_neox', 'marian', 'mbart', 'mpt', 'opt', 'llama',\
          \ 'pegasus'].\r\n```"
        updatedAt: '2023-11-19T01:34:36.477Z'
      numEdits: 0
      reactions: []
    id: 6559662c16de0c93ae966c68
    type: comment
  author: dmmagdal
  content: "I've gone through the optimum documentation trying to export my falcon-7b\
    \ model to onnx with no success. \r\n\r\nIve tried all the following methods explored\
    \ here (https://huggingface.co/blog/convert-transformers-to-onnx) and got quite\
    \ a number of errors with each of them (see below). Is there any more specific\
    \ guidance you can give for how to export these models (I saw you also added support\
    \ for Mistral as well)? \r\n\r\n\"low-level\" error message:\r\n```\r\nbatch_size,\
    \ num_heads, kv_length, head_dim = past_key_value[0][0].shape\r\nValueError: not\
    \ enough values to unpack (expected 4, got 0)\r\n```\r\n\r\n\"mid-level\" error\
    \ message:\r\n```\r\nKeyError: \"falcon is not supported yet. Only ['albert',\
    \ 'bart', 'beit', 'bert', 'big-bird', 'bigbird-pegasus', 'blenderbot', 'blenderbot-small',\
    \ 'bloom', 'camembert', 'clip', 'codegen', 'convbert', 'convnext', 'data2vec-text',\
    \ 'data2vec-vision', 'deberta', 'deberta-v2', 'deit', 'detr', 'distilbert', 'electra',\
    \ 'flaubert', 'gpt2', 'gptj', 'gpt-neo', 'groupvit', 'ibert', 'imagegpt', 'layoutlm',\
    \ 'layoutlmv3', 'levit', 'longt5', 'longformer', 'marian', 'mbart', 'mobilebert',\
    \ 'mobilenet-v1', 'mobilenet-v2', 'mobilevit', 'mt5', 'm2m-100', 'owlvit', 'perceiver',\
    \ 'poolformer', 'rembert', 'resnet', 'roberta', 'roformer', 'segformer', 'squeezebert',\
    \ 'swin', 't5', 'vision-encoder-decoder', 'vit', 'whisper', 'xlm', 'xlm-roberta',\
    \ 'yolos'] are supported. If you want to support falcon please propose a PR or\
    \ open up an issue.\"\r\n```\r\n\r\n\"high-level\" error message:\r\n```\r\nValueError:\
    \ Trying to export a falcon model, that is a custom or unsupported architecture\
    \ for the task text-generation-with-past, but no custom onnx configuration was\
    \ passed as `custom_onnx_configs`. Please refer to https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model#custom-export-of-transformers-models\
    \ for an example on how to export custom models. For the task text-generation-with-past,\
    \ the Optimum ONNX exporter supports natively the architectures: ['bart', 'blenderbot',\
    \ 'blenderbot_small', 'bloom', 'codegen', 'gpt2', 'gpt_bigcode', 'gptj', 'gpt_neo',\
    \ 'gpt_neox', 'marian', 'mbart', 'mpt', 'opt', 'llama', 'pegasus'].\r\n```"
  created_at: 2023-11-19 01:34:36+00:00
  edited: false
  hidden: false
  id: 6559662c16de0c93ae966c68
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Xenova/tiny-random-falcon-7b
repo_type: model
status: open
target_branch: null
title: Not entirely clear on the process for exporting/converting falcon model to
  ONNX
