!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kp1234
conflicting_files: null
created_at: 2023-08-19 07:43:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/581ff38572e3d0652e16a8956725e3f8.svg
      fullname: KP
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kp1234
      type: user
    createdAt: '2023-08-19T08:43:35.000Z'
    data:
      edited: false
      editors:
      - kp1234
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7987992763519287
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/581ff38572e3d0652e16a8956725e3f8.svg
          fullname: KP
          isHf: false
          isPro: false
          name: kp1234
          type: user
        html: '<p>I am trying to obtain the alignment positions of the source to the
          target words to place xml tags in the translations.<br>Therefore I wanted
          to gain the attention_scores but when using model.generate(**encoded_hi,
          forced_bos_token_id=tokenizer.lang_code_to_id["de_DE"], output_attentions=True,
          output_hidden_states=True)<br>print(generated_tokens) only the generated
          tokes and no attentions are returned. </p>

          <p>Does someone have an idea how this could be archived?<br>Best regards,
          Kai</p>

          '
        raw: "I am trying to obtain the alignment positions of the source to the target\
          \ words to place xml tags in the translations.\r\nTherefore I wanted to\
          \ gain the attention_scores but when using model.generate(**encoded_hi,\
          \ forced_bos_token_id=tokenizer.lang_code_to_id[\"de_DE\"], output_attentions=True,\
          \ output_hidden_states=True)\r\nprint(generated_tokens) only the generated\
          \ tokes and no attentions are returned. \r\n\r\nDoes someone have an idea\
          \ how this could be archived?\r\nBest regards, Kai"
        updatedAt: '2023-08-19T08:43:35.749Z'
      numEdits: 0
      reactions: []
    id: 64e080b7b5cc372803c0a332
    type: comment
  author: kp1234
  content: "I am trying to obtain the alignment positions of the source to the target\
    \ words to place xml tags in the translations.\r\nTherefore I wanted to gain the\
    \ attention_scores but when using model.generate(**encoded_hi, forced_bos_token_id=tokenizer.lang_code_to_id[\"\
    de_DE\"], output_attentions=True, output_hidden_states=True)\r\nprint(generated_tokens)\
    \ only the generated tokes and no attentions are returned. \r\n\r\nDoes someone\
    \ have an idea how this could be archived?\r\nBest regards, Kai"
  created_at: 2023-08-19 07:43:35+00:00
  edited: false
  hidden: false
  id: 64e080b7b5cc372803c0a332
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661470101034-605a1630f70acedea1f74abc.png?w=200&h=200&f=face
      fullname: David Dale
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: cointegrated
      type: user
    createdAt: '2023-09-12T12:21:52.000Z'
    data:
      edited: false
      editors:
      - cointegrated
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6690849661827087
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661470101034-605a1630f70acedea1f74abc.png?w=200&h=200&f=face
          fullname: David Dale
          isHf: false
          isPro: false
          name: cointegrated
          type: user
        html: "<p>When running <code>generate</code>, target tokens are added one\
          \ at a time, and attention from the previous tokens are not preserved. </p>\n\
          <p>Thus, to obtain the full cross-attention map, you'll have to run inference\
          \ one more with the full generated translation, like</p>\n<pre><code class=\"\
          language-Python\"><span class=\"hljs-keyword\">import</span> torch\n<span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> MBartForConditionalGeneration, MBart50TokenizerFast\n\nmodel\
          \ = MBartForConditionalGeneration.from_pretrained(<span class=\"hljs-string\"\
          >\"facebook/mbart-large-50-many-to-many-mmt\"</span>)\ntokenizer = MBart50TokenizerFast.from_pretrained(<span\
          \ class=\"hljs-string\">\"facebook/mbart-large-50-many-to-many-mmt\"</span>)\n\
          \narticle_hi = <span class=\"hljs-string\">\"\u0938\u0902\u092F\u0941\u0915\
          \u094D\u0924 \u0930\u093E\u0937\u094D\u091F\u094D\u0930 \u0915\u0947 \u092A\
          \u094D\u0930\u092E\u0941\u0916 \u0915\u093E \u0915\u0939\u0928\u093E \u0939\
          \u0948 \u0915\u093F \u0938\u0940\u0930\u093F\u092F\u093E \u092E\u0947\u0902\
          \ \u0915\u094B\u0908 \u0938\u0948\u0928\u094D\u092F \u0938\u092E\u093E\u0927\
          \u093E\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\"</span>\ntokenizer.src_lang\
          \ = <span class=\"hljs-string\">\"hi_IN\"</span>\nencoded_hi = tokenizer(article_hi,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\ngenerated_tokens\
          \ = model.generate(\n    **encoded_hi,\n    forced_bos_token_id=tokenizer.lang_code_to_id[<span\
          \ class=\"hljs-string\">\"fr_XX\"</span>]\n)\ntranslated = tokenizer.batch_decode(generated_tokens,\
          \ skip_special_tokens=<span class=\"hljs-literal\">True</span>)[<span class=\"\
          hljs-number\">0</span>]\n<span class=\"hljs-built_in\">print</span>(translated)\
          \  <span class=\"hljs-comment\"># Le chef de l 'ONU affirme qu 'il n 'y\
          \ a pas de solution militaire en Syria.</span>\n\ntokenizer.src_lang = <span\
          \ class=\"hljs-string\">\"fr_XX\"</span>\nencoded_fr = tokenizer(translated,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\n<span class=\"\
          hljs-keyword\">with</span> torch.inference_mode():\n    result = model(**encoded_hi,\
          \ decoder_input_ids=encoded_fr.input_ids, output_attentions=<span class=\"\
          hljs-literal\">True</span>,)\n\nall_cross_attentions = torch.cat(result.cross_attentions)\n\
          <span class=\"hljs-built_in\">print</span>(all_cross_attentions.shape) \
          \ <span class=\"hljs-comment\"># torch.Size([12, 16, 23, 18])</span>\n<span\
          \ class=\"hljs-comment\"># 12 layers * 16 heads * 23 input tokens * 18 output\
          \ tokens</span>\n</code></pre>\n<p>Now you can average the <code>all_cross_attentions</code>\
          \ across the first two dimensions, or pick the best layer and attention\
          \ head to compute the alignment. </p>\n<p>However, this alignment is not\
          \ guaranteed to be meaningful, because (1) sometimes token attributions\
          \ are indirect, and (2) sometimes neural networks compensate high attention\
          \ weights with low attention values, or vice versa. So if you are not satisfied\
          \ with attention alignment, consider taking a look at its extended version,\
          \ <a rel=\"nofollow\" href=\"https://github.com/mt-upc/transformer-contributions-nmt\"\
          >ALTI+</a> by Ferrando et al, 2022.</p>\n"
        raw: "When running `generate`, target tokens are added one at a time, and\
          \ attention from the previous tokens are not preserved. \n\nThus, to obtain\
          \ the full cross-attention map, you'll have to run inference one more with\
          \ the full generated translation, like\n\n```Python\nimport torch\nfrom\
          \ transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\
          \nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\"\
          )\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\"\
          )\n\narticle_hi = \"\u0938\u0902\u092F\u0941\u0915\u094D\u0924 \u0930\u093E\
          \u0937\u094D\u091F\u094D\u0930 \u0915\u0947 \u092A\u094D\u0930\u092E\u0941\
          \u0916 \u0915\u093E \u0915\u0939\u0928\u093E \u0939\u0948 \u0915\u093F \u0938\
          \u0940\u0930\u093F\u092F\u093E \u092E\u0947\u0902 \u0915\u094B\u0908 \u0938\
          \u0948\u0928\u094D\u092F \u0938\u092E\u093E\u0927\u093E\u0928 \u0928\u0939\
          \u0940\u0902 \u0939\u0948\"\ntokenizer.src_lang = \"hi_IN\"\nencoded_hi\
          \ = tokenizer(article_hi, return_tensors=\"pt\")\ngenerated_tokens = model.generate(\n\
          \    **encoded_hi,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"\
          fr_XX\"]\n)\ntranslated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n\
          print(translated)  # Le chef de l 'ONU affirme qu 'il n 'y a pas de solution\
          \ militaire en Syria.\n\ntokenizer.src_lang = \"fr_XX\"\nencoded_fr = tokenizer(translated,\
          \ return_tensors=\"pt\")\nwith torch.inference_mode():\n    result = model(**encoded_hi,\
          \ decoder_input_ids=encoded_fr.input_ids, output_attentions=True,)\n\nall_cross_attentions\
          \ = torch.cat(result.cross_attentions)\nprint(all_cross_attentions.shape)\
          \  # torch.Size([12, 16, 23, 18])\n# 12 layers * 16 heads * 23 input tokens\
          \ * 18 output tokens\n```\nNow you can average the `all_cross_attentions`\
          \ across the first two dimensions, or pick the best layer and attention\
          \ head to compute the alignment. \n\nHowever, this alignment is not guaranteed\
          \ to be meaningful, because (1) sometimes token attributions are indirect,\
          \ and (2) sometimes neural networks compensate high attention weights with\
          \ low attention values, or vice versa. So if you are not satisfied with\
          \ attention alignment, consider taking a look at its extended version, [ALTI+](https://github.com/mt-upc/transformer-contributions-nmt)\
          \ by Ferrando et al, 2022."
        updatedAt: '2023-09-12T12:21:52.586Z'
      numEdits: 0
      reactions: []
    id: 650057e03587d3ebfd938c23
    type: comment
  author: cointegrated
  content: "When running `generate`, target tokens are added one at a time, and attention\
    \ from the previous tokens are not preserved. \n\nThus, to obtain the full cross-attention\
    \ map, you'll have to run inference one more with the full generated translation,\
    \ like\n\n```Python\nimport torch\nfrom transformers import MBartForConditionalGeneration,\
    \ MBart50TokenizerFast\n\nmodel = MBartForConditionalGeneration.from_pretrained(\"\
    facebook/mbart-large-50-many-to-many-mmt\")\ntokenizer = MBart50TokenizerFast.from_pretrained(\"\
    facebook/mbart-large-50-many-to-many-mmt\")\n\narticle_hi = \"\u0938\u0902\u092F\
    \u0941\u0915\u094D\u0924 \u0930\u093E\u0937\u094D\u091F\u094D\u0930 \u0915\u0947\
    \ \u092A\u094D\u0930\u092E\u0941\u0916 \u0915\u093E \u0915\u0939\u0928\u093E \u0939\
    \u0948 \u0915\u093F \u0938\u0940\u0930\u093F\u092F\u093E \u092E\u0947\u0902 \u0915\
    \u094B\u0908 \u0938\u0948\u0928\u094D\u092F \u0938\u092E\u093E\u0927\u093E\u0928\
    \ \u0928\u0939\u0940\u0902 \u0939\u0948\"\ntokenizer.src_lang = \"hi_IN\"\nencoded_hi\
    \ = tokenizer(article_hi, return_tensors=\"pt\")\ngenerated_tokens = model.generate(\n\
    \    **encoded_hi,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"\
    ]\n)\ntranslated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n\
    print(translated)  # Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire\
    \ en Syria.\n\ntokenizer.src_lang = \"fr_XX\"\nencoded_fr = tokenizer(translated,\
    \ return_tensors=\"pt\")\nwith torch.inference_mode():\n    result = model(**encoded_hi,\
    \ decoder_input_ids=encoded_fr.input_ids, output_attentions=True,)\n\nall_cross_attentions\
    \ = torch.cat(result.cross_attentions)\nprint(all_cross_attentions.shape)  # torch.Size([12,\
    \ 16, 23, 18])\n# 12 layers * 16 heads * 23 input tokens * 18 output tokens\n\
    ```\nNow you can average the `all_cross_attentions` across the first two dimensions,\
    \ or pick the best layer and attention head to compute the alignment. \n\nHowever,\
    \ this alignment is not guaranteed to be meaningful, because (1) sometimes token\
    \ attributions are indirect, and (2) sometimes neural networks compensate high\
    \ attention weights with low attention values, or vice versa. So if you are not\
    \ satisfied with attention alignment, consider taking a look at its extended version,\
    \ [ALTI+](https://github.com/mt-upc/transformer-contributions-nmt) by Ferrando\
    \ et al, 2022."
  created_at: 2023-09-12 11:21:52+00:00
  edited: false
  hidden: false
  id: 650057e03587d3ebfd938c23
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: facebook/mbart-large-50-many-to-many-mmt
repo_type: model
status: open
target_branch: null
title: Obtain alignment information
