!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cheepoin
conflicting_files: null
created_at: 2023-03-17 05:42:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/82d241b974bd1871fd348c9b9a1f8900.svg
      fullname: cheepoin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cheepoin
      type: user
    createdAt: '2023-03-17T06:42:03.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/82d241b974bd1871fd348c9b9a1f8900.svg
          fullname: cheepoin
          isHf: false
          isPro: false
          name: cheepoin
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-03-31T13:00:46.007Z'
      numEdits: 0
      reactions: []
    id: 64140bbb996b2e426f23fa88
    type: comment
  author: cheepoin
  content: This comment has been hidden
  created_at: 2023-03-17 05:42:03+00:00
  edited: true
  hidden: true
  id: 64140bbb996b2e426f23fa88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/82d241b974bd1871fd348c9b9a1f8900.svg
      fullname: cheepoin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cheepoin
      type: user
    createdAt: '2023-03-31T13:00:31.000Z'
    data:
      status: closed
    id: 6426d96f214b6d74b393d78d
    type: status-change
  author: cheepoin
  created_at: 2023-03-31 12:00:31+00:00
  id: 6426d96f214b6d74b393d78d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/82d241b974bd1871fd348c9b9a1f8900.svg
      fullname: cheepoin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cheepoin
      type: user
    createdAt: '2023-03-31T13:00:53.000Z'
    data:
      from: how to train this model?
      to: test
    id: 6426d98588b35f30f59869cd
    type: title-change
  author: cheepoin
  created_at: 2023-03-31 12:00:53+00:00
  id: 6426d98588b35f30f59869cd
  new_title: test
  old_title: how to train this model?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/82d241b974bd1871fd348c9b9a1f8900.svg
      fullname: cheepoin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cheepoin
      type: user
    createdAt: '2023-03-31T13:01:38.000Z'
    data:
      from: test
      to: How to quantify and accelerate this model
    id: 6426d9b288b35f30f5986b1b
    type: title-change
  author: cheepoin
  created_at: 2023-03-31 12:01:38+00:00
  id: 6426d9b288b35f30f5986b1b
  new_title: How to quantify and accelerate this model
  old_title: test
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/82d241b974bd1871fd348c9b9a1f8900.svg
      fullname: cheepoin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cheepoin
      type: user
    createdAt: '2023-03-31T13:02:56.000Z'
    data:
      edited: false
      editors:
      - cheepoin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/82d241b974bd1871fd348c9b9a1f8900.svg
          fullname: cheepoin
          isHf: false
          isPro: false
          name: cheepoin
          type: user
        html: '<p>I tried faster-transformer and failed on that. Any ideas?</p>

          '
        raw: I tried faster-transformer and failed on that. Any ideas?
        updatedAt: '2023-03-31T13:02:56.783Z'
      numEdits: 0
      reactions: []
    id: 6426da0069b9dc3ac0bf30fe
    type: comment
  author: cheepoin
  content: I tried faster-transformer and failed on that. Any ideas?
  created_at: 2023-03-31 12:02:56+00:00
  edited: false
  hidden: false
  id: 6426da0069b9dc3ac0bf30fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/82d241b974bd1871fd348c9b9a1f8900.svg
      fullname: cheepoin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cheepoin
      type: user
    createdAt: '2023-03-31T13:03:37.000Z'
    data:
      status: open
    id: 6426da290ac1e7075cc94d80
    type: status-change
  author: cheepoin
  created_at: 2023-03-31 12:03:37+00:00
  id: 6426da290ac1e7075cc94d80
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/pTE_XXI9e06MS4sIfJfq5.jpeg?w=200&h=200&f=face
      fullname: Andrey Moskvin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gembird
      type: user
    createdAt: '2023-10-13T20:33:04.000Z'
    data:
      edited: false
      editors:
      - gembird
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8715136647224426
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/pTE_XXI9e06MS4sIfJfq5.jpeg?w=200&h=200&f=face
          fullname: Andrey Moskvin
          isHf: false
          isPro: false
          name: gembird
          type: user
        html: '<p>year, any ideas to accelerate the model? Translation in google colab
          (CPU and GPU) are extremely slow. Longer than I can translate manually.</p>

          '
        raw: year, any ideas to accelerate the model? Translation in google colab
          (CPU and GPU) are extremely slow. Longer than I can translate manually.
        updatedAt: '2023-10-13T20:33:04.984Z'
      numEdits: 0
      reactions: []
    id: 6529a9803b5997ed716d0708
    type: comment
  author: gembird
  content: year, any ideas to accelerate the model? Translation in google colab (CPU
    and GPU) are extremely slow. Longer than I can translate manually.
  created_at: 2023-10-13 19:33:04+00:00
  edited: false
  hidden: false
  id: 6529a9803b5997ed716d0708
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-16T18:57:40.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: tr
        probability: 0.10496710985898972
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'cc @ybelkada '
        updatedAt: '2023-10-16T18:57:40.927Z'
      numEdits: 0
      reactions: []
    id: 652d87a4442fb6963b74cd0f
    type: comment
  author: lysandre
  content: 'cc @ybelkada '
  created_at: 2023-10-16 17:57:40+00:00
  edited: false
  hidden: false
  id: 652d87a4442fb6963b74cd0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-16T19:40:05.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5881588459014893
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;gembird&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gembird\">@<span class=\"\
          underline\">gembird</span></a></span>\n\n\t</span></span> </p>\n<p>Currently\
          \ the only way to accelerate inference on CPU &amp; GPU is to use the <code>BetterTransformer</code>\
          \ API for the encoder part of MBart, I believe this will not accelerate\
          \ much the translation as most of the bottleneck happens on the decoder\
          \ side I believe. </p>\n<p>On GPU, in case you are running your translations\
          \ with batch_size=1 you can try your hands with quantization and fast kernel\
          \ from <code>bitsandbytes</code>, by making sure you load your model with\
          \ <code>bnb_4bit_compute_dtype=torch.float16</code></p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> torch\n<span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> MBartForConditionalGeneration, MBart50TokenizerFast, BitsAndBytesConfig\n\
          \nquantization_config = BitsAndBytesConfig(load_in_4bit=<span class=\"hljs-literal\"\
          >True</span>, bnb_4bit_compute_dtype=torch.float16)\n\narticle_hi = <span\
          \ class=\"hljs-string\">\"\u0938\u0902\u092F\u0941\u0915\u094D\u0924 \u0930\
          \u093E\u0937\u094D\u091F\u094D\u0930 \u0915\u0947 \u092A\u094D\u0930\u092E\
          \u0941\u0916 \u0915\u093E \u0915\u0939\u0928\u093E \u0939\u0948 \u0915\u093F\
          \ \u0938\u0940\u0930\u093F\u092F\u093E \u092E\u0947\u0902 \u0915\u094B\u0908\
          \ \u0938\u0948\u0928\u094D\u092F \u0938\u092E\u093E\u0927\u093E\u0928 \u0928\
          \u0939\u0940\u0902 \u0939\u0948\"</span>\narticle_ar = <span class=\"hljs-string\"\
          >\"\u0627\u0644\u0623\u0645\u064A\u0646 \u0627\u0644\u0639\u0627\u0645 \u0644\
          \u0644\u0623\u0645\u0645 \u0627\u0644\u0645\u062A\u062D\u062F\u0629 \u064A\
          \u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\u0627 \u064A\u0648\u062C\u062F\
          \ \u062D\u0644 \u0639\u0633\u0643\u0631\u064A \u0641\u064A \u0633\u0648\u0631\
          \u064A\u0627.\"</span>\n\nmodel = MBartForConditionalGeneration.from_pretrained(<span\
          \ class=\"hljs-string\">\"facebook/mbart-large-50-many-to-many-mmt\"</span>,\
          \ quantization_config=quantization_config)\ntokenizer = MBart50TokenizerFast.from_pretrained(<span\
          \ class=\"hljs-string\">\"facebook/mbart-large-50-many-to-many-mmt\"</span>)\n\
          \n<span class=\"hljs-comment\"># translate Hindi to French</span>\ntokenizer.src_lang\
          \ = <span class=\"hljs-string\">\"hi_IN\"</span>\nencoded_hi = tokenizer(article_hi,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\ngenerated_tokens\
          \ = model.generate(\n    **encoded_hi,\n    forced_bos_token_id=tokenizer.lang_code_to_id[<span\
          \ class=\"hljs-string\">\"fr_XX\"</span>]\n)\ntokenizer.batch_decode(generated_tokens,\
          \ skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n<span class=\"\
          hljs-comment\"># =&gt; \"Le chef de l 'ONU affirme qu 'il n 'y a pas de\
          \ solution militaire dans la Syrie.\"</span>\n\n<span class=\"hljs-comment\"\
          ># translate Arabic to English</span>\ntokenizer.src_lang = <span class=\"\
          hljs-string\">\"ar_AR\"</span>\nencoded_ar = tokenizer(article_ar, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>)\ngenerated_tokens = model.generate(\n\
          \    **encoded_ar,\n    forced_bos_token_id=tokenizer.lang_code_to_id[<span\
          \ class=\"hljs-string\">\"en_XX\"</span>]\n)\ntokenizer.batch_decode(generated_tokens,\
          \ skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n</code></pre>\n\
          <p>More generally we are currently migrating attention layers to use <code>torch.scaled_dot_product_attention</code>\
          \ in transformers core, which should lead to much faster inference. Please\
          \ have a look at <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/pull/26572\"\
          >https://github.com/huggingface/transformers/pull/26572</a> for further\
          \ details and make sure you can test that feature directly once the support\
          \ is going to be added on most architecutres, including MBart</p>\n"
        raw: "Hi @gembird \n\nCurrently the only way to accelerate inference on CPU\
          \ & GPU is to use the `BetterTransformer` API for the encoder part of MBart,\
          \ I believe this will not accelerate much the translation as most of the\
          \ bottleneck happens on the decoder side I believe. \n\nOn GPU, in case\
          \ you are running your translations with batch_size=1 you can try your hands\
          \ with quantization and fast kernel from `bitsandbytes`, by making sure\
          \ you load your model with `bnb_4bit_compute_dtype=torch.float16`\n\n```python\n\
          import torch\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast,\
          \ BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True,\
          \ bnb_4bit_compute_dtype=torch.float16)\n\narticle_hi = \"\u0938\u0902\u092F\
          \u0941\u0915\u094D\u0924 \u0930\u093E\u0937\u094D\u091F\u094D\u0930 \u0915\
          \u0947 \u092A\u094D\u0930\u092E\u0941\u0916 \u0915\u093E \u0915\u0939\u0928\
          \u093E \u0939\u0948 \u0915\u093F \u0938\u0940\u0930\u093F\u092F\u093E \u092E\
          \u0947\u0902 \u0915\u094B\u0908 \u0938\u0948\u0928\u094D\u092F \u0938\u092E\
          \u093E\u0927\u093E\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\"\narticle_ar\
          \ = \"\u0627\u0644\u0623\u0645\u064A\u0646 \u0627\u0644\u0639\u0627\u0645\
          \ \u0644\u0644\u0623\u0645\u0645 \u0627\u0644\u0645\u062A\u062D\u062F\u0629\
          \ \u064A\u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\u0627 \u064A\u0648\u062C\
          \u062F \u062D\u0644 \u0639\u0633\u0643\u0631\u064A \u0641\u064A \u0633\u0648\
          \u0631\u064A\u0627.\"\n\nmodel = MBartForConditionalGeneration.from_pretrained(\"\
          facebook/mbart-large-50-many-to-many-mmt\", quantization_config=quantization_config)\n\
          tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\"\
          )\n\n# translate Hindi to French\ntokenizer.src_lang = \"hi_IN\"\nencoded_hi\
          \ = tokenizer(article_hi, return_tensors=\"pt\")\ngenerated_tokens = model.generate(\n\
          \    **encoded_hi,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"\
          fr_XX\"]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\
          # => \"Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire\
          \ dans la Syrie.\"\n\n# translate Arabic to English\ntokenizer.src_lang\
          \ = \"ar_AR\"\nencoded_ar = tokenizer(article_ar, return_tensors=\"pt\"\
          )\ngenerated_tokens = model.generate(\n    **encoded_ar,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"\
          en_XX\"]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\
          ```\n\nMore generally we are currently migrating attention layers to use\
          \ `torch.scaled_dot_product_attention` in transformers core, which should\
          \ lead to much faster inference. Please have a look at https://github.com/huggingface/transformers/pull/26572\
          \ for further details and make sure you can test that feature directly once\
          \ the support is going to be added on most architecutres, including MBart"
        updatedAt: '2023-10-16T19:40:05.370Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - gembird
    id: 652d9195da9c60b50ddd5b3b
    type: comment
  author: ybelkada
  content: "Hi @gembird \n\nCurrently the only way to accelerate inference on CPU\
    \ & GPU is to use the `BetterTransformer` API for the encoder part of MBart, I\
    \ believe this will not accelerate much the translation as most of the bottleneck\
    \ happens on the decoder side I believe. \n\nOn GPU, in case you are running your\
    \ translations with batch_size=1 you can try your hands with quantization and\
    \ fast kernel from `bitsandbytes`, by making sure you load your model with `bnb_4bit_compute_dtype=torch.float16`\n\
    \n```python\nimport torch\nfrom transformers import MBartForConditionalGeneration,\
    \ MBart50TokenizerFast, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True,\
    \ bnb_4bit_compute_dtype=torch.float16)\n\narticle_hi = \"\u0938\u0902\u092F\u0941\
    \u0915\u094D\u0924 \u0930\u093E\u0937\u094D\u091F\u094D\u0930 \u0915\u0947 \u092A\
    \u094D\u0930\u092E\u0941\u0916 \u0915\u093E \u0915\u0939\u0928\u093E \u0939\u0948\
    \ \u0915\u093F \u0938\u0940\u0930\u093F\u092F\u093E \u092E\u0947\u0902 \u0915\u094B\
    \u0908 \u0938\u0948\u0928\u094D\u092F \u0938\u092E\u093E\u0927\u093E\u0928 \u0928\
    \u0939\u0940\u0902 \u0939\u0948\"\narticle_ar = \"\u0627\u0644\u0623\u0645\u064A\
    \u0646 \u0627\u0644\u0639\u0627\u0645 \u0644\u0644\u0623\u0645\u0645 \u0627\u0644\
    \u0645\u062A\u062D\u062F\u0629 \u064A\u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\
    \u0627 \u064A\u0648\u062C\u062F \u062D\u0644 \u0639\u0633\u0643\u0631\u064A \u0641\
    \u064A \u0633\u0648\u0631\u064A\u0627.\"\n\nmodel = MBartForConditionalGeneration.from_pretrained(\"\
    facebook/mbart-large-50-many-to-many-mmt\", quantization_config=quantization_config)\n\
    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\"\
    )\n\n# translate Hindi to French\ntokenizer.src_lang = \"hi_IN\"\nencoded_hi =\
    \ tokenizer(article_hi, return_tensors=\"pt\")\ngenerated_tokens = model.generate(\n\
    \    **encoded_hi,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"\
    ]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# =>\
    \ \"Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire dans la\
    \ Syrie.\"\n\n# translate Arabic to English\ntokenizer.src_lang = \"ar_AR\"\n\
    encoded_ar = tokenizer(article_ar, return_tensors=\"pt\")\ngenerated_tokens =\
    \ model.generate(\n    **encoded_ar,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"\
    en_XX\"]\n)\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\
    ```\n\nMore generally we are currently migrating attention layers to use `torch.scaled_dot_product_attention`\
    \ in transformers core, which should lead to much faster inference. Please have\
    \ a look at https://github.com/huggingface/transformers/pull/26572 for further\
    \ details and make sure you can test that feature directly once the support is\
    \ going to be added on most architecutres, including MBart"
  created_at: 2023-10-16 18:40:05+00:00
  edited: false
  hidden: false
  id: 652d9195da9c60b50ddd5b3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664819946227-noauth.png?w=200&h=200&f=face
      fullname: Robert Campbell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rcampbell
      type: user
    createdAt: '2023-10-22T20:46:21.000Z'
    data:
      edited: false
      editors:
      - rcampbell
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8956590890884399
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664819946227-noauth.png?w=200&h=200&f=face
          fullname: Robert Campbell
          isHf: false
          isPro: false
          name: rcampbell
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>,\
          \ I gave that quantization example you provided a shot and I'm getting a\
          \ weird result. GPU inference without quantization works fine but when I\
          \ add the quantization config I'm now getting something like ['okay', 'okay']\
          \ when I run inference with a sample sentence. Seems to be just random tokens\
          \ so I'm wondering if there's an issue with the quantization configuration.</p>\n"
        raw: Hi @ybelkada, I gave that quantization example you provided a shot and
          I'm getting a weird result. GPU inference without quantization works fine
          but when I add the quantization config I'm now getting something like ['okay',
          'okay'] when I run inference with a sample sentence. Seems to be just random
          tokens so I'm wondering if there's an issue with the quantization configuration.
        updatedAt: '2023-10-22T20:46:21.311Z'
      numEdits: 0
      reactions: []
    id: 65358a1d24173495ad3a0d17
    type: comment
  author: rcampbell
  content: Hi @ybelkada, I gave that quantization example you provided a shot and
    I'm getting a weird result. GPU inference without quantization works fine but
    when I add the quantization config I'm now getting something like ['okay', 'okay']
    when I run inference with a sample sentence. Seems to be just random tokens so
    I'm wondering if there's an issue with the quantization configuration.
  created_at: 2023-10-22 19:46:21+00:00
  edited: false
  hidden: false
  id: 65358a1d24173495ad3a0d17
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: facebook/mbart-large-50-many-to-many-mmt
repo_type: model
status: open
target_branch: null
title: How to quantify and accelerate this model
