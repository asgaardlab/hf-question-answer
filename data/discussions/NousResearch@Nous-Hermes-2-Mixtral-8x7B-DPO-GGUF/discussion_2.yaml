!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheYuriLover
conflicting_files: null
created_at: 2024-01-18 05:01:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2024-01-18T05:01:42.000Z'
    data:
      edited: true
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.898496687412262
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>Source : <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/discussions/5006">https://github.com/ggerganov/llama.cpp/discussions/5006</a><br>The
          problem we have when using a calibration dataset is the overfitting to a
          certain style and then in consequence, make the model worse on other aspects.<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/FnMHvo0nKXyWVPc-iDaR1.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/FnMHvo0nKXyWVPc-iDaR1.png"></a><br>Supposedly,
          the suggestion to fix this is to use a calibration dataset composed of random
          tokens instead.</p>

          '
        raw: 'Source : https://github.com/ggerganov/llama.cpp/discussions/5006

          The problem we have when using a calibration dataset is the overfitting
          to a certain style and then in consequence, make the model worse on other
          aspects.

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/FnMHvo0nKXyWVPc-iDaR1.png)

          Supposedly, the suggestion to fix this is to use a calibration dataset composed
          of random tokens instead.'
        updatedAt: '2024-01-18T05:57:45.871Z'
      numEdits: 2
      reactions: []
    id: 65a8b0b6b33c64c60eb74463
    type: comment
  author: TheYuriLover
  content: 'Source : https://github.com/ggerganov/llama.cpp/discussions/5006

    The problem we have when using a calibration dataset is the overfitting to a certain
    style and then in consequence, make the model worse on other aspects.

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/FnMHvo0nKXyWVPc-iDaR1.png)

    Supposedly, the suggestion to fix this is to use a calibration dataset composed
    of random tokens instead.'
  created_at: 2024-01-18 05:01:42+00:00
  edited: true
  hidden: false
  id: 65a8b0b6b33c64c60eb74463
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2024-01-18T05:02:59.000Z'
    data:
      from: Using the new gguf quant method may result in a woese overall performance
        than that of the old gguf quants.
      to: Using the new gguf quant method may result in a worse overall performance
        than that of the old gguf quants.
    id: 65a8b10326598b99553e21cc
    type: title-change
  author: TheYuriLover
  created_at: 2024-01-18 05:02:59+00:00
  id: 65a8b10326598b99553e21cc
  new_title: Using the new gguf quant method may result in a worse overall performance
    than that of the old gguf quants.
  old_title: Using the new gguf quant method may result in a woese overall performance
    than that of the old gguf quants.
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2024-01-18T09:08:21.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7654237151145935
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>Thank you, we reverted to old llama cpp and it fixed it afaik</p>

          '
        raw: Thank you, we reverted to old llama cpp and it fixed it afaik
        updatedAt: '2024-01-18T09:08:21.668Z'
      numEdits: 0
      reactions: []
    id: 65a8ea8591ec5d1ec6000270
    type: comment
  author: teknium
  content: Thank you, we reverted to old llama cpp and it fixed it afaik
  created_at: 2024-01-18 09:08:21+00:00
  edited: false
  hidden: false
  id: 65a8ea8591ec5d1ec6000270
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO-GGUF
repo_type: model
status: open
target_branch: null
title: Using the new gguf quant method may result in a worse overall performance than
  that of the old gguf quants.
