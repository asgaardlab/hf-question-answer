!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lazyDataScientist
conflicting_files: null
created_at: 2023-12-15 23:10:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a39ec8f27255b6b571101/7J_BcRm7ua0WZNIGwEzlo.png?w=200&h=200&f=face
      fullname: Cedrick Hesketh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lazyDataScientist
      type: user
    createdAt: '2023-12-15T23:10:15.000Z'
    data:
      edited: false
      editors:
      - lazyDataScientist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5822597742080688
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a39ec8f27255b6b571101/7J_BcRm7ua0WZNIGwEzlo.png?w=200&h=200&f=face
          fullname: Cedrick Hesketh
          isHf: false
          isPro: false
          name: lazyDataScientist
          type: user
        html: '<p>Running into an issue while using Runpod with a A100. After downloading
          the model I get this error message for all versions of the model (both Qn_0
          and Qn_k).<br>You mentioned that you got it working on a single A100, did
          you need to do any extra steps to get the text-generation-webui working
          with Mixtral models?</p>

          <pre><code>Traceback (most recent call last):


          File "/workspace/text-generation-webui/modules/ui_model_menu.py", line 209,
          in load_model_wrapper



          shared.model, shared.tokenizer = load_model(selected_model, loader)

          File "/workspace/text-generation-webui/modules/models.py", line 89, in load_model



          output = load_func_map[loader](model_name)

          File "/workspace/text-generation-webui/modules/models.py", line 259, in
          llamacpp_loader



          model, tokenizer = LlamaCppModel.from_pretrained(model_file)

          File "/workspace/text-generation-webui/modules/llamacpp_model.py", line
          91, in from_pretrained



          result.model = Llama(**params)

          File "/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/llama.py",
          line 923, in init



          self._n_vocab = self.n_vocab()

          File "/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/llama.py",
          line 2184, in n_vocab



          return self._model.n_vocab()

          File "/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/llama.py",
          line 250, in n_vocab



          assert self.model is not None

          AssertionError

          </code></pre>

          '
        raw: "Running into an issue while using Runpod with a A100. After downloading\
          \ the model I get this error message for all versions of the model (both\
          \ Qn_0 and Qn_k).\r\nYou mentioned that you got it working on a single A100,\
          \ did you need to do any extra steps to get the text-generation-webui working\
          \ with Mixtral models?\r\n\r\n```\r\nTraceback (most recent call last):\r\
          \n\r\nFile \"/workspace/text-generation-webui/modules/ui_model_menu.py\"\
          , line 209, in load_model_wrapper\r\n\r\n\r\nshared.model, shared.tokenizer\
          \ = load_model(selected_model, loader)\r\nFile \"/workspace/text-generation-webui/modules/models.py\"\
          , line 89, in load_model\r\n\r\n\r\noutput = load_func_map[loader](model_name)\r\
          \nFile \"/workspace/text-generation-webui/modules/models.py\", line 259,\
          \ in llamacpp_loader\r\n\r\n\r\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
          \nFile \"/workspace/text-generation-webui/modules/llamacpp_model.py\", line\
          \ 91, in from_pretrained\r\n\r\n\r\nresult.model = Llama(**params)\r\nFile\
          \ \"/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/llama.py\", line\
          \ 923, in init\r\n\r\n\r\nself._n_vocab = self.n_vocab()\r\nFile \"/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/llama.py\"\
          , line 2184, in n_vocab\r\n\r\n\r\nreturn self._model.n_vocab()\r\nFile\
          \ \"/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/llama.py\", line\
          \ 250, in n_vocab\r\n\r\n\r\nassert self.model is not None\r\nAssertionError\r\
          \n```"
        updatedAt: '2023-12-15T23:10:15.976Z'
      numEdits: 0
      reactions: []
    id: 657cdcd7af1698aaac0914c0
    type: comment
  author: lazyDataScientist
  content: "Running into an issue while using Runpod with a A100. After downloading\
    \ the model I get this error message for all versions of the model (both Qn_0\
    \ and Qn_k).\r\nYou mentioned that you got it working on a single A100, did you\
    \ need to do any extra steps to get the text-generation-webui working with Mixtral\
    \ models?\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\nFile \"/workspace/text-generation-webui/modules/ui_model_menu.py\"\
    , line 209, in load_model_wrapper\r\n\r\n\r\nshared.model, shared.tokenizer =\
    \ load_model(selected_model, loader)\r\nFile \"/workspace/text-generation-webui/modules/models.py\"\
    , line 89, in load_model\r\n\r\n\r\noutput = load_func_map[loader](model_name)\r\
    \nFile \"/workspace/text-generation-webui/modules/models.py\", line 259, in llamacpp_loader\r\
    \n\r\n\r\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\nFile\
    \ \"/workspace/text-generation-webui/modules/llamacpp_model.py\", line 91, in\
    \ from_pretrained\r\n\r\n\r\nresult.model = Llama(**params)\r\nFile \"/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/llama.py\"\
    , line 923, in init\r\n\r\n\r\nself._n_vocab = self.n_vocab()\r\nFile \"/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/llama.py\"\
    , line 2184, in n_vocab\r\n\r\n\r\nreturn self._model.n_vocab()\r\nFile \"/usr/local/lib/python3.10/dist-packages/llama_cpp_cuda/llama.py\"\
    , line 250, in n_vocab\r\n\r\n\r\nassert self.model is not None\r\nAssertionError\r\
    \n```"
  created_at: 2023-12-15 23:10:15+00:00
  edited: false
  hidden: false
  id: 657cdcd7af1698aaac0914c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Undi95
      type: user
    createdAt: '2023-12-15T23:12:54.000Z'
    data:
      edited: true
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7927424311637878
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>You need to update Transformers on Runpod before launching it, I
          followed this tutorial : <a rel="nofollow" href="https://youtu.be/WjiX3lCnwUI?si=RnhYQR4eWWfeXCms&amp;t=560">https://youtu.be/WjiX3lCnwUI?si=RnhYQR4eWWfeXCms&amp;t=560</a><br>4x13B
          work on a single A100 using 96% of GPU with FP16, so use this.<br>For GGUF,
          I think last Ooba update work, with the last llama.cpp release, but I don''t
          use GGUF in Ooba. Sorry!</p>

          <p>tl;dr : If you use an A100 of runpod, use the unquantized files, it work!</p>

          '
        raw: 'You need to update Transformers on Runpod before launching it, I followed
          this tutorial : https://youtu.be/WjiX3lCnwUI?si=RnhYQR4eWWfeXCms&t=560

          4x13B work on a single A100 using 96% of GPU with FP16, so use this.

          For GGUF, I think last Ooba update work, with the last llama.cpp release,
          but I don''t use GGUF in Ooba. Sorry!


          tl;dr : If you use an A100 of runpod, use the unquantized files, it work!'
        updatedAt: '2023-12-15T23:13:33.301Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - lazyDataScientist
    id: 657cdd768888ccb894ac5f2f
    type: comment
  author: Undi95
  content: 'You need to update Transformers on Runpod before launching it, I followed
    this tutorial : https://youtu.be/WjiX3lCnwUI?si=RnhYQR4eWWfeXCms&t=560

    4x13B work on a single A100 using 96% of GPU with FP16, so use this.

    For GGUF, I think last Ooba update work, with the last llama.cpp release, but
    I don''t use GGUF in Ooba. Sorry!


    tl;dr : If you use an A100 of runpod, use the unquantized files, it work!'
  created_at: 2023-12-15 23:12:54+00:00
  edited: true
  hidden: false
  id: 657cdd768888ccb894ac5f2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a39ec8f27255b6b571101/7J_BcRm7ua0WZNIGwEzlo.png?w=200&h=200&f=face
      fullname: Cedrick Hesketh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lazyDataScientist
      type: user
    createdAt: '2023-12-15T23:17:56.000Z'
    data:
      edited: false
      editors:
      - lazyDataScientist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9611566066741943
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a39ec8f27255b6b571101/7J_BcRm7ua0WZNIGwEzlo.png?w=200&h=200&f=face
          fullname: Cedrick Hesketh
          isHf: false
          isPro: false
          name: lazyDataScientist
          type: user
        html: '<p>Awesome! Thank you! Love the work you have been doing!</p>

          '
        raw: Awesome! Thank you! Love the work you have been doing!
        updatedAt: '2023-12-15T23:17:56.229Z'
      numEdits: 0
      reactions: []
    id: 657cdea43687559a67336443
    type: comment
  author: lazyDataScientist
  content: Awesome! Thank you! Love the work you have been doing!
  created_at: 2023-12-15 23:17:56+00:00
  edited: false
  hidden: false
  id: 657cdea43687559a67336443
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Undi95/Llamix2-MLewd-4x13B-GGUF
repo_type: model
status: open
target_branch: null
title: Load model text-generation-webui issues
