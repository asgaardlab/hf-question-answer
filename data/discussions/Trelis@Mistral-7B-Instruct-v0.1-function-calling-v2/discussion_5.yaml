!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Jdo300
conflicting_files: null
created_at: 2023-11-14 14:12:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eff53237b1c45c79835165483841d587.svg
      fullname: Jason Owens
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jdo300
      type: user
    createdAt: '2023-11-14T14:12:19.000Z'
    data:
      edited: true
      editors:
      - Jdo300
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9548508524894714
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eff53237b1c45c79835165483841d587.svg
          fullname: Jason Owens
          isHf: false
          isPro: false
          name: Jdo300
          type: user
        html: '<p>Hello,</p>

          <p>I have a simple program working with this model but realized that I wasn''t
          sure how to send the function''s output back to the model. Do you just put
          the raw value back as part of the user message or does it need to be wrapped
          in a special tag so the LLM knows that it is a response from the last function
          call?</p>

          <p>Thanks,<br>Jason O</p>

          '
        raw: 'Hello,


          I have a simple program working with this model but realized that I wasn''t
          sure how to send the function''s output back to the model. Do you just put
          the raw value back as part of the user message or does it need to be wrapped
          in a special tag so the LLM knows that it is a response from the last function
          call?


          Thanks,

          Jason O'
        updatedAt: '2023-11-14T14:13:16.823Z'
      numEdits: 1
      reactions: []
    id: 65538043d7e306c340bea4e6
    type: comment
  author: Jdo300
  content: 'Hello,


    I have a simple program working with this model but realized that I wasn''t sure
    how to send the function''s output back to the model. Do you just put the raw
    value back as part of the user message or does it need to be wrapped in a special
    tag so the LLM knows that it is a response from the last function call?


    Thanks,

    Jason O'
  created_at: 2023-11-14 14:12:19+00:00
  edited: true
  hidden: false
  id: 65538043d7e306c340bea4e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-11-14T17:43:17.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8258099555969238
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Two options (depending on what kind of function you''re using):</p>

          <p>A. Replace the json in the assistant''s message with the function call
          response. (if your function returns a clean answer that is useful for the
          user).<br>B. Add the function response as a new user message, prepended
          by "Here is the response to that function call:\n\n{function_response}",
          then let the llm generate a new assistant response now that it has the required
          data. After getting back this new assistant response, you can optionally
          drop the function call message and the function response message from the
          array of messages (so that the user is blind to the entire process).</p>

          '
        raw: 'Two options (depending on what kind of function you''re using):


          A. Replace the json in the assistant''s message with the function call response.
          (if your function returns a clean answer that is useful for the user).

          B. Add the function response as a new user message, prepended by "Here is
          the response to that function call:\n\n{function_response}", then let the
          llm generate a new assistant response now that it has the required data.
          After getting back this new assistant response, you can optionally drop
          the function call message and the function response message from the array
          of messages (so that the user is blind to the entire process).'
        updatedAt: '2023-11-14T17:43:17.872Z'
      numEdits: 0
      reactions: []
    id: 6553b1b5f99d6ba41c2b8539
    type: comment
  author: RonanMcGovern
  content: 'Two options (depending on what kind of function you''re using):


    A. Replace the json in the assistant''s message with the function call response.
    (if your function returns a clean answer that is useful for the user).

    B. Add the function response as a new user message, prepended by "Here is the
    response to that function call:\n\n{function_response}", then let the llm generate
    a new assistant response now that it has the required data. After getting back
    this new assistant response, you can optionally drop the function call message
    and the function response message from the array of messages (so that the user
    is blind to the entire process).'
  created_at: 2023-11-14 17:43:17+00:00
  edited: false
  hidden: false
  id: 6553b1b5f99d6ba41c2b8539
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eff53237b1c45c79835165483841d587.svg
      fullname: Jason Owens
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jdo300
      type: user
    createdAt: '2023-11-16T14:35:06.000Z'
    data:
      edited: false
      editors:
      - Jdo300
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8307346701622009
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eff53237b1c45c79835165483841d587.svg
          fullname: Jason Owens
          isHf: false
          isPro: false
          name: Jdo300
          type: user
        html: '<p>Thank you for getting back with me!</p>

          <p>I tried approach B and it seems to kind of work. But I just realized
          that I may not be formatting my prompts correctly and wanted to confirm
          with you if I am doing it correctly.</p>

          <p>So far, I''m able to get the model to send function calls and I''m able
          to execute them and return the value. (How the model is responding is another
          story that I''ll take a look at next). My first question is about the use
          of the <s></s> tags and their placement. </p>

          <p>Example with 1 user prompt:</p>

          <pre><code>&lt;FUNCTIONS&gt;


          &lt;/FUNCTIONS&gt;


          [INST]&lt;&lt;SYS&gt;&gt;

          System Prompt here

          &lt;&lt;/SYS&gt;&gt;


          User Prompt 1 [/INST]

          </code></pre>

          <p>Example with two user prompts:</p>

          <pre><code>&lt;FUNCTIONS&gt;


          &lt;/FUNCTIONS&gt;


          &lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt;

          System Prompt here

          &lt;&lt;/SYS&gt;&gt;


          User Prompt 1 [/INST] Assistant response here&lt;/s&gt;

          [INST]User Prompt 2 [/INST]

          </code></pre>

          <p>Example with three user prompts:</p>

          <pre><code>&lt;FUNCTIONS&gt;


          &lt;/FUNCTIONS&gt;


          &lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt;

          System Prompt here

          &lt;&lt;/SYS&gt;&gt;


          User Prompt 1 [/INST] Assistant response 1&lt;/s&gt;

          &lt;s&gt;[INST]User Prompt 2 [/INST] Assistant response 2&lt;/s&gt;

          [INST]User Prompt 3 [/INST]

          </code></pre>

          <p>Example with four user prompts:</p>

          <pre><code>&lt;FUNCTIONS&gt;


          &lt;/FUNCTIONS&gt;


          &lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt;

          System Prompt here

          &lt;&lt;/SYS&gt;&gt;


          User Prompt 1 [/INST] Assistant response 1&lt;/s&gt;

          &lt;s&gt;[INST]User Prompt 2 [/INST] Assistant response 2&lt;/s&gt;

          &lt;s&gt;[INST]User Prompt 3 [/INST] Assistant response 3&lt;/s&gt;

          [INST]User Prompt 4 [/INST]

          </code></pre>

          <p>Does this look correct?</p>

          '
        raw: "Thank you for getting back with me!\n\nI tried approach B and it seems\
          \ to kind of work. But I just realized that I may not be formatting my prompts\
          \ correctly and wanted to confirm with you if I am doing it correctly.\n\
          \nSo far, I'm able to get the model to send function calls and I'm able\
          \ to execute them and return the value. (How the model is responding is\
          \ another story that I'll take a look at next). My first question is about\
          \ the use of the <s></s> tags and their placement. \n\nExample with 1 user\
          \ prompt:\n```\n<FUNCTIONS>\n\n</FUNCTIONS>\n\n[INST]<<SYS>>\nSystem Prompt\
          \ here\n<</SYS>>\n\nUser Prompt 1 [/INST]\n```\n\nExample with two user\
          \ prompts:\n```\n<FUNCTIONS>\n\n</FUNCTIONS>\n\n<s>[INST]<<SYS>>\nSystem\
          \ Prompt here\n<</SYS>>\n\nUser Prompt 1 [/INST] Assistant response here</s>\n\
          [INST]User Prompt 2 [/INST]\n```\n\nExample with three user prompts:\n```\n\
          <FUNCTIONS>\n\n</FUNCTIONS>\n\n<s>[INST]<<SYS>>\nSystem Prompt here\n<</SYS>>\n\
          \nUser Prompt 1 [/INST] Assistant response 1</s>\n<s>[INST]User Prompt 2\
          \ [/INST] Assistant response 2</s>\n[INST]User Prompt 3 [/INST]\n```\n\n\
          Example with four user prompts:\n```\n<FUNCTIONS>\n\n</FUNCTIONS>\n\n<s>[INST]<<SYS>>\n\
          System Prompt here\n<</SYS>>\n\nUser Prompt 1 [/INST] Assistant response\
          \ 1</s>\n<s>[INST]User Prompt 2 [/INST] Assistant response 2</s>\n<s>[INST]User\
          \ Prompt 3 [/INST] Assistant response 3</s>\n[INST]User Prompt 4 [/INST]\n\
          ```\n\nDoes this look correct?"
        updatedAt: '2023-11-16T14:35:06.960Z'
      numEdits: 0
      reactions: []
    id: 6556289ae5f426493b9b9510
    type: comment
  author: Jdo300
  content: "Thank you for getting back with me!\n\nI tried approach B and it seems\
    \ to kind of work. But I just realized that I may not be formatting my prompts\
    \ correctly and wanted to confirm with you if I am doing it correctly.\n\nSo far,\
    \ I'm able to get the model to send function calls and I'm able to execute them\
    \ and return the value. (How the model is responding is another story that I'll\
    \ take a look at next). My first question is about the use of the <s></s> tags\
    \ and their placement. \n\nExample with 1 user prompt:\n```\n<FUNCTIONS>\n\n</FUNCTIONS>\n\
    \n[INST]<<SYS>>\nSystem Prompt here\n<</SYS>>\n\nUser Prompt 1 [/INST]\n```\n\n\
    Example with two user prompts:\n```\n<FUNCTIONS>\n\n</FUNCTIONS>\n\n<s>[INST]<<SYS>>\n\
    System Prompt here\n<</SYS>>\n\nUser Prompt 1 [/INST] Assistant response here</s>\n\
    [INST]User Prompt 2 [/INST]\n```\n\nExample with three user prompts:\n```\n<FUNCTIONS>\n\
    \n</FUNCTIONS>\n\n<s>[INST]<<SYS>>\nSystem Prompt here\n<</SYS>>\n\nUser Prompt\
    \ 1 [/INST] Assistant response 1</s>\n<s>[INST]User Prompt 2 [/INST] Assistant\
    \ response 2</s>\n[INST]User Prompt 3 [/INST]\n```\n\nExample with four user prompts:\n\
    ```\n<FUNCTIONS>\n\n</FUNCTIONS>\n\n<s>[INST]<<SYS>>\nSystem Prompt here\n<</SYS>>\n\
    \nUser Prompt 1 [/INST] Assistant response 1</s>\n<s>[INST]User Prompt 2 [/INST]\
    \ Assistant response 2</s>\n<s>[INST]User Prompt 3 [/INST] Assistant response\
    \ 3</s>\n[INST]User Prompt 4 [/INST]\n```\n\nDoes this look correct?"
  created_at: 2023-11-16 14:35:06+00:00
  edited: false
  hidden: false
  id: 6556289ae5f426493b9b9510
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-11-17T17:06:57.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9828620553016663
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Yeah, that looks about right.</p>

          <p>BTW, with Llama it''s common to have a space after [INST] and before
          [/INST], so it would be:</p>

          <pre><code>[INST] {prompt} [/INST]

          </code></pre>

          <p>You can see that in the model card.</p>

          '
        raw: 'Yeah, that looks about right.


          BTW, with Llama it''s common to have a space after [INST] and before [/INST],
          so it would be:

          ```

          [INST] {prompt} [/INST]

          ```

          You can see that in the model card.'
        updatedAt: '2023-11-17T17:06:57.326Z'
      numEdits: 0
      reactions: []
    id: 65579db101aed573e7a67333
    type: comment
  author: RonanMcGovern
  content: 'Yeah, that looks about right.


    BTW, with Llama it''s common to have a space after [INST] and before [/INST],
    so it would be:

    ```

    [INST] {prompt} [/INST]

    ```

    You can see that in the model card.'
  created_at: 2023-11-17 17:06:57+00:00
  edited: false
  hidden: false
  id: 65579db101aed573e7a67333
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-11-22T17:12:25.000Z'
    data:
      status: closed
    id: 655e36793beaa281e5079627
    type: status-change
  author: RonanMcGovern
  created_at: 2023-11-22 17:12:25+00:00
  id: 655e36793beaa281e5079627
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: Trelis/Mistral-7B-Instruct-v0.1-function-calling-v2
repo_type: model
status: closed
target_branch: null
title: How to properly return function call return values
