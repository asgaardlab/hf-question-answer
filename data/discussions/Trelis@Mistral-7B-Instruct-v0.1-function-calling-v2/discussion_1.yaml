!!python/object:huggingface_hub.community.DiscussionWithDetails
author: santhosh-ai
conflicting_files: null
created_at: 2023-10-13 08:46:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/63ef7763175327b860cb8661985c89e5.svg
      fullname: yamana
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: santhosh-ai
      type: user
    createdAt: '2023-10-13T09:46:26.000Z'
    data:
      edited: false
      editors:
      - santhosh-ai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9182572364807129
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/63ef7763175327b860cb8661985c89e5.svg
          fullname: yamana
          isHf: false
          isPro: false
          name: santhosh-ai
          type: user
        html: '<p>Hello, </p>

          <p>I am able to download the model and run the model successfully with the
          below command.</p>

          <p>"""<br>model=Trelis/Mistral-7B-Instruct-v0.1-function-calling-v2<br>docker
          run -d --gpus "device=1" --shm-size 1g -e HUGGING_FACE_HUB_TOKEN=$token
          -p 6691:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest
          --model-id $model  --max-input-length 4000 --max-total-tokens 4096<br>"""</p>

          <p>Inference code :<br>"""<br>from text_generation import Client</p>

          <p>client = Client("<a rel="nofollow" href="http://127.0.0.1:6691&quot;">http://127.0.0.1:6691"</a>,
          timeout=100)</p>

          <p>text = ""</p>

          <p>data = client.generate(text, max_new_tokens=700).generated_text<br>"""</p>

          <p>I am expecting the response in json format with below conversation, prompt
          and system prompt. Could you please send me the sample function calling
          format to the TGI inference API.?</p>

          <p>Input context :<br>customer : Hello hello is anyone there this is amanda
          here can you hear me. ,agent : Yes you are audible amanda first of all good
          morning to you my name is robert how may i help you. ,customer : Yeah hi
          robert so i have actually lost my phone and and this is pretty urgent i
          have a very important meeting to go to can you help me with this i am calling
          from a payphone here i don''t know what exactly do so yeah tell me tell
          me what to do help me with. ,agent : Certainly amanda so first of all i
          would need to check some details so first of all you have to tell me the
          number that you are calling from is it the same number. ,customer : Yeah
          no this is this is a pay phone i just told you right the payphone this is
          not my number. ,agent : Oh i am so sorry i think i missed out on that so
          just tell me the name and some more details like how you lost it. ,customer
          : Last i remember i i left my house and i joke to my office and i stopped
          trying between at the grocery store to get the job after coming back to
          my car that i don''t have it i check in the grocery store they are saying
          they don''t know anything about it they are not ready to take any kind of
          responsibilities about this and i feel like it has been it has been stoll
          and there itself but i do not know how to go forward with this and. ,customer
          : Yeah it is pretty much about how i lost me. ,agent : I am so sorry for
          this just tell me your number that was on the last phone. ,customer : Yeah
          so the number is if you can note it down it is. ,customer : One one one.
          ,customer : Nine seven eight three. ,customer : One zero. ,customer : One
          zero two. ,agent : Great i have not in the number can you just repeat the
          number again. ,customer : It is one one one nine seven eight three one zero
          two. ,agent : Alright please hold the line for a while let me check the
          details for you. ,agent : Thanks for the long hold as i can check in our
          system i think i need to know a couple of things from your side could you
          please confirm. ,agent : Your name and the last dial number on your phone.
          ,customer : Okay that''s going to be hard so my full name is amanda rama
          krishnan and the last number i dialed. ,customer : Last number that i did
          was i need. ,customer : Yeah my house me but i don''t know her number how
          i know in these days you don''t you never remember your number how do i.
          ,agent : Alright let me help you with this. ,agent : Do you remember the
          last person you called maybe your mother or your friend. ,</p>

          <p>prompt:<br>how empathetic the agent is in the given conversation on a
          score of 0 to 10 with explanation.</p>

          <p>system_prompt:<br>You are a call quality analyst. You should assess the
          agent''s performance.</p>

          <p>Expected sample output:</p>

          <p>{<br>"score": 8,<br>"explanation": "The agent is empathetic towards the
          customer and is actively trying to help the customer find their lost phone.
          The agent is patient and understanding with the customer, and is taking
          the time to gather all the necessary information to assist the customer.
          The agent is also apologetic for the inconvenience caused to the customer
          and is doing their best to provide a solution."<br>}</p>

          <p>Thanks,<br>Santhosh.</p>

          '
        raw: "Hello, \r\n\r\nI am able to download the model and run the model successfully\
          \ with the below command.\r\n\r\n\"\"\"\r\nmodel=Trelis/Mistral-7B-Instruct-v0.1-function-calling-v2\r\
          \ndocker run -d --gpus \"device=1\" --shm-size 1g -e HUGGING_FACE_HUB_TOKEN=$token\
          \ -p 6691:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest\
          \ --model-id $model  --max-input-length 4000 --max-total-tokens 4096\r\n\
          \"\"\"\r\n\r\nInference code :\r\n\"\"\"\r\nfrom text_generation import\
          \ Client\r\n\r\nclient = Client(\"http://127.0.0.1:6691\", timeout=100)\r\
          \n\r\ntext = \"\"\r\n\r\ndata = client.generate(text, max_new_tokens=700).generated_text\r\
          \n\"\"\"\r\n\r\nI am expecting the response in json format with below conversation,\
          \ prompt and system prompt. Could you please send me the sample function\
          \ calling format to the TGI inference API.?\r\n\r\nInput context :\r\ncustomer\
          \ : Hello hello is anyone there this is amanda here can you hear me. ,agent\
          \ : Yes you are audible amanda first of all good morning to you my name\
          \ is robert how may i help you. ,customer : Yeah hi robert so i have actually\
          \ lost my phone and and this is pretty urgent i have a very important meeting\
          \ to go to can you help me with this i am calling from a payphone here i\
          \ don't know what exactly do so yeah tell me tell me what to do help me\
          \ with. ,agent : Certainly amanda so first of all i would need to check\
          \ some details so first of all you have to tell me the number that you are\
          \ calling from is it the same number. ,customer : Yeah no this is this is\
          \ a pay phone i just told you right the payphone this is not my number.\
          \ ,agent : Oh i am so sorry i think i missed out on that so just tell me\
          \ the name and some more details like how you lost it. ,customer : Last\
          \ i remember i i left my house and i joke to my office and i stopped trying\
          \ between at the grocery store to get the job after coming back to my car\
          \ that i don't have it i check in the grocery store they are saying they\
          \ don't know anything about it they are not ready to take any kind of responsibilities\
          \ about this and i feel like it has been it has been stoll and there itself\
          \ but i do not know how to go forward with this and. ,customer : Yeah it\
          \ is pretty much about how i lost me. ,agent : I am so sorry for this just\
          \ tell me your number that was on the last phone. ,customer : Yeah so the\
          \ number is if you can note it down it is. ,customer : One one one. ,customer\
          \ : Nine seven eight three. ,customer : One zero. ,customer : One zero two.\
          \ ,agent : Great i have not in the number can you just repeat the number\
          \ again. ,customer : It is one one one nine seven eight three one zero two.\
          \ ,agent : Alright please hold the line for a while let me check the details\
          \ for you. ,agent : Thanks for the long hold as i can check in our system\
          \ i think i need to know a couple of things from your side could you please\
          \ confirm. ,agent : Your name and the last dial number on your phone. ,customer\
          \ : Okay that's going to be hard so my full name is amanda rama krishnan\
          \ and the last number i dialed. ,customer : Last number that i did was i\
          \ need. ,customer : Yeah my house me but i don't know her number how i know\
          \ in these days you don't you never remember your number how do i. ,agent\
          \ : Alright let me help you with this. ,agent : Do you remember the last\
          \ person you called maybe your mother or your friend. ,\r\n\r\nprompt:\r\
          \nhow empathetic the agent is in the given conversation on a score of 0\
          \ to 10 with explanation.\r\n\r\nsystem_prompt:\r\nYou are a call quality\
          \ analyst. You should assess the agent's performance.\r\n\r\nExpected sample\
          \ output:\r\n\r\n{\r\n\"score\": 8,\r\n\"explanation\": \"The agent is empathetic\
          \ towards the customer and is actively trying to help the customer find\
          \ their lost phone. The agent is patient and understanding with the customer,\
          \ and is taking the time to gather all the necessary information to assist\
          \ the customer. The agent is also apologetic for the inconvenience caused\
          \ to the customer and is doing their best to provide a solution.\"\r\n}\r\
          \n\r\nThanks,\r\nSanthosh."
        updatedAt: '2023-10-13T09:46:26.786Z'
      numEdits: 0
      reactions: []
    id: 652911f2f0042c8301e11a42
    type: comment
  author: santhosh-ai
  content: "Hello, \r\n\r\nI am able to download the model and run the model successfully\
    \ with the below command.\r\n\r\n\"\"\"\r\nmodel=Trelis/Mistral-7B-Instruct-v0.1-function-calling-v2\r\
    \ndocker run -d --gpus \"device=1\" --shm-size 1g -e HUGGING_FACE_HUB_TOKEN=$token\
    \ -p 6691:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest\
    \ --model-id $model  --max-input-length 4000 --max-total-tokens 4096\r\n\"\"\"\
    \r\n\r\nInference code :\r\n\"\"\"\r\nfrom text_generation import Client\r\n\r\
    \nclient = Client(\"http://127.0.0.1:6691\", timeout=100)\r\n\r\ntext = \"\"\r\
    \n\r\ndata = client.generate(text, max_new_tokens=700).generated_text\r\n\"\"\"\
    \r\n\r\nI am expecting the response in json format with below conversation, prompt\
    \ and system prompt. Could you please send me the sample function calling format\
    \ to the TGI inference API.?\r\n\r\nInput context :\r\ncustomer : Hello hello\
    \ is anyone there this is amanda here can you hear me. ,agent : Yes you are audible\
    \ amanda first of all good morning to you my name is robert how may i help you.\
    \ ,customer : Yeah hi robert so i have actually lost my phone and and this is\
    \ pretty urgent i have a very important meeting to go to can you help me with\
    \ this i am calling from a payphone here i don't know what exactly do so yeah\
    \ tell me tell me what to do help me with. ,agent : Certainly amanda so first\
    \ of all i would need to check some details so first of all you have to tell me\
    \ the number that you are calling from is it the same number. ,customer : Yeah\
    \ no this is this is a pay phone i just told you right the payphone this is not\
    \ my number. ,agent : Oh i am so sorry i think i missed out on that so just tell\
    \ me the name and some more details like how you lost it. ,customer : Last i remember\
    \ i i left my house and i joke to my office and i stopped trying between at the\
    \ grocery store to get the job after coming back to my car that i don't have it\
    \ i check in the grocery store they are saying they don't know anything about\
    \ it they are not ready to take any kind of responsibilities about this and i\
    \ feel like it has been it has been stoll and there itself but i do not know how\
    \ to go forward with this and. ,customer : Yeah it is pretty much about how i\
    \ lost me. ,agent : I am so sorry for this just tell me your number that was on\
    \ the last phone. ,customer : Yeah so the number is if you can note it down it\
    \ is. ,customer : One one one. ,customer : Nine seven eight three. ,customer :\
    \ One zero. ,customer : One zero two. ,agent : Great i have not in the number\
    \ can you just repeat the number again. ,customer : It is one one one nine seven\
    \ eight three one zero two. ,agent : Alright please hold the line for a while\
    \ let me check the details for you. ,agent : Thanks for the long hold as i can\
    \ check in our system i think i need to know a couple of things from your side\
    \ could you please confirm. ,agent : Your name and the last dial number on your\
    \ phone. ,customer : Okay that's going to be hard so my full name is amanda rama\
    \ krishnan and the last number i dialed. ,customer : Last number that i did was\
    \ i need. ,customer : Yeah my house me but i don't know her number how i know\
    \ in these days you don't you never remember your number how do i. ,agent : Alright\
    \ let me help you with this. ,agent : Do you remember the last person you called\
    \ maybe your mother or your friend. ,\r\n\r\nprompt:\r\nhow empathetic the agent\
    \ is in the given conversation on a score of 0 to 10 with explanation.\r\n\r\n\
    system_prompt:\r\nYou are a call quality analyst. You should assess the agent's\
    \ performance.\r\n\r\nExpected sample output:\r\n\r\n{\r\n\"score\": 8,\r\n\"\
    explanation\": \"The agent is empathetic towards the customer and is actively\
    \ trying to help the customer find their lost phone. The agent is patient and\
    \ understanding with the customer, and is taking the time to gather all the necessary\
    \ information to assist the customer. The agent is also apologetic for the inconvenience\
    \ caused to the customer and is doing their best to provide a solution.\"\r\n\
    }\r\n\r\nThanks,\r\nSanthosh."
  created_at: 2023-10-13 08:46:26+00:00
  edited: false
  hidden: false
  id: 652911f2f0042c8301e11a42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-10-13T09:51:29.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8703821897506714
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Hi Santhosh.</p>\n<p>I\u2019m confused and maybe I\u2019m misunderstanding\
          \ your question\u2026</p>\n<p>A) the custom function format is on the model\
          \ card for this repo if you scroll down</p>\n<p>B) there is a link for tgi\
          \ that brings you to a GitHub repo with instructions for detailed setup.</p>\n\
          <p>Are you asking something else?</p>\n"
        raw: "Hi Santhosh.\n\nI\u2019m confused and maybe I\u2019m misunderstanding\
          \ your question\u2026\n\nA) the custom function format is on the model card\
          \ for this repo if you scroll down\n\nB) there is a link for tgi that brings\
          \ you to a GitHub repo with instructions for detailed setup.\n\nAre you\
          \ asking something else?"
        updatedAt: '2023-10-13T09:51:29.643Z'
      numEdits: 0
      reactions: []
    id: 65291321550efc55c47666fe
    type: comment
  author: RonanMcGovern
  content: "Hi Santhosh.\n\nI\u2019m confused and maybe I\u2019m misunderstanding\
    \ your question\u2026\n\nA) the custom function format is on the model card for\
    \ this repo if you scroll down\n\nB) there is a link for tgi that brings you to\
    \ a GitHub repo with instructions for detailed setup.\n\nAre you asking something\
    \ else?"
  created_at: 2023-10-13 08:51:29+00:00
  edited: false
  hidden: false
  id: 65291321550efc55c47666fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/63ef7763175327b860cb8661985c89e5.svg
      fullname: yamana
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: santhosh-ai
      type: user
    createdAt: '2023-10-16T08:23:11.000Z'
    data:
      edited: true
      editors:
      - santhosh-ai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8836527466773987
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/63ef7763175327b860cb8661985c89e5.svg
          fullname: yamana
          isHf: false
          isPro: false
          name: santhosh-ai
          type: user
        html: '<p>Hello,<br>I tried with 20 samples for getting sentiment of the conversation
          but there is no consistency on json response.<br>Below is the code for it.  Please
          asses if there are any changes required in the format.</p>

          <p>sentiment_meta_data = {<br>    "function": "get_sentiment",<br>    "description":
          "what is the sentiment of the overall call conversation",<br>    "arguments":
          [<br>        {<br>            "name": "score",<br>            "type": "float",<br>            "description":
          "score on sentiment on a scale of 0 to 100"<br>        },<br>        {<br>            "name":
          "explanation",<br>            "type": "string",<br>            "description":
          "explanation on sentiment in the given conversation"<br>        }<br>    ]<br>}</p>

          <p>functionList = ''''</p>

          <p>functionList += json.dumps(search_bing_metadata, indent=4, separators=('','',
          '': ''))</p>

          <p>print(functionList)</p>

          <p>def mistral_json_format(context):<br>    # Define the roles and markers<br>    B_INST,
          E_INST = "[INST]", "[/INST]"<br>    B_FUNC, E_FUNC = "", "\n\n"<br>    B_SYS,
          E_SYS = "&lt;&gt;\n", "\n&lt;&gt;\n\n"</p>

          <pre><code># assuming functionList is defined as above

          system_prompt = ''You are a call qulaity analyst. Responsible for assess
          the agent performance''

          user_prompt = ''what is the overall sentiment score in the given conversation
          on score of range of 0 to 100 with explanation''


          # Format your prompt template

          prompt = f"{B_FUNC}{functionList.strip()}{E_FUNC}{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()}
          {E_INST} context: {context}\n\n"


          return prompt

          </code></pre>

          <p>context = "sample conversation ...."</p>

          <p>Results:<br>1.<br>{''function'': ''get_sentiment'', ''arguments'': {''score'':
          75, ''explanation'': "The sentiment of the conversation is positive. The
          agent was able to resolve the customer''s issue and provide helpful information.
          The customer was also satisfied with the resolution."}}<br>2.{''sentiment_score'':
          75, ''explanation'': ''The sentiment of the conversation is positive. The
          customer was able to get the information they needed and the agent was able
          to provide helpful information. The customer was also able to express their
          frustration with the wait time, but the agent was able to address their
          concerns and provide a solution. Overall, the conversation was productive
          and the customer was satisfied with the outcome.''}<br>3. some are failed
          ...</p>

          <p>The above results shows no consistency in the response.<br>Please check
          and resolve it.</p>

          <p>Thanks,<br>santhosh</p>

          '
        raw: "Hello, \nI tried with 20 samples for getting sentiment of the conversation\
          \ but there is no consistency on json response. \nBelow is the code for\
          \ it.  Please asses if there are any changes required in the format.\n\n\
          sentiment_meta_data = {\n    \"function\": \"get_sentiment\",\n    \"description\"\
          : \"what is the sentiment of the overall call conversation\",\n    \"arguments\"\
          : [\n        {\n            \"name\": \"score\",\n            \"type\":\
          \ \"float\",\n            \"description\": \"score on sentiment on a scale\
          \ of 0 to 100\"\n        },\n        {\n            \"name\": \"explanation\"\
          ,\n            \"type\": \"string\",\n            \"description\": \"explanation\
          \ on sentiment in the given conversation\"\n        }\n    ]\n}\n\nfunctionList\
          \ = ''\n\nfunctionList += json.dumps(search_bing_metadata, indent=4, separators=(',',\
          \ ': '))\n\nprint(functionList)\n\ndef mistral_json_format(context):\n \
          \   # Define the roles and markers\n    B_INST, E_INST = \"[INST]\", \"\
          [/INST]\"\n    B_FUNC, E_FUNC = \"<FUNCTIONS>\", \"</FUNCTIONS>\\n\\n\"\n\
          \    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n\n    # assuming\
          \ functionList is defined as above\n    system_prompt = 'You are a call\
          \ qulaity analyst. Responsible for assess the agent performance'\n    user_prompt\
          \ = 'what is the overall sentiment score in the given conversation on score\
          \ of range of 0 to 100 with explanation'\n\n    # Format your prompt template\n\
          \    prompt = f\"{B_FUNC}{functionList.strip()}{E_FUNC}{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()}\
          \ {E_INST} context: {context}\\n\\n\"\n\n    return prompt\n\ncontext =\
          \ \"sample conversation ....\"\n\nResults:\n1.\n{'function': 'get_sentiment',\
          \ 'arguments': {'score': 75, 'explanation': \"The sentiment of the conversation\
          \ is positive. The agent was able to resolve the customer's issue and provide\
          \ helpful information. The customer was also satisfied with the resolution.\"\
          }}\n2.{'sentiment_score': 75, 'explanation': 'The sentiment of the conversation\
          \ is positive. The customer was able to get the information they needed\
          \ and the agent was able to provide helpful information. The customer was\
          \ also able to express their frustration with the wait time, but the agent\
          \ was able to address their concerns and provide a solution. Overall, the\
          \ conversation was productive and the customer was satisfied with the outcome.'}\n\
          3. some are failed ...\n\nThe above results shows no consistency in the\
          \ response. \nPlease check and resolve it.\n\nThanks,\nsanthosh"
        updatedAt: '2023-10-16T08:23:40.893Z'
      numEdits: 1
      reactions: []
    id: 652cf2ef2aa5b27c770d1b7d
    type: comment
  author: santhosh-ai
  content: "Hello, \nI tried with 20 samples for getting sentiment of the conversation\
    \ but there is no consistency on json response. \nBelow is the code for it.  Please\
    \ asses if there are any changes required in the format.\n\nsentiment_meta_data\
    \ = {\n    \"function\": \"get_sentiment\",\n    \"description\": \"what is the\
    \ sentiment of the overall call conversation\",\n    \"arguments\": [\n      \
    \  {\n            \"name\": \"score\",\n            \"type\": \"float\",\n   \
    \         \"description\": \"score on sentiment on a scale of 0 to 100\"\n   \
    \     },\n        {\n            \"name\": \"explanation\",\n            \"type\"\
    : \"string\",\n            \"description\": \"explanation on sentiment in the\
    \ given conversation\"\n        }\n    ]\n}\n\nfunctionList = ''\n\nfunctionList\
    \ += json.dumps(search_bing_metadata, indent=4, separators=(',', ': '))\n\nprint(functionList)\n\
    \ndef mistral_json_format(context):\n    # Define the roles and markers\n    B_INST,\
    \ E_INST = \"[INST]\", \"[/INST]\"\n    B_FUNC, E_FUNC = \"<FUNCTIONS>\", \"</FUNCTIONS>\\\
    n\\n\"\n    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n\n    # assuming\
    \ functionList is defined as above\n    system_prompt = 'You are a call qulaity\
    \ analyst. Responsible for assess the agent performance'\n    user_prompt = 'what\
    \ is the overall sentiment score in the given conversation on score of range of\
    \ 0 to 100 with explanation'\n\n    # Format your prompt template\n    prompt\
    \ = f\"{B_FUNC}{functionList.strip()}{E_FUNC}{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()}\
    \ {E_INST} context: {context}\\n\\n\"\n\n    return prompt\n\ncontext = \"sample\
    \ conversation ....\"\n\nResults:\n1.\n{'function': 'get_sentiment', 'arguments':\
    \ {'score': 75, 'explanation': \"The sentiment of the conversation is positive.\
    \ The agent was able to resolve the customer's issue and provide helpful information.\
    \ The customer was also satisfied with the resolution.\"}}\n2.{'sentiment_score':\
    \ 75, 'explanation': 'The sentiment of the conversation is positive. The customer\
    \ was able to get the information they needed and the agent was able to provide\
    \ helpful information. The customer was also able to express their frustration\
    \ with the wait time, but the agent was able to address their concerns and provide\
    \ a solution. Overall, the conversation was productive and the customer was satisfied\
    \ with the outcome.'}\n3. some are failed ...\n\nThe above results shows no consistency\
    \ in the response. \nPlease check and resolve it.\n\nThanks,\nsanthosh"
  created_at: 2023-10-16 07:23:11+00:00
  edited: true
  hidden: false
  id: 652cf2ef2aa5b27c770d1b7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-10-16T10:44:55.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8620157837867737
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>1. </p>

          <pre><code># Format your prompt template

          prompt = f"{B_FUNC}{functionList.strip()}{E_FUNC}{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()}
          {E_INST} context: {context}\n\n"

          </code></pre>

          <p>Any user input needs to be within B_INST and E_INST, not afterwards.
          i.e. this should be:</p>

          <pre><code>prompt = f"{B_FUNC}{functionList.strip()}{E_FUNC}{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()}\n\ncontext:
          {context}  {E_INST}\n\n"

          </code></pre>

          <p>2.<br>I don''t know why there is this line referring to search bing:</p>

          <pre><code>functionList += json.dumps(search_bing_metadata, indent=4, separators=('','',
          '': ''))

          </code></pre>

          <p>I''m unclear what your function metadata is and I don''t see where function
          list is correctly set.</p>

          <p>3.<br>Thanks for your patience with any further queries as I have limited
          bandwidth for support. If you have further questions, please provide complete
          code and an example for replication.</p>

          <ol start="4">

          <li></li>

          </ol>

          <p>On a separate note, perhaps what you are doing here requires significantly
          more than function calling (although you should indeed be getting back correct
          json objects).</p>

          <p>Function calling trains a model to return a structured response. Here,
          the model is additionally required to grade the inputs and then put that
          info in the function call. To achieve this you may need a combination of:</p>

          <ol>

          <li>A very strong model (maybe 70B Llama or Falcon 180B)</li>

          <li>Much better prompting. You need to give some examples (maybe 3-5) to
          the language model of how to grade different inputs. It''s very unlikely
          to be able to grade without this background.</li>

          <li>If <a href="/Trelis/Mistral-7B-Instruct-v0.1-function-calling-v2/discussions/2">#2</a>
          doesn''t work, you may even need to go beyond prompting with some fine-tuning,
          although ideally you want to avoid this.</li>

          </ol>

          '
        raw: "1. \n```\n# Format your prompt template\nprompt = f\"{B_FUNC}{functionList.strip()}{E_FUNC}{B_INST}\
          \ {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST} context:\
          \ {context}\\n\\n\"\n```\nAny user input needs to be within B_INST and E_INST,\
          \ not afterwards. i.e. this should be:\n```\nprompt = f\"{B_FUNC}{functionList.strip()}{E_FUNC}{B_INST}\
          \ {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()}\\n\\ncontext:\
          \ {context}  {E_INST}\\n\\n\"\n```\n\n2. \nI don't know why there is this\
          \ line referring to search bing:\n```\nfunctionList += json.dumps(search_bing_metadata,\
          \ indent=4, separators=(',', ': '))\n```\nI'm unclear what your function\
          \ metadata is and I don't see where function list is correctly set.\n\n\
          3. \nThanks for your patience with any further queries as I have limited\
          \ bandwidth for support. If you have further questions, please provide complete\
          \ code and an example for replication.\n\n4.\n\nOn a separate note, perhaps\
          \ what you are doing here requires significantly more than function calling\
          \ (although you should indeed be getting back correct json objects).\n\n\
          Function calling trains a model to return a structured response. Here, the\
          \ model is additionally required to grade the inputs and then put that info\
          \ in the function call. To achieve this you may need a combination of:\n\
          1. A very strong model (maybe 70B Llama or Falcon 180B)\n2. Much better\
          \ prompting. You need to give some examples (maybe 3-5) to the language\
          \ model of how to grade different inputs. It's very unlikely to be able\
          \ to grade without this background.\n3. If #2 doesn't work, you may even\
          \ need to go beyond prompting with some fine-tuning, although ideally you\
          \ want to avoid this.\n"
        updatedAt: '2023-10-16T10:44:55.292Z'
      numEdits: 0
      reactions: []
    id: 652d14273184d39abdbd1cde
    type: comment
  author: RonanMcGovern
  content: "1. \n```\n# Format your prompt template\nprompt = f\"{B_FUNC}{functionList.strip()}{E_FUNC}{B_INST}\
    \ {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST} context:\
    \ {context}\\n\\n\"\n```\nAny user input needs to be within B_INST and E_INST,\
    \ not afterwards. i.e. this should be:\n```\nprompt = f\"{B_FUNC}{functionList.strip()}{E_FUNC}{B_INST}\
    \ {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()}\\n\\ncontext: {context}\
    \  {E_INST}\\n\\n\"\n```\n\n2. \nI don't know why there is this line referring\
    \ to search bing:\n```\nfunctionList += json.dumps(search_bing_metadata, indent=4,\
    \ separators=(',', ': '))\n```\nI'm unclear what your function metadata is and\
    \ I don't see where function list is correctly set.\n\n3. \nThanks for your patience\
    \ with any further queries as I have limited bandwidth for support. If you have\
    \ further questions, please provide complete code and an example for replication.\n\
    \n4.\n\nOn a separate note, perhaps what you are doing here requires significantly\
    \ more than function calling (although you should indeed be getting back correct\
    \ json objects).\n\nFunction calling trains a model to return a structured response.\
    \ Here, the model is additionally required to grade the inputs and then put that\
    \ info in the function call. To achieve this you may need a combination of:\n\
    1. A very strong model (maybe 70B Llama or Falcon 180B)\n2. Much better prompting.\
    \ You need to give some examples (maybe 3-5) to the language model of how to grade\
    \ different inputs. It's very unlikely to be able to grade without this background.\n\
    3. If #2 doesn't work, you may even need to go beyond prompting with some fine-tuning,\
    \ although ideally you want to avoid this.\n"
  created_at: 2023-10-16 09:44:55+00:00
  edited: false
  hidden: false
  id: 652d14273184d39abdbd1cde
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/63ef7763175327b860cb8661985c89e5.svg
      fullname: yamana
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: santhosh-ai
      type: user
    createdAt: '2023-10-17T16:07:46.000Z'
    data:
      edited: false
      editors:
      - santhosh-ai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4870988726615906
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/63ef7763175327b860cb8661985c89e5.svg
          fullname: yamana
          isHf: false
          isPro: false
          name: santhosh-ai
          type: user
        html: '<p>Hello,</p>

          <p>Please check the below sample code.<br><a rel="nofollow" href="https://colab.research.google.com/drive/1wbTP0pNOut_eHbFrdQWDgZZ95BEzv025?usp=sharing">https://colab.research.google.com/drive/1wbTP0pNOut_eHbFrdQWDgZZ95BEzv025?usp=sharing</a><br>Thanks,<br>Santhosh</p>

          '
        raw: 'Hello,


          Please check the below sample code.

          https://colab.research.google.com/drive/1wbTP0pNOut_eHbFrdQWDgZZ95BEzv025?usp=sharing

          Thanks,

          Santhosh'
        updatedAt: '2023-10-17T16:07:46.316Z'
      numEdits: 0
      reactions: []
    id: 652eb1526504631c10e1c574
    type: comment
  author: santhosh-ai
  content: 'Hello,


    Please check the below sample code.

    https://colab.research.google.com/drive/1wbTP0pNOut_eHbFrdQWDgZZ95BEzv025?usp=sharing

    Thanks,

    Santhosh'
  created_at: 2023-10-17 15:07:46+00:00
  edited: false
  hidden: false
  id: 652eb1526504631c10e1c574
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-10-20T21:51:13.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9483782052993774
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>howdy, i''m on holidays for 10 days and didn''t get to read in detail.</p>

          <p>Your colab is very helpful though.</p>

          <p>It seems that most calls are working, and the ones that aren''t are pre-pending
          [FUNCTION].</p>

          <p>My thoughts:</p>

          <ol>

          <li><p>I''ve never seen that [FUNCTION] pre-pend before. One tweak worth
          trying is to remove the space here in the prompt ''{E_INST} \n\n''. There
          should be no space before \n\n if you match syntax exactly. Small differences
          like this can have an effect.</p>

          </li>

          <li><p>Fix typos in your system message and instruction.</p>

          </li>

          <li><p>You can consider just extracting the json from the response, which
          will cut out anything prepended like [FUNCTION]. This is a bit hacky but
          may work. Alternately, yes, if you fine-tune with your specific function
          I believe there is a strong chance that will fix the issue.</p>

          </li>

          </ol>

          <p>All of what I say above is about getting the right syntax. If the contents
          of the json are not good quality enough, that indicates a strong base model
          is needed.</p>

          '
        raw: 'howdy, i''m on holidays for 10 days and didn''t get to read in detail.


          Your colab is very helpful though.


          It seems that most calls are working, and the ones that aren''t are pre-pending
          [FUNCTION].


          My thoughts:

          1. I''ve never seen that [FUNCTION] pre-pend before. One tweak worth trying
          is to remove the space here in the prompt ''{E_INST} \n\n''. There should
          be no space before \n\n if you match syntax exactly. Small differences like
          this can have an effect.


          2. Fix typos in your system message and instruction.


          3. You can consider just extracting the json from the response, which will
          cut out anything prepended like [FUNCTION]. This is a bit hacky but may
          work. Alternately, yes, if you fine-tune with your specific function I believe
          there is a strong chance that will fix the issue.


          All of what I say above is about getting the right syntax. If the contents
          of the json are not good quality enough, that indicates a strong base model
          is needed.

          '
        updatedAt: '2023-10-20T21:51:13.647Z'
      numEdits: 0
      reactions: []
    id: 6532f651e778506c5be79094
    type: comment
  author: RonanMcGovern
  content: 'howdy, i''m on holidays for 10 days and didn''t get to read in detail.


    Your colab is very helpful though.


    It seems that most calls are working, and the ones that aren''t are pre-pending
    [FUNCTION].


    My thoughts:

    1. I''ve never seen that [FUNCTION] pre-pend before. One tweak worth trying is
    to remove the space here in the prompt ''{E_INST} \n\n''. There should be no space
    before \n\n if you match syntax exactly. Small differences like this can have
    an effect.


    2. Fix typos in your system message and instruction.


    3. You can consider just extracting the json from the response, which will cut
    out anything prepended like [FUNCTION]. This is a bit hacky but may work. Alternately,
    yes, if you fine-tune with your specific function I believe there is a strong
    chance that will fix the issue.


    All of what I say above is about getting the right syntax. If the contents of
    the json are not good quality enough, that indicates a strong base model is needed.

    '
  created_at: 2023-10-20 20:51:13+00:00
  edited: false
  hidden: false
  id: 6532f651e778506c5be79094
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-10-27T20:33:52.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9396821856498718
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;santhosh-ai&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/santhosh-ai\"\
          >@<span class=\"underline\">santhosh-ai</span></a></span>\n\n\t</span></span>\
          \ closing this off, feel free to re-open with more Qs.</p>\n"
        raw: '@santhosh-ai closing this off, feel free to re-open with more Qs.'
        updatedAt: '2023-10-27T20:33:52.739Z'
      numEdits: 0
      reactions: []
      relatedEventId: 653c1eb181f52ceb4df2b53d
    id: 653c1eb081f52ceb4df2b52c
    type: comment
  author: RonanMcGovern
  content: '@santhosh-ai closing this off, feel free to re-open with more Qs.'
  created_at: 2023-10-27 19:33:52+00:00
  edited: false
  hidden: false
  id: 653c1eb081f52ceb4df2b52c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-10-27T20:33:53.000Z'
    data:
      status: closed
    id: 653c1eb181f52ceb4df2b53d
    type: status-change
  author: RonanMcGovern
  created_at: 2023-10-27 19:33:53+00:00
  id: 653c1eb181f52ceb4df2b53d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Trelis/Mistral-7B-Instruct-v0.1-function-calling-v2
repo_type: model
status: closed
target_branch: null
title: Need sample format for custom function with context.
