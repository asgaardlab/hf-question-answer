!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Mykee
conflicting_files: null
created_at: 2023-07-17 07:17:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b49b41b696f6bd5802594f772a83786.svg
      fullname: Miklos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mykee
      type: user
    createdAt: '2023-07-17T08:17:25.000Z'
    data:
      edited: false
      editors:
      - Mykee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48821866512298584
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b49b41b696f6bd5802594f772a83786.svg
          fullname: Miklos
          isHf: false
          isPro: false
          name: Mykee
          type: user
        html: '<p>I tried loading the 5.0 and 5.1 models in oobabooga with llama.cpp,
          but I get this error:</p>

          <p>llama.cpp: loading model from models\Fredithefish_RedPajama-INCITE-7B-Chat-GGML\ggml-RedPajama-INCITE-7B-Chat-q5_0.bin<br>error
          loading model: unexpectedly reached end of file<br>llama_load_model_from_file:
          failed to load model<br>2023-07-17 10:15:32 ERROR:Failed to load the model.<br>Traceback
          (most recent call last):<br>  File "C:\oobabooga\text-generation-webui\server.py",
          line 68, in load_model_wrapper<br>    shared.model, shared.tokenizer = load_model(shared.model_name,
          loader)<br>  File "C:\oobabooga\text-generation-webui\modules\models.py",
          line 79, in load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "C:\oobabooga\text-generation-webui\modules\models.py", line 268, in llamacpp_loader<br>    model,
          tokenizer = LlamaCppModel.from_pretrained(model_file)<br>  File "C:\oobabooga\text-generation-webui\modules\llamacpp_model.py",
          line 56, in from_pretrained<br>    result.model = Llama(**params)<br>  File
          "C:\oobabooga\installer_files\env\lib\site-packages\llama_cpp\llama.py",
          line 305, in <strong>init</strong><br>    assert self.model is not None<br>AssertionError</p>

          <p>Exception ignored in: &lt;function Llama.__del__ at 0x0000015D35DDECB0&gt;<br>Traceback
          (most recent call last):<br>  File "C:\oobabooga\installer_files\env\lib\site-packages\llama_cpp\llama.py",
          line 1502, in <strong>del</strong><br>    if self.ctx is not None:<br>AttributeError:
          ''Llama'' object has no attribute ''ctx''<br>Exception ignored in: &lt;function
          LlamaCppModel.__del__ at 0x0000015D35D52290&gt;<br>Traceback (most recent
          call last):<br>  File "C:\oobabooga\text-generation-webui\modules\llamacpp_model.py",
          line 29, in <strong>del</strong><br>    self.model.<strong>del</strong>()<br>AttributeError:
          ''LlamaCppModel'' object has no attribute ''model''</p>

          <p>How can I fix it?</p>

          '
        raw: "I tried loading the 5.0 and 5.1 models in oobabooga with llama.cpp,\
          \ but I get this error:\r\n\r\nllama.cpp: loading model from models\\Fredithefish_RedPajama-INCITE-7B-Chat-GGML\\\
          ggml-RedPajama-INCITE-7B-Chat-q5_0.bin\r\nerror loading model: unexpectedly\
          \ reached end of file\r\nllama_load_model_from_file: failed to load model\r\
          \n2023-07-17 10:15:32 ERROR:Failed to load the model.\r\nTraceback (most\
          \ recent call last):\r\n  File \"C:\\oobabooga\\text-generation-webui\\\
          server.py\", line 68, in load_model_wrapper\r\n    shared.model, shared.tokenizer\
          \ = load_model(shared.model_name, loader)\r\n  File \"C:\\oobabooga\\text-generation-webui\\\
          modules\\models.py\", line 79, in load_model\r\n    output = load_func_map[loader](model_name)\r\
          \n  File \"C:\\oobabooga\\text-generation-webui\\modules\\models.py\", line\
          \ 268, in llamacpp_loader\r\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
          \n  File \"C:\\oobabooga\\text-generation-webui\\modules\\llamacpp_model.py\"\
          , line 56, in from_pretrained\r\n    result.model = Llama(**params)\r\n\
          \  File \"C:\\oobabooga\\installer_files\\env\\lib\\site-packages\\llama_cpp\\\
          llama.py\", line 305, in __init__\r\n    assert self.model is not None\r\
          \nAssertionError\r\n\r\nException ignored in: <function Llama.__del__ at\
          \ 0x0000015D35DDECB0>\r\nTraceback (most recent call last):\r\n  File \"\
          C:\\oobabooga\\installer_files\\env\\lib\\site-packages\\llama_cpp\\llama.py\"\
          , line 1502, in __del__\r\n    if self.ctx is not None:\r\nAttributeError:\
          \ 'Llama' object has no attribute 'ctx'\r\nException ignored in: <function\
          \ LlamaCppModel.__del__ at 0x0000015D35D52290>\r\nTraceback (most recent\
          \ call last):\r\n  File \"C:\\oobabooga\\text-generation-webui\\modules\\\
          llamacpp_model.py\", line 29, in __del__\r\n    self.model.__del__()\r\n\
          AttributeError: 'LlamaCppModel' object has no attribute 'model'\r\n\r\n\r\
          \nHow can I fix it?"
        updatedAt: '2023-07-17T08:17:25.407Z'
      numEdits: 0
      reactions: []
    id: 64b4f9153aecf08f7846d19c
    type: comment
  author: Mykee
  content: "I tried loading the 5.0 and 5.1 models in oobabooga with llama.cpp, but\
    \ I get this error:\r\n\r\nllama.cpp: loading model from models\\Fredithefish_RedPajama-INCITE-7B-Chat-GGML\\\
    ggml-RedPajama-INCITE-7B-Chat-q5_0.bin\r\nerror loading model: unexpectedly reached\
    \ end of file\r\nllama_load_model_from_file: failed to load model\r\n2023-07-17\
    \ 10:15:32 ERROR:Failed to load the model.\r\nTraceback (most recent call last):\r\
    \n  File \"C:\\oobabooga\\text-generation-webui\\server.py\", line 68, in load_model_wrapper\r\
    \n    shared.model, shared.tokenizer = load_model(shared.model_name, loader)\r\
    \n  File \"C:\\oobabooga\\text-generation-webui\\modules\\models.py\", line 79,\
    \ in load_model\r\n    output = load_func_map[loader](model_name)\r\n  File \"\
    C:\\oobabooga\\text-generation-webui\\modules\\models.py\", line 268, in llamacpp_loader\r\
    \n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\n  File \"\
    C:\\oobabooga\\text-generation-webui\\modules\\llamacpp_model.py\", line 56, in\
    \ from_pretrained\r\n    result.model = Llama(**params)\r\n  File \"C:\\oobabooga\\\
    installer_files\\env\\lib\\site-packages\\llama_cpp\\llama.py\", line 305, in\
    \ __init__\r\n    assert self.model is not None\r\nAssertionError\r\n\r\nException\
    \ ignored in: <function Llama.__del__ at 0x0000015D35DDECB0>\r\nTraceback (most\
    \ recent call last):\r\n  File \"C:\\oobabooga\\installer_files\\env\\lib\\site-packages\\\
    llama_cpp\\llama.py\", line 1502, in __del__\r\n    if self.ctx is not None:\r\
    \nAttributeError: 'Llama' object has no attribute 'ctx'\r\nException ignored in:\
    \ <function LlamaCppModel.__del__ at 0x0000015D35D52290>\r\nTraceback (most recent\
    \ call last):\r\n  File \"C:\\oobabooga\\text-generation-webui\\modules\\llamacpp_model.py\"\
    , line 29, in __del__\r\n    self.model.__del__()\r\nAttributeError: 'LlamaCppModel'\
    \ object has no attribute 'model'\r\n\r\n\r\nHow can I fix it?"
  created_at: 2023-07-17 07:17:25+00:00
  edited: false
  hidden: false
  id: 64b4f9153aecf08f7846d19c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Fredithefish/RedPajama-INCITE-7B-Chat-GGML
repo_type: model
status: open
target_branch: null
title: Error with llama.cpp under Oobabooga
