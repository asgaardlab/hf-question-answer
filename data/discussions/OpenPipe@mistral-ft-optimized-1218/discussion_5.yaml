!!python/object:huggingface_hub.community.DiscussionWithDetails
author: acastanza
conflicting_files: null
created_at: 2023-12-26 22:26:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59399ee42f844c053fa2eda8e556e94e.svg
      fullname: Anthony Castanza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: acastanza
      type: user
    createdAt: '2023-12-26T22:26:32.000Z'
    data:
      edited: true
      editors:
      - acastanza
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9771941304206848
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59399ee42f844c053fa2eda8e556e94e.svg
          fullname: Anthony Castanza
          isHf: false
          isPro: false
          name: acastanza
          type: user
        html: '<p>Any comment to this post from the /r/LocalLLaMA subreddit that the
          model weights you''ve claimed to produce are identical to those of another
          model (Weyaxi/Seraph-7B)?</p>

          <p>I consider this concerning as this reddit post is written by Weyaxi,
          the producers of one of the models you''ve claimed to use in your merge
          (Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp), and the model that you''ve
          claimed, that apparently has identical weights was produced by this same
          individual previously (Weyaxi/Seraph-7B), but was coincidentally omitted
          from all the testing you''ve described in your blog post.</p>

          <p>No one is necessarily accusing anyone of any impropriety but identical
          sha256 hashes of the model weights to a model that predates yours by a full
          week would at least appear to deserve some explanation.</p>

          <p>Note that I haven''t personally validated the findings here, but I did
          think that the reddit post merited discussion especially considering you''re
          attempting to monetize.</p>

          <p>Edit; forgot to add the link to the reddit post - <a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/18rhv8r/mistralftoptimized1218s_weights_were_already_on/">https://www.reddit.com/r/LocalLLaMA/comments/18rhv8r/mistralftoptimized1218s_weights_were_already_on/</a></p>

          '
        raw: 'Any comment to this post from the /r/LocalLLaMA subreddit that the model
          weights you''ve claimed to produce are identical to those of another model
          (Weyaxi/Seraph-7B)?


          I consider this concerning as this reddit post is written by Weyaxi, the
          producers of one of the models you''ve claimed to use in your merge (Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp),
          and the model that you''ve claimed, that apparently has identical weights
          was produced by this same individual previously (Weyaxi/Seraph-7B), but
          was coincidentally omitted from all the testing you''ve described in your
          blog post.


          No one is necessarily accusing anyone of any impropriety but identical sha256
          hashes of the model weights to a model that predates yours by a full week
          would at least appear to deserve some explanation.


          Note that I haven''t personally validated the findings here, but I did think
          that the reddit post merited discussion especially considering you''re attempting
          to monetize.


          Edit; forgot to add the link to the reddit post - https://www.reddit.com/r/LocalLLaMA/comments/18rhv8r/mistralftoptimized1218s_weights_were_already_on/'
        updatedAt: '2023-12-26T22:48:22.990Z'
      numEdits: 1
      reactions: []
    id: 658b531860969e6d66e24e58
    type: comment
  author: acastanza
  content: 'Any comment to this post from the /r/LocalLLaMA subreddit that the model
    weights you''ve claimed to produce are identical to those of another model (Weyaxi/Seraph-7B)?


    I consider this concerning as this reddit post is written by Weyaxi, the producers
    of one of the models you''ve claimed to use in your merge (Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp),
    and the model that you''ve claimed, that apparently has identical weights was
    produced by this same individual previously (Weyaxi/Seraph-7B), but was coincidentally
    omitted from all the testing you''ve described in your blog post.


    No one is necessarily accusing anyone of any impropriety but identical sha256
    hashes of the model weights to a model that predates yours by a full week would
    at least appear to deserve some explanation.


    Note that I haven''t personally validated the findings here, but I did think that
    the reddit post merited discussion especially considering you''re attempting to
    monetize.


    Edit; forgot to add the link to the reddit post - https://www.reddit.com/r/LocalLLaMA/comments/18rhv8r/mistralftoptimized1218s_weights_were_already_on/'
  created_at: 2023-12-26 22:26:32+00:00
  edited: true
  hidden: false
  id: 658b531860969e6d66e24e58
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6242f124b23d3e5324f34673/_mdr-Rkir7ZOI7Islcway.png?w=200&h=200&f=face
      fullname: Kyle Corbitt
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: corbt
      type: user
    createdAt: '2023-12-26T23:26:28.000Z'
    data:
      edited: true
      editors:
      - corbt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9319732785224915
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6242f124b23d3e5324f34673/_mdr-Rkir7ZOI7Islcway.png?w=200&h=200&f=face
          fullname: Kyle Corbitt
          isHf: false
          isPro: false
          name: corbt
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;acastanza&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/acastanza\"\
          >@<span class=\"underline\">acastanza</span></a></span>\n\n\t</span></span>,\
          \ thanks for bringing this up. I just read that Reddit thread and checked\
          \ out the Seraph model. It does look like Weyaxi merged the same models\
          \ using the same mergekit defaults we did here. So assuming he didn't change\
          \ anything else and was using a similar version of Mergekit, it likely is\
          \ the same model. I'll update the README to credit him. Full response here:\
          \ <a rel=\"nofollow\" href=\"https://www.reddit.com/r/LocalLLaMA/comments/18rhv8r/comment/kf1sp75/?utm_source=share&amp;utm_medium=web2x&amp;context=3\"\
          >https://www.reddit.com/r/LocalLLaMA/comments/18rhv8r/comment/kf1sp75/?utm_source=share&amp;utm_medium=web2x&amp;context=3</a></p>\n"
        raw: 'Hey @acastanza, thanks for bringing this up. I just read that Reddit
          thread and checked out the Seraph model. It does look like Weyaxi merged
          the same models using the same mergekit defaults we did here. So assuming
          he didn''t change anything else and was using a similar version of Mergekit,
          it likely is the same model. I''ll update the README to credit him. Full
          response here: https://www.reddit.com/r/LocalLLaMA/comments/18rhv8r/comment/kf1sp75/?utm_source=share&utm_medium=web2x&context=3'
        updatedAt: '2023-12-26T23:26:46.221Z'
      numEdits: 1
      reactions: []
    id: 658b6124fdf2279d471609b5
    type: comment
  author: corbt
  content: 'Hey @acastanza, thanks for bringing this up. I just read that Reddit thread
    and checked out the Seraph model. It does look like Weyaxi merged the same models
    using the same mergekit defaults we did here. So assuming he didn''t change anything
    else and was using a similar version of Mergekit, it likely is the same model.
    I''ll update the README to credit him. Full response here: https://www.reddit.com/r/LocalLLaMA/comments/18rhv8r/comment/kf1sp75/?utm_source=share&utm_medium=web2x&context=3'
  created_at: 2023-12-26 23:26:28+00:00
  edited: true
  hidden: false
  id: 658b6124fdf2279d471609b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59399ee42f844c053fa2eda8e556e94e.svg
      fullname: Anthony Castanza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: acastanza
      type: user
    createdAt: '2023-12-26T23:39:47.000Z'
    data:
      edited: false
      editors:
      - acastanza
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9708720445632935
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59399ee42f844c053fa2eda8e556e94e.svg
          fullname: Anthony Castanza
          isHf: false
          isPro: false
          name: acastanza
          type: user
        html: '<p>Thanks for the response! Considering both what you''ve said and
          other comments in the thread coming to similar conclusions as to the likely
          cause, I think this is satisfactorily resolved.<br>Same merge, with same
          tools, with same parameters = identical weights. Glad to see this ended
          up being a friendly misunderstanding!</p>

          '
        raw: 'Thanks for the response! Considering both what you''ve said and other
          comments in the thread coming to similar conclusions as to the likely cause,
          I think this is satisfactorily resolved.

          Same merge, with same tools, with same parameters = identical weights. Glad
          to see this ended up being a friendly misunderstanding!'
        updatedAt: '2023-12-26T23:39:47.507Z'
      numEdits: 0
      reactions: []
    id: 658b64437549e4e07e6ed35f
    type: comment
  author: acastanza
  content: 'Thanks for the response! Considering both what you''ve said and other
    comments in the thread coming to similar conclusions as to the likely cause, I
    think this is satisfactorily resolved.

    Same merge, with same tools, with same parameters = identical weights. Glad to
    see this ended up being a friendly misunderstanding!'
  created_at: 2023-12-26 23:39:47+00:00
  edited: false
  hidden: false
  id: 658b64437549e4e07e6ed35f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: OpenPipe/mistral-ft-optimized-1218
repo_type: model
status: open
target_branch: null
title: Reddit Comments on Model Weights
