!!python/object:huggingface_hub.community.DiscussionWithDetails
author: deter3
conflicting_files: null
created_at: 2023-12-25 02:30:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dc3bd3c60c100ec7846a0a70a58c38cc.svg
      fullname: richmao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deter3
      type: user
    createdAt: '2023-12-25T02:30:33.000Z'
    data:
      edited: false
      editors:
      - deter3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8898099660873413
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dc3bd3c60c100ec7846a0a70a58c38cc.svg
          fullname: richmao
          isHf: false
          isPro: false
          name: deter3
          type: user
        html: '<p>when using transformers , when the context is getting longer (5572
          words ), A6000 48GB is not enough .  if going for 2 GPUs , The code in model
          card is showing "RuntimeError: Expected all tensors to be on the same device,
          but found at least two devices, cuda:0 and cuda:1!" .</p>

          <p>when using docker deployed in runpod , always got the error "RuntimeError:
          weight gptq_bits does not exist" for A6000 .  (HUGGING_FACE_HUB_TOKEN has
          been setup and model download has no problem , but when doing the sharding
          , got errors )</p>

          '
        raw: "when using transformers , when the context is getting longer (5572 words\
          \ ), A6000 48GB is not enough .  if going for 2 GPUs , The code in model\
          \ card is showing \"RuntimeError: Expected all tensors to be on the same\
          \ device, but found at least two devices, cuda:0 and cuda:1!\" .\r\n\r\n\
          when using docker deployed in runpod , always got the error \"RuntimeError:\
          \ weight gptq_bits does not exist\" for A6000 .  (HUGGING_FACE_HUB_TOKEN\
          \ has been setup and model download has no problem , but when doing the\
          \ sharding , got errors )"
        updatedAt: '2023-12-25T02:30:33.536Z'
      numEdits: 0
      reactions: []
    id: 6588e94974d1a1cbd028a782
    type: comment
  author: deter3
  content: "when using transformers , when the context is getting longer (5572 words\
    \ ), A6000 48GB is not enough .  if going for 2 GPUs , The code in model card\
    \ is showing \"RuntimeError: Expected all tensors to be on the same device, but\
    \ found at least two devices, cuda:0 and cuda:1!\" .\r\n\r\nwhen using docker\
    \ deployed in runpod , always got the error \"RuntimeError: weight gptq_bits does\
    \ not exist\" for A6000 .  (HUGGING_FACE_HUB_TOKEN has been setup and model download\
    \ has no problem , but when doing the sharding , got errors )"
  created_at: 2023-12-25 02:30:33+00:00
  edited: false
  hidden: false
  id: 6588e94974d1a1cbd028a782
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-25T09:05:51.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8625459671020508
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Can you share the full command you are using.</p>\n<p>You\u2019\
          ll need to specify quantization as awq or bitsandbytes NF4 (not gptq) and\
          \ also set the revision (branch) to awq if using awq.</p>\n<p>One A6000\
          \ should get you to 16k . Two should get you to 64k.</p>\n"
        raw: "Can you share the full command you are using.\n\nYou\u2019ll need to\
          \ specify quantization as awq or bitsandbytes NF4 (not gptq) and also set\
          \ the revision (branch) to awq if using awq.\n\nOne A6000 should get you\
          \ to 16k . Two should get you to 64k."
        updatedAt: '2023-12-25T09:05:51.930Z'
      numEdits: 0
      reactions: []
    id: 658945efbbb04840e3add54a
    type: comment
  author: RonanMcGovern
  content: "Can you share the full command you are using.\n\nYou\u2019ll need to specify\
    \ quantization as awq or bitsandbytes NF4 (not gptq) and also set the revision\
    \ (branch) to awq if using awq.\n\nOne A6000 should get you to 16k . Two should\
    \ get you to 64k."
  created_at: 2023-12-25 09:05:51+00:00
  edited: false
  hidden: false
  id: 658945efbbb04840e3add54a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dc3bd3c60c100ec7846a0a70a58c38cc.svg
      fullname: richmao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deter3
      type: user
    createdAt: '2023-12-25T12:12:08.000Z'
    data:
      edited: false
      editors:
      - deter3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6093060374259949
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dc3bd3c60c100ec7846a0a70a58c38cc.svg
          fullname: richmao
          isHf: false
          isPro: false
          name: deter3
          type: user
        html: "<blockquote>\n<p>Can you share the full command you are using.</p>\n\
          <p>You\u2019ll need to specify quantization as awq or bitsandbytes NF4 (not\
          \ gptq) and also set the revision (branch) to awq if using awq.</p>\n<p>One\
          \ A6000 should get you to 16k . Two should get you to 64k.</p>\n</blockquote>\n\
          <p>--model-id Trelis/Mistral-7B-Instruct-v0.1-Summarize-64k --port 8080\
          \ --max-input-length 63000 --max-total-tokens 64000 --max-batch-prefill-tokens\
          \ 64000 --quantize awq --revision awq \uFF0C it's from the template link\
          \ of this model card . you can try it by yourself . </p>\n"
        raw: "> Can you share the full command you are using.\n> \n> You\u2019ll need\
          \ to specify quantization as awq or bitsandbytes NF4 (not gptq) and also\
          \ set the revision (branch) to awq if using awq.\n> \n> One A6000 should\
          \ get you to 16k . Two should get you to 64k.\n\n--model-id Trelis/Mistral-7B-Instruct-v0.1-Summarize-64k\
          \ --port 8080 --max-input-length 63000 --max-total-tokens 64000 --max-batch-prefill-tokens\
          \ 64000 --quantize awq --revision awq \uFF0C it's from the template link\
          \ of this model card . you can try it by yourself . "
        updatedAt: '2023-12-25T12:12:08.814Z'
      numEdits: 0
      reactions: []
    id: 658971981b44d0e69441c12b
    type: comment
  author: deter3
  content: "> Can you share the full command you are using.\n> \n> You\u2019ll need\
    \ to specify quantization as awq or bitsandbytes NF4 (not gptq) and also set the\
    \ revision (branch) to awq if using awq.\n> \n> One A6000 should get you to 16k\
    \ . Two should get you to 64k.\n\n--model-id Trelis/Mistral-7B-Instruct-v0.1-Summarize-64k\
    \ --port 8080 --max-input-length 63000 --max-total-tokens 64000 --max-batch-prefill-tokens\
    \ 64000 --quantize awq --revision awq \uFF0C it's from the template link of this\
    \ model card . you can try it by yourself . "
  created_at: 2023-12-25 12:12:08+00:00
  edited: false
  hidden: false
  id: 658971981b44d0e69441c12b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-25T13:33:12.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9433141946792603
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Sorry for the trouble. TGI is still having issues running Mistral
          models in AWQ.</p>

          <p>I''ve updated the template to use --quantize bitsandbytes-nf4 and tested
          it works now. You can also play around with --quantize eetq, which will
          be faster if you''re using larger batch sizes but may fit slightly less
          context.</p>

          <p>I''ll need to report further on the AWQ issue and will get to this later
          after xmas, cheers</p>

          '
        raw: 'Sorry for the trouble. TGI is still having issues running Mistral models
          in AWQ.


          I''ve updated the template to use --quantize bitsandbytes-nf4 and tested
          it works now. You can also play around with --quantize eetq, which will
          be faster if you''re using larger batch sizes but may fit slightly less
          context.


          I''ll need to report further on the AWQ issue and will get to this later
          after xmas, cheers'
        updatedAt: '2023-12-25T13:33:12.514Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - deter3
    id: 658984981a576242ef37cc9a
    type: comment
  author: RonanMcGovern
  content: 'Sorry for the trouble. TGI is still having issues running Mistral models
    in AWQ.


    I''ve updated the template to use --quantize bitsandbytes-nf4 and tested it works
    now. You can also play around with --quantize eetq, which will be faster if you''re
    using larger batch sizes but may fit slightly less context.


    I''ll need to report further on the AWQ issue and will get to this later after
    xmas, cheers'
  created_at: 2023-12-25 13:33:12+00:00
  edited: false
  hidden: false
  id: 658984981a576242ef37cc9a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dc3bd3c60c100ec7846a0a70a58c38cc.svg
      fullname: richmao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deter3
      type: user
    createdAt: '2023-12-25T15:51:27.000Z'
    data:
      edited: false
      editors:
      - deter3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.931905210018158
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dc3bd3c60c100ec7846a0a70a58c38cc.svg
          fullname: richmao
          isHf: false
          isPro: false
          name: deter3
          type: user
        html: '<blockquote>

          <p>Sorry for the trouble. TGI is still having issues running Mistral models
          in AWQ.</p>

          <p>I''ve updated the template to use --quantize bitsandbytes-nf4 and tested
          it works now. You can also play around with --quantize eetq, which will
          be faster if you''re using larger batch sizes but may fit slightly less
          context.</p>

          <p>I''ll need to report further on the AWQ issue and will get to this later
          after xmas, cheers</p>

          </blockquote>

          <p>thanks for the quick response , Ronan , let''s deal with it after xmas
          . Cheers . </p>

          '
        raw: "> Sorry for the trouble. TGI is still having issues running Mistral\
          \ models in AWQ.\n> \n> I've updated the template to use --quantize bitsandbytes-nf4\
          \ and tested it works now. You can also play around with --quantize eetq,\
          \ which will be faster if you're using larger batch sizes but may fit slightly\
          \ less context.\n> \n> I'll need to report further on the AWQ issue and\
          \ will get to this later after xmas, cheers\n\nthanks for the quick response\
          \ , Ronan , let's deal with it after xmas . Cheers . "
        updatedAt: '2023-12-25T15:51:27.047Z'
      numEdits: 0
      reactions: []
    id: 6589a4ff16a6a0082040e006
    type: comment
  author: deter3
  content: "> Sorry for the trouble. TGI is still having issues running Mistral models\
    \ in AWQ.\n> \n> I've updated the template to use --quantize bitsandbytes-nf4\
    \ and tested it works now. You can also play around with --quantize eetq, which\
    \ will be faster if you're using larger batch sizes but may fit slightly less\
    \ context.\n> \n> I'll need to report further on the AWQ issue and will get to\
    \ this later after xmas, cheers\n\nthanks for the quick response , Ronan , let's\
    \ deal with it after xmas . Cheers . "
  created_at: 2023-12-25 15:51:27+00:00
  edited: false
  hidden: false
  id: 6589a4ff16a6a0082040e006
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-26T10:43:07.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8540694117546082
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Howdy <span data-props=\"{&quot;user&quot;:&quot;deter3&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/deter3\"\
          >@<span class=\"underline\">deter3</span></a></span>\n\n\t</span></span>\
          \ , I've just tested this out completely, have a look <a rel=\"nofollow\"\
          \ href=\"https://share.descript.com/view/MVTRldkty24\">here for the full\
          \ video</a>:</p>\n"
        raw: 'Howdy @deter3 , I''ve just tested this out completely, have a look [here
          for the full video](https://share.descript.com/view/MVTRldkty24):


          '
        updatedAt: '2023-12-26T10:43:07.745Z'
      numEdits: 0
      reactions: []
      relatedEventId: 658aae3b35f23c0f1c6350b6
    id: 658aae3b35f23c0f1c6350b5
    type: comment
  author: RonanMcGovern
  content: 'Howdy @deter3 , I''ve just tested this out completely, have a look [here
    for the full video](https://share.descript.com/view/MVTRldkty24):


    '
  created_at: 2023-12-26 10:43:07+00:00
  edited: false
  hidden: false
  id: 658aae3b35f23c0f1c6350b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-26T10:43:07.000Z'
    data:
      status: closed
    id: 658aae3b35f23c0f1c6350b6
    type: status-change
  author: RonanMcGovern
  created_at: 2023-12-26 10:43:07+00:00
  id: 658aae3b35f23c0f1c6350b6
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dc3bd3c60c100ec7846a0a70a58c38cc.svg
      fullname: richmao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deter3
      type: user
    createdAt: '2023-12-27T15:22:30.000Z'
    data:
      edited: false
      editors:
      - deter3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8348661661148071
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dc3bd3c60c100ec7846a0a70a58c38cc.svg
          fullname: richmao
          isHf: false
          isPro: false
          name: deter3
          type: user
        html: '<p>Hi , Ronan,<br>Below is the problem I came cross , Can you let me
          know what''s the root cause for this problem ? I am using 64k model more
          about a Q&amp;A purpose than summation  .  Shall I train for myself or i
          can tweak this model to fit my purpose ? We can talk about it after xmas
          . Cheers !!<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/639246f17685a6b68f5eaaae/B-GvSFvsDLZVGqKAcT7xE.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/639246f17685a6b68f5eaaae/B-GvSFvsDLZVGqKAcT7xE.png"></a></p>

          '
        raw: "Hi , Ronan,  \nBelow is the problem I came cross , Can you let me know\
          \ what's the root cause for this problem ? I am using 64k model more about\
          \ a Q&A purpose than summation  .  Shall I train for myself or i can tweak\
          \ this model to fit my purpose ? We can talk about it after xmas . Cheers\
          \ !!\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/639246f17685a6b68f5eaaae/B-GvSFvsDLZVGqKAcT7xE.png)\n"
        updatedAt: '2023-12-27T15:22:30.327Z'
      numEdits: 0
      reactions: []
    id: 658c41368a3ef26890d149cf
    type: comment
  author: deter3
  content: "Hi , Ronan,  \nBelow is the problem I came cross , Can you let me know\
    \ what's the root cause for this problem ? I am using 64k model more about a Q&A\
    \ purpose than summation  .  Shall I train for myself or i can tweak this model\
    \ to fit my purpose ? We can talk about it after xmas . Cheers !!\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/639246f17685a6b68f5eaaae/B-GvSFvsDLZVGqKAcT7xE.png)\n"
  created_at: 2023-12-27 15:22:30+00:00
  edited: false
  hidden: false
  id: 658c41368a3ef26890d149cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-28T09:52:42.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7482580542564392
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Before going to the trouble of training, I recommend:</p>

          <ol>

          <li>Using the prompt on the model card, minus the summarization. Instead
          of:</li>

          </ol>

          <pre><code>B_INST, E_INST = "[INST] ", " [/INST]"

          prompt = {B_INST}Provide a summary of the following text:\n\n[TEXT_START]\n\n{text
          to summarize}\n\n[TEXT_END]\n\n{E_INST}

          </code></pre>

          <p>Use something like:</p>

          <pre><code>prompt= #put your question here

          B_INST, E_INST = "[INST] ", " [/INST]"

          formatted_prompt = f"{B_INST}{prompt}{E_INST}"

          </code></pre>

          <p>That should help quite a bit.<br>2. Test on 50k tokens (just a little
          lower than the upper bound).</p>

          <ol start="3">

          <li>If the above doesn''t work, you can add the following:</li>

          </ol>

          <pre><code>B_INST, E_INST = "[INST] ", " [/INST]"

          systemPrompt=f"The following is a discussion between a user and a helpful
          assistant. {B_INST} is pre-pended to user messages and {E_INST} is pre-pended
          to assistant messages."

          prompt= #put your question here

          formatted_prompt = f"{systemPrompt}\n\n{B_INST}{prompt}{E_INST}"

          </code></pre>

          '
        raw: 'Before going to the trouble of training, I recommend:


          1. Using the prompt on the model card, minus the summarization. Instead
          of:

          ```

          B_INST, E_INST = "[INST] ", " [/INST]"

          prompt = {B_INST}Provide a summary of the following text:\n\n[TEXT_START]\n\n{text
          to summarize}\n\n[TEXT_END]\n\n{E_INST}

          ```

          Use something like:

          ```

          prompt= #put your question here

          B_INST, E_INST = "[INST] ", " [/INST]"

          formatted_prompt = f"{B_INST}{prompt}{E_INST}"

          ```

          That should help quite a bit.

          2. Test on 50k tokens (just a little lower than the upper bound).


          3. If the above doesn''t work, you can add the following:

          ```

          B_INST, E_INST = "[INST] ", " [/INST]"

          systemPrompt=f"The following is a discussion between a user and a helpful
          assistant. {B_INST} is pre-pended to user messages and {E_INST} is pre-pended
          to assistant messages."

          prompt= #put your question here

          formatted_prompt = f"{systemPrompt}\n\n{B_INST}{prompt}{E_INST}"

          ```

          '
        updatedAt: '2023-12-28T09:52:42.938Z'
      numEdits: 0
      reactions: []
    id: 658d456ad28043a0068b599a
    type: comment
  author: RonanMcGovern
  content: 'Before going to the trouble of training, I recommend:


    1. Using the prompt on the model card, minus the summarization. Instead of:

    ```

    B_INST, E_INST = "[INST] ", " [/INST]"

    prompt = {B_INST}Provide a summary of the following text:\n\n[TEXT_START]\n\n{text
    to summarize}\n\n[TEXT_END]\n\n{E_INST}

    ```

    Use something like:

    ```

    prompt= #put your question here

    B_INST, E_INST = "[INST] ", " [/INST]"

    formatted_prompt = f"{B_INST}{prompt}{E_INST}"

    ```

    That should help quite a bit.

    2. Test on 50k tokens (just a little lower than the upper bound).


    3. If the above doesn''t work, you can add the following:

    ```

    B_INST, E_INST = "[INST] ", " [/INST]"

    systemPrompt=f"The following is a discussion between a user and a helpful assistant.
    {B_INST} is pre-pended to user messages and {E_INST} is pre-pended to assistant
    messages."

    prompt= #put your question here

    formatted_prompt = f"{systemPrompt}\n\n{B_INST}{prompt}{E_INST}"

    ```

    '
  created_at: 2023-12-28 09:52:42+00:00
  edited: false
  hidden: false
  id: 658d456ad28043a0068b599a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-28T09:52:45.000Z'
    data:
      status: open
    id: 658d456d79c9c9b95f54d278
    type: status-change
  author: RonanMcGovern
  created_at: 2023-12-28 09:52:45+00:00
  id: 658d456d79c9c9b95f54d278
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dc3bd3c60c100ec7846a0a70a58c38cc.svg
      fullname: richmao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deter3
      type: user
    createdAt: '2023-12-29T15:29:58.000Z'
    data:
      edited: false
      editors:
      - deter3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.749127209186554
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dc3bd3c60c100ec7846a0a70a58c38cc.svg
          fullname: richmao
          isHf: false
          isPro: false
          name: deter3
          type: user
        html: '<blockquote>

          <p>Before going to the trouble of training, I recommend:</p>

          <ol>

          <li>Using the prompt on the model card, minus the summarization. Instead
          of:</li>

          </ol>

          <pre><code>B_INST, E_INST = "[INST] ", " [/INST]"

          prompt = {B_INST}Provide a summary of the following text:\n\n[TEXT_START]\n\n{text
          to summarize}\n\n[TEXT_END]\n\n{E_INST}

          </code></pre>

          <p>Use something like:</p>

          <pre><code>prompt= #put your question here

          B_INST, E_INST = "[INST] ", " [/INST]"

          formatted_prompt = f"{B_INST}{prompt}{E_INST}"

          </code></pre>

          <p>That should help quite a bit.<br>2. Test on 50k tokens (just a little
          lower than the upper bound).</p>

          <ol start="3">

          <li>If the above doesn''t work, you can add the following:</li>

          </ol>

          <pre><code>B_INST, E_INST = "[INST] ", " [/INST]"

          systemPrompt=f"The following is a discussion between a user and a helpful
          assistant. {B_INST} is pre-pended to user messages and {E_INST} is pre-pended
          to assistant messages."

          prompt= #put your question here

          formatted_prompt = f"{systemPrompt}\n\n{B_INST}{prompt}{E_INST}"

          </code></pre>

          </blockquote>

          <p>Thanks for the information . Tried and I figured my use case need more
          reasoning in Q&amp;A  , so the result is not good . </p>

          '
        raw: "> Before going to the trouble of training, I recommend:\n> \n> 1. Using\
          \ the prompt on the model card, minus the summarization. Instead of:\n>\
          \ ```\n> B_INST, E_INST = \"[INST] \", \" [/INST]\"\n> prompt = {B_INST}Provide\
          \ a summary of the following text:\\n\\n[TEXT_START]\\n\\n{text to summarize}\\\
          n\\n[TEXT_END]\\n\\n{E_INST}\n> ```\n> Use something like:\n> ```\n> prompt=\
          \ #put your question here\n> B_INST, E_INST = \"[INST] \", \" [/INST]\"\n\
          > formatted_prompt = f\"{B_INST}{prompt}{E_INST}\"\n> ```\n> That should\
          \ help quite a bit.\n> 2. Test on 50k tokens (just a little lower than the\
          \ upper bound).\n> \n> 3. If the above doesn't work, you can add the following:\n\
          > ```\n> B_INST, E_INST = \"[INST] \", \" [/INST]\"\n> systemPrompt=f\"\
          The following is a discussion between a user and a helpful assistant. {B_INST}\
          \ is pre-pended to user messages and {E_INST} is pre-pended to assistant\
          \ messages.\"\n> prompt= #put your question here\n> formatted_prompt = f\"\
          {systemPrompt}\\n\\n{B_INST}{prompt}{E_INST}\"\n> ```\n\nThanks for the\
          \ information . Tried and I figured my use case need more reasoning in Q&A\
          \  , so the result is not good . "
        updatedAt: '2023-12-29T15:29:58.023Z'
      numEdits: 0
      reactions: []
    id: 658ee5f62a0a886ef0ebcad9
    type: comment
  author: deter3
  content: "> Before going to the trouble of training, I recommend:\n> \n> 1. Using\
    \ the prompt on the model card, minus the summarization. Instead of:\n> ```\n\
    > B_INST, E_INST = \"[INST] \", \" [/INST]\"\n> prompt = {B_INST}Provide a summary\
    \ of the following text:\\n\\n[TEXT_START]\\n\\n{text to summarize}\\n\\n[TEXT_END]\\\
    n\\n{E_INST}\n> ```\n> Use something like:\n> ```\n> prompt= #put your question\
    \ here\n> B_INST, E_INST = \"[INST] \", \" [/INST]\"\n> formatted_prompt = f\"\
    {B_INST}{prompt}{E_INST}\"\n> ```\n> That should help quite a bit.\n> 2. Test\
    \ on 50k tokens (just a little lower than the upper bound).\n> \n> 3. If the above\
    \ doesn't work, you can add the following:\n> ```\n> B_INST, E_INST = \"[INST]\
    \ \", \" [/INST]\"\n> systemPrompt=f\"The following is a discussion between a\
    \ user and a helpful assistant. {B_INST} is pre-pended to user messages and {E_INST}\
    \ is pre-pended to assistant messages.\"\n> prompt= #put your question here\n\
    > formatted_prompt = f\"{systemPrompt}\\n\\n{B_INST}{prompt}{E_INST}\"\n> ```\n\
    \nThanks for the information . Tried and I figured my use case need more reasoning\
    \ in Q&A  , so the result is not good . "
  created_at: 2023-12-29 15:29:58+00:00
  edited: false
  hidden: false
  id: 658ee5f62a0a886ef0ebcad9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-30T10:44:41.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9387486577033997
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Ok, interesting. Hard for me to say without seeing an example, but
          depending on what the model is doing, it may mean:</p>

          <ul>

          <li>prompt needs improvement</li>

          <li>Mistral 7B is not strong enough so a stronger model is needed (e.g.
          you could try Yi 34B, which has a function calling chat fine-tuned version).</li>

          </ul>

          '
        raw: 'Ok, interesting. Hard for me to say without seeing an example, but depending
          on what the model is doing, it may mean:

          - prompt needs improvement

          - Mistral 7B is not strong enough so a stronger model is needed (e.g. you
          could try Yi 34B, which has a function calling chat fine-tuned version).'
        updatedAt: '2023-12-30T10:44:41.035Z'
      numEdits: 0
      reactions: []
      relatedEventId: 658ff499315340de5f2bfaaf
    id: 658ff499315340de5f2bfaa5
    type: comment
  author: RonanMcGovern
  content: 'Ok, interesting. Hard for me to say without seeing an example, but depending
    on what the model is doing, it may mean:

    - prompt needs improvement

    - Mistral 7B is not strong enough so a stronger model is needed (e.g. you could
    try Yi 34B, which has a function calling chat fine-tuned version).'
  created_at: 2023-12-30 10:44:41+00:00
  edited: false
  hidden: false
  id: 658ff499315340de5f2bfaa5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-30T10:44:41.000Z'
    data:
      status: closed
    id: 658ff499315340de5f2bfaaf
    type: status-change
  author: RonanMcGovern
  created_at: 2023-12-30 10:44:41+00:00
  id: 658ff499315340de5f2bfaaf
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dc3bd3c60c100ec7846a0a70a58c38cc.svg
      fullname: richmao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deter3
      type: user
    createdAt: '2024-01-02T11:06:11.000Z'
    data:
      edited: false
      editors:
      - deter3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8232294321060181
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dc3bd3c60c100ec7846a0a70a58c38cc.svg
          fullname: richmao
          isHf: false
          isPro: false
          name: deter3
          type: user
        html: "<p>I pulled couple of news from New York Times ,  put all news into\
          \ one txt file . One of the news is <a rel=\"nofollow\" href=\"https://www.nytimes.com/2024/01/01/world/asia/south-korea-opposition-lee-jae-myung.html\"\
          >https://www.nytimes.com/2024/01/01/world/asia/south-korea-opposition-lee-jae-myung.html</a>\
          \ .  I want to test this 64k model be able to answer the question with factual\
          \ information from the context .</p>\n<p>What I found are </p>\n<ol>\n<li><p>if\
          \ there is information for the question , the answer can be good most of\
          \ time . But if there's no information for the question , then the answer\
          \ is full of  fabrication or keeping repeating till max token . I guess\
          \ the fine tune training material does not include such situation . </p>\n\
          </li>\n<li><p>the 64k model is not following the instructions well , i guess\
          \ it's originally from base model .</p>\n</li>\n</ol>\n<p>Code : </p>\n\
          <p>with open('./data/text6.txt', 'r') as f:<br>    text = f.read()</p>\n\
          <p>question = \"\"\"<br>what's Lee Jae-myung main ideas and Philosophical\
          \ thought?<br>\"\"\"</p>\n<p>prompt = f\"\"\"<br>Your tasks are :</p>\n\
          <ol>\n<li>Based on the  solely , Please provide around 200-300 word answer\
          \ to the following . Do not make things up . if  is not enough to answer\
          \ the  , please reply \"I don't know\" .</li>\n<li>Also provide a concise\
          \ 50-100 word summary of any relevant statistics, facts, or case studies\
          \ that support your answer.</li>\n<li>Also provide a concise 50-100 word\
          \ explanation of the key logical reasoning, conceptual framework, or analysis\
          \ that underpins your answer.</li>\n</ol>\n\n{question}\n\n\n\n{text}\n\n\
          \"\"\"\nB_INST, E_INST = \"[INST] \", \" [/INST]\"\nheaders = {\n    \"\
          Content-Type\": \"application/json\",\n}\n\n<p>data = {<br>    'inputs':\
          \ f'{B_INST}{prompt}{E_INST}',<br>    'parameters': {<br>        'max_new_tokens':\
          \ 4000,<br>        'temperature': 0.01<br>    },<br>}</p>\n<p>response =\
          \ requests.post('<a rel=\"nofollow\" href=\"https://2s.proxy.runpod.net/generate'\"\
          >https://2s.proxy.runpod.net/generate'</a>, headers=headers, json=data)<br>try:<br>\
          \    data1=response.json()<br>    print(data1[\"generated_text\"])<br>except\
          \ Exception as e:<br>    print(e)<br>    print(response.text)</p>\n"
        raw: "I pulled couple of news from New York Times ,  put all news into one\
          \ txt file . One of the news is https://www.nytimes.com/2024/01/01/world/asia/south-korea-opposition-lee-jae-myung.html\
          \ .  I want to test this 64k model be able to answer the question with factual\
          \ information from the context .\n\nWhat I found are \n\n1.  if there is\
          \ information for the question , the answer can be good most of time . But\
          \ if there's no information for the question , then the answer is full of\
          \  fabrication or keeping repeating till max token . I guess the fine tune\
          \ training material does not include such situation . \n\n2. the 64k model\
          \ is not following the instructions well , i guess it's originally from\
          \ base model .  \n\nCode : \n\nwith open('./data/text6.txt', 'r') as f:\n\
          \    text = f.read()\n    \nquestion = \"\"\"\nwhat's Lee Jae-myung main\
          \ ideas and Philosophical thought? \n\"\"\"\n\nprompt = f\"\"\"\nYour tasks\
          \ are :\n1. Based on the <context> solely , Please provide around 200-300\
          \ word answer to the following <question>. Do not make things up . if <context>\
          \ is not enough to answer the <question> , please reply \"I don't know\"\
          \ .\n2. Also provide a concise 50-100 word summary of any relevant statistics,\
          \ facts, or case studies that support your answer.\n3. Also provide a concise\
          \ 50-100 word explanation of the key logical reasoning, conceptual framework,\
          \ or analysis that underpins your answer.\n\n<question>\n{question}\n</question>\n\
          \n<context>\n{text}\n</context>\n\"\"\"\nB_INST, E_INST = \"[INST] \", \"\
          \ [/INST]\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n\
          }\n\ndata = {\n    'inputs': f'{B_INST}{prompt}{E_INST}',\n    'parameters':\
          \ {\n        'max_new_tokens': 4000,\n        'temperature': 0.01\n    },\n\
          }\n\nresponse = requests.post('https://2s.proxy.runpod.net/generate', headers=headers,\
          \ json=data)\ntry:\n    data1=response.json()\n    print(data1[\"generated_text\"\
          ])\nexcept Exception as e:\n    print(e)\n    print(response.text)"
        updatedAt: '2024-01-02T11:06:11.662Z'
      numEdits: 0
      reactions: []
    id: 6593ee235070805494d5ed09
    type: comment
  author: deter3
  content: "I pulled couple of news from New York Times ,  put all news into one txt\
    \ file . One of the news is https://www.nytimes.com/2024/01/01/world/asia/south-korea-opposition-lee-jae-myung.html\
    \ .  I want to test this 64k model be able to answer the question with factual\
    \ information from the context .\n\nWhat I found are \n\n1.  if there is information\
    \ for the question , the answer can be good most of time . But if there's no information\
    \ for the question , then the answer is full of  fabrication or keeping repeating\
    \ till max token . I guess the fine tune training material does not include such\
    \ situation . \n\n2. the 64k model is not following the instructions well , i\
    \ guess it's originally from base model .  \n\nCode : \n\nwith open('./data/text6.txt',\
    \ 'r') as f:\n    text = f.read()\n    \nquestion = \"\"\"\nwhat's Lee Jae-myung\
    \ main ideas and Philosophical thought? \n\"\"\"\n\nprompt = f\"\"\"\nYour tasks\
    \ are :\n1. Based on the <context> solely , Please provide around 200-300 word\
    \ answer to the following <question>. Do not make things up . if <context> is\
    \ not enough to answer the <question> , please reply \"I don't know\" .\n2. Also\
    \ provide a concise 50-100 word summary of any relevant statistics, facts, or\
    \ case studies that support your answer.\n3. Also provide a concise 50-100 word\
    \ explanation of the key logical reasoning, conceptual framework, or analysis\
    \ that underpins your answer.\n\n<question>\n{question}\n</question>\n\n<context>\n\
    {text}\n</context>\n\"\"\"\nB_INST, E_INST = \"[INST] \", \" [/INST]\"\nheaders\
    \ = {\n    \"Content-Type\": \"application/json\",\n}\n\ndata = {\n    'inputs':\
    \ f'{B_INST}{prompt}{E_INST}',\n    'parameters': {\n        'max_new_tokens':\
    \ 4000,\n        'temperature': 0.01\n    },\n}\n\nresponse = requests.post('https://2s.proxy.runpod.net/generate',\
    \ headers=headers, json=data)\ntry:\n    data1=response.json()\n    print(data1[\"\
    generated_text\"])\nexcept Exception as e:\n    print(e)\n    print(response.text)"
  created_at: 2024-01-02 11:06:11+00:00
  edited: false
  hidden: false
  id: 6593ee235070805494d5ed09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-02T14:16:25.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9659721851348877
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Howdy <span data-props=\"{&quot;user&quot;:&quot;deter3&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/deter3\"\
          >@<span class=\"underline\">deter3</span></a></span>\n\n\t</span></span>\
          \ .</p>\n<ol>\n<li><p>Yes, you're seeing hallucination. This is a problem\
          \ across all llms, particularly smaller ones.</p>\n</li>\n<li><p>Yes, makes\
          \ sense the model isn't following your instruction as it differs from the\
          \ prompt template this model was tuned on. Small models aren't good at one-shot\
          \ or zero shot performance.</p>\n</li>\n</ol>\n<p>To get better performance,\
          \ I think you need a larger model (maybe Yi 34B 200k model). If you want\
          \ a similar size, OpenChat 3.5 is probably better than Mistral 7B (although\
          \ openchat would need to be fine-tuned for longer context).</p>\n"
        raw: 'Howdy @deter3 .


          1. Yes, you''re seeing hallucination. This is a problem across all llms,
          particularly smaller ones.


          2. Yes, makes sense the model isn''t following your instruction as it differs
          from the prompt template this model was tuned on. Small models aren''t good
          at one-shot or zero shot performance.


          To get better performance, I think you need a larger model (maybe Yi 34B
          200k model). If you want a similar size, OpenChat 3.5 is probably better
          than Mistral 7B (although openchat would need to be fine-tuned for longer
          context).'
        updatedAt: '2024-01-02T14:16:25.435Z'
      numEdits: 0
      reactions: []
    id: 65941ab91adf6d577e2d868a
    type: comment
  author: RonanMcGovern
  content: 'Howdy @deter3 .


    1. Yes, you''re seeing hallucination. This is a problem across all llms, particularly
    smaller ones.


    2. Yes, makes sense the model isn''t following your instruction as it differs
    from the prompt template this model was tuned on. Small models aren''t good at
    one-shot or zero shot performance.


    To get better performance, I think you need a larger model (maybe Yi 34B 200k
    model). If you want a similar size, OpenChat 3.5 is probably better than Mistral
    7B (although openchat would need to be fine-tuned for longer context).'
  created_at: 2024-01-02 14:16:25+00:00
  edited: false
  hidden: false
  id: 65941ab91adf6d577e2d868a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Trelis/Mistral-7B-Instruct-v0.1-Summarize-64k
repo_type: model
status: closed
target_branch: null
title: Problems of using this mdoel
