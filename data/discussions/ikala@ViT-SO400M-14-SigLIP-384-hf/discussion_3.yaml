!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rwightman
conflicting_files: null
created_at: 2023-12-04 17:28:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2023-12-04T17:28:29.000Z'
    data:
      edited: true
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9620688557624817
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: '<p>I don''t want to be a downer here, but this approach to SigLIP models
          will not work, they are not compatible with CLIP. It is misleading to suggest
          that this could work. It''s best to wait for <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/26522">https://github.com/huggingface/transformers/pull/26522</a>
          to land before Transformers SigLIP models are available. </p>

          <ul>

          <li>CLIP ViT has no bias on the patch embed and has an extra pre-norm after
          the patch embed &amp; pos embed. That means the signal is irreconcilably
          wrong for this approach after the first layer</li>

          <li>The missing attention pooling at the end of the vision model is a significant
          difference</li>

          <li>The pooling at the end of the text model is notably different too</li>

          <li>Text tokenization requires an bit of code to perform cleaning for full
          accuracy so this is being implemented as a new tokenizer in the Transformers
          port and was implemented as part of a tokenizer wrapper in OpenCLIP.</li>

          </ul>

          '
        raw: "I don't want to be a downer here, but this approach to SigLIP models\
          \ will not work, they are not compatible with CLIP. It is misleading to\
          \ suggest that this could work. It's best to wait for https://github.com/huggingface/transformers/pull/26522\
          \ to land before Transformers SigLIP models are available. \n\n* CLIP ViT\
          \ has no bias on the patch embed and has an extra pre-norm after the patch\
          \ embed & pos embed. That means the signal is irreconcilably wrong for this\
          \ approach after the first layer\n* The missing attention pooling at the\
          \ end of the vision model is a significant difference\n* The pooling at\
          \ the end of the text model is notably different too\n* Text tokenization\
          \ requires an bit of code to perform cleaning for full accuracy so this\
          \ is being implemented as a new tokenizer in the Transformers port and was\
          \ implemented as part of a tokenizer wrapper in OpenCLIP.\n"
        updatedAt: '2023-12-04T17:37:44.068Z'
      numEdits: 1
      reactions: []
    id: 656e0c3d801ed9952ff9667a
    type: comment
  author: rwightman
  content: "I don't want to be a downer here, but this approach to SigLIP models will\
    \ not work, they are not compatible with CLIP. It is misleading to suggest that\
    \ this could work. It's best to wait for https://github.com/huggingface/transformers/pull/26522\
    \ to land before Transformers SigLIP models are available. \n\n* CLIP ViT has\
    \ no bias on the patch embed and has an extra pre-norm after the patch embed &\
    \ pos embed. That means the signal is irreconcilably wrong for this approach after\
    \ the first layer\n* The missing attention pooling at the end of the vision model\
    \ is a significant difference\n* The pooling at the end of the text model is notably\
    \ different too\n* Text tokenization requires an bit of code to perform cleaning\
    \ for full accuracy so this is being implemented as a new tokenizer in the Transformers\
    \ port and was implemented as part of a tokenizer wrapper in OpenCLIP.\n"
  created_at: 2023-12-04 17:28:29+00:00
  edited: true
  hidden: false
  id: 656e0c3d801ed9952ff9667a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: ikala/ViT-SO400M-14-SigLIP-384-hf
repo_type: model
status: open
target_branch: null
title: This will not work
