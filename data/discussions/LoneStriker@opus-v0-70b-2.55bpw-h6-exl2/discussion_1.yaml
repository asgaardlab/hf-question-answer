!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AbiTabby
conflicting_files: null
created_at: 2023-11-15 08:42:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fe12a9f4567bafbe9afdb1105cfdc80e.svg
      fullname: Abigail
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbiTabby
      type: user
    createdAt: '2023-11-15T08:42:58.000Z'
    data:
      edited: false
      editors:
      - AbiTabby
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9024064540863037
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fe12a9f4567bafbe9afdb1105cfdc80e.svg
          fullname: Abigail
          isHf: false
          isPro: false
          name: AbiTabby
          type: user
        html: '<p>Firstly thanks for all the exl2 conversions.<br>But can I ask why
          you dropped output from  your regular 2.6 to 2.55bpw?<br>The difference
          may well be insignificant, but I always found 2.6bpw to be a sweet spot
          on my setup (28GB Vram).</p>

          '
        raw: "Firstly thanks for all the exl2 conversions.\r\nBut can I ask why you\
          \ dropped output from  your regular 2.6 to 2.55bpw?\r\nThe difference may\
          \ well be insignificant, but I always found 2.6bpw to be a sweet spot on\
          \ my setup (28GB Vram)."
        updatedAt: '2023-11-15T08:42:58.202Z'
      numEdits: 0
      reactions: []
    id: 6554849237b3d85662a7a900
    type: comment
  author: AbiTabby
  content: "Firstly thanks for all the exl2 conversions.\r\nBut can I ask why you\
    \ dropped output from  your regular 2.6 to 2.55bpw?\r\nThe difference may well\
    \ be insignificant, but I always found 2.6bpw to be a sweet spot on my setup (28GB\
    \ Vram)."
  created_at: 2023-11-15 08:42:58+00:00
  edited: false
  hidden: false
  id: 6554849237b3d85662a7a900
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-15T16:38:30.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9864810705184937
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Mixed messages from folks requesting different bpw. I believe 2.55
          was enough to get some folks extra context length. 2.6 was originally selected
          as that was what Turboderp said would fit in a single 24GB VRAM card. I
          can switch back to 2.6.</p>

          '
        raw: Mixed messages from folks requesting different bpw. I believe 2.55 was
          enough to get some folks extra context length. 2.6 was originally selected
          as that was what Turboderp said would fit in a single 24GB VRAM card. I
          can switch back to 2.6.
        updatedAt: '2023-11-15T16:38:30.914Z'
      numEdits: 0
      reactions: []
    id: 6554f406710bb1dad2012eb7
    type: comment
  author: LoneStriker
  content: Mixed messages from folks requesting different bpw. I believe 2.55 was
    enough to get some folks extra context length. 2.6 was originally selected as
    that was what Turboderp said would fit in a single 24GB VRAM card. I can switch
    back to 2.6.
  created_at: 2023-11-15 16:38:30+00:00
  edited: false
  hidden: false
  id: 6554f406710bb1dad2012eb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fe12a9f4567bafbe9afdb1105cfdc80e.svg
      fullname: Abigail
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbiTabby
      type: user
    createdAt: '2023-11-15T19:18:19.000Z'
    data:
      edited: false
      editors:
      - AbiTabby
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9627748727798462
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fe12a9f4567bafbe9afdb1105cfdc80e.svg
          fullname: Abigail
          isHf: false
          isPro: false
          name: AbiTabby
          type: user
        html: "<p>If more people are requesting 2.55bpw, then stick with it. Though\
          \ I would think that 2.4 is a better fit for those with 24GB VRAM, depending\
          \ on what other programs they have open that are using VRAM.<br>Whatever\
          \ you decide I'm very grateful for your quants.<br>Currently driving the\
          \ Nous-Capybara-34B-5.0bpw that you posted, and finding it very responsive\
          \ and coherent. But for some reason Alpaca Instruct is working better than\
          \ Vicuna via ST frontend.<br>Anyway take care \U0001F603</p>\n"
        raw: "If more people are requesting 2.55bpw, then stick with it. Though I\
          \ would think that 2.4 is a better fit for those with 24GB VRAM, depending\
          \ on what other programs they have open that are using VRAM.\nWhatever you\
          \ decide I'm very grateful for your quants.\nCurrently driving the Nous-Capybara-34B-5.0bpw\
          \ that you posted, and finding it very responsive and coherent. But for\
          \ some reason Alpaca Instruct is working better than Vicuna via ST frontend.\n\
          Anyway take care \U0001F603"
        updatedAt: '2023-11-15T19:18:19.713Z'
      numEdits: 0
      reactions: []
    id: 6555197b9dc61e22c5fcfbc3
    type: comment
  author: AbiTabby
  content: "If more people are requesting 2.55bpw, then stick with it. Though I would\
    \ think that 2.4 is a better fit for those with 24GB VRAM, depending on what other\
    \ programs they have open that are using VRAM.\nWhatever you decide I'm very grateful\
    \ for your quants.\nCurrently driving the Nous-Capybara-34B-5.0bpw that you posted,\
    \ and finding it very responsive and coherent. But for some reason Alpaca Instruct\
    \ is working better than Vicuna via ST frontend.\nAnyway take care \U0001F603"
  created_at: 2023-11-15 19:18:19+00:00
  edited: false
  hidden: false
  id: 6555197b9dc61e22c5fcfbc3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-15T19:49:24.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.851775586605072
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Things are getting even a bit more complicated. If you enable the
          <code>cache_8bit</code> option for the ExLlamav2 loader, you can fit even
          more bits; supposedly only trading speed of inference for more VRAM space
          (no degradation of quality):<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/rsPAraHD7dKovpmsd7TYe.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/rsPAraHD7dKovpmsd7TYe.png"></a></p>

          <p>We may have to do another calibration of what the best bpw settings are
          again with and without the <code>cache_8bit</code> option.</p>

          '
        raw: 'Things are getting even a bit more complicated. If you enable the `cache_8bit`
          option for the ExLlamav2 loader, you can fit even more bits; supposedly
          only trading speed of inference for more VRAM space (no degradation of quality):

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/rsPAraHD7dKovpmsd7TYe.png)


          We may have to do another calibration of what the best bpw settings are
          again with and without the `cache_8bit` option.'
        updatedAt: '2023-11-15T19:49:24.430Z'
      numEdits: 0
      reactions: []
    id: 655520c4f0cf8603f35f0f57
    type: comment
  author: LoneStriker
  content: 'Things are getting even a bit more complicated. If you enable the `cache_8bit`
    option for the ExLlamav2 loader, you can fit even more bits; supposedly only trading
    speed of inference for more VRAM space (no degradation of quality):

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6331f59718711776b46afb5e/rsPAraHD7dKovpmsd7TYe.png)


    We may have to do another calibration of what the best bpw settings are again
    with and without the `cache_8bit` option.'
  created_at: 2023-11-15 19:49:24+00:00
  edited: false
  hidden: false
  id: 655520c4f0cf8603f35f0f57
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/opus-v0-70b-2.55bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: Change from 2.6 to 2.55bpw
