!!python/object:huggingface_hub.community.DiscussionWithDetails
author: perelmanych
conflicting_files: null
created_at: 2023-08-02 06:48:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
      fullname: Roman Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: perelmanych
      type: user
    createdAt: '2023-08-02T07:48:51.000Z'
    data:
      edited: false
      editors:
      - perelmanych
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4580501616001129
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
          fullname: Roman Ivanov
          isHf: false
          isPro: false
          name: perelmanych
          type: user
        html: "<p>Hi thanks for your work! In my case only AutoGPTQ works, others\
          \ give the following errors.</p>\n<p>With ExLlama:<br>Traceback (most recent\
          \ call last): File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 68, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name, loader) File \u201CC:\\AI\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 78, in load_model\
          \ output = load_func_maploader File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 293, in ExLlama_loader model, tokenizer =\
          \ ExllamaModel.from_pretrained(model_name) File \u201CC:\\AI\\oobabooga_windows\\\
          text-generation-webui\\modules\\exllama.py\u201D, line 49, in from_pretrained\
          \ config = ExLlamaConfig(str(model_config_path)) File \u201CC:\\AI\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\exllama\\model.py\u201D, line\
          \ 52, in init self.pad_token_id = read_config[\u201Cpad_token_id\u201D]\
          \ KeyError: \u2018pad_token_id\u2019</p>\n<p>I have tried to add 'pad_token_id'\
          \ manually, but then I got errors about other missing tokens.</p>\n<p>With\
          \ GPTQ-for-LLaMa:<br>Traceback (most recent call last): File \u201CC:\\\
          AI\\oobabooga_windows\\text-generation-webui\\server.py\u201D, line 68,\
          \ in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader) File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 78, in load_model output = load_func_maploader\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 279, in GPTQ_loader model = modules.GPTQ_loader.load_quantized(model_name)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          GPTQ_loader.py\u201D, line 177, in load_quantized model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          GPTQ_loader.py\u201D, line 77, in _load_quant make_quant(**make_quant_kwargs)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\quant.py\u201D, line 446, in make_quant make_quant(child,\
          \ names, bits, groupsize, faster, name + \u2018.\u2019 + name1 if name !=\
          \ \u2018\u2019 else name1, kernel_switch_threshold=kernel_switch_threshold)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\quant.py\u201D, line 446, in make_quant make_quant(child,\
          \ names, bits, groupsize, faster, name + \u2018.\u2019 + name1 if name !=\
          \ \u2018\u2019 else name1, kernel_switch_threshold=kernel_switch_threshold)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\quant.py\u201D, line 446, in make_quant make_quant(child,\
          \ names, bits, groupsize, faster, name + \u2018.\u2019 + name1 if name !=\
          \ \u2018\u2019 else name1, kernel_switch_threshold=kernel_switch_threshold)\
          \ [Previous line repeated 1 more time] File \u201CC:\\AI\\oobabooga_windows\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\u201D, line\
          \ 443, in make_quant module, attr, QuantLinear(bits, groupsize, tmp.in_features,\
          \ tmp.out_features, faster=faster, kernel_switch_threshold=kernel_switch_threshold)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\quant.py\u201D, line 142, in init raise NotImplementedError(\u201C\
          Only 2,3,4,8 bits are supported.\u201D) NotImplementedError: Only 2,3,4,8\
          \ bits are supported.</p>\n"
        raw: "Hi thanks for your work! In my case only AutoGPTQ works, others give\
          \ the following errors.\r\n\r\nWith ExLlama:\r\nTraceback (most recent call\
          \ last): File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\server.py\u201D\
          , line 68, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader) File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 78, in load_model output = load_func_maploader\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 293, in ExLlama_loader model, tokenizer = ExllamaModel.from_pretrained(model_name)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          exllama.py\u201D, line 49, in from_pretrained config = ExLlamaConfig(str(model_config_path))\
          \ File \u201CC:\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          exllama\\model.py\u201D, line 52, in init self.pad_token_id = read_config[\u201C\
          pad_token_id\u201D] KeyError: \u2018pad_token_id\u2019\r\n\r\nI have tried\
          \ to add 'pad_token_id' manually, but then I got errors about other missing\
          \ tokens.\r\n\r\n\r\nWith GPTQ-for-LLaMa:\r\nTraceback (most recent call\
          \ last): File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\server.py\u201D\
          , line 68, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader) File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 78, in load_model output = load_func_maploader\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          models.py\u201D, line 279, in GPTQ_loader model = modules.GPTQ_loader.load_quantized(model_name)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          GPTQ_loader.py\u201D, line 177, in load_quantized model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
          GPTQ_loader.py\u201D, line 77, in _load_quant make_quant(**make_quant_kwargs)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\quant.py\u201D, line 446, in make_quant make_quant(child,\
          \ names, bits, groupsize, faster, name + \u2018.\u2019 + name1 if name !=\
          \ \u2018\u2019 else name1, kernel_switch_threshold=kernel_switch_threshold)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\quant.py\u201D, line 446, in make_quant make_quant(child,\
          \ names, bits, groupsize, faster, name + \u2018.\u2019 + name1 if name !=\
          \ \u2018\u2019 else name1, kernel_switch_threshold=kernel_switch_threshold)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\quant.py\u201D, line 446, in make_quant make_quant(child,\
          \ names, bits, groupsize, faster, name + \u2018.\u2019 + name1 if name !=\
          \ \u2018\u2019 else name1, kernel_switch_threshold=kernel_switch_threshold)\
          \ [Previous line repeated 1 more time] File \u201CC:\\AI\\oobabooga_windows\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\u201D, line\
          \ 443, in make_quant module, attr, QuantLinear(bits, groupsize, tmp.in_features,\
          \ tmp.out_features, faster=faster, kernel_switch_threshold=kernel_switch_threshold)\
          \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\quant.py\u201D, line 142, in init raise NotImplementedError(\u201C\
          Only 2,3,4,8 bits are supported.\u201D) NotImplementedError: Only 2,3,4,8\
          \ bits are supported."
        updatedAt: '2023-08-02T07:48:51.881Z'
      numEdits: 0
      reactions: []
    id: 64ca0a63cbcae9f106071bd6
    type: comment
  author: perelmanych
  content: "Hi thanks for your work! In my case only AutoGPTQ works, others give the\
    \ following errors.\r\n\r\nWith ExLlama:\r\nTraceback (most recent call last):\
    \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\server.py\u201D\
    , line 68, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader) File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
    models.py\u201D, line 78, in load_model output = load_func_maploader File \u201C\
    C:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
    \ 293, in ExLlama_loader model, tokenizer = ExllamaModel.from_pretrained(model_name)\
    \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\exllama.py\u201D\
    , line 49, in from_pretrained config = ExLlamaConfig(str(model_config_path)) File\
    \ \u201CC:\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    exllama\\model.py\u201D, line 52, in init self.pad_token_id = read_config[\u201C\
    pad_token_id\u201D] KeyError: \u2018pad_token_id\u2019\r\n\r\nI have tried to\
    \ add 'pad_token_id' manually, but then I got errors about other missing tokens.\r\
    \n\r\n\r\nWith GPTQ-for-LLaMa:\r\nTraceback (most recent call last): File \u201C\
    C:\\AI\\oobabooga_windows\\text-generation-webui\\server.py\u201D, line 68, in\
    \ load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader) File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\\
    models.py\u201D, line 78, in load_model output = load_func_maploader File \u201C\
    C:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
    \ 279, in GPTQ_loader model = modules.GPTQ_loader.load_quantized(model_name) File\
    \ \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
    , line 177, in load_quantized model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
    \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
    , line 77, in _load_quant make_quant(**make_quant_kwargs) File \u201CC:\\AI\\\
    oobabooga_windows\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\u201D\
    , line 446, in make_quant make_quant(child, names, bits, groupsize, faster, name\
    \ + \u2018.\u2019 + name1 if name != \u2018\u2019 else name1, kernel_switch_threshold=kernel_switch_threshold)\
    \ File \u201CC:\\AI\\oobabooga_windows\\text-generation-webui\\repositories\\\
    GPTQ-for-LLaMa\\quant.py\u201D, line 446, in make_quant make_quant(child, names,\
    \ bits, groupsize, faster, name + \u2018.\u2019 + name1 if name != \u2018\u2019\
    \ else name1, kernel_switch_threshold=kernel_switch_threshold) File \u201CC:\\\
    AI\\oobabooga_windows\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\u201D\
    , line 446, in make_quant make_quant(child, names, bits, groupsize, faster, name\
    \ + \u2018.\u2019 + name1 if name != \u2018\u2019 else name1, kernel_switch_threshold=kernel_switch_threshold)\
    \ [Previous line repeated 1 more time] File \u201CC:\\AI\\oobabooga_windows\\\
    text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\u201D, line 443,\
    \ in make_quant module, attr, QuantLinear(bits, groupsize, tmp.in_features, tmp.out_features,\
    \ faster=faster, kernel_switch_threshold=kernel_switch_threshold) File \u201C\
    C:\\AI\\oobabooga_windows\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
    quant.py\u201D, line 142, in init raise NotImplementedError(\u201COnly 2,3,4,8\
    \ bits are supported.\u201D) NotImplementedError: Only 2,3,4,8 bits are supported."
  created_at: 2023-08-02 06:48:51+00:00
  edited: false
  hidden: false
  id: 64ca0a63cbcae9f106071bd6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-02T07:59:03.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8517914414405823
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah ExLlama only works with 4-bit Llama models, and this is not
          a Llama model.  Please use AutoGPTQ for this model.</p>

          <p>In general, please check the "Provided Files" table, there''s a column
          that indicates if a model is compatible with ExLlama or not.</p>

          '
        raw: 'Yeah ExLlama only works with 4-bit Llama models, and this is not a Llama
          model.  Please use AutoGPTQ for this model.


          In general, please check the "Provided Files" table, there''s a column that
          indicates if a model is compatible with ExLlama or not.'
        updatedAt: '2023-08-02T07:59:49.144Z'
      numEdits: 1
      reactions: []
    id: 64ca0cc7c96a10fa85df3842
    type: comment
  author: TheBloke
  content: 'Yeah ExLlama only works with 4-bit Llama models, and this is not a Llama
    model.  Please use AutoGPTQ for this model.


    In general, please check the "Provided Files" table, there''s a column that indicates
    if a model is compatible with ExLlama or not.'
  created_at: 2023-08-02 06:59:03+00:00
  edited: true
  hidden: false
  id: 64ca0cc7c96a10fa85df3842
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/WizardCoder-Guanaco-15B-V1.1-GPTQ
repo_type: model
status: open
target_branch: null
title: Running with ExLlama and GPTQ-for-LLaMa in text-generation-webui gives errors
