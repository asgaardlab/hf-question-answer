!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sciumo
conflicting_files: null
created_at: 2023-06-02 12:37:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f7e2c8d7ae91e0a74529422da1bb312.svg
      fullname: Sciumo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: Sciumo
      type: user
    createdAt: '2023-06-02T13:37:58.000Z'
    data:
      edited: false
      editors:
      - Sciumo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f7e2c8d7ae91e0a74529422da1bb312.svg
          fullname: Sciumo
          isHf: false
          isPro: true
          name: Sciumo
          type: user
        html: '<p>It was unclear what the prompt should be. Shouldn''t model cards
          have the associated prompts?<br>Here is what I used:<br>template = """{instruct}<br>USER:
          {question}<br>ASSISTANT:<br>"""<br>The performance was 446.87 ms per token
          on a TR Pro 3995 with 64 cores and 256 GB RAM. I classify that as slow.<br>Apparently
          CPU doesn''t really help, with a single NUMA just spins on memory access.
          I''m going to try <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63ff78e2b09f82a81a1d6f52/ANEyzdX6BDrak8wRwCpOU.png"><img
          alt="mspertoken_1.png" src="https://cdn-uploads.huggingface.co/production/uploads/63ff78e2b09f82a81a1d6f52/ANEyzdX6BDrak8wRwCpOU.png"></a></p>

          <p>Here is the utilization:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63ff78e2b09f82a81a1d6f52/evRPXEdpKr4i_-dj443Ta.png"><img
          alt="cpu_1.png" src="https://cdn-uploads.huggingface.co/production/uploads/63ff78e2b09f82a81a1d6f52/evRPXEdpKr4i_-dj443Ta.png"></a></p>

          '
        raw: "It was unclear what the prompt should be. Shouldn't model cards have\
          \ the associated prompts?\r\nHere is what I used:\r\ntemplate = \"\"\"{instruct}\r\
          \nUSER: {question}\r\nASSISTANT:\r\n\"\"\"\r\nThe performance was 446.87\
          \ ms per token on a TR Pro 3995 with 64 cores and 256 GB RAM. I classify\
          \ that as slow.\r\nApparently CPU doesn't really help, with a single NUMA\
          \ just spins on memory access. I'm going to try https://github.com/huggingface/text-generation-inference\r\
          \n\r\n![mspertoken_1.png](https://cdn-uploads.huggingface.co/production/uploads/63ff78e2b09f82a81a1d6f52/ANEyzdX6BDrak8wRwCpOU.png)\r\
          \n\r\nHere is the utilization:\r\n![cpu_1.png](https://cdn-uploads.huggingface.co/production/uploads/63ff78e2b09f82a81a1d6f52/evRPXEdpKr4i_-dj443Ta.png)\r\
          \n"
        updatedAt: '2023-06-02T13:37:58.080Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - max-fry
        - PrimeD
    id: 6479f0b68d17fcf64760bc32
    type: comment
  author: Sciumo
  content: "It was unclear what the prompt should be. Shouldn't model cards have the\
    \ associated prompts?\r\nHere is what I used:\r\ntemplate = \"\"\"{instruct}\r\
    \nUSER: {question}\r\nASSISTANT:\r\n\"\"\"\r\nThe performance was 446.87 ms per\
    \ token on a TR Pro 3995 with 64 cores and 256 GB RAM. I classify that as slow.\r\
    \nApparently CPU doesn't really help, with a single NUMA just spins on memory\
    \ access. I'm going to try https://github.com/huggingface/text-generation-inference\r\
    \n\r\n![mspertoken_1.png](https://cdn-uploads.huggingface.co/production/uploads/63ff78e2b09f82a81a1d6f52/ANEyzdX6BDrak8wRwCpOU.png)\r\
    \n\r\nHere is the utilization:\r\n![cpu_1.png](https://cdn-uploads.huggingface.co/production/uploads/63ff78e2b09f82a81a1d6f52/evRPXEdpKr4i_-dj443Ta.png)\r\
    \n"
  created_at: 2023-06-02 12:37:58+00:00
  edited: false
  hidden: false
  id: 6479f0b68d17fcf64760bc32
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GGML
repo_type: model
status: open
target_branch: null
title: Prompt and performance
