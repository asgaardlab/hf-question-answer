!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DevElCuy
conflicting_files: null
created_at: 2023-09-09 14:32:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6bc85e277168bc4a3b006c8c578779b6.svg
      fullname: Fernando Paredes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DevElCuy
      type: user
    createdAt: '2023-09-09T15:32:17.000Z'
    data:
      edited: true
      editors:
      - DevElCuy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5677130818367004
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6bc85e277168bc4a3b006c8c578779b6.svg
          fullname: Fernando Paredes
          isHf: false
          isPro: false
          name: DevElCuy
          type: user
        html: "<p>First of all thanks for this great model!<br>I managed to convert\
          \ it to GGML with <a rel=\"nofollow\" href=\"https://github.com/bigcode-project/starcoder.cpp\"\
          >https://github.com/bigcode-project/starcoder.cpp</a><br>And this is how\
          \ I load it:</p>\n<pre><code class=\"language-python\">    <span class=\"\
          hljs-keyword\">from</span> ctransformers <span class=\"hljs-keyword\">import</span>\
          \ (\n        AutoModelForCausalLM,\n        AutoTokenizer\n    )\n    <span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> pipeline\n    <span class=\"hljs-keyword\">from</span> langchain.llms\
          \ <span class=\"hljs-keyword\">import</span> HuggingFacePipeline\n\n   \
          \ model_name = <span class=\"hljs-string\">\"models/abacaj--starcoderbase-1b-sft-ggml.bin\"\
          </span>\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n\
          \        model_type=<span class=\"hljs-string\">\"gpt_bigcode\"</span>,\n\
          \        gpu_layers=<span class=\"hljs-number\">1024</span>,\n        hf=<span\
          \ class=\"hljs-literal\">True</span>\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model)\n\
          \n    pipe = pipeline(\n        <span class=\"hljs-string\">\"text-generation\"\
          </span>,\n        model=model,\n        tokenizer=tokenizer,\n        max_length=<span\
          \ class=\"hljs-number\">2048</span>,\n    )\n\n    llm = HuggingFacePipeline(\n\
          \        pipeline=pipe,\n    )\n</code></pre>\n<p>Dependencies (requirements.txt):</p>\n\
          <pre><code class=\"language-txt\">torch==2.0.1\ntransformers==4.33.1\nlangchain==0.0.285\n\
          ctransformers==0.2.26\n</code></pre>\n"
        raw: "First of all thanks for this great model!\nI managed to convert it to\
          \ GGML with https://github.com/bigcode-project/starcoder.cpp\nAnd this is\
          \ how I load it:\n\n```python\n    from ctransformers import (\n       \
          \ AutoModelForCausalLM,\n        AutoTokenizer\n    )\n    from transformers\
          \ import pipeline\n    from langchain.llms import HuggingFacePipeline\n\n\
          \    model_name = \"models/abacaj--starcoderbase-1b-sft-ggml.bin\"\n   \
          \ model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n \
          \       model_type=\"gpt_bigcode\",\n        gpu_layers=1024,\n        hf=True\n\
          \    )\n    tokenizer = AutoTokenizer.from_pretrained(model)\n\n    pipe\
          \ = pipeline(\n        \"text-generation\",\n        model=model,\n    \
          \    tokenizer=tokenizer,\n        max_length=2048,\n    )\n\n    llm =\
          \ HuggingFacePipeline(\n        pipeline=pipe,\n    )\n```\n\nDependencies\
          \ (requirements.txt):\n```txt\ntorch==2.0.1\ntransformers==4.33.1\nlangchain==0.0.285\n\
          ctransformers==0.2.26\n```\n"
        updatedAt: '2023-09-09T15:32:44.586Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - abacaj
    id: 64fc9001d268b2f1adc7eaea
    type: comment
  author: DevElCuy
  content: "First of all thanks for this great model!\nI managed to convert it to\
    \ GGML with https://github.com/bigcode-project/starcoder.cpp\nAnd this is how\
    \ I load it:\n\n```python\n    from ctransformers import (\n        AutoModelForCausalLM,\n\
    \        AutoTokenizer\n    )\n    from transformers import pipeline\n    from\
    \ langchain.llms import HuggingFacePipeline\n\n    model_name = \"models/abacaj--starcoderbase-1b-sft-ggml.bin\"\
    \n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n   \
    \     model_type=\"gpt_bigcode\",\n        gpu_layers=1024,\n        hf=True\n\
    \    )\n    tokenizer = AutoTokenizer.from_pretrained(model)\n\n    pipe = pipeline(\n\
    \        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n\
    \        max_length=2048,\n    )\n\n    llm = HuggingFacePipeline(\n        pipeline=pipe,\n\
    \    )\n```\n\nDependencies (requirements.txt):\n```txt\ntorch==2.0.1\ntransformers==4.33.1\n\
    langchain==0.0.285\nctransformers==0.2.26\n```\n"
  created_at: 2023-09-09 14:32:17+00:00
  edited: true
  hidden: false
  id: 64fc9001d268b2f1adc7eaea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: abacaj/starcoderbase-1b-sft
repo_type: model
status: open
target_branch: null
title: QGML with Starcoder.cpp only needs less than 3GB GPU
