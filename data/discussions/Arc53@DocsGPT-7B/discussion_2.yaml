!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-10-07 02:18:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-10-07T03:18:34.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.942467987537384
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Would you like to elaborate more on how the 50k high quality documentation
          answering dataset is created?</p>

          <p>Are they bootstrapped from handcrafted questions that are commonly used
          in DocsGPT, and then used answers generated by e.g. gpt4 or claude2 to pair
          up a set of Q&amp;As, or are they human generated answers?</p>

          <p>I am a bit astonished by the 50k quantities, you usually can hardly find
          such amount of domain specific data for LoRA fine-tuning.</p>

          '
        raw: 'Would you like to elaborate more on how the 50k high quality documentation
          answering dataset is created?


          Are they bootstrapped from handcrafted questions that are commonly used
          in DocsGPT, and then used answers generated by e.g. gpt4 or claude2 to pair
          up a set of Q&As, or are they human generated answers?


          I am a bit astonished by the 50k quantities, you usually can hardly find
          such amount of domain specific data for LoRA fine-tuning.'
        updatedAt: '2023-10-07T03:19:07.593Z'
      numEdits: 1
      reactions: []
    id: 6520ce0abafd014bf67e1e3d
    type: comment
  author: Yhyu13
  content: 'Would you like to elaborate more on how the 50k high quality documentation
    answering dataset is created?


    Are they bootstrapped from handcrafted questions that are commonly used in DocsGPT,
    and then used answers generated by e.g. gpt4 or claude2 to pair up a set of Q&As,
    or are they human generated answers?


    I am a bit astonished by the 50k quantities, you usually can hardly find such
    amount of domain specific data for LoRA fine-tuning.'
  created_at: 2023-10-07 02:18:34+00:00
  edited: true
  hidden: false
  id: 6520ce0abafd014bf67e1e3d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Arc53/DocsGPT-7B
repo_type: model
status: open
target_branch: null
title: How is the 50k high quality dataset created?
