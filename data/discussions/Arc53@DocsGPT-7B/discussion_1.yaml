!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YairF
conflicting_files: null
created_at: 2023-07-02 12:53:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/223537554c5098c4915da828a3dd61a2.svg
      fullname: Yair Friedman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YairF
      type: user
    createdAt: '2023-07-02T13:53:05.000Z'
    data:
      edited: false
      editors:
      - YairF
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6216850876808167
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/223537554c5098c4915da828a3dd61a2.svg
          fullname: Yair Friedman
          isHf: false
          isPro: false
          name: YairF
          type: user
        html: '<p>when using the model locally with my macBook pro with:<br>from transformers
          import AutoTokenizer, AutoModelForCausalLM, pipeline<br>from langchain.llms
          import HuggingFacePipeline</p>

          <p>os.environ["HUGGINGFACEHUB_API_TOKEN"] = ''***********************''<br>tokenizer
          = AutoTokenizer.from_pretrained("Arc53/DocsGPT-7B")</p>

          <p>model = AutoModelForCausalLM.from_pretrained("Arc53/DocsGPT-7B",<br>                                                  trust_remote_code=True)</p>

          <p>pipe = pipeline(<br>      "text-generation",<br>         model=model,<br>         tokenizer=tokenizer,<br>         max_length=1024<br>)<br>local_llm
          = HuggingFacePipeline(pipeline=pipe)<br>.....<br>got :<br>the model ''MPTForCausalLM''
          is not supported for text-generation. Supported models are [''BartForCausalLM'',
          ''BertLMHeadModel'', ''BertGenerationDecoder'', ''BigBirdForCausalLM''.....</p>

          <blockquote>

          <p>Entering new  chain...<br>Input length of input_ids is 1636, but <code>max_length</code>
          is set to 1024. This can lead to unexpected behavior. You should consider
          increasing <code>max_new_tokens</code>.</p>

          </blockquote>

          <p>do i have any way of using this model with a langchain''s agent on a
          CPU computer ?</p>

          '
        raw: "when using the model locally with my macBook pro with:\r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM, pipeline\r\nfrom langchain.llms\
          \ import HuggingFacePipeline\r\n    \r\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"\
          ] = '***********************'\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          Arc53/DocsGPT-7B\")\r\n    \r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          Arc53/DocsGPT-7B\",\r\n                                                \
          \  trust_remote_code=True)\r\n    \r\npipe = pipeline(\r\n      \"text-generation\"\
          ,\r\n         model=model,\r\n         tokenizer=tokenizer,\r\n        \
          \ max_length=1024\r\n)\r\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\r\
          \n.....\r\ngot :\r\nthe model 'MPTForCausalLM' is not supported for text-generation.\
          \ Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
          \ 'BigBirdForCausalLM'.....\r\n> Entering new  chain...\r\nInput length\
          \ of input_ids is 1636, but `max_length` is set to 1024. This can lead to\
          \ unexpected behavior. You should consider increasing `max_new_tokens`.\r\
          \n\r\ndo i have any way of using this model with a langchain's agent on\
          \ a CPU computer ?"
        updatedAt: '2023-07-02T13:53:05.886Z'
      numEdits: 0
      reactions: []
    id: 64a18141f09ad861c83f98f6
    type: comment
  author: YairF
  content: "when using the model locally with my macBook pro with:\r\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM, pipeline\r\nfrom langchain.llms\
    \ import HuggingFacePipeline\r\n    \r\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"\
    ] = '***********************'\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    Arc53/DocsGPT-7B\")\r\n    \r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    Arc53/DocsGPT-7B\",\r\n                                                  trust_remote_code=True)\r\
    \n    \r\npipe = pipeline(\r\n      \"text-generation\",\r\n         model=model,\r\
    \n         tokenizer=tokenizer,\r\n         max_length=1024\r\n)\r\nlocal_llm\
    \ = HuggingFacePipeline(pipeline=pipe)\r\n.....\r\ngot :\r\nthe model 'MPTForCausalLM'\
    \ is not supported for text-generation. Supported models are ['BartForCausalLM',\
    \ 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM'.....\r\n> Entering\
    \ new  chain...\r\nInput length of input_ids is 1636, but `max_length` is set\
    \ to 1024. This can lead to unexpected behavior. You should consider increasing\
    \ `max_new_tokens`.\r\n\r\ndo i have any way of using this model with a langchain's\
    \ agent on a CPU computer ?"
  created_at: 2023-07-02 12:53:05+00:00
  edited: false
  hidden: false
  id: 64a18141f09ad861c83f98f6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Arc53/DocsGPT-7B
repo_type: model
status: open
target_branch: null
title: serving the model locally with cpu
