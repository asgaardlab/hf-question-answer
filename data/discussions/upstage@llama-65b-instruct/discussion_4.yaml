!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YuxinXiao
conflicting_files: null
created_at: 2023-07-25 11:32:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/afb96d2bbf90411f4b1a030ebebff300.svg
      fullname: Yuxin Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YuxinXiao
      type: user
    createdAt: '2023-07-25T12:32:18.000Z'
    data:
      edited: false
      editors:
      - YuxinXiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9429384469985962
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/afb96d2bbf90411f4b1a030ebebff300.svg
          fullname: Yuxin Xiao
          isHf: false
          isPro: false
          name: YuxinXiao
          type: user
        html: '<p>Hi, I tried to load the tokenizer of this model, but met the following
          error: TypeError: not a string.<br>I think it is because tokenizer.model
          is missing in this entry.<br>Could you please check and upload it? Thanks!</p>

          '
        raw: "Hi, I tried to load the tokenizer of this model, but met the following\
          \ error: TypeError: not a string.\r\nI think it is because tokenizer.model\
          \ is missing in this entry.\r\nCould you please check and upload it? Thanks!"
        updatedAt: '2023-07-25T12:32:18.791Z'
      numEdits: 0
      reactions: []
    id: 64bfc0d2623ce87c701a3e9b
    type: comment
  author: YuxinXiao
  content: "Hi, I tried to load the tokenizer of this model, but met the following\
    \ error: TypeError: not a string.\r\nI think it is because tokenizer.model is\
    \ missing in this entry.\r\nCould you please check and upload it? Thanks!"
  created_at: 2023-07-25 11:32:18+00:00
  edited: false
  hidden: false
  id: 64bfc0d2623ce87c701a3e9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64823fc42eeb6920563c7b07/7o2pArIZkW8Gu0rF2X96F.jpeg?w=200&h=200&f=face
      fullname: Sanghoon Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Limerobot
      type: user
    createdAt: '2023-07-26T01:20:50.000Z'
    data:
      edited: false
      editors:
      - Limerobot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8268837332725525
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64823fc42eeb6920563c7b07/7o2pArIZkW8Gu0rF2X96F.jpeg?w=200&h=200&f=face
          fullname: Sanghoon Kim
          isHf: false
          isPro: false
          name: Limerobot
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;YuxinXiao&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/YuxinXiao\">@<span class=\"\
          underline\">YuxinXiao</span></a></span>\n\n\t</span></span><br>Hi,<br>Could\
          \ you share your transformers version? I will check tokenizer loading in\
          \ your transformers version. FYI, our transformers version is '4.31.0'.</p>\n"
        raw: "@YuxinXiao \nHi, \nCould you share your transformers version? I will\
          \ check tokenizer loading in your transformers version. FYI, our transformers\
          \ version is '4.31.0'."
        updatedAt: '2023-07-26T01:20:50.182Z'
      numEdits: 0
      reactions: []
    id: 64c074f2e9263c783d449ec2
    type: comment
  author: Limerobot
  content: "@YuxinXiao \nHi, \nCould you share your transformers version? I will check\
    \ tokenizer loading in your transformers version. FYI, our transformers version\
    \ is '4.31.0'."
  created_at: 2023-07-26 00:20:50+00:00
  edited: false
  hidden: false
  id: 64c074f2e9263c783d449ec2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641e81e75c348064a8259d5d/uPGIDF5NOoVCInxpAscm_.jpeg?w=200&h=200&f=face
      fullname: Wonho Song
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wonhosong
      type: user
    createdAt: '2023-07-26T01:47:08.000Z'
    data:
      edited: false
      editors:
      - wonhosong
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9044567942619324
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641e81e75c348064a8259d5d/uPGIDF5NOoVCInxpAscm_.jpeg?w=200&h=200&f=face
          fullname: Wonho Song
          isHf: false
          isPro: false
          name: wonhosong
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;YuxinXiao&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/YuxinXiao\">@<span class=\"\
          underline\">YuxinXiao</span></a></span>\n\n\t</span></span> Hello,<br> I\
          \ tested with the code below and it was successful in loading the tokenizer.</p>\n\
          <pre><code>tokenizer = AutoTokenizer.from_pretrained(\n    \"upstage/llama-65b-instruct\"\
          ,\n    force_download=True\n)\n</code></pre>\n<p>I've confirmed that it\
          \ works on both <code>transformer==4.30.0</code> and <code>transformer==4.30.1</code>.</p>\n"
        raw: "@YuxinXiao Hello,\n I tested with the code below and it was successful\
          \ in loading the tokenizer.\n```\ntokenizer = AutoTokenizer.from_pretrained(\n\
          \    \"upstage/llama-65b-instruct\",\n    force_download=True\n)\n```\n\n\
          I've confirmed that it works on both `transformer==4.30.0` and `transformer==4.30.1`."
        updatedAt: '2023-07-26T01:47:08.204Z'
      numEdits: 0
      reactions: []
    id: 64c07b1cf04093c08820f2da
    type: comment
  author: wonhosong
  content: "@YuxinXiao Hello,\n I tested with the code below and it was successful\
    \ in loading the tokenizer.\n```\ntokenizer = AutoTokenizer.from_pretrained(\n\
    \    \"upstage/llama-65b-instruct\",\n    force_download=True\n)\n```\n\nI've\
    \ confirmed that it works on both `transformer==4.30.0` and `transformer==4.30.1`."
  created_at: 2023-07-26 00:47:08+00:00
  edited: false
  hidden: false
  id: 64c07b1cf04093c08820f2da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/afb96d2bbf90411f4b1a030ebebff300.svg
      fullname: Yuxin Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YuxinXiao
      type: user
    createdAt: '2023-07-26T01:57:22.000Z'
    data:
      edited: false
      editors:
      - YuxinXiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4275495111942291
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/afb96d2bbf90411f4b1a030ebebff300.svg
          fullname: Yuxin Xiao
          isHf: false
          isPro: false
          name: YuxinXiao
          type: user
        html: "<p>Hi, I'm using <code>transformer==4.31.0</code>.<br>When I run </p>\n\
          <pre><code>name = 'upstage/llama-65b-instruct'\ntokenizer = AutoTokenizer.from_pretrained(name,\
          \ use_fast=False, force_download=True)\n</code></pre>\n<p>I get the following\
          \ error</p>\n<pre><code>TypeError                                 Traceback\
          \ (most recent call last)\nCell In[5], line 2\n      1 name = 'upstage/llama-65b-instruct'\n\
          ----&gt; 2 tokenizer = AutoTokenizer.from_pretrained(name, use_fast=False,\
          \ force_download=True)\n      3 # model = AutoModelForCausalLM.from_pretrained(name,\
          \ trust_remote_code=True, low_cpu_mem_usage=True, torch_dtype=torch.float16,\
          \ device_map='auto')\n\nFile ~/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:702,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\n    698     if tokenizer_class is None:\n    699         raise\
          \ ValueError(\n    700             f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\n    701         )\n--&gt;\
          \ 702     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n    704 # Otherwise we have to be creative.\n    705\
          \ # if model is an encoder decoder, the encoder tokenizer class is used\
          \ by default\n    706 if isinstance(config, EncoderDecoderConfig):\n\nFile\
          \ ~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1841,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\n   1838     else:\n   1839         logger.info(f\"loading file\
          \ {file_path} from cache at {resolved_vocab_files[file_id]}\")\n-&gt; 1841\
          \ return cls._from_pretrained(\n   1842     resolved_vocab_files,\n   1843\
          \     pretrained_model_name_or_path,\n   1844     init_configuration,\n\
          \   1845     *init_inputs,\n   1846     use_auth_token=token,\n   1847 \
          \    cache_dir=cache_dir,\n   1848     local_files_only=local_files_only,\n\
          \   1849     _commit_hash=commit_hash,\n   1850     _is_local=is_local,\n\
          \   1851     **kwargs,\n   1852 )\n\nFile ~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2004,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir,\
          \ local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\n \
          \  2002 # Instantiate tokenizer.\n   2003 try:\n-&gt; 2004     tokenizer\
          \ = cls(*init_inputs, **init_kwargs)\n   2005 except OSError:\n   2006 \
          \    raise OSError(\n   2007         \"Unable to load vocabulary from file.\
          \ \"\n   2008         \"Please check that the provided vocabulary is accessible\
          \ and not corrupted.\"\n   2009     )\n\nFile ~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py:144,\
          \ in LlamaTokenizer.__init__(self, vocab_file, unk_token, bos_token, eos_token,\
          \ pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces,\
          \ legacy, **kwargs)\n    142 self.add_eos_token = add_eos_token\n    143\
          \ self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n--&gt;\
          \ 144 self.sp_model.Load(vocab_file)\n\nFile ~/miniconda3/lib/python3.10/site-packages/sentencepiece/__init__.py:905,\
          \ in SentencePieceProcessor.Load(self, model_file, model_proto)\n    903\
          \ if model_proto:\n    904   return self.LoadFromSerializedProto(model_proto)\n\
          --&gt; 905 return self.LoadFromFile(model_file)\n\nFile ~/miniconda3/lib/python3.10/site-packages/sentencepiece/__init__.py:310,\
          \ in SentencePieceProcessor.LoadFromFile(self, arg)\n    309 def LoadFromFile(self,\
          \ arg):\n--&gt; 310     return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\n\nTypeError: not a string\n</code></pre>\n<p>In fact, you can find\
          \ <code>tokenizer.model</code> in the \"files and versions\" folder of <code>upstage/llama-30b-instruct</code>,\
          \ but you can't see it here.<br>So I think the error is due to the missing\
          \ <code>tokenizer.model</code>.</p>\n"
        raw: "Hi, I'm using `transformer==4.31.0`.\nWhen I run \n```\nname = 'upstage/llama-65b-instruct'\n\
          tokenizer = AutoTokenizer.from_pretrained(name, use_fast=False, force_download=True)\n\
          ```\nI get the following error\n```\nTypeError                         \
          \        Traceback (most recent call last)\nCell In[5], line 2\n      1\
          \ name = 'upstage/llama-65b-instruct'\n----> 2 tokenizer = AutoTokenizer.from_pretrained(name,\
          \ use_fast=False, force_download=True)\n      3 # model = AutoModelForCausalLM.from_pretrained(name,\
          \ trust_remote_code=True, low_cpu_mem_usage=True, torch_dtype=torch.float16,\
          \ device_map='auto')\n\nFile ~/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:702,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\n    698     if tokenizer_class is None:\n    699         raise\
          \ ValueError(\n    700             f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\n    701         )\n-->\
          \ 702     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n    704 # Otherwise we have to be creative.\n    705\
          \ # if model is an encoder decoder, the encoder tokenizer class is used\
          \ by default\n    706 if isinstance(config, EncoderDecoderConfig):\n\nFile\
          \ ~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1841,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
          \ **kwargs)\n   1838     else:\n   1839         logger.info(f\"loading file\
          \ {file_path} from cache at {resolved_vocab_files[file_id]}\")\n-> 1841\
          \ return cls._from_pretrained(\n   1842     resolved_vocab_files,\n   1843\
          \     pretrained_model_name_or_path,\n   1844     init_configuration,\n\
          \   1845     *init_inputs,\n   1846     use_auth_token=token,\n   1847 \
          \    cache_dir=cache_dir,\n   1848     local_files_only=local_files_only,\n\
          \   1849     _commit_hash=commit_hash,\n   1850     _is_local=is_local,\n\
          \   1851     **kwargs,\n   1852 )\n\nFile ~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2004,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir,\
          \ local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\n \
          \  2002 # Instantiate tokenizer.\n   2003 try:\n-> 2004     tokenizer =\
          \ cls(*init_inputs, **init_kwargs)\n   2005 except OSError:\n   2006   \
          \  raise OSError(\n   2007         \"Unable to load vocabulary from file.\
          \ \"\n   2008         \"Please check that the provided vocabulary is accessible\
          \ and not corrupted.\"\n   2009     )\n\nFile ~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py:144,\
          \ in LlamaTokenizer.__init__(self, vocab_file, unk_token, bos_token, eos_token,\
          \ pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces,\
          \ legacy, **kwargs)\n    142 self.add_eos_token = add_eos_token\n    143\
          \ self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-->\
          \ 144 self.sp_model.Load(vocab_file)\n\nFile ~/miniconda3/lib/python3.10/site-packages/sentencepiece/__init__.py:905,\
          \ in SentencePieceProcessor.Load(self, model_file, model_proto)\n    903\
          \ if model_proto:\n    904   return self.LoadFromSerializedProto(model_proto)\n\
          --> 905 return self.LoadFromFile(model_file)\n\nFile ~/miniconda3/lib/python3.10/site-packages/sentencepiece/__init__.py:310,\
          \ in SentencePieceProcessor.LoadFromFile(self, arg)\n    309 def LoadFromFile(self,\
          \ arg):\n--> 310     return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\n\nTypeError: not a string\n```\nIn fact, you can find `tokenizer.model`\
          \ in the \"files and versions\" folder of `upstage/llama-30b-instruct`,\
          \ but you can't see it here.\nSo I think the error is due to the missing\
          \ `tokenizer.model`."
        updatedAt: '2023-07-26T01:57:22.021Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - wonhosong
    id: 64c07d8284191336fa00091d
    type: comment
  author: YuxinXiao
  content: "Hi, I'm using `transformer==4.31.0`.\nWhen I run \n```\nname = 'upstage/llama-65b-instruct'\n\
    tokenizer = AutoTokenizer.from_pretrained(name, use_fast=False, force_download=True)\n\
    ```\nI get the following error\n```\nTypeError                               \
    \  Traceback (most recent call last)\nCell In[5], line 2\n      1 name = 'upstage/llama-65b-instruct'\n\
    ----> 2 tokenizer = AutoTokenizer.from_pretrained(name, use_fast=False, force_download=True)\n\
    \      3 # model = AutoModelForCausalLM.from_pretrained(name, trust_remote_code=True,\
    \ low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map='auto')\n\nFile\
    \ ~/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:702,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\n    698     if tokenizer_class is None:\n    699         raise ValueError(\n\
    \    700             f\"Tokenizer class {tokenizer_class_candidate} does not exist\
    \ or is not currently imported.\"\n    701         )\n--> 702     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\n    704 # Otherwise we have to be creative.\n    705 # if\
    \ model is an encoder decoder, the encoder tokenizer class is used by default\n\
    \    706 if isinstance(config, EncoderDecoderConfig):\n\nFile ~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1841,\
    \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
    \ cache_dir, force_download, local_files_only, token, revision, *init_inputs,\
    \ **kwargs)\n   1838     else:\n   1839         logger.info(f\"loading file {file_path}\
    \ from cache at {resolved_vocab_files[file_id]}\")\n-> 1841 return cls._from_pretrained(\n\
    \   1842     resolved_vocab_files,\n   1843     pretrained_model_name_or_path,\n\
    \   1844     init_configuration,\n   1845     *init_inputs,\n   1846     use_auth_token=token,\n\
    \   1847     cache_dir=cache_dir,\n   1848     local_files_only=local_files_only,\n\
    \   1849     _commit_hash=commit_hash,\n   1850     _is_local=is_local,\n   1851\
    \     **kwargs,\n   1852 )\n\nFile ~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2004,\
    \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
    \ _is_local, *init_inputs, **kwargs)\n   2002 # Instantiate tokenizer.\n   2003\
    \ try:\n-> 2004     tokenizer = cls(*init_inputs, **init_kwargs)\n   2005 except\
    \ OSError:\n   2006     raise OSError(\n   2007         \"Unable to load vocabulary\
    \ from file. \"\n   2008         \"Please check that the provided vocabulary is\
    \ accessible and not corrupted.\"\n   2009     )\n\nFile ~/miniconda3/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py:144,\
    \ in LlamaTokenizer.__init__(self, vocab_file, unk_token, bos_token, eos_token,\
    \ pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces,\
    \ legacy, **kwargs)\n    142 self.add_eos_token = add_eos_token\n    143 self.sp_model\
    \ = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n--> 144 self.sp_model.Load(vocab_file)\n\
    \nFile ~/miniconda3/lib/python3.10/site-packages/sentencepiece/__init__.py:905,\
    \ in SentencePieceProcessor.Load(self, model_file, model_proto)\n    903 if model_proto:\n\
    \    904   return self.LoadFromSerializedProto(model_proto)\n--> 905 return self.LoadFromFile(model_file)\n\
    \nFile ~/miniconda3/lib/python3.10/site-packages/sentencepiece/__init__.py:310,\
    \ in SentencePieceProcessor.LoadFromFile(self, arg)\n    309 def LoadFromFile(self,\
    \ arg):\n--> 310     return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\n\nTypeError: not a string\n```\nIn fact, you can find `tokenizer.model`\
    \ in the \"files and versions\" folder of `upstage/llama-30b-instruct`, but you\
    \ can't see it here.\nSo I think the error is due to the missing `tokenizer.model`."
  created_at: 2023-07-26 00:57:22+00:00
  edited: false
  hidden: false
  id: 64c07d8284191336fa00091d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641e81e75c348064a8259d5d/uPGIDF5NOoVCInxpAscm_.jpeg?w=200&h=200&f=face
      fullname: Wonho Song
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wonhosong
      type: user
    createdAt: '2023-07-26T02:07:29.000Z'
    data:
      edited: false
      editors:
      - wonhosong
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9663647413253784
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641e81e75c348064a8259d5d/uPGIDF5NOoVCInxpAscm_.jpeg?w=200&h=200&f=face
          fullname: Wonho Song
          isHf: false
          isPro: false
          name: wonhosong
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;YuxinXiao&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/YuxinXiao\">@<span class=\"\
          underline\">YuxinXiao</span></a></span>\n\n\t</span></span> Thanks a lot.</p>\n\
          <p>For <code>use_fast=False</code>, it seems to require the tokenizer.model\
          \ and we uploaded it.<br>We checked that it loaded without any problem.<br>Could\
          \ you give it one more try?</p>\n"
        raw: '@YuxinXiao Thanks a lot.


          For `use_fast=False`, it seems to require the tokenizer.model and we uploaded
          it.

          We checked that it loaded without any problem.

          Could you give it one more try?'
        updatedAt: '2023-07-26T02:07:29.286Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Limerobot
    id: 64c07fe1e9263c783d4627b8
    type: comment
  author: wonhosong
  content: '@YuxinXiao Thanks a lot.


    For `use_fast=False`, it seems to require the tokenizer.model and we uploaded
    it.

    We checked that it loaded without any problem.

    Could you give it one more try?'
  created_at: 2023-07-26 01:07:29+00:00
  edited: false
  hidden: false
  id: 64c07fe1e9263c783d4627b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641e81e75c348064a8259d5d/uPGIDF5NOoVCInxpAscm_.jpeg?w=200&h=200&f=face
      fullname: Wonho Song
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wonhosong
      type: user
    createdAt: '2023-08-01T02:39:07.000Z'
    data:
      status: closed
    id: 64c8704b4515835c4d888cc2
    type: status-change
  author: wonhosong
  created_at: 2023-08-01 01:39:07+00:00
  id: 64c8704b4515835c4d888cc2
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/afb96d2bbf90411f4b1a030ebebff300.svg
      fullname: Yuxin Xiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YuxinXiao
      type: user
    createdAt: '2023-08-01T11:26:21.000Z'
    data:
      edited: false
      editors:
      - YuxinXiao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.992882490158081
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/afb96d2bbf90411f4b1a030ebebff300.svg
          fullname: Yuxin Xiao
          isHf: false
          isPro: false
          name: YuxinXiao
          type: user
        html: '<p>thanks for uploading it! it works fine now.</p>

          '
        raw: thanks for uploading it! it works fine now.
        updatedAt: '2023-08-01T11:26:21.332Z'
      numEdits: 0
      reactions: []
    id: 64c8ebdd4515835c4d9818aa
    type: comment
  author: YuxinXiao
  content: thanks for uploading it! it works fine now.
  created_at: 2023-08-01 10:26:21+00:00
  edited: false
  hidden: false
  id: 64c8ebdd4515835c4d9818aa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: upstage/llama-65b-instruct
repo_type: model
status: closed
target_branch: null
title: tokenizer.model is missing
