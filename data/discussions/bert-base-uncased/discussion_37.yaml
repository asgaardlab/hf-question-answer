!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hsuwill
conflicting_files: null
created_at: 2023-03-29 17:30:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e553f8e5c8d7c85e77d029f7a16958f9.svg
      fullname: William Hsu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hsuwill
      type: user
    createdAt: '2023-03-29T18:30:09.000Z'
    data:
      edited: false
      editors:
      - hsuwill
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e553f8e5c8d7c85e77d029f7a16958f9.svg
          fullname: William Hsu
          isHf: false
          isPro: false
          name: hsuwill
          type: user
        html: "<p>Input (given in the form of a sentence/paragraph)= \u201Can application\
          \ that convert speech to text using AI\u201D</p>\n<p>Desired Output:  category\
          \ 1 match = 0.1 category 2 match = 0.9 \u2026</p>\n<hr>\n<p>Category 1 description\
          \ = \u201Cmedical equipment includes wearable devices\u201D<br>Category\
          \ 2 description =\u201Csoftware for SaaS or downloadable\u201D<br>\u2026\
          </p>\n<p>In other words I am trying to ask \"hugging face\" to learn the\
          \ description of various categories that are given in sentences or item\
          \ lists.</p>\n<p>Then I am asking \"hugging face\" to classify a paragraph\
          \ description (my input) to the categories that best match this input description.</p>\n"
        raw: "Input (given in the form of a sentence/paragraph)= \u201Can application\
          \ that convert speech to text using AI\u201D\r\n\r\nDesired Output:  category\
          \ 1 match = 0.1 category 2 match = 0.9 \u2026\r\n\r\n--------------\r\n\
          Category 1 description = \u201Cmedical equipment includes wearable devices\u201D\
          \r\nCategory 2 description =\u201Csoftware for SaaS or downloadable\u201D\
          \r\n\u2026\r\n\r\nIn other words I am trying to ask \"hugging face\" to\
          \ learn the description of various categories that are given in sentences\
          \ or item lists.\r\n\r\nThen I am asking \"hugging face\" to classify a\
          \ paragraph description (my input) to the categories that best match this\
          \ input description."
        updatedAt: '2023-03-29T18:30:09.345Z'
      numEdits: 0
      reactions: []
    id: 642483b1f1d18f46decaab67
    type: comment
  author: hsuwill
  content: "Input (given in the form of a sentence/paragraph)= \u201Can application\
    \ that convert speech to text using AI\u201D\r\n\r\nDesired Output:  category\
    \ 1 match = 0.1 category 2 match = 0.9 \u2026\r\n\r\n--------------\r\nCategory\
    \ 1 description = \u201Cmedical equipment includes wearable devices\u201D\r\n\
    Category 2 description =\u201Csoftware for SaaS or downloadable\u201D\r\n\u2026\
    \r\n\r\nIn other words I am trying to ask \"hugging face\" to learn the description\
    \ of various categories that are given in sentences or item lists.\r\n\r\nThen\
    \ I am asking \"hugging face\" to classify a paragraph description (my input)\
    \ to the categories that best match this input description."
  created_at: 2023-03-29 17:30:09+00:00
  edited: false
  hidden: false
  id: 642483b1f1d18f46decaab67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679093906166-noauth.jpeg?w=200&h=200&f=face
      fullname: Saif Khayoon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Khayoon
      type: user
    createdAt: '2023-04-18T17:37:54.000Z'
    data:
      edited: false
      editors:
      - Khayoon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679093906166-noauth.jpeg?w=200&h=200&f=face
          fullname: Saif Khayoon
          isHf: false
          isPro: false
          name: Khayoon
          type: user
        html: "<p>learning_rate=5e-5<br>weight_decay=0.010<br>evaluation_strategy=\"\
          epoch\"</p>\n<p>The adjustments I picked for matching a text paragraph to\
          \ existing categories description. </p>\n<pre><code>pip install transformers\
          \ \n</code></pre>\n<p>To use the HF Transformers library </p>\n<p>Then clean\
          \ the data </p>\n<pre><code>{\n    \"text\": \"an application that convert\
          \ speech to text using AI\",\n    \"label\": 2\n},\n{\n    \"text\": \"\
          medical equipment includes wearable devices\",\n    \"label\": 1\n},\n{\n\
          \    \"text\": \"software for SaaS or downloadable\",\n    \"label\": 2\n\
          }\n</code></pre>\n<p>Use a script like the one below:</p>\n<pre><code>from\
          \ transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments,\
          \ Trainer\n#import from this model\ntokenizer = BertTokenizer.from_pretrained(\"\
          bert-base-uncased\")\nmodel = BertForSequenceClassification.from_pretrained(\"\
          bert-base-uncased\", num_labels=3)\n\ndef tokenize(batch):\n    return tokenizer(batch[\"\
          text\"], padding=True, truncation=True, max_length=512)\n\ntrain_dataset\
          \ = train_dataset.map(tokenize, batched=True, batch_size=None)\nval_dataset\
          \ = val_dataset.map(tokenize, batched=True, batch_size=None)\n\ntraining_args\
          \ = TrainingArguments(\n    output_dir=\"results\",\n    num_train_epochs=5,\n\
          \    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n\
          \    logging_dir=\"logs\",\n    logging_steps=100,\n    save_steps=1000,\n\
          \    evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    weight_decay=0.01,\n\
          )\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n\
          \    eval_dataset=val_dataset,\n)\ntrainer.train()\n</code></pre>\n<p>Save\
          \ it</p>\n<pre><code>trainer.save_model(\"fine-tuned-model\")\n\n# Load\
          \ the fine-tuned model\ntokenizer = BertTokenizer.from_pretrained(\"fine-tuned-model\"\
          )\nmodel = BertForSequenceClassification.from_pretrained(\"fine-tuned-model\"\
          )\n</code></pre>\n<p>Use it</p>\n<pre><code>def predict(text):\n    inputs\
          \ = tokenizer(text, return_tensors=\"pt\")\n    outputs = model(**inputs)\n\
          \    logits = outputs.logits\n    category = logits.argmax(dim=-1).item()\n\
          \    return category\n\ntext = \"an application that convert speech to text\
          \ using AI\"\ncategory = predict(text)\nprint(f\"Category match: {category}\"\
          )\n</code></pre>\n<p>in case this is no beuno for your case, here's another\
          \ good set of parameters you can try:</p>\n<pre><code># Set training arguments\n\
          training_args = TrainingArguments(\n    output_dir=\"results\",\n    num_train_epochs=4,\n\
          \    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n\
          \    logging_dir=\"logs\",\n    logging_steps=1000,\n    save_steps=5000,\n\
          \    evaluation_strategy=\"steps\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n\
          \    warmup_steps=500,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"\
          accuracy\",\n    greater_is_better=True,\n)\n</code></pre>\n"
        raw: "learning_rate=5e-5\nweight_decay=0.010\nevaluation_strategy=\"epoch\"\
          \n\nThe adjustments I picked for matching a text paragraph to existing categories\
          \ description. \n\n```\npip install transformers \n```\n\nTo use the HF\
          \ Transformers library \n\nThen clean the data \n```\n{\n    \"text\": \"\
          an application that convert speech to text using AI\",\n    \"label\": 2\n\
          },\n{\n    \"text\": \"medical equipment includes wearable devices\",\n\
          \    \"label\": 1\n},\n{\n    \"text\": \"software for SaaS or downloadable\"\
          ,\n    \"label\": 2\n}\n```\nUse a script like the one below:\n```\nfrom\
          \ transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments,\
          \ Trainer\n#import from this model\ntokenizer = BertTokenizer.from_pretrained(\"\
          bert-base-uncased\")\nmodel = BertForSequenceClassification.from_pretrained(\"\
          bert-base-uncased\", num_labels=3)\n\ndef tokenize(batch):\n    return tokenizer(batch[\"\
          text\"], padding=True, truncation=True, max_length=512)\n\ntrain_dataset\
          \ = train_dataset.map(tokenize, batched=True, batch_size=None)\nval_dataset\
          \ = val_dataset.map(tokenize, batched=True, batch_size=None)\n\ntraining_args\
          \ = TrainingArguments(\n    output_dir=\"results\",\n    num_train_epochs=5,\n\
          \    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n\
          \    logging_dir=\"logs\",\n    logging_steps=100,\n    save_steps=1000,\n\
          \    evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    weight_decay=0.01,\n\
          )\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n\
          \    eval_dataset=val_dataset,\n)\ntrainer.train()\n```\n\nSave it\n\n```\n\
          trainer.save_model(\"fine-tuned-model\")\n\n# Load the fine-tuned model\n\
          tokenizer = BertTokenizer.from_pretrained(\"fine-tuned-model\")\nmodel =\
          \ BertForSequenceClassification.from_pretrained(\"fine-tuned-model\")\n\n\
          ```\nUse it\n```\ndef predict(text):\n    inputs = tokenizer(text, return_tensors=\"\
          pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n    category\
          \ = logits.argmax(dim=-1).item()\n    return category\n\ntext = \"an application\
          \ that convert speech to text using AI\"\ncategory = predict(text)\nprint(f\"\
          Category match: {category}\")\n```\nin case this is no beuno for your case,\
          \ here's another good set of parameters you can try:\n```\n# Set training\
          \ arguments\ntraining_args = TrainingArguments(\n    output_dir=\"results\"\
          ,\n    num_train_epochs=4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n\
          \    logging_dir=\"logs\",\n    logging_steps=1000,\n    save_steps=5000,\n\
          \    evaluation_strategy=\"steps\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n\
          \    warmup_steps=500,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"\
          accuracy\",\n    greater_is_better=True,\n)\n```"
        updatedAt: '2023-04-18T17:37:54.286Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - hahahaxiao
    id: 643ed572f2ed3bc5c061c306
    type: comment
  author: Khayoon
  content: "learning_rate=5e-5\nweight_decay=0.010\nevaluation_strategy=\"epoch\"\n\
    \nThe adjustments I picked for matching a text paragraph to existing categories\
    \ description. \n\n```\npip install transformers \n```\n\nTo use the HF Transformers\
    \ library \n\nThen clean the data \n```\n{\n    \"text\": \"an application that\
    \ convert speech to text using AI\",\n    \"label\": 2\n},\n{\n    \"text\": \"\
    medical equipment includes wearable devices\",\n    \"label\": 1\n},\n{\n    \"\
    text\": \"software for SaaS or downloadable\",\n    \"label\": 2\n}\n```\nUse\
    \ a script like the one below:\n```\nfrom transformers import BertTokenizer, BertForSequenceClassification,\
    \ TrainingArguments, Trainer\n#import from this model\ntokenizer = BertTokenizer.from_pretrained(\"\
    bert-base-uncased\")\nmodel = BertForSequenceClassification.from_pretrained(\"\
    bert-base-uncased\", num_labels=3)\n\ndef tokenize(batch):\n    return tokenizer(batch[\"\
    text\"], padding=True, truncation=True, max_length=512)\n\ntrain_dataset = train_dataset.map(tokenize,\
    \ batched=True, batch_size=None)\nval_dataset = val_dataset.map(tokenize, batched=True,\
    \ batch_size=None)\n\ntraining_args = TrainingArguments(\n    output_dir=\"results\"\
    ,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n\
    \    logging_dir=\"logs\",\n    logging_steps=100,\n    save_steps=1000,\n   \
    \ evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    weight_decay=0.01,\n\
    )\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n\
    \    eval_dataset=val_dataset,\n)\ntrainer.train()\n```\n\nSave it\n\n```\ntrainer.save_model(\"\
    fine-tuned-model\")\n\n# Load the fine-tuned model\ntokenizer = BertTokenizer.from_pretrained(\"\
    fine-tuned-model\")\nmodel = BertForSequenceClassification.from_pretrained(\"\
    fine-tuned-model\")\n\n```\nUse it\n```\ndef predict(text):\n    inputs = tokenizer(text,\
    \ return_tensors=\"pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n\
    \    category = logits.argmax(dim=-1).item()\n    return category\n\ntext = \"\
    an application that convert speech to text using AI\"\ncategory = predict(text)\n\
    print(f\"Category match: {category}\")\n```\nin case this is no beuno for your\
    \ case, here's another good set of parameters you can try:\n```\n# Set training\
    \ arguments\ntraining_args = TrainingArguments(\n    output_dir=\"results\",\n\
    \    num_train_epochs=4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n\
    \    logging_dir=\"logs\",\n    logging_steps=1000,\n    save_steps=5000,\n  \
    \  evaluation_strategy=\"steps\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n\
    \    warmup_steps=500,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"\
    accuracy\",\n    greater_is_better=True,\n)\n```"
  created_at: 2023-04-18 16:37:54+00:00
  edited: false
  hidden: false
  id: 643ed572f2ed3bc5c061c306
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/82eb3afff930f7dc8b4e7b00a8152a88.svg
      fullname: hahahaxiao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hahahaxiao
      type: user
    createdAt: '2023-04-19T06:37:21.000Z'
    data:
      edited: false
      editors:
      - hahahaxiao
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/82eb3afff930f7dc8b4e7b00a8152a88.svg
          fullname: hahahaxiao
          isHf: false
          isPro: false
          name: hahahaxiao
          type: user
        html: '<p>cool</p>

          '
        raw: cool
        updatedAt: '2023-04-19T06:37:21.337Z'
      numEdits: 0
      reactions: []
    id: 643f8c21ba7506b57e376004
    type: comment
  author: hahahaxiao
  content: cool
  created_at: 2023-04-19 05:37:21+00:00
  edited: false
  hidden: false
  id: 643f8c21ba7506b57e376004
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 37
repo_id: bert-base-uncased
repo_type: model
status: open
target_branch: null
title: 'How to use transformer to match a text paragraph to existing categories description? '
