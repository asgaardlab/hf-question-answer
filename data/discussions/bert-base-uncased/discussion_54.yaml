!!python/object:huggingface_hub.community.DiscussionWithDetails
author: model-sizer-bot
conflicting_files: null
created_at: 2023-09-07 13:41:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cedfda2c9719121a4f3e44c650636650.svg
      fullname: Model Sizer Bot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: model-sizer-bot
      type: user
    createdAt: '2023-09-07T14:41:35.000Z'
    data:
      edited: false
      editors:
      - model-sizer-bot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7365753650665283
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cedfda2c9719121a4f3e44c650636650.svg
          fullname: Model Sizer Bot
          isHf: false
          isPro: false
          name: model-sizer-bot
          type: user
        html: "<h1 id=\"model-memory-requirements\">Model Memory Requirements</h1>\n\
          <p>You will need about {'dtype': 'float16/bfloat16', 'Largest Layer or Residual\
          \ Group': '44.71 MB', 'Total Size': '208.82 MB', 'Training using Adam':\
          \ '835.3 MB'} VRAM to load this model for inference, and {'dtype': 'int4',\
          \ 'Largest Layer or Residual Group': '11.18 MB', 'Total Size': '52.21 MB',\
          \ 'Training using Adam': '208.82 MB'} VRAM to train it using Adam.</p>\n\
          <p>These calculations were measured from the <a rel=\"nofollow\" href=\"\
          https://hf.co/spaces/hf-accelerate/model-memory-utility\">Model Memory Utility\
          \ Space</a> on the Hub.</p>\n<p>The minimum recommended vRAM needed for\
          \ this model assumes using <a href=\"https://huggingface.co/docs/accelerate/usage_guides/big_modeling\"\
          >Accelerate or <code>device_map=\"auto\"</code></a> and is denoted by the\
          \ size of the \"largest layer\".<br>When performing inference, expect to\
          \ add up to an additional 20% to this, as found by <a rel=\"nofollow\" href=\"\
          https://blog.eleuther.ai/transformer-math/\">EleutherAI</a>. More tests\
          \ will be performed in the future to get a more accurate benchmark for each\
          \ model.</p>\n<p>When training with <code>Adam</code>, you can expect roughly\
          \ 4x the reported results to be used. (1x for the model, 1x for the gradients,\
          \ and 2x for the optimizer).</p>\n<h2 id=\"results\">Results:</h2>\n<div\
          \ class=\"max-w-full overflow-auto\">\n\t<table>\n\t\t<thead><tr>\n<th align=\"\
          left\">dtype</th>\n<th align=\"left\">Largest Layer or Residual Group</th>\n\
          <th align=\"left\">Total Size</th>\n<th align=\"left\">Training using Adam</th>\n\
          </tr>\n\n\t\t</thead><tbody><tr>\n<td align=\"left\">float32</td>\n<td align=\"\
          left\">89.42 MB</td>\n<td align=\"left\">417.65 MB</td>\n<td align=\"left\"\
          >1.63 GB</td>\n</tr>\n<tr>\n<td align=\"left\">float16/bfloat16</td>\n<td\
          \ align=\"left\">44.71 MB</td>\n<td align=\"left\">208.82 MB</td>\n<td align=\"\
          left\">835.3 MB</td>\n</tr>\n<tr>\n<td align=\"left\">int8</td>\n<td align=\"\
          left\">22.35 MB</td>\n<td align=\"left\">104.41 MB</td>\n<td align=\"left\"\
          >417.65 MB</td>\n</tr>\n<tr>\n<td align=\"left\">int4</td>\n<td align=\"\
          left\">11.18 MB</td>\n<td align=\"left\">52.21 MB</td>\n<td align=\"left\"\
          >208.82 MB</td>\n</tr>\n</tbody>\n\t</table>\n</div>\n"
        raw: "# Model Memory Requirements\n\n\nYou will need about {'dtype': 'float16/bfloat16',\
          \ 'Largest Layer or Residual Group': '44.71 MB', 'Total Size': '208.82 MB',\
          \ 'Training using Adam': '835.3 MB'} VRAM to load this model for inference,\
          \ and {'dtype': 'int4', 'Largest Layer or Residual Group': '11.18 MB', 'Total\
          \ Size': '52.21 MB', 'Training using Adam': '208.82 MB'} VRAM to train it\
          \ using Adam.\n    \nThese calculations were measured from the [Model Memory\
          \ Utility Space](https://hf.co/spaces/hf-accelerate/model-memory-utility)\
          \ on the Hub.\n    \nThe minimum recommended vRAM needed for this model\
          \ assumes using [Accelerate or `device_map=\"auto\"`](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)\
          \ and is denoted by the size of the \"largest layer\". \nWhen performing\
          \ inference, expect to add up to an additional 20% to this, as found by\
          \ [EleutherAI](https://blog.eleuther.ai/transformer-math/). More tests will\
          \ be performed in the future to get a more accurate benchmark for each model.\n\
          \nWhen training with `Adam`, you can expect roughly 4x the reported results\
          \ to be used. (1x for the model, 1x for the gradients, and 2x for the optimizer).\n\
          \n## Results:\n\n| dtype            | Largest Layer or Residual Group  \
          \ | Total Size   | Training using Adam   |\n|:-----------------|:----------------------------------|:-------------|:----------------------|\n\
          | float32          | 89.42 MB                          | 417.65 MB    |\
          \ 1.63 GB               |\n| float16/bfloat16 | 44.71 MB               \
          \           | 208.82 MB    | 835.3 MB              |\n| int8           \
          \  | 22.35 MB                          | 104.41 MB    | 417.65 MB      \
          \       |\n| int4             | 11.18 MB                          | 52.21\
          \ MB     | 208.82 MB             |"
        updatedAt: '2023-09-07T14:41:35.910Z'
      numEdits: 0
      reactions: []
    id: 64f9e11f10c2811c40a36163
    type: comment
  author: model-sizer-bot
  content: "# Model Memory Requirements\n\n\nYou will need about {'dtype': 'float16/bfloat16',\
    \ 'Largest Layer or Residual Group': '44.71 MB', 'Total Size': '208.82 MB', 'Training\
    \ using Adam': '835.3 MB'} VRAM to load this model for inference, and {'dtype':\
    \ 'int4', 'Largest Layer or Residual Group': '11.18 MB', 'Total Size': '52.21\
    \ MB', 'Training using Adam': '208.82 MB'} VRAM to train it using Adam.\n    \n\
    These calculations were measured from the [Model Memory Utility Space](https://hf.co/spaces/hf-accelerate/model-memory-utility)\
    \ on the Hub.\n    \nThe minimum recommended vRAM needed for this model assumes\
    \ using [Accelerate or `device_map=\"auto\"`](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)\
    \ and is denoted by the size of the \"largest layer\". \nWhen performing inference,\
    \ expect to add up to an additional 20% to this, as found by [EleutherAI](https://blog.eleuther.ai/transformer-math/).\
    \ More tests will be performed in the future to get a more accurate benchmark\
    \ for each model.\n\nWhen training with `Adam`, you can expect roughly 4x the\
    \ reported results to be used. (1x for the model, 1x for the gradients, and 2x\
    \ for the optimizer).\n\n## Results:\n\n| dtype            | Largest Layer or\
    \ Residual Group   | Total Size   | Training using Adam   |\n|:-----------------|:----------------------------------|:-------------|:----------------------|\n\
    | float32          | 89.42 MB                          | 417.65 MB    | 1.63 GB\
    \               |\n| float16/bfloat16 | 44.71 MB                          | 208.82\
    \ MB    | 835.3 MB              |\n| int8             | 22.35 MB             \
    \             | 104.41 MB    | 417.65 MB             |\n| int4             | 11.18\
    \ MB                          | 52.21 MB     | 208.82 MB             |"
  created_at: 2023-09-07 13:41:35+00:00
  edited: false
  hidden: false
  id: 64f9e11f10c2811c40a36163
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2023-09-08T11:47:17.000Z'
    data:
      edited: false
      editors:
      - julien-c
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5807797312736511
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
          fullname: Julien Chaumond
          isHf: true
          isPro: true
          name: julien-c
          type: user
        html: "<p>duplicate of <a href=\"/bert-base-uncased/discussions/53\">#53</a>\
          \ so closing this one, cc <span data-props=\"{&quot;user&quot;:&quot;muellerzr&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/muellerzr\"\
          >@<span class=\"underline\">muellerzr</span></a></span>\n\n\t</span></span></p>\n"
        raw: 'duplicate of #53 so closing this one, cc @muellerzr'
        updatedAt: '2023-09-08T11:47:17.221Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64fb09c5d82fc6977d653aeb
    id: 64fb09c5d82fc6977d653aea
    type: comment
  author: julien-c
  content: 'duplicate of #53 so closing this one, cc @muellerzr'
  created_at: 2023-09-08 10:47:17+00:00
  edited: false
  hidden: false
  id: 64fb09c5d82fc6977d653aea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg?w=200&h=200&f=face
      fullname: Julien Chaumond
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: true
      name: julien-c
      type: user
    createdAt: '2023-09-08T11:47:17.000Z'
    data:
      status: closed
    id: 64fb09c5d82fc6977d653aeb
    type: status-change
  author: julien-c
  created_at: 2023-09-08 10:47:17+00:00
  id: 64fb09c5d82fc6977d653aeb
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 54
repo_id: bert-base-uncased
repo_type: model
status: closed
target_branch: null
title: '[AUTOMATED] Model Memory Requirements'
