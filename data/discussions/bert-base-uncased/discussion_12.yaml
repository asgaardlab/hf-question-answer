!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ethanjyx
conflicting_files: null
created_at: 2022-10-02 04:37:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98f61914f8b164db64e94c9785bdff01.svg
      fullname: Ethan Jiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ethanjyx
      type: user
    createdAt: '2022-10-02T05:37:08.000Z'
    data:
      edited: false
      editors:
      - ethanjyx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98f61914f8b164db64e94c9785bdff01.svg
          fullname: Ethan Jiang
          isHf: false
          isPro: false
          name: ethanjyx
          type: user
        html: '<p>Hey there, I am interested in finetuning this <code>bert-base-uncased</code>,
          how can I do it?</p>

          <p>I found this tutorial <a href="https://huggingface.co/docs/transformers/training">https://huggingface.co/docs/transformers/training</a>,
          but it focuses on finetuning a prediction head rather than the backbone
          weights. </p>

          <p>I would like to </p>

          <ol>

          <li>finetune the backbone weights here, by dumping large corpus of texts
          from my domain,</li>

          <li>train a prediction head with a more limited dataset from my domain<br>is
          that possible?</li>

          </ol>

          '
        raw: "Hey there, I am interested in finetuning this `bert-base-uncased`, how\
          \ can I do it?\r\n\r\nI found this tutorial https://huggingface.co/docs/transformers/training,\
          \ but it focuses on finetuning a prediction head rather than the backbone\
          \ weights. \r\n\r\nI would like to \r\n1. finetune the backbone weights\
          \ here, by dumping large corpus of texts from my domain,\r\n2. train a prediction\
          \ head with a more limited dataset from my domain\r\nis that possible? "
        updatedAt: '2022-10-02T05:37:08.784Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - nikharanirghin
        - CWKSC
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - thefivespace
        - CWKSC
    id: 6339238420fc636fd8b83676
    type: comment
  author: ethanjyx
  content: "Hey there, I am interested in finetuning this `bert-base-uncased`, how\
    \ can I do it?\r\n\r\nI found this tutorial https://huggingface.co/docs/transformers/training,\
    \ but it focuses on finetuning a prediction head rather than the backbone weights.\
    \ \r\n\r\nI would like to \r\n1. finetune the backbone weights here, by dumping\
    \ large corpus of texts from my domain,\r\n2. train a prediction head with a more\
    \ limited dataset from my domain\r\nis that possible? "
  created_at: 2022-10-02 04:37:08+00:00
  edited: false
  hidden: false
  id: 6339238420fc636fd8b83676
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9684f04a54bda40fbc3fd0522b16dfe9.svg
      fullname: Nikhara Nirghin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nikharanirghin
      type: user
    createdAt: '2022-11-23T19:46:47.000Z'
    data:
      edited: false
      editors:
      - nikharanirghin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9684f04a54bda40fbc3fd0522b16dfe9.svg
          fullname: Nikhara Nirghin
          isHf: false
          isPro: false
          name: nikharanirghin
          type: user
        html: '<p>Hey Ethan! Would love to chat on this if you have a few minutes
          to spare. Please let me know :)</p>

          '
        raw: Hey Ethan! Would love to chat on this if you have a few minutes to spare.
          Please let me know :)
        updatedAt: '2022-11-23T19:46:47.444Z'
      numEdits: 0
      reactions: []
    id: 637e78a7ed78be2e9058e314
    type: comment
  author: nikharanirghin
  content: Hey Ethan! Would love to chat on this if you have a few minutes to spare.
    Please let me know :)
  created_at: 2022-11-23 19:46:47+00:00
  edited: false
  hidden: false
  id: 637e78a7ed78be2e9058e314
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98f61914f8b164db64e94c9785bdff01.svg
      fullname: Ethan Jiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ethanjyx
      type: user
    createdAt: '2022-11-23T21:49:28.000Z'
    data:
      edited: false
      editors:
      - ethanjyx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98f61914f8b164db64e94c9785bdff01.svg
          fullname: Ethan Jiang
          isHf: false
          isPro: false
          name: ethanjyx
          type: user
        html: "<blockquote>\n<p>Hey Ethan! Would love to chat on this if you have\
          \ a few minutes to spare. Please let me know :)</p>\n</blockquote>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;nikharanirghin&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nikharanirghin\">@<span\
          \ class=\"underline\">nikharanirghin</span></a></span>\n\n\t</span></span>\
          \ ? what do you want to chat about?</p>\n"
        raw: '> Hey Ethan! Would love to chat on this if you have a few minutes to
          spare. Please let me know :)


          @nikharanirghin ? what do you want to chat about?'
        updatedAt: '2022-11-23T21:49:28.623Z'
      numEdits: 0
      reactions: []
    id: 637e9568fb70160a30688cbe
    type: comment
  author: ethanjyx
  content: '> Hey Ethan! Would love to chat on this if you have a few minutes to spare.
    Please let me know :)


    @nikharanirghin ? what do you want to chat about?'
  created_at: 2022-11-23 21:49:28+00:00
  edited: false
  hidden: false
  id: 637e9568fb70160a30688cbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9684f04a54bda40fbc3fd0522b16dfe9.svg
      fullname: Nikhara Nirghin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nikharanirghin
      type: user
    createdAt: '2022-11-23T21:50:47.000Z'
    data:
      edited: false
      editors:
      - nikharanirghin
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9684f04a54bda40fbc3fd0522b16dfe9.svg
          fullname: Nikhara Nirghin
          isHf: false
          isPro: false
          name: nikharanirghin
          type: user
        html: '<p>Feedback on finetuning bert!</p>

          '
        raw: Feedback on finetuning bert!
        updatedAt: '2022-11-23T21:50:47.136Z'
      numEdits: 0
      reactions: []
    id: 637e95b70fa26740e54ecdfa
    type: comment
  author: nikharanirghin
  content: Feedback on finetuning bert!
  created_at: 2022-11-23 21:50:47+00:00
  edited: false
  hidden: false
  id: 637e95b70fa26740e54ecdfa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/279f50f651d7fa74744459dd4777191e.svg
      fullname: Away Raided W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nasa2808
      type: user
    createdAt: '2022-12-02T15:08:46.000Z'
    data:
      edited: false
      editors:
      - Nasa2808
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/279f50f651d7fa74744459dd4777191e.svg
          fullname: Away Raided W
          isHf: false
          isPro: false
          name: Nasa2808
          type: user
        html: '<p>can you help me too ? I want to fine-tuning this model too</p>

          '
        raw: can you help me too ? I want to fine-tuning this model too
        updatedAt: '2022-12-02T15:08:46.771Z'
      numEdits: 0
      reactions: []
    id: 638a14fe997299b15ba470ba
    type: comment
  author: Nasa2808
  content: can you help me too ? I want to fine-tuning this model too
  created_at: 2022-12-02 15:08:46+00:00
  edited: false
  hidden: false
  id: 638a14fe997299b15ba470ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
      fullname: bread null
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: breadlicker45
      type: user
    createdAt: '2023-01-19T13:50:27.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
          fullname: bread null
          isHf: false
          isPro: false
          name: breadlicker45
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-01-27T16:44:28.817Z'
      numEdits: 0
      reactions: []
    id: 63c94aa38afd58b4409a8345
    type: comment
  author: breadlicker45
  content: This comment has been hidden
  created_at: 2023-01-19 13:50:27+00:00
  edited: true
  hidden: true
  id: 63c94aa38afd58b4409a8345
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/87d90ec748708e40ecdec215a8cc5af8.svg
      fullname: Alex Jenkins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Immortalizer
      type: user
    createdAt: '2023-01-24T04:05:33.000Z'
    data:
      edited: false
      editors:
      - Immortalizer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/87d90ec748708e40ecdec215a8cc5af8.svg
          fullname: Alex Jenkins
          isHf: false
          isPro: false
          name: Immortalizer
          type: user
        html: "<p>You can simply train the complete model with a very low learning\
          \ rate to fine-tune the entire model.<br>When you load the pretrained model\
          \ and set model.train() it will, by default, have all the layers enabled\
          \ for back propagation.<br>IE:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >single_training_epoch</span>(<span class=\"hljs-params\">model, optimizer,\
          \ train_dataloader</span>):\n    model.train()\n    <span class=\"hljs-comment\"\
          ># Loop over the training set</span>\n    <span class=\"hljs-keyword\">for</span>\
          \ input_ids, attention_masks, labels <span class=\"hljs-keyword\">in</span>\
          \ train_dataloader:\n        <span class=\"hljs-comment\"># Clear the gradients</span>\n\
          \        optimizer.zero_grad()\n        <span class=\"hljs-comment\"># Forward\
          \ pass</span>\n        outputs = model(input_ids, attention_mask=attention_masks,\
          \ labels=labels)\n        loss = outputs[<span class=\"hljs-number\">0</span>]\n\
          \        <span class=\"hljs-comment\"># Backward pass</span>\n        loss.backward()\n\
          \        optimizer.step()\n    <span class=\"hljs-keyword\">return</span>\
          \ model, optimizer\n</code></pre>\n<p>You can also manually set each layer\
          \ (True = updates/trains, False = do not update during back-prop) using\
          \ a loop:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >for</span> param <span class=\"hljs-keyword\">in</span> model.bert.parameters():\n\
          \    param.requires_grad = <span class=\"hljs-literal\">True</span>\n</code></pre>\n\
          <p>My recommendation from there is to save the model out and then train\
          \ a new model taking the output from BERT as an input.<br>This enables a\
          \ few things:</p>\n<ul>\n<li>WAY faster training/retraining of your 'prediction\
          \ head' model as you can run the data through the previous model a single\
          \ time and then train your smaller model.</li>\n<li>Easier to retrain and\
          \ experiment with different architectures of your 'prediction head' without\
          \ even interacting with the BERT model.</li>\n<li>Able to add additional\
          \ values into your model (such as ints and floats that could be present\
          \ in other data fields - or your features you've created yourself)</li>\n\
          </ul>\n<p>The one drawback is slightly slower inference time... but this\
          \ can be mitigated by creating a proper pipeline (or a more advanced method\
          \ would be to load them separately with their weights and merge the models\
          \ together).</p>\n"
        raw: "You can simply train the complete model with a very low learning rate\
          \ to fine-tune the entire model.\nWhen you load the pretrained model and\
          \ set model.train() it will, by default, have all the layers enabled for\
          \ back propagation.\nIE:\n``` python\ndef single_training_epoch(model, optimizer,\
          \ train_dataloader):\n    model.train()\n    # Loop over the training set\n\
          \    for input_ids, attention_masks, labels in train_dataloader:\n     \
          \   # Clear the gradients\n        optimizer.zero_grad()\n        # Forward\
          \ pass\n        outputs = model(input_ids, attention_mask=attention_masks,\
          \ labels=labels)\n        loss = outputs[0]\n        # Backward pass\n \
          \       loss.backward()\n        optimizer.step()\n    return model, optimizer\n\
          ```\n\nYou can also manually set each layer (True = updates/trains, False\
          \ = do not update during back-prop) using a loop:\n``` python\nfor param\
          \ in model.bert.parameters():\n    param.requires_grad = True\n```\nMy recommendation\
          \ from there is to save the model out and then train a new model taking\
          \ the output from BERT as an input.\nThis enables a few things:\n- WAY faster\
          \ training/retraining of your 'prediction head' model as you can run the\
          \ data through the previous model a single time and then train your smaller\
          \ model.\n- Easier to retrain and experiment with different architectures\
          \ of your 'prediction head' without even interacting with the BERT model.\n\
          - Able to add additional values into your model (such as ints and floats\
          \ that could be present in other data fields - or your features you've created\
          \ yourself)\n\nThe one drawback is slightly slower inference time... but\
          \ this can be mitigated by creating a proper pipeline (or a more advanced\
          \ method would be to load them separately with their weights and merge the\
          \ models together)."
        updatedAt: '2023-01-24T04:05:33.270Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - abdallahm13
    id: 63cf590df971d796853d4519
    type: comment
  author: Immortalizer
  content: "You can simply train the complete model with a very low learning rate\
    \ to fine-tune the entire model.\nWhen you load the pretrained model and set model.train()\
    \ it will, by default, have all the layers enabled for back propagation.\nIE:\n\
    ``` python\ndef single_training_epoch(model, optimizer, train_dataloader):\n \
    \   model.train()\n    # Loop over the training set\n    for input_ids, attention_masks,\
    \ labels in train_dataloader:\n        # Clear the gradients\n        optimizer.zero_grad()\n\
    \        # Forward pass\n        outputs = model(input_ids, attention_mask=attention_masks,\
    \ labels=labels)\n        loss = outputs[0]\n        # Backward pass\n       \
    \ loss.backward()\n        optimizer.step()\n    return model, optimizer\n```\n\
    \nYou can also manually set each layer (True = updates/trains, False = do not\
    \ update during back-prop) using a loop:\n``` python\nfor param in model.bert.parameters():\n\
    \    param.requires_grad = True\n```\nMy recommendation from there is to save\
    \ the model out and then train a new model taking the output from BERT as an input.\n\
    This enables a few things:\n- WAY faster training/retraining of your 'prediction\
    \ head' model as you can run the data through the previous model a single time\
    \ and then train your smaller model.\n- Easier to retrain and experiment with\
    \ different architectures of your 'prediction head' without even interacting with\
    \ the BERT model.\n- Able to add additional values into your model (such as ints\
    \ and floats that could be present in other data fields - or your features you've\
    \ created yourself)\n\nThe one drawback is slightly slower inference time... but\
    \ this can be mitigated by creating a proper pipeline (or a more advanced method\
    \ would be to load them separately with their weights and merge the models together)."
  created_at: 2023-01-24 04:05:33+00:00
  edited: false
  hidden: false
  id: 63cf590df971d796853d4519
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: bert-base-uncased
repo_type: model
status: open
target_branch: null
title: How to I fine tune this model?
