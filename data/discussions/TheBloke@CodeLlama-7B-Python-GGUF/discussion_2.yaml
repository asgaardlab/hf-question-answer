!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mukel
conflicting_files: null
created_at: 2023-08-25 00:18:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6f7ab90fee325076fe9b19fd5a158e2f.svg
      fullname: "Alfonso\xB2 Peterssen"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mukel
      type: user
    createdAt: '2023-08-25T01:18:25.000Z'
    data:
      edited: false
      editors:
      - mukel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9370924830436707
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6f7ab90fee325076fe9b19fd5a158e2f.svg
          fullname: "Alfonso\xB2 Peterssen"
          isHf: false
          isPro: false
          name: mukel
          type: user
        html: '<p>The regular and instruct models are fine, it only happens with this
          one (the python one).<br>F32 is fine, but not all consumers of the model
          support the more esoteric k-quants types.</p>

          '
        raw: "The regular and instruct models are fine, it only happens with this\
          \ one (the python one).\r\nF32 is fine, but not all consumers of the model\
          \ support the more esoteric k-quants types."
        updatedAt: '2023-08-25T01:18:25.183Z'
      numEdits: 0
      reactions: []
    id: 64e80161fb0dc2f84ca214b8
    type: comment
  author: mukel
  content: "The regular and instruct models are fine, it only happens with this one\
    \ (the python one).\r\nF32 is fine, but not all consumers of the model support\
    \ the more esoteric k-quants types."
  created_at: 2023-08-25 00:18:25+00:00
  edited: false
  hidden: false
  id: 64e80161fb0dc2f84ca214b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-25T07:26:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9709572196006775
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK, If you think that''s a bug you''d need to report it to llama.cpp.
          I just told it to make Q8_0.</p>

          <p>However I''m not sure what you mean by ''not all consumers'' supporting
          this?  This is GGUF, the new format. Currently most third party libraries
          do not support GGUF at all. But when they do, they will support everything,
          the same as llama.cpp.    Maybe consumers of GGML don''t support mixed tensors
          in q8_0 (I don''t know), but this is not GGML any more.</p>

          '
        raw: 'OK, If you think that''s a bug you''d need to report it to llama.cpp.
          I just told it to make Q8_0.


          However I''m not sure what you mean by ''not all consumers'' supporting
          this?  This is GGUF, the new format. Currently most third party libraries
          do not support GGUF at all. But when they do, they will support everything,
          the same as llama.cpp.    Maybe consumers of GGML don''t support mixed tensors
          in q8_0 (I don''t know), but this is not GGML any more.'
        updatedAt: '2023-08-25T07:26:33.046Z'
      numEdits: 0
      reactions: []
    id: 64e857a964e7b5f642e52e0c
    type: comment
  author: TheBloke
  content: 'OK, If you think that''s a bug you''d need to report it to llama.cpp.
    I just told it to make Q8_0.


    However I''m not sure what you mean by ''not all consumers'' supporting this?  This
    is GGUF, the new format. Currently most third party libraries do not support GGUF
    at all. But when they do, they will support everything, the same as llama.cpp.    Maybe
    consumers of GGML don''t support mixed tensors in q8_0 (I don''t know), but this
    is not GGML any more.'
  created_at: 2023-08-25 06:26:33+00:00
  edited: false
  hidden: false
  id: 64e857a964e7b5f642e52e0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6f7ab90fee325076fe9b19fd5a158e2f.svg
      fullname: "Alfonso\xB2 Peterssen"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mukel
      type: user
    createdAt: '2023-08-25T12:53:25.000Z'
    data:
      edited: false
      editors:
      - mukel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.885840654373169
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6f7ab90fee325076fe9b19fd5a158e2f.svg
          fullname: "Alfonso\xB2 Peterssen"
          isHf: false
          isPro: false
          name: mukel
          type: user
        html: '<p>From this PR: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/1684">https://github.com/ggerganov/llama.cpp/pull/1684</a><br>It
          is stated in the description:<br>"Not mentioned explicitly above is the
          fact that with this PR, all quantization variants use 6-bit quantization
          for the output.weight tensor. This lowers the perplexity of, e.g., Q4_0
          by about 0.03 at 7B."<br>This is a possible explanation why the conversion
          mix tensor types.</p>

          '
        raw: 'From this PR: https://github.com/ggerganov/llama.cpp/pull/1684

          It is stated in the description:

          "Not mentioned explicitly above is the fact that with this PR, all quantization
          variants use 6-bit quantization for the output.weight tensor. This lowers
          the perplexity of, e.g., Q4_0 by about 0.03 at 7B."

          This is a possible explanation why the conversion mix tensor types.'
        updatedAt: '2023-08-25T12:53:25.587Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 64e8a445d021ea7dfab915b0
    type: comment
  author: mukel
  content: 'From this PR: https://github.com/ggerganov/llama.cpp/pull/1684

    It is stated in the description:

    "Not mentioned explicitly above is the fact that with this PR, all quantization
    variants use 6-bit quantization for the output.weight tensor. This lowers the
    perplexity of, e.g., Q4_0 by about 0.03 at 7B."

    This is a possible explanation why the conversion mix tensor types.'
  created_at: 2023-08-25 11:53:25+00:00
  edited: false
  hidden: false
  id: 64e8a445d021ea7dfab915b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-25T13:05:08.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9921785593032837
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK thanks, good to know.  I do agree that it''s a bit odd that q8_0
          would not be 100% q8_0 in all tensors.</p>

          <p>I just re-read your first message - are you saying that the Q8_0 models
          for the 7B and 7B-Instruct models are different?  They don''t have q6_k
          in their q8_0?  Because they should all be the same, they were all created
          in exactly the same way.</p>

          '
        raw: 'OK thanks, good to know.  I do agree that it''s a bit odd that q8_0
          would not be 100% q8_0 in all tensors.


          I just re-read your first message - are you saying that the Q8_0 models
          for the 7B and 7B-Instruct models are different?  They don''t have q6_k
          in their q8_0?  Because they should all be the same, they were all created
          in exactly the same way.'
        updatedAt: '2023-08-25T13:05:08.199Z'
      numEdits: 0
      reactions: []
    id: 64e8a7040150435eaf7afd2f
    type: comment
  author: TheBloke
  content: 'OK thanks, good to know.  I do agree that it''s a bit odd that q8_0 would
    not be 100% q8_0 in all tensors.


    I just re-read your first message - are you saying that the Q8_0 models for the
    7B and 7B-Instruct models are different?  They don''t have q6_k in their q8_0?  Because
    they should all be the same, they were all created in exactly the same way.'
  created_at: 2023-08-25 12:05:08+00:00
  edited: false
  hidden: false
  id: 64e8a7040150435eaf7afd2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6f7ab90fee325076fe9b19fd5a158e2f.svg
      fullname: "Alfonso\xB2 Peterssen"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mukel
      type: user
    createdAt: '2023-09-06T18:22:23.000Z'
    data:
      edited: false
      editors:
      - mukel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8718320727348328
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6f7ab90fee325076fe9b19fd5a158e2f.svg
          fullname: "Alfonso\xB2 Peterssen"
          isHf: false
          isPro: false
          name: mukel
          type: user
        html: '<p>The issue is fixed with the latest update, all tensors are either
          Q8_0 or F32.</p>

          '
        raw: The issue is fixed with the latest update, all tensors are either Q8_0
          or F32.
        updatedAt: '2023-09-06T18:22:23.843Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64f8c35f9433a36edf654a7f
    id: 64f8c35f9433a36edf654a7d
    type: comment
  author: mukel
  content: The issue is fixed with the latest update, all tensors are either Q8_0
    or F32.
  created_at: 2023-09-06 17:22:23+00:00
  edited: false
  hidden: false
  id: 64f8c35f9433a36edf654a7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6f7ab90fee325076fe9b19fd5a158e2f.svg
      fullname: "Alfonso\xB2 Peterssen"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mukel
      type: user
    createdAt: '2023-09-06T18:22:23.000Z'
    data:
      status: closed
    id: 64f8c35f9433a36edf654a7f
    type: status-change
  author: mukel
  created_at: 2023-09-06 17:22:23+00:00
  id: 64f8c35f9433a36edf654a7f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/CodeLlama-7B-Python-GGUF
repo_type: model
status: closed
target_branch: null
title: The Q8_0 have mixed tensor types e.g. 'output.norm' is of type  Q6_K.
