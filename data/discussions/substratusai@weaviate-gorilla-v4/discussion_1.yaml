!!python/object:huggingface_hub.community.DiscussionWithDetails
author: randallgann
conflicting_files: null
created_at: 2023-09-18 16:53:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9432a795e4d8a7d48ee551d5dd6bb047.svg
      fullname: Randall Gann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: randallgann
      type: user
    createdAt: '2023-09-18T17:53:12.000Z'
    data:
      edited: false
      editors:
      - randallgann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9217666983604431
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9432a795e4d8a7d48ee551d5dd6bb047.svg
          fullname: Randall Gann
          isHf: false
          isPro: false
          name: randallgann
          type: user
        html: '<p>Hi, I''m trying to understand why the model output would be different
          when deployed to an Inference Endpoint using the default settings.  I''m
          using the same example prompt provided in the colab notebook but when using
          the Inference Endpoint deployment the output is<br>{<br>  Get {<br>    HistoricalEvent(<br>      bm25:
          {</p>

          <p>Any insight as to what I may be doing wrong with deploying the model
          to an inference endpoint would be much appreciated.</p>

          '
        raw: "Hi, I'm trying to understand why the model output would be different\
          \ when deployed to an Inference Endpoint using the default settings.  I'm\
          \ using the same example prompt provided in the colab notebook but when\
          \ using the Inference Endpoint deployment the output is \r\n{\r\n  Get {\r\
          \n    HistoricalEvent(\r\n      bm25: {\r\n\r\nAny insight as to what I\
          \ may be doing wrong with deploying the model to an inference endpoint would\
          \ be much appreciated."
        updatedAt: '2023-09-18T17:53:12.533Z'
      numEdits: 0
      reactions: []
    id: 65088e88f37afbab0d19b26c
    type: comment
  author: randallgann
  content: "Hi, I'm trying to understand why the model output would be different when\
    \ deployed to an Inference Endpoint using the default settings.  I'm using the\
    \ same example prompt provided in the colab notebook but when using the Inference\
    \ Endpoint deployment the output is \r\n{\r\n  Get {\r\n    HistoricalEvent(\r\
    \n      bm25: {\r\n\r\nAny insight as to what I may be doing wrong with deploying\
    \ the model to an inference endpoint would be much appreciated."
  created_at: 2023-09-18 16:53:12+00:00
  edited: false
  hidden: false
  id: 65088e88f37afbab0d19b26c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: substratusai/weaviate-gorilla-v4
repo_type: model
status: open
target_branch: null
title: Model output is different when deployed to Inference Endpoint
