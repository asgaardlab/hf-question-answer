!!python/object:huggingface_hub.community.DiscussionWithDetails
author: raihankhan
conflicting_files: null
created_at: 2023-01-06 10:21:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d60ccc8da101a1dc861a2a16c696d127.svg
      fullname: Raihan Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: raihankhan
      type: user
    createdAt: '2023-01-06T10:21:10.000Z'
    data:
      edited: false
      editors:
      - raihankhan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d60ccc8da101a1dc861a2a16c696d127.svg
          fullname: Raihan Khan
          isHf: false
          isPro: false
          name: raihankhan
          type: user
        html: '<p>For now, there is no documentation regarding the parameters of the
          Inference API. However, the API call returns an error when the input is
          too long.  Hence, documentation for parameters such as max tokens are required
          to be mentioned in the README.md</p>

          '
        raw: For now, there is no documentation regarding the parameters of the Inference
          API. However, the API call returns an error when the input is too long.  Hence,
          documentation for parameters such as max tokens are required to be mentioned
          in the README.md
        updatedAt: '2023-01-06T10:21:10.815Z'
      numEdits: 0
      reactions: []
    id: 63b7f6164705f0ed5d7ab612
    type: comment
  author: raihankhan
  content: For now, there is no documentation regarding the parameters of the Inference
    API. However, the API call returns an error when the input is too long.  Hence,
    documentation for parameters such as max tokens are required to be mentioned in
    the README.md
  created_at: 2023-01-06 10:21:10+00:00
  edited: false
  hidden: false
  id: 63b7f6164705f0ed5d7ab612
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6032802e1f993496bc14d9e3/w6hr-DEQot4VVkoyRIBiy.png?w=200&h=200&f=face
      fullname: Omar Sanseviero
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: osanseviero
      type: user
    createdAt: '2023-01-09T17:36:02.000Z'
    data:
      edited: false
      editors:
      - osanseviero
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6032802e1f993496bc14d9e3/w6hr-DEQot4VVkoyRIBiy.png?w=200&h=200&f=face
          fullname: Omar Sanseviero
          isHf: true
          isPro: false
          name: osanseviero
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Narsil\">@<span class=\"\
          underline\">Narsil</span></a></span>\n\n\t</span></span></p>\n"
        raw: cc @Narsil
        updatedAt: '2023-01-09T17:36:02.324Z'
      numEdits: 0
      reactions: []
    id: 63bc50823c229a497badb96c
    type: comment
  author: osanseviero
  content: cc @Narsil
  created_at: 2023-01-09 17:36:02+00:00
  edited: false
  hidden: false
  id: 63bc50823c229a497badb96c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2023-01-10T19:26:09.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: "<p>Unfortunately, <code>max_tokens</code> will  not necessarily work,\
          \ you can probably use <code>truncation</code>.</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> os\n\nAPI_TOKEN\
          \ = os.getenv(<span class=\"hljs-string\">\"HF_API_TOKEN\"</span>)\n<span\
          \ class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\"\
          >import</span> requests\nheaders = {<span class=\"hljs-string\">\"Authorization\"\
          </span>: <span class=\"hljs-string\">f\"Bearer <span class=\"hljs-subst\"\
          >{API_TOKEN}</span>\"</span>}\nAPI_URL = <span class=\"hljs-string\">\"\
          https://api-inference.huggingface.co/models/roberta-base-openai-detector\"\
          </span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\
          \ function_\">query</span>(<span class=\"hljs-params\">payload</span>):\n\
          \    data = json.dumps(payload)\n    response = requests.request(<span class=\"\
          hljs-string\">\"POST\"</span>, API_URL, headers=headers, data=data)\n  \
          \  <span class=\"hljs-keyword\">return</span> json.loads(response.content.decode(<span\
          \ class=\"hljs-string\">\"utf-8\"</span>))\ndata = query({<span class=\"\
          hljs-string\">\"inputs\"</span>: <span class=\"hljs-string\">\"I like you.\
          \ I love you\"</span> * <span class=\"hljs-number\">100</span>, <span class=\"\
          hljs-string\">\"parameters\"</span>:  { <span class=\"hljs-string\">\"truncation\"\
          </span>: <span class=\"hljs-literal\">True</span>}})\n<span class=\"hljs-built_in\"\
          >print</span>(data)\n</code></pre>\n<p>This is undocumented indeed, will\
          \ work on it since this model is getting a lot more usage. In general most\
          \ <code>pipeline</code> parameters from <code>transformers</code> are supported\
          \ and knowingly not necessarily documented (so we can modify/accept/reject\
          \ them in the API where it makes sense. This is mostly to prevent abuse\
          \ when we do it.</p>\n<p>Is that clearer to you ? Does the solution work\
          \ for you ?</p>\n<p>Cheers,<br>Nicolas</p>\n"
        raw: "Unfortunately, `max_tokens` will  not necessarily work, you can probably\
          \ use `truncation`.\n\n```python\nimport os\n\nAPI_TOKEN = os.getenv(\"\
          HF_API_TOKEN\")\nimport json\nimport requests\nheaders = {\"Authorization\"\
          : f\"Bearer {API_TOKEN}\"}\nAPI_URL = \"https://api-inference.huggingface.co/models/roberta-base-openai-detector\"\
          \ndef query(payload):\n    data = json.dumps(payload)\n    response = requests.request(\"\
          POST\", API_URL, headers=headers, data=data)\n    return json.loads(response.content.decode(\"\
          utf-8\"))\ndata = query({\"inputs\": \"I like you. I love you\" * 100, \"\
          parameters\":  { \"truncation\": True}})\nprint(data)\n```\n\nThis is undocumented\
          \ indeed, will work on it since this model is getting a lot more usage.\
          \ In general most `pipeline` parameters from `transformers` are supported\
          \ and knowingly not necessarily documented (so we can modify/accept/reject\
          \ them in the API where it makes sense. This is mostly to prevent abuse\
          \ when we do it.\n\nIs that clearer to you ? Does the solution work for\
          \ you ?\n\nCheers,\nNicolas"
        updatedAt: '2023-01-10T19:26:09.345Z'
      numEdits: 0
      reactions: []
    id: 63bdbbd1bb80ec4ef14db05b
    type: comment
  author: Narsil
  content: "Unfortunately, `max_tokens` will  not necessarily work, you can probably\
    \ use `truncation`.\n\n```python\nimport os\n\nAPI_TOKEN = os.getenv(\"HF_API_TOKEN\"\
    )\nimport json\nimport requests\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"\
    }\nAPI_URL = \"https://api-inference.huggingface.co/models/roberta-base-openai-detector\"\
    \ndef query(payload):\n    data = json.dumps(payload)\n    response = requests.request(\"\
    POST\", API_URL, headers=headers, data=data)\n    return json.loads(response.content.decode(\"\
    utf-8\"))\ndata = query({\"inputs\": \"I like you. I love you\" * 100, \"parameters\"\
    :  { \"truncation\": True}})\nprint(data)\n```\n\nThis is undocumented indeed,\
    \ will work on it since this model is getting a lot more usage. In general most\
    \ `pipeline` parameters from `transformers` are supported and knowingly not necessarily\
    \ documented (so we can modify/accept/reject them in the API where it makes sense.\
    \ This is mostly to prevent abuse when we do it.\n\nIs that clearer to you ? Does\
    \ the solution work for you ?\n\nCheers,\nNicolas"
  created_at: 2023-01-10 19:26:09+00:00
  edited: false
  hidden: false
  id: 63bdbbd1bb80ec4ef14db05b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d60ccc8da101a1dc861a2a16c696d127.svg
      fullname: Raihan Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: raihankhan
      type: user
    createdAt: '2023-01-11T11:57:33.000Z'
    data:
      edited: false
      editors:
      - raihankhan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d60ccc8da101a1dc861a2a16c696d127.svg
          fullname: Raihan Khan
          isHf: false
          isPro: false
          name: raihankhan
          type: user
        html: "<p>hey <span data-props=\"{&quot;user&quot;:&quot;Narsil&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Narsil\">@<span class=\"\
          underline\">Narsil</span></a></span>\n\n\t</span></span> thanks for such\
          \ a wonderful explanation. I just had one more small doubt : could you explain\
          \ to me what truncation actually does?</p>\n<p>Thanks in advance.</p>\n\
          <p>Regards,<br>Raihan</p>\n"
        raw: 'hey @Narsil thanks for such a wonderful explanation. I just had one
          more small doubt : could you explain to me what truncation actually does?


          Thanks in advance.


          Regards,

          Raihan'
        updatedAt: '2023-01-11T11:57:33.972Z'
      numEdits: 0
      reactions: []
    id: 63bea42d82f7306d07464409
    type: comment
  author: raihankhan
  content: 'hey @Narsil thanks for such a wonderful explanation. I just had one more
    small doubt : could you explain to me what truncation actually does?


    Thanks in advance.


    Regards,

    Raihan'
  created_at: 2023-01-11 11:57:33+00:00
  edited: false
  hidden: false
  id: 63bea42d82f7306d07464409
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
      fullname: Nicolas Patry
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Narsil
      type: user
    createdAt: '2023-01-16T10:49:19.000Z'
    data:
      edited: false
      editors:
      - Narsil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608285816082-5e2967b819407e3277369b95.png?w=200&h=200&f=face
          fullname: Nicolas Patry
          isHf: true
          isPro: false
          name: Narsil
          type: user
        html: '<p><code>truncation</code> works by dropping some tokens. In this case
          the rightmost ones (so the end of the text).</p>

          <p>This is usually preferred for text-classification, since initial parts
          of the text are usually good enough (for regular text-classification, like
          is this text about politics or science).<br>There is no real way to make
          a finite range model work on large range. The best you can do is send each
          part of the text and try and come up with some kind of aggregate decision.<br>If
          the start is not openai generated, and the middle is, does that mean that
          the text was generated ? There''s just no way to tell tbh. So the API, just
          like <code>transformers</code> pipeline reflect that by just throwing errors
          by default.</p>

          <p>Does that answer your question ?</p>

          '
        raw: '`truncation` works by dropping some tokens. In this case the rightmost
          ones (so the end of the text).


          This is usually preferred for text-classification, since initial parts of
          the text are usually good enough (for regular text-classification, like
          is this text about politics or science).

          There is no real way to make a finite range model work on large range. The
          best you can do is send each part of the text and try and come up with some
          kind of aggregate decision.

          If the start is not openai generated, and the middle is, does that mean
          that the text was generated ? There''s just no way to tell tbh. So the API,
          just like `transformers` pipeline reflect that by just throwing errors by
          default.


          Does that answer your question ?'
        updatedAt: '2023-01-16T10:49:19.744Z'
      numEdits: 0
      reactions: []
    id: 63c52baffb9a6b829d81b74b
    type: comment
  author: Narsil
  content: '`truncation` works by dropping some tokens. In this case the rightmost
    ones (so the end of the text).


    This is usually preferred for text-classification, since initial parts of the
    text are usually good enough (for regular text-classification, like is this text
    about politics or science).

    There is no real way to make a finite range model work on large range. The best
    you can do is send each part of the text and try and come up with some kind of
    aggregate decision.

    If the start is not openai generated, and the middle is, does that mean that the
    text was generated ? There''s just no way to tell tbh. So the API, just like `transformers`
    pipeline reflect that by just throwing errors by default.


    Does that answer your question ?'
  created_at: 2023-01-16 10:49:19+00:00
  edited: false
  hidden: false
  id: 63c52baffb9a6b829d81b74b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d60ccc8da101a1dc861a2a16c696d127.svg
      fullname: Raihan Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: raihankhan
      type: user
    createdAt: '2023-01-18T04:42:07.000Z'
    data:
      edited: false
      editors:
      - raihankhan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d60ccc8da101a1dc861a2a16c696d127.svg
          fullname: Raihan Khan
          isHf: false
          isPro: false
          name: raihankhan
          type: user
        html: '<p>Yeah. Thanks for the awesome explanation!</p>

          '
        raw: Yeah. Thanks for the awesome explanation!
        updatedAt: '2023-01-18T04:42:07.317Z'
      numEdits: 0
      reactions: []
    id: 63c7789f02d8c962334bf645
    type: comment
  author: raihankhan
  content: Yeah. Thanks for the awesome explanation!
  created_at: 2023-01-18 04:42:07+00:00
  edited: false
  hidden: false
  id: 63c7789f02d8c962334bf645
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: openai-community/roberta-base-openai-detector
repo_type: model
status: open
target_branch: null
title: Documentation required for parameters of the Inference API
