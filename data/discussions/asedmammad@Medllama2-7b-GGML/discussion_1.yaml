!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Jeximo
conflicting_files: null
created_at: 2023-08-23 12:24:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64554c42273f649830277263/A7Gpv_D00XZwZyJyGdUZ_.jpeg?w=200&h=200&f=face
      fullname: Jack Jollimore
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jeximo
      type: user
    createdAt: '2023-08-23T13:24:50.000Z'
    data:
      edited: false
      editors:
      - Jeximo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9214609861373901
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64554c42273f649830277263/A7Gpv_D00XZwZyJyGdUZ_.jpeg?w=200&h=200&f=face
          fullname: Jack Jollimore
          isHf: false
          isPro: false
          name: Jeximo
          type: user
        html: '<p>Hi, thanks for this model. I want to use it effectively, is there
          a prompt template to follow?</p>

          '
        raw: Hi, thanks for this model. I want to use it effectively, is there a prompt
          template to follow?
        updatedAt: '2023-08-23T13:24:50.679Z'
      numEdits: 0
      reactions: []
    id: 64e608a25c2a6eff4372a991
    type: comment
  author: Jeximo
  content: Hi, thanks for this model. I want to use it effectively, is there a prompt
    template to follow?
  created_at: 2023-08-23 12:24:50+00:00
  edited: false
  hidden: false
  id: 64e608a25c2a6eff4372a991
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4561708a411481220b39dee25d74c13a.svg
      fullname: Mammad
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: asedmammad
      type: user
    createdAt: '2023-08-27T09:44:55.000Z'
    data:
      edited: true
      editors:
      - asedmammad
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8110308647155762
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4561708a411481220b39dee25d74c13a.svg
          fullname: Mammad
          isHf: false
          isPro: false
          name: asedmammad
          type: user
        html: '<p>Hi, The original model author did not provide a prompt template
          but since dataset medalpaca/medical_meadow_medqa was used, this instruction
          following template should work:</p>

          <pre><code>### Instruction:

          {instruction}


          ### Input:

          {input}


          ### Output:

          </code></pre>

          <p>Alpaca template is also working:</p>

          <pre><code>### Instruction:

          {instruction}


          ### Response:

          </code></pre>

          <p>Also using <code>-ins</code>flag (which results this reverse prompt:
          <code>''### Instruction:''</code>) in <code>llama.cpp</code> seems to work.</p>

          '
        raw: 'Hi, The original model author did not provide a prompt template but
          since dataset medalpaca/medical_meadow_medqa was used, this instruction
          following template should work:

          ```

          ### Instruction:

          {instruction}


          ### Input:

          {input}


          ### Output:

          ```

          Alpaca template is also working:

          ```

          ### Instruction:

          {instruction}


          ### Response:

          ```

          Also using `-ins`flag (which results this reverse prompt: `''### Instruction:''`)
          in `llama.cpp` seems to work.'
        updatedAt: '2023-08-27T10:04:24.701Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Jeximo
    id: 64eb1b17f494f8b2a0838e97
    type: comment
  author: asedmammad
  content: 'Hi, The original model author did not provide a prompt template but since
    dataset medalpaca/medical_meadow_medqa was used, this instruction following template
    should work:

    ```

    ### Instruction:

    {instruction}


    ### Input:

    {input}


    ### Output:

    ```

    Alpaca template is also working:

    ```

    ### Instruction:

    {instruction}


    ### Response:

    ```

    Also using `-ins`flag (which results this reverse prompt: `''### Instruction:''`)
    in `llama.cpp` seems to work.'
  created_at: 2023-08-27 08:44:55+00:00
  edited: true
  hidden: false
  id: 64eb1b17f494f8b2a0838e97
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/4561708a411481220b39dee25d74c13a.svg
      fullname: Mammad
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: asedmammad
      type: user
    createdAt: '2023-09-28T13:13:16.000Z'
    data:
      status: closed
    id: 65157bec3e5a12e0ce9d4a0c
    type: status-change
  author: asedmammad
  created_at: 2023-09-28 12:13:16+00:00
  id: 65157bec3e5a12e0ce9d4a0c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: asedmammad/Medllama2-7b-GGML
repo_type: model
status: closed
target_branch: null
title: Template
