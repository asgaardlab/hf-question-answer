!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Asaf-Yehudai
conflicting_files: null
created_at: 2023-06-07 08:12:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg
      fullname: Asaf Yehudai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Asaf-Yehudai
      type: user
    createdAt: '2023-06-07T09:12:52.000Z'
    data:
      edited: false
      editors:
      - Asaf-Yehudai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6385364532470703
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg
          fullname: Asaf Yehudai
          isHf: false
          isPro: false
          name: Asaf-Yehudai
          type: user
        html: '<p>Great work!<br>it seems you applied Lora only to query_key_value,
          but Tim in this notebook (also in the paper) said it needs to be applied
          to additional places to get full-fine tuning results. See the code below:</p>

          <p>(<a rel="nofollow" href="https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing&amp;s=08#scrollTo=dQdvjTYTT1vQ">https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing&amp;s=08#scrollTo=dQdvjTYTT1vQ</a>)</p>

          <p>from peft import LoraConfig</p>

          <p>lora_alpha = 16<br>lora_dropout = 0.1<br>lora_r = 64</p>

          <p>peft_config = LoraConfig(<br>    lora_alpha=lora_alpha,<br>    lora_dropout=lora_dropout,<br>    r=lora_r,<br>    bias="none",<br>    task_type="CAUSAL_LM",<br>    target_modules=[<br>        "query_key_value",<br>        "dense",<br>        "dense_h_to_4h",<br>        "dense_4h_to_h",<br>    ]<br>)</p>

          '
        raw: "Great work!\r\nit seems you applied Lora only to query_key_value, but\
          \ Tim in this notebook (also in the paper) said it needs to be applied to\
          \ additional places to get full-fine tuning results. See the code below:\r\
          \n\r\n(https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing&s=08#scrollTo=dQdvjTYTT1vQ)\r\
          \n\r\nfrom peft import LoraConfig\r\n\r\nlora_alpha = 16\r\nlora_dropout\
          \ = 0.1\r\nlora_r = 64\r\n\r\npeft_config = LoraConfig(\r\n    lora_alpha=lora_alpha,\r\
          \n    lora_dropout=lora_dropout,\r\n    r=lora_r,\r\n    bias=\"none\",\r\
          \n    task_type=\"CAUSAL_LM\",\r\n    target_modules=[\r\n        \"query_key_value\"\
          ,\r\n        \"dense\",\r\n        \"dense_h_to_4h\",\r\n        \"dense_4h_to_h\"\
          ,\r\n    ]\r\n)"
        updatedAt: '2023-06-07T09:12:52.785Z'
      numEdits: 0
      reactions: []
    id: 64804a145409aa3e3bb9f51d
    type: comment
  author: Asaf-Yehudai
  content: "Great work!\r\nit seems you applied Lora only to query_key_value, but\
    \ Tim in this notebook (also in the paper) said it needs to be applied to additional\
    \ places to get full-fine tuning results. See the code below:\r\n\r\n(https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing&s=08#scrollTo=dQdvjTYTT1vQ)\r\
    \n\r\nfrom peft import LoraConfig\r\n\r\nlora_alpha = 16\r\nlora_dropout = 0.1\r\
    \nlora_r = 64\r\n\r\npeft_config = LoraConfig(\r\n    lora_alpha=lora_alpha,\r\
    \n    lora_dropout=lora_dropout,\r\n    r=lora_r,\r\n    bias=\"none\",\r\n  \
    \  task_type=\"CAUSAL_LM\",\r\n    target_modules=[\r\n        \"query_key_value\"\
    ,\r\n        \"dense\",\r\n        \"dense_h_to_4h\",\r\n        \"dense_4h_to_h\"\
    ,\r\n    ]\r\n)"
  created_at: 2023-06-07 08:12:52+00:00
  edited: false
  hidden: false
  id: 64804a145409aa3e3bb9f51d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
      fullname: Daniel Furman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dfurmanWMP
      type: user
    createdAt: '2023-06-07T19:04:31.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/b5fcda6de81114616dcf08a8c13cb8b8.svg
          fullname: Daniel Furman
          isHf: false
          isPro: false
          name: dfurmanWMP
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-06-07T19:06:05.191Z'
      numEdits: 0
      reactions: []
    id: 6480d4bfbb25a636c9e0de87
    type: comment
  author: dfurmanWMP
  content: This comment has been hidden
  created_at: 2023-06-07 18:04:31+00:00
  edited: true
  hidden: true
  id: 6480d4bfbb25a636c9e0de87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62afc20ca5bd7cef3e1ab3f4/h1cNaEshcUDc-XrycEp6o.jpeg?w=200&h=200&f=face
      fullname: Daniel Furman
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: dfurman
      type: user
    createdAt: '2023-06-07T19:05:25.000Z'
    data:
      edited: false
      editors:
      - dfurman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9773552417755127
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62afc20ca5bd7cef3e1ab3f4/h1cNaEshcUDc-XrycEp6o.jpeg?w=200&h=200&f=face
          fullname: Daniel Furman
          isHf: false
          isPro: false
          name: dfurman
          type: user
        html: '<p>I don''t think "need" is the right word. In my implementation I
          wanted to minimally perturb the base model, and I therefore only applied
          LoRA to the one layer in the attention block. I think it is up to the end
          user to decide how much they want to change the base model. Marking this
          as closed.</p>

          '
        raw: I don't think "need" is the right word. In my implementation I wanted
          to minimally perturb the base model, and I therefore only applied LoRA to
          the one layer in the attention block. I think it is up to the end user to
          decide how much they want to change the base model. Marking this as closed.
        updatedAt: '2023-06-07T19:05:25.080Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6480d4f5bb25a636c9e0e254
    id: 6480d4f5bb25a636c9e0e251
    type: comment
  author: dfurman
  content: I don't think "need" is the right word. In my implementation I wanted to
    minimally perturb the base model, and I therefore only applied LoRA to the one
    layer in the attention block. I think it is up to the end user to decide how much
    they want to change the base model. Marking this as closed.
  created_at: 2023-06-07 18:05:25+00:00
  edited: false
  hidden: false
  id: 6480d4f5bb25a636c9e0e251
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62afc20ca5bd7cef3e1ab3f4/h1cNaEshcUDc-XrycEp6o.jpeg?w=200&h=200&f=face
      fullname: Daniel Furman
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: dfurman
      type: user
    createdAt: '2023-06-07T19:05:25.000Z'
    data:
      status: closed
    id: 6480d4f5bb25a636c9e0e254
    type: status-change
  author: dfurman
  created_at: 2023-06-07 18:05:25+00:00
  id: 6480d4f5bb25a636c9e0e254
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg
      fullname: Asaf Yehudai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Asaf-Yehudai
      type: user
    createdAt: '2023-06-19T11:54:54.000Z'
    data:
      edited: false
      editors:
      - Asaf-Yehudai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9447855949401855
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg
          fullname: Asaf Yehudai
          isHf: false
          isPro: false
          name: Asaf-Yehudai
          type: user
        html: '<p>"need" for getting similar results to regular 16fb fine-tuning.
          But yes, agree with the general comment.</p>

          '
        raw: '"need" for getting similar results to regular 16fb fine-tuning. But
          yes, agree with the general comment.'
        updatedAt: '2023-06-19T11:54:54.850Z'
      numEdits: 0
      reactions: []
    id: 6490420ed9881b1709ec377e
    type: comment
  author: Asaf-Yehudai
  content: '"need" for getting similar results to regular 16fb fine-tuning. But yes,
    agree with the general comment.'
  created_at: 2023-06-19 10:54:54+00:00
  edited: false
  hidden: false
  id: 6490420ed9881b1709ec377e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: dfurman/Falcon-40B-Chat-v0.1
repo_type: model
status: closed
target_branch: null
title: 'qlora - need to be applied and few more places '
