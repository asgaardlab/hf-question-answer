!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nexesenex
conflicting_files: null
created_at: 2023-09-14 03:02:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-09-14T04:02:27.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8655124306678772
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>I think that for CodeLlama 2,  "max_position_embeddings": 16384,
          is the correct line. 4096 is for Llama 2.</p>

          '
        raw: 'I think that for CodeLlama 2,  "max_position_embeddings": 16384, is
          the correct line. 4096 is for Llama 2.'
        updatedAt: '2023-09-14T04:02:27.084Z'
      numEdits: 0
      reactions: []
    id: 650285d3af4d74533e78579b
    type: comment
  author: Nexesenex
  content: 'I think that for CodeLlama 2,  "max_position_embeddings": 16384, is the
    correct line. 4096 is for Llama 2.'
  created_at: 2023-09-14 03:02:27+00:00
  edited: false
  hidden: false
  id: 650285d3af4d74533e78579b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-09-14T12:18:13.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9436399936676025
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>This is true, however the fine-tuning data only had items going
          up to 4k context.  I can change it back to 16384, but results may not be
          great beyond 4k.</p>

          '
        raw: This is true, however the fine-tuning data only had items going up to
          4k context.  I can change it back to 16384, but results may not be great
          beyond 4k.
        updatedAt: '2023-09-14T12:18:13.598Z'
      numEdits: 0
      reactions: []
    id: 6502fa053767e3952cf0908d
    type: comment
  author: jondurbin
  content: This is true, however the fine-tuning data only had items going up to 4k
    context.  I can change it back to 16384, but results may not be great beyond 4k.
  created_at: 2023-09-14 11:18:13+00:00
  edited: false
  hidden: false
  id: 6502fa053767e3952cf0908d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-09-14T13:08:49.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9224809408187866
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Can the fine tuning in 4k affect Codellama rope, when the base model
          is on 16k?</p>

          <p>Anyway, that''s what I have in The Bloke GGUF quants of SB 2.2 :</p>

          <p>llm_load_print_meta: n_ctx_train    = 16384<br>llm_load_print_meta: n_ctx          =
          16384 (my pick)<br>llm_load_print_meta: n_embd         = 8192</p>

          <p>It''s difficult to understand all these changes from the base model to
          the fine tuning (that part I understand, even if I wonder if the fine tuning
          totally takes over the initial training of the base model), then to the
          quant (displaying different values) for us profanes ! ^^</p>

          <p>Anyway, thanks for your amazing work, Jon. I''m hooked since your very
          first version, that I downloaded as soon as it was on HF.</p>

          '
        raw: 'Can the fine tuning in 4k affect Codellama rope, when the base model
          is on 16k?


          Anyway, that''s what I have in The Bloke GGUF quants of SB 2.2 :


          llm_load_print_meta: n_ctx_train    = 16384

          llm_load_print_meta: n_ctx          = 16384 (my pick)

          llm_load_print_meta: n_embd         = 8192


          It''s difficult to understand all these changes from the base model to the
          fine tuning (that part I understand, even if I wonder if the fine tuning
          totally takes over the initial training of the base model), then to the
          quant (displaying different values) for us profanes ! ^^


          Anyway, thanks for your amazing work, Jon. I''m hooked since your very first
          version, that I downloaded as soon as it was on HF.'
        updatedAt: '2023-09-14T13:10:28.109Z'
      numEdits: 1
      reactions: []
    id: 650305e1814e13d7fec90e13
    type: comment
  author: Nexesenex
  content: 'Can the fine tuning in 4k affect Codellama rope, when the base model is
    on 16k?


    Anyway, that''s what I have in The Bloke GGUF quants of SB 2.2 :


    llm_load_print_meta: n_ctx_train    = 16384

    llm_load_print_meta: n_ctx          = 16384 (my pick)

    llm_load_print_meta: n_embd         = 8192


    It''s difficult to understand all these changes from the base model to the fine
    tuning (that part I understand, even if I wonder if the fine tuning totally takes
    over the initial training of the base model), then to the quant (displaying different
    values) for us profanes ! ^^


    Anyway, thanks for your amazing work, Jon. I''m hooked since your very first version,
    that I downloaded as soon as it was on HF.'
  created_at: 2023-09-14 12:08:49+00:00
  edited: true
  hidden: false
  id: 650305e1814e13d7fec90e13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-09-14T15:07:23.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9880120754241943
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>It''s probably fine TBH, I was just playing it safe.</p>

          <p>I don''t think the fine-tuning on 4k would completely degrade the 16k
          performance, although I would imagine the model will likely be resistant
          to producing more than 4k tokens, and things like contextual question answering
          may suffer beyond that just due to lack of fine-tuning data, but I haven''t
          had the time to really analyze it.</p>

          <p>Glad you like the models!</p>

          '
        raw: 'It''s probably fine TBH, I was just playing it safe.


          I don''t think the fine-tuning on 4k would completely degrade the 16k performance,
          although I would imagine the model will likely be resistant to producing
          more than 4k tokens, and things like contextual question answering may suffer
          beyond that just due to lack of fine-tuning data, but I haven''t had the
          time to really analyze it.


          Glad you like the models!'
        updatedAt: '2023-09-14T15:07:23.124Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 650321ab1e4cba6f175a8ca7
    type: comment
  author: jondurbin
  content: 'It''s probably fine TBH, I was just playing it safe.


    I don''t think the fine-tuning on 4k would completely degrade the 16k performance,
    although I would imagine the model will likely be resistant to producing more
    than 4k tokens, and things like contextual question answering may suffer beyond
    that just due to lack of fine-tuning data, but I haven''t had the time to really
    analyze it.


    Glad you like the models!'
  created_at: 2023-09-14 14:07:23+00:00
  edited: false
  hidden: false
  id: 650321ab1e4cba6f175a8ca7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jondurbin/spicyboros-c34b-2.2
repo_type: model
status: open
target_branch: null
title: Max position embeddings
