!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AlpYu-HubX
conflicting_files: null
created_at: 2023-03-14 12:05:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/507dea22cc9193d60a82d186d08a787e.svg
      fullname: Alp Yucesoy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AlpYu-HubX
      type: user
    createdAt: '2023-03-14T13:05:13.000Z'
    data:
      edited: false
      editors:
      - AlpYu-HubX
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/507dea22cc9193d60a82d186d08a787e.svg
          fullname: Alp Yucesoy
          isHf: false
          isPro: false
          name: AlpYu-HubX
          type: user
        html: '<p>Maybe I''m doing something wrong but when I try to use the pipeline
          method to do some inference with a cuda device (16 GB of GPU RAM), I still
          get a "Killed" message meaning my CPU RAM is running out.</p>

          <p>from transformers import pipeline<br>import torch<br>use_cuda = torch.cuda.is_available()<br>print(use_cuda)<br>pipe
          = pipeline(model=''togethercomputer/GPT-NeoXT-Chat-Base-20B'', device="cuda")</p>

          <p>def generate_response(input):<br>    response = pipe(input)<br>    print(response)<br>    return</p>

          <p>if __name__ == "__main__":<br>    prompt = "&lt;human&gt;: Hello!\n&lt;bot&gt;:"<br>    generate_response(prompt)</p>

          '
        raw: "Maybe I'm doing something wrong but when I try to use the pipeline method\
          \ to do some inference with a cuda device (16 GB of GPU RAM), I still get\
          \ a \"Killed\" message meaning my CPU RAM is running out.\r\n\r\nfrom transformers\
          \ import pipeline\r\nimport torch\r\nuse_cuda = torch.cuda.is_available()\r\
          \nprint(use_cuda)\r\npipe = pipeline(model='togethercomputer/GPT-NeoXT-Chat-Base-20B',\
          \ device=\"cuda\")\r\n\r\ndef generate_response(input):\r\n    response\
          \ = pipe(input)\r\n    print(response)\r\n    return\r\n\r\n\r\nif \\_\\\
          _name\\_\\_ == \"\\_\\_main\\_\\_\":\r\n    prompt = \"\\<human\\>: Hello!\\\
          n\\<bot\\>:\"\r\n    generate_response(prompt)"
        updatedAt: '2023-03-14T13:05:13.345Z'
      numEdits: 0
      reactions: []
    id: 641071097a15af878ae7595d
    type: comment
  author: AlpYu-HubX
  content: "Maybe I'm doing something wrong but when I try to use the pipeline method\
    \ to do some inference with a cuda device (16 GB of GPU RAM), I still get a \"\
    Killed\" message meaning my CPU RAM is running out.\r\n\r\nfrom transformers import\
    \ pipeline\r\nimport torch\r\nuse_cuda = torch.cuda.is_available()\r\nprint(use_cuda)\r\
    \npipe = pipeline(model='togethercomputer/GPT-NeoXT-Chat-Base-20B', device=\"\
    cuda\")\r\n\r\ndef generate_response(input):\r\n    response = pipe(input)\r\n\
    \    print(response)\r\n    return\r\n\r\n\r\nif \\_\\_name\\_\\_ == \"\\_\\_main\\\
    _\\_\":\r\n    prompt = \"\\<human\\>: Hello!\\n\\<bot\\>:\"\r\n    generate_response(prompt)"
  created_at: 2023-03-14 12:05:13+00:00
  edited: false
  hidden: false
  id: 641071097a15af878ae7595d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625156462799-6053bc853efc404ddc7c0c11.png?w=200&h=200&f=face
      fullname: Hooman Sedghamiz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hooman650
      type: user
    createdAt: '2023-03-17T15:40:09.000Z'
    data:
      edited: false
      editors:
      - hooman650
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625156462799-6053bc853efc404ddc7c0c11.png?w=200&h=200&f=face
          fullname: Hooman Sedghamiz
          isHf: false
          isPro: false
          name: hooman650
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;AlpYu-HubX&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/AlpYu-HubX\">@<span class=\"\
          underline\">AlpYu-HubX</span></a></span>\n\n\t</span></span> You need a\
          \ GPU with more RAM or multiple GPUs also you could use following to load\
          \ in 8bit and distribute to CPU (but you will still need a better GPU),\
          \ for instance G5 instances on SageMaker:</p>\n<pre><code class=\"language-python\"\
          >model_8bit = AutoModelForCausalLM.from_pretrained(model_dir, device_map=<span\
          \ class=\"hljs-string\">\"auto\"</span>, load_in_8bit=<span class=\"hljs-literal\"\
          >True</span>)\n</code></pre>\n<p>You need the following dependencies though:</p>\n\
          <pre><code class=\"language-bash\">bitsandbytes\naccelerate\n</code></pre>\n"
        raw: '@AlpYu-HubX You need a GPU with more RAM or multiple GPUs also you could
          use following to load in 8bit and distribute to CPU (but you will still
          need a better GPU), for instance G5 instances on SageMaker:

          ```python

          model_8bit = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto",
          load_in_8bit=True)

          ```


          You need the following dependencies though:

          ```bash

          bitsandbytes

          accelerate

          ```'
        updatedAt: '2023-03-17T15:40:09.707Z'
      numEdits: 0
      reactions: []
    id: 641489d9834b006e2358c4d0
    type: comment
  author: hooman650
  content: '@AlpYu-HubX You need a GPU with more RAM or multiple GPUs also you could
    use following to load in 8bit and distribute to CPU (but you will still need a
    better GPU), for instance G5 instances on SageMaker:

    ```python

    model_8bit = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto",
    load_in_8bit=True)

    ```


    You need the following dependencies though:

    ```bash

    bitsandbytes

    accelerate

    ```'
  created_at: 2023-03-17 14:40:09+00:00
  edited: false
  hidden: false
  id: 641489d9834b006e2358c4d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-03-23T02:48:48.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;AlpYu-HubX&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/AlpYu-HubX\">@<span class=\"\
          underline\">AlpYu-HubX</span></a></span>\n\n\t</span></span> You seem to\
          \ have encountered an OOM problem. Unfortunately, I suspect the 8-bit solution\
          \ still won't work for you, as it requires &gt;20GB to load the model in\
          \ 8-bit. But it can be distributed to multiple GPUs as a workaround.</p>\n"
        raw: '@AlpYu-HubX You seem to have encountered an OOM problem. Unfortunately,
          I suspect the 8-bit solution still won''t work for you, as it requires >20GB
          to load the model in 8-bit. But it can be distributed to multiple GPUs as
          a workaround.'
        updatedAt: '2023-03-23T02:48:48.725Z'
      numEdits: 0
      reactions: []
    id: 641bbe10d211522281210b83
    type: comment
  author: juewang
  content: '@AlpYu-HubX You seem to have encountered an OOM problem. Unfortunately,
    I suspect the 8-bit solution still won''t work for you, as it requires >20GB to
    load the model in 8-bit. But it can be distributed to multiple GPUs as a workaround.'
  created_at: 2023-03-23 01:48:48+00:00
  edited: false
  hidden: false
  id: 641bbe10d211522281210b83
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: togethercomputer/GPT-NeoXT-Chat-Base-20B
repo_type: model
status: open
target_branch: null
title: Issue with loading model to GPU when using pipeline
