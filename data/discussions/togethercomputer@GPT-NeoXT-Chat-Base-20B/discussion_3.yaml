!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joanfmendo
conflicting_files: null
created_at: 2023-03-13 19:41:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a781c70fbfc0a3011565d82425bb319.svg
      fullname: Joan Mendoza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joanfmendo
      type: user
    createdAt: '2023-03-13T20:41:08.000Z'
    data:
      edited: false
      editors:
      - joanfmendo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a781c70fbfc0a3011565d82425bb319.svg
          fullname: Joan Mendoza
          isHf: false
          isPro: false
          name: joanfmendo
          type: user
        html: "<p>Hello!</p>\n<p>I'm new using this model, as many of you! I'm trying\
          \ to create a Chatbot in my local machine but I'm facing a very big problem:\
          \ the model is loading in memory, and it's consuming 100% of the resources.\
          \ I have 64GB RAM, a very good CPU and also a very good GPU, but I cannot\
          \ run the model because it is taking a lot of time to load.</p>\n<p>What\
          \ Am I doing wrong? Here is my code in Python:</p>\n<pre><code>from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name = \"togethercomputer/GPT-NeoXT-Chat-Base-20B\"\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\
          \n# Define a function to generate a response from the model given an input\
          \ message\ndef generate_response(input_message):\n    # Encode the input\
          \ message using the tokenizer\n    input_ids = tokenizer.encode(input_message,\
          \ return_tensors=\"pt\")\n    # Generate a response from the model\n   \
          \ output = model.generate(input_ids, max_length=1024, do_sample=True, top_p=0.9,\
          \ top_k=50)\n    # Decode the output tokens and return the response as a\
          \ string\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\n\
          \    return response\n\n# Define a loop to take user input and generate\
          \ responses\nwhile True:\n    # Take user input\n    user_input = input(\"\
          You: \")\n    # Generate a response from the model\n    response = generate_response(user_input)\n\
          \    # Print the response\n    print(\"Bot:\", response)\n</code></pre>\n"
        raw: "Hello!\r\n\r\nI'm new using this model, as many of you! I'm trying to\
          \ create a Chatbot in my local machine but I'm facing a very big problem:\
          \ the model is loading in memory, and it's consuming 100% of the resources.\
          \ I have 64GB RAM, a very good CPU and also a very good GPU, but I cannot\
          \ run the model because it is taking a lot of time to load.\r\n\r\nWhat\
          \ Am I doing wrong? Here is my code in Python:\r\n\r\n```\r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\n\r\nmodel_name = \"togethercomputer/GPT-NeoXT-Chat-Base-20B\"\
          \r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\r\
          \n\r\n# Define a function to generate a response from the model given an\
          \ input message\r\ndef generate_response(input_message):\r\n    # Encode\
          \ the input message using the tokenizer\r\n    input_ids = tokenizer.encode(input_message,\
          \ return_tensors=\"pt\")\r\n    # Generate a response from the model\r\n\
          \    output = model.generate(input_ids, max_length=1024, do_sample=True,\
          \ top_p=0.9, top_k=50)\r\n    # Decode the output tokens and return the\
          \ response as a string\r\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\r\
          \n    return response\r\n\r\n# Define a loop to take user input and generate\
          \ responses\r\nwhile True:\r\n    # Take user input\r\n    user_input =\
          \ input(\"You: \")\r\n    # Generate a response from the model\r\n    response\
          \ = generate_response(user_input)\r\n    # Print the response\r\n    print(\"\
          Bot:\", response)\r\n\r\n```"
        updatedAt: '2023-03-13T20:41:08.814Z'
      numEdits: 0
      reactions: []
    id: 640f8a64f2d7c41a1e9f3d19
    type: comment
  author: joanfmendo
  content: "Hello!\r\n\r\nI'm new using this model, as many of you! I'm trying to\
    \ create a Chatbot in my local machine but I'm facing a very big problem: the\
    \ model is loading in memory, and it's consuming 100% of the resources. I have\
    \ 64GB RAM, a very good CPU and also a very good GPU, but I cannot run the model\
    \ because it is taking a lot of time to load.\r\n\r\nWhat Am I doing wrong? Here\
    \ is my code in Python:\r\n\r\n```\r\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\r\n\r\nmodel_name = \"togethercomputer/GPT-NeoXT-Chat-Base-20B\"\
    \r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\r\
    \n\r\n# Define a function to generate a response from the model given an input\
    \ message\r\ndef generate_response(input_message):\r\n    # Encode the input message\
    \ using the tokenizer\r\n    input_ids = tokenizer.encode(input_message, return_tensors=\"\
    pt\")\r\n    # Generate a response from the model\r\n    output = model.generate(input_ids,\
    \ max_length=1024, do_sample=True, top_p=0.9, top_k=50)\r\n    # Decode the output\
    \ tokens and return the response as a string\r\n    response = tokenizer.decode(output[0],\
    \ skip_special_tokens=True)\r\n    return response\r\n\r\n# Define a loop to take\
    \ user input and generate responses\r\nwhile True:\r\n    # Take user input\r\n\
    \    user_input = input(\"You: \")\r\n    # Generate a response from the model\r\
    \n    response = generate_response(user_input)\r\n    # Print the response\r\n\
    \    print(\"Bot:\", response)\r\n\r\n```"
  created_at: 2023-03-13 19:41:08+00:00
  edited: false
  hidden: false
  id: 640f8a64f2d7c41a1e9f3d19
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654807248087-60f63ac289c17af51b3fbe42.jpeg?w=200&h=200&f=face
      fullname: Kevin Chung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: banalyst
      type: user
    createdAt: '2023-03-13T22:56:45.000Z'
    data:
      edited: false
      editors:
      - banalyst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654807248087-60f63ac289c17af51b3fbe42.jpeg?w=200&h=200&f=face
          fullname: Kevin Chung
          isHf: false
          isPro: false
          name: banalyst
          type: user
        html: '<p>You should load the weights in 8bits or bfloat16. Should cut the
          resource consumption quite a bit. Make sure to pip install accelerate.</p>

          '
        raw: You should load the weights in 8bits or bfloat16. Should cut the resource
          consumption quite a bit. Make sure to pip install accelerate.
        updatedAt: '2023-03-13T22:56:45.904Z'
      numEdits: 0
      reactions: []
    id: 640faa2ddf24a851b3790d90
    type: comment
  author: banalyst
  content: You should load the weights in 8bits or bfloat16. Should cut the resource
    consumption quite a bit. Make sure to pip install accelerate.
  created_at: 2023-03-13 21:56:45+00:00
  edited: false
  hidden: false
  id: 640faa2ddf24a851b3790d90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667224527927-62fc0a9121c444a56f7ce78f.png?w=200&h=200&f=face
      fullname: Linh Ngo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: swiftest
      type: user
    createdAt: '2023-03-14T12:06:29.000Z'
    data:
      edited: false
      editors:
      - swiftest
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667224527927-62fc0a9121c444a56f7ce78f.png?w=200&h=200&f=face
          fullname: Linh Ngo
          isHf: false
          isPro: false
          name: swiftest
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;banalyst&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/banalyst\">@<span class=\"\
          underline\">banalyst</span></a></span>\n\n\t</span></span> Do you have some\
          \ code I could try out 8bit inference?</p>\n"
        raw: '@banalyst Do you have some code I could try out 8bit inference?'
        updatedAt: '2023-03-14T12:06:29.001Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - olbustosa
        - mitra-mir
    id: 641063452a593afb553d335e
    type: comment
  author: swiftest
  content: '@banalyst Do you have some code I could try out 8bit inference?'
  created_at: 2023-03-14 11:06:29+00:00
  edited: false
  hidden: false
  id: 641063452a593afb553d335e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-03-16T15:34:22.000Z'
    data:
      edited: true
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;banalyst&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/banalyst\"\
          >@<span class=\"underline\">banalyst</span></a></span>\n\n\t</span></span>\
          \ Do you have some code I could try out 8bit inference?<br>This should work\
          \ <code>model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"\
          auto\", load_in_8bit=True)</code></p>\n</blockquote>\n<p>And make sure have\
          \ <code>accelerate </code> and <code>bitsandbytes</code> installed. :)</p>\n"
        raw: '> @banalyst Do you have some code I could try out 8bit inference?

          This should work `model = AutoModelForCausalLM.from_pretrained(model_name,
          device_map="auto", load_in_8bit=True)`


          And make sure have `accelerate ` and `bitsandbytes` installed. :)'
        updatedAt: '2023-03-16T15:34:33.452Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - joanfmendo
    id: 641336fefac63676340ac4c1
    type: comment
  author: juewang
  content: '> @banalyst Do you have some code I could try out 8bit inference?

    This should work `model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto",
    load_in_8bit=True)`


    And make sure have `accelerate ` and `bitsandbytes` installed. :)'
  created_at: 2023-03-16 14:34:22+00:00
  edited: true
  hidden: false
  id: 641336fefac63676340ac4c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
      fullname: SHashank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smjain
      type: user
    createdAt: '2023-03-21T01:25:00.000Z'
    data:
      edited: false
      editors:
      - smjain
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
          fullname: SHashank
          isHf: false
          isPro: false
          name: smjain
          type: user
        html: "<blockquote>\n<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;banalyst&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/banalyst\"\
          >@<span class=\"underline\">banalyst</span></a></span>\n\n\t</span></span>\
          \ Do you have some code I could try out 8bit inference?<br>This should work\
          \ <code>model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"\
          auto\", load_in_8bit=True)</code></p>\n</blockquote>\n<p>And make sure have\
          \ <code>accelerate </code> and <code>bitsandbytes</code> installed. :)</p>\n\
          </blockquote>\n<p>I get this error \"topk_cpu\" not implemented for 'Half'\
          \ on colab</p>\n"
        raw: "> > @banalyst Do you have some code I could try out 8bit inference?\n\
          > This should work `model = AutoModelForCausalLM.from_pretrained(model_name,\
          \ device_map=\"auto\", load_in_8bit=True)`\n> \n> And make sure have `accelerate\
          \ ` and `bitsandbytes` installed. :)\n\nI get this error \"topk_cpu\" not\
          \ implemented for 'Half' on colab"
        updatedAt: '2023-03-21T01:25:00.268Z'
      numEdits: 0
      reactions: []
    id: 6419076c11cfb28df91683b8
    type: comment
  author: smjain
  content: "> > @banalyst Do you have some code I could try out 8bit inference?\n\
    > This should work `model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"\
    auto\", load_in_8bit=True)`\n> \n> And make sure have `accelerate ` and `bitsandbytes`\
    \ installed. :)\n\nI get this error \"topk_cpu\" not implemented for 'Half' on\
    \ colab"
  created_at: 2023-03-21 00:25:00+00:00
  edited: false
  hidden: false
  id: 6419076c11cfb28df91683b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
      fullname: SHashank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smjain
      type: user
    createdAt: '2023-03-21T01:31:54.000Z'
    data:
      edited: false
      editors:
      - smjain
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
          fullname: SHashank
          isHf: false
          isPro: false
          name: smjain
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;banalyst&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/banalyst\"\
          >@<span class=\"underline\">banalyst</span></a></span>\n\n\t</span></span>\
          \ Do you have some code I could try out 8bit inference?<br>This should work\
          \ <code>model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"\
          auto\", load_in_8bit=True)</code></p>\n</blockquote>\n<p>And make sure have\
          \ <code>accelerate </code> and <code>bitsandbytes</code> installed. :)</p>\n\
          </blockquote>\n<p>I get this error \"topk_cpu\" not implemented for 'Half'\
          \ on colab</p>\n</blockquote>\n<p>Able to fix this with sample=false, but\
          \ the response is not coming even for 2 minutes on a premium colab. Attached\
          \ screenshot</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1679362302596-62ac1143cae4462c0c8a1112.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1679362302596-62ac1143cae4462c0c8a1112.png\"\
          ></a></p>\n"
        raw: "> > > @banalyst Do you have some code I could try out 8bit inference?\n\
          > > This should work `model = AutoModelForCausalLM.from_pretrained(model_name,\
          \ device_map=\"auto\", load_in_8bit=True)`\n> > \n> > And make sure have\
          \ `accelerate ` and `bitsandbytes` installed. :)\n> \n> I get this error\
          \ \"topk_cpu\" not implemented for 'Half' on colab\n\nAble to fix this with\
          \ sample=false, but the response is not coming even for 2 minutes on a premium\
          \ colab. Attached screenshot\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1679362302596-62ac1143cae4462c0c8a1112.png)"
        updatedAt: '2023-03-21T01:31:54.098Z'
      numEdits: 0
      reactions: []
    id: 6419090a11cfb28df91693ea
    type: comment
  author: smjain
  content: "> > > @banalyst Do you have some code I could try out 8bit inference?\n\
    > > This should work `model = AutoModelForCausalLM.from_pretrained(model_name,\
    \ device_map=\"auto\", load_in_8bit=True)`\n> > \n> > And make sure have `accelerate\
    \ ` and `bitsandbytes` installed. :)\n> \n> I get this error \"topk_cpu\" not\
    \ implemented for 'Half' on colab\n\nAble to fix this with sample=false, but the\
    \ response is not coming even for 2 minutes on a premium colab. Attached screenshot\n\
    \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/1679362302596-62ac1143cae4462c0c8a1112.png)"
  created_at: 2023-03-21 00:31:54+00:00
  edited: false
  hidden: false
  id: 6419090a11cfb28df91693ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a781c70fbfc0a3011565d82425bb319.svg
      fullname: Joan Mendoza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joanfmendo
      type: user
    createdAt: '2023-04-04T00:52:45.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/0a781c70fbfc0a3011565d82425bb319.svg
          fullname: Joan Mendoza
          isHf: false
          isPro: false
          name: joanfmendo
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-04-04T00:54:15.888Z'
      numEdits: 0
      reactions: []
    id: 642b74ddbf3d1ac926dc736a
    type: comment
  author: joanfmendo
  content: This comment has been hidden
  created_at: 2023-04-03 23:52:45+00:00
  edited: true
  hidden: true
  id: 642b74ddbf3d1ac926dc736a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a781c70fbfc0a3011565d82425bb319.svg
      fullname: Joan Mendoza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joanfmendo
      type: user
    createdAt: '2023-04-04T20:56:31.000Z'
    data:
      edited: true
      editors:
      - joanfmendo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a781c70fbfc0a3011565d82425bb319.svg
          fullname: Joan Mendoza
          isHf: false
          isPro: false
          name: joanfmendo
          type: user
        html: "<blockquote>\n<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;banalyst&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/banalyst\"\
          >@<span class=\"underline\">banalyst</span></a></span>\n\n\t</span></span>\
          \ Do you have some code I could try out 8bit inference?<br>This should work\
          \ <code>model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"\
          auto\", load_in_8bit=True)</code></p>\n</blockquote>\n<p>And make sure have\
          \ <code>accelerate </code> and <code>bitsandbytes</code> installed. :)</p>\n\
          </blockquote>\n<p>Thanks!</p>\n<p>Now I am facing this:</p>\n<p><code>CUDA_SETUP:\
          \ WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\
          \ CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed?\
          \ If you are on a cluster, make sure you are on a CUDA machine! CUDA SETUP:\
          \ Loading binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\
          \ argument of type 'WindowsPath' is not iterable CUDA_SETUP: WARNING! libcudart.so\
          \ not found in any environmental path. Searching /usr/local/cuda/lib64...\
          \ CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed?\
          \ If you are on a cluster, make sure you are on a CUDA machine! CUDA SETUP:\
          \ Loading binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\
          \ argument of type 'WindowsPath' is not iterable CUDA_SETUP: WARNING! libcudart.so\
          \ not found in any environmental path. Searching /usr/local/cuda/lib64...\
          \ CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed?\
          \ If you are on a cluster, make sure you are on a CUDA machine! CUDA SETUP:\
          \ Loading binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\
          \ argument of type 'WindowsPath' is not iterable CUDA SETUP: Problem: The\
          \ main issue seems to be that the main CUDA library was not detected. CUDA\
          \ SETUP: Solution 1): Your paths are probably not up-to-date. You can update\
          \ them via: sudo ldconfig. CUDA SETUP: Solution 2): If you do not have sudo\
          \ rights, you can do the following: CUDA SETUP: Solution 2a): Find the cuda\
          \ library via: find / -name libcuda.so 2&gt;/dev/null CUDA SETUP: Solution\
          \ 2b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a\
          \ CUDA SETUP: Solution 2c): For a permanent solution add the export from\
          \ 2b into your .bashrc file, located at ~/.bashrc</code></p>\n<p>IDK what\
          \ to do. I just pip-installed <code>transformers</code> and my PC is running\
          \ with Windows 10, I have CUDA 10.1 installed... Can anyone help me?</p>\n"
        raw: "> > @banalyst Do you have some code I could try out 8bit inference?\n\
          > This should work `model = AutoModelForCausalLM.from_pretrained(model_name,\
          \ device_map=\"auto\", load_in_8bit=True)`\n> \n> And make sure have `accelerate\
          \ ` and `bitsandbytes` installed. :)\n\nThanks!\n\nNow I am facing this:\n\
          \n`CUDA_SETUP: WARNING! libcudart.so not found in any environmental path.\
          \ Searching /usr/local/cuda/lib64...\nCUDA SETUP: WARNING! libcuda.so not\
          \ found! Do you have a CUDA driver installed? If you are on a cluster, make\
          \ sure you are on a CUDA machine!\nCUDA SETUP: Loading binary C:\\Users\\\
          Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n\
          argument of type 'WindowsPath' is not iterable\nCUDA_SETUP: WARNING! libcudart.so\
          \ not found in any environmental path. Searching /usr/local/cuda/lib64...\n\
          CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed?\
          \ If you are on a cluster, make sure you are on a CUDA machine!\nCUDA SETUP:\
          \ Loading binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n\
          argument of type 'WindowsPath' is not iterable\nCUDA_SETUP: WARNING! libcudart.so\
          \ not found in any environmental path. Searching /usr/local/cuda/lib64...\n\
          CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed?\
          \ If you are on a cluster, make sure you are on a CUDA machine!\nCUDA SETUP:\
          \ Loading binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n\
          argument of type 'WindowsPath' is not iterable\nCUDA SETUP: Problem: The\
          \ main issue seems to be that the main CUDA library was not detected.\n\
          CUDA SETUP: Solution 1): Your paths are probably not up-to-date. You can\
          \ update them via: sudo ldconfig.\nCUDA SETUP: Solution 2): If you do not\
          \ have sudo rights, you can do the following:\nCUDA SETUP: Solution 2a):\
          \ Find the cuda library via: find / -name libcuda.so 2>/dev/null\nCUDA SETUP:\
          \ Solution 2b): Once the library is found add it to the LD_LIBRARY_PATH:\
          \ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a\nCUDA SETUP:\
          \ Solution 2c): For a permanent solution add the export from 2b into your\
          \ .bashrc file, located at ~/.bashrc`\n\nIDK what to do. I just pip-installed\
          \ `transformers` and my PC is running with Windows 10, I have CUDA 10.1\
          \ installed... Can anyone help me?"
        updatedAt: '2023-04-04T20:57:00.862Z'
      numEdits: 1
      reactions: []
    id: 642c8eff77fcbd9c33060e17
    type: comment
  author: joanfmendo
  content: "> > @banalyst Do you have some code I could try out 8bit inference?\n\
    > This should work `model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"\
    auto\", load_in_8bit=True)`\n> \n> And make sure have `accelerate ` and `bitsandbytes`\
    \ installed. :)\n\nThanks!\n\nNow I am facing this:\n\n`CUDA_SETUP: WARNING! libcudart.so\
    \ not found in any environmental path. Searching /usr/local/cuda/lib64...\nCUDA\
    \ SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed? If\
    \ you are on a cluster, make sure you are on a CUDA machine!\nCUDA SETUP: Loading\
    \ binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
    LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n\
    argument of type 'WindowsPath' is not iterable\nCUDA_SETUP: WARNING! libcudart.so\
    \ not found in any environmental path. Searching /usr/local/cuda/lib64...\nCUDA\
    \ SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed? If\
    \ you are on a cluster, make sure you are on a CUDA machine!\nCUDA SETUP: Loading\
    \ binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
    LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n\
    argument of type 'WindowsPath' is not iterable\nCUDA_SETUP: WARNING! libcudart.so\
    \ not found in any environmental path. Searching /usr/local/cuda/lib64...\nCUDA\
    \ SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed? If\
    \ you are on a cluster, make sure you are on a CUDA machine!\nCUDA SETUP: Loading\
    \ binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
    LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n\
    argument of type 'WindowsPath' is not iterable\nCUDA SETUP: Problem: The main\
    \ issue seems to be that the main CUDA library was not detected.\nCUDA SETUP:\
    \ Solution 1): Your paths are probably not up-to-date. You can update them via:\
    \ sudo ldconfig.\nCUDA SETUP: Solution 2): If you do not have sudo rights, you\
    \ can do the following:\nCUDA SETUP: Solution 2a): Find the cuda library via:\
    \ find / -name libcuda.so 2>/dev/null\nCUDA SETUP: Solution 2b): Once the library\
    \ is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a\n\
    CUDA SETUP: Solution 2c): For a permanent solution add the export from 2b into\
    \ your .bashrc file, located at ~/.bashrc`\n\nIDK what to do. I just pip-installed\
    \ `transformers` and my PC is running with Windows 10, I have CUDA 10.1 installed...\
    \ Can anyone help me?"
  created_at: 2023-04-04 19:56:31+00:00
  edited: true
  hidden: false
  id: 642c8eff77fcbd9c33060e17
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/40fb0922a1388a2d717fd6882c6f9fe2.svg
      fullname: Yucheng Lu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: EugeneLu
      type: user
    createdAt: '2023-04-07T01:19:44.000Z'
    data:
      edited: false
      editors:
      - EugeneLu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/40fb0922a1388a2d717fd6882c6f9fe2.svg
          fullname: Yucheng Lu
          isHf: false
          isPro: false
          name: EugeneLu
          type: user
        html: "<blockquote>\n<blockquote>\n<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;banalyst&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/banalyst\"\
          >@<span class=\"underline\">banalyst</span></a></span>\n\n\t</span></span>\
          \ Do you have some code I could try out 8bit inference?<br>This should work\
          \ <code>model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"\
          auto\", load_in_8bit=True)</code></p>\n</blockquote>\n<p>And make sure have\
          \ <code>accelerate </code> and <code>bitsandbytes</code> installed. :)</p>\n\
          </blockquote>\n<p>Thanks!</p>\n<p>Now I am facing this:</p>\n<p><code>CUDA_SETUP:\
          \ WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\
          \ CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed?\
          \ If you are on a cluster, make sure you are on a CUDA machine! CUDA SETUP:\
          \ Loading binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\
          \ argument of type 'WindowsPath' is not iterable CUDA_SETUP: WARNING! libcudart.so\
          \ not found in any environmental path. Searching /usr/local/cuda/lib64...\
          \ CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed?\
          \ If you are on a cluster, make sure you are on a CUDA machine! CUDA SETUP:\
          \ Loading binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\
          \ argument of type 'WindowsPath' is not iterable CUDA_SETUP: WARNING! libcudart.so\
          \ not found in any environmental path. Searching /usr/local/cuda/lib64...\
          \ CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed?\
          \ If you are on a cluster, make sure you are on a CUDA machine! CUDA SETUP:\
          \ Loading binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\
          \ argument of type 'WindowsPath' is not iterable CUDA SETUP: Problem: The\
          \ main issue seems to be that the main CUDA library was not detected. CUDA\
          \ SETUP: Solution 1): Your paths are probably not up-to-date. You can update\
          \ them via: sudo ldconfig. CUDA SETUP: Solution 2): If you do not have sudo\
          \ rights, you can do the following: CUDA SETUP: Solution 2a): Find the cuda\
          \ library via: find / -name libcuda.so 2&gt;/dev/null CUDA SETUP: Solution\
          \ 2b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a\
          \ CUDA SETUP: Solution 2c): For a permanent solution add the export from\
          \ 2b into your .bashrc file, located at ~/.bashrc</code></p>\n<p>IDK what\
          \ to do. I just pip-installed <code>transformers</code> and my PC is running\
          \ with Windows 10, I have CUDA 10.1 installed... Can anyone help me?</p>\n\
          </blockquote>\n<p>Hi, this seems to be that libcuda.so is not specified\
          \ in any of the environmental path. Have you tried the solutions at the\
          \ end? i.e., </p>\n<p>CUDA SETUP: Solution 1): Your paths are probably not\
          \ up-to-date. You can update them via: sudo ldconfig.<br>CUDA SETUP: Solution\
          \ 2): If you do not have sudo rights, you can do the following:<br>CUDA\
          \ SETUP: Solution 2a): Find the cuda library via: find / -name libcuda.so\
          \ 2&gt;/dev/null<br>CUDA SETUP: Solution 2b): Once the library is found\
          \ add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a<br>CUDA\
          \ SETUP: Solution 2c): For a permanent solution add the export from 2b into\
          \ your .bashrc file, located at ~/.bashrc`</p>\n<p>Did it give any further\
          \ information after you tried them?</p>\n"
        raw: "> > > @banalyst Do you have some code I could try out 8bit inference?\n\
          > > This should work `model = AutoModelForCausalLM.from_pretrained(model_name,\
          \ device_map=\"auto\", load_in_8bit=True)`\n> > \n> > And make sure have\
          \ `accelerate ` and `bitsandbytes` installed. :)\n> \n> Thanks!\n> \n> Now\
          \ I am facing this:\n> \n> `CUDA_SETUP: WARNING! libcudart.so not found\
          \ in any environmental path. Searching /usr/local/cuda/lib64...\n> CUDA\
          \ SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed?\
          \ If you are on a cluster, make sure you are on a CUDA machine!\n> CUDA\
          \ SETUP: Loading binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n\
          > argument of type 'WindowsPath' is not iterable\n> CUDA_SETUP: WARNING!\
          \ libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n\
          > CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed?\
          \ If you are on a cluster, make sure you are on a CUDA machine!\n> CUDA\
          \ SETUP: Loading binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n\
          > argument of type 'WindowsPath' is not iterable\n> CUDA_SETUP: WARNING!\
          \ libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n\
          > CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed?\
          \ If you are on a cluster, make sure you are on a CUDA machine!\n> CUDA\
          \ SETUP: Loading binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
          LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n\
          > argument of type 'WindowsPath' is not iterable\n> CUDA SETUP: Problem:\
          \ The main issue seems to be that the main CUDA library was not detected.\n\
          > CUDA SETUP: Solution 1): Your paths are probably not up-to-date. You can\
          \ update them via: sudo ldconfig.\n> CUDA SETUP: Solution 2): If you do\
          \ not have sudo rights, you can do the following:\n> CUDA SETUP: Solution\
          \ 2a): Find the cuda library via: find / -name libcuda.so 2>/dev/null\n\
          > CUDA SETUP: Solution 2b): Once the library is found add it to the LD_LIBRARY_PATH:\
          \ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a\n> CUDA SETUP:\
          \ Solution 2c): For a permanent solution add the export from 2b into your\
          \ .bashrc file, located at ~/.bashrc`\n> \n> IDK what to do. I just pip-installed\
          \ `transformers` and my PC is running with Windows 10, I have CUDA 10.1\
          \ installed... Can anyone help me?\n\nHi, this seems to be that libcuda.so\
          \ is not specified in any of the environmental path. Have you tried the\
          \ solutions at the end? i.e., \n\nCUDA SETUP: Solution 1): Your paths are\
          \ probably not up-to-date. You can update them via: sudo ldconfig.\nCUDA\
          \ SETUP: Solution 2): If you do not have sudo rights, you can do the following:\n\
          CUDA SETUP: Solution 2a): Find the cuda library via: find / -name libcuda.so\
          \ 2>/dev/null\nCUDA SETUP: Solution 2b): Once the library is found add it\
          \ to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a\n\
          CUDA SETUP: Solution 2c): For a permanent solution add the export from 2b\
          \ into your .bashrc file, located at ~/.bashrc`\n\nDid it give any further\
          \ information after you tried them?"
        updatedAt: '2023-04-07T01:19:44.135Z'
      numEdits: 0
      reactions: []
    id: 642f6fb005e13b04d8a51d81
    type: comment
  author: EugeneLu
  content: "> > > @banalyst Do you have some code I could try out 8bit inference?\n\
    > > This should work `model = AutoModelForCausalLM.from_pretrained(model_name,\
    \ device_map=\"auto\", load_in_8bit=True)`\n> > \n> > And make sure have `accelerate\
    \ ` and `bitsandbytes` installed. :)\n> \n> Thanks!\n> \n> Now I am facing this:\n\
    > \n> `CUDA_SETUP: WARNING! libcudart.so not found in any environmental path.\
    \ Searching /usr/local/cuda/lib64...\n> CUDA SETUP: WARNING! libcuda.so not found!\
    \ Do you have a CUDA driver installed? If you are on a cluster, make sure you\
    \ are on a CUDA machine!\n> CUDA SETUP: Loading binary C:\\Users\\Felipe\\AppData\\\
    Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\\
    local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n\
    > argument of type 'WindowsPath' is not iterable\n> CUDA_SETUP: WARNING! libcudart.so\
    \ not found in any environmental path. Searching /usr/local/cuda/lib64...\n> CUDA\
    \ SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed? If\
    \ you are on a cluster, make sure you are on a CUDA machine!\n> CUDA SETUP: Loading\
    \ binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
    LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n\
    > argument of type 'WindowsPath' is not iterable\n> CUDA_SETUP: WARNING! libcudart.so\
    \ not found in any environmental path. Searching /usr/local/cuda/lib64...\n> CUDA\
    \ SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed? If\
    \ you are on a cluster, make sure you are on a CUDA machine!\n> CUDA SETUP: Loading\
    \ binary C:\\Users\\Felipe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\
    LocalCache\\local-packages\\Python38\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n\
    > argument of type 'WindowsPath' is not iterable\n> CUDA SETUP: Problem: The main\
    \ issue seems to be that the main CUDA library was not detected.\n> CUDA SETUP:\
    \ Solution 1): Your paths are probably not up-to-date. You can update them via:\
    \ sudo ldconfig.\n> CUDA SETUP: Solution 2): If you do not have sudo rights, you\
    \ can do the following:\n> CUDA SETUP: Solution 2a): Find the cuda library via:\
    \ find / -name libcuda.so 2>/dev/null\n> CUDA SETUP: Solution 2b): Once the library\
    \ is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a\n\
    > CUDA SETUP: Solution 2c): For a permanent solution add the export from 2b into\
    \ your .bashrc file, located at ~/.bashrc`\n> \n> IDK what to do. I just pip-installed\
    \ `transformers` and my PC is running with Windows 10, I have CUDA 10.1 installed...\
    \ Can anyone help me?\n\nHi, this seems to be that libcuda.so is not specified\
    \ in any of the environmental path. Have you tried the solutions at the end? i.e.,\
    \ \n\nCUDA SETUP: Solution 1): Your paths are probably not up-to-date. You can\
    \ update them via: sudo ldconfig.\nCUDA SETUP: Solution 2): If you do not have\
    \ sudo rights, you can do the following:\nCUDA SETUP: Solution 2a): Find the cuda\
    \ library via: find / -name libcuda.so 2>/dev/null\nCUDA SETUP: Solution 2b):\
    \ Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a\n\
    CUDA SETUP: Solution 2c): For a permanent solution add the export from 2b into\
    \ your .bashrc file, located at ~/.bashrc`\n\nDid it give any further information\
    \ after you tried them?"
  created_at: 2023-04-07 00:19:44+00:00
  edited: false
  hidden: false
  id: 642f6fb005e13b04d8a51d81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64732e7f7be71eb8b1b572a8/bEgqSstenzd6kjkGWjrgd.png?w=200&h=200&f=face
      fullname: Dan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dejanseo
      type: user
    createdAt: '2023-12-27T23:51:26.000Z'
    data:
      edited: false
      editors:
      - dejanseo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9320113658905029
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64732e7f7be71eb8b1b572a8/bEgqSstenzd6kjkGWjrgd.png?w=200&h=200&f=face
          fullname: Dan
          isHf: false
          isPro: false
          name: dejanseo
          type: user
        html: '<blockquote>

          <p>CUDA SETUP: Solution 1): Your paths are probably not up-to-date. You
          can update them via: sudo ldconfig.</p>

          </blockquote>

          <p>from the paths (C:\Users...) it looks like Felipe is running this on
          a PC and as far as I know bitsandbytes isn''t supported on Windows (yet?).</p>

          '
        raw: '> CUDA SETUP: Solution 1): Your paths are probably not up-to-date. You
          can update them via: sudo ldconfig.


          from the paths (C:\Users\...) it looks like Felipe is running this on a
          PC and as far as I know bitsandbytes isn''t supported on Windows (yet?).'
        updatedAt: '2023-12-27T23:51:26.918Z'
      numEdits: 0
      reactions: []
    id: 658cb87e79c9c9b95f400fef
    type: comment
  author: dejanseo
  content: '> CUDA SETUP: Solution 1): Your paths are probably not up-to-date. You
    can update them via: sudo ldconfig.


    from the paths (C:\Users\...) it looks like Felipe is running this on a PC and
    as far as I know bitsandbytes isn''t supported on Windows (yet?).'
  created_at: 2023-12-27 23:51:26+00:00
  edited: false
  hidden: false
  id: 658cb87e79c9c9b95f400fef
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: togethercomputer/GPT-NeoXT-Chat-Base-20B
repo_type: model
status: open
target_branch: null
title: This model requires A LOT of resources... But how much? Trying to build a chatbot
