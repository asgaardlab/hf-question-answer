!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DaveBowman
conflicting_files: null
created_at: 2023-05-30 14:08:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a078c37c07794e6929317cbeed7ad690.svg
      fullname: Alexander Loburev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DaveBowman
      type: user
    createdAt: '2023-05-30T15:08:00.000Z'
    data:
      edited: false
      editors:
      - DaveBowman
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a078c37c07794e6929317cbeed7ad690.svg
          fullname: Alexander Loburev
          isHf: false
          isPro: false
          name: DaveBowman
          type: user
        html: "<p>Greetings, I am trying to install a model on Ubuntu 20.04 on wsl2\
          \ (Cuda version 11.7 installed), but I ran into a problem.<br>I'm trying\
          \ to run the code from here: <a href=\"https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B#gpu-inference-in-int8\"\
          >https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B#gpu-inference-in-int8</a>,\
          \ but an error occurs when I try to run it:</p>\n<pre><code>Traceback (most\
          \ recent call last):\n  File \"./GPT-NeoXT-Chat-Base-20B.py\", line 4, in\
          \ &lt;module&gt;\n    model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\"\
          , device_map=\"auto\", load_in_8bit=True)\n  File \"/home/dave/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 467, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/home/dave/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\"\
          , line 2159, in from_pretrained\n    is_8bit_serializable = version.parse(importlib_metadata.version(\"\
          bitsandbytes\")) &gt; version.parse(\"0.37.2\")\n  File \"/usr/lib/python3.8/importlib/metadata.py\"\
          , line 530, in version\n    return distribution(distribution_name).version\n\
          \  File \"/usr/lib/python3.8/importlib/metadata.py\", line 503, in distribution\n\
          \    return Distribution.from_name(distribution_name)\n  File \"/usr/lib/python3.8/importlib/metadata.py\"\
          , line 177, in from_name\n    raise PackageNotFoundError(name)\nimportlib.metadata.PackageNotFoundError:\
          \ bitsandbytes\n</code></pre>\n<p>Even though I have installed the bitsanbytes\
          \ library:</p>\n<pre><code class=\"language-bash\">pip install bitsandbytes-cuda117\n\
          </code></pre>\n<p>Please tell me what to do in this situation.</p>\n"
        raw: "Greetings, I am trying to install a model on Ubuntu 20.04 on wsl2 (Cuda\
          \ version 11.7 installed), but I ran into a problem.\r\nI'm trying to run\
          \ the code from here: https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B#gpu-inference-in-int8,\
          \ but an error occurs when I try to run it:\r\n\r\n```\r\nTraceback (most\
          \ recent call last):\r\n  File \"./GPT-NeoXT-Chat-Base-20B.py\", line 4,\
          \ in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\"\
          , device_map=\"auto\", load_in_8bit=True)\r\n  File \"/home/dave/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 467, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n  File \"/home/dave/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\"\
          , line 2159, in from_pretrained\r\n    is_8bit_serializable = version.parse(importlib_metadata.version(\"\
          bitsandbytes\")) > version.parse(\"0.37.2\")\r\n  File \"/usr/lib/python3.8/importlib/metadata.py\"\
          , line 530, in version\r\n    return distribution(distribution_name).version\r\
          \n  File \"/usr/lib/python3.8/importlib/metadata.py\", line 503, in distribution\r\
          \n    return Distribution.from_name(distribution_name)\r\n  File \"/usr/lib/python3.8/importlib/metadata.py\"\
          , line 177, in from_name\r\n    raise PackageNotFoundError(name)\r\nimportlib.metadata.PackageNotFoundError:\
          \ bitsandbytes\r\n```\r\n\r\nEven though I have installed the bitsanbytes\
          \ library:\r\n\r\n```bash\r\npip install bitsandbytes-cuda117\r\n```\r\n\
          \r\nPlease tell me what to do in this situation."
        updatedAt: '2023-05-30T15:08:00.974Z'
      numEdits: 0
      reactions: []
    id: 6476115015dbffee63415669
    type: comment
  author: DaveBowman
  content: "Greetings, I am trying to install a model on Ubuntu 20.04 on wsl2 (Cuda\
    \ version 11.7 installed), but I ran into a problem.\r\nI'm trying to run the\
    \ code from here: https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B#gpu-inference-in-int8,\
    \ but an error occurs when I try to run it:\r\n\r\n```\r\nTraceback (most recent\
    \ call last):\r\n  File \"./GPT-NeoXT-Chat-Base-20B.py\", line 4, in <module>\r\
    \n    model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\"\
    , device_map=\"auto\", load_in_8bit=True)\r\n  File \"/home/dave/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 467, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
    \  File \"/home/dave/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\"\
    , line 2159, in from_pretrained\r\n    is_8bit_serializable = version.parse(importlib_metadata.version(\"\
    bitsandbytes\")) > version.parse(\"0.37.2\")\r\n  File \"/usr/lib/python3.8/importlib/metadata.py\"\
    , line 530, in version\r\n    return distribution(distribution_name).version\r\
    \n  File \"/usr/lib/python3.8/importlib/metadata.py\", line 503, in distribution\r\
    \n    return Distribution.from_name(distribution_name)\r\n  File \"/usr/lib/python3.8/importlib/metadata.py\"\
    , line 177, in from_name\r\n    raise PackageNotFoundError(name)\r\nimportlib.metadata.PackageNotFoundError:\
    \ bitsandbytes\r\n```\r\n\r\nEven though I have installed the bitsanbytes library:\r\
    \n\r\n```bash\r\npip install bitsandbytes-cuda117\r\n```\r\n\r\nPlease tell me\
    \ what to do in this situation."
  created_at: 2023-05-30 14:08:00+00:00
  edited: false
  hidden: false
  id: 6476115015dbffee63415669
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: togethercomputer/GPT-NeoXT-Chat-Base-20B
repo_type: model
status: open
target_branch: null
title: 'How to resolve the ''importlib.metadata.PackageNotFoundError: bitsandbytes''
  error after running ''pip install bitsandbytes-cuda117'' in Ubuntu?'
