!!python/object:huggingface_hub.community.DiscussionWithDetails
author: juusohugs
conflicting_files: null
created_at: 2023-03-16 15:23:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675681614196-629f4d9f5d4565ed65a12f59.jpeg?w=200&h=200&f=face
      fullname: JH
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: juusohugs
      type: user
    createdAt: '2023-03-16T16:23:34.000Z'
    data:
      edited: false
      editors:
      - juusohugs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675681614196-629f4d9f5d4565ed65a12f59.jpeg?w=200&h=200&f=face
          fullname: JH
          isHf: false
          isPro: false
          name: juusohugs
          type: user
        html: '<p>For simply to try deployment and inference with up to 500 words.
          Any help?</p>

          '
        raw: For simply to try deployment and inference with up to 500 words. Any
          help?
        updatedAt: '2023-03-16T16:23:34.065Z'
      numEdits: 0
      reactions: []
    id: 64134286a1d8fdc60a940f32
    type: comment
  author: juusohugs
  content: For simply to try deployment and inference with up to 500 words. Any help?
  created_at: 2023-03-16 15:23:34+00:00
  edited: false
  hidden: false
  id: 64134286a1d8fdc60a940f32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-03-17T16:03:07.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: '<p>Hi! The model has approximately 20 billion parameters and thus loading
          the model in FP16 format would require approximately 40GB of VRAM, with
          additional memory required for inference.<br>However, if you allow some
          loss of precision, you can also try 8bit quantization which should halve
          the memory footprint.<br>After installing <code>accelerate</code> and <code>bitsandbytes</code>,
          just try to load the model as follows:</p>

          <pre><code class="language-python">model = AutoModelForCausalLM.from_pretrained(<span
          class="hljs-string">''togethercomputer/GPT-NeoXT-Chat-Base-20B''</span>,
          device_map=<span class="hljs-string">"auto"</span>, load_in_8bit=<span class="hljs-literal">True</span>)

          </code></pre>

          '
        raw: 'Hi! The model has approximately 20 billion parameters and thus loading
          the model in FP16 format would require approximately 40GB of VRAM, with
          additional memory required for inference.

          However, if you allow some loss of precision, you can also try 8bit quantization
          which should halve the memory footprint.

          After installing `accelerate` and `bitsandbytes`, just try to load the model
          as follows:

          ```python

          model = AutoModelForCausalLM.from_pretrained(''togethercomputer/GPT-NeoXT-Chat-Base-20B'',
          device_map="auto", load_in_8bit=True)

          ````'
        updatedAt: '2023-03-17T16:03:07.548Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - aboundy
        - juusohugs
    id: 64148f3b9a030910ca0dab1f
    type: comment
  author: juewang
  content: 'Hi! The model has approximately 20 billion parameters and thus loading
    the model in FP16 format would require approximately 40GB of VRAM, with additional
    memory required for inference.

    However, if you allow some loss of precision, you can also try 8bit quantization
    which should halve the memory footprint.

    After installing `accelerate` and `bitsandbytes`, just try to load the model as
    follows:

    ```python

    model = AutoModelForCausalLM.from_pretrained(''togethercomputer/GPT-NeoXT-Chat-Base-20B'',
    device_map="auto", load_in_8bit=True)

    ````'
  created_at: 2023-03-17 15:03:07+00:00
  edited: false
  hidden: false
  id: 64148f3b9a030910ca0dab1f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675681614196-629f4d9f5d4565ed65a12f59.jpeg?w=200&h=200&f=face
      fullname: JH
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: juusohugs
      type: user
    createdAt: '2023-03-20T10:33:08.000Z'
    data:
      edited: false
      editors:
      - juusohugs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675681614196-629f4d9f5d4565ed65a12f59.jpeg?w=200&h=200&f=face
          fullname: JH
          isHf: false
          isPro: false
          name: juusohugs
          type: user
        html: "<p>Thanks for the help. Managed to deploy OK but unfortunately getting\
          \ an error during inference (I deployed using very big instances (ml.g4dn.12xlarge\
          \ that has 48 vCPU and 192 GiB mem). I wonder if the huggingface version\
          \ (amazon supports up to transformers_version=\"4.17.0\") is a problem or\
          \ if there's something else going on.</p>\n<p>I could deploy &amp; do inference\
          \ just fine on neo 125MB model, but this big 20B model is causing an error\
          \ during inference:</p>\n<code>\nModelError: An error occurred (ModelError)\
          \ when calling the InvokeEndpoint operation: Received client error (400)\
          \ from primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\"\
          ,\n  \"message\": \"\\u0027gpt_neox\\u0027\"\n}\n</code>"
        raw: "Thanks for the help. Managed to deploy OK but unfortunately getting\
          \ an error during inference (I deployed using very big instances (ml.g4dn.12xlarge\
          \ that has 48 vCPU and 192 GiB mem). I wonder if the huggingface version\
          \ (amazon supports up to transformers_version=\"4.17.0\") is a problem or\
          \ if there's something else going on.\n\nI could deploy & do inference just\
          \ fine on neo 125MB model, but this big 20B model is causing an error during\
          \ inference:\n\n<code>\nModelError: An error occurred (ModelError) when\
          \ calling the InvokeEndpoint operation: Received client error (400) from\
          \ primary with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\"\
          ,\n  \"message\": \"\\u0027gpt_neox\\u0027\"\n}\n</code>"
        updatedAt: '2023-03-20T10:33:08.223Z'
      numEdits: 0
      reactions: []
    id: 64183664fa5500a41e599163
    type: comment
  author: juusohugs
  content: "Thanks for the help. Managed to deploy OK but unfortunately getting an\
    \ error during inference (I deployed using very big instances (ml.g4dn.12xlarge\
    \ that has 48 vCPU and 192 GiB mem). I wonder if the huggingface version (amazon\
    \ supports up to transformers_version=\"4.17.0\") is a problem or if there's something\
    \ else going on.\n\nI could deploy & do inference just fine on neo 125MB model,\
    \ but this big 20B model is causing an error during inference:\n\n<code>\nModelError:\
    \ An error occurred (ModelError) when calling the InvokeEndpoint operation: Received\
    \ client error (400) from primary with message \"{\n  \"code\": 400,\n  \"type\"\
    : \"InternalServerException\",\n  \"message\": \"\\u0027gpt_neox\\u0027\"\n}\n\
    </code>"
  created_at: 2023-03-20 09:33:08+00:00
  edited: false
  hidden: false
  id: 64183664fa5500a41e599163
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675681614196-629f4d9f5d4565ed65a12f59.jpeg?w=200&h=200&f=face
      fullname: JH
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: juusohugs
      type: user
    createdAt: '2023-03-20T13:02:41.000Z'
    data:
      edited: false
      editors:
      - juusohugs
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675681614196-629f4d9f5d4565ed65a12f59.jpeg?w=200&h=200&f=face
          fullname: JH
          isHf: false
          isPro: false
          name: juusohugs
          type: user
        html: '<p>It seems that most likely the problem is with the version. (Cross-posting
          this same link) ...here''s a potential solution:</p>

          <p><a rel="nofollow" href="https://towardsdatascience.com/unlock-the-latest-transformer-models-with-amazon-sagemaker-7fe65130d993">https://towardsdatascience.com/unlock-the-latest-transformer-models-with-amazon-sagemaker-7fe65130d993</a></p>

          '
        raw: 'It seems that most likely the problem is with the version. (Cross-posting
          this same link) ...here''s a potential solution:


          https://towardsdatascience.com/unlock-the-latest-transformer-models-with-amazon-sagemaker-7fe65130d993'
        updatedAt: '2023-03-20T13:02:41.518Z'
      numEdits: 0
      reactions: []
    id: 641859712fcc7664a1f60d81
    type: comment
  author: juusohugs
  content: 'It seems that most likely the problem is with the version. (Cross-posting
    this same link) ...here''s a potential solution:


    https://towardsdatascience.com/unlock-the-latest-transformer-models-with-amazon-sagemaker-7fe65130d993'
  created_at: 2023-03-20 12:02:41+00:00
  edited: false
  hidden: false
  id: 641859712fcc7664a1f60d81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8e6e1dbebee2324c0953db2b0fee2577.svg
      fullname: Anan Sarah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anansarah
      type: user
    createdAt: '2023-04-10T17:42:28.000Z'
    data:
      edited: false
      editors:
      - anansarah
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8e6e1dbebee2324c0953db2b0fee2577.svg
          fullname: Anan Sarah
          isHf: false
          isPro: false
          name: anansarah
          type: user
        html: '<p>HI, were you able to solve this with the above solution?</p>

          '
        raw: HI, were you able to solve this with the above solution?
        updatedAt: '2023-04-10T17:42:28.789Z'
      numEdits: 0
      reactions: []
    id: 64344a841a1ba6b55b18de3e
    type: comment
  author: anansarah
  content: HI, were you able to solve this with the above solution?
  created_at: 2023-04-10 16:42:28+00:00
  edited: false
  hidden: false
  id: 64344a841a1ba6b55b18de3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-04-14T06:41:48.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: '<p>It appears that 8-bit quantization works well on version 4.22.1,
          but not on version 4.21.1. So it is likely a version issue.</p>

          '
        raw: It appears that 8-bit quantization works well on version 4.22.1, but
          not on version 4.21.1. So it is likely a version issue.
        updatedAt: '2023-04-14T06:41:48.489Z'
      numEdits: 0
      reactions: []
    id: 6438f5ac74461ff502ea4bd9
    type: comment
  author: juewang
  content: It appears that 8-bit quantization works well on version 4.22.1, but not
    on version 4.21.1. So it is likely a version issue.
  created_at: 2023-04-14 05:41:48+00:00
  edited: false
  hidden: false
  id: 6438f5ac74461ff502ea4bd9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: togethercomputer/GPT-NeoXT-Chat-Base-20B
repo_type: model
status: open
target_branch: null
title: What kind of machine would be suitable for this model (in amazon sagemaker)?
