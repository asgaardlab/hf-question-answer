!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ai2p
conflicting_files: null
created_at: 2023-04-08 15:04:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22c019ff9b23d81ec9457c461d9076cf.svg
      fullname: Aoum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ai2p
      type: user
    createdAt: '2023-04-08T16:04:14.000Z'
    data:
      edited: false
      editors:
      - ai2p
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22c019ff9b23d81ec9457c461d9076cf.svg
          fullname: Aoum
          isHf: false
          isPro: false
          name: ai2p
          type: user
        html: '<p>Can it correctly span VRAM between many GPU cards? Or it needs to
          have all required VRAM  in one videocard only?</p>

          '
        raw: Can it correctly span VRAM between many GPU cards? Or it needs to have
          all required VRAM  in one videocard only?
        updatedAt: '2023-04-08T16:04:14.503Z'
      numEdits: 0
      reactions: []
    id: 6431907ef76c34519e92a8c9
    type: comment
  author: ai2p
  content: Can it correctly span VRAM between many GPU cards? Or it needs to have
    all required VRAM  in one videocard only?
  created_at: 2023-04-08 15:04:14+00:00
  edited: false
  hidden: false
  id: 6431907ef76c34519e92a8c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6caa5f4d7d5606c028f4ed836f29910.svg
      fullname: luis laguna
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jinxlo
      type: user
    createdAt: '2023-04-10T03:30:35.000Z'
    data:
      edited: true
      editors:
      - jinxlo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6caa5f4d7d5606c028f4ed836f29910.svg
          fullname: luis laguna
          isHf: false
          isPro: false
          name: jinxlo
          type: user
        html: '<p>Yes</p>

          '
        raw: 'Yes'
        updatedAt: '2023-04-10T03:31:10.339Z'
      numEdits: 1
      reactions: []
    id: 643382db0cdd0c3686f564bf
    type: comment
  author: jinxlo
  content: 'Yes'
  created_at: 2023-04-10 02:30:35+00:00
  edited: true
  hidden: false
  id: 643382db0cdd0c3686f564bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
      fullname: Jue Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: juewang
      type: user
    createdAt: '2023-04-14T06:29:28.000Z'
    data:
      edited: false
      editors:
      - juewang
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669380535028-61dce5c2af6d5e733e0fb08b.jpeg?w=200&h=200&f=face
          fullname: Jue Wang
          isHf: false
          isPro: false
          name: juewang
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ai2p&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ai2p\">@<span class=\"\
          underline\">ai2p</span></a></span>\n\n\t</span></span> Sure you can! Here\
          \ is an example to load model across multiple devices (need to install <code>accelerate</code>\
          \ first):</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> accelerate <span class=\"hljs-keyword\">import</span> init_empty_weights,\
          \ load_checkpoint_and_dispatch\n<span class=\"hljs-keyword\">from</span>\
          \ accelerate.utils <span class=\"hljs-keyword\">import</span> get_balanced_memory,\
          \ infer_auto_device_map\n<span class=\"hljs-keyword\">from</span> huggingface_hub\
          \ <span class=\"hljs-keyword\">import</span> snapshot_download\n<span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ AutoConfig, AutoTokenizer, AutoModelForCausalLM\n<span class=\"hljs-keyword\"\
          >import</span> torch\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">load_model</span>(<span class=\"hljs-params\">model_name</span>):\n\
          \    weights_path = snapshot_download(model_name)\n\n    config = AutoConfig.from_pretrained(model_name)\n\
          \n    <span class=\"hljs-comment\"># This will init model with meta tensors,\
          \ which basically does nothing.</span>\n    <span class=\"hljs-keyword\"\
          >with</span> init_empty_weights():\n        model = AutoModelForCausalLM.from_config(config)\n\
          \n    max_memory = get_balanced_memory(\n        model,\n        max_memory=<span\
          \ class=\"hljs-literal\">None</span>,\n        no_split_module_classes=[<span\
          \ class=\"hljs-string\">\"GPTNeoXLayer\"</span>],\n        dtype=<span class=\"\
          hljs-string\">'float16'</span>,\n        low_zero=<span class=\"hljs-literal\"\
          >False</span>,\n    )\n\n    device_map = infer_auto_device_map(\n     \
          \   model, \n        max_memory=max_memory,\n        no_split_module_classes=[<span\
          \ class=\"hljs-string\">\"GPTNeoXLayer\"</span>], \n        dtype=<span\
          \ class=\"hljs-string\">'float16'</span>\n    )\n\n    model = load_checkpoint_and_dispatch(\n\
          \        model, weights_path, device_map=device_map, no_split_module_classes=[<span\
          \ class=\"hljs-string\">\"GPTNeoXLayer\"</span>]\n    )\n\n    <span class=\"\
          hljs-keyword\">return</span> model\n\nmodel_name = <span class=\"hljs-string\"\
          >'togethercomputer/GPT-NeoXT-Chat-Base-20B'</span>\nmodel = load_model(model_name)\n\
          </code></pre>\n"
        raw: "@ai2p Sure you can! Here is an example to load model across multiple\
          \ devices (need to install `accelerate` first):\n```python\nfrom accelerate\
          \ import init_empty_weights, load_checkpoint_and_dispatch\nfrom accelerate.utils\
          \ import get_balanced_memory, infer_auto_device_map\nfrom huggingface_hub\
          \ import snapshot_download\nfrom transformers import AutoConfig, AutoTokenizer,\
          \ AutoModelForCausalLM\nimport torch\n\ndef load_model(model_name):\n  \
          \  weights_path = snapshot_download(model_name)\n\n    config = AutoConfig.from_pretrained(model_name)\n\
          \n    # This will init model with meta tensors, which basically does nothing.\n\
          \    with init_empty_weights():\n        model = AutoModelForCausalLM.from_config(config)\n\
          \n    max_memory = get_balanced_memory(\n        model,\n        max_memory=None,\n\
          \        no_split_module_classes=[\"GPTNeoXLayer\"],\n        dtype='float16',\n\
          \        low_zero=False,\n    )\n\n    device_map = infer_auto_device_map(\n\
          \        model, \n        max_memory=max_memory,\n        no_split_module_classes=[\"\
          GPTNeoXLayer\"], \n        dtype='float16'\n    )\n\n    model = load_checkpoint_and_dispatch(\n\
          \        model, weights_path, device_map=device_map, no_split_module_classes=[\"\
          GPTNeoXLayer\"]\n    )\n\n    return model\n\nmodel_name = 'togethercomputer/GPT-NeoXT-Chat-Base-20B'\n\
          model = load_model(model_name)\n```"
        updatedAt: '2023-04-14T06:29:28.633Z'
      numEdits: 0
      reactions: []
    id: 6438f2c84abd95b99d824e40
    type: comment
  author: juewang
  content: "@ai2p Sure you can! Here is an example to load model across multiple devices\
    \ (need to install `accelerate` first):\n```python\nfrom accelerate import init_empty_weights,\
    \ load_checkpoint_and_dispatch\nfrom accelerate.utils import get_balanced_memory,\
    \ infer_auto_device_map\nfrom huggingface_hub import snapshot_download\nfrom transformers\
    \ import AutoConfig, AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ndef\
    \ load_model(model_name):\n    weights_path = snapshot_download(model_name)\n\n\
    \    config = AutoConfig.from_pretrained(model_name)\n\n    # This will init model\
    \ with meta tensors, which basically does nothing.\n    with init_empty_weights():\n\
    \        model = AutoModelForCausalLM.from_config(config)\n\n    max_memory =\
    \ get_balanced_memory(\n        model,\n        max_memory=None,\n        no_split_module_classes=[\"\
    GPTNeoXLayer\"],\n        dtype='float16',\n        low_zero=False,\n    )\n\n\
    \    device_map = infer_auto_device_map(\n        model, \n        max_memory=max_memory,\n\
    \        no_split_module_classes=[\"GPTNeoXLayer\"], \n        dtype='float16'\n\
    \    )\n\n    model = load_checkpoint_and_dispatch(\n        model, weights_path,\
    \ device_map=device_map, no_split_module_classes=[\"GPTNeoXLayer\"]\n    )\n\n\
    \    return model\n\nmodel_name = 'togethercomputer/GPT-NeoXT-Chat-Base-20B'\n\
    model = load_model(model_name)\n```"
  created_at: 2023-04-14 05:29:28+00:00
  edited: false
  hidden: false
  id: 6438f2c84abd95b99d824e40
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: togethercomputer/GPT-NeoXT-Chat-Base-20B
repo_type: model
status: open
target_branch: null
title: 'Will it be possible to run this on PC with 8 GeForce RTX 3060 with 8 Gb VRAM
  each? '
