!!python/object:huggingface_hub.community.DiscussionWithDetails
author: shubhamagarwal92
conflicting_files: null
created_at: 2023-09-19 17:00:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652085632597-617bacfb191221bded6ed2c4.jpeg?w=200&h=200&f=face
      fullname: Shubham Agarwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shubhamagarwal92
      type: user
    createdAt: '2023-09-19T18:00:02.000Z'
    data:
      edited: false
      editors:
      - shubhamagarwal92
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.32335522770881653
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652085632597-617bacfb191221bded6ed2c4.jpeg?w=200&h=200&f=face
          fullname: Shubham Agarwal
          isHf: false
          isPro: false
          name: shubhamagarwal92
          type: user
        html: "<p>Hi, I get this runtime error when I try without flash attention,\
          \ ie. if I don't use <code>trust_remote_code=True</code></p>\n<pre><code>\
          \  File \"/mnt/home/miniconda/envs/autoreview/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/home/miniconda/envs/autoreview/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 305, in forward\n    query_states = self.q_proj(hidden_states)\n\
          \                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/home/miniconda/envs/autoreview/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/home/miniconda/envs/autoreview/lib/python3.11/site-packages/torch/nn/modules/linear.py\"\
          , line 114, in forward\n    return F.linear(input, self.weight, self.bias)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: \"addmm_impl_cpu_\"\
          \ not implemented for 'Half'\n</code></pre>\n"
        raw: "Hi, I get this runtime error when I try without flash attention, ie.\
          \ if I don't use `trust_remote_code=True`\r\n```\r\n  File \"/mnt/home/miniconda/envs/autoreview/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/home/miniconda/envs/autoreview/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 305, in forward\r\n    query_states = self.q_proj(hidden_states)\r\
          \n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/home/miniconda/envs/autoreview/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/home/miniconda/envs/autoreview/lib/python3.11/site-packages/torch/nn/modules/linear.py\"\
          , line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: \"\
          addmm_impl_cpu_\" not implemented for 'Half'\r\n```"
        updatedAt: '2023-09-19T18:00:02.887Z'
      numEdits: 0
      reactions: []
    id: 6509e1a27b68c4a6f681f133
    type: comment
  author: shubhamagarwal92
  content: "Hi, I get this runtime error when I try without flash attention, ie. if\
    \ I don't use `trust_remote_code=True`\r\n```\r\n  File \"/mnt/home/miniconda/envs/autoreview/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n   \
    \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/home/miniconda/envs/autoreview/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 305, in forward\r\n    query_states = self.q_proj(hidden_states)\r\n  \
    \                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/home/miniconda/envs/autoreview/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n   \
    \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/mnt/home/miniconda/envs/autoreview/lib/python3.11/site-packages/torch/nn/modules/linear.py\"\
    , line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: \"addmm_impl_cpu_\"\
    \ not implemented for 'Half'\r\n```"
  created_at: 2023-09-19 17:00:02+00:00
  edited: false
  hidden: false
  id: 6509e1a27b68c4a6f681f133
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-09-21T12:08:42.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8960244059562683
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;shubhamagarwal92&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/shubhamagarwal92\"\
          >@<span class=\"underline\">shubhamagarwal92</span></a></span>\n\n\t</span></span>\
          \ , this is likely because you're using float16 data type. Can you try to\
          \ change it to float32 and see if the error persists?</p>\n"
        raw: Hi @shubhamagarwal92 , this is likely because you're using float16 data
          type. Can you try to change it to float32 and see if the error persists?
        updatedAt: '2023-09-21T12:08:42.336Z'
      numEdits: 0
      reactions: []
    id: 650c324aa84110ffe07d0d97
    type: comment
  author: mauriceweber
  content: Hi @shubhamagarwal92 , this is likely because you're using float16 data
    type. Can you try to change it to float32 and see if the error persists?
  created_at: 2023-09-21 11:08:42+00:00
  edited: false
  hidden: false
  id: 650c324aa84110ffe07d0d97
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4070157397d9d617bcb73a7ee2ce14b.svg
      fullname: Mohammad Anash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anash
      type: user
    createdAt: '2023-10-06T04:10:46.000Z'
    data:
      edited: false
      editors:
      - Anash
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7644219398498535
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4070157397d9d617bcb73a7ee2ce14b.svg
          fullname: Mohammad Anash
          isHf: false
          isPro: false
          name: Anash
          type: user
        html: '<p>i have used float32 the error persists</p>

          '
        raw: 'i have used float32 the error persists

          '
        updatedAt: '2023-10-06T04:10:46.150Z'
      numEdits: 0
      reactions: []
    id: 651f88c6e17ef6a94e0a3656
    type: comment
  author: Anash
  content: 'i have used float32 the error persists

    '
  created_at: 2023-10-06 03:10:46+00:00
  edited: false
  hidden: false
  id: 651f88c6e17ef6a94e0a3656
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-10-13T07:31:33.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8973793387413025
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Anash&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Anash\">@<span class=\"\
          underline\">Anash</span></a></span>\n\n\t</span></span> , can you provide\
          \ the code you use that results in this error?</p>\n"
        raw: Hi @Anash , can you provide the code you use that results in this error?
        updatedAt: '2023-10-13T07:31:33.382Z'
      numEdits: 0
      reactions: []
    id: 6528f2559903f7a1c90a4e8c
    type: comment
  author: mauriceweber
  content: Hi @Anash , can you provide the code you use that results in this error?
  created_at: 2023-10-13 06:31:33+00:00
  edited: false
  hidden: false
  id: 6528f2559903f7a1c90a4e8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/def6daca459d81cd257366911e1f491d.svg
      fullname: waley W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: waleyWang
      type: user
    createdAt: '2023-11-09T07:44:25.000Z'
    data:
      edited: false
      editors:
      - waleyWang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5016207098960876
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/def6daca459d81cd257366911e1f491d.svg
          fullname: waley W
          isHf: false
          isPro: false
          name: waleyWang
          type: user
        html: "<blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Anash&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Anash\"\
          >@<span class=\"underline\">Anash</span></a></span>\n\n\t</span></span>\
          \ , can you provide the code you use that results in this error?</p>\n</blockquote>\n\
          <p>Hi mauriceweber, I have the same question, my code is below:<br>from\
          \ transformers import AutoTokenizer, AutoModelForCausalLM<br>import torch<br>model_path\
          \ = r'/home/model/LLaMA-2-7B-32K'<br>tokenizer = AutoTokenizer.from_pretrained(model_path)<br>model\
          \ = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,\
          \ torch_dtype=torch.float16)<br>input_context = \"Your text here\"<br>input_ids\
          \ = tokenizer.encode(input_context, return_tensors=\"pt\")<br>output = model.generate(input_ids,\
          \ max_new_tokens=6000, temperature=0.7)<br>output_text = tokenizer.decode(output[0],\
          \ skip_special_tokens=True)<br>output_text_path=r'/qa/txt/split-cat-00/test.txt'<br>with\
          \ open(output_text_path, 'w', encoding='utf-8') as output_file:<br>    output_file.write(output_text)<br>print('success')</p>\n\
          <p>RuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'</p>\n<p>if\
          \ I change the float16 to float32, there is new error:<br>File \"\", line\
          \ 21, in rotary_kernel<br>KeyError: ('2-.-0-.-0-1e8410f206c822547fb50e2ea86e45a6-d6252949da17ceb5f3a278a70250af1<br>3-1af5134066c618146d2cd009138944a0-9b9585c66493f30389231e55f159817f-3498c340fd4b6ee780<br>5fd54b882a04f5-e1f133f98d04093da2078dfc51c36b72-b26258bf01f839199e39d64851821f26-d7c06<br>e3b46e708006c15224aac7a1378-f585402118c8a136948ce0a49cfe122c',\
          \ (torch.float32, torch.f<br>loat32, torch.float32, torch.float32, None,\
          \ 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',<br>'i32', 'i32', 'i32', 'i32',\
          \ 'i32', 'i32', 'i32', 'i32'), (128, False, False, False, Fa<br>lse, 4),\
          \ (True, True, True, True, (False,), (True, False), (False, False), (True,\
          \ Fals<br>e), (True, False), (False, False), (False, False), (True, False),\
          \ (True, False), (True<br>, False), (False, True), (True, False), (True,\
          \ False), (True, False), (False, True)))</p>\n"
        raw: "> Hi @Anash , can you provide the code you use that results in this\
          \ error?\n\nHi mauriceweber, I have the same question, my code is below:\n\
          from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\
          model_path = r'/home/model/LLaMA-2-7B-32K'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\
          model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,\
          \ torch_dtype=torch.float16)\ninput_context = \"Your text here\"\ninput_ids\
          \ = tokenizer.encode(input_context, return_tensors=\"pt\")\noutput = model.generate(input_ids,\
          \ max_new_tokens=6000, temperature=0.7)\noutput_text = tokenizer.decode(output[0],\
          \ skip_special_tokens=True)\noutput_text_path=r'/qa/txt/split-cat-00/test.txt'\n\
          with open(output_text_path, 'w', encoding='utf-8') as output_file:\n   \
          \ output_file.write(output_text)\nprint('success')\n\nRuntimeError: \"addmm_impl_cpu_\"\
          \ not implemented for 'Half'\n\nif I change the float16 to float32, there\
          \ is new error:\nFile \"<string>\", line 21, in rotary_kernel\nKeyError:\
          \ ('2-.-0-.-0-1e8410f206c822547fb50e2ea86e45a6-d6252949da17ceb5f3a278a70250af1\n\
          3-1af5134066c618146d2cd009138944a0-9b9585c66493f30389231e55f159817f-3498c340fd4b6ee780\n\
          5fd54b882a04f5-e1f133f98d04093da2078dfc51c36b72-b26258bf01f839199e39d64851821f26-d7c06\n\
          e3b46e708006c15224aac7a1378-f585402118c8a136948ce0a49cfe122c', (torch.float32,\
          \ torch.f\nloat32, torch.float32, torch.float32, None, 'i32', 'i32', 'i32',\
          \ 'i32', 'i32', 'i32',\n'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32',\
          \ 'i32'), (128, False, False, False, Fa\nlse, 4), (True, True, True, True,\
          \ (False,), (True, False), (False, False), (True, Fals\ne), (True, False),\
          \ (False, False), (False, False), (True, False), (True, False), (True\n\
          , False), (False, True), (True, False), (True, False), (True, False), (False,\
          \ True)))\n"
        updatedAt: '2023-11-09T07:44:25.737Z'
      numEdits: 0
      reactions: []
    id: 654c8dd9e82a71cb487a1048
    type: comment
  author: waleyWang
  content: "> Hi @Anash , can you provide the code you use that results in this error?\n\
    \nHi mauriceweber, I have the same question, my code is below:\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM\nimport torch\nmodel_path = r'/home/model/LLaMA-2-7B-32K'\n\
    tokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
    \ trust_remote_code=True, torch_dtype=torch.float16)\ninput_context = \"Your text\
    \ here\"\ninput_ids = tokenizer.encode(input_context, return_tensors=\"pt\")\n\
    output = model.generate(input_ids, max_new_tokens=6000, temperature=0.7)\noutput_text\
    \ = tokenizer.decode(output[0], skip_special_tokens=True)\noutput_text_path=r'/qa/txt/split-cat-00/test.txt'\n\
    with open(output_text_path, 'w', encoding='utf-8') as output_file:\n    output_file.write(output_text)\n\
    print('success')\n\nRuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'\n\
    \nif I change the float16 to float32, there is new error:\nFile \"<string>\",\
    \ line 21, in rotary_kernel\nKeyError: ('2-.-0-.-0-1e8410f206c822547fb50e2ea86e45a6-d6252949da17ceb5f3a278a70250af1\n\
    3-1af5134066c618146d2cd009138944a0-9b9585c66493f30389231e55f159817f-3498c340fd4b6ee780\n\
    5fd54b882a04f5-e1f133f98d04093da2078dfc51c36b72-b26258bf01f839199e39d64851821f26-d7c06\n\
    e3b46e708006c15224aac7a1378-f585402118c8a136948ce0a49cfe122c', (torch.float32,\
    \ torch.f\nloat32, torch.float32, torch.float32, None, 'i32', 'i32', 'i32', 'i32',\
    \ 'i32', 'i32',\n'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32'), (128,\
    \ False, False, False, Fa\nlse, 4), (True, True, True, True, (False,), (True,\
    \ False), (False, False), (True, Fals\ne), (True, False), (False, False), (False,\
    \ False), (True, False), (True, False), (True\n, False), (False, True), (True,\
    \ False), (True, False), (True, False), (False, True)))\n"
  created_at: 2023-11-09 07:44:25+00:00
  edited: false
  hidden: false
  id: 654c8dd9e82a71cb487a1048
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 29
repo_id: togethercomputer/LLaMA-2-7B-32K
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: "addmm_impl_cpu_" not implemented for ''Half'''
