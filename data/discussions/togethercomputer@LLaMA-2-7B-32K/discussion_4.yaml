!!python/object:huggingface_hub.community.DiscussionWithDetails
author: s3nh
conflicting_files: null
created_at: 2023-07-29 18:48:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-07-29T19:48:22.000Z'
    data:
      edited: false
      editors:
      - s3nh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6967947483062744
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
          fullname: s3nh
          isHf: false
          isPro: false
          name: s3nh
          type: user
        html: '<p>Outstanding work! just convert it to ggml, check it out if your
          are interested! <a href="https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML">https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML</a></p>

          '
        raw: Outstanding work! just convert it to ggml, check it out if your are interested!
          https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML
        updatedAt: '2023-07-29T19:48:22.504Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - dzupin
        - mauriceweber
        - michaelmistaken
    id: 64c56d0661cc71b9c0af91b0
    type: comment
  author: s3nh
  content: Outstanding work! just convert it to ggml, check it out if your are interested!
    https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML
  created_at: 2023-07-29 18:48:22+00:00
  edited: false
  hidden: false
  id: 64c56d0661cc71b9c0af91b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-07-29T20:19:56.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8595961928367615
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;s3nh&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/s3nh\">@<span class=\"\
          underline\">s3nh</span></a></span>\n\n\t</span></span> Will your converted\
          \ model can run on colab's CPU easily? </p>\n"
        raw: '@s3nh Will your converted model can run on colab''s CPU easily? '
        updatedAt: '2023-07-29T20:19:56.617Z'
      numEdits: 0
      reactions: []
    id: 64c5746c5e43ae1ab60062b9
    type: comment
  author: deepakkaura26
  content: '@s3nh Will your converted model can run on colab''s CPU easily? '
  created_at: 2023-07-29 19:19:56+00:00
  edited: false
  hidden: false
  id: 64c5746c5e43ae1ab60062b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-08-02T08:06:06.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9328035116195679
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;deepakkaura26&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/deepakkaura26\"\
          >@<span class=\"underline\">deepakkaura26</span></a></span>\n\n\t</span></span>\
          \ I think so! by default you get 2 vCPUs on colab with 13G RAM which should\
          \ be enough to run the ggml versions</p>\n"
        raw: '@deepakkaura26 I think so! by default you get 2 vCPUs on colab with
          13G RAM which should be enough to run the ggml versions'
        updatedAt: '2023-08-02T08:06:06.162Z'
      numEdits: 0
      reactions: []
    id: 64ca0e6e5381684d3ed39929
    type: comment
  author: mauriceweber
  content: '@deepakkaura26 I think so! by default you get 2 vCPUs on colab with 13G
    RAM which should be enough to run the ggml versions'
  created_at: 2023-08-02 07:06:06+00:00
  edited: false
  hidden: false
  id: 64ca0e6e5381684d3ed39929
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-08-02T10:30:37.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8162475228309631
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mauriceweber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mauriceweber\"\
          >@<span class=\"underline\">mauriceweber</span></a></span>\n\n\t</span></span>\
          \ actually I tried it but whether I choose CPU or GPU my colab got crashed\
          \ 5 times. </p>\n"
        raw: '@mauriceweber actually I tried it but whether I choose CPU or GPU my
          colab got crashed 5 times. '
        updatedAt: '2023-08-02T10:30:37.734Z'
      numEdits: 0
      reactions: []
    id: 64ca304d547d7092d79fbb86
    type: comment
  author: deepakkaura26
  content: '@mauriceweber actually I tried it but whether I choose CPU or GPU my colab
    got crashed 5 times. '
  created_at: 2023-08-02 09:30:37+00:00
  edited: false
  hidden: false
  id: 64ca304d547d7092d79fbb86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-08-02T10:43:17.000Z'
    data:
      edited: true
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.794579803943634
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: '<p>Which quantization did you try? I tried the 4bit version on colab
          and could run it without problems.</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          ctransformers

          <span class="hljs-keyword">from</span> ctransformers <span class="hljs-keyword">import</span>
          AutoModelForCausalLM


          model_file = <span class="hljs-string">"LLaMA-2-7B-32K.ggmlv3.q4_0.bin"</span>

          model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">"s3nh/LLaMA-2-7B-32K-GGML"</span>,  model_type=<span
          class="hljs-string">"llama"</span>, model_file=model_file)


          prompt = <span class="hljs-string">"Whales have been living in the oceans
          for millions of years "</span>

          model(prompt, max_new_tokens=<span class="hljs-number">128</span>, temperature=<span
          class="hljs-number">0.9</span>, top_p= <span class="hljs-number">0.7</span>)

          </code></pre>

          <p>EDIT: load model directly from hub.</p>

          '
        raw: 'Which quantization did you try? I tried the 4bit version on colab and
          could run it without problems.


          ```python

          import ctransformers

          from ctransformers import AutoModelForCausalLM


          model_file = "LLaMA-2-7B-32K.ggmlv3.q4_0.bin"

          model = AutoModelForCausalLM.from_pretrained("s3nh/LLaMA-2-7B-32K-GGML",  model_type="llama",
          model_file=model_file)


          prompt = "Whales have been living in the oceans for millions of years "

          model(prompt, max_new_tokens=128, temperature=0.9, top_p= 0.7)

          ```


          EDIT: load model directly from hub.'
        updatedAt: '2023-08-03T06:25:09.458Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - s3nh
    id: 64ca334540ce646cc7ad8538
    type: comment
  author: mauriceweber
  content: 'Which quantization did you try? I tried the 4bit version on colab and
    could run it without problems.


    ```python

    import ctransformers

    from ctransformers import AutoModelForCausalLM


    model_file = "LLaMA-2-7B-32K.ggmlv3.q4_0.bin"

    model = AutoModelForCausalLM.from_pretrained("s3nh/LLaMA-2-7B-32K-GGML",  model_type="llama",
    model_file=model_file)


    prompt = "Whales have been living in the oceans for millions of years "

    model(prompt, max_new_tokens=128, temperature=0.9, top_p= 0.7)

    ```


    EDIT: load model directly from hub.'
  created_at: 2023-08-02 09:43:17+00:00
  edited: true
  hidden: false
  id: 64ca334540ce646cc7ad8538
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-08-02T15:26:30.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4045805335044861
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mauriceweber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mauriceweber\"\
          >@<span class=\"underline\">mauriceweber</span></a></span>\n\n\t</span></span>\
          \  I have use this same example which is present in this model website </p>\n\
          <p>from transformers import AutoTokenizer, AutoModelForCausalLM</p>\n<p>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\")<br>model\
          \ = AutoModelForCausalLM.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\"\
          , trust_remote_code=True, torch_dtype=torch.float16)</p>\n<p>input_context\
          \ = \"Your text here\"<br>input_ids = tokenizer.encode(input_context, return_tensors=\"\
          pt\")<br>output = model.generate(input_ids, max_length=128, temperature=0.7)<br>output_text\
          \ = tokenizer.decode(output[0], skip_special_tokens=True)<br>print(output_text)</p>\n"
        raw: "@mauriceweber  I have use this same example which is present in this\
          \ model website \n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\
          \ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\"\
          )\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\"\
          , trust_remote_code=True, torch_dtype=torch.float16)\n\ninput_context =\
          \ \"Your text here\"\ninput_ids = tokenizer.encode(input_context, return_tensors=\"\
          pt\")\noutput = model.generate(input_ids, max_length=128, temperature=0.7)\n\
          output_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(output_text)\n"
        updatedAt: '2023-08-02T15:26:30.547Z'
      numEdits: 0
      reactions: []
    id: 64ca75a6275c763046ca4f93
    type: comment
  author: deepakkaura26
  content: "@mauriceweber  I have use this same example which is present in this model\
    \ website \n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\
    tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\"\
    )\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\"\
    , trust_remote_code=True, torch_dtype=torch.float16)\n\ninput_context = \"Your\
    \ text here\"\ninput_ids = tokenizer.encode(input_context, return_tensors=\"pt\"\
    )\noutput = model.generate(input_ids, max_length=128, temperature=0.7)\noutput_text\
    \ = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(output_text)\n"
  created_at: 2023-08-02 14:26:30+00:00
  edited: false
  hidden: false
  id: 64ca75a6275c763046ca4f93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-08-02T19:16:42.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6238939762115479
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mauriceweber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mauriceweber\"\
          >@<span class=\"underline\">mauriceweber</span></a></span>\n\n\t</span></span>\
          \ I tried to run your codes which you showed they give me this following\
          \ error </p>\n<hr>\n<p>HTTPError                                 Traceback\
          \ (most recent call last)<br>/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\
          \ in hf_raise_for_status(response, endpoint_name)<br>    260     try:<br>--&gt;\
          \ 261         response.raise_for_status()<br>    262     except HTTPError\
          \ as e:</p>\n<p>11 frames<br>HTTPError: 401 Client Error: Unauthorized for\
          \ url: <a href=\"https://huggingface.co/api/models/LLaMA-2-7B-32K.ggmlv3.q4_0.bin/revision/main\"\
          >https://huggingface.co/api/models/LLaMA-2-7B-32K.ggmlv3.q4_0.bin/revision/main</a></p>\n\
          <p>The above exception was the direct cause of the following exception:</p>\n\
          <p>RepositoryNotFoundError                   Traceback (most recent call\
          \ last)<br>/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\
          \ in hf_raise_for_status(response, endpoint_name)<br>    291           \
          \      \" make sure you are authenticated.\"<br>    292             )<br>--&gt;\
          \ 293             raise RepositoryNotFoundError(message, response) from\
          \ e<br>    294<br>    295         elif response.status_code == 400:</p>\n\
          <p>RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-64caab34-5bd826d76686f26a76b02644;7f562443-2822-41e5-bcd0-37c62aef99f9)</p>\n\
          <p>Repository Not Found for url: <a href=\"https://huggingface.co/api/models/LLaMA-2-7B-32K.ggmlv3.q4_0.bin/revision/main\"\
          >https://huggingface.co/api/models/LLaMA-2-7B-32K.ggmlv3.q4_0.bin/revision/main</a>.<br>Please\
          \ make sure you specified the correct <code>repo_id</code> and <code>repo_type</code>.<br>If\
          \ you are trying to access a private or gated repo, make sure you are authenticated.<br>Invalid\
          \ username or password.</p>\n"
        raw: "@mauriceweber I tried to run your codes which you showed they give me\
          \ this following error \n\n---------------------------------------------------------------------------\n\
          HTTPError                                 Traceback (most recent call last)\n\
          /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\
          \ in hf_raise_for_status(response, endpoint_name)\n    260     try:\n-->\
          \ 261         response.raise_for_status()\n    262     except HTTPError\
          \ as e:\n\n11 frames\nHTTPError: 401 Client Error: Unauthorized for url:\
          \ https://huggingface.co/api/models/LLaMA-2-7B-32K.ggmlv3.q4_0.bin/revision/main\n\
          \nThe above exception was the direct cause of the following exception:\n\
          \nRepositoryNotFoundError                   Traceback (most recent call\
          \ last)\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\
          \ in hf_raise_for_status(response, endpoint_name)\n    291             \
          \    \" make sure you are authenticated.\"\n    292             )\n--> 293\
          \             raise RepositoryNotFoundError(message, response) from e\n\
          \    294 \n    295         elif response.status_code == 400:\n\nRepositoryNotFoundError:\
          \ 401 Client Error. (Request ID: Root=1-64caab34-5bd826d76686f26a76b02644;7f562443-2822-41e5-bcd0-37c62aef99f9)\n\
          \nRepository Not Found for url: https://huggingface.co/api/models/LLaMA-2-7B-32K.ggmlv3.q4_0.bin/revision/main.\n\
          Please make sure you specified the correct `repo_id` and `repo_type`.\n\
          If you are trying to access a private or gated repo, make sure you are authenticated.\n\
          Invalid username or password."
        updatedAt: '2023-08-02T19:16:42.448Z'
      numEdits: 0
      reactions: []
    id: 64caab9a667f4f808520d48b
    type: comment
  author: deepakkaura26
  content: "@mauriceweber I tried to run your codes which you showed they give me\
    \ this following error \n\n---------------------------------------------------------------------------\n\
    HTTPError                                 Traceback (most recent call last)\n\
    /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py in hf_raise_for_status(response,\
    \ endpoint_name)\n    260     try:\n--> 261         response.raise_for_status()\n\
    \    262     except HTTPError as e:\n\n11 frames\nHTTPError: 401 Client Error:\
    \ Unauthorized for url: https://huggingface.co/api/models/LLaMA-2-7B-32K.ggmlv3.q4_0.bin/revision/main\n\
    \nThe above exception was the direct cause of the following exception:\n\nRepositoryNotFoundError\
    \                   Traceback (most recent call last)\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\
    \ in hf_raise_for_status(response, endpoint_name)\n    291                 \"\
    \ make sure you are authenticated.\"\n    292             )\n--> 293         \
    \    raise RepositoryNotFoundError(message, response) from e\n    294 \n    295\
    \         elif response.status_code == 400:\n\nRepositoryNotFoundError: 401 Client\
    \ Error. (Request ID: Root=1-64caab34-5bd826d76686f26a76b02644;7f562443-2822-41e5-bcd0-37c62aef99f9)\n\
    \nRepository Not Found for url: https://huggingface.co/api/models/LLaMA-2-7B-32K.ggmlv3.q4_0.bin/revision/main.\n\
    Please make sure you specified the correct `repo_id` and `repo_type`.\nIf you\
    \ are trying to access a private or gated repo, make sure you are authenticated.\n\
    Invalid username or password."
  created_at: 2023-08-02 18:16:42+00:00
  edited: false
  hidden: false
  id: 64caab9a667f4f808520d48b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-08-03T06:30:07.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7678587436676025
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;mauriceweber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mauriceweber\"\
          >@<span class=\"underline\">mauriceweber</span></a></span>\n\n\t</span></span>\
          \  I have use this same example which is present in this model website </p>\n\
          <p>from transformers import AutoTokenizer, AutoModelForCausalLM</p>\n<p>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\")<br>model\
          \ = AutoModelForCausalLM.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\"\
          , trust_remote_code=True, torch_dtype=torch.float16)</p>\n<p>input_context\
          \ = \"Your text here\"<br>input_ids = tokenizer.encode(input_context, return_tensors=\"\
          pt\")<br>output = model.generate(input_ids, max_length=128, temperature=0.7)<br>output_text\
          \ = tokenizer.decode(output[0], skip_special_tokens=True)<br>print(output_text)</p>\n\
          </blockquote>\n<p>Here you are not using the quantized (ggml) models, which\
          \ is why you are running out of memory (you need around 14GB RAM for the\
          \ 7B model with float16).</p>\n<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;mauriceweber&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mauriceweber\"\
          >@<span class=\"underline\">mauriceweber</span></a></span>\n\n\t</span></span>\
          \ I tried to run your codes which you showed they give me this following\
          \ error</p>\n</blockquote>\n<p>This is error is because the model is not\
          \ downloaded yet (I was assuming you had it downloaded to colab) -- I adjusted\
          \ the code snippet above so that the model file gets pulled directly from\
          \ the repo. You can check the other model versions <a href=\"https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML/tree/main\"\
          >here</a>.</p>\n<p>Let us know how it goes!:)</p>\n"
        raw: "> @mauriceweber  I have use this same example which is present in this\
          \ model website \n> \n> from transformers import AutoTokenizer, AutoModelForCausalLM\n\
          > \n> tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\"\
          )\n> model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\"\
          , trust_remote_code=True, torch_dtype=torch.float16)\n> \n> input_context\
          \ = \"Your text here\"\n> input_ids = tokenizer.encode(input_context, return_tensors=\"\
          pt\")\n> output = model.generate(input_ids, max_length=128, temperature=0.7)\n\
          > output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\
          > print(output_text)\n\nHere you are not using the quantized (ggml) models,\
          \ which is why you are running out of memory (you need around 14GB RAM for\
          \ the 7B model with float16).\n\n> @mauriceweber I tried to run your codes\
          \ which you showed they give me this following error\n\nThis is error is\
          \ because the model is not downloaded yet (I was assuming you had it downloaded\
          \ to colab) -- I adjusted the code snippet above so that the model file\
          \ gets pulled directly from the repo. You can check the other model versions\
          \ [here](https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML/tree/main).\n\n\
          Let us know how it goes!:)\n"
        updatedAt: '2023-08-03T06:30:07.275Z'
      numEdits: 0
      reactions: []
    id: 64cb496f79d99f9e7a354e60
    type: comment
  author: mauriceweber
  content: "> @mauriceweber  I have use this same example which is present in this\
    \ model website \n> \n> from transformers import AutoTokenizer, AutoModelForCausalLM\n\
    > \n> tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\"\
    )\n> model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\"\
    , trust_remote_code=True, torch_dtype=torch.float16)\n> \n> input_context = \"\
    Your text here\"\n> input_ids = tokenizer.encode(input_context, return_tensors=\"\
    pt\")\n> output = model.generate(input_ids, max_length=128, temperature=0.7)\n\
    > output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n> print(output_text)\n\
    \nHere you are not using the quantized (ggml) models, which is why you are running\
    \ out of memory (you need around 14GB RAM for the 7B model with float16).\n\n\
    > @mauriceweber I tried to run your codes which you showed they give me this following\
    \ error\n\nThis is error is because the model is not downloaded yet (I was assuming\
    \ you had it downloaded to colab) -- I adjusted the code snippet above so that\
    \ the model file gets pulled directly from the repo. You can check the other model\
    \ versions [here](https://huggingface.co/s3nh/LLaMA-2-7B-32K-GGML/tree/main).\n\
    \nLet us know how it goes!:)\n"
  created_at: 2023-08-03 05:30:07+00:00
  edited: false
  hidden: false
  id: 64cb496f79d99f9e7a354e60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df90c855e1f66378b29d97e63ed7dcc3.svg
      fullname: Ivan Bokarev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sc0urge
      type: user
    createdAt: '2023-08-25T18:56:06.000Z'
    data:
      edited: false
      editors:
      - Sc0urge
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5193652510643005
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df90c855e1f66378b29d97e63ed7dcc3.svg
          fullname: Ivan Bokarev
          isHf: false
          isPro: false
          name: Sc0urge
          type: user
        html: '<p>Is this model already trained? running the example code just gives
          me this:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63cc3efef488db9bb3c841c9/34FlFROY95lTBcGak1bcn.png"><img
          alt="Screenshot 2023-08-25 at 20.54.51.png" src="https://cdn-uploads.huggingface.co/production/uploads/63cc3efef488db9bb3c841c9/34FlFROY95lTBcGak1bcn.png"></a></p>

          '
        raw: "Is this model already trained? running the example code just gives me\
          \ this: \n![Screenshot 2023-08-25 at 20.54.51.png](https://cdn-uploads.huggingface.co/production/uploads/63cc3efef488db9bb3c841c9/34FlFROY95lTBcGak1bcn.png)\n"
        updatedAt: '2023-08-25T18:56:06.424Z'
      numEdits: 0
      reactions: []
    id: 64e8f946f6d7c8bdfb6983f5
    type: comment
  author: Sc0urge
  content: "Is this model already trained? running the example code just gives me\
    \ this: \n![Screenshot 2023-08-25 at 20.54.51.png](https://cdn-uploads.huggingface.co/production/uploads/63cc3efef488db9bb3c841c9/34FlFROY95lTBcGak1bcn.png)\n"
  created_at: 2023-08-25 17:56:06+00:00
  edited: false
  hidden: false
  id: 64e8f946f6d7c8bdfb6983f5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: togethercomputer/LLaMA-2-7B-32K
repo_type: model
status: open
target_branch: null
title: GGML Version
