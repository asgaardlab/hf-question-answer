!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sayoyo
conflicting_files: null
created_at: 2023-07-31 10:46:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630e55148df86f1e5bf0c588/fNS9VJjPW9UzLG9oB6A6y.jpeg?w=200&h=200&f=face
      fullname: Majo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sayoyo
      type: user
    createdAt: '2023-07-31T11:46:18.000Z'
    data:
      edited: false
      editors:
      - Sayoyo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8884412050247192
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630e55148df86f1e5bf0c588/fNS9VJjPW9UzLG9oB6A6y.jpeg?w=200&h=200&f=face
          fullname: Majo
          isHf: false
          isPro: false
          name: Sayoyo
          type: user
        html: '<p>Just taking llama-2-7b as an example, I want to know how to train
          the context that can be extended to 32k. I saw that there is only fine-tuning
          llama-2-7b-32k code in openchatkit. If I want to train llama-2-7b to llama-2-7b-32k
          from scratch, what should I do?</p>

          '
        raw: Just taking llama-2-7b as an example, I want to know how to train the
          context that can be extended to 32k. I saw that there is only fine-tuning
          llama-2-7b-32k code in openchatkit. If I want to train llama-2-7b to llama-2-7b-32k
          from scratch, what should I do?
        updatedAt: '2023-07-31T11:46:18.342Z'
      numEdits: 0
      reactions: []
    id: 64c79f0a2c77e266440f7eda
    type: comment
  author: Sayoyo
  content: Just taking llama-2-7b as an example, I want to know how to train the context
    that can be extended to 32k. I saw that there is only fine-tuning llama-2-7b-32k
    code in openchatkit. If I want to train llama-2-7b to llama-2-7b-32k from scratch,
    what should I do?
  created_at: 2023-07-31 10:46:18+00:00
  edited: false
  hidden: false
  id: 64c79f0a2c77e266440f7eda
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
      fullname: Gardner Bickford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gardner
      type: user
    createdAt: '2023-08-01T01:30:59.000Z'
    data:
      edited: false
      editors:
      - gardner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7586929798126221
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
          fullname: Gardner Bickford
          isHf: false
          isPro: false
          name: gardner
          type: user
        html: "<p>Together published a <a rel=\"nofollow\" href=\"https://together.ai/blog/llama-2-7b-32k\"\
          >blog post</a> that describes the process in great detail:</p>\n<blockquote>\n\
          <p>On the modeling side, we follow Meta\u2019s recent paper and use <a rel=\"\
          nofollow\" href=\"https://arxiv.org/abs/2306.15595\">linear interpolation</a>\
          \ to extend the context length. This provides a powerful way to extend the\
          \ context length for models with rotary positional embeddings. We take the\
          \ LLaMA-2 checkpoint, and continue pre-training/fine-tuning it with linear\
          \ interpolation for 1.5B tokens.</p>\n</blockquote>\n<p>Start by reading\
          \ the paper: <a rel=\"nofollow\" href=\"https://arxiv.org/abs/2306.15595\"\
          >https://arxiv.org/abs/2306.15595</a></p>\n"
        raw: "Together published a [blog post](https://together.ai/blog/llama-2-7b-32k)\
          \ that describes the process in great detail:\n\n> On the modeling side,\
          \ we follow Meta\u2019s recent paper and use [linear interpolation](https://arxiv.org/abs/2306.15595)\
          \ to extend the context length. This provides a powerful way to extend the\
          \ context length for models with rotary positional embeddings. We take the\
          \ LLaMA-2 checkpoint, and continue pre-training/fine-tuning it with linear\
          \ interpolation for 1.5B tokens.\n\nStart by reading the paper: https://arxiv.org/abs/2306.15595\n"
        updatedAt: '2023-08-01T01:30:59.326Z'
      numEdits: 0
      reactions: []
    id: 64c86053bc292c606042319b
    type: comment
  author: gardner
  content: "Together published a [blog post](https://together.ai/blog/llama-2-7b-32k)\
    \ that describes the process in great detail:\n\n> On the modeling side, we follow\
    \ Meta\u2019s recent paper and use [linear interpolation](https://arxiv.org/abs/2306.15595)\
    \ to extend the context length. This provides a powerful way to extend the context\
    \ length for models with rotary positional embeddings. We take the LLaMA-2 checkpoint,\
    \ and continue pre-training/fine-tuning it with linear interpolation for 1.5B\
    \ tokens.\n\nStart by reading the paper: https://arxiv.org/abs/2306.15595\n"
  created_at: 2023-08-01 00:30:59+00:00
  edited: false
  hidden: false
  id: 64c86053bc292c606042319b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630e55148df86f1e5bf0c588/fNS9VJjPW9UzLG9oB6A6y.jpeg?w=200&h=200&f=face
      fullname: Majo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sayoyo
      type: user
    createdAt: '2023-08-01T04:14:09.000Z'
    data:
      edited: false
      editors:
      - Sayoyo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9590522050857544
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630e55148df86f1e5bf0c588/fNS9VJjPW9UzLG9oB6A6y.jpeg?w=200&h=200&f=face
          fullname: Majo
          isHf: false
          isPro: false
          name: Sayoyo
          type: user
        html: '<p>So this part of the code is not open source, right? I''ll look into
          this, thank you~</p>

          '
        raw: So this part of the code is not open source, right? I'll look into this,
          thank you~
        updatedAt: '2023-08-01T04:14:09.652Z'
      numEdits: 0
      reactions: []
    id: 64c88691ac1016256b836462
    type: comment
  author: Sayoyo
  content: So this part of the code is not open source, right? I'll look into this,
    thank you~
  created_at: 2023-08-01 03:14:09+00:00
  edited: false
  hidden: false
  id: 64c88691ac1016256b836462
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/277ff242cf15b380b80bdabfc0cfa030.svg
      fullname: Ce Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zhangce
      type: user
    createdAt: '2023-08-01T12:23:42.000Z'
    data:
      edited: false
      editors:
      - zhangce
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8148831725120544
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/277ff242cf15b380b80bdabfc0cfa030.svg
          fullname: Ce Zhang
          isHf: false
          isPro: false
          name: zhangce
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sayoyo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sayoyo\">@<span class=\"\
          underline\">Sayoyo</span></a></span>\n\n\t</span></span> the training code\
          \ is here <a rel=\"nofollow\" href=\"https://github.com/togethercomputer/OpenChatKit\"\
          >https://github.com/togethercomputer/OpenChatKit</a> ; and the datasets\
          \ are here <a href=\"https://huggingface.co/datasets/togethercomputer/Long-Data-Collections\"\
          >https://huggingface.co/datasets/togethercomputer/Long-Data-Collections</a></p>\n"
        raw: '@Sayoyo the training code is here https://github.com/togethercomputer/OpenChatKit
          ; and the datasets are here https://huggingface.co/datasets/togethercomputer/Long-Data-Collections'
        updatedAt: '2023-08-01T12:23:42.824Z'
      numEdits: 0
      reactions: []
    id: 64c8f94ed33e6a7f82ab8736
    type: comment
  author: zhangce
  content: '@Sayoyo the training code is here https://github.com/togethercomputer/OpenChatKit
    ; and the datasets are here https://huggingface.co/datasets/togethercomputer/Long-Data-Collections'
  created_at: 2023-08-01 11:23:42+00:00
  edited: false
  hidden: false
  id: 64c8f94ed33e6a7f82ab8736
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
      fullname: Gardner Bickford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gardner
      type: user
    createdAt: '2023-08-01T20:05:29.000Z'
    data:
      edited: true
      editors:
      - gardner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.875317394733429
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
          fullname: Gardner Bickford
          isHf: false
          isPro: false
          name: gardner
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sayoyo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sayoyo\">@<span class=\"\
          underline\">Sayoyo</span></a></span>\n\n\t</span></span> the paper states:</p>\n\
          <blockquote>\n<p>Models extended via Position Interpolation retain its original\
          \ architecture and can reuse most pre-existing optimization and infrastructure.</p>\n\
          </blockquote>\n<p>While the Together blog post states:</p>\n<blockquote>\n\
          <p>continue pre-training/fine-tuning it</p>\n</blockquote>\n<p>You would\
          \ have to try it out but this would suggest that starting from llama-2 base\
          \ weights can be done using the code in the GitHub <a rel=\"nofollow\" href=\"\
          https://github.com/togethercomputer/OpenChatKit\">repo</a>. The important\
          \ part seems to be using the <code>RotaryEmbedding</code> defined in <code>training/modules/llama_modules.py</code>\
          \ during training. You can see that this gets picked up in the <code>GPTStageBase</code>\
          \ class and ultimately used in training in the async implementation of Gpipe\
          \ via <code>get_pp_module()</code>.</p>\n<p>More specifically, to go from\
          \ llama-2 base you could try to pass the weights into the <code>prepare.py</code>\
          \ script:</p>\n<pre><code>python pretrained/Llama-2-7B-32K-beta/prepare.py\
          \ --model-name huggyllama/llama-7b # you might need these locally\n</code></pre>\n\
          <p>Then look in <code>training/finetune_llama-2-7b-32k-mqa.sh</code> for\
          \ ideas on what parameters you want to use while crunching through the long\
          \ dataset that <span data-props=\"{&quot;user&quot;:&quot;zhangce&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/zhangce\"\
          >@<span class=\"underline\">zhangce</span></a></span>\n\n\t</span></span>\
          \ shared, or try your own long dataset!</p>\n<p>As stated elsewhere, you\
          \ can expect the training process to require much more VRAM than for other\
          \ 7B models.</p>\n<p>Please report back with your results.</p>\n"
        raw: '@Sayoyo the paper states:


          > Models extended via Position Interpolation retain its original architecture
          and can reuse most pre-existing optimization and infrastructure.


          While the Together blog post states:

          > continue pre-training/fine-tuning it


          You would have to try it out but this would suggest that starting from llama-2
          base weights can be done using the code in the GitHub [repo](https://github.com/togethercomputer/OpenChatKit).
          The important part seems to be using the `RotaryEmbedding` defined in `training/modules/llama_modules.py`
          during training. You can see that this gets picked up in the `GPTStageBase`
          class and ultimately used in training in the async implementation of Gpipe
          via `get_pp_module()`.


          More specifically, to go from llama-2 base you could try to pass the weights
          into the `prepare.py` script:


          ```

          python pretrained/Llama-2-7B-32K-beta/prepare.py --model-name huggyllama/llama-7b
          # you might need these locally

          ```


          Then look in `training/finetune_llama-2-7b-32k-mqa.sh` for ideas on what
          parameters you want to use while crunching through the long dataset that
          @zhangce shared, or try your own long dataset!


          As stated elsewhere, you can expect the training process to require much
          more VRAM than for other 7B models.


          Please report back with your results.'
        updatedAt: '2023-08-01T20:07:36.671Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Sayoyo
    id: 64c965890d3d1b209d382451
    type: comment
  author: gardner
  content: '@Sayoyo the paper states:


    > Models extended via Position Interpolation retain its original architecture
    and can reuse most pre-existing optimization and infrastructure.


    While the Together blog post states:

    > continue pre-training/fine-tuning it


    You would have to try it out but this would suggest that starting from llama-2
    base weights can be done using the code in the GitHub [repo](https://github.com/togethercomputer/OpenChatKit).
    The important part seems to be using the `RotaryEmbedding` defined in `training/modules/llama_modules.py`
    during training. You can see that this gets picked up in the `GPTStageBase` class
    and ultimately used in training in the async implementation of Gpipe via `get_pp_module()`.


    More specifically, to go from llama-2 base you could try to pass the weights into
    the `prepare.py` script:


    ```

    python pretrained/Llama-2-7B-32K-beta/prepare.py --model-name huggyllama/llama-7b
    # you might need these locally

    ```


    Then look in `training/finetune_llama-2-7b-32k-mqa.sh` for ideas on what parameters
    you want to use while crunching through the long dataset that @zhangce shared,
    or try your own long dataset!


    As stated elsewhere, you can expect the training process to require much more
    VRAM than for other 7B models.


    Please report back with your results.'
  created_at: 2023-08-01 19:05:29+00:00
  edited: true
  hidden: false
  id: 64c965890d3d1b209d382451
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/277ff242cf15b380b80bdabfc0cfa030.svg
      fullname: Ce Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zhangce
      type: user
    createdAt: '2023-08-01T22:28:14.000Z'
    data:
      edited: false
      editors:
      - zhangce
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5961840152740479
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/277ff242cf15b380b80bdabfc0cfa030.svg
          fullname: Ce Zhang
          isHf: false
          isPro: false
          name: zhangce
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;gardner&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gardner\"\
          >@<span class=\"underline\">gardner</span></a></span>\n\n\t</span></span>!!\
          \ <span data-props=\"{&quot;user&quot;:&quot;Sayoyo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sayoyo\">@<span class=\"\
          underline\">Sayoyo</span></a></span>\n\n\t</span></span> let us know if\
          \ you get stuck!</p>\n"
        raw: Thanks @gardner!! @Sayoyo let us know if you get stuck!
        updatedAt: '2023-08-01T22:28:14.427Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F917"
        users:
        - gardner
        - Sayoyo
    id: 64c986fef7f4ccb5ea5f05ee
    type: comment
  author: zhangce
  content: Thanks @gardner!! @Sayoyo let us know if you get stuck!
  created_at: 2023-08-01 21:28:14+00:00
  edited: false
  hidden: false
  id: 64c986fef7f4ccb5ea5f05ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630e55148df86f1e5bf0c588/fNS9VJjPW9UzLG9oB6A6y.jpeg?w=200&h=200&f=face
      fullname: Majo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sayoyo
      type: user
    createdAt: '2023-08-02T03:16:22.000Z'
    data:
      edited: false
      editors:
      - Sayoyo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6797853708267212
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630e55148df86f1e5bf0c588/fNS9VJjPW9UzLG9oB6A6y.jpeg?w=200&h=200&f=face
          fullname: Majo
          isHf: false
          isPro: false
          name: Sayoyo
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gardner&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gardner\">@<span class=\"\
          underline\">gardner</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;zhangce&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/zhangce\">@<span class=\"underline\">zhangce</span></a></span>\n\
          \n\t</span></span> Get it! Thanks! I will try it~</p>\n"
        raw: '@gardner @zhangce Get it! Thanks! I will try it~'
        updatedAt: '2023-08-02T03:16:22.778Z'
      numEdits: 0
      reactions: []
    id: 64c9ca867205a6656c39d286
    type: comment
  author: Sayoyo
  content: '@gardner @zhangce Get it! Thanks! I will try it~'
  created_at: 2023-08-02 02:16:22+00:00
  edited: false
  hidden: false
  id: 64c9ca867205a6656c39d286
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/80b62ced6ce12113b0ce2a6c680d96c1.svg
      fullname: Lin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Johnatmars
      type: user
    createdAt: '2023-10-16T13:47:03.000Z'
    data:
      edited: false
      editors:
      - Johnatmars
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7786039710044861
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/80b62ced6ce12113b0ce2a6c680d96c1.svg
          fullname: Lin
          isHf: false
          isPro: false
          name: Johnatmars
          type: user
        html: "<blockquote>\n<p>the training code is here <a rel=\"nofollow\" href=\"\
          https://github.com/togethercomputer/OpenChatKit\">https://github.com/togethercomputer/OpenChatKit</a>\
          \ ; and the datasets are here <a href=\"https://huggingface.co/datasets/togethercomputer/Long-Data-Collections\"\
          >https://huggingface.co/datasets/togethercomputer/Long-Data-Collections</a></p>\n\
          </blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;zhangce&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/zhangce\"\
          >@<span class=\"underline\">zhangce</span></a></span>\n\n\t</span></span>,\
          \  just wonder if I can pretrain/fine-tune the model, for example, llama-2-13b\
          \  to get llama-2-13b-32k by using OpenChatKit ? If that is possible, can\
          \ you show me the path? Thanks</p>\n"
        raw: '> the training code is here https://github.com/togethercomputer/OpenChatKit
          ; and the datasets are here https://huggingface.co/datasets/togethercomputer/Long-Data-Collections


          Hi @zhangce,  just wonder if I can pretrain/fine-tune the model, for example,
          llama-2-13b  to get llama-2-13b-32k by using OpenChatKit ? If that is possible,
          can you show me the path? Thanks'
        updatedAt: '2023-10-16T13:47:03.693Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - gardner
    id: 652d3ed70c1bb6f33cc0b898
    type: comment
  author: Johnatmars
  content: '> the training code is here https://github.com/togethercomputer/OpenChatKit
    ; and the datasets are here https://huggingface.co/datasets/togethercomputer/Long-Data-Collections


    Hi @zhangce,  just wonder if I can pretrain/fine-tune the model, for example,
    llama-2-13b  to get llama-2-13b-32k by using OpenChatKit ? If that is possible,
    can you show me the path? Thanks'
  created_at: 2023-10-16 12:47:03+00:00
  edited: false
  hidden: false
  id: 652d3ed70c1bb6f33cc0b898
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: togethercomputer/LLaMA-2-7B-32K
repo_type: model
status: open
target_branch: null
title: How to training a llama-2-7B-32k from llama-2-7B?
