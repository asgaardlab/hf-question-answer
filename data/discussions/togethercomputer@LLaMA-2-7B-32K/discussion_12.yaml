!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ag0
conflicting_files: null
created_at: 2023-08-03 14:53:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0f8dd578dfc9ee41db586bdc7df4cafb.svg
      fullname: ag0
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ag0
      type: user
    createdAt: '2023-08-03T15:53:53.000Z'
    data:
      edited: false
      editors:
      - ag0
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7550080418586731
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0f8dd578dfc9ee41db586bdc7df4cafb.svg
          fullname: ag0
          isHf: false
          isPro: false
          name: ag0
          type: user
        html: '<p>Hello,</p>

          <p>In config.json, a linear rope_scaling of 8 is defined, and max_position_embeddings
          has been increased to 32768.</p>

          <p>However, the huggingface Llama2 doc specifies that when a rope scaling
          strategy is used, max_position_embeddings  should not be updated.<br><a
          href="https://huggingface.co/docs/transformers/main/model_doc/llama2#transformers.LlamaConfig.rope_scaling">https://huggingface.co/docs/transformers/main/model_doc/llama2#transformers.LlamaConfig.rope_scaling</a></p>

          <p>Wouldn''t the existing config result in the RoPE scaling being applied
          twice (especially when setting trust_remote_code=False)?</p>

          '
        raw: "Hello,\r\n\r\nIn config.json, a linear rope_scaling of 8 is defined,\
          \ and max_position_embeddings has been increased to 32768.\r\n\r\nHowever,\
          \ the huggingface Llama2 doc specifies that when a rope scaling strategy\
          \ is used, max_position_embeddings  should not be updated.\r\nhttps://huggingface.co/docs/transformers/main/model_doc/llama2#transformers.LlamaConfig.rope_scaling\r\
          \n\r\nWouldn't the existing config result in the RoPE scaling being applied\
          \ twice (especially when setting trust_remote_code=False)?"
        updatedAt: '2023-08-03T15:53:53.696Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Yhyu13
        - lllyyyyqqqq
    id: 64cbcd9100377e2848830f1c
    type: comment
  author: ag0
  content: "Hello,\r\n\r\nIn config.json, a linear rope_scaling of 8 is defined, and\
    \ max_position_embeddings has been increased to 32768.\r\n\r\nHowever, the huggingface\
    \ Llama2 doc specifies that when a rope scaling strategy is used, max_position_embeddings\
    \  should not be updated.\r\nhttps://huggingface.co/docs/transformers/main/model_doc/llama2#transformers.LlamaConfig.rope_scaling\r\
    \n\r\nWouldn't the existing config result in the RoPE scaling being applied twice\
    \ (especially when setting trust_remote_code=False)?"
  created_at: 2023-08-03 14:53:53+00:00
  edited: false
  hidden: false
  id: 64cbcd9100377e2848830f1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-08-04T10:02:29.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8503132462501526
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>This should be fixed</p>

          '
        raw: This should be fixed
        updatedAt: '2023-08-04T10:02:29.127Z'
      numEdits: 0
      reactions: []
    id: 64ccccb58174e45ae082fa51
    type: comment
  author: Yhyu13
  content: This should be fixed
  created_at: 2023-08-04 09:02:29+00:00
  edited: false
  hidden: false
  id: 64ccccb58174e45ae082fa51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-08-04T15:58:42.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8177810907363892
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ag0&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ag0\">@<span class=\"\
          underline\">ag0</span></a></span>\n\n\t</span></span> , thanks for bringing\
          \ this up! I think this only affects the NTK scaling but not the linear\
          \ scaling (which is what is adopted here): <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/fdd81aea12f06e24ab5cf5ba3c7316df3ab1a779/src/transformers/models/llama/modeling_llama.py#L135-L144\"\
          >https://github.com/huggingface/transformers/blob/fdd81aea12f06e24ab5cf5ba3c7316df3ab1a779/src/transformers/models/llama/modeling_llama.py#L135-L144</a></p>\n\
          <p>Let us know what you think!:)</p>\n"
        raw: 'Hi @ag0 , thanks for bringing this up! I think this only affects the
          NTK scaling but not the linear scaling (which is what is adopted here):
          https://github.com/huggingface/transformers/blob/fdd81aea12f06e24ab5cf5ba3c7316df3ab1a779/src/transformers/models/llama/modeling_llama.py#L135-L144


          Let us know what you think!:)


          '
        updatedAt: '2023-08-04T15:58:42.072Z'
      numEdits: 0
      reactions: []
    id: 64cd203257ff08e3d16735a1
    type: comment
  author: mauriceweber
  content: 'Hi @ag0 , thanks for bringing this up! I think this only affects the NTK
    scaling but not the linear scaling (which is what is adopted here): https://github.com/huggingface/transformers/blob/fdd81aea12f06e24ab5cf5ba3c7316df3ab1a779/src/transformers/models/llama/modeling_llama.py#L135-L144


    Let us know what you think!:)


    '
  created_at: 2023-08-04 14:58:42+00:00
  edited: false
  hidden: false
  id: 64cd203257ff08e3d16735a1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: togethercomputer/LLaMA-2-7B-32K
repo_type: model
status: open
target_branch: null
title: 'RoPE scaling and max_position_embeddings '
