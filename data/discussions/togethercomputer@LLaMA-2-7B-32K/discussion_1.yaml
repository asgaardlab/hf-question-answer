!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Said2k
conflicting_files: null
created_at: 2023-07-29 01:24:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8c049d12c947b8aea6c7a988a6e7c68.svg
      fullname: Anon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Said2k
      type: user
    createdAt: '2023-07-29T02:24:53.000Z'
    data:
      edited: false
      editors:
      - Said2k
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9772740602493286
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8c049d12c947b8aea6c7a988a6e7c68.svg
          fullname: Anon
          isHf: false
          isPro: false
          name: Said2k
          type: user
        html: '<p>What is the VRAM requirement of this model? I have 8 GB VRAM and
          I was wondering if this model could be run on that much?</p>

          '
        raw: What is the VRAM requirement of this model? I have 8 GB VRAM and I was
          wondering if this model could be run on that much?
        updatedAt: '2023-07-29T02:24:53.345Z'
      numEdits: 0
      reactions: []
    id: 64c47875bf19548901908c8d
    type: comment
  author: Said2k
  content: What is the VRAM requirement of this model? I have 8 GB VRAM and I was
    wondering if this model could be run on that much?
  created_at: 2023-07-29 01:24:53+00:00
  edited: false
  hidden: false
  id: 64c47875bf19548901908c8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c48bdd59b89cfe7405c14e7797bfcf17.svg
      fullname: Braxton Brown
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ThatOneShortGuy
      type: user
    createdAt: '2023-07-29T10:55:00.000Z'
    data:
      edited: false
      editors:
      - ThatOneShortGuy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7373115420341492
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c48bdd59b89cfe7405c14e7797bfcf17.svg
          fullname: Braxton Brown
          isHf: false
          isPro: false
          name: ThatOneShortGuy
          type: user
        html: '<p>If you have <code>bitsandbytes</code>, you should be able to load
          the model with <code>load_in_8bit=True</code>param in your <code>AutoModelForCausalLM</code>
          func</p>

          '
        raw: If you have `bitsandbytes`, you should be able to load the model with
          `load_in_8bit=True`param in your `AutoModelForCausalLM` func
        updatedAt: '2023-07-29T10:55:00.485Z'
      numEdits: 0
      reactions: []
    id: 64c4f004aedc4337780199ab
    type: comment
  author: ThatOneShortGuy
  content: If you have `bitsandbytes`, you should be able to load the model with `load_in_8bit=True`param
    in your `AutoModelForCausalLM` func
  created_at: 2023-07-29 09:55:00+00:00
  edited: false
  hidden: false
  id: 64c4f004aedc4337780199ab
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/277ff242cf15b380b80bdabfc0cfa030.svg
      fullname: Ce Zhang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zhangce
      type: user
    createdAt: '2023-07-29T13:27:57.000Z'
    data:
      edited: false
      editors:
      - zhangce
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9592850208282471
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/277ff242cf15b380b80bdabfc0cfa030.svg
          fullname: Ce Zhang
          isHf: false
          isPro: false
          name: zhangce
          type: user
        html: '<p>I don''t think VRAM 8GB is enough for this unfortunately (especially
          given that when we go to 32K, the size of KV cache becomes quite large too)
          -- we are pushing to decrease this! (e.g., we could do some KV cache quantization
          similar to what we have done in <a rel="nofollow" href="https://arxiv.org/abs/2303.06865">https://arxiv.org/abs/2303.06865</a>,
          but it will take time)</p>

          <p>In the meantime, you can go to <a rel="nofollow" href="https://api.together.xyz/playground">https://api.together.xyz/playground</a>
          to play with it!</p>

          <p>Ce</p>

          '
        raw: 'I don''t think VRAM 8GB is enough for this unfortunately (especially
          given that when we go to 32K, the size of KV cache becomes quite large too)
          -- we are pushing to decrease this! (e.g., we could do some KV cache quantization
          similar to what we have done in https://arxiv.org/abs/2303.06865, but it
          will take time)


          In the meantime, you can go to https://api.together.xyz/playground to play
          with it!


          Ce'
        updatedAt: '2023-07-29T13:27:57.854Z'
      numEdits: 0
      reactions: []
    id: 64c513dd5671d42e0abf604f
    type: comment
  author: zhangce
  content: 'I don''t think VRAM 8GB is enough for this unfortunately (especially given
    that when we go to 32K, the size of KV cache becomes quite large too) -- we are
    pushing to decrease this! (e.g., we could do some KV cache quantization similar
    to what we have done in https://arxiv.org/abs/2303.06865, but it will take time)


    In the meantime, you can go to https://api.together.xyz/playground to play with
    it!


    Ce'
  created_at: 2023-07-29 12:27:57+00:00
  edited: false
  hidden: false
  id: 64c513dd5671d42e0abf604f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2d17956d016e5534eafe3ab27ee8b8a0.svg
      fullname: Ostwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BajrangWappnet
      type: user
    createdAt: '2023-08-03T07:15:53.000Z'
    data:
      edited: false
      editors:
      - BajrangWappnet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5406314134597778
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2d17956d016e5534eafe3ab27ee8b8a0.svg
          fullname: Ostwal
          isHf: false
          isPro: false
          name: BajrangWappnet
          type: user
        html: '<p>How can we load the model using bitsandbytes ?</p>

          '
        raw: 'How can we load the model using bitsandbytes ?

          '
        updatedAt: '2023-08-03T07:15:53.640Z'
      numEdits: 0
      reactions: []
    id: 64cb5429ead94891d1f77a39
    type: comment
  author: BajrangWappnet
  content: 'How can we load the model using bitsandbytes ?

    '
  created_at: 2023-08-03 06:15:53+00:00
  edited: false
  hidden: false
  id: 64cb5429ead94891d1f77a39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-08-04T15:39:46.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.575692892074585
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;BajrangWappnet&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/BajrangWappnet\"\
          >@<span class=\"underline\">BajrangWappnet</span></a></span>\n\n\t</span></span>\
          \ , I think you can just do something like this:</p>\n<pre><code class=\"\
          language-python\">model = AutoModelForCausalLM.from_pretrained(\n    <span\
          \ class=\"hljs-string\">\"togethercomputer/LLaMA-2-7B-32K\"</span>, \n \
          \   trust_remote_code=<span class=\"hljs-literal\">False</span>, \n    torch_dtype=torch.float16,\n\
          \    load_in_8bit=<span class=\"hljs-literal\">True</span>\n)\n</code></pre>\n\
          <p>Here's a more detailed example on how to use <code>bitsandbytes</code>:\
          \ <a rel=\"nofollow\" href=\"https://github.com/TimDettmers/bitsandbytes/blob/main/examples/int8_inference_huggingface.py\"\
          >https://github.com/TimDettmers/bitsandbytes/blob/main/examples/int8_inference_huggingface.py</a></p>\n"
        raw: "@BajrangWappnet , I think you can just do something like this:\n\n```python\n\
          model = AutoModelForCausalLM.from_pretrained(\n    \"togethercomputer/LLaMA-2-7B-32K\"\
          , \n    trust_remote_code=False, \n    torch_dtype=torch.float16,\n    load_in_8bit=True\n\
          )\n```\nHere's a more detailed example on how to use `bitsandbytes`: https://github.com/TimDettmers/bitsandbytes/blob/main/examples/int8_inference_huggingface.py"
        updatedAt: '2023-08-04T15:39:46.119Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - ThatOneShortGuy
        - dineth9d
      - count: 1
        reaction: "\U0001F91D"
        users:
        - dineth9d
    id: 64cd1bc296cd8840e00e1ca1
    type: comment
  author: mauriceweber
  content: "@BajrangWappnet , I think you can just do something like this:\n\n```python\n\
    model = AutoModelForCausalLM.from_pretrained(\n    \"togethercomputer/LLaMA-2-7B-32K\"\
    , \n    trust_remote_code=False, \n    torch_dtype=torch.float16,\n    load_in_8bit=True\n\
    )\n```\nHere's a more detailed example on how to use `bitsandbytes`: https://github.com/TimDettmers/bitsandbytes/blob/main/examples/int8_inference_huggingface.py"
  created_at: 2023-08-04 14:39:46+00:00
  edited: false
  hidden: false
  id: 64cd1bc296cd8840e00e1ca1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9ba2e8bf38437dcec6da17b28bf73b5.svg
      fullname: Mathew S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MathewOpt
      type: user
    createdAt: '2023-08-15T22:57:42.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/c9ba2e8bf38437dcec6da17b28bf73b5.svg
          fullname: Mathew S
          isHf: false
          isPro: false
          name: MathewOpt
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-08-15T22:59:08.028Z'
      numEdits: 0
      reactions: []
    id: 64dc02e65f144aa29f11e379
    type: comment
  author: MathewOpt
  content: This comment has been hidden
  created_at: 2023-08-15 21:57:42+00:00
  edited: true
  hidden: true
  id: 64dc02e65f144aa29f11e379
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: togethercomputer/LLaMA-2-7B-32K
repo_type: model
status: open
target_branch: null
title: What is the VRAM requirement of this model?
