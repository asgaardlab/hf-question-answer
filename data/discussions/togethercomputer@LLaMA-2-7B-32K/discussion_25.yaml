!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ajash
conflicting_files: null
created_at: 2023-09-04 23:52:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
      fullname: Ambarish Jash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajash
      type: user
    createdAt: '2023-09-05T00:52:41.000Z'
    data:
      edited: true
      editors:
      - ajash
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5525025725364685
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
          fullname: Ambarish Jash
          isHf: false
          isPro: false
          name: ajash
          type: user
        html: '<p>I have installed all the required dependencies to run flash attn.:<br>!
          pip install flash-attn --no-build-isolation<br>! pip install git+<a rel="nofollow"
          href="https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary">https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary</a></p>

          <p>model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=''auto'',
          trust_remote_code=True, torch_dtype=torch.bfloat16, revision="refs/pr/17")<br>This
          is not working. Error:</p>

          <p>ImportError: Please install RoPE kernels: <code>pip install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary</code></p>

          <p>I have already installed this dependency.</p>

          '
        raw: "I have installed all the required dependencies to run flash attn.:\n\
          ! pip install flash-attn --no-build-isolation\n! pip install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map='auto',\
          \ trust_remote_code=True, torch_dtype=torch.bfloat16, revision=\"refs/pr/17\"\
          ) \nThis is not working. Error:\n\nImportError: Please install RoPE kernels:\
          \ `pip install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary`\n\
          \nI have already installed this dependency."
        updatedAt: '2023-09-05T00:53:20.720Z'
      numEdits: 1
      reactions: []
    id: 64f67bd96ad07ea81785a860
    type: comment
  author: ajash
  content: "I have installed all the required dependencies to run flash attn.:\n!\
    \ pip install flash-attn --no-build-isolation\n! pip install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\n\
    \nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map='auto', trust_remote_code=True,\
    \ torch_dtype=torch.bfloat16, revision=\"refs/pr/17\") \nThis is not working.\
    \ Error:\n\nImportError: Please install RoPE kernels: `pip install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary`\n\
    \nI have already installed this dependency."
  created_at: 2023-09-04 23:52:41+00:00
  edited: true
  hidden: false
  id: 64f67bd96ad07ea81785a860
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
      fullname: Ambarish Jash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajash
      type: user
    createdAt: '2023-09-05T01:08:13.000Z'
    data:
      edited: false
      editors:
      - ajash
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4991939663887024
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
          fullname: Ambarish Jash
          isHf: false
          isPro: false
          name: ajash
          type: user
        html: "<p>Output of:<br>model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\
          \ device_map='auto', trust_remote_code=True, torch_dtype=torch.bfloat16)</p>\n\
          <p>Downloading (\u2026)lve/main/config.json: 100%<br>709/709 [00:00&lt;00:00,\
          \ 62.2kB/s]<br>Downloading (\u2026)eling_flash_llama.py: 100%<br>45.3k/45.3k\
          \ [00:00&lt;00:00, 3.74MB/s]<br>A new version of the following files was\
          \ downloaded from <a href=\"https://huggingface.co/togethercomputer/LLaMA-2-7B-32K\"\
          >https://huggingface.co/togethercomputer/LLaMA-2-7B-32K</a>:</p>\n<ul>\n\
          <li>modeling_flash_llama.py<br>. Make sure to double-check they do not contain\
          \ any added malicious code. To avoid downloading new versions of the code\
          \ file, you can pin a revision.<blockquote>\n<blockquote>\n<blockquote>\n\
          <blockquote>\n<p>Flash Attention installed</p>\n</blockquote>\n</blockquote>\n\
          </blockquote>\n</blockquote>\n</li>\n</ul>\n<hr>\n<p>ModuleNotFoundError\
          \                       Traceback (most recent call last)<br>~/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/aef6d8946ae1015bdb65c478a2dd73b58daaef47/modeling_flash_llama.py\
          \ in <br>     51 try:<br>---&gt; 52     from flash_attn.layers.rotary import\
          \ apply_rotary_emb_func<br>     53     flash_rope_installed = True</p>\n\
          <p>12 frames<br>ModuleNotFoundError: No module named 'flash_attn.ops.triton'</p>\n\
          <p>During handling of the above exception, another exception occurred:</p>\n\
          <p>ImportError                               Traceback (most recent call\
          \ last)<br>~/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/aef6d8946ae1015bdb65c478a2dd73b58daaef47/modeling_flash_llama.py\
          \ in <br>     55 except ImportError:<br>     56     flash_rope_installed\
          \ = False<br>---&gt; 57     raise ImportError('Please install RoPE kernels:\
          \ <code>pip install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary</code>')<br>\
          \     58<br>     59 </p>\n<p>ImportError: Please install RoPE kernels: <code>pip\
          \ install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary</code></p>\n"
        raw: "Output of:\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map='auto',\
          \ trust_remote_code=True, torch_dtype=torch.bfloat16)\n\nDownloading (\u2026\
          )lve/main/config.json: 100%\n709/709 [00:00<00:00, 62.2kB/s]\nDownloading\
          \ (\u2026)eling_flash_llama.py: 100%\n45.3k/45.3k [00:00<00:00, 3.74MB/s]\n\
          A new version of the following files was downloaded from https://huggingface.co/togethercomputer/LLaMA-2-7B-32K:\n\
          - modeling_flash_llama.py\n. Make sure to double-check they do not contain\
          \ any added malicious code. To avoid downloading new versions of the code\
          \ file, you can pin a revision.\n>>>> Flash Attention installed\n---------------------------------------------------------------------------\n\
          ModuleNotFoundError                       Traceback (most recent call last)\n\
          ~/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/aef6d8946ae1015bdb65c478a2dd73b58daaef47/modeling_flash_llama.py\
          \ in <module>\n     51 try:\n---> 52     from flash_attn.layers.rotary import\
          \ apply_rotary_emb_func\n     53     flash_rope_installed = True\n\n12 frames\n\
          ModuleNotFoundError: No module named 'flash_attn.ops.triton'\n\nDuring handling\
          \ of the above exception, another exception occurred:\n\nImportError   \
          \                            Traceback (most recent call last)\n~/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/aef6d8946ae1015bdb65c478a2dd73b58daaef47/modeling_flash_llama.py\
          \ in <module>\n     55 except ImportError:\n     56     flash_rope_installed\
          \ = False\n---> 57     raise ImportError('Please install RoPE kernels: `pip\
          \ install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary`')\n\
          \     58 \n     59 \n\nImportError: Please install RoPE kernels: `pip install\
          \ git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary`"
        updatedAt: '2023-09-05T01:08:13.682Z'
      numEdits: 0
      reactions: []
    id: 64f67f7daad4f0fee08f226c
    type: comment
  author: ajash
  content: "Output of:\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map='auto',\
    \ trust_remote_code=True, torch_dtype=torch.bfloat16)\n\nDownloading (\u2026)lve/main/config.json:\
    \ 100%\n709/709 [00:00<00:00, 62.2kB/s]\nDownloading (\u2026)eling_flash_llama.py:\
    \ 100%\n45.3k/45.3k [00:00<00:00, 3.74MB/s]\nA new version of the following files\
    \ was downloaded from https://huggingface.co/togethercomputer/LLaMA-2-7B-32K:\n\
    - modeling_flash_llama.py\n. Make sure to double-check they do not contain any\
    \ added malicious code. To avoid downloading new versions of the code file, you\
    \ can pin a revision.\n>>>> Flash Attention installed\n---------------------------------------------------------------------------\n\
    ModuleNotFoundError                       Traceback (most recent call last)\n\
    ~/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/aef6d8946ae1015bdb65c478a2dd73b58daaef47/modeling_flash_llama.py\
    \ in <module>\n     51 try:\n---> 52     from flash_attn.layers.rotary import\
    \ apply_rotary_emb_func\n     53     flash_rope_installed = True\n\n12 frames\n\
    ModuleNotFoundError: No module named 'flash_attn.ops.triton'\n\nDuring handling\
    \ of the above exception, another exception occurred:\n\nImportError         \
    \                      Traceback (most recent call last)\n~/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/aef6d8946ae1015bdb65c478a2dd73b58daaef47/modeling_flash_llama.py\
    \ in <module>\n     55 except ImportError:\n     56     flash_rope_installed =\
    \ False\n---> 57     raise ImportError('Please install RoPE kernels: `pip install\
    \ git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary`')\n\
    \     58 \n     59 \n\nImportError: Please install RoPE kernels: `pip install\
    \ git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary`"
  created_at: 2023-09-05 00:08:13+00:00
  edited: false
  hidden: false
  id: 64f67f7daad4f0fee08f226c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-09-05T15:46:28.000Z'
    data:
      edited: true
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5024199485778809
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>Currently a <a rel="nofollow" href="https://github.com/Dao-AILab/flash-attention/issues/519">bug
          in flash-attn</a>. Try installing v2.1.1 for now:</p>

          <pre><code>pip install flash-attn==2.1.1 --no-build-isolation

          pip install git+https://github.com/HazyResearch/flash-attention.git@v2.1.1#subdirectory=csrc/rotary

          </code></pre>

          '
        raw: 'Currently a [bug in flash-attn](https://github.com/Dao-AILab/flash-attention/issues/519).
          Try installing v2.1.1 for now:

          ```

          pip install flash-attn==2.1.1 --no-build-isolation

          pip install git+https://github.com/HazyResearch/flash-attention.git@v2.1.1#subdirectory=csrc/rotary

          ```'
        updatedAt: '2023-09-05T15:46:40.381Z'
      numEdits: 1
      reactions: []
    id: 64f74d5405961fa1275c3dc6
    type: comment
  author: bjoernp
  content: 'Currently a [bug in flash-attn](https://github.com/Dao-AILab/flash-attention/issues/519).
    Try installing v2.1.1 for now:

    ```

    pip install flash-attn==2.1.1 --no-build-isolation

    pip install git+https://github.com/HazyResearch/flash-attention.git@v2.1.1#subdirectory=csrc/rotary

    ```'
  created_at: 2023-09-05 14:46:28+00:00
  edited: true
  hidden: false
  id: 64f74d5405961fa1275c3dc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
      fullname: Ambarish Jash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajash
      type: user
    createdAt: '2023-09-06T03:12:27.000Z'
    data:
      edited: false
      editors:
      - ajash
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.991226851940155
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
          fullname: Ambarish Jash
          isHf: false
          isPro: false
          name: ajash
          type: user
        html: '<p>that worked... thanks<br>how does one figure this out by themselves
          :)</p>

          '
        raw: 'that worked... thanks

          how does one figure this out by themselves :)'
        updatedAt: '2023-09-06T03:12:27.576Z'
      numEdits: 0
      reactions: []
    id: 64f7ee1bcb48ea77522ded82
    type: comment
  author: ajash
  content: 'that worked... thanks

    how does one figure this out by themselves :)'
  created_at: 2023-09-06 02:12:27+00:00
  edited: false
  hidden: false
  id: 64f7ee1bcb48ea77522ded82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/74754d3fd5d563778a6eedfed75ccabc.svg
      fullname: MaZeN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaZeNsMz
      type: user
    createdAt: '2023-09-18T18:08:53.000Z'
    data:
      edited: false
      editors:
      - MaZeNsMz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4474746286869049
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/74754d3fd5d563778a6eedfed75ccabc.svg
          fullname: MaZeN
          isHf: false
          isPro: false
          name: MaZeNsMz
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64c677643f3387bcfa3ae54b/iR4EvP3wF6I61M2d2XVml.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64c677643f3387bcfa3ae54b/iR4EvP3wF6I61M2d2XVml.png"></a></p>

          '
        raw: '

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c677643f3387bcfa3ae54b/iR4EvP3wF6I61M2d2XVml.png)

          '
        updatedAt: '2023-09-18T18:08:53.717Z'
      numEdits: 0
      reactions: []
    id: 650892356d2284f732206e83
    type: comment
  author: MaZeNsMz
  content: '

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c677643f3387bcfa3ae54b/iR4EvP3wF6I61M2d2XVml.png)

    '
  created_at: 2023-09-18 17:08:53+00:00
  edited: false
  hidden: false
  id: 650892356d2284f732206e83
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: togethercomputer/LLaMA-2-7B-32K
repo_type: model
status: open
target_branch: null
title: Installing ! pip install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary
  but flah_llama still erroring out
