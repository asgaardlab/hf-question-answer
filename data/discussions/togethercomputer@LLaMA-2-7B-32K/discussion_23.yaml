!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rozek
conflicting_files: null
created_at: 2023-08-30 04:52:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
      fullname: Andreas Rozek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rozek
      type: user
    createdAt: '2023-08-30T05:52:27.000Z'
    data:
      edited: false
      editors:
      - rozek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8400479555130005
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
          fullname: Andreas Rozek
          isHf: false
          isPro: false
          name: rozek
          type: user
        html: '<p>Thank you very much for this marvellous work! Being able to use
          long contexts (depending on the amount of available RAM, of course) is wonderful!</p>

          <p>In order to use your model with <a rel="nofollow" href="https://github.com/rozek/llama.cpp">llama.cpp</a>,
          I''ve generated some <a href="https://huggingface.co/rozek/LLaMA-2-7B-32K_GGUF">quantizations
          in GGUF format</a>.</p>

          '
        raw: "Thank you very much for this marvellous work! Being able to use long\
          \ contexts (depending on the amount of available RAM, of course) is wonderful!\r\
          \n\r\nIn order to use your model with [llama.cpp](https://github.com/rozek/llama.cpp),\
          \ I've generated some [quantizations in GGUF format](https://huggingface.co/rozek/LLaMA-2-7B-32K_GGUF)."
        updatedAt: '2023-08-30T05:52:27.572Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - amaliak
        - krzspam
        - nospotfer
    id: 64eed91b02444512fa82d53c
    type: comment
  author: rozek
  content: "Thank you very much for this marvellous work! Being able to use long contexts\
    \ (depending on the amount of available RAM, of course) is wonderful!\r\n\r\n\
    In order to use your model with [llama.cpp](https://github.com/rozek/llama.cpp),\
    \ I've generated some [quantizations in GGUF format](https://huggingface.co/rozek/LLaMA-2-7B-32K_GGUF)."
  created_at: 2023-08-30 04:52:27+00:00
  edited: false
  hidden: false
  id: 64eed91b02444512fa82d53c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8a92585ea4ad105a7db29b46dfa7c26.svg
      fullname: Amalia Kostopoulou
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: amaliak
      type: user
    createdAt: '2023-08-30T15:59:05.000Z'
    data:
      edited: false
      editors:
      - amaliak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8350734710693359
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8a92585ea4ad105a7db29b46dfa7c26.svg
          fullname: Amalia Kostopoulou
          isHf: false
          isPro: false
          name: amaliak
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rozek&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rozek\">@<span class=\"\
          underline\">rozek</span></a></span>\n\n\t</span></span> Thank you for your\
          \ support!</p>\n"
        raw: '@rozek Thank you for your support!'
        updatedAt: '2023-08-30T15:59:05.715Z'
      numEdits: 0
      reactions: []
    id: 64ef6749a52ce115af22af0f
    type: comment
  author: amaliak
  content: '@rozek Thank you for your support!'
  created_at: 2023-08-30 14:59:05+00:00
  edited: false
  hidden: false
  id: 64ef6749a52ce115af22af0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
      fullname: Andreas Rozek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rozek
      type: user
    createdAt: '2023-08-31T01:05:46.000Z'
    data:
      edited: false
      editors:
      - rozek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9154372215270996
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
          fullname: Andreas Rozek
          isHf: false
          isPro: false
          name: rozek
          type: user
        html: '<p>You''re welcome! If you want any changes in my description of your
          model, just tell me!</p>

          '
        raw: You're welcome! If you want any changes in my description of your model,
          just tell me!
        updatedAt: '2023-08-31T01:05:46.670Z'
      numEdits: 0
      reactions: []
    id: 64efe76a4fd2e522dea49895
    type: comment
  author: rozek
  content: You're welcome! If you want any changes in my description of your model,
    just tell me!
  created_at: 2023-08-31 00:05:46+00:00
  edited: false
  hidden: false
  id: 64efe76a4fd2e522dea49895
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/182b1994ad37ed23d8a066caeaef83d5.svg
      fullname: dario
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prudant
      type: user
    createdAt: '2023-09-30T06:18:38.000Z'
    data:
      edited: false
      editors:
      - prudant
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6422111392021179
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/182b1994ad37ed23d8a066caeaef83d5.svg
          fullname: dario
          isHf: false
          isPro: false
          name: prudant
          type: user
        html: '<p>llama cpp supports long contexts? (more than 4096) ? thanks!</p>

          '
        raw: llama cpp supports long contexts? (more than 4096) ? thanks!
        updatedAt: '2023-09-30T06:18:38.021Z'
      numEdits: 0
      reactions: []
    id: 6517bdbe7069441423419570
    type: comment
  author: prudant
  content: llama cpp supports long contexts? (more than 4096) ? thanks!
  created_at: 2023-09-30 05:18:38+00:00
  edited: false
  hidden: false
  id: 6517bdbe7069441423419570
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
      fullname: Andreas Rozek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rozek
      type: user
    createdAt: '2023-10-02T06:19:49.000Z'
    data:
      edited: true
      editors:
      - rozek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9711341261863708
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/313a9e2786513613725c10b7a1a01176.svg
          fullname: Andreas Rozek
          isHf: false
          isPro: false
          name: rozek
          type: user
        html: '<p>Hello! Sorry for the late response, but I have been quite busy in
          the last few days.</p>

          <p>Yes, it supports longer contexts - provided that you change the limits
          in the source and recompile - as I did in <a rel="nofollow" href="https://github.com/rozek/llama.cpp">my
          own fork</a> <del>(which is currently a bit behind, let''s see if I find
          the time to sync it with the original again)</del> (which I just synced
          with the original branch again)</p>

          '
        raw: 'Hello! Sorry for the late response, but I have been quite busy in the
          last few days.


          Yes, it supports longer contexts - provided that you change the limits in
          the source and recompile - as I did in [my own fork](https://github.com/rozek/llama.cpp)
          ~~(which is currently a bit behind, let''s see if I find the time to sync
          it with the original again)~~ (which I just synced with the original branch
          again)'
        updatedAt: '2023-10-02T06:34:29.434Z'
      numEdits: 1
      reactions: []
    id: 651a6105ff197684a55c50ef
    type: comment
  author: rozek
  content: 'Hello! Sorry for the late response, but I have been quite busy in the
    last few days.


    Yes, it supports longer contexts - provided that you change the limits in the
    source and recompile - as I did in [my own fork](https://github.com/rozek/llama.cpp)
    ~~(which is currently a bit behind, let''s see if I find the time to sync it with
    the original again)~~ (which I just synced with the original branch again)'
  created_at: 2023-10-02 05:19:49+00:00
  edited: true
  hidden: false
  id: 651a6105ff197684a55c50ef
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: togethercomputer/LLaMA-2-7B-32K
repo_type: model
status: open
target_branch: null
title: Quantizations for llama.cpp
