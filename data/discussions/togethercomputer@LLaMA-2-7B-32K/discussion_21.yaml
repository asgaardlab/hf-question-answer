!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NABARKA
conflicting_files: null
created_at: 2023-08-25 05:52:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/44d972b255b2bcbc7d5a60b7fe165dc9.svg
      fullname: DEB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NABARKA
      type: user
    createdAt: '2023-08-25T06:52:37.000Z'
    data:
      edited: false
      editors:
      - NABARKA
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44218480587005615
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/44d972b255b2bcbc7d5a60b7fe165dc9.svg
          fullname: DEB
          isHf: false
          isPro: false
          name: NABARKA
          type: user
        html: '<p>What should be the MAX_INPUT_LENGTH,MAX_TOTAL_TOKENS, MAX_BATCH_TOTAL_TOKENS
          any idea?</p>

          <h1 id="sagemaker-config">sagemaker config</h1>

          <p>instance_type = "ml.g5.2xlarge"<br>number_of_gpu = 1<br>health_check_timeout
          = 300</p>

          <h1 id="define-model-and-endpoint-configuration-parameter">Define Model
          and Endpoint configuration parameter</h1>

          <p>config = {<br>  ''HF_MODEL_ID'': "togethercomputer/Llama-2-7B-32K-Instruct",
          # model_id from hf.co/models<br>  ''SM_NUM_GPUS'': json.dumps(number_of_gpu),
          # Number of GPU used per replica<br>  ''MAX_INPUT_LENGTH'': json.dumps(MAX_INPUT_LENGTH),  #
          Max length of input text<br>  ''MAX_TOTAL_TOKENS'': json.dumps(MAX_TOTAL_TOKENS),  #
          Max length of the generation (including input text)<br>  ''MAX_BATCH_TOTAL_TOKENS'':
          json.dumps(MAX_BATCH_TOTAL_TOKEN),  # Limits the number of tokens that can
          be processed in parallel during the generation<br>  ''HUGGING_FACE_HUB_TOKEN'':
          json.dumps("HF_TOKEN")<br>}</p>

          <h1 id="check-if-token-is-set">check if token is set</h1>

          <p>assert config[''HUGGING_FACE_HUB_TOKEN''] != "HF_TOKEN", "Please set
          your Hugging Face Hub token"</p>

          <h1 id="create-huggingfacemodel-with-the-image-uri">create HuggingFaceModel
          with the image uri</h1>

          <p>llm_model = HuggingFaceModel(<br>  role=role,<br>  image_uri=llm_image,<br>  env=config<br>)</p>

          '
        raw: "What should be the MAX_INPUT_LENGTH,MAX_TOTAL_TOKENS, MAX_BATCH_TOTAL_TOKENS\
          \ any idea?\r\n\r\n# sagemaker config\r\ninstance_type = \"ml.g5.2xlarge\"\
          \r\nnumber_of_gpu = 1\r\nhealth_check_timeout = 300\r\n\r\n# Define Model\
          \ and Endpoint configuration parameter\r\nconfig = {\r\n  'HF_MODEL_ID':\
          \ \"togethercomputer/Llama-2-7B-32K-Instruct\", # model_id from hf.co/models\r\
          \n  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\r\
          \n  'MAX_INPUT_LENGTH': json.dumps(MAX_INPUT_LENGTH),  # Max length of input\
          \ text\r\n  'MAX_TOTAL_TOKENS': json.dumps(MAX_TOTAL_TOKENS),  # Max length\
          \ of the generation (including input text)\r\n  'MAX_BATCH_TOTAL_TOKENS':\
          \ json.dumps(MAX_BATCH_TOTAL_TOKEN),  # Limits the number of tokens that\
          \ can be processed in parallel during the generation\r\n  'HUGGING_FACE_HUB_TOKEN':\
          \ json.dumps(\"HF_TOKEN\")\r\n}\r\n\r\n# check if token is set\r\nassert\
          \ config['HUGGING_FACE_HUB_TOKEN'] != \"HF_TOKEN\", \"Please set your Hugging\
          \ Face Hub token\"\r\n\r\n# create HuggingFaceModel with the image uri\r\
          \nllm_model = HuggingFaceModel(\r\n  role=role,\r\n  image_uri=llm_image,\r\
          \n  env=config\r\n)"
        updatedAt: '2023-08-25T06:52:37.977Z'
      numEdits: 0
      reactions: []
    id: 64e84fb55856b887222455a2
    type: comment
  author: NABARKA
  content: "What should be the MAX_INPUT_LENGTH,MAX_TOTAL_TOKENS, MAX_BATCH_TOTAL_TOKENS\
    \ any idea?\r\n\r\n# sagemaker config\r\ninstance_type = \"ml.g5.2xlarge\"\r\n\
    number_of_gpu = 1\r\nhealth_check_timeout = 300\r\n\r\n# Define Model and Endpoint\
    \ configuration parameter\r\nconfig = {\r\n  'HF_MODEL_ID': \"togethercomputer/Llama-2-7B-32K-Instruct\"\
    , # model_id from hf.co/models\r\n  'SM_NUM_GPUS': json.dumps(number_of_gpu),\
    \ # Number of GPU used per replica\r\n  'MAX_INPUT_LENGTH': json.dumps(MAX_INPUT_LENGTH),\
    \  # Max length of input text\r\n  'MAX_TOTAL_TOKENS': json.dumps(MAX_TOTAL_TOKENS),\
    \  # Max length of the generation (including input text)\r\n  'MAX_BATCH_TOTAL_TOKENS':\
    \ json.dumps(MAX_BATCH_TOTAL_TOKEN),  # Limits the number of tokens that can be\
    \ processed in parallel during the generation\r\n  'HUGGING_FACE_HUB_TOKEN': json.dumps(\"\
    HF_TOKEN\")\r\n}\r\n\r\n# check if token is set\r\nassert config['HUGGING_FACE_HUB_TOKEN']\
    \ != \"HF_TOKEN\", \"Please set your Hugging Face Hub token\"\r\n\r\n# create\
    \ HuggingFaceModel with the image uri\r\nllm_model = HuggingFaceModel(\r\n  role=role,\r\
    \n  image_uri=llm_image,\r\n  env=config\r\n)"
  created_at: 2023-08-25 05:52:37+00:00
  edited: false
  hidden: false
  id: 64e84fb55856b887222455a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-08-28T06:47:56.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9138275384902954
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;NABARKA&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/NABARKA\">@<span class=\"\
          underline\">NABARKA</span></a></span>\n\n\t</span></span> ,</p>\n<p>This\
          \ model has been trained to handle context length up to 32k, so I would\
          \ recommend setting <code>MAX_INPUT_LENGTH</code> to at most 32K. The <code>MAX_TOTAL_TOKENS</code>\
          \ parameter also depends on your application, i.e., how long you want the\
          \ model answers to be (e.g., if you are interested in summarization or QA,\
          \ you can set it to something below 512). The <code>MAX_BATCH_TOTAL_TOKEN</code>\
          \ is also affected by your hardware (with more memory you can handle larger\
          \ batches). I don't know whether Sagemaker itself has limitations on these\
          \ parameters though.</p>\n<p>Let us know how it goes!:)</p>\n"
        raw: 'Hi @NABARKA ,


          This model has been trained to handle context length up to 32k, so I would
          recommend setting `MAX_INPUT_LENGTH` to at most 32K. The `MAX_TOTAL_TOKENS`
          parameter also depends on your application, i.e., how long you want the
          model answers to be (e.g., if you are interested in summarization or QA,
          you can set it to something below 512). The `MAX_BATCH_TOTAL_TOKEN` is also
          affected by your hardware (with more memory you can handle larger batches).
          I don''t know whether Sagemaker itself has limitations on these parameters
          though.


          Let us know how it goes!:)'
        updatedAt: '2023-08-28T06:47:56.099Z'
      numEdits: 0
      reactions: []
    id: 64ec431cc782d648d28bef48
    type: comment
  author: mauriceweber
  content: 'Hi @NABARKA ,


    This model has been trained to handle context length up to 32k, so I would recommend
    setting `MAX_INPUT_LENGTH` to at most 32K. The `MAX_TOTAL_TOKENS` parameter also
    depends on your application, i.e., how long you want the model answers to be (e.g.,
    if you are interested in summarization or QA, you can set it to something below
    512). The `MAX_BATCH_TOTAL_TOKEN` is also affected by your hardware (with more
    memory you can handle larger batches). I don''t know whether Sagemaker itself
    has limitations on these parameters though.


    Let us know how it goes!:)'
  created_at: 2023-08-28 05:47:56+00:00
  edited: false
  hidden: false
  id: 64ec431cc782d648d28bef48
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: togethercomputer/LLaMA-2-7B-32K
repo_type: model
status: open
target_branch: null
title: ENDPOINT CONFIGURATION ON AWS SAGEMAKER
