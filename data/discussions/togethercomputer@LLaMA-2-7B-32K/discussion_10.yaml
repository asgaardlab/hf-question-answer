!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alyssavance
conflicting_files: null
created_at: 2023-08-01 17:45:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2010c1c79bcfae824a908d10d969943.svg
      fullname: Alyssa Vance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alyssavance
      type: user
    createdAt: '2023-08-01T18:45:47.000Z'
    data:
      edited: false
      editors:
      - alyssavance
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8834854960441589
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2010c1c79bcfae824a908d10d969943.svg
          fullname: Alyssa Vance
          isHf: false
          isPro: false
          name: alyssavance
          type: user
        html: '<p>Posted the issue here, but happy to discuss further if anyone can
          help. The divergence happens after ~20 steps/six hours. Thanks</p>

          <p><a rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/issues/663">https://github.com/TimDettmers/bitsandbytes/issues/663</a></p>

          '
        raw: "Posted the issue here, but happy to discuss further if anyone can help.\
          \ The divergence happens after ~20 steps/six hours. Thanks\r\n\r\nhttps://github.com/TimDettmers/bitsandbytes/issues/663"
        updatedAt: '2023-08-01T18:45:47.454Z'
      numEdits: 0
      reactions: []
    id: 64c952dbd45e142a0ecb45c3
    type: comment
  author: alyssavance
  content: "Posted the issue here, but happy to discuss further if anyone can help.\
    \ The divergence happens after ~20 steps/six hours. Thanks\r\n\r\nhttps://github.com/TimDettmers/bitsandbytes/issues/663"
  created_at: 2023-08-01 17:45:47+00:00
  edited: false
  hidden: false
  id: 64c952dbd45e142a0ecb45c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
      fullname: Gardner Bickford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gardner
      type: user
    createdAt: '2023-08-01T20:14:10.000Z'
    data:
      edited: false
      editors:
      - gardner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.843042254447937
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638581711769b7c4b10f0523/v95VFXJqueQ_pp9ZURu4R.jpeg?w=200&h=200&f=face
          fullname: Gardner Bickford
          isHf: false
          isPro: false
          name: gardner
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;alyssavance&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/alyssavance\"\
          >@<span class=\"underline\">alyssavance</span></a></span>\n\n\t</span></span>\
          \ , have you read this? <a href=\"https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/2\"\
          >https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/2</a></p>\n\
          <blockquote>\n<p>since you are doing QLoRA, you might need to set trust_remote_code=False\
          \ to use HF's llama implementation, flash attention only works for float16\
          \ or bfloat16.</p>\n</blockquote>\n"
        raw: 'Hi @alyssavance , have you read this? https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/2


          > since you are doing QLoRA, you might need to set trust_remote_code=False
          to use HF''s llama implementation, flash attention only works for float16
          or bfloat16.


          '
        updatedAt: '2023-08-01T20:14:10.076Z'
      numEdits: 0
      reactions: []
    id: 64c967921c25d2c581c2f36c
    type: comment
  author: gardner
  content: 'Hi @alyssavance , have you read this? https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/2


    > since you are doing QLoRA, you might need to set trust_remote_code=False to
    use HF''s llama implementation, flash attention only works for float16 or bfloat16.


    '
  created_at: 2023-08-01 19:14:10+00:00
  edited: false
  hidden: false
  id: 64c967921c25d2c581c2f36c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2010c1c79bcfae824a908d10d969943.svg
      fullname: Alyssa Vance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alyssavance
      type: user
    createdAt: '2023-08-01T20:21:48.000Z'
    data:
      edited: false
      editors:
      - alyssavance
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.968032717704773
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2010c1c79bcfae824a908d10d969943.svg
          fullname: Alyssa Vance
          isHf: false
          isPro: false
          name: alyssavance
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gardner&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gardner\">@<span class=\"\
          underline\">gardner</span></a></span>\n\n\t</span></span> I did, I had some\
          \ type problems but fixed them by removing the JIT decorator from rmsnorm.\
          \ Right now it runs with no type errors, it does inference fine, it just\
          \ gradually diverges after the first few dozen steps.</p>\n"
        raw: '@gardner I did, I had some type problems but fixed them by removing
          the JIT decorator from rmsnorm. Right now it runs with no type errors, it
          does inference fine, it just gradually diverges after the first few dozen
          steps.'
        updatedAt: '2023-08-01T20:21:48.868Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - gardner
    id: 64c9695c904317f42dd27bde
    type: comment
  author: alyssavance
  content: '@gardner I did, I had some type problems but fixed them by removing the
    JIT decorator from rmsnorm. Right now it runs with no type errors, it does inference
    fine, it just gradually diverges after the first few dozen steps.'
  created_at: 2023-08-01 19:21:48+00:00
  edited: false
  hidden: false
  id: 64c9695c904317f42dd27bde
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-08-02T09:44:15.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8664584159851074
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;alyssavance&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/alyssavance\"\
          >@<span class=\"underline\">alyssavance</span></a></span>\n\n\t</span></span>\
          \ , did you try a smaller learning rate? Instead of 1e-4, it might be worth\
          \ to try out 2e-5 (same as in the <a rel=\"nofollow\" href=\"https://arxiv.org/pdf/2306.15595.pdf\"\
          >linear interpolation paper</a>).</p>\n"
        raw: Hi @alyssavance , did you try a smaller learning rate? Instead of 1e-4,
          it might be worth to try out 2e-5 (same as in the [linear interpolation
          paper](https://arxiv.org/pdf/2306.15595.pdf)).
        updatedAt: '2023-08-02T09:44:15.950Z'
      numEdits: 0
      reactions: []
    id: 64ca256f1af278541d3a39c1
    type: comment
  author: mauriceweber
  content: Hi @alyssavance , did you try a smaller learning rate? Instead of 1e-4,
    it might be worth to try out 2e-5 (same as in the [linear interpolation paper](https://arxiv.org/pdf/2306.15595.pdf)).
  created_at: 2023-08-02 08:44:15+00:00
  edited: false
  hidden: false
  id: 64ca256f1af278541d3a39c1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: togethercomputer/LLaMA-2-7B-32K
repo_type: model
status: open
target_branch: null
title: 'Training diverges when used with Llama 2 70B and 4-bit QLoRA '
