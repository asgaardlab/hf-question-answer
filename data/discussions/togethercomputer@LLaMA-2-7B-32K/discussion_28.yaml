!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ajash
conflicting_files: null
created_at: 2023-09-16 20:21:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
      fullname: Ambarish Jash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajash
      type: user
    createdAt: '2023-09-16T21:21:01.000Z'
    data:
      edited: false
      editors:
      - ajash
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4354020059108734
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
          fullname: Ambarish Jash
          isHf: false
          isPro: false
          name: ajash
          type: user
        html: '<p>I have installed flash attention &amp; rotary emb:<br>pip install
          flash-attn==2.1.1 --no-build-isolation<br>pip install git+<a rel="nofollow"
          href="https://github.com/HazyResearch/flash-attention.git@v2.1.1#subdirectory=csrc/rotary">https://github.com/HazyResearch/flash-attention.git@v2.1.1#subdirectory=csrc/rotary</a></p>

          <p>The machine has 4 A100 gpus. While using the accelerate API I get the
          following error:</p>

          <p>Traceback (most recent call last):<br>  File "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 38, in <br>    from flash_attn.flash_attn_interface import (<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/<strong>init</strong>.py",
          line 3, in <br>    from flash_attn.flash_attn_interface import (<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py",
          line 7, in <br>    import flash_attn_2_cuda as flash_attn_cuda<br>ImportError:
          libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 76, in <br>    run_model()<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 66, in run_model<br>    model = rv_model.AmznCausalLMTrainingMultiGPU(<br>  File
          "/home/paperspace/DigitalSynapse/models/reviews_model.py", line 24, in <strong>init</strong><br>    model
          = AutoModelForCausalLM.from_pretrained(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py",
          line 480, in from_pretrained<br>    model_class = get_class_from_dynamic_module(<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py",
          line 443, in get_class_from_dynamic_module<br>    return get_class_in_module(class_name,
          final_module.replace(".py", ""))<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py",
          line 164, in get_class_in_module<br>    module = importlib.import_module(module_path)<br>  File
          "/usr/lib/python3.9/importlib/<strong>init</strong>.py", line 127, in import_module<br>    return
          _bootstrap._gcd_import(name[level:], package, level)<br>  File "", line
          1030, in _gcd_import<br>  File "", line 1007, in _find_and_load<br>  File
          "", line 986, in _find_and_load_unlocked<br>  File "", line 680, in _load_unlocked<br>  File
          "", line 850, in exec_module<br>  File "", line 228, in _call_with_frames_removed<br>  File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 49, in <br>    raise ImportError(''Please install Flash Attention:
          <code>pip install flash-attn --no-build-isolation</code>'')<br>ImportError:
          Please install Flash Attention: <code>pip install flash-attn --no-build-isolation</code><br>Traceback
          (most recent call last):<br>  File "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 38, in <br>    from flash_attn.flash_attn_interface import (<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/<strong>init</strong>.py",
          line 3, in <br>    from flash_attn.flash_attn_interface import (<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py",
          line 7, in <br>    import flash_attn_2_cuda as flash_attn_cuda<br>ImportError:
          libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 76, in <br>    run_model()<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 66, in run_model<br>    model = rv_model.AmznCausalLMTrainingMultiGPU(<br>  File
          "/home/paperspace/DigitalSynapse/models/reviews_model.py", line 24, in <strong>init</strong><br>    model
          = AutoModelForCausalLM.from_pretrained(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py",
          line 480, in from_pretrained<br>    model_class = get_class_from_dynamic_module(<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py",
          line 443, in get_class_from_dynamic_module<br>    return get_class_in_module(class_name,
          final_module.replace(".py", ""))<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py",
          line 164, in get_class_in_module<br>    module = importlib.import_module(module_path)<br>  File
          "/usr/lib/python3.9/importlib/<strong>init</strong>.py", line 127, in import_module<br>    return
          _bootstrap._gcd_import(name[level:], package, level)<br>  File "", line
          1030, in _gcd_import<br>  File "", line 1007, in _find_and_load<br>  File
          "", line 986, in _find_and_load_unlocked<br>  File "", line 680, in _load_unlocked<br>  File
          "", line 850, in exec_module<br>  File "", line 228, in _call_with_frames_removed<br>  File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 49, in <br>    raise ImportError(''Please install Flash Attention:
          <code>pip install flash-attn --no-build-isolation</code>'')<br>ImportError:
          Please install Flash Attention: <code>pip install flash-attn --no-build-isolation</code><br>Traceback
          (most recent call last):<br>  File "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 38, in <br>    from flash_attn.flash_attn_interface import (<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/<strong>init</strong>.py",
          line 3, in <br>    from flash_attn.flash_attn_interface import (<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py",
          line 7, in <br>    import flash_attn_2_cuda as flash_attn_cuda<br>ImportError:
          libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 76, in <br>    run_model()<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 66, in run_model<br>    model = rv_model.AmznCausalLMTrainingMultiGPU(<br>  File
          "/home/paperspace/DigitalSynapse/models/reviews_model.py", line 24, in <strong>init</strong><br>    model
          = AutoModelForCausalLM.from_pretrained(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py",
          line 480, in from_pretrained<br>    model_class = get_class_from_dynamic_module(<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py",
          line 443, in get_class_from_dynamic_module<br>    return get_class_in_module(class_name,
          final_module.replace(".py", ""))<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py",
          line 164, in get_class_in_module<br>    module = importlib.import_module(module_path)<br>  File
          "/usr/lib/python3.9/importlib/<strong>init</strong>.py", line 127, in import_module<br>    return
          _bootstrap._gcd_import(name[level:], package, level)<br>  File "", line
          1030, in _gcd_import<br>  File "", line 1007, in _find_and_load<br>  File
          "", line 986, in _find_and_load_unlocked<br>  File "", line 680, in _load_unlocked<br>  File
          "", line 850, in exec_module<br>  File "", line 228, in _call_with_frames_removed<br>  File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 49, in <br>    raise ImportError(''Please install Flash Attention:
          <code>pip install flash-attn --no-build-isolation</code>'')<br>ImportError:
          Please install Flash Attention: <code>pip install flash-attn --no-build-isolation</code><br>Traceback
          (most recent call last):<br>  File "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 38, in <br>    from flash_attn.flash_attn_interface import (<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/<strong>init</strong>.py",
          line 3, in <br>    from flash_attn.flash_attn_interface import (<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py",
          line 7, in <br>    import flash_attn_2_cuda as flash_attn_cuda<br>ImportError:
          libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 76, in <br>    run_model()<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 66, in run_model<br>    model = rv_model.AmznCausalLMTrainingMultiGPU(<br>  File
          "/home/paperspace/DigitalSynapse/models/reviews_model.py", line 24, in <strong>init</strong><br>    model
          = AutoModelForCausalLM.from_pretrained(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py",
          line 480, in from_pretrained<br>    model_class = get_class_from_dynamic_module(<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py",
          line 443, in get_class_from_dynamic_module<br>    return get_class_in_module(class_name,
          final_module.replace(".py", ""))<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py",
          line 164, in get_class_in_module<br>    module = importlib.import_module(module_path)<br>  File
          "/usr/lib/python3.9/importlib/<strong>init</strong>.py", line 127, in import_module<br>    return
          _bootstrap._gcd_import(name[level:], package, level)<br>  File "", line
          1030, in _gcd_import<br>  File "", line 1007, in _find_and_load<br>  File
          "", line 986, in _find_and_load_unlocked<br>  File "", line 680, in _load_unlocked<br>  File
          "", line 850, in exec_module<br>  File "", line 228, in _call_with_frames_removed<br>  File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 49, in <br>    raise ImportError(''Please install Flash Attention:
          <code>pip install flash-attn --no-build-isolation</code>'')<br>ImportError:
          Please install Flash Attention: <code>pip install flash-attn --no-build-isolation</code><br>ERROR:torch.distributed.elastic.multiprocessing.api:failed
          (exitcode: 1) local_rank: 0 (pid: 11162) of binary: /usr/bin/python3.9<br>Traceback
          (most recent call last):<br>  File "/home/paperspace/.local/bin/accelerate",
          line 8, in <br>    sys.exit(main())<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py",
          line 45, in main<br>    args.func(args)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/launch.py",
          line 970, in launch_command<br>    multi_gpu_launcher(args)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/launch.py",
          line 646, in multi_gpu_launcher<br>    distrib_run.run(args)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/run.py",
          line 785, in run<br>    elastic_launch(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py",
          line 134, in <strong>call</strong><br>    return launch_agent(self._config,
          self._entrypoint, list(args))<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py",
          line 250, in launch_agent<br>    raise ChildFailedError(<br>torch.distributed.elastic.multiprocessing.errors.ChildFailedError:</p>

          '
        raw: "I have installed flash attention & rotary emb:\r\npip install flash-attn==2.1.1\
          \ --no-build-isolation\r\npip install git+https://github.com/HazyResearch/flash-attention.git@v2.1.1#subdirectory=csrc/rotary\r\
          \n\r\nThe machine has 4 A100 gpus. While using the accelerate API I get\
          \ the following error:\r\n\r\nTraceback (most recent call last):\r\n  File\
          \ \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 38, in <module>\r\n    from flash_attn.flash_attn_interface import\
          \ (\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/__init__.py\"\
          , line 3, in <module>\r\n    from flash_attn.flash_attn_interface import\
          \ (\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py\"\
          , line 7, in <module>\r\n    import flash_attn_2_cuda as flash_attn_cuda\r\
          \nImportError: libtorch_cuda_cpp.so: cannot open shared object file: No\
          \ such file or directory\r\n\r\nDuring handling of the above exception,\
          \ another exception occurred:\r\n\r\nTraceback (most recent call last):\r\
          \n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\", line\
          \ 76, in <module>\r\n    run_model()\r\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
          , line 66, in run_model\r\n    model = rv_model.AmznCausalLMTrainingMultiGPU(\r\
          \n  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line\
          \ 24, in __init__\r\n    model = AutoModelForCausalLM.from_pretrained(\r\
          \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 480, in from_pretrained\r\n    model_class = get_class_from_dynamic_module(\r\
          \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
          , line 443, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
          \ final_module.replace(\".py\", \"\"))\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
          , line 164, in get_class_in_module\r\n    module = importlib.import_module(module_path)\r\
          \n  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\
          \n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\r\
          \n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\r\
          \n  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\r\
          \n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\
          \n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 49, in <module>\r\n    raise ImportError('Please install Flash Attention:\
          \ `pip install flash-attn --no-build-isolation`')\r\nImportError: Please\
          \ install Flash Attention: `pip install flash-attn --no-build-isolation`\r\
          \nTraceback (most recent call last):\r\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 38, in <module>\r\n    from flash_attn.flash_attn_interface import\
          \ (\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/__init__.py\"\
          , line 3, in <module>\r\n    from flash_attn.flash_attn_interface import\
          \ (\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py\"\
          , line 7, in <module>\r\n    import flash_attn_2_cuda as flash_attn_cuda\r\
          \nImportError: libtorch_cuda_cpp.so: cannot open shared object file: No\
          \ such file or directory\r\n\r\nDuring handling of the above exception,\
          \ another exception occurred:\r\n\r\nTraceback (most recent call last):\r\
          \n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\", line\
          \ 76, in <module>\r\n    run_model()\r\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
          , line 66, in run_model\r\n    model = rv_model.AmznCausalLMTrainingMultiGPU(\r\
          \n  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line\
          \ 24, in __init__\r\n    model = AutoModelForCausalLM.from_pretrained(\r\
          \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 480, in from_pretrained\r\n    model_class = get_class_from_dynamic_module(\r\
          \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
          , line 443, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
          \ final_module.replace(\".py\", \"\"))\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
          , line 164, in get_class_in_module\r\n    module = importlib.import_module(module_path)\r\
          \n  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\
          \n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\r\
          \n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\r\
          \n  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\r\
          \n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\
          \n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 49, in <module>\r\n    raise ImportError('Please install Flash Attention:\
          \ `pip install flash-attn --no-build-isolation`')\r\nImportError: Please\
          \ install Flash Attention: `pip install flash-attn --no-build-isolation`\r\
          \nTraceback (most recent call last):\r\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 38, in <module>\r\n    from flash_attn.flash_attn_interface import\
          \ (\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/__init__.py\"\
          , line 3, in <module>\r\n    from flash_attn.flash_attn_interface import\
          \ (\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py\"\
          , line 7, in <module>\r\n    import flash_attn_2_cuda as flash_attn_cuda\r\
          \nImportError: libtorch_cuda_cpp.so: cannot open shared object file: No\
          \ such file or directory\r\n\r\nDuring handling of the above exception,\
          \ another exception occurred:\r\n\r\nTraceback (most recent call last):\r\
          \n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\", line\
          \ 76, in <module>\r\n    run_model()\r\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
          , line 66, in run_model\r\n    model = rv_model.AmznCausalLMTrainingMultiGPU(\r\
          \n  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line\
          \ 24, in __init__\r\n    model = AutoModelForCausalLM.from_pretrained(\r\
          \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 480, in from_pretrained\r\n    model_class = get_class_from_dynamic_module(\r\
          \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
          , line 443, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
          \ final_module.replace(\".py\", \"\"))\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
          , line 164, in get_class_in_module\r\n    module = importlib.import_module(module_path)\r\
          \n  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\
          \n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\r\
          \n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\r\
          \n  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\r\
          \n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\
          \n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 49, in <module>\r\n    raise ImportError('Please install Flash Attention:\
          \ `pip install flash-attn --no-build-isolation`')\r\nImportError: Please\
          \ install Flash Attention: `pip install flash-attn --no-build-isolation`\r\
          \nTraceback (most recent call last):\r\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 38, in <module>\r\n    from flash_attn.flash_attn_interface import\
          \ (\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/__init__.py\"\
          , line 3, in <module>\r\n    from flash_attn.flash_attn_interface import\
          \ (\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py\"\
          , line 7, in <module>\r\n    import flash_attn_2_cuda as flash_attn_cuda\r\
          \nImportError: libtorch_cuda_cpp.so: cannot open shared object file: No\
          \ such file or directory\r\n\r\nDuring handling of the above exception,\
          \ another exception occurred:\r\n\r\nTraceback (most recent call last):\r\
          \n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\", line\
          \ 76, in <module>\r\n    run_model()\r\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
          , line 66, in run_model\r\n    model = rv_model.AmznCausalLMTrainingMultiGPU(\r\
          \n  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line\
          \ 24, in __init__\r\n    model = AutoModelForCausalLM.from_pretrained(\r\
          \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 480, in from_pretrained\r\n    model_class = get_class_from_dynamic_module(\r\
          \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
          , line 443, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
          \ final_module.replace(\".py\", \"\"))\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
          , line 164, in get_class_in_module\r\n    module = importlib.import_module(module_path)\r\
          \n  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\
          \n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File\
          \ \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\r\
          \n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\r\
          \n  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\r\
          \n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\
          \n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 49, in <module>\r\n    raise ImportError('Please install Flash Attention:\
          \ `pip install flash-attn --no-build-isolation`')\r\nImportError: Please\
          \ install Flash Attention: `pip install flash-attn --no-build-isolation`\r\
          \nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode:\
          \ 1) local_rank: 0 (pid: 11162) of binary: /usr/bin/python3.9\r\nTraceback\
          \ (most recent call last):\r\n  File \"/home/paperspace/.local/bin/accelerate\"\
          , line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\"\
          , line 45, in main\r\n    args.func(args)\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\"\
          , line 970, in launch_command\r\n    multi_gpu_launcher(args)\r\n  File\
          \ \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\"\
          , line 646, in multi_gpu_launcher\r\n    distrib_run.run(args)\r\n  File\
          \ \"/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/run.py\"\
          , line 785, in run\r\n    elastic_launch(\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py\"\
          , line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint,\
          \ list(args))\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py\"\
          , line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:"
        updatedAt: '2023-09-16T21:21:01.169Z'
      numEdits: 0
      reactions: []
    id: 65061c3d04d04d653dd29f3a
    type: comment
  author: ajash
  content: "I have installed flash attention & rotary emb:\r\npip install flash-attn==2.1.1\
    \ --no-build-isolation\r\npip install git+https://github.com/HazyResearch/flash-attention.git@v2.1.1#subdirectory=csrc/rotary\r\
    \n\r\nThe machine has 4 A100 gpus. While using the accelerate API I get the following\
    \ error:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 38, in <module>\r\n    from flash_attn.flash_attn_interface import (\r\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/__init__.py\"\
    , line 3, in <module>\r\n    from flash_attn.flash_attn_interface import (\r\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py\"\
    , line 7, in <module>\r\n    import flash_attn_2_cuda as flash_attn_cuda\r\nImportError:\
    \ libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory\r\
    \n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\
    \nTraceback (most recent call last):\r\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 76, in <module>\r\n    run_model()\r\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 66, in run_model\r\n    model = rv_model.AmznCausalLMTrainingMultiGPU(\r\
    \n  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line 24,\
    \ in __init__\r\n    model = AutoModelForCausalLM.from_pretrained(\r\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 480, in from_pretrained\r\n    model_class = get_class_from_dynamic_module(\r\
    \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
    , line 443, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
    \ final_module.replace(\".py\", \"\"))\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
    , line 164, in get_class_in_module\r\n    module = importlib.import_module(module_path)\r\
    \n  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\
    \n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"\
    <frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File \"<frozen\
    \ importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\"\
    , line 986, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\"\
    , line 680, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\"\
    , line 850, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 228,\
    \ in _call_with_frames_removed\r\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 49, in <module>\r\n    raise ImportError('Please install Flash Attention:\
    \ `pip install flash-attn --no-build-isolation`')\r\nImportError: Please install\
    \ Flash Attention: `pip install flash-attn --no-build-isolation`\r\nTraceback\
    \ (most recent call last):\r\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 38, in <module>\r\n    from flash_attn.flash_attn_interface import (\r\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/__init__.py\"\
    , line 3, in <module>\r\n    from flash_attn.flash_attn_interface import (\r\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py\"\
    , line 7, in <module>\r\n    import flash_attn_2_cuda as flash_attn_cuda\r\nImportError:\
    \ libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory\r\
    \n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\
    \nTraceback (most recent call last):\r\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 76, in <module>\r\n    run_model()\r\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 66, in run_model\r\n    model = rv_model.AmznCausalLMTrainingMultiGPU(\r\
    \n  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line 24,\
    \ in __init__\r\n    model = AutoModelForCausalLM.from_pretrained(\r\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 480, in from_pretrained\r\n    model_class = get_class_from_dynamic_module(\r\
    \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
    , line 443, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
    \ final_module.replace(\".py\", \"\"))\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
    , line 164, in get_class_in_module\r\n    module = importlib.import_module(module_path)\r\
    \n  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\
    \n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"\
    <frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File \"<frozen\
    \ importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\"\
    , line 986, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\"\
    , line 680, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\"\
    , line 850, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 228,\
    \ in _call_with_frames_removed\r\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 49, in <module>\r\n    raise ImportError('Please install Flash Attention:\
    \ `pip install flash-attn --no-build-isolation`')\r\nImportError: Please install\
    \ Flash Attention: `pip install flash-attn --no-build-isolation`\r\nTraceback\
    \ (most recent call last):\r\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 38, in <module>\r\n    from flash_attn.flash_attn_interface import (\r\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/__init__.py\"\
    , line 3, in <module>\r\n    from flash_attn.flash_attn_interface import (\r\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py\"\
    , line 7, in <module>\r\n    import flash_attn_2_cuda as flash_attn_cuda\r\nImportError:\
    \ libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory\r\
    \n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\
    \nTraceback (most recent call last):\r\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 76, in <module>\r\n    run_model()\r\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 66, in run_model\r\n    model = rv_model.AmznCausalLMTrainingMultiGPU(\r\
    \n  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line 24,\
    \ in __init__\r\n    model = AutoModelForCausalLM.from_pretrained(\r\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 480, in from_pretrained\r\n    model_class = get_class_from_dynamic_module(\r\
    \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
    , line 443, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
    \ final_module.replace(\".py\", \"\"))\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
    , line 164, in get_class_in_module\r\n    module = importlib.import_module(module_path)\r\
    \n  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\
    \n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"\
    <frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File \"<frozen\
    \ importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\"\
    , line 986, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\"\
    , line 680, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\"\
    , line 850, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 228,\
    \ in _call_with_frames_removed\r\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 49, in <module>\r\n    raise ImportError('Please install Flash Attention:\
    \ `pip install flash-attn --no-build-isolation`')\r\nImportError: Please install\
    \ Flash Attention: `pip install flash-attn --no-build-isolation`\r\nTraceback\
    \ (most recent call last):\r\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 38, in <module>\r\n    from flash_attn.flash_attn_interface import (\r\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/__init__.py\"\
    , line 3, in <module>\r\n    from flash_attn.flash_attn_interface import (\r\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/flash_attn/flash_attn_interface.py\"\
    , line 7, in <module>\r\n    import flash_attn_2_cuda as flash_attn_cuda\r\nImportError:\
    \ libtorch_cuda_cpp.so: cannot open shared object file: No such file or directory\r\
    \n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\
    \nTraceback (most recent call last):\r\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 76, in <module>\r\n    run_model()\r\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 66, in run_model\r\n    model = rv_model.AmznCausalLMTrainingMultiGPU(\r\
    \n  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line 24,\
    \ in __init__\r\n    model = AutoModelForCausalLM.from_pretrained(\r\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 480, in from_pretrained\r\n    model_class = get_class_from_dynamic_module(\r\
    \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
    , line 443, in get_class_from_dynamic_module\r\n    return get_class_in_module(class_name,\
    \ final_module.replace(\".py\", \"\"))\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/transformers/dynamic_module_utils.py\"\
    , line 164, in get_class_in_module\r\n    module = importlib.import_module(module_path)\r\
    \n  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\
    \n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"\
    <frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File \"<frozen\
    \ importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\"\
    , line 986, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\"\
    , line 680, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\"\
    , line 850, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 228,\
    \ in _call_with_frames_removed\r\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 49, in <module>\r\n    raise ImportError('Please install Flash Attention:\
    \ `pip install flash-attn --no-build-isolation`')\r\nImportError: Please install\
    \ Flash Attention: `pip install flash-attn --no-build-isolation`\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed\
    \ (exitcode: 1) local_rank: 0 (pid: 11162) of binary: /usr/bin/python3.9\r\nTraceback\
    \ (most recent call last):\r\n  File \"/home/paperspace/.local/bin/accelerate\"\
    , line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\"\
    , line 45, in main\r\n    args.func(args)\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\"\
    , line 970, in launch_command\r\n    multi_gpu_launcher(args)\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\"\
    , line 646, in multi_gpu_launcher\r\n    distrib_run.run(args)\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/run.py\"\
    , line 785, in run\r\n    elastic_launch(\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py\"\
    , line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint,\
    \ list(args))\r\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py\"\
    , line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:"
  created_at: 2023-09-16 20:21:01+00:00
  edited: false
  hidden: false
  id: 65061c3d04d04d653dd29f3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
      fullname: Ambarish Jash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajash
      type: user
    createdAt: '2023-09-16T21:21:16.000Z'
    data:
      from: Using Accelerate API to train the model.
      to: Using Accelerate API to train model multiple GPUs
    id: 65061c4cdacc94cd6cf6f2ca
    type: title-change
  author: ajash
  created_at: 2023-09-16 20:21:16+00:00
  id: 65061c4cdacc94cd6cf6f2ca
  new_title: Using Accelerate API to train model multiple GPUs
  old_title: Using Accelerate API to train the model.
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
      fullname: Ambarish Jash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajash
      type: user
    createdAt: '2023-09-16T21:21:55.000Z'
    data:
      from: Using Accelerate API to train model multiple GPUs
      to: Using the Accelerate API to train models on multiple GPUs
    id: 65061c73bf91e72fa951bc9a
    type: title-change
  author: ajash
  created_at: 2023-09-16 20:21:55+00:00
  id: 65061c73bf91e72fa951bc9a
  new_title: Using the Accelerate API to train models on multiple GPUs
  old_title: Using Accelerate API to train model multiple GPUs
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-09-21T11:40:09.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9180379509925842
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ajash&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ajash\">@<span class=\"\
          underline\">ajash</span></a></span>\n\n\t</span></span>, it looks like this\
          \ is an issue arising from incompatible versions of pytorch / cuda / flash_attn.\
          \ Can you provide more details about your setup? what versions do you have\
          \ installed?</p>\n"
        raw: 'Hi @ajash, it looks like this is an issue arising from incompatible
          versions of pytorch / cuda / flash_attn. Can you provide more details about
          your setup? what versions do you have installed?


          '
        updatedAt: '2023-09-21T11:40:09.377Z'
      numEdits: 0
      reactions: []
    id: 650c2b9934abfe2dc3624b64
    type: comment
  author: mauriceweber
  content: 'Hi @ajash, it looks like this is an issue arising from incompatible versions
    of pytorch / cuda / flash_attn. Can you provide more details about your setup?
    what versions do you have installed?


    '
  created_at: 2023-09-21 10:40:09+00:00
  edited: false
  hidden: false
  id: 650c2b9934abfe2dc3624b64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
      fullname: Ambarish Jash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajash
      type: user
    createdAt: '2023-09-26T03:50:28.000Z'
    data:
      edited: false
      editors:
      - ajash
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5336654186248779
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
          fullname: Ambarish Jash
          isHf: false
          isPro: false
          name: ajash
          type: user
        html: '<p>nvcc --version (cuda version)<br>nvcc: NVIDIA (R) Cuda compiler
          driver<br>Copyright (c) 2005-2022 NVIDIA Corporation<br>Built on Wed_Jun__8_16:49:14_PDT_2022<br>Cuda
          compilation tools, release 11.7, V11.7.99<br>Build cuda_11.7.r11.7/compiler.31442593_0</p>

          <p>python3 -c "import torch; print(torch.<strong>version</strong>)".   (pytorch
          version)<br>2.0.1+cu117</p>

          <p>pip freeze | grep flash-attn<br>flash-attn==2.1.1<br>flash attention
          was installed with:   pip install flash-attn --no-build-isolation</p>

          '
        raw: 'nvcc --version (cuda version)

          nvcc: NVIDIA (R) Cuda compiler driver

          Copyright (c) 2005-2022 NVIDIA Corporation

          Built on Wed_Jun__8_16:49:14_PDT_2022

          Cuda compilation tools, release 11.7, V11.7.99

          Build cuda_11.7.r11.7/compiler.31442593_0


          python3 -c "import torch; print(torch.__version__)".   (pytorch version)

          2.0.1+cu117


          pip freeze | grep flash-attn

          flash-attn==2.1.1

          flash attention was installed with:   pip install flash-attn --no-build-isolation

          '
        updatedAt: '2023-09-26T03:50:28.891Z'
      numEdits: 0
      reactions: []
    id: 6512550444ab66c44ff96cad
    type: comment
  author: ajash
  content: 'nvcc --version (cuda version)

    nvcc: NVIDIA (R) Cuda compiler driver

    Copyright (c) 2005-2022 NVIDIA Corporation

    Built on Wed_Jun__8_16:49:14_PDT_2022

    Cuda compilation tools, release 11.7, V11.7.99

    Build cuda_11.7.r11.7/compiler.31442593_0


    python3 -c "import torch; print(torch.__version__)".   (pytorch version)

    2.0.1+cu117


    pip freeze | grep flash-attn

    flash-attn==2.1.1

    flash attention was installed with:   pip install flash-attn --no-build-isolation

    '
  created_at: 2023-09-26 02:50:28+00:00
  edited: false
  hidden: false
  id: 6512550444ab66c44ff96cad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-09-27T15:10:59.000Z'
    data:
      edited: false
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8652645349502563
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: '<p>ok, the version seem to be compatible. It might be that installing
          flash-attn with <code>pip install ...</code> points to a different python
          environment since according to the stacktrace it can''t find the library.
          Can you try to install flash attention using <code>python3 -m pip install
          flash-attn --no-build-isolation</code> ?</p>

          '
        raw: ok, the version seem to be compatible. It might be that installing flash-attn
          with `pip install ...` points to a different python environment since according
          to the stacktrace it can't find the library. Can you try to install flash
          attention using `python3 -m pip install flash-attn --no-build-isolation`
          ?
        updatedAt: '2023-09-27T15:10:59.550Z'
      numEdits: 0
      reactions: []
    id: 65144603c3194ade8c62afcc
    type: comment
  author: mauriceweber
  content: ok, the version seem to be compatible. It might be that installing flash-attn
    with `pip install ...` points to a different python environment since according
    to the stacktrace it can't find the library. Can you try to install flash attention
    using `python3 -m pip install flash-attn --no-build-isolation` ?
  created_at: 2023-09-27 14:10:59+00:00
  edited: false
  hidden: false
  id: 65144603c3194ade8c62afcc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
      fullname: Ambarish Jash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajash
      type: user
    createdAt: '2023-09-28T03:50:42.000Z'
    data:
      edited: false
      editors:
      - ajash
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4886942505836487
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
          fullname: Ambarish Jash
          isHf: false
          isPro: false
          name: ajash
          type: user
        html: '<p>I uninstalled flsh-attn and then installed it back... that seemed
          to have worked. I am getting another error: RuntimeError: Expected all tensors
          to be on the same device, but found at least two devices, cuda:1 and cuda:0!</p>

          <p>I have printed out the entire stack trace here:</p>

          <p>Traceback (most recent call last):<br>Traceback (most recent call last):<br>  File
          "/home/paperspace/DigitalSynapse/models/run_models.py", line 81, in <br>  File
          "/home/paperspace/DigitalSynapse/models/run_models.py", line 81, in <br>        run_model()run_model()</p>

          <p>  File "/home/paperspace/DigitalSynapse/models/run_models.py", line 77,
          in run_model<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 77, in run_model<br>        model.train_model(gradient_accum_steps=args.batch_size,model.train_model(gradient_accum_steps=args.batch_size,</p>

          <p>  File "/home/paperspace/DigitalSynapse/models/reviews_model.py", line
          103, in train_model<br>  File "/home/paperspace/DigitalSynapse/models/reviews_model.py",
          line 103, in train_model<br>    outputs = model(**batch)<br>      File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>outputs = model(**batch)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py",
          line 1156, in forward<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py",
          line 1156, in forward<br>Traceback (most recent call last):<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 81, in <br>    output = self._run_ddp_forward(*inputs, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py",
          line 1110, in _run_ddp_forward<br>    output = self._run_ddp_forward(*inputs,
          **kwargs)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py",
          line 1110, in _run_ddp_forward<br>    run_model()<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 77, in run_model<br>    model.train_model(gradient_accum_steps=args.batch_size,<br>  File
          "/home/paperspace/DigitalSynapse/models/reviews_model.py", line 103, in
          train_model<br>    return module_to_run(*inputs[0], **kwargs[0])  # type:
          ignore[index]<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return module_to_run(*inputs[0], **kwargs[0])  #
          type: ignore[index]<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    outputs = model(**batch)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/peft/peft_model.py",
          line 922, in forward<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/peft/peft_model.py",
          line 922, in forward<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py",
          line 1156, in forward<br>    return self.base_model(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return self.base_model(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    output = self._run_ddp_forward(*inputs,
          **kwargs)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py",
          line 1110, in _run_ddp_forward<br>    return forward_call(*args, **kwargs)<br>      File
          "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py",
          line 165, in new_forward<br>return forward_call(*args, **kwargs)<br>      File
          "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py",
          line 165, in new_forward<br>return module_to_run(*inputs[0], **kwargs[0])  #
          type: ignore[index]<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    output = old_forward(*args, **kwargs)<br>      File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 812, in forward<br>output = old_forward(*args, **kwargs)<br>  File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 812, in forward<br>    outputs = self.model(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>        return forward_call(*args, **kwargs)outputs
          = self.model(</p>

          <p>  File "/home/paperspace/.local/lib/python3.9/site-packages/peft/peft_model.py",
          line 922, in forward<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>      File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 656, in forward<br>return self.base_model(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 656, in forward<br>    inputs_embeds = self.embed_tokens(input_ids)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1538, in _call_impl<br>    inputs_embeds = self.embed_tokens(input_ids)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1538, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py",
          line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 812, in forward<br>    result = forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py",
          line 165, in new_forward<br>    result = forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py",
          line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/sparse.py",
          line 162, in forward<br>    output = old_forward(*args, **kwargs)<br>      File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/sparse.py",
          line 162, in forward<br>outputs = self.model(<br>return F.embedding(  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl</p>

          <p>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/functional.py",
          line 2210, in embedding<br>    return F.embedding(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/functional.py",
          line 2210, in embedding<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 656, in forward<br>    return torch.embedding(weight, input, padding_idx,
          scale_grad_by_freq, sparse)<br>RuntimeError        : inputs_embeds = self.embed_tokens(input_ids)return
          torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)Expected
          all tensors to be on the same device, but found at least two devices, cuda:1
          and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)</p>

          <p>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1538, in _call_impl</p>

          <p>RuntimeError: Expected all tensors to be on the same device, but found
          at least two devices, cuda:3 and cuda:0! (when checking argument for argument
          index in method wrapper_CUDA__index_select)<br>    result = forward_call(*args,
          **kwargs)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py",
          line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/sparse.py",
          line 162, in forward<br>    return F.embedding(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/functional.py",
          line 2210, in embedding<br>    return torch.embedding(weight, input, padding_idx,
          scale_grad_by_freq, sparse)<br>RuntimeError: Expected all tensors to be
          on the same device, but found at least two devices, cuda:2 and cuda:0! (when
          checking argument for argument index in method wrapper_CUDA__index_select)<br>/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py:350:
          UserWarning: operator() profile_node %34 : int[] = prim::profile_ivalue(%32)<br>
          does not have profile information (Triggered internally at ../third_party/nvfuser/csrc/graph_fuser.cpp:104.)<br>  kv
          = repeat_kv(kv, self.num_key_value_groups)<br>Traceback (most recent call
          last):<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 81, in <br>    run_model()<br>  File "/home/paperspace/DigitalSynapse/models/run_models.py",
          line 77, in run_model<br>    model.train_model(gradient_accum_steps=args.batch_size,<br>  File
          "/home/paperspace/DigitalSynapse/models/reviews_model.py", line 103, in
          train_model<br>    outputs = model(**batch)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py",
          line 1156, in forward<br>    output = self._run_ddp_forward(*inputs, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py",
          line 1110, in _run_ddp_forward<br>    return module_to_run(*inputs[0], **kwargs[0])  #
          type: ignore[index]<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/peft/peft_model.py",
          line 922, in forward<br>    return self.base_model(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py",
          line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 812, in forward<br>    outputs = self.model(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 687, in forward<br>    layer_outputs = torch.utils.checkpoint.checkpoint(<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py",
          line 249, in checkpoint<br>    return CheckpointFunction.apply(function,
          preserve, *args)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/autograd/function.py",
          line 506, in apply<br>    return super().apply(*args, **kwargs)  # type:
          ignore[misc]<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py",
          line 107, in forward<br>    outputs = run_function(*args)<br>  File "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 683, in custom_forward<br>    return module(*inputs, output_attentions,
          None)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py",
          line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 444, in forward<br>    hidden_states = self.input_layernorm(hidden_states)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py",
          line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>  File
          "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 88, in forward<br>    return rmsnorm_func(hidden_states, self.weight,
          self.variance_epsilon)<br>  File "/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py",
          line 70, in rmsnorm_func<br>    hidden_states = hidden_states * torch.rsqrt(variance
          + variance_epsilon)<br>RuntimeError: Expected all tensors to be on the same
          device, but found at least two devices, cuda:1 and cuda:0!<br>ERROR:torch.distributed.elastic.multiprocessing.api:failed
          (exitcode: 1) local_rank: 0 (pid: 5654) of binary: /usr/bin/python3.9<br>Traceback
          (most recent call last):<br>  File "/home/paperspace/.local/bin/accelerate",
          line 8, in <br>    sys.exit(main())<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py",
          line 45, in main<br>    args.func(args)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/launch.py",
          line 970, in launch_command<br>    multi_gpu_launcher(args)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/launch.py",
          line 646, in multi_gpu_launcher<br>    distrib_run.run(args)<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/run.py",
          line 785, in run<br>    elastic_launch(<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py",
          line 134, in <strong>call</strong><br>    return launch_agent(self._config,
          self._entrypoint, list(args))<br>  File "/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py",
          line 250, in launch_agent<br>    raise ChildFailedError(<br>torch.distributed.elastic.multiprocessing.errors.ChildFailedError:<br>============================================================<br>run_models.py
          FAILED</p>

          <hr>

          <p>Failures:<br>[1]:<br>  time      : 2023-09-28_03:41:06<br>  host      :
          psvxc2krd<br>  rank      : 1 (local_rank: 1)<br>  exitcode  : 1 (pid: 5655)<br>  error_file:
          &lt;N/A&gt;<br>  traceback : To enable traceback see: <a rel="nofollow"
          href="https://pytorch.org/docs/stable/elastic/errors.html">https://pytorch.org/docs/stable/elastic/errors.html</a><br>[2]:<br>  time      :
          2023-09-28_03:41:06<br>  host      : psvxc2krd<br>  rank      : 2 (local_rank:
          2)<br>  exitcode  : 1 (pid: 5656)<br>  error_file: &lt;N/A&gt;<br>  traceback
          : To enable traceback see: <a rel="nofollow" href="https://pytorch.org/docs/stable/elastic/errors.html">https://pytorch.org/docs/stable/elastic/errors.html</a><br>[3]:<br>  time      :
          2023-09-28_03:41:06<br>  host      : psvxc2krd<br>  rank      : 3 (local_rank:
          3)<br>  exitcode  : 1 (pid: 5657)<br>  error_file: &lt;N/A&gt;<br>  traceback
          : To enable traceback see: <a rel="nofollow" href="https://pytorch.org/docs/stable/elastic/errors.html">https://pytorch.org/docs/stable/elastic/errors.html</a></p>

          <hr>

          <p>Root Cause (first observed failure):<br>[0]:<br>  time      : 2023-09-28_03:41:06<br>  host      :
          psvxc2krd<br>  rank      : 0 (local_rank: 0)<br>  exitcode  : 1 (pid: 5654)<br>  error_file:
          &lt;N/A&gt;<br>  traceback : To enable traceback see: <a rel="nofollow"
          href="https://pytorch.org/docs/stable/elastic/errors.html">https://pytorch.org/docs/stable/elastic/errors.html</a></p>

          '
        raw: "I uninstalled flsh-attn and then installed it back... that seemed to\
          \ have worked. I am getting another error: RuntimeError: Expected all tensors\
          \ to be on the same device, but found at least two devices, cuda:1 and cuda:0!\n\
          \nI have printed out the entire stack trace here:\n\nTraceback (most recent\
          \ call last):\nTraceback (most recent call last):\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
          , line 81, in <module>\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
          , line 81, in <module>\n        run_model()run_model()\n\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
          , line 77, in run_model\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
          , line 77, in run_model\n        model.train_model(gradient_accum_steps=args.batch_size,model.train_model(gradient_accum_steps=args.batch_size,\n\
          \n  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line\
          \ 103, in train_model\n  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\"\
          , line 103, in train_model\n    outputs = model(**batch)\n      File \"\
          /home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\noutputs = model(**batch)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1156, in forward\n    return forward_call(*args, **kwargs)\n  File\
          \ \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1156, in forward\nTraceback (most recent call last):\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
          , line 81, in <module>\n    output = self._run_ddp_forward(*inputs, **kwargs)\n\
          \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1110, in _run_ddp_forward\n    output = self._run_ddp_forward(*inputs,\
          \ **kwargs)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1110, in _run_ddp_forward\n    run_model()\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
          , line 77, in run_model\n    model.train_model(gradient_accum_steps=args.batch_size,\n\
          \  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line\
          \ 103, in train_model\n    return module_to_run(*inputs[0], **kwargs[0])\
          \  # type: ignore[index]\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return module_to_run(*inputs[0], **kwargs[0])\
          \  # type: ignore[index]\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    outputs = model(**batch)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/paperspace/.local/lib/python3.9/site-packages/peft/peft_model.py\"\
          , line 922, in forward\n    return forward_call(*args, **kwargs)\n  File\
          \ \"/home/paperspace/.local/lib/python3.9/site-packages/peft/peft_model.py\"\
          , line 922, in forward\n    return forward_call(*args, **kwargs)\n  File\
          \ \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1156, in forward\n    return self.base_model(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return self.base_model(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    output = self._run_ddp_forward(*inputs,\
          \ **kwargs)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1110, in _run_ddp_forward\n    return forward_call(*args, **kwargs)\n\
          \      File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\nreturn forward_call(*args, **kwargs)\n     \
          \ File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\nreturn module_to_run(*inputs[0], **kwargs[0])\
          \  # type: ignore[index]\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    output = old_forward(*args, **kwargs)\n\
          \      File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 812, in forward\noutput = old_forward(*args, **kwargs)\n  File \"\
          /home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 812, in forward\n    outputs = self.model(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n        return forward_call(*args, **kwargs)outputs\
          \ = self.model(\n\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/peft/peft_model.py\"\
          , line 922, in forward\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \     File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 656, in forward\nreturn self.base_model(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 656, in forward\n    inputs_embeds = self.embed_tokens(input_ids)\n\
          \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1538, in _call_impl\n    inputs_embeds = self.embed_tokens(input_ids)\n\
          \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1538, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 812, in forward\n    result = forward_call(*args, **kwargs)\n  File\
          \ \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    result = forward_call(*args, **kwargs)\n\
          \  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/sparse.py\"\
          , line 162, in forward\n    output = old_forward(*args, **kwargs)\n    \
          \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/sparse.py\"\
          , line 162, in forward\noutputs = self.model(\nreturn F.embedding(  File\
          \ \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/functional.py\"\
          , line 2210, in embedding\n    return F.embedding(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/functional.py\"\
          , line 2210, in embedding\n    return forward_call(*args, **kwargs)\n  File\
          \ \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 656, in forward\n    return torch.embedding(weight, input, padding_idx,\
          \ scale_grad_by_freq, sparse)\nRuntimeError        : inputs_embeds = self.embed_tokens(input_ids)return\
          \ torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)Expected\
          \ all tensors to be on the same device, but found at least two devices,\
          \ cuda:1 and cuda:0! (when checking argument for argument index in method\
          \ wrapper_CUDA__index_select)\n\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1538, in _call_impl\n\nRuntimeError: Expected all tensors to be on\
          \ the same device, but found at least two devices, cuda:3 and cuda:0! (when\
          \ checking argument for argument index in method wrapper_CUDA__index_select)\n\
          \    result = forward_call(*args, **kwargs)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/sparse.py\"\
          , line 162, in forward\n    return F.embedding(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/functional.py\"\
          , line 2210, in embedding\n    return torch.embedding(weight, input, padding_idx,\
          \ scale_grad_by_freq, sparse)\nRuntimeError: Expected all tensors to be\
          \ on the same device, but found at least two devices, cuda:2 and cuda:0!\
          \ (when checking argument for argument index in method wrapper_CUDA__index_select)\n\
          /home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py:350:\
          \ UserWarning: operator() profile_node %34 : int[] = prim::profile_ivalue(%32)\n\
          \ does not have profile information (Triggered internally at ../third_party/nvfuser/csrc/graph_fuser.cpp:104.)\n\
          \  kv = repeat_kv(kv, self.num_key_value_groups)\nTraceback (most recent\
          \ call last):\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
          , line 81, in <module>\n    run_model()\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
          , line 77, in run_model\n    model.train_model(gradient_accum_steps=args.batch_size,\n\
          \  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line\
          \ 103, in train_model\n    outputs = model(**batch)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1156, in forward\n    output = self._run_ddp_forward(*inputs, **kwargs)\n\
          \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
          , line 1110, in _run_ddp_forward\n    return module_to_run(*inputs[0], **kwargs[0])\
          \  # type: ignore[index]\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/paperspace/.local/lib/python3.9/site-packages/peft/peft_model.py\"\
          , line 922, in forward\n    return self.base_model(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 812, in forward\n    outputs = self.model(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 687, in forward\n    layer_outputs = torch.utils.checkpoint.checkpoint(\n\
          \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py\"\
          , line 249, in checkpoint\n    return CheckpointFunction.apply(function,\
          \ preserve, *args)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/autograd/function.py\"\
          , line 506, in apply\n    return super().apply(*args, **kwargs)  # type:\
          \ ignore[misc]\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py\"\
          , line 107, in forward\n    outputs = run_function(*args)\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 683, in custom_forward\n    return module(*inputs, output_attentions,\
          \ None)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 444, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n\
          \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 88, in forward\n    return rmsnorm_func(hidden_states, self.weight,\
          \ self.variance_epsilon)\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
          , line 70, in rmsnorm_func\n    hidden_states = hidden_states * torch.rsqrt(variance\
          \ + variance_epsilon)\nRuntimeError: Expected all tensors to be on the same\
          \ device, but found at least two devices, cuda:1 and cuda:0!\nERROR:torch.distributed.elastic.multiprocessing.api:failed\
          \ (exitcode: 1) local_rank: 0 (pid: 5654) of binary: /usr/bin/python3.9\n\
          Traceback (most recent call last):\n  File \"/home/paperspace/.local/bin/accelerate\"\
          , line 8, in <module>\n    sys.exit(main())\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\"\
          , line 45, in main\n    args.func(args)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\"\
          , line 970, in launch_command\n    multi_gpu_launcher(args)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\"\
          , line 646, in multi_gpu_launcher\n    distrib_run.run(args)\n  File \"\
          /home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/run.py\"\
          , line 785, in run\n    elastic_launch(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py\"\
          , line 134, in __call__\n    return launch_agent(self._config, self._entrypoint,\
          \ list(args))\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py\"\
          , line 250, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n\
          ============================================================\nrun_models.py\
          \ FAILED\n------------------------------------------------------------\n\
          Failures:\n[1]:\n  time      : 2023-09-28_03:41:06\n  host      : psvxc2krd\n\
          \  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 5655)\n  error_file:\
          \ <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n\
          [2]:\n  time      : 2023-09-28_03:41:06\n  host      : psvxc2krd\n  rank\
          \      : 2 (local_rank: 2)\n  exitcode  : 1 (pid: 5656)\n  error_file: <N/A>\n\
          \  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n\
          [3]:\n  time      : 2023-09-28_03:41:06\n  host      : psvxc2krd\n  rank\
          \      : 3 (local_rank: 3)\n  exitcode  : 1 (pid: 5657)\n  error_file: <N/A>\n\
          \  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n\
          ------------------------------------------------------------\nRoot Cause\
          \ (first observed failure):\n[0]:\n  time      : 2023-09-28_03:41:06\n \
          \ host      : psvxc2krd\n  rank      : 0 (local_rank: 0)\n  exitcode  :\
          \ 1 (pid: 5654)\n  error_file: <N/A>\n  traceback : To enable traceback\
          \ see: https://pytorch.org/docs/stable/elastic/errors.html"
        updatedAt: '2023-09-28T03:50:42.989Z'
      numEdits: 0
      reactions: []
    id: 6514f812023a480a7351dbfd
    type: comment
  author: ajash
  content: "I uninstalled flsh-attn and then installed it back... that seemed to have\
    \ worked. I am getting another error: RuntimeError: Expected all tensors to be\
    \ on the same device, but found at least two devices, cuda:1 and cuda:0!\n\nI\
    \ have printed out the entire stack trace here:\n\nTraceback (most recent call\
    \ last):\nTraceback (most recent call last):\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 81, in <module>\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 81, in <module>\n        run_model()run_model()\n\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 77, in run_model\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 77, in run_model\n        model.train_model(gradient_accum_steps=args.batch_size,model.train_model(gradient_accum_steps=args.batch_size,\n\
    \n  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line 103,\
    \ in train_model\n  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\"\
    , line 103, in train_model\n    outputs = model(**batch)\n      File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\noutputs = model(**batch)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1156, in forward\n    return forward_call(*args, **kwargs)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1156, in forward\nTraceback (most recent call last):\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 81, in <module>\n    output = self._run_ddp_forward(*inputs, **kwargs)\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1110, in _run_ddp_forward\n    output = self._run_ddp_forward(*inputs,\
    \ **kwargs)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1110, in _run_ddp_forward\n    run_model()\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 77, in run_model\n    model.train_model(gradient_accum_steps=args.batch_size,\n\
    \  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line 103,\
    \ in train_model\n    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return module_to_run(*inputs[0], **kwargs[0])\
    \  # type: ignore[index]\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    outputs = model(**batch)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/peft/peft_model.py\", line\
    \ 922, in forward\n    return forward_call(*args, **kwargs)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/peft/peft_model.py\"\
    , line 922, in forward\n    return forward_call(*args, **kwargs)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1156, in forward\n    return self.base_model(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return self.base_model(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    output = self._run_ddp_forward(*inputs, **kwargs)\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1110, in _run_ddp_forward\n    return forward_call(*args, **kwargs)\n \
    \     File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\nreturn forward_call(*args, **kwargs)\n      File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\", line\
    \ 165, in new_forward\nreturn module_to_run(*inputs[0], **kwargs[0])  # type:\
    \ ignore[index]\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    output = old_forward(*args, **kwargs)\n      File\
    \ \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 812, in forward\noutput = old_forward(*args, **kwargs)\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 812, in forward\n    outputs = self.model(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n        return forward_call(*args, **kwargs)outputs\
    \ = self.model(\n\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/peft/peft_model.py\"\
    , line 922, in forward\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n      File\
    \ \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 656, in forward\nreturn self.base_model(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 656, in forward\n    inputs_embeds = self.embed_tokens(input_ids)\n  File\
    \ \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1538, in _call_impl\n    inputs_embeds = self.embed_tokens(input_ids)\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1538, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\", line\
    \ 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 812, in forward\n    result = forward_call(*args, **kwargs)\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\", line\
    \ 165, in new_forward\n    result = forward_call(*args, **kwargs)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File\
    \ \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/sparse.py\"\
    , line 162, in forward\n    output = old_forward(*args, **kwargs)\n      File\
    \ \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/sparse.py\"\
    , line 162, in forward\noutputs = self.model(\nreturn F.embedding(  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/functional.py\"\
    , line 2210, in embedding\n    return F.embedding(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/functional.py\"\
    , line 2210, in embedding\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 656, in forward\n    return torch.embedding(weight, input, padding_idx,\
    \ scale_grad_by_freq, sparse)\nRuntimeError        : inputs_embeds = self.embed_tokens(input_ids)return\
    \ torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)Expected\
    \ all tensors to be on the same device, but found at least two devices, cuda:1\
    \ and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\n\
    \n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1538, in _call_impl\n\nRuntimeError: Expected all tensors to be on the\
    \ same device, but found at least two devices, cuda:3 and cuda:0! (when checking\
    \ argument for argument index in method wrapper_CUDA__index_select)\n    result\
    \ = forward_call(*args, **kwargs)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File\
    \ \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/sparse.py\"\
    , line 162, in forward\n    return F.embedding(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/functional.py\"\
    , line 2210, in embedding\n    return torch.embedding(weight, input, padding_idx,\
    \ scale_grad_by_freq, sparse)\nRuntimeError: Expected all tensors to be on the\
    \ same device, but found at least two devices, cuda:2 and cuda:0! (when checking\
    \ argument for argument index in method wrapper_CUDA__index_select)\n/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py:350:\
    \ UserWarning: operator() profile_node %34 : int[] = prim::profile_ivalue(%32)\n\
    \ does not have profile information (Triggered internally at ../third_party/nvfuser/csrc/graph_fuser.cpp:104.)\n\
    \  kv = repeat_kv(kv, self.num_key_value_groups)\nTraceback (most recent call\
    \ last):\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\", line\
    \ 81, in <module>\n    run_model()\n  File \"/home/paperspace/DigitalSynapse/models/run_models.py\"\
    , line 77, in run_model\n    model.train_model(gradient_accum_steps=args.batch_size,\n\
    \  File \"/home/paperspace/DigitalSynapse/models/reviews_model.py\", line 103,\
    \ in train_model\n    outputs = model(**batch)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1156, in forward\n    output = self._run_ddp_forward(*inputs, **kwargs)\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\"\
    , line 1110, in _run_ddp_forward\n    return module_to_run(*inputs[0], **kwargs[0])\
    \  # type: ignore[index]\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/peft/peft_model.py\", line\
    \ 922, in forward\n    return self.base_model(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\", line\
    \ 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 812, in forward\n    outputs = self.model(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 687, in forward\n    layer_outputs = torch.utils.checkpoint.checkpoint(\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py\"\
    , line 249, in checkpoint\n    return CheckpointFunction.apply(function, preserve,\
    \ *args)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/autograd/function.py\"\
    , line 506, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py\"\
    , line 107, in forward\n    outputs = run_function(*args)\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 683, in custom_forward\n    return module(*inputs, output_attentions, None)\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\", line\
    \ 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 444, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n\
    \  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /home/paperspace/.local/lib/python3.9/site-packages/accelerate/hooks.py\", line\
    \ 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 88, in forward\n    return rmsnorm_func(hidden_states, self.weight, self.variance_epsilon)\n\
    \  File \"/home/paperspace/.cache/huggingface/modules/transformers_modules/togethercomputer/LLaMA-2-7B-32K/e6c58dac682e6c33b0e8fa0923ac5c79b76047c6/modeling_flash_llama.py\"\
    , line 70, in rmsnorm_func\n    hidden_states = hidden_states * torch.rsqrt(variance\
    \ + variance_epsilon)\nRuntimeError: Expected all tensors to be on the same device,\
    \ but found at least two devices, cuda:1 and cuda:0!\nERROR:torch.distributed.elastic.multiprocessing.api:failed\
    \ (exitcode: 1) local_rank: 0 (pid: 5654) of binary: /usr/bin/python3.9\nTraceback\
    \ (most recent call last):\n  File \"/home/paperspace/.local/bin/accelerate\"\
    , line 8, in <module>\n    sys.exit(main())\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\"\
    , line 45, in main\n    args.func(args)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\"\
    , line 970, in launch_command\n    multi_gpu_launcher(args)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\"\
    , line 646, in multi_gpu_launcher\n    distrib_run.run(args)\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/run.py\"\
    , line 785, in run\n    elastic_launch(\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py\"\
    , line 134, in __call__\n    return launch_agent(self._config, self._entrypoint,\
    \ list(args))\n  File \"/home/paperspace/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py\"\
    , line 250, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n\
    ============================================================\nrun_models.py FAILED\n\
    ------------------------------------------------------------\nFailures:\n[1]:\n\
    \  time      : 2023-09-28_03:41:06\n  host      : psvxc2krd\n  rank      : 1 (local_rank:\
    \ 1)\n  exitcode  : 1 (pid: 5655)\n  error_file: <N/A>\n  traceback : To enable\
    \ traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n[2]:\n \
    \ time      : 2023-09-28_03:41:06\n  host      : psvxc2krd\n  rank      : 2 (local_rank:\
    \ 2)\n  exitcode  : 1 (pid: 5656)\n  error_file: <N/A>\n  traceback : To enable\
    \ traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n[3]:\n \
    \ time      : 2023-09-28_03:41:06\n  host      : psvxc2krd\n  rank      : 3 (local_rank:\
    \ 3)\n  exitcode  : 1 (pid: 5657)\n  error_file: <N/A>\n  traceback : To enable\
    \ traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n------------------------------------------------------------\n\
    Root Cause (first observed failure):\n[0]:\n  time      : 2023-09-28_03:41:06\n\
    \  host      : psvxc2krd\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid:\
    \ 5654)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html"
  created_at: 2023-09-28 02:50:42+00:00
  edited: false
  hidden: false
  id: 6514f812023a480a7351dbfd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
      fullname: Ambarish Jash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajash
      type: user
    createdAt: '2023-09-29T22:37:57.000Z'
    data:
      edited: false
      editors:
      - ajash
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8131139874458313
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
          fullname: Ambarish Jash
          isHf: false
          isPro: false
          name: ajash
          type: user
        html: '<p>after looking at the stack trace a bit more feels like the layernorm
          is complaining: <a href="https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/blob/08639a72e17836184096ae6a7e2766f2a34c3e36/modeling_flash_llama.py#L444">https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/blob/08639a72e17836184096ae6a7e2766f2a34c3e36/modeling_flash_llama.py#L444</a><br>Is
          it because of model sharding.... output of the same layer is sharded across
          devices so its causing a problem?</p>

          '
        raw: 'after looking at the stack trace a bit more feels like the layernorm
          is complaining: https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/blob/08639a72e17836184096ae6a7e2766f2a34c3e36/modeling_flash_llama.py#L444

          Is it because of model sharding.... output of the same layer is sharded
          across devices so its causing a problem?'
        updatedAt: '2023-09-29T22:37:57.530Z'
      numEdits: 0
      reactions: []
    id: 651751c54fadbeb643b5ae10
    type: comment
  author: ajash
  content: 'after looking at the stack trace a bit more feels like the layernorm is
    complaining: https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/blob/08639a72e17836184096ae6a7e2766f2a34c3e36/modeling_flash_llama.py#L444

    Is it because of model sharding.... output of the same layer is sharded across
    devices so its causing a problem?'
  created_at: 2023-09-29 21:37:57+00:00
  edited: false
  hidden: false
  id: 651751c54fadbeb643b5ae10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
      fullname: Maurice Weber
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mauriceweber
      type: user
    createdAt: '2023-10-04T09:06:19.000Z'
    data:
      edited: true
      editors:
      - mauriceweber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9674438238143921
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg?w=200&h=200&f=face
          fullname: Maurice Weber
          isHf: false
          isPro: false
          name: mauriceweber
          type: user
        html: '<p>great that the installation now worked! re the new error -- it''s
          hard to say without seeing the code but I think your hunch is correct. How
          does your setup look like? and how are you distributing the model to different
          devices?</p>

          '
        raw: great that the installation now worked! re the new error -- it's hard
          to say without seeing the code but I think your hunch is correct. How does
          your setup look like? and how are you distributing the model to different
          devices?
        updatedAt: '2023-10-04T09:06:25.262Z'
      numEdits: 1
      reactions: []
    id: 651d2b0bcd0f8fef4b775c14
    type: comment
  author: mauriceweber
  content: great that the installation now worked! re the new error -- it's hard to
    say without seeing the code but I think your hunch is correct. How does your setup
    look like? and how are you distributing the model to different devices?
  created_at: 2023-10-04 08:06:19+00:00
  edited: true
  hidden: false
  id: 651d2b0bcd0f8fef4b775c14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
      fullname: Ambarish Jash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajash
      type: user
    createdAt: '2023-10-05T03:21:23.000Z'
    data:
      edited: true
      editors:
      - ajash
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.06487871706485748
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
          fullname: Ambarish Jash
          isHf: false
          isPro: false
          name: ajash
          type: user
        html: "<p>My code is very basic:</p>\n<h1 id=\"start-of-code\">Start of code</h1>\n\
          <p>def train_model(self, gradient_accum_steps, model_hub_loc):<br>    #\
          \ Multi-gpu implementation will use accelerates implementation<br>    accelerator\
          \ = Accelerator(gradient_accumulation_steps=gradient_accum_steps)<br>  \
          \  device = accelerator.device<br>    model = self.model<br>    model.train().to(device)<br>\
          \    dataset = self.dataset.with_format(\"torch\")<br>    dataloader = DataLoader(dataset,\
          \ collate_fn=DataCollatorForLanguageModeling(<br>        self.data_processor.tokenizer,\
          \ mlm=False), batch_size=1)<br>    #optimizer = self.set_optimizer()<br>\
          \    # Do better<br>    optimizer = torch.optim.AdamW(model.parameters(),\
          \ lr=1e-5)<br>    lr_scheduler = get_scheduler(<br>      \"linear\",<br>\
          \      optimizer=optimizer,<br>      num_warmup_steps=0,<br>      num_training_steps=10000,<br>\
          \    )<br>    # Accelerator specific code<br>    model, optimizer, dataloader,\
          \ lr_scheduler = accelerator.prepare(<br>      model, optimizer, dataloader,\
          \ lr_scheduler<br>    )</p>\n<pre><code># ######################################\n\
          if self.is_debug_mode:\n  self.print_model_device_placement(model)\n  #\
          \ There is no model training.\n  return\n# ######################################\n\
          for i, batch in enumerate(dataloader):\n  with accelerator.accumulate(model):\n\
          \    #batch = {k: v for k, v in batch.items()}\n    outputs = model(**batch)\n\
          \    loss = outputs[0]\n    # Gradient accumulation need not be done manually\n\
          \    # Instead of loss.backward()\n    accelerator.backward(loss)\n    optimizer.step()\n\
          \    lr_scheduler.step()\n    optimizer.zero_grad()\n    if i % 100 == 0:\
          \  # Poor mans logging\n      print(f\"loss: {loss}, steps: {i}\")\nif model_hub_loc:\n\
          \  model.push_to_hub(model_hub_loc)\n</code></pre>\n<p>I have the sharding\
          \ info as well. Pasting it below:</p>\n<p>module.base_model.model.model.embed_tokens.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.q_proj.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.k_proj.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.v_proj.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.o_proj.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.mlp.gate_proj.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.mlp.up_proj.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.mlp.down_proj.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.input_layernorm.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.post_attention_layernorm.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.1.self_attn.q_proj.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.1.self_attn.k_proj.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.1.self_attn.v_proj.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.1.self_attn.o_proj.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\
          \ -&gt; cuda:0module.base_model.model.model.embed_tokens.weight -&gt; cuda:1<br>module.base_model.model.model.embed_tokens.weight\
          \ -&gt; cuda:2<br>module.base_model.model.model.layers.1.mlp.gate_proj.weight\
          \ -&gt; cuda:0</p>\n<p>module.base_model.model.model.layers.0.self_attn.q_proj.weight\
          \ -&gt; cuda:1module.base_model.model.model.embed_tokens.weight -&gt; cuda:3<br>module.base_model.model.model.layers.1.mlp.up_proj.weight\
          \ -&gt; cuda:0module.base_model.model.model.layers.0.self_attn.q_proj.weight\
          \ -&gt; cuda:2</p>\n<p>module.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\
          \ -&gt; cuda:1</p>\n<p>module.base_model.model.model.layers.1.mlp.down_proj.weight\
          \ -&gt; cuda:0module.base_model.model.model.layers.0.self_attn.q_proj.weight\
          \ -&gt; cuda:3<br>module.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\
          \ -&gt; cuda:1module.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\
          \ -&gt; cuda:2</p>\n<p>module.base_model.model.model.layers.1.input_layernorm.weight\
          \ -&gt; cuda:0</p>\n<p>module.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\
          \ -&gt; cuda:3module.base_model.model.model.layers.0.self_attn.k_proj.weight\
          \ -&gt; cuda:1module.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\
          \ -&gt; cuda:2<br>module.base_model.model.model.layers.1.post_attention_layernorm.weight\
          \ -&gt; cuda:0</p>\n<p>module.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\
          \ -&gt; cuda:3module.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\
          \ -&gt; cuda:1<br>module.base_model.model.model.layers.0.self_attn.k_proj.weight\
          \ -&gt; cuda:2module.base_model.model.model.layers.2.self_attn.q_proj.weight\
          \ -&gt; cuda:0</p>\n<p>module.base_model.model.model.layers.0.self_attn.k_proj.weight\
          \ -&gt; cuda:3module.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\
          \ -&gt; cuda:1<br>module.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\
          \ -&gt; cuda:2<br>module.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\
          \ -&gt; cuda:3<br>module.base_model.model.model.layers.0.self_attn.v_proj.weight\
          \ -&gt; cuda:1<br>module.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\
          \ -&gt; cuda:2<br>module.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\
          \ -&gt; cuda:3<br>module.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\
          \ -&gt; cuda:1<br>module.base_model.model.model.layers.0.self_attn.v_proj.weight\
          \ -&gt; cuda:2<br>module.base_model.model.model.layers.2.self_attn.k_proj.weight\
          \ -&gt; cuda:0module.base_model.model.model.layers.0.self_attn.v_proj.weight\
          \ -&gt; cuda:3</p>\n<p>module.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\
          \ -&gt; cuda:1<br>module.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\
          \ -&gt; cuda:2<br>module.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\
          \ -&gt; cuda:3<br>module.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\
          \ -&gt; cuda:2module.base_model.model.model.layers.0.self_attn.o_proj.weight\
          \ -&gt; cuda:1</p>\n<p>module.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\
          \ -&gt; cuda:3<br>module.base_model.model.model.layers.0.self_attn.o_proj.weight\
          \ -&gt; cuda:2<br>module.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\
          \ -&gt; cuda:1<br>module.base_model.model.model.layers.2.self_attn.v_proj.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.o_proj.weight\
          \ -&gt; cuda:3<br>module.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\
          \ -&gt; cuda:2<br>module.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\
          \ -&gt; cuda:1<br>module.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\
          \ -&gt; cuda:0<br>module.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\
          \ -&gt; cuda:3<br>module.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\
          \ -&gt; cuda:2</p>\n"
        raw: "My code is very basic:\n\n# Start of code\ndef train_model(self, gradient_accum_steps,\
          \ model_hub_loc):\n    # Multi-gpu implementation will use accelerates implementation\n\
          \    accelerator = Accelerator(gradient_accumulation_steps=gradient_accum_steps)\n\
          \    device = accelerator.device\n    model = self.model\n    model.train().to(device)\n\
          \    dataset = self.dataset.with_format(\"torch\")\n    dataloader = DataLoader(dataset,\
          \ collate_fn=DataCollatorForLanguageModeling(\n        self.data_processor.tokenizer,\
          \ mlm=False), batch_size=1)\n    #optimizer = self.set_optimizer()\n   \
          \ # Do better\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\
          \    lr_scheduler = get_scheduler(\n      \"linear\",\n      optimizer=optimizer,\n\
          \      num_warmup_steps=0,\n      num_training_steps=10000,\n    )\n   \
          \ # Accelerator specific code\n    model, optimizer, dataloader, lr_scheduler\
          \ = accelerator.prepare(\n      model, optimizer, dataloader, lr_scheduler\n\
          \    )\n    \n    # ######################################\n    if self.is_debug_mode:\n\
          \      self.print_model_device_placement(model)\n      # There is no model\
          \ training.\n      return\n    # ######################################\n\
          \    for i, batch in enumerate(dataloader):\n      with accelerator.accumulate(model):\n\
          \        #batch = {k: v for k, v in batch.items()}\n        outputs = model(**batch)\n\
          \        loss = outputs[0]\n        # Gradient accumulation need not be\
          \ done manually\n        # Instead of loss.backward()\n        accelerator.backward(loss)\n\
          \        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\
          \        if i % 100 == 0:  # Poor mans logging\n          print(f\"loss:\
          \ {loss}, steps: {i}\")\n    if model_hub_loc:\n      model.push_to_hub(model_hub_loc)\n\
          \nI have the sharding info as well. Pasting it below:\n\nmodule.base_model.model.model.embed_tokens.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.q_proj.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.k_proj.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.v_proj.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.o_proj.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.mlp.gate_proj.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.mlp.up_proj.weight ->\
          \ cuda:0\nmodule.base_model.model.model.layers.0.mlp.down_proj.weight ->\
          \ cuda:0\nmodule.base_model.model.model.layers.0.input_layernorm.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.post_attention_layernorm.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.q_proj.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.k_proj.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.v_proj.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.o_proj.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\
          \ -> cuda:0module.base_model.model.model.embed_tokens.weight -> cuda:1\n\
          module.base_model.model.model.embed_tokens.weight -> cuda:2\nmodule.base_model.model.model.layers.1.mlp.gate_proj.weight\
          \ -> cuda:0\n\nmodule.base_model.model.model.layers.0.self_attn.q_proj.weight\
          \ -> cuda:1module.base_model.model.model.embed_tokens.weight -> cuda:3\n\
          module.base_model.model.model.layers.1.mlp.up_proj.weight -> cuda:0module.base_model.model.model.layers.0.self_attn.q_proj.weight\
          \ -> cuda:2\n\nmodule.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\
          \ -> cuda:1\n\nmodule.base_model.model.model.layers.1.mlp.down_proj.weight\
          \ -> cuda:0module.base_model.model.model.layers.0.self_attn.q_proj.weight\
          \ -> cuda:3\nmodule.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\
          \ -> cuda:1module.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\
          \ -> cuda:2\n\nmodule.base_model.model.model.layers.1.input_layernorm.weight\
          \ -> cuda:0\n\nmodule.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\
          \ -> cuda:3module.base_model.model.model.layers.0.self_attn.k_proj.weight\
          \ -> cuda:1module.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\
          \ -> cuda:2\nmodule.base_model.model.model.layers.1.post_attention_layernorm.weight\
          \ -> cuda:0\n\n\nmodule.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\
          \ -> cuda:3module.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\
          \ -> cuda:1\nmodule.base_model.model.model.layers.0.self_attn.k_proj.weight\
          \ -> cuda:2module.base_model.model.model.layers.2.self_attn.q_proj.weight\
          \ -> cuda:0\n\n\nmodule.base_model.model.model.layers.0.self_attn.k_proj.weight\
          \ -> cuda:3module.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\
          \ -> cuda:1\nmodule.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\
          \ -> cuda:2\nmodule.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\
          \ -> cuda:3\nmodule.base_model.model.model.layers.0.self_attn.v_proj.weight\
          \ -> cuda:1\nmodule.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\
          \ -> cuda:2\nmodule.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\
          \ -> cuda:3\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\
          \ -> cuda:1\nmodule.base_model.model.model.layers.0.self_attn.v_proj.weight\
          \ -> cuda:2\nmodule.base_model.model.model.layers.2.self_attn.k_proj.weight\
          \ -> cuda:0module.base_model.model.model.layers.0.self_attn.v_proj.weight\
          \ -> cuda:3\n\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\
          \ -> cuda:1\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\
          \ -> cuda:2\nmodule.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\
          \ -> cuda:3\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\
          \ -> cuda:2module.base_model.model.model.layers.0.self_attn.o_proj.weight\
          \ -> cuda:1\n\nmodule.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\
          \ -> cuda:3\nmodule.base_model.model.model.layers.0.self_attn.o_proj.weight\
          \ -> cuda:2\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\
          \ -> cuda:1\nmodule.base_model.model.model.layers.2.self_attn.v_proj.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.o_proj.weight\
          \ -> cuda:3\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\
          \ -> cuda:2\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\
          \ -> cuda:1\nmodule.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\
          \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\
          \ -> cuda:3\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\
          \ -> cuda:2\n\n"
        updatedAt: '2023-10-05T03:27:18.718Z'
      numEdits: 3
      reactions: []
    id: 651e2bb3b043205444c76d1c
    type: comment
  author: ajash
  content: "My code is very basic:\n\n# Start of code\ndef train_model(self, gradient_accum_steps,\
    \ model_hub_loc):\n    # Multi-gpu implementation will use accelerates implementation\n\
    \    accelerator = Accelerator(gradient_accumulation_steps=gradient_accum_steps)\n\
    \    device = accelerator.device\n    model = self.model\n    model.train().to(device)\n\
    \    dataset = self.dataset.with_format(\"torch\")\n    dataloader = DataLoader(dataset,\
    \ collate_fn=DataCollatorForLanguageModeling(\n        self.data_processor.tokenizer,\
    \ mlm=False), batch_size=1)\n    #optimizer = self.set_optimizer()\n    # Do better\n\
    \    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n    lr_scheduler\
    \ = get_scheduler(\n      \"linear\",\n      optimizer=optimizer,\n      num_warmup_steps=0,\n\
    \      num_training_steps=10000,\n    )\n    # Accelerator specific code\n   \
    \ model, optimizer, dataloader, lr_scheduler = accelerator.prepare(\n      model,\
    \ optimizer, dataloader, lr_scheduler\n    )\n    \n    # ######################################\n\
    \    if self.is_debug_mode:\n      self.print_model_device_placement(model)\n\
    \      # There is no model training.\n      return\n    # ######################################\n\
    \    for i, batch in enumerate(dataloader):\n      with accelerator.accumulate(model):\n\
    \        #batch = {k: v for k, v in batch.items()}\n        outputs = model(**batch)\n\
    \        loss = outputs[0]\n        # Gradient accumulation need not be done manually\n\
    \        # Instead of loss.backward()\n        accelerator.backward(loss)\n  \
    \      optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\
    \        if i % 100 == 0:  # Poor mans logging\n          print(f\"loss: {loss},\
    \ steps: {i}\")\n    if model_hub_loc:\n      model.push_to_hub(model_hub_loc)\n\
    \nI have the sharding info as well. Pasting it below:\n\nmodule.base_model.model.model.embed_tokens.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.q_proj.weight ->\
    \ cuda:0\nmodule.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.k_proj.weight ->\
    \ cuda:0\nmodule.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.v_proj.weight ->\
    \ cuda:0\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.o_proj.weight ->\
    \ cuda:0\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.mlp.gate_proj.weight -> cuda:0\n\
    module.base_model.model.model.layers.0.mlp.up_proj.weight -> cuda:0\nmodule.base_model.model.model.layers.0.mlp.down_proj.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.input_layernorm.weight ->\
    \ cuda:0\nmodule.base_model.model.model.layers.0.post_attention_layernorm.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.q_proj.weight ->\
    \ cuda:0\nmodule.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.k_proj.weight ->\
    \ cuda:0\nmodule.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.v_proj.weight ->\
    \ cuda:0\nmodule.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.o_proj.weight ->\
    \ cuda:0\nmodule.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\
    \ -> cuda:0module.base_model.model.model.embed_tokens.weight -> cuda:1\nmodule.base_model.model.model.embed_tokens.weight\
    \ -> cuda:2\nmodule.base_model.model.model.layers.1.mlp.gate_proj.weight -> cuda:0\n\
    \nmodule.base_model.model.model.layers.0.self_attn.q_proj.weight -> cuda:1module.base_model.model.model.embed_tokens.weight\
    \ -> cuda:3\nmodule.base_model.model.model.layers.1.mlp.up_proj.weight -> cuda:0module.base_model.model.model.layers.0.self_attn.q_proj.weight\
    \ -> cuda:2\n\nmodule.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\
    \ -> cuda:1\n\nmodule.base_model.model.model.layers.1.mlp.down_proj.weight ->\
    \ cuda:0module.base_model.model.model.layers.0.self_attn.q_proj.weight -> cuda:3\n\
    module.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\
    \ -> cuda:1module.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\
    \ -> cuda:2\n\nmodule.base_model.model.model.layers.1.input_layernorm.weight ->\
    \ cuda:0\n\nmodule.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\
    \ -> cuda:3module.base_model.model.model.layers.0.self_attn.k_proj.weight -> cuda:1module.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\
    \ -> cuda:2\nmodule.base_model.model.model.layers.1.post_attention_layernorm.weight\
    \ -> cuda:0\n\n\nmodule.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\
    \ -> cuda:3module.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\
    \ -> cuda:1\nmodule.base_model.model.model.layers.0.self_attn.k_proj.weight ->\
    \ cuda:2module.base_model.model.model.layers.2.self_attn.q_proj.weight -> cuda:0\n\
    \n\nmodule.base_model.model.model.layers.0.self_attn.k_proj.weight -> cuda:3module.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\
    \ -> cuda:1\nmodule.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\
    \ -> cuda:2\nmodule.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\
    \ -> cuda:3\nmodule.base_model.model.model.layers.0.self_attn.v_proj.weight ->\
    \ cuda:1\nmodule.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\
    \ -> cuda:2\nmodule.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\
    \ -> cuda:3\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\
    \ -> cuda:1\nmodule.base_model.model.model.layers.0.self_attn.v_proj.weight ->\
    \ cuda:2\nmodule.base_model.model.model.layers.2.self_attn.k_proj.weight -> cuda:0module.base_model.model.model.layers.0.self_attn.v_proj.weight\
    \ -> cuda:3\n\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\
    \ -> cuda:1\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\
    \ -> cuda:2\nmodule.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\
    \ -> cuda:3\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\
    \ -> cuda:2module.base_model.model.model.layers.0.self_attn.o_proj.weight -> cuda:1\n\
    \nmodule.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\
    \ -> cuda:3\nmodule.base_model.model.model.layers.0.self_attn.o_proj.weight ->\
    \ cuda:2\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\
    \ -> cuda:1\nmodule.base_model.model.model.layers.2.self_attn.v_proj.weight ->\
    \ cuda:0\nmodule.base_model.model.model.layers.0.self_attn.o_proj.weight -> cuda:3\n\
    module.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\
    \ -> cuda:2\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\
    \ -> cuda:1\nmodule.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\
    \ -> cuda:0\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\
    \ -> cuda:3\nmodule.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\
    \ -> cuda:2\n\n"
  created_at: 2023-10-05 02:21:23+00:00
  edited: true
  hidden: false
  id: 651e2bb3b043205444c76d1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
      fullname: Ambarish Jash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ajash
      type: user
    createdAt: '2023-10-18T18:13:22.000Z'
    data:
      edited: false
      editors:
      - ajash
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.991023600101471
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70fd92770ae9008fcb03a15739ed44f4.svg
          fullname: Ambarish Jash
          isHf: false
          isPro: false
          name: ajash
          type: user
        html: '<p>would love some help...</p>

          '
        raw: would love some help...
        updatedAt: '2023-10-18T18:13:22.559Z'
      numEdits: 0
      reactions: []
    id: 65302042a9390a5ddf7df46a
    type: comment
  author: ajash
  content: would love some help...
  created_at: 2023-10-18 17:13:22+00:00
  edited: false
  hidden: false
  id: 65302042a9390a5ddf7df46a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: togethercomputer/LLaMA-2-7B-32K
repo_type: model
status: open
target_branch: null
title: Using the Accelerate API to train models on multiple GPUs
