!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ndurkee
conflicting_files: null
created_at: 2023-10-03 11:45:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/53fe807d9ffaf2c23ac8a13756a2486b.svg
      fullname: Nick Durkee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ndurkee
      type: user
    createdAt: '2023-10-03T12:45:55.000Z'
    data:
      edited: false
      editors:
      - ndurkee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9539243578910828
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/53fe807d9ffaf2c23ac8a13756a2486b.svg
          fullname: Nick Durkee
          isHf: false
          isPro: false
          name: ndurkee
          type: user
        html: '<p>What command did you use to quantize the model? I don''t fully understand
          how your quantizer works when the context length is longer.</p>

          '
        raw: What command did you use to quantize the model? I don't fully understand
          how your quantizer works when the context length is longer.
        updatedAt: '2023-10-03T12:45:55.938Z'
      numEdits: 0
      reactions: []
    id: 651c0d0363cd12f4ba9109eb
    type: comment
  author: ndurkee
  content: What command did you use to quantize the model? I don't fully understand
    how your quantizer works when the context length is longer.
  created_at: 2023-10-03 11:45:55+00:00
  edited: false
  hidden: false
  id: 651c0d0363cd12f4ba9109eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6383dc174c48969dcf1b4fce/4N-GY7jVvdk08kp2B8DLh.jpeg?w=200&h=200&f=face
      fullname: turboderp
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: turboderp
      type: user
    createdAt: '2023-10-04T21:11:27.000Z'
    data:
      edited: true
      editors:
      - turboderp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8815641403198242
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6383dc174c48969dcf1b4fce/4N-GY7jVvdk08kp2B8DLh.jpeg?w=200&h=200&f=face
          fullname: turboderp
          isHf: false
          isPro: false
          name: turboderp
          type: user
        html: '<p>I used this to measure:</p>

          <p><code>python convert.py -nr -i /.../mistral-7b-instruct/ -o /.../ -c
          /.../wikitext_test.parquet -gr 100 -om /.../measurement.json</code></p>

          <p>Then to convert:</p>

          <p><code>python convert.py -nr -i /.../mistral-7b-instruct/ -o /.../ -c
          /.../wikitext_test.parquet -gr 100 -m /.../measurement.json -cf /.../mistral-7b-instruct-exl2/4.65bpw/
          -b 4.65</code></p>

          <p>Note that <code>-nr</code> clears out the working directory (<code>-o</code>)
          in either command, so be a little careful with it. Don''t do <code>-nr -o
          ~</code> or something like that, you''ll have a bad day. The <code>-gr 100</code>
          requires a bit of VRAM, so you might want to omit it if you have less than
          24 GB of VRAM.</p>

          <p>As for the longer context length, it''s not really critical to consider
          it when quantizing the model. I find it doesn''t have much of an effect
          to use longer calibration rows, except it increases the overall amount of
          calibration data. Other people have been experimenting and not measured
          any difference between <code>-l 2048 -r 100</code> (which would be the default)
          and <code>-l 4096 -r 50</code>, for instance. The key thing is that <code>-l</code>
          specifies the length of each sequence (in tokens) that''s forwarded through
          the model in order to measure and hopefully somewhat mitigate the error
          introduced by quantization. <code>-r</code> is the total number of such
          sequences.</p>

          '
        raw: 'I used this to measure:


          `python convert.py -nr -i /.../mistral-7b-instruct/ -o /.../ -c /.../wikitext_test.parquet
          -gr 100 -om /.../measurement.json`


          Then to convert:


          `python convert.py -nr -i /.../mistral-7b-instruct/ -o /.../ -c /.../wikitext_test.parquet
          -gr 100 -m /.../measurement.json -cf /.../mistral-7b-instruct-exl2/4.65bpw/
          -b 4.65`


          Note that `-nr` clears out the working directory (`-o`) in either command,
          so be a little careful with it. Don''t do `-nr -o ~` or something like that,
          you''ll have a bad day. The `-gr 100` requires a bit of VRAM, so you might
          want to omit it if you have less than 24 GB of VRAM.


          As for the longer context length, it''s not really critical to consider
          it when quantizing the model. I find it doesn''t have much of an effect
          to use longer calibration rows, except it increases the overall amount of
          calibration data. Other people have been experimenting and not measured
          any difference between `-l 2048 -r 100` (which would be the default) and
          `-l 4096 -r 50`, for instance. The key thing is that `-l` specifies the
          length of each sequence (in tokens) that''s forwarded through the model
          in order to measure and hopefully somewhat mitigate the error introduced
          by quantization. `-r` is the total number of such sequences.'
        updatedAt: '2023-10-04T21:12:34.005Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Thireus
        - ZoneOverreach
    id: 651dd4ff24d8d47df719cc53
    type: comment
  author: turboderp
  content: 'I used this to measure:


    `python convert.py -nr -i /.../mistral-7b-instruct/ -o /.../ -c /.../wikitext_test.parquet
    -gr 100 -om /.../measurement.json`


    Then to convert:


    `python convert.py -nr -i /.../mistral-7b-instruct/ -o /.../ -c /.../wikitext_test.parquet
    -gr 100 -m /.../measurement.json -cf /.../mistral-7b-instruct-exl2/4.65bpw/ -b
    4.65`


    Note that `-nr` clears out the working directory (`-o`) in either command, so
    be a little careful with it. Don''t do `-nr -o ~` or something like that, you''ll
    have a bad day. The `-gr 100` requires a bit of VRAM, so you might want to omit
    it if you have less than 24 GB of VRAM.


    As for the longer context length, it''s not really critical to consider it when
    quantizing the model. I find it doesn''t have much of an effect to use longer
    calibration rows, except it increases the overall amount of calibration data.
    Other people have been experimenting and not measured any difference between `-l
    2048 -r 100` (which would be the default) and `-l 4096 -r 50`, for instance. The
    key thing is that `-l` specifies the length of each sequence (in tokens) that''s
    forwarded through the model in order to measure and hopefully somewhat mitigate
    the error introduced by quantization. `-r` is the total number of such sequences.'
  created_at: 2023-10-04 20:11:27+00:00
  edited: true
  hidden: false
  id: 651dd4ff24d8d47df719cc53
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: turboderp/Mistral-7B-instruct-exl2
repo_type: model
status: open
target_branch: null
title: Command used to quantize model
