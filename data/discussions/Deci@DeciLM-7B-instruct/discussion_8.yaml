!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Rick-29
conflicting_files: null
created_at: 2023-12-27 02:11:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/X0ECgbNkFIp4z3Ey2ztGI.jpeg?w=200&h=200&f=face
      fullname: Alexandre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rick-29
      type: user
    createdAt: '2023-12-27T02:11:31.000Z'
    data:
      edited: false
      editors:
      - Rick-29
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.867480456829071
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/X0ECgbNkFIp4z3Ey2ztGI.jpeg?w=200&h=200&f=face
          fullname: Alexandre
          isHf: false
          isPro: false
          name: Rick-29
          type: user
        html: '<p>Is there a wat to save the model as a pytorch model instance instead
          of loading it every time with the transformers module? I have tried with
          wrapping the code inside a class that inherits <code>torch.nn.Module</code>
          but when I try to save the model (all the model not only the state dict)
          it throws an error.<br>Thanks</p>

          '
        raw: "Is there a wat to save the model as a pytorch model instance instead\
          \ of loading it every time with the transformers module? I have tried with\
          \ wrapping the code inside a class that inherits `torch.nn.Module` but when\
          \ I try to save the model (all the model not only the state dict) it throws\
          \ an error.\r\nThanks"
        updatedAt: '2023-12-27T02:11:31.986Z'
      numEdits: 0
      reactions: []
    id: 658b87d3925aadd433f8552d
    type: comment
  author: Rick-29
  content: "Is there a wat to save the model as a pytorch model instance instead of\
    \ loading it every time with the transformers module? I have tried with wrapping\
    \ the code inside a class that inherits `torch.nn.Module` but when I try to save\
    \ the model (all the model not only the state dict) it throws an error.\r\nThanks"
  created_at: 2023-12-27 02:11:31+00:00
  edited: false
  hidden: false
  id: 658b87d3925aadd433f8552d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1a3fef0131688e92e272cbd80856fc3.svg
      fullname: Najeeb Nabwani
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: NajeebDeci
      type: user
    createdAt: '2023-12-31T12:08:20.000Z'
    data:
      edited: false
      editors:
      - NajeebDeci
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9438921213150024
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1a3fef0131688e92e272cbd80856fc3.svg
          fullname: Najeeb Nabwani
          isHf: false
          isPro: false
          name: NajeebDeci
          type: user
        html: '<p>Hi Rick,</p>

          <p>Please provide more details on what you need exactly and code snippets
          in order for us to help.<br>Are you having issues with <code>.save_pretrained</code>
          ?</p>

          <p>Thanks</p>

          '
        raw: "Hi Rick,\n\nPlease provide more details on what you need exactly and\
          \ code snippets in order for us to help. \nAre you having issues with `.save_pretrained`\
          \ ?\n\nThanks"
        updatedAt: '2023-12-31T12:08:20.556Z'
      numEdits: 0
      reactions: []
    id: 659159b433d72b44f0e6a364
    type: comment
  author: NajeebDeci
  content: "Hi Rick,\n\nPlease provide more details on what you need exactly and code\
    \ snippets in order for us to help. \nAre you having issues with `.save_pretrained`\
    \ ?\n\nThanks"
  created_at: 2023-12-31 12:08:20+00:00
  edited: false
  hidden: false
  id: 659159b433d72b44f0e6a364
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/X0ECgbNkFIp4z3Ey2ztGI.jpeg?w=200&h=200&f=face
      fullname: Alexandre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rick-29
      type: user
    createdAt: '2023-12-31T20:47:14.000Z'
    data:
      edited: false
      editors:
      - Rick-29
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49673715233802795
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/X0ECgbNkFIp4z3Ey2ztGI.jpeg?w=200&h=200&f=face
          fullname: Alexandre
          isHf: false
          isPro: false
          name: Rick-29
          type: user
        html: "<p>Hi Najeeb,</p>\n<p>No, I ment saving completly the model architecture\
          \ and weigths as a .pth file, I tried using a model wrapper using <code>torch.nn.Module</code>,\
          \ something like this:</p>\n<pre><code>import torch\nfrom torch import nn\n\
          \nclass Wrapper(nn.Module):\n    def __init__(self, model, tokenizer):\n\
          \        # The model and tokenizer are loaded exaclty like in the `DeciLM-7B-Instruct.ipynb`\
          \ colab notebook \n        super().__init__()\n        self.model = model\n\
          \        self.tokenizer = tokenizer\n    \n    def forward(self, x):\n \
          \       inputs = self.tokenizer(SYSTEM_PROMPT_TEMPLATE.format(instruction=x),\
          \ return_tensors=\"pt\")\n        if torch.cuda.is_available():  # Ensure\
          \ input tensors are on the GPU if model is on GPU\n            inputs =\
          \ inputs.to('cuda')\n        output = self.model.generate(**inputs,\n  \
          \                              max_new_tokens=3000,\n                  \
          \              num_beams=5,\n                                no_repeat_ngram_size=4,\n\
          \                                early_stopping=True\n                 \
          \               )\n        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n\
          \nwrapper = Wrapper(model, tokenizer)\nmodel = torch.jit.script(wrapper)\n\
          torch.jit.save(model, \"model.pth\")\n</code></pre>\n<p>But it doesn't work.</p>\n\
          <p>What should I do?</p>\n<p>Thanks</p>\n"
        raw: "Hi Najeeb,\n\nNo, I ment saving completly the model architecture and\
          \ weigths as a .pth file, I tried using a model wrapper using `torch.nn.Module`,\
          \ something like this:\n```\nimport torch\nfrom torch import nn\n\nclass\
          \ Wrapper(nn.Module):\n    def __init__(self, model, tokenizer):\n     \
          \   # The model and tokenizer are loaded exaclty like in the `DeciLM-7B-Instruct.ipynb`\
          \ colab notebook \n        super().__init__()\n        self.model = model\n\
          \        self.tokenizer = tokenizer\n    \n    def forward(self, x):\n \
          \       inputs = self.tokenizer(SYSTEM_PROMPT_TEMPLATE.format(instruction=x),\
          \ return_tensors=\"pt\")\n        if torch.cuda.is_available():  # Ensure\
          \ input tensors are on the GPU if model is on GPU\n            inputs =\
          \ inputs.to('cuda')\n        output = self.model.generate(**inputs,\n  \
          \                              max_new_tokens=3000,\n                  \
          \              num_beams=5,\n                                no_repeat_ngram_size=4,\n\
          \                                early_stopping=True\n                 \
          \               )\n        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n\
          \nwrapper = Wrapper(model, tokenizer)\nmodel = torch.jit.script(wrapper)\n\
          torch.jit.save(model, \"model.pth\")\n```\nBut it doesn't work.\n\nWhat\
          \ should I do?\n\nThanks"
        updatedAt: '2023-12-31T20:47:14.704Z'
      numEdits: 0
      reactions: []
    id: 6591d35268d0b76331f7cff8
    type: comment
  author: Rick-29
  content: "Hi Najeeb,\n\nNo, I ment saving completly the model architecture and weigths\
    \ as a .pth file, I tried using a model wrapper using `torch.nn.Module`, something\
    \ like this:\n```\nimport torch\nfrom torch import nn\n\nclass Wrapper(nn.Module):\n\
    \    def __init__(self, model, tokenizer):\n        # The model and tokenizer\
    \ are loaded exaclty like in the `DeciLM-7B-Instruct.ipynb` colab notebook \n\
    \        super().__init__()\n        self.model = model\n        self.tokenizer\
    \ = tokenizer\n    \n    def forward(self, x):\n        inputs = self.tokenizer(SYSTEM_PROMPT_TEMPLATE.format(instruction=x),\
    \ return_tensors=\"pt\")\n        if torch.cuda.is_available():  # Ensure input\
    \ tensors are on the GPU if model is on GPU\n            inputs = inputs.to('cuda')\n\
    \        output = self.model.generate(**inputs,\n                            \
    \    max_new_tokens=3000,\n                                num_beams=5,\n    \
    \                            no_repeat_ngram_size=4,\n                       \
    \         early_stopping=True\n                                )\n        return\
    \ self.tokenizer.decode(output[0], skip_special_tokens=True)\n\nwrapper = Wrapper(model,\
    \ tokenizer)\nmodel = torch.jit.script(wrapper)\ntorch.jit.save(model, \"model.pth\"\
    )\n```\nBut it doesn't work.\n\nWhat should I do?\n\nThanks"
  created_at: 2023-12-31 20:47:14+00:00
  edited: false
  hidden: false
  id: 6591d35268d0b76331f7cff8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: Deci/DeciLM-7B-instruct
repo_type: model
status: open
target_branch: null
title: how to save the model as a pytorch or tensorflow model
