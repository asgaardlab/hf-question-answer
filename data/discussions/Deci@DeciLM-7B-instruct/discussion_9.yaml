!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vikram0711
conflicting_files: null
created_at: 2024-01-05 01:08:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bff5cbd0bbd9795784a5125e1ed918b9.svg
      fullname: Vikram Shetty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vikram0711
      type: user
    createdAt: '2024-01-05T01:08:43.000Z'
    data:
      edited: false
      editors:
      - vikram0711
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9562055468559265
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bff5cbd0bbd9795784a5125e1ed918b9.svg
          fullname: Vikram Shetty
          isHf: false
          isPro: false
          name: vikram0711
          type: user
        html: '<p>I''ve been searching for SLMs in the 7b - 20b range with a longer
          context length and I stumbled upon this one (8k). </p>

          <p>According to the evals, this model''s supposed to be better than Mistral''s
          first 7b Instruct model. That is not the experience I''m having. </p>

          <p>It stuffers from the repetition problem (word vomit) &amp; its pretty
          bad. I did not see this problem occur with the original mistral model nor
          any of its fine tunes.</p>

          <p>Another reason why we shouldn''t blindly trust evals. Data contamination
          is a real issue &amp; we need to figure out a way of best making sure that
          eval datasets do not leak into pre-training, SFT or preference datasets.</p>

          '
        raw: "I've been searching for SLMs in the 7b - 20b range with a longer context\
          \ length and I stumbled upon this one (8k). \r\n\r\nAccording to the evals,\
          \ this model's supposed to be better than Mistral's first 7b Instruct model.\
          \ That is not the experience I'm having. \r\n\r\nIt stuffers from the repetition\
          \ problem (word vomit) & its pretty bad. I did not see this problem occur\
          \ with the original mistral model nor any of its fine tunes.\r\n\r\nAnother\
          \ reason why we shouldn't blindly trust evals. Data contamination is a real\
          \ issue & we need to figure out a way of best making sure that eval datasets\
          \ do not leak into pre-training, SFT or preference datasets."
        updatedAt: '2024-01-05T01:08:43.593Z'
      numEdits: 0
      reactions: []
    id: 6597569bce76219628db5e22
    type: comment
  author: vikram0711
  content: "I've been searching for SLMs in the 7b - 20b range with a longer context\
    \ length and I stumbled upon this one (8k). \r\n\r\nAccording to the evals, this\
    \ model's supposed to be better than Mistral's first 7b Instruct model. That is\
    \ not the experience I'm having. \r\n\r\nIt stuffers from the repetition problem\
    \ (word vomit) & its pretty bad. I did not see this problem occur with the original\
    \ mistral model nor any of its fine tunes.\r\n\r\nAnother reason why we shouldn't\
    \ blindly trust evals. Data contamination is a real issue & we need to figure\
    \ out a way of best making sure that eval datasets do not leak into pre-training,\
    \ SFT or preference datasets."
  created_at: 2024-01-05 01:08:43+00:00
  edited: false
  hidden: false
  id: 6597569bce76219628db5e22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630904f2c038bf42d56d9d11/TrTU67BkrwoLYnwWgjdCE.jpeg?w=200&h=200&f=face
      fullname: Harpreet Sahota
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: harpreetsahota
      type: user
    createdAt: '2024-01-05T16:17:34.000Z'
    data:
      edited: false
      editors:
      - harpreetsahota
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.850287139415741
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630904f2c038bf42d56d9d11/TrTU67BkrwoLYnwWgjdCE.jpeg?w=200&h=200&f=face
          fullname: Harpreet Sahota
          isHf: false
          isPro: false
          name: harpreetsahota
          type: user
        html: "<p>Hey, thanks for the comment <span data-props=\"{&quot;user&quot;:&quot;vikram0711&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/vikram0711\"\
          >@<span class=\"underline\">vikram0711</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Two questions:</p>\n<ol>\n<li><p>Did you use the correct prompt\
          \ template/system messages?</p>\n</li>\n<li><p>Do you have a reproducible\
          \ notebook so I can examine what you did? Did you assess each model with\
          \ their appropriate prompt templates? Did you use the same generation parameters?\
          \ I'd like to see how you're reaching this conclusion.</p>\n</li>\n</ol>\n\
          <p>Here are a few resources you can read (with links to notebooks so you\
          \ can reproduce) which compare DeciLM to Mistral: </p>\n<ul>\n<li><a rel=\"\
          nofollow\" href=\"https://deci.ai/blog/llm-evaluation-and-how-decoding-strategies-impact-instruction-following/\"\
          >https://deci.ai/blog/llm-evaluation-and-how-decoding-strategies-impact-instruction-following/</a></li>\n\
          <li><a rel=\"nofollow\" href=\"https://deci.ai/blog/decilm-7b-vs-mistral-7b-on-chain-of-thought-tasks/\"\
          >https://deci.ai/blog/decilm-7b-vs-mistral-7b-on-chain-of-thought-tasks/</a></li>\n\
          </ul>\n<p>Cheers</p>\n"
        raw: "Hey, thanks for the comment @vikram0711 \n\nTwo questions:\n\n1) Did\
          \ you use the correct prompt template/system messages?\n\n2) Do you have\
          \ a reproducible notebook so I can examine what you did? Did you assess\
          \ each model with their appropriate prompt templates? Did you use the same\
          \ generation parameters? I'd like to see how you're reaching this conclusion.\
          \ \n\nHere are a few resources you can read (with links to notebooks so\
          \ you can reproduce) which compare DeciLM to Mistral: \n - https://deci.ai/blog/llm-evaluation-and-how-decoding-strategies-impact-instruction-following/\n\
          \ - https://deci.ai/blog/decilm-7b-vs-mistral-7b-on-chain-of-thought-tasks/\n\
          \nCheers"
        updatedAt: '2024-01-05T16:17:34.977Z'
      numEdits: 0
      reactions: []
    id: 65982b9ea4e09f3675c24511
    type: comment
  author: harpreetsahota
  content: "Hey, thanks for the comment @vikram0711 \n\nTwo questions:\n\n1) Did you\
    \ use the correct prompt template/system messages?\n\n2) Do you have a reproducible\
    \ notebook so I can examine what you did? Did you assess each model with their\
    \ appropriate prompt templates? Did you use the same generation parameters? I'd\
    \ like to see how you're reaching this conclusion. \n\nHere are a few resources\
    \ you can read (with links to notebooks so you can reproduce) which compare DeciLM\
    \ to Mistral: \n - https://deci.ai/blog/llm-evaluation-and-how-decoding-strategies-impact-instruction-following/\n\
    \ - https://deci.ai/blog/decilm-7b-vs-mistral-7b-on-chain-of-thought-tasks/\n\n\
    Cheers"
  created_at: 2024-01-05 16:17:34+00:00
  edited: false
  hidden: false
  id: 65982b9ea4e09f3675c24511
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bff5cbd0bbd9795784a5125e1ed918b9.svg
      fullname: Vikram Shetty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vikram0711
      type: user
    createdAt: '2024-01-05T19:27:49.000Z'
    data:
      edited: true
      editors:
      - vikram0711
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9236448407173157
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bff5cbd0bbd9795784a5125e1ed918b9.svg
          fullname: Vikram Shetty
          isHf: false
          isPro: false
          name: vikram0711
          type: user
        html: '<ol>

          <li><p>Yes, I used the correct chat template. The Jinja template in <code>tokenizer_config.json</code>
          is the one I used. </p>

          </li>

          <li><p>Unfortunately, I cannot share my notebook since it contains proprietary
          company datasets. But, I can assure you, yes I did assess each model with
          its appropriate template, I used Greedy sampling (<code>do_sample = False</code>)
          for both. And the Mistral fine tune I''m using performs significantly better
          in comparison. Even as a standalone model for this use case, I''m seeing
          subpar performance at best from <code>DeciLM-7B-instruct</code> . I expected
          to see word vomit only as the size of the context passed in the prompt increases
          (see <a href="https://huggingface.co/blog/tomaarsen/attention-sinks">here</a>),
          but I started observing the word vomit problem for smaller context sizes
          itself. It could be that the task itself is too complicated for the model,
          I''m not sure. Maybe the fact that this model wasn''t aligned at all might
          be an issue or it could also be a problem with the SFT-ing too, because
          I''m not too happy with its instruction following either.</p>

          </li>

          <li><p>Maybe its just the case that this model is not well suited for my
          use case or I might need to tune the prompt I''m using, but these are my
          observations off the bat, with the ChatGPT prompt that was being used for
          this use case.</p>

          </li>

          </ol>

          '
        raw: "1. Yes, I used the correct chat template. The Jinja template in ```tokenizer_config.json```\
          \ is the one I used. \n\n2. Unfortunately, I cannot share my notebook since\
          \ it contains proprietary company datasets. But, I can assure you, yes I\
          \ did assess each model with its appropriate template, I used Greedy sampling\
          \ (```do_sample = False```) for both. And the Mistral fine tune I'm using\
          \ performs significantly better in comparison. Even as a standalone model\
          \ for this use case, I'm seeing subpar performance at best from ```DeciLM-7B-instruct```\
          \ . I expected to see word vomit only as the size of the context passed\
          \ in the prompt increases (see [here](https://huggingface.co/blog/tomaarsen/attention-sinks)),\
          \ but I started observing the word vomit problem for smaller context sizes\
          \ itself. It could be that the task itself is too complicated for the model,\
          \ I'm not sure. Maybe the fact that this model wasn't aligned at all might\
          \ be an issue or it could also be a problem with the SFT-ing too, because\
          \ I'm not too happy with its instruction following either.\n\n3. Maybe its\
          \ just the case that this model is not well suited for my use case or I\
          \ might need to tune the prompt I'm using, but these are my observations\
          \ off the bat, with the ChatGPT prompt that was being used for this use\
          \ case."
        updatedAt: '2024-01-05T19:29:45.835Z'
      numEdits: 1
      reactions: []
    id: 659858352bc3a1e0f6cd5fbd
    type: comment
  author: vikram0711
  content: "1. Yes, I used the correct chat template. The Jinja template in ```tokenizer_config.json```\
    \ is the one I used. \n\n2. Unfortunately, I cannot share my notebook since it\
    \ contains proprietary company datasets. But, I can assure you, yes I did assess\
    \ each model with its appropriate template, I used Greedy sampling (```do_sample\
    \ = False```) for both. And the Mistral fine tune I'm using performs significantly\
    \ better in comparison. Even as a standalone model for this use case, I'm seeing\
    \ subpar performance at best from ```DeciLM-7B-instruct``` . I expected to see\
    \ word vomit only as the size of the context passed in the prompt increases (see\
    \ [here](https://huggingface.co/blog/tomaarsen/attention-sinks)), but I started\
    \ observing the word vomit problem for smaller context sizes itself. It could\
    \ be that the task itself is too complicated for the model, I'm not sure. Maybe\
    \ the fact that this model wasn't aligned at all might be an issue or it could\
    \ also be a problem with the SFT-ing too, because I'm not too happy with its instruction\
    \ following either.\n\n3. Maybe its just the case that this model is not well\
    \ suited for my use case or I might need to tune the prompt I'm using, but these\
    \ are my observations off the bat, with the ChatGPT prompt that was being used\
    \ for this use case."
  created_at: 2024-01-05 19:27:49+00:00
  edited: true
  hidden: false
  id: 659858352bc3a1e0f6cd5fbd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630904f2c038bf42d56d9d11/TrTU67BkrwoLYnwWgjdCE.jpeg?w=200&h=200&f=face
      fullname: Harpreet Sahota
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: harpreetsahota
      type: user
    createdAt: '2024-01-06T16:33:26.000Z'
    data:
      edited: false
      editors:
      - harpreetsahota
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9699269533157349
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630904f2c038bf42d56d9d11/TrTU67BkrwoLYnwWgjdCE.jpeg?w=200&h=200&f=face
          fullname: Harpreet Sahota
          isHf: false
          isPro: false
          name: harpreetsahota
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;vikram0711&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vikram0711\">@<span class=\"\
          underline\">vikram0711</span></a></span>\n\n\t</span></span>, I'm happy\
          \ to hop on a call with you if you'd like.</p>\n"
        raw: '@vikram0711, I''m happy to hop on a call with you if you''d like.'
        updatedAt: '2024-01-06T16:33:26.624Z'
      numEdits: 0
      reactions: []
    id: 659980d694cadc9cdcdc6485
    type: comment
  author: harpreetsahota
  content: '@vikram0711, I''m happy to hop on a call with you if you''d like.'
  created_at: 2024-01-06 16:33:26+00:00
  edited: false
  hidden: false
  id: 659980d694cadc9cdcdc6485
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: Deci/DeciLM-7B-instruct
repo_type: model
status: open
target_branch: null
title: '[Word Vomit pretty early on]'
