!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Rick-29
conflicting_files: null
created_at: 2023-12-27 02:06:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/X0ECgbNkFIp4z3Ey2ztGI.jpeg?w=200&h=200&f=face
      fullname: Alexandre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rick-29
      type: user
    createdAt: '2023-12-27T02:06:59.000Z'
    data:
      edited: false
      editors:
      - Rick-29
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9416420459747314
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/X0ECgbNkFIp4z3Ey2ztGI.jpeg?w=200&h=200&f=face
          fullname: Alexandre
          isHf: false
          isPro: false
          name: Rick-29
          type: user
        html: '<p>All the models that I know recieve a batch of data as an input and
          returns a batch of data as an output but when I try to pass multiple queries
          to the model at the same time it throws me an error.<br>Is there a way to
          make that?<br>Thanks.</p>

          '
        raw: "All the models that I know recieve a batch of data as an input and returns\
          \ a batch of data as an output but when I try to pass multiple queries to\
          \ the model at the same time it throws me an error.\r\nIs there a way to\
          \ make that?\r\nThanks."
        updatedAt: '2023-12-27T02:06:59.876Z'
      numEdits: 0
      reactions: []
    id: 658b86c31e9d03c06a32139f
    type: comment
  author: Rick-29
  content: "All the models that I know recieve a batch of data as an input and returns\
    \ a batch of data as an output but when I try to pass multiple queries to the\
    \ model at the same time it throws me an error.\r\nIs there a way to make that?\r\
    \nThanks."
  created_at: 2023-12-27 02:06:59+00:00
  edited: false
  hidden: false
  id: 658b86c31e9d03c06a32139f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1a3fef0131688e92e272cbd80856fc3.svg
      fullname: Najeeb Nabwani
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: NajeebDeci
      type: user
    createdAt: '2023-12-31T12:03:57.000Z'
    data:
      edited: false
      editors:
      - NajeebDeci
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8147550821304321
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1a3fef0131688e92e272cbd80856fc3.svg
          fullname: Najeeb Nabwani
          isHf: false
          isPro: false
          name: NajeebDeci
          type: user
        html: '<p>Hi Rick,</p>

          <p>Can you please share a code snippet demonstrating the error?</p>

          <p>Thanks</p>

          '
        raw: 'Hi Rick,


          Can you please share a code snippet demonstrating the error?


          Thanks'
        updatedAt: '2023-12-31T12:03:57.762Z'
      numEdits: 0
      reactions: []
    id: 659158adc0b1372b2ee93727
    type: comment
  author: NajeebDeci
  content: 'Hi Rick,


    Can you please share a code snippet demonstrating the error?


    Thanks'
  created_at: 2023-12-31 12:03:57+00:00
  edited: false
  hidden: false
  id: 659158adc0b1372b2ee93727
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/X0ECgbNkFIp4z3Ey2ztGI.jpeg?w=200&h=200&f=face
      fullname: Alexandre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rick-29
      type: user
    createdAt: '2023-12-31T20:09:26.000Z'
    data:
      edited: false
      editors:
      - Rick-29
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7181903719902039
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/X0ECgbNkFIp4z3Ey2ztGI.jpeg?w=200&h=200&f=face
          fullname: Alexandre
          isHf: false
          isPro: false
          name: Rick-29
          type: user
        html: "<p>I mean pass more than one question to the model at the same time\
          \ like this:</p>\n<pre><code># Function to construct the prompt using the\
          \ new system prompt template\ndef get_prompt_with_template(message: str)\
          \ -&gt; str:\n    return SYSTEM_PROMPT_TEMPLATE.format(instruction=message)\n\
          \n# Function to handle the generation of the model's response using the\
          \ constructed prompt\ndef generate_model_response(messages: list) -&gt;\
          \ list[str]:\n    prompt = list(map(lambda message: get_prompt_with_template(message),\
          \ messages))\n    inputs = tokenizer(prompt, return_tensors='pt')\n    if\
          \ torch.cuda.is_available():  # Ensure input tensors are on the GPU if model\
          \ is on GPU\n        inputs = inputs.to('cuda')\n    output = model.generate(**inputs,\n\
          \                            max_new_tokens=3000,\n                    \
          \        num_beams=5,\n                            no_repeat_ngram_size=4,\n\
          \                            early_stopping=True\n                     \
          \       )\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\
          \nmessages= [\n    \"How can I teach my toddler son to be more patient and\
          \ not throw tantrums?\",\n    \"I have Bisquick flour, eggs, milk, butter.\
          \ How can I make peanut butter pancakes?\",\n    \"How do I make french\
          \ toast? Think through it step by step\"\n]\nresponses = generate_model_response(messages)\n\
          (The code before that comes from the `DeciLM-7B-Instruct.ipynb` colab notebook)\n\
          </code></pre>\n<p>But that raises the following error<br><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/658b4cd83c34b2ccf6ea3fa7/-bkMFTlZRpzdivD-vnp_7.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/658b4cd83c34b2ccf6ea3fa7/-bkMFTlZRpzdivD-vnp_7.png\"\
          ></a><br>Is there any thing I can do to solve that problem? Like transform\
          \ the input list into a PyTorch tensor or something like that?</p>\n<p>Also,\
          \ I noticed that the model took about 3 minutes to generate each response\
          \ but the model didn't use all the available resources, is there a way to\
          \ make the model use all the available resources (RAM, VRAM) to generate\
          \ responses faster?</p>\n<p>Thanks. </p>\n"
        raw: "I mean pass more than one question to the model at the same time like\
          \ this:\n\n```\n# Function to construct the prompt using the new system\
          \ prompt template\ndef get_prompt_with_template(message: str) -> str:\n\
          \    return SYSTEM_PROMPT_TEMPLATE.format(instruction=message)\n\n# Function\
          \ to handle the generation of the model's response using the constructed\
          \ prompt\ndef generate_model_response(messages: list) -> list[str]:\n  \
          \  prompt = list(map(lambda message: get_prompt_with_template(message),\
          \ messages))\n    inputs = tokenizer(prompt, return_tensors='pt')\n    if\
          \ torch.cuda.is_available():  # Ensure input tensors are on the GPU if model\
          \ is on GPU\n        inputs = inputs.to('cuda')\n    output = model.generate(**inputs,\n\
          \                            max_new_tokens=3000,\n                    \
          \        num_beams=5,\n                            no_repeat_ngram_size=4,\n\
          \                            early_stopping=True\n                     \
          \       )\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\
          \nmessages= [\n    \"How can I teach my toddler son to be more patient and\
          \ not throw tantrums?\",\n    \"I have Bisquick flour, eggs, milk, butter.\
          \ How can I make peanut butter pancakes?\",\n    \"How do I make french\
          \ toast? Think through it step by step\"\n]\nresponses = generate_model_response(messages)\n\
          (The code before that comes from the `DeciLM-7B-Instruct.ipynb` colab notebook)\n\
          ```\nBut that raises the following error \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/658b4cd83c34b2ccf6ea3fa7/-bkMFTlZRpzdivD-vnp_7.png)\n\
          Is there any thing I can do to solve that problem? Like transform the input\
          \ list into a PyTorch tensor or something like that?\n\nAlso, I noticed\
          \ that the model took about 3 minutes to generate each response but the\
          \ model didn't use all the available resources, is there a way to make the\
          \ model use all the available resources (RAM, VRAM) to generate responses\
          \ faster?\n\nThanks. \n"
        updatedAt: '2023-12-31T20:09:26.904Z'
      numEdits: 0
      reactions: []
    id: 6591ca76a41c3cbad5bb049c
    type: comment
  author: Rick-29
  content: "I mean pass more than one question to the model at the same time like\
    \ this:\n\n```\n# Function to construct the prompt using the new system prompt\
    \ template\ndef get_prompt_with_template(message: str) -> str:\n    return SYSTEM_PROMPT_TEMPLATE.format(instruction=message)\n\
    \n# Function to handle the generation of the model's response using the constructed\
    \ prompt\ndef generate_model_response(messages: list) -> list[str]:\n    prompt\
    \ = list(map(lambda message: get_prompt_with_template(message), messages))\n \
    \   inputs = tokenizer(prompt, return_tensors='pt')\n    if torch.cuda.is_available():\
    \  # Ensure input tensors are on the GPU if model is on GPU\n        inputs =\
    \ inputs.to('cuda')\n    output = model.generate(**inputs,\n                 \
    \           max_new_tokens=3000,\n                            num_beams=5,\n \
    \                           no_repeat_ngram_size=4,\n                        \
    \    early_stopping=True\n                            )\n    return tokenizer.decode(output[0],\
    \ skip_special_tokens=True)\n\nmessages= [\n    \"How can I teach my toddler son\
    \ to be more patient and not throw tantrums?\",\n    \"I have Bisquick flour,\
    \ eggs, milk, butter. How can I make peanut butter pancakes?\",\n    \"How do\
    \ I make french toast? Think through it step by step\"\n]\nresponses = generate_model_response(messages)\n\
    (The code before that comes from the `DeciLM-7B-Instruct.ipynb` colab notebook)\n\
    ```\nBut that raises the following error \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/658b4cd83c34b2ccf6ea3fa7/-bkMFTlZRpzdivD-vnp_7.png)\n\
    Is there any thing I can do to solve that problem? Like transform the input list\
    \ into a PyTorch tensor or something like that?\n\nAlso, I noticed that the model\
    \ took about 3 minutes to generate each response but the model didn't use all\
    \ the available resources, is there a way to make the model use all the available\
    \ resources (RAM, VRAM) to generate responses faster?\n\nThanks. \n"
  created_at: 2023-12-31 20:09:26+00:00
  edited: false
  hidden: false
  id: 6591ca76a41c3cbad5bb049c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1a3fef0131688e92e272cbd80856fc3.svg
      fullname: Najeeb Nabwani
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: NajeebDeci
      type: user
    createdAt: '2024-01-02T11:23:57.000Z'
    data:
      edited: false
      editors:
      - NajeebDeci
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7293224334716797
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1a3fef0131688e92e272cbd80856fc3.svg
          fullname: Najeeb Nabwani
          isHf: false
          isPro: false
          name: NajeebDeci
          type: user
        html: "<p>Hi,</p>\n<p>You're getting this error because your batched input\
          \ has variable lengths, you need a batch where all samples are the same\
          \ length. The easiest way to do that is via padding. In addition, I modified\
          \ the tokenizer code to do batched_decoding. The following code should work\
          \ for you:</p>\n<pre><code># Function to construct the prompt using the\
          \ new system prompt template\ndef get_prompt_with_template(message: str)\
          \ -&gt; str:\n    return SYSTEM_PROMPT_TEMPLATE.format(instruction=message)\n\
          \n# Function to handle the generation of the model's response using the\
          \ constructed prompt\ndef generate_model_response(messages: list) -&gt;\
          \ list[str]:\n    prompt = list(map(lambda message: get_prompt_with_template(message),\
          \ messages))\n    inputs = tokenizer(prompt, return_tensors='pt', padding=\"\
          longest\")\n    if torch.cuda.is_available():  # Ensure input tensors are\
          \ on the GPU if model is on GPU\n        inputs = inputs.to('cuda')\n  \
          \  output = model.generate(**inputs,\n                            max_new_tokens=500,\
          \ # 3000 is probably too slow\n                            num_beams=5,\n\
          \                            no_repeat_ngram_size=4,\n                 \
          \           early_stopping=True\n                            )\n    return\
          \ tokenizer.batch_decode(output, skip_special_tokens=True)\n\nmessages=\
          \ [\n    \"How can I teach my toddler son to be more patient and not throw\
          \ tantrums?\",\n    \"I have Bisquick flour, eggs, milk, butter. How can\
          \ I make peanut butter pancakes?\",\n    \"How do I make french toast? Think\
          \ through it step by step\"\n]\nresponses = generate_model_response(messages)\n\
          </code></pre>\n<p>In terms of performance, there are many variables to this,\
          \ for example, using beam search improves model output but slows down inference\
          \ speed.<br>We recommend using our runtime Infery-LLM for the best performance\
          \ for our LLMs. Here's a link to try it out yourself: <a rel=\"nofollow\"\
          \ href=\"https://console.deci.ai/infery-llm-demo\">https://console.deci.ai/infery-llm-demo</a></p>\n\
          <p>Let us know if you need anything else :)</p>\n"
        raw: "Hi,\n\nYou're getting this error because your batched input has variable\
          \ lengths, you need a batch where all samples are the same length. The easiest\
          \ way to do that is via padding. In addition, I modified the tokenizer code\
          \ to do batched_decoding. The following code should work for you:\n\n```\n\
          # Function to construct the prompt using the new system prompt template\n\
          def get_prompt_with_template(message: str) -> str:\n    return SYSTEM_PROMPT_TEMPLATE.format(instruction=message)\n\
          \n# Function to handle the generation of the model's response using the\
          \ constructed prompt\ndef generate_model_response(messages: list) -> list[str]:\n\
          \    prompt = list(map(lambda message: get_prompt_with_template(message),\
          \ messages))\n    inputs = tokenizer(prompt, return_tensors='pt', padding=\"\
          longest\")\n    if torch.cuda.is_available():  # Ensure input tensors are\
          \ on the GPU if model is on GPU\n        inputs = inputs.to('cuda')\n  \
          \  output = model.generate(**inputs,\n                            max_new_tokens=500,\
          \ # 3000 is probably too slow\n                            num_beams=5,\n\
          \                            no_repeat_ngram_size=4,\n                 \
          \           early_stopping=True\n                            )\n    return\
          \ tokenizer.batch_decode(output, skip_special_tokens=True)\n\nmessages=\
          \ [\n    \"How can I teach my toddler son to be more patient and not throw\
          \ tantrums?\",\n    \"I have Bisquick flour, eggs, milk, butter. How can\
          \ I make peanut butter pancakes?\",\n    \"How do I make french toast? Think\
          \ through it step by step\"\n]\nresponses = generate_model_response(messages)\n\
          ```\n\nIn terms of performance, there are many variables to this, for example,\
          \ using beam search improves model output but slows down inference speed.\
          \ \nWe recommend using our runtime Infery-LLM for the best performance for\
          \ our LLMs. Here's a link to try it out yourself: https://console.deci.ai/infery-llm-demo\n\
          \nLet us know if you need anything else :)"
        updatedAt: '2024-01-02T11:23:57.289Z'
      numEdits: 0
      reactions: []
    id: 6593f24da2607099285ef19d
    type: comment
  author: NajeebDeci
  content: "Hi,\n\nYou're getting this error because your batched input has variable\
    \ lengths, you need a batch where all samples are the same length. The easiest\
    \ way to do that is via padding. In addition, I modified the tokenizer code to\
    \ do batched_decoding. The following code should work for you:\n\n```\n# Function\
    \ to construct the prompt using the new system prompt template\ndef get_prompt_with_template(message:\
    \ str) -> str:\n    return SYSTEM_PROMPT_TEMPLATE.format(instruction=message)\n\
    \n# Function to handle the generation of the model's response using the constructed\
    \ prompt\ndef generate_model_response(messages: list) -> list[str]:\n    prompt\
    \ = list(map(lambda message: get_prompt_with_template(message), messages))\n \
    \   inputs = tokenizer(prompt, return_tensors='pt', padding=\"longest\")\n   \
    \ if torch.cuda.is_available():  # Ensure input tensors are on the GPU if model\
    \ is on GPU\n        inputs = inputs.to('cuda')\n    output = model.generate(**inputs,\n\
    \                            max_new_tokens=500, # 3000 is probably too slow\n\
    \                            num_beams=5,\n                            no_repeat_ngram_size=4,\n\
    \                            early_stopping=True\n                           \
    \ )\n    return tokenizer.batch_decode(output, skip_special_tokens=True)\n\nmessages=\
    \ [\n    \"How can I teach my toddler son to be more patient and not throw tantrums?\"\
    ,\n    \"I have Bisquick flour, eggs, milk, butter. How can I make peanut butter\
    \ pancakes?\",\n    \"How do I make french toast? Think through it step by step\"\
    \n]\nresponses = generate_model_response(messages)\n```\n\nIn terms of performance,\
    \ there are many variables to this, for example, using beam search improves model\
    \ output but slows down inference speed. \nWe recommend using our runtime Infery-LLM\
    \ for the best performance for our LLMs. Here's a link to try it out yourself:\
    \ https://console.deci.ai/infery-llm-demo\n\nLet us know if you need anything\
    \ else :)"
  created_at: 2024-01-02 11:23:57+00:00
  edited: false
  hidden: false
  id: 6593f24da2607099285ef19d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/X0ECgbNkFIp4z3Ey2ztGI.jpeg?w=200&h=200&f=face
      fullname: Alexandre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rick-29
      type: user
    createdAt: '2024-01-03T01:36:47.000Z'
    data:
      edited: false
      editors:
      - Rick-29
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8639523386955261
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/X0ECgbNkFIp4z3Ey2ztGI.jpeg?w=200&h=200&f=face
          fullname: Alexandre
          isHf: false
          isPro: false
          name: Rick-29
          type: user
        html: '<p>Hi,<br>Now it works correctly. Thank you very much! </p>

          '
        raw: "Hi, \nNow it works correctly. Thank you very much! \n"
        updatedAt: '2024-01-03T01:36:47.621Z'
      numEdits: 0
      reactions: []
    id: 6594ba2fde82e1ef7bd4ff17
    type: comment
  author: Rick-29
  content: "Hi, \nNow it works correctly. Thank you very much! \n"
  created_at: 2024-01-03 01:36:47+00:00
  edited: false
  hidden: false
  id: 6594ba2fde82e1ef7bd4ff17
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: Deci/DeciLM-7B-instruct
repo_type: model
status: open
target_branch: null
title: How to pass a batch of entries to the model at the same time?
