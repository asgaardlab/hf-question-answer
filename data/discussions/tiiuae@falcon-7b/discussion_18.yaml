!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Noecora
conflicting_files: null
created_at: 2023-06-03 03:03:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c52f0d2f5746f5e4709350a405100b1b.svg
      fullname: Nithin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Noecora
      type: user
    createdAt: '2023-06-03T04:03:49.000Z'
    data:
      edited: false
      editors:
      - Noecora
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6228259801864624
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c52f0d2f5746f5e4709350a405100b1b.svg
          fullname: Nithin
          isHf: false
          isPro: false
          name: Noecora
          type: user
        html: '<p>I''m getting an error despite loading all dependencies. Is anybody
          else facing the same issue?</p>

          <p>Traceback (most recent call last):<br>  File "/Users/***/PycharmProjects/Falcon-7b/main.py",
          line 8, in <br>    pipeline = transformers.pipeline(<br>               ^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/nks/PycharmProjects/Falcon-7b/venv/lib/python3.11/site-packages/transformers/pipelines/<strong>init</strong>.py",
          line 788, in pipeline<br>    framework, model = infer_framework_load_model(<br>                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/Users/nks/PycharmProjects/Falcon-7b/venv/lib/python3.11/site-packages/transformers/pipelines/base.py",
          line 279, in infer_framework_load_model<br>    raise ValueError(f"Could
          not load model {model} with any of the following classes: {class_tuple}.")<br>ValueError:
          Could not load model tiiuae/falcon-7b with any of the following classes:
          (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).</p>

          <p>Any leads would be appreciated</p>

          '
        raw: "I'm getting an error despite loading all dependencies. Is anybody else\
          \ facing the same issue?\r\n\r\nTraceback (most recent call last):\r\n \
          \ File \"/Users/***/PycharmProjects/Falcon-7b/main.py\", line 8, in <module>\r\
          \n    pipeline = transformers.pipeline(\r\n               ^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/Users/nks/PycharmProjects/Falcon-7b/venv/lib/python3.11/site-packages/transformers/pipelines/__init__.py\"\
          , line 788, in pipeline\r\n    framework, model = infer_framework_load_model(\r\
          \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/nks/PycharmProjects/Falcon-7b/venv/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
          , line 279, in infer_framework_load_model\r\n    raise ValueError(f\"Could\
          \ not load model {model} with any of the following classes: {class_tuple}.\"\
          )\r\nValueError: Could not load model tiiuae/falcon-7b with any of the following\
          \ classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\r\
          \n\r\nAny leads would be appreciated"
        updatedAt: '2023-06-03T04:03:49.837Z'
      numEdits: 0
      reactions: []
    id: 647abba58de08112d71e28d4
    type: comment
  author: Noecora
  content: "I'm getting an error despite loading all dependencies. Is anybody else\
    \ facing the same issue?\r\n\r\nTraceback (most recent call last):\r\n  File \"\
    /Users/***/PycharmProjects/Falcon-7b/main.py\", line 8, in <module>\r\n    pipeline\
    \ = transformers.pipeline(\r\n               ^^^^^^^^^^^^^^^^^^^^^^\r\n  File\
    \ \"/Users/nks/PycharmProjects/Falcon-7b/venv/lib/python3.11/site-packages/transformers/pipelines/__init__.py\"\
    , line 788, in pipeline\r\n    framework, model = infer_framework_load_model(\r\
    \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/nks/PycharmProjects/Falcon-7b/venv/lib/python3.11/site-packages/transformers/pipelines/base.py\"\
    , line 279, in infer_framework_load_model\r\n    raise ValueError(f\"Could not\
    \ load model {model} with any of the following classes: {class_tuple}.\")\r\n\
    ValueError: Could not load model tiiuae/falcon-7b with any of the following classes:\
    \ (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).\r\n\
    \r\nAny leads would be appreciated"
  created_at: 2023-06-03 03:03:49+00:00
  edited: false
  hidden: false
  id: 647abba58de08112d71e28d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e354046b4c81c0224205e223ecc852b8.svg
      fullname: luka gamulin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lukaga
      type: user
    createdAt: '2023-06-03T05:46:13.000Z'
    data:
      edited: true
      editors:
      - lukaga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7015954852104187
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e354046b4c81c0224205e223ecc852b8.svg
          fullname: luka gamulin
          isHf: false
          isPro: false
          name: lukaga
          type: user
        html: "<p>Same here :(</p>\n<pre><code>    277 \n    278         if isinstance(model,\
          \ str):\n--&gt; 279             raise ValueError(f\"Could not load model\
          \ {model} with any of the following classes: {class_tuple}.\")\n    280\
          \ \n    281     framework = \"tf\" if \"keras.engine.training.Model\" in\
          \ str(inspect.getmro(model.__class__)) else \"pt\"\n\nValueError: Could\
          \ not load model tiiuae/falcon-7b with any of the following classes: (&lt;class\
          \ 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;, &lt;class\
          \ 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;).\n\
          </code></pre>\n"
        raw: "Same here :(\n\n```\n    277 \n    278         if isinstance(model,\
          \ str):\n--> 279             raise ValueError(f\"Could not load model {model}\
          \ with any of the following classes: {class_tuple}.\")\n    280 \n    281\
          \     framework = \"tf\" if \"keras.engine.training.Model\" in str(inspect.getmro(model.__class__))\
          \ else \"pt\"\n\nValueError: Could not load model tiiuae/falcon-7b with\
          \ any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\n\
          \n```"
        updatedAt: '2023-06-03T05:46:41.635Z'
      numEdits: 1
      reactions: []
    id: 647ad3a542abe277476a9baf
    type: comment
  author: lukaga
  content: "Same here :(\n\n```\n    277 \n    278         if isinstance(model, str):\n\
    --> 279             raise ValueError(f\"Could not load model {model} with any\
    \ of the following classes: {class_tuple}.\")\n    280 \n    281     framework\
    \ = \"tf\" if \"keras.engine.training.Model\" in str(inspect.getmro(model.__class__))\
    \ else \"pt\"\n\nValueError: Could not load model tiiuae/falcon-7b with any of\
    \ the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\n\
    \n```"
  created_at: 2023-06-03 04:46:13+00:00
  edited: true
  hidden: false
  id: 647ad3a542abe277476a9baf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cfa230fc7971a203f1494f6dbe094170.svg
      fullname: MG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: minggnim
      type: user
    createdAt: '2023-06-06T06:11:56.000Z'
    data:
      edited: false
      editors:
      - minggnim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6347586512565613
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cfa230fc7971a203f1494f6dbe094170.svg
          fullname: MG
          isHf: false
          isPro: false
          name: minggnim
          type: user
        html: '<p>Adding this line before pipeline should fix it<br><code>model =
          AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True)</code></p>

          '
        raw: 'Adding this line before pipeline should fix it

          `model = AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True)`'
        updatedAt: '2023-06-06T06:11:56.082Z'
      numEdits: 0
      reactions: []
    id: 647ece2c2a7bcaa307935d75
    type: comment
  author: minggnim
  content: 'Adding this line before pipeline should fix it

    `model = AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True)`'
  created_at: 2023-06-06 05:11:56+00:00
  edited: false
  hidden: false
  id: 647ece2c2a7bcaa307935d75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9afb6855674631132f4041e2599cbd0e.svg
      fullname: Varun Mathur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: varun500
      type: user
    createdAt: '2023-06-06T14:52:20.000Z'
    data:
      edited: false
      editors:
      - varun500
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.31554245948791504
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9afb6855674631132f4041e2599cbd0e.svg
          fullname: Varun Mathur
          isHf: false
          isPro: false
          name: varun500
          type: user
        html: '<h2 id="while-loading-it-in-8bit-the-model-gives-this-error">While
          loading it in 8bit the model gives this error:</h2>

          <p>TypeError                                 Traceback (most recent call
          last)<br>Cell In[5], line 3<br>      1 from transformers import AutoModelForCausalLM<br>----&gt;
          3 model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-7b",trust_remote_code=True,load_in_8bit
          = True,device_map=''auto'')</p>

          <p>File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:462,
          in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,
          *model_args, **kwargs)<br>    458     class_ref = config.auto_map[cls.<strong>name</strong>]<br>    459     model_class
          = get_class_from_dynamic_module(<br>    460         class_ref, pretrained_model_name_or_path,
          **hub_kwargs, **kwargs<br>    461     )<br>--&gt; 462     return model_class.from_pretrained(<br>    463         pretrained_model_name_or_path,
          *model_args, config=config, **hub_kwargs, **kwargs<br>    464     )<br>    465
          elif type(config) in cls._model_mapping.keys():<br>    466     model_class
          = _get_model_class(config, cls._model_mapping)</p>

          <p>File /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2828,
          in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args,
          **kwargs)<br>   2826 # Dispatch model with hooks on all devices if necessary<br>   2827
          if device_map is not None:<br>-&gt; 2828     dispatch_model(model, device_map=device_map,
          offload_dir=offload_folder, offload_index=offload_index)<br>   2830 if output_loading_info:<br>   2831     if
          loading_info is None:</p>

          <p>TypeError: dispatch_model() got an unexpected keyword argument ''offload_index''</p>

          '
        raw: "While loading it in 8bit the model gives this error:\n---------------------------------------------------------------------------\n\
          TypeError                                 Traceback (most recent call last)\n\
          Cell In[5], line 3\n      1 from transformers import AutoModelForCausalLM\n\
          ----> 3 model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\"\
          ,trust_remote_code=True,load_in_8bit = True,device_map='auto')\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:462,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    458     class_ref = config.auto_map[cls.__name__]\n\
          \    459     model_class = get_class_from_dynamic_module(\n    460     \
          \    class_ref, pretrained_model_name_or_path, **hub_kwargs, **kwargs\n\
          \    461     )\n--> 462     return model_class.from_pretrained(\n    463\
          \         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
          \ **kwargs\n    464     )\n    465 elif type(config) in cls._model_mapping.keys():\n\
          \    466     model_class = _get_model_class(config, cls._model_mapping)\n\
          \nFile /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2828,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n   2826 # Dispatch model with hooks on all devices\
          \ if necessary\n   2827 if device_map is not None:\n-> 2828     dispatch_model(model,\
          \ device_map=device_map, offload_dir=offload_folder, offload_index=offload_index)\n\
          \   2830 if output_loading_info:\n   2831     if loading_info is None:\n\
          \nTypeError: dispatch_model() got an unexpected keyword argument 'offload_index'"
        updatedAt: '2023-06-06T14:52:20.971Z'
      numEdits: 0
      reactions: []
    id: 647f4824f41cf810e37f344f
    type: comment
  author: varun500
  content: "While loading it in 8bit the model gives this error:\n---------------------------------------------------------------------------\n\
    TypeError                                 Traceback (most recent call last)\n\
    Cell In[5], line 3\n      1 from transformers import AutoModelForCausalLM\n---->\
    \ 3 model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\",trust_remote_code=True,load_in_8bit\
    \ = True,device_map='auto')\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:462,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    458     class_ref = config.auto_map[cls.__name__]\n    459  \
    \   model_class = get_class_from_dynamic_module(\n    460         class_ref, pretrained_model_name_or_path,\
    \ **hub_kwargs, **kwargs\n    461     )\n--> 462     return model_class.from_pretrained(\n\
    \    463         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
    \ **kwargs\n    464     )\n    465 elif type(config) in cls._model_mapping.keys():\n\
    \    466     model_class = _get_model_class(config, cls._model_mapping)\n\nFile\
    \ /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2828,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n   2826 # Dispatch model with hooks on all devices if necessary\n\
    \   2827 if device_map is not None:\n-> 2828     dispatch_model(model, device_map=device_map,\
    \ offload_dir=offload_folder, offload_index=offload_index)\n   2830 if output_loading_info:\n\
    \   2831     if loading_info is None:\n\nTypeError: dispatch_model() got an unexpected\
    \ keyword argument 'offload_index'"
  created_at: 2023-06-06 13:52:20+00:00
  edited: false
  hidden: false
  id: 647f4824f41cf810e37f344f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T13:56:02.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9149706363677979
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>Base on similar issues in <code>accelerate</code>, you might need
          to upgrade your version of the library by running <code>pip install --upgrade
          accelerate</code>: <a rel="nofollow" href="https://github.com/huggingface/peft/issues/186">https://github.com/huggingface/peft/issues/186</a></p>

          '
        raw: 'Base on similar issues in `accelerate`, you might need to upgrade your
          version of the library by running `pip install --upgrade accelerate`: https://github.com/huggingface/peft/issues/186'
        updatedAt: '2023-06-09T13:56:02.083Z'
      numEdits: 0
      reactions: []
    id: 64832f7241ac8b8247520896
    type: comment
  author: FalconLLM
  content: 'Base on similar issues in `accelerate`, you might need to upgrade your
    version of the library by running `pip install --upgrade accelerate`: https://github.com/huggingface/peft/issues/186'
  created_at: 2023-06-09 12:56:02+00:00
  edited: false
  hidden: false
  id: 64832f7241ac8b8247520896
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cc4b74174fbae2f9eeeeb9f5588da263.svg
      fullname: G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Andi2022HH
      type: user
    createdAt: '2023-06-12T09:40:49.000Z'
    data:
      edited: false
      editors:
      - Andi2022HH
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5936368107795715
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cc4b74174fbae2f9eeeeb9f5588da263.svg
          fullname: G
          isHf: false
          isPro: false
          name: Andi2022HH
          type: user
        html: '<p>I am getting the following error where I already changed to float32:</p>

          <p>File "falcon_7b.py", line 32, in <br>    pipeline_ = pipeline("text-generation",
          model=model_folder, device=0, trust_remote_code=True, model_kwargs={"torch_dtype":
          torch.float32})<br>  File "python3.10/site-packages/transformers/pipelines/<strong>init</strong>.py",
          line 727, in pipeline<br>    framework, model = infer_framework_load_model(<br>  File
          "python3.10/site-packages/transformers/pipelines/base.py", line 266, in
          infer_framework_load_model<br>    raise ValueError(f"Could not load model
          {model} with any of the following classes: {class_tuple}.")<br>ValueError:
          Could not load model ../models/falcon_7b with any of the following classes:
          (&lt;class ''transformers.models.auto.modeling_auto.AutoModelForCausalLM''&gt;,).</p>

          '
        raw: "I am getting the following error where I already changed to float32:\n\
          \nFile \"falcon_7b.py\", line 32, in <module>\n    pipeline_ = pipeline(\"\
          text-generation\", model=model_folder, device=0, trust_remote_code=True,\
          \ model_kwargs={\"torch_dtype\": torch.float32})\n  File \"python3.10/site-packages/transformers/pipelines/__init__.py\"\
          , line 727, in pipeline\n    framework, model = infer_framework_load_model(\n\
          \  File \"python3.10/site-packages/transformers/pipelines/base.py\", line\
          \ 266, in infer_framework_load_model\n    raise ValueError(f\"Could not\
          \ load model {model} with any of the following classes: {class_tuple}.\"\
          )\nValueError: Could not load model ../models/falcon_7b with any of the\
          \ following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
        updatedAt: '2023-06-12T09:40:49.335Z'
      numEdits: 0
      reactions: []
    id: 6486e8213a3befe633f43a85
    type: comment
  author: Andi2022HH
  content: "I am getting the following error where I already changed to float32:\n\
    \nFile \"falcon_7b.py\", line 32, in <module>\n    pipeline_ = pipeline(\"text-generation\"\
    , model=model_folder, device=0, trust_remote_code=True, model_kwargs={\"torch_dtype\"\
    : torch.float32})\n  File \"python3.10/site-packages/transformers/pipelines/__init__.py\"\
    , line 727, in pipeline\n    framework, model = infer_framework_load_model(\n\
    \  File \"python3.10/site-packages/transformers/pipelines/base.py\", line 266,\
    \ in infer_framework_load_model\n    raise ValueError(f\"Could not load model\
    \ {model} with any of the following classes: {class_tuple}.\")\nValueError: Could\
    \ not load model ../models/falcon_7b with any of the following classes: (<class\
    \ 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
  created_at: 2023-06-12 08:40:49+00:00
  edited: false
  hidden: false
  id: 6486e8213a3befe633f43a85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f319794db1d0807fe9758f7186c88d06.svg
      fullname: "S\xE9bastien"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: demsking
      type: user
    createdAt: '2023-06-16T07:55:43.000Z'
    data:
      edited: false
      editors:
      - demsking
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48066580295562744
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f319794db1d0807fe9758f7186c88d06.svg
          fullname: "S\xE9bastien"
          isHf: false
          isPro: false
          name: demsking
          type: user
        html: "<p>I have similar issue even after adding:</p>\n<pre><code class=\"\
          language-py\">model = AutoModelForCausalLM.from_pretrained(model, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n</code></pre>\n<p>Error:</p>\n<pre><code\
          \ class=\"language-rs\">HFValidationError: Repo id must <span class=\"hljs-keyword\"\
          >use</span> alphanumeric chars or <span class=\"hljs-string\">'-'</span>,\
          \ <span class=\"hljs-string\">'_'</span>, <span class=\"hljs-string\">'.'</span>,\
          \ '--' and '..' are forbidden, <span class=\"hljs-string\">'-'</span> and\
          \ <span class=\"hljs-string\">'.'</span> cannot start or end the name, max\
          \ length is\n<span class=\"hljs-number\">96</span>: <span class=\"hljs-symbol\"\
          >'RWForCausalLM</span>(\n  (transformer): <span class=\"hljs-title function_\
          \ invoke__\">RWModel</span>(\n    (word_embeddings): <span class=\"hljs-title\
          \ function_ invoke__\">Embedding</span>(<span class=\"hljs-number\">65024</span>,\
          \ <span class=\"hljs-number\">4544</span>)\n    (h): <span class=\"hljs-title\
          \ function_ invoke__\">ModuleList</span>(\n      (<span class=\"hljs-number\"\
          >0</span>-<span class=\"hljs-number\">31</span>): <span class=\"hljs-number\"\
          >32</span> x <span class=\"hljs-title function_ invoke__\">DecoderLayer</span>(\n\
          \        (input_layernorm): <span class=\"hljs-title function_ invoke__\"\
          >LayerNorm</span>((<span class=\"hljs-number\">4544</span>,), eps=<span\
          \ class=\"hljs-number\">1e-05</span>, elementwise_affine=True)\n       \
          \ (self_attention): <span class=\"hljs-title function_ invoke__\">Attention</span>(\n\
          \          (maybe_rotary): <span class=\"hljs-title function_ invoke__\"\
          >RotaryEmbedding</span>()\n          (query_key_value): <span class=\"hljs-title\
          \ function_ invoke__\">Linear</span>(in_features=<span class=\"hljs-number\"\
          >4544</span>, out_features=<span class=\"hljs-number\">4672</span>, bias=False)\n\
          \          (dense): <span class=\"hljs-title function_ invoke__\">Linear</span>(in_features=<span\
          \ class=\"hljs-number\">4544</span>, out_features=<span class=\"hljs-number\"\
          >4544</span>, bias=False)\n          (attention_dropout): <span class=\"\
          hljs-title function_ invoke__\">Dropout</span>(p=<span class=\"hljs-number\"\
          >0.0</span>, inplace=False)\n        )\n        (mlp): <span class=\"hljs-title\
          \ function_ invoke__\">MLP</span>(\n          (dense_h_to_4h): <span class=\"\
          hljs-title function_ invoke__\">Linear</span>(in_features=<span class=\"\
          hljs-number\">4544</span>, out_features=<span class=\"hljs-number\">18176</span>,\
          \ bias=False)\n          (act): <span class=\"hljs-title function_ invoke__\"\
          >GELU</span>(approximate=<span class=\"hljs-symbol\">'none</span>')\n  \
          \        (dense_4h_to_h): <span class=\"hljs-title function_ invoke__\"\
          >Linear</span>(in_features=<span class=\"hljs-number\">18176</span>, out_features=<span\
          \ class=\"hljs-number\">4544</span>, bias=False)\n        )\n      )\n \
          \   )\n    (ln_f): <span class=\"hljs-title function_ invoke__\">LayerNorm</span>((<span\
          \ class=\"hljs-number\">4544</span>,), eps=<span class=\"hljs-number\">1e-05</span>,\
          \ elementwise_affine=True)\n  )\n  (lm_head): <span class=\"hljs-title function_\
          \ invoke__\">Linear</span>(in_features=<span class=\"hljs-number\">4544</span>,\
          \ out_features=<span class=\"hljs-number\">65024</span>, bias=False)\n)'.\n\
          </code></pre>\n"
        raw: "I have similar issue even after adding:\n```py\nmodel = AutoModelForCausalLM.from_pretrained(model,\
          \ trust_remote_code=True)\n```\nError:\n```rs\nHFValidationError: Repo id\
          \ must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden,\
          \ '-' and '.' cannot start or end the name, max length is\n96: 'RWForCausalLM(\n\
          \  (transformer): RWModel(\n    (word_embeddings): Embedding(65024, 4544)\n\
          \    (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n        (input_layernorm):\
          \ LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n        (self_attention):\
          \ Attention(\n          (maybe_rotary): RotaryEmbedding()\n          (query_key_value):\
          \ Linear(in_features=4544, out_features=4672, bias=False)\n          (dense):\
          \ Linear(in_features=4544, out_features=4544, bias=False)\n          (attention_dropout):\
          \ Dropout(p=0.0, inplace=False)\n        )\n        (mlp): MLP(\n      \
          \    (dense_h_to_4h): Linear(in_features=4544, out_features=18176, bias=False)\n\
          \          (act): GELU(approximate='none')\n          (dense_4h_to_h): Linear(in_features=18176,\
          \ out_features=4544, bias=False)\n        )\n      )\n    )\n    (ln_f):\
          \ LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head):\
          \ Linear(in_features=4544, out_features=65024, bias=False)\n)'.\n```"
        updatedAt: '2023-06-16T07:55:43.302Z'
      numEdits: 0
      reactions: []
    id: 648c157fcaf726309384dd13
    type: comment
  author: demsking
  content: "I have similar issue even after adding:\n```py\nmodel = AutoModelForCausalLM.from_pretrained(model,\
    \ trust_remote_code=True)\n```\nError:\n```rs\nHFValidationError: Repo id must\
    \ use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and\
    \ '.' cannot start or end the name, max length is\n96: 'RWForCausalLM(\n  (transformer):\
    \ RWModel(\n    (word_embeddings): Embedding(65024, 4544)\n    (h): ModuleList(\n\
    \      (0-31): 32 x DecoderLayer(\n        (input_layernorm): LayerNorm((4544,),\
    \ eps=1e-05, elementwise_affine=True)\n        (self_attention): Attention(\n\
    \          (maybe_rotary): RotaryEmbedding()\n          (query_key_value): Linear(in_features=4544,\
    \ out_features=4672, bias=False)\n          (dense): Linear(in_features=4544,\
    \ out_features=4544, bias=False)\n          (attention_dropout): Dropout(p=0.0,\
    \ inplace=False)\n        )\n        (mlp): MLP(\n          (dense_h_to_4h): Linear(in_features=4544,\
    \ out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n\
    \          (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)\n\
    \        )\n      )\n    )\n    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n\
    \  )\n  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n\
    )'.\n```"
  created_at: 2023-06-16 06:55:43+00:00
  edited: false
  hidden: false
  id: 648c157fcaf726309384dd13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/adLUysfQCBOaoYk8LVGBD.jpeg?w=200&h=200&f=face
      fullname: JEREMY D GAMET
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: archonlith
      type: user
    createdAt: '2023-08-14T15:56:36.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/adLUysfQCBOaoYk8LVGBD.jpeg?w=200&h=200&f=face
          fullname: JEREMY D GAMET
          isHf: false
          isPro: false
          name: archonlith
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-08-14T19:10:46.654Z'
      numEdits: 2
      reactions: []
    id: 64da4eb470446182be532ea7
    type: comment
  author: archonlith
  content: This comment has been hidden
  created_at: 2023-08-14 14:56:36+00:00
  edited: true
  hidden: true
  id: 64da4eb470446182be532ea7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: Error loading tiiuae/falcon-7b
