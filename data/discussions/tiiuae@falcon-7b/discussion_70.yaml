!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Akshadv
conflicting_files: null
created_at: 2023-07-20 03:44:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/97d4a44a3c5e4e8521dcb717193f33bb.svg
      fullname: akshad verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Akshadv
      type: user
    createdAt: '2023-07-20T04:44:58.000Z'
    data:
      edited: true
      editors:
      - Akshadv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5354806780815125
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/97d4a44a3c5e4e8521dcb717193f33bb.svg
          fullname: akshad verma
          isHf: false
          isPro: false
          name: Akshadv
          type: user
        html: '<p>tokenizer = AutoTokenizer.from_pretrained(model_path)</p>

          <p>falcon_pipeline = pipeline(<br>    "text-generation",<br>    model=model_path,<br>    tokenizer=tokenizer,<br>    max_new_tokens=256,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>    device_map
          = ''auto'',<br>    do_sample=True,<br>    top_k = 10,<br>    temperature=0.7,<br>    eos_token_id=tokenizer.eos_token_id<br>)</p>

          <p>using this code + llmchain for inference am i doing something wrong or
          any thing needs to be fixed to get full inference on gpu?</p>

          <p>CPU is always hitting 100%</p>

          '
        raw: "tokenizer = AutoTokenizer.from_pretrained(model_path)\n\nfalcon_pipeline\
          \ = pipeline(\n    \"text-generation\",\n    model=model_path,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=256,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map = 'auto',\n    do_sample=True,\n    top_k = 10,\n    temperature=0.7,\n\
          \    eos_token_id=tokenizer.eos_token_id\n)\n\nusing this code + llmchain\
          \ for inference am i doing something wrong or any thing needs to be fixed\
          \ to get full inference on gpu?\n\nCPU is always hitting 100%\n"
        updatedAt: '2023-07-20T04:45:40.090Z'
      numEdits: 1
      reactions: []
    id: 64b8bbca0de7289de36f198f
    type: comment
  author: Akshadv
  content: "tokenizer = AutoTokenizer.from_pretrained(model_path)\n\nfalcon_pipeline\
    \ = pipeline(\n    \"text-generation\",\n    model=model_path,\n    tokenizer=tokenizer,\n\
    \    max_new_tokens=256,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
    \    device_map = 'auto',\n    do_sample=True,\n    top_k = 10,\n    temperature=0.7,\n\
    \    eos_token_id=tokenizer.eos_token_id\n)\n\nusing this code + llmchain for\
    \ inference am i doing something wrong or any thing needs to be fixed to get full\
    \ inference on gpu?\n\nCPU is always hitting 100%\n"
  created_at: 2023-07-20 03:44:58+00:00
  edited: true
  hidden: false
  id: 64b8bbca0de7289de36f198f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 70
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: Falcon 7b instruct using cpu for inference even on NVIDIA A40 cards with 50GB
  VRAM
