!!python/object:huggingface_hub.community.DiscussionWithDetails
author: karolzak13
conflicting_files: null
created_at: 2023-08-10 13:42:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e96e1445d69700cf27536a4591bee39d.svg
      fullname: Karol Zak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: karolzak13
      type: user
    createdAt: '2023-08-10T14:42:01.000Z'
    data:
      edited: false
      editors:
      - karolzak13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7058601975440979
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e96e1445d69700cf27536a4591bee39d.svg
          fullname: Karol Zak
          isHf: false
          isPro: false
          name: karolzak13
          type: user
        html: "<p>Hey all!<br>I really wanted to try out one of Falcon models with\
          \ question answering task so I followed this tutorial in the docs: <a href=\"\
          https://huggingface.co/docs/transformers/tasks/question_answering\">Question\
          \ answering (huggingface.co)</a> since on the top of it it says it's applicable\
          \ with Falcon models.<br>So I reused the code and switch distilbert model\
          \ to falcon-7b and my code fails this error message:</p>\n<pre><code>TypeError:\
          \ forward() got an unexpected keyword argument 'token_type_ids'\n</code></pre>\n\
          <p>My code looks like this:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoModelForQuestionAnswering\n\nmodel_name = <span class=\"\
          hljs-string\">\"tiiuae/falcon-7b\"</span>\nmodel = AutoModelForQuestionAnswering.from_pretrained(\n\
          \    pretrained_model_name_or_path=model_name,\n    cache_dir=<span class=\"\
          hljs-string\">\"/mnt/tmp\"</span>,\n    trust_remote_code=<span class=\"\
          hljs-literal\">True</span>\n)\n\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ PretrainedConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\n    pretrained_model_name_or_path=model_name,\n\
          \    cache_dir=<span class=\"hljs-string\">\"/mnt/tmp\"</span>,\n    trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>,\n    return_tensors=<span class=\"\
          hljs-string\">\"pt\"</span>,\n)\n\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> pipeline\n\nquestion_answerer\
          \ = pipeline(\n    <span class=\"hljs-string\">\"question-answering\"</span>,\n\
          \    model=model,\n    tokenizer=tokenizer\n)\n\nquestion_answerer(question=question,\
          \ context=context)\n</code></pre>\n<p>Code fails on the very last step when\
          \ I try to run inferencing on the pipeline. Any ideas?</p>\n"
        raw: "Hey all!\r\nI really wanted to try out one of Falcon models with question\
          \ answering task so I followed this tutorial in the docs: [Question answering\
          \ (huggingface.co)](https://huggingface.co/docs/transformers/tasks/question_answering)\
          \ since on the top of it it says it's applicable with Falcon models.\r\n\
          So I reused the code and switch distilbert model to falcon-7b and my code\
          \ fails this error message:\r\n```\r\nTypeError: forward() got an unexpected\
          \ keyword argument 'token_type_ids'\r\n```\r\nMy code looks like this:\r\
          \n```python\r\nfrom transformers import AutoModelForQuestionAnswering\r\n\
          \r\nmodel_name = \"tiiuae/falcon-7b\"\r\nmodel = AutoModelForQuestionAnswering.from_pretrained(\r\
          \n    pretrained_model_name_or_path=model_name,\r\n    cache_dir=\"/mnt/tmp\"\
          ,\r\n    trust_remote_code=True\r\n)\r\n\r\nfrom transformers import AutoTokenizer,\
          \ PretrainedConfig\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\r\n\
          \    pretrained_model_name_or_path=model_name,\r\n    cache_dir=\"/mnt/tmp\"\
          ,\r\n    trust_remote_code=True,\r\n    return_tensors=\"pt\",\r\n)\r\n\r\
          \nfrom transformers import pipeline\r\n\r\nquestion_answerer = pipeline(\r\
          \n    \"question-answering\",\r\n    model=model,\r\n    tokenizer=tokenizer\r\
          \n)\r\n\r\nquestion_answerer(question=question, context=context)\r\n```\r\
          \n\r\nCode fails on the very last step when I try to run inferencing on\
          \ the pipeline. Any ideas?"
        updatedAt: '2023-08-10T14:42:01.192Z'
      numEdits: 0
      reactions: []
    id: 64d4f739c8d03cca8f6b2185
    type: comment
  author: karolzak13
  content: "Hey all!\r\nI really wanted to try out one of Falcon models with question\
    \ answering task so I followed this tutorial in the docs: [Question answering\
    \ (huggingface.co)](https://huggingface.co/docs/transformers/tasks/question_answering)\
    \ since on the top of it it says it's applicable with Falcon models.\r\nSo I reused\
    \ the code and switch distilbert model to falcon-7b and my code fails this error\
    \ message:\r\n```\r\nTypeError: forward() got an unexpected keyword argument 'token_type_ids'\r\
    \n```\r\nMy code looks like this:\r\n```python\r\nfrom transformers import AutoModelForQuestionAnswering\r\
    \n\r\nmodel_name = \"tiiuae/falcon-7b\"\r\nmodel = AutoModelForQuestionAnswering.from_pretrained(\r\
    \n    pretrained_model_name_or_path=model_name,\r\n    cache_dir=\"/mnt/tmp\"\
    ,\r\n    trust_remote_code=True\r\n)\r\n\r\nfrom transformers import AutoTokenizer,\
    \ PretrainedConfig\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\r\n    pretrained_model_name_or_path=model_name,\r\
    \n    cache_dir=\"/mnt/tmp\",\r\n    trust_remote_code=True,\r\n    return_tensors=\"\
    pt\",\r\n)\r\n\r\nfrom transformers import pipeline\r\n\r\nquestion_answerer =\
    \ pipeline(\r\n    \"question-answering\",\r\n    model=model,\r\n    tokenizer=tokenizer\r\
    \n)\r\n\r\nquestion_answerer(question=question, context=context)\r\n```\r\n\r\n\
    Code fails on the very last step when I try to run inferencing on the pipeline.\
    \ Any ideas?"
  created_at: 2023-08-10 13:42:01+00:00
  edited: false
  hidden: false
  id: 64d4f739c8d03cca8f6b2185
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
      fullname: Saptarshi Sengupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saptarshi7
      type: user
    createdAt: '2023-09-12T18:43:35.000Z'
    data:
      edited: false
      editors:
      - Saptarshi7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8023778200149536
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
          fullname: Saptarshi Sengupta
          isHf: false
          isPro: false
          name: Saptarshi7
          type: user
        html: "<p>I'm not sure you would be able to use HF pipeline for Falcon for\
          \ this kind of QA since Falcon is a decoder-only model not expected to produce\
          \ start/end logits. Do you get any errors when you run?</p>\n<pre><code>model\
          \ = AutoModelForQuestionAnswering.from_pretrained(\n    pretrained_model_name_or_path=model_name,\n\
          \    cache_dir=\"/mnt/tmp\",\n    trust_remote_code=True\n)\n</code></pre>\n"
        raw: "I'm not sure you would be able to use HF pipeline for Falcon for this\
          \ kind of QA since Falcon is a decoder-only model not expected to produce\
          \ start/end logits. Do you get any errors when you run?\n```\nmodel = AutoModelForQuestionAnswering.from_pretrained(\n\
          \    pretrained_model_name_or_path=model_name,\n    cache_dir=\"/mnt/tmp\"\
          ,\n    trust_remote_code=True\n)\n```"
        updatedAt: '2023-09-12T18:43:35.139Z'
      numEdits: 0
      reactions: []
    id: 6500b1575d76377562c14f8c
    type: comment
  author: Saptarshi7
  content: "I'm not sure you would be able to use HF pipeline for Falcon for this\
    \ kind of QA since Falcon is a decoder-only model not expected to produce start/end\
    \ logits. Do you get any errors when you run?\n```\nmodel = AutoModelForQuestionAnswering.from_pretrained(\n\
    \    pretrained_model_name_or_path=model_name,\n    cache_dir=\"/mnt/tmp\",\n\
    \    trust_remote_code=True\n)\n```"
  created_at: 2023-09-12 17:43:35+00:00
  edited: false
  hidden: false
  id: 6500b1575d76377562c14f8c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 75
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: 'Question answering task with falcon model fails with "TypeError: forward()
  got an unexpected keyword argument ''token_type_ids''"'
