!!python/object:huggingface_hub.community.DiscussionWithDetails
author: chintan4560
conflicting_files: null
created_at: 2023-07-13 07:07:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd6c6178b7400643520d9dad4fc75d32.svg
      fullname: Chintan Bhavsar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chintan4560
      type: user
    createdAt: '2023-07-13T08:07:06.000Z'
    data:
      edited: false
      editors:
      - chintan4560
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8265050053596497
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd6c6178b7400643520d9dad4fc75d32.svg
          fullname: Chintan Bhavsar
          isHf: false
          isPro: false
          name: chintan4560
          type: user
        html: '<p>When I used finetuning code for Falcon using this model_name: "ybelkada/falcon-7b-sharded-bf16".<br>I
          am getting this error: OSError: tiiuae/falcon-7b does not appear to have
          a file named configuration_RW.py</p>

          <p>Please help me to resolve this.</p>

          '
        raw: "When I used finetuning code for Falcon using this model_name: \"ybelkada/falcon-7b-sharded-bf16\"\
          . \r\nI am getting this error: OSError: tiiuae/falcon-7b does not appear\
          \ to have a file named configuration_RW.py\r\n\r\nPlease help me to resolve\
          \ this."
        updatedAt: '2023-07-13T08:07:06.977Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - prakash8486
        - AmlanSamanta
        - usholanb
    id: 64afb0aa73151ec37ef102f1
    type: comment
  author: chintan4560
  content: "When I used finetuning code for Falcon using this model_name: \"ybelkada/falcon-7b-sharded-bf16\"\
    . \r\nI am getting this error: OSError: tiiuae/falcon-7b does not appear to have\
    \ a file named configuration_RW.py\r\n\r\nPlease help me to resolve this."
  created_at: 2023-07-13 07:07:06+00:00
  edited: false
  hidden: false
  id: 64afb0aa73151ec37ef102f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/-LeCTMuzJuBJDK80GRy76.jpeg?w=200&h=200&f=face
      fullname: Amlan Samanta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AmlanSamanta
      type: user
    createdAt: '2023-07-13T17:36:46.000Z'
    data:
      edited: true
      editors:
      - AmlanSamanta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9971093535423279
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/-LeCTMuzJuBJDK80GRy76.jpeg?w=200&h=200&f=face
          fullname: Amlan Samanta
          isHf: false
          isPro: false
          name: AmlanSamanta
          type: user
        html: '<p>Same issue faced by me. This commit is very disappointing. I think
          we can think of 2 below workarounds </p>

          <p>A. Can we use the missing file from local storage where the script is
          executing from?</p>

          <p>B. Also if  can  load a previous revision of a model commit in the above
          code. Then I think we can bypass this.</p>

          '
        raw: "Same issue faced by me. This commit is very disappointing. I think we\
          \ can think of 2 below workarounds \n\nA. Can we use the missing file from\
          \ local storage where the script is executing from?\n\nB. Also if  can \
          \ load a previous revision of a model commit in the above code. Then I think\
          \ we can bypass this."
        updatedAt: '2023-07-13T17:44:11.397Z'
      numEdits: 1
      reactions: []
    id: 64b0362eb02b95456daeaf8a
    type: comment
  author: AmlanSamanta
  content: "Same issue faced by me. This commit is very disappointing. I think we\
    \ can think of 2 below workarounds \n\nA. Can we use the missing file from local\
    \ storage where the script is executing from?\n\nB. Also if  can  load a previous\
    \ revision of a model commit in the above code. Then I think we can bypass this."
  created_at: 2023-07-13 16:36:46+00:00
  edited: true
  hidden: false
  id: 64b0362eb02b95456daeaf8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
      fullname: Sylvain Gugger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sgugger
      type: user
    createdAt: '2023-07-13T18:38:28.000Z'
    data:
      edited: false
      editors:
      - sgugger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8256796002388
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1593126474392-5ef50182b71947201082a4e5.jpeg?w=200&h=200&f=face
          fullname: Sylvain Gugger
          isHf: false
          isPro: false
          name: sgugger
          type: user
        html: '<p>You can use the <code>code_revision</code> argument to specify a
          specific revision (so commit for instance) for the code files. Since we
          are migrating this model into Transformers, it''s probably safer to do this
          until the integration is finished when using <code>"ybelkada/falcon-7b-sharded-bf16"</code>.</p>

          '
        raw: You can use the `code_revision` argument to specify a specific revision
          (so commit for instance) for the code files. Since we are migrating this
          model into Transformers, it's probably safer to do this until the integration
          is finished when using `"ybelkada/falcon-7b-sharded-bf16"`.
        updatedAt: '2023-07-13T18:38:28.284Z'
      numEdits: 0
      reactions: []
    id: 64b044a44eba44dac2309511
    type: comment
  author: sgugger
  content: You can use the `code_revision` argument to specify a specific revision
    (so commit for instance) for the code files. Since we are migrating this model
    into Transformers, it's probably safer to do this until the integration is finished
    when using `"ybelkada/falcon-7b-sharded-bf16"`.
  created_at: 2023-07-13 17:38:28+00:00
  edited: false
  hidden: false
  id: 64b044a44eba44dac2309511
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/-LeCTMuzJuBJDK80GRy76.jpeg?w=200&h=200&f=face
      fullname: Amlan Samanta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AmlanSamanta
      type: user
    createdAt: '2023-07-14T02:12:54.000Z'
    data:
      edited: false
      editors:
      - AmlanSamanta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4745153486728668
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641c00cc999935676ec73f9c/-LeCTMuzJuBJDK80GRy76.jpeg?w=200&h=200&f=face
          fullname: Amlan Samanta
          isHf: false
          isPro: false
          name: AmlanSamanta
          type: user
        html: '<p>I also tried similar approach biut failed yesterday. May I know
          in which line I can use the argument you highlighted. This is my code.</p>

          <p>import torch<br>from peft import PeftModel, PeftConfig<br>from transformers
          import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig</p>

          <p>peft_model_id = "mrm8488/falcon-7b-ft-codeAlpaca_20k-v2"<br>config =
          PeftConfig.from_pretrained(peft_model_id)<br>model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,
          return_dict=True, load_in_8bit=True, device_map = {"":0}, trust_remote_code=True)<br>tokenizer
          = AutoTokenizer.from_pretrained(peft_model_id)</p>

          <p>model = PeftModel.from_pretrained(model, peft_model_id)<br>model.eval()</p>

          '
        raw: 'I also tried similar approach biut failed yesterday. May I know in which
          line I can use the argument you highlighted. This is my code.


          import torch

          from peft import PeftModel, PeftConfig

          from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,
          GenerationConfig


          peft_model_id = "mrm8488/falcon-7b-ft-codeAlpaca_20k-v2"

          config = PeftConfig.from_pretrained(peft_model_id)

          model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,
          return_dict=True, load_in_8bit=True, device_map = {"":0}, trust_remote_code=True)

          tokenizer = AutoTokenizer.from_pretrained(peft_model_id)


          model = PeftModel.from_pretrained(model, peft_model_id)

          model.eval()'
        updatedAt: '2023-07-14T02:12:54.732Z'
      numEdits: 0
      reactions: []
    id: 64b0af262a98a6d649499064
    type: comment
  author: AmlanSamanta
  content: 'I also tried similar approach biut failed yesterday. May I know in which
    line I can use the argument you highlighted. This is my code.


    import torch

    from peft import PeftModel, PeftConfig

    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,
    GenerationConfig


    peft_model_id = "mrm8488/falcon-7b-ft-codeAlpaca_20k-v2"

    config = PeftConfig.from_pretrained(peft_model_id)

    model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True,
    load_in_8bit=True, device_map = {"":0}, trust_remote_code=True)

    tokenizer = AutoTokenizer.from_pretrained(peft_model_id)


    model = PeftModel.from_pretrained(model, peft_model_id)

    model.eval()'
  created_at: 2023-07-14 01:12:54+00:00
  edited: false
  hidden: false
  id: 64b0af262a98a6d649499064
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b0eb728d12064ba4115733383dee48b.svg
      fullname: Amin Harig
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: amiiin
      type: user
    createdAt: '2023-10-02T12:56:54.000Z'
    data:
      edited: false
      editors:
      - amiiin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8409533500671387
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b0eb728d12064ba4115733383dee48b.svg
          fullname: Amin Harig
          isHf: false
          isPro: false
          name: amiiin
          type: user
        html: '<p>I got this same error again today</p>

          '
        raw: I got this same error again today
        updatedAt: '2023-10-02T12:56:54.543Z'
      numEdits: 0
      reactions: []
    id: 651abe1669d3438f0f258a88
    type: comment
  author: amiiin
  content: I got this same error again today
  created_at: 2023-10-02 11:56:54+00:00
  edited: false
  hidden: false
  id: 651abe1669d3438f0f258a88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/626399fd2385ab7445b1ddba7c3f56cf.svg
      fullname: hard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Draculhard
      type: user
    createdAt: '2024-01-06T11:02:03.000Z'
    data:
      edited: false
      editors:
      - Draculhard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6601990461349487
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/626399fd2385ab7445b1ddba7c3f56cf.svg
          fullname: hard
          isHf: false
          isPro: false
          name: Draculhard
          type: user
        html: '<p>am still getting the error, can someone please help me out here
          is my code<br>pipeline = transformers.pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    torch_dtype=torch.bfloat16,<br>    trust_remote_code=True,<br>    device_map="auto",<br>)</p>

          '
        raw: "am still getting the error, can someone please help me out here is my\
          \ code \npipeline = transformers.pipeline(\n    \"text-generation\",\n \
          \   model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\",\n)\n"
        updatedAt: '2024-01-06T11:02:03.718Z'
      numEdits: 0
      reactions: []
    id: 6599332b07198ffcf795d9a5
    type: comment
  author: Draculhard
  content: "am still getting the error, can someone please help me out here is my\
    \ code \npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
    \    device_map=\"auto\",\n)\n"
  created_at: 2024-01-06 11:02:03+00:00
  edited: false
  hidden: false
  id: 6599332b07198ffcf795d9a5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 62
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: 'OSError: tiiuae/falcon-7b does not appear to have a file named configuration_RW.py'
