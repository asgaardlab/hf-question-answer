!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yassin-sameh
conflicting_files: null
created_at: 2023-06-23 07:46:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34f0cd0b759a7755aa34c91231eaa10b.svg
      fullname: Yassin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yassin-sameh
      type: user
    createdAt: '2023-06-23T08:46:27.000Z'
    data:
      edited: true
      editors:
      - Yassin-sameh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40660759806632996
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34f0cd0b759a7755aa34c91231eaa10b.svg
          fullname: Yassin
          isHf: false
          isPro: false
          name: Yassin-sameh
          type: user
        html: "<p>I have been unable to get any output from the model. Running on\
          \ a Azure Notebook with a Compute instance Standard_E4ds_v4, 4 core, 32GB.<br>Any\
          \ assistance is appreciated.</p>\n<p>Code:</p>\n<pre><code>!source activate\
          \ llm_env\n\n%pip install conda\nimport conda\n%conda install cudatoolkit\n\
          \n%pip install torch\n%pip install einops\n%pip install accelerate\n%pip\
          \ install transformers==4.27.4\n%pip install huggingface-hub\n%pip install\
          \ chardet\n%pip install cchardet\n\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM, TFAutoModelForCausalLM\nimport transformers\nimport\
          \ torch\n\nmodel = \"tiiuae/falcon-7b\"\nrrmodel = AutoModelForCausalLM.from_pretrained(model,\
          \ \n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",)\ntokenizer = AutoTokenizer.from_pretrained(model)\n\ninput_text\
          \ = \"What is a giraffe?\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\n\
          \nattention_mask = torch.ones(input_ids.shape)\noutput = rrmodel.generate(input_ids,\
          \ \n            attention_mask=attention_mask, \n            max_length=2000,\n\
          \            do_sample=True,\n            pad_token_id = 50256,\n      \
          \      top_k=10,\n            num_return_sequences=1,\n            eos_token_id=tokenizer.eos_token_id,)\n\
          #Never goes into this section\nprint(f\"Got output: {output}\")\noutput_text\
          \ = tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(output_text)\n\
          </code></pre>\n"
        raw: "I have been unable to get any output from the model. Running on a Azure\
          \ Notebook with a Compute instance Standard_E4ds_v4, 4 core, 32GB.\nAny\
          \ assistance is appreciated.\n\nCode:\n\n```\n!source activate llm_env\n\
          \n%pip install conda\nimport conda\n%conda install cudatoolkit\n\n%pip install\
          \ torch\n%pip install einops\n%pip install accelerate\n%pip install transformers==4.27.4\n\
          %pip install huggingface-hub\n%pip install chardet\n%pip install cchardet\n\
          \nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TFAutoModelForCausalLM\n\
          import transformers\nimport torch\n\nmodel = \"tiiuae/falcon-7b\"\nrrmodel\
          \ = AutoModelForCausalLM.from_pretrained(model, \n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\",)\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          \ninput_text = \"What is a giraffe?\"\ninput_ids = tokenizer.encode(input_text,\
          \ return_tensors='pt')\n\nattention_mask = torch.ones(input_ids.shape)\n\
          output = rrmodel.generate(input_ids, \n            attention_mask=attention_mask,\
          \ \n            max_length=2000,\n            do_sample=True,\n        \
          \    pad_token_id = 50256,\n            top_k=10,\n            num_return_sequences=1,\n\
          \            eos_token_id=tokenizer.eos_token_id,)\n#Never goes into this\
          \ section\nprint(f\"Got output: {output}\")\noutput_text = tokenizer.decode(output[0],\
          \ skip_special_tokens=True)\n\nprint(output_text)\n```\n\n"
        updatedAt: '2023-06-23T08:49:18.499Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - Yassin-sameh
        - everglow617
      - count: 1
        reaction: "\U0001F44D"
        users:
        - samutamm
    id: 64955be344fda58902333243
    type: comment
  author: Yassin-sameh
  content: "I have been unable to get any output from the model. Running on a Azure\
    \ Notebook with a Compute instance Standard_E4ds_v4, 4 core, 32GB.\nAny assistance\
    \ is appreciated.\n\nCode:\n\n```\n!source activate llm_env\n\n%pip install conda\n\
    import conda\n%conda install cudatoolkit\n\n%pip install torch\n%pip install einops\n\
    %pip install accelerate\n%pip install transformers==4.27.4\n%pip install huggingface-hub\n\
    %pip install chardet\n%pip install cchardet\n\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM, TFAutoModelForCausalLM\nimport transformers\nimport torch\n\
    \nmodel = \"tiiuae/falcon-7b\"\nrrmodel = AutoModelForCausalLM.from_pretrained(model,\
    \ \n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
    auto\",)\ntokenizer = AutoTokenizer.from_pretrained(model)\n\ninput_text = \"\
    What is a giraffe?\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\n\
    \nattention_mask = torch.ones(input_ids.shape)\noutput = rrmodel.generate(input_ids,\
    \ \n            attention_mask=attention_mask, \n            max_length=2000,\n\
    \            do_sample=True,\n            pad_token_id = 50256,\n            top_k=10,\n\
    \            num_return_sequences=1,\n            eos_token_id=tokenizer.eos_token_id,)\n\
    #Never goes into this section\nprint(f\"Got output: {output}\")\noutput_text =\
    \ tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(output_text)\n\
    ```\n\n"
  created_at: 2023-06-23 07:46:27+00:00
  edited: true
  hidden: false
  id: 64955be344fda58902333243
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 45
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: No Output is generated, Running on Cloud
