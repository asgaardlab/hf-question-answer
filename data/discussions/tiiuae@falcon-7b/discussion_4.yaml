!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sloba
conflicting_files: null
created_at: 2023-05-28 08:59:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
      fullname: Slobodan Ninkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sloba
      type: user
    createdAt: '2023-05-28T09:59:42.000Z'
    data:
      edited: true
      editors:
      - Sloba
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
          fullname: Slobodan Ninkov
          isHf: false
          isPro: false
          name: Sloba
          type: user
        html: "<ol>\n<li><p>git clone <a href=\"https://huggingface.co/tiiuae/falcon-7b\"\
          >https://huggingface.co/tiiuae/falcon-7b</a> -&gt; Saves into local directory</p>\n\
          </li>\n<li><p>Create new anaconda environment with Transformers=4.27.4 and\
          \ python=3.9<br>a) conda create --name falcon python=3.9<br>b) conda activate\
          \ falcon<br>c) pip install transformers==4.27.4<br>d) pip install huggingface-hub<br>e)\
          \ pip install chardet<br>f) pip install cchardet<br>g) pip install torch<br>h)\
          \ pip install einops<br>i) pip install accelerate<br>j) conda install cudatoolkit</p>\n\
          </li>\n<li><p>Following code finally gave results:</p>\n</li>\n</ol>\n<pre><code>from\
          \ transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
          import torch\n</code></pre>\n<pre><code>model = (local windows path to directory\
          \ (i.e. \"X:\\\\ai\\\\falcon-7b\") where is config.json pulled in step 1.\n\
          \nrrmodel = AutoModelForCausalLM.from_pretrained(model, \n    torch_dtype=torch.bfloat16,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\",)\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          \n\ninput_text = \"Once upon a time\"\ninput_ids = tokenizer.encode(input_text,\
          \ return_tensors='pt')\n</code></pre>\n<h1 id=\"generate-text\">Generate\
          \ text</h1>\n<pre><code>attention_mask = torch.ones(input_ids.shape)\n\n\
          output = rrmodel.generate(input_ids, \n            attention_mask=attention_mask,\
          \ \n            max_length=200,\n            do_sample=True,\n         \
          \   top_k=10,\n            num_return_sequences=1,\n            eos_token_id=tokenizer.eos_token_id,)\n\
          </code></pre>\n<h1 id=\"decode-the-output\">Decode the output</h1>\n<pre><code>output_text\
          \ = tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(output_text)\n\
          </code></pre>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n\
          <blockquote>\n<blockquote>\n<blockquote>\n<p>output &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>Once\
          \ upon a time, a man named Charlie Brown walked into a candy store. He asked\
          \ the lady behind the counter if she had any good chocolate. The lady said\
          \ that she had some very good chocolate.<br>Charlie Brown said, \"That sounds\
          \ good. Can you give me a pound of it?\"<br>The lady said, \"Sure,\" and\
          \ she put the pound of chocolate in a bag and rang up the sale.<br>Charlie\
          \ Brown said, \"That's $7.<br>&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; output &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</p>\n\
          </blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n\
          </blockquote>\n</blockquote>\n"
        raw: "1. git clone https://huggingface.co/tiiuae/falcon-7b -> Saves into local\
          \ directory\n2. Create new anaconda environment with Transformers=4.27.4\
          \ and python=3.9\na) conda create --name falcon python=3.9\nb) conda activate\
          \ falcon\nc) pip install transformers==4.27.4\nd) pip install huggingface-hub\n\
          e) pip install chardet\nf) pip install cchardet\ng) pip install torch\n\
          h) pip install einops\ni) pip install accelerate\nj) conda install cudatoolkit\n\
          \n3. Following code finally gave results:\n```\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\
          ```\n```\nmodel = (local windows path to directory (i.e. \"X:\\\\ai\\\\\
          falcon-7b\") where is config.json pulled in step 1.\n\nrrmodel = AutoModelForCausalLM.from_pretrained(model,\
          \ \n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",)\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\n\ninput_text\
          \ = \"Once upon a time\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\n\
          ```\n# Generate text\n```\nattention_mask = torch.ones(input_ids.shape)\n\
          \noutput = rrmodel.generate(input_ids, \n            attention_mask=attention_mask,\
          \ \n            max_length=200,\n            do_sample=True,\n         \
          \   top_k=10,\n            num_return_sequences=1,\n            eos_token_id=tokenizer.eos_token_id,)\n\
          ```\n# Decode the output\n```\noutput_text = tokenizer.decode(output[0],\
          \ skip_special_tokens=True)\n\nprint(output_text)\n```\n>>>>>>> output >>>>>>>>>>\n\
          Once upon a time, a man named Charlie Brown walked into a candy store. He\
          \ asked the lady behind the counter if she had any good chocolate. The lady\
          \ said that she had some very good chocolate.\nCharlie Brown said, \"That\
          \ sounds good. Can you give me a pound of it?\"\nThe lady said, \"Sure,\"\
          \ and she put the pound of chocolate in a bag and rang up the sale.\nCharlie\
          \ Brown said, \"That's $7.\n<<<<<<<< output <<<<<<<<"
        updatedAt: '2023-05-28T15:23:57.355Z'
      numEdits: 3
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Pimentell
        - LV1871
        - LauG
    id: 6473260e0da364bd0dfba707
    type: comment
  author: Sloba
  content: "1. git clone https://huggingface.co/tiiuae/falcon-7b -> Saves into local\
    \ directory\n2. Create new anaconda environment with Transformers=4.27.4 and python=3.9\n\
    a) conda create --name falcon python=3.9\nb) conda activate falcon\nc) pip install\
    \ transformers==4.27.4\nd) pip install huggingface-hub\ne) pip install chardet\n\
    f) pip install cchardet\ng) pip install torch\nh) pip install einops\ni) pip install\
    \ accelerate\nj) conda install cudatoolkit\n\n3. Following code finally gave results:\n\
    ```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
    import torch\n```\n```\nmodel = (local windows path to directory (i.e. \"X:\\\\\
    ai\\\\falcon-7b\") where is config.json pulled in step 1.\n\nrrmodel = AutoModelForCausalLM.from_pretrained(model,\
    \ \n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
    auto\",)\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\n\ninput_text =\
    \ \"Once upon a time\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\n\
    ```\n# Generate text\n```\nattention_mask = torch.ones(input_ids.shape)\n\noutput\
    \ = rrmodel.generate(input_ids, \n            attention_mask=attention_mask, \n\
    \            max_length=200,\n            do_sample=True,\n            top_k=10,\n\
    \            num_return_sequences=1,\n            eos_token_id=tokenizer.eos_token_id,)\n\
    ```\n# Decode the output\n```\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\
    \nprint(output_text)\n```\n>>>>>>> output >>>>>>>>>>\nOnce upon a time, a man\
    \ named Charlie Brown walked into a candy store. He asked the lady behind the\
    \ counter if she had any good chocolate. The lady said that she had some very\
    \ good chocolate.\nCharlie Brown said, \"That sounds good. Can you give me a pound\
    \ of it?\"\nThe lady said, \"Sure,\" and she put the pound of chocolate in a bag\
    \ and rang up the sale.\nCharlie Brown said, \"That's $7.\n<<<<<<<< output <<<<<<<<"
  created_at: 2023-05-28 08:59:42+00:00
  edited: true
  hidden: false
  id: 6473260e0da364bd0dfba707
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb7c74a381fd283e338c25e7100bcf0e.svg
      fullname: Max Russe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xorat
      type: user
    createdAt: '2023-05-28T15:12:26.000Z'
    data:
      edited: false
      editors:
      - Xorat
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb7c74a381fd283e338c25e7100bcf0e.svg
          fullname: Max Russe
          isHf: false
          isPro: false
          name: Xorat
          type: user
        html: '<p>[attention_mask?](NameError: name ''attention_mask'' is not defined)
          ?</p>

          '
        raw: '[attention_mask?](NameError: name ''attention_mask'' is not defined)
          ?'
        updatedAt: '2023-05-28T15:12:26.633Z'
      numEdits: 0
      reactions: []
    id: 64736f5a63001a0002cba4d4
    type: comment
  author: Xorat
  content: '[attention_mask?](NameError: name ''attention_mask'' is not defined) ?'
  created_at: 2023-05-28 14:12:26+00:00
  edited: false
  hidden: false
  id: 64736f5a63001a0002cba4d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
      fullname: Slobodan Ninkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sloba
      type: user
    createdAt: '2023-05-28T15:25:02.000Z'
    data:
      edited: false
      editors:
      - Sloba
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
          fullname: Slobodan Ninkov
          isHf: false
          isPro: false
          name: Sloba
          type: user
        html: '<blockquote>

          <p>[attention_mask?](NameError: name ''attention_mask'' is not defined)
          ?</p>

          </blockquote>

          <p>Thanks, updated original message with fix.<br>Just add:<br>attention_mask
          = torch.ones(input_ids.shape)</p>

          <p>before .... generate....</p>

          '
        raw: '> [attention_mask?](NameError: name ''attention_mask'' is not defined)
          ?


          Thanks, updated original message with fix.

          Just add:

          attention_mask = torch.ones(input_ids.shape)


          before .... generate....'
        updatedAt: '2023-05-28T15:25:02.941Z'
      numEdits: 0
      reactions: []
    id: 6473724e63001a0002cbd408
    type: comment
  author: Sloba
  content: '> [attention_mask?](NameError: name ''attention_mask'' is not defined)
    ?


    Thanks, updated original message with fix.

    Just add:

    attention_mask = torch.ones(input_ids.shape)


    before .... generate....'
  created_at: 2023-05-28 14:25:02+00:00
  edited: false
  hidden: false
  id: 6473724e63001a0002cbd408
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb7c74a381fd283e338c25e7100bcf0e.svg
      fullname: Max Russe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xorat
      type: user
    createdAt: '2023-05-28T15:40:29.000Z'
    data:
      edited: false
      editors:
      - Xorat
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb7c74a381fd283e338c25e7100bcf0e.svg
          fullname: Max Russe
          isHf: false
          isPro: false
          name: Xorat
          type: user
        html: '<p>also added:<br>input_ids = input_ids.to(''cuda'')<br>before :<br>attention_mask
          = torch.ones(input_ids.shape)<br>now working nice! thx</p>

          '
        raw: "also added: \ninput_ids = input_ids.to('cuda')\nbefore :\nattention_mask\
          \ = torch.ones(input_ids.shape)\nnow working nice! thx"
        updatedAt: '2023-05-28T15:40:29.897Z'
      numEdits: 0
      reactions: []
    id: 647375ed6cff2f8672059735
    type: comment
  author: Xorat
  content: "also added: \ninput_ids = input_ids.to('cuda')\nbefore :\nattention_mask\
    \ = torch.ones(input_ids.shape)\nnow working nice! thx"
  created_at: 2023-05-28 14:40:29+00:00
  edited: false
  hidden: false
  id: 647375ed6cff2f8672059735
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/114e6991a581a3e18d64d2a0e334b26b.svg
      fullname: Bilel Moulahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bilelm
      type: user
    createdAt: '2023-05-29T09:16:38.000Z'
    data:
      edited: false
      editors:
      - bilelm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/114e6991a581a3e18d64d2a0e334b26b.svg
          fullname: Bilel Moulahi
          isHf: false
          isPro: false
          name: bilelm
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sloba&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sloba\">@<span class=\"\
          underline\">Sloba</span></a></span>\n\n\t</span></span> on what server specs\
          \ did you test your model ? and how was the inference time ?</p>\n"
        raw: '@Sloba on what server specs did you test your model ? and how was the
          inference time ?'
        updatedAt: '2023-05-29T09:16:38.489Z'
      numEdits: 0
      reactions: []
    id: 64746d7682907acddde93a34
    type: comment
  author: bilelm
  content: '@Sloba on what server specs did you test your model ? and how was the
    inference time ?'
  created_at: 2023-05-29 08:16:38+00:00
  edited: false
  hidden: false
  id: 64746d7682907acddde93a34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
      fullname: Slobodan Ninkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sloba
      type: user
    createdAt: '2023-05-29T10:01:38.000Z'
    data:
      edited: false
      editors:
      - Sloba
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
          fullname: Slobodan Ninkov
          isHf: false
          isPro: false
          name: Sloba
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;bilelm&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bilelm\">@<span class=\"\
          underline\">bilelm</span></a></span>\n\n\t</span></span><br>It is private\
          \ desktop computer, i am interested in creating local LLM-s:<br>AMD Ryzen\
          \ 9 7900X 12-Core (24 thread)  4.7GHz<br>64Gb DDR5 Ram 4800MHz<br>NVidia\
          \ GeForce RTX 3090 24Gb<br>Model is loaded from NAS over 1GBit/s network\
          \ (it takes ~2min)</p>\n<p>It took 78 seconds for:<br><strong>Question</strong>:\
          \ Where girrafe lives, and how tall is giraffe?<br><strong>Answer</strong>:\
          \ Giraffes live in the African continent, they are the tallest land animals.<br>The\
          \ tallest one of them is the African male giraffe, which can stand up to\
          \ 5.9 meters tall.<br>Giraffes are herbivorous animals, they feed on leaves\
          \ and grasses.<br>They are not dangerous animals. They are very friendly\
          \ and kind to humans, they don\u2019t attack or eat people.<br>Giraffes\
          \ have a lifespan of 20 years.&lt;|endoftext|&gt;</p>\n<p>It took 156 seconds\
          \ for:<br><strong>Question</strong>: Where llama lives, and how tall is\
          \ llama?<br><strong>Answer</strong>: The llama is a South American camelid,\
          \ a member of the camel family. It is a large, sturdy animal with a thick\
          \ coat. Llamas are domesticated and are used for meat, wool, and milk.<br>The\
          \ llama is a South American camelid, a member of the camel family. It is\
          \ a large, sturdy animal with a thick coat. Llamas are domesticated and\
          \ are used for meat, wool, and milk.<br>What is a llama? A llama is a South\
          \ American camelid, a member of the camel family. They are domesticated\
          \ and used for meat, wool, and milk.<br>How do llamas look? Llama\u2019\
          s are a large, furry animal that looks like a mix between a camel and a\
          \ giraffe. They are native to South America but are now found in many other\
          \ places around the world.<br>Where do llamas live? Llamas live in the Andes\
          \ mountains, where they graze on vegetation.</p>\n<p>I hope this helps,\
          \ for what is worth, Falcon-7B answers are pretty good.</p>\n"
        raw: "@bilelm  \nIt is private desktop computer, i am interested in creating\
          \ local LLM-s:\nAMD Ryzen 9 7900X 12-Core (24 thread)  4.7GHz\n64Gb DDR5\
          \ Ram 4800MHz\nNVidia GeForce RTX 3090 24Gb\nModel is loaded from NAS over\
          \ 1GBit/s network (it takes ~2min)\n\nIt took 78 seconds for:\n**Question**:\
          \ Where girrafe lives, and how tall is giraffe?\n**Answer**: Giraffes live\
          \ in the African continent, they are the tallest land animals.\nThe tallest\
          \ one of them is the African male giraffe, which can stand up to 5.9 meters\
          \ tall.\nGiraffes are herbivorous animals, they feed on leaves and grasses.\n\
          They are not dangerous animals. They are very friendly and kind to humans,\
          \ they don\u2019t attack or eat people.\nGiraffes have a lifespan of 20\
          \ years.<|endoftext|>\n\nIt took 156 seconds for:\n**Question**: Where llama\
          \ lives, and how tall is llama?\n**Answer**: The llama is a South American\
          \ camelid, a member of the camel family. It is a large, sturdy animal with\
          \ a thick coat. Llamas are domesticated and are used for meat, wool, and\
          \ milk.\nThe llama is a South American camelid, a member of the camel family.\
          \ It is a large, sturdy animal with a thick coat. Llamas are domesticated\
          \ and are used for meat, wool, and milk.\nWhat is a llama? A llama is a\
          \ South American camelid, a member of the camel family. They are domesticated\
          \ and used for meat, wool, and milk.\nHow do llamas look? Llama\u2019s are\
          \ a large, furry animal that looks like a mix between a camel and a giraffe.\
          \ They are native to South America but are now found in many other places\
          \ around the world.\nWhere do llamas live? Llamas live in the Andes mountains,\
          \ where they graze on vegetation.\n\n\nI hope this helps, for what is worth,\
          \ Falcon-7B answers are pretty good."
        updatedAt: '2023-05-29T10:01:38.860Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - LV1871
        - jctanking
    id: 6474780282907acdddea08b4
    type: comment
  author: Sloba
  content: "@bilelm  \nIt is private desktop computer, i am interested in creating\
    \ local LLM-s:\nAMD Ryzen 9 7900X 12-Core (24 thread)  4.7GHz\n64Gb DDR5 Ram 4800MHz\n\
    NVidia GeForce RTX 3090 24Gb\nModel is loaded from NAS over 1GBit/s network (it\
    \ takes ~2min)\n\nIt took 78 seconds for:\n**Question**: Where girrafe lives,\
    \ and how tall is giraffe?\n**Answer**: Giraffes live in the African continent,\
    \ they are the tallest land animals.\nThe tallest one of them is the African male\
    \ giraffe, which can stand up to 5.9 meters tall.\nGiraffes are herbivorous animals,\
    \ they feed on leaves and grasses.\nThey are not dangerous animals. They are very\
    \ friendly and kind to humans, they don\u2019t attack or eat people.\nGiraffes\
    \ have a lifespan of 20 years.<|endoftext|>\n\nIt took 156 seconds for:\n**Question**:\
    \ Where llama lives, and how tall is llama?\n**Answer**: The llama is a South\
    \ American camelid, a member of the camel family. It is a large, sturdy animal\
    \ with a thick coat. Llamas are domesticated and are used for meat, wool, and\
    \ milk.\nThe llama is a South American camelid, a member of the camel family.\
    \ It is a large, sturdy animal with a thick coat. Llamas are domesticated and\
    \ are used for meat, wool, and milk.\nWhat is a llama? A llama is a South American\
    \ camelid, a member of the camel family. They are domesticated and used for meat,\
    \ wool, and milk.\nHow do llamas look? Llama\u2019s are a large, furry animal\
    \ that looks like a mix between a camel and a giraffe. They are native to South\
    \ America but are now found in many other places around the world.\nWhere do llamas\
    \ live? Llamas live in the Andes mountains, where they graze on vegetation.\n\n\
    \nI hope this helps, for what is worth, Falcon-7B answers are pretty good."
  created_at: 2023-05-29 09:01:38+00:00
  edited: false
  hidden: false
  id: 6474780282907acdddea08b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/114e6991a581a3e18d64d2a0e334b26b.svg
      fullname: Bilel Moulahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bilelm
      type: user
    createdAt: '2023-05-29T17:12:39.000Z'
    data:
      edited: false
      editors:
      - bilelm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/114e6991a581a3e18d64d2a0e334b26b.svg
          fullname: Bilel Moulahi
          isHf: false
          isPro: false
          name: bilelm
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sloba&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sloba\">@<span class=\"\
          underline\">Sloba</span></a></span>\n\n\t</span></span> thank you so much\
          \ for your answer.<br>I'm looking to test it on French, for tasks like summarization\
          \ or information extraction.</p>\n"
        raw: "@Sloba thank you so much for your answer. \nI'm looking to test it on\
          \ French, for tasks like summarization or information extraction."
        updatedAt: '2023-05-29T17:12:39.866Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Sloba
        - xiao111
    id: 6474dd07f9e3e0b312f07411
    type: comment
  author: bilelm
  content: "@Sloba thank you so much for your answer. \nI'm looking to test it on\
    \ French, for tasks like summarization or information extraction."
  created_at: 2023-05-29 16:12:39+00:00
  edited: false
  hidden: false
  id: 6474dd07f9e3e0b312f07411
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T14:55:26.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Sloba&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sloba\">@<span class=\"\
          underline\">Sloba</span></a></span>\n\n\t</span></span>, thank you for writing\
          \ this short guide, we will pin it to make it easily accessible!</p>\n"
        raw: Hi @Sloba, thank you for writing this short guide, we will pin it to
          make it easily accessible!
        updatedAt: '2023-05-30T14:55:26.723Z'
      numEdits: 0
      reactions: []
    id: 64760e5e400d8a9ea631080d
    type: comment
  author: FalconLLM
  content: Hi @Sloba, thank you for writing this short guide, we will pin it to make
    it easily accessible!
  created_at: 2023-05-30 13:55:26+00:00
  edited: false
  hidden: false
  id: 64760e5e400d8a9ea631080d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionEvent
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T14:55:42.000Z'
    data:
      pinned: true
    id: 64760e6e1cf070114ea66dd0
    type: pinning-change
  author: FalconLLM
  created_at: 2023-05-30 13:55:42+00:00
  id: 64760e6e1cf070114ea66dd0
  type: pinning-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5dbd54dc1994d45683b19e49848cc7fc.svg
      fullname: Ishani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivyas
      type: user
    createdAt: '2023-05-31T19:48:57.000Z'
    data:
      edited: false
      editors:
      - ivyas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5dbd54dc1994d45683b19e49848cc7fc.svg
          fullname: Ishani
          isHf: false
          isPro: false
          name: ivyas
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;FalconLLM&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/FalconLLM\">@<span class=\"\
          underline\">FalconLLM</span></a></span>\n\n\t</span></span> , <span data-props=\"\
          {&quot;user&quot;:&quot;Sloba&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Sloba\">@<span class=\"underline\">Sloba</span></a></span>\n\
          \n\t</span></span>  Quick question, Can I run it on Macbook Pro with intel\
          \ chip with 32 RAM?</p>\n"
        raw: '@FalconLLM , @Sloba  Quick question, Can I run it on Macbook Pro with
          intel chip with 32 RAM?'
        updatedAt: '2023-05-31T19:48:57.423Z'
      numEdits: 0
      reactions: []
    id: 6477a4a933a888101f7c2904
    type: comment
  author: ivyas
  content: '@FalconLLM , @Sloba  Quick question, Can I run it on Macbook Pro with
    intel chip with 32 RAM?'
  created_at: 2023-05-31 18:48:57+00:00
  edited: false
  hidden: false
  id: 6477a4a933a888101f7c2904
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
      fullname: Slobodan Ninkov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sloba
      type: user
    createdAt: '2023-06-01T08:03:53.000Z'
    data:
      edited: false
      editors:
      - Sloba
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6432f4e34521083b9d286a48/v9bX1bMorcB7XWlmG2aUi.jpeg?w=200&h=200&f=face
          fullname: Slobodan Ninkov
          isHf: false
          isPro: false
          name: Sloba
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ivyas&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ivyas\">@<span class=\"\
          underline\">ivyas</span></a></span>\n\n\t</span></span>  Unfortunately I\
          \ don't have access to MBP with 32G RAM to try it out.<br>If you decide\
          \ to try it out, don't hesitate to share the results. Maybe there is someone\
          \ who needs exactly the info you find in your test.</p>\n"
        raw: "@ivyas  Unfortunately I don't have access to MBP with 32G RAM to try\
          \ it out. \nIf you decide to try it out, don't hesitate to share the results.\
          \ Maybe there is someone who needs exactly the info you find in your test."
        updatedAt: '2023-06-01T08:03:53.714Z'
      numEdits: 0
      reactions: []
    id: 647850e9159a889d001ea42e
    type: comment
  author: Sloba
  content: "@ivyas  Unfortunately I don't have access to MBP with 32G RAM to try it\
    \ out. \nIf you decide to try it out, don't hesitate to share the results. Maybe\
    \ there is someone who needs exactly the info you find in your test."
  created_at: 2023-06-01 07:03:53+00:00
  edited: false
  hidden: false
  id: 647850e9159a889d001ea42e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d7f0bfd71b62f66af23a406c0b2441c.svg
      fullname: Mario VANHECKE
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BliepBlop
      type: user
    createdAt: '2023-06-02T15:24:27.000Z'
    data:
      edited: false
      editors:
      - BliepBlop
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d7f0bfd71b62f66af23a406c0b2441c.svg
          fullname: Mario VANHECKE
          isHf: false
          isPro: false
          name: BliepBlop
          type: user
        html: "<p>I'm trying to run this on a Apple M1 Max..<br>the code I use is\
          \ this:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\
          import transformers\nimport torch\nmodel = \"./falcon-7b\"\ndevice_name\
          \ = 'cpu'\ndevice = torch.device(device_name)\nrrmodel = AutoModelForCausalLM.from_pretrained(model,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\")\nrrmodel = rrmodel.to(device)\n\
          tokenizer = AutoTokenizer.from_pretrained(model)\n\ninput_text = \"Once\
          \ upon a time\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\n\
          input_ids = input_ids.to(device)\nattention_mask = torch.ones(input_ids.shape)\n\
          attention_mask = attention_mask.to(device)\n\noutput = rrmodel.generate(input_ids,\n\
          \            attention_mask=attention_mask,\n            max_length=200,\n\
          \            do_sample=True,\n            top_k=10,\n            num_return_sequences=1,\n\
          \            eos_token_id=tokenizer.eos_token_id,)\n\noutput_text = tokenizer.decode(output[0],\
          \ skip_special_tokens=True)\n\nprint(output_text)\n</code></pre>\n<p>Using\
          \ device_name = 'cpu' this take 5m 50s mins to run.</p>\n<p>I try to use\
          \ device_name = 'mps' for acceleration on the m1 chip.<br>But I get this\
          \ error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/Users/mario/Downloads/main.py\"\
          , line 19, in &lt;module&gt;\n    output = rrmodel.generate(input_ids,\n\
          \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 1565, in generate\n    return self.sample(\n  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 2612, in sample\n    outputs = self(\n  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/Users/mario/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py\"\
          , line 753, in forward\n    transformer_outputs = self.transformer(\n  File\
          \ \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/Users/mario/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py\"\
          , line 590, in forward\n    inputs_embeds = self.word_embeddings(input_ids)\n\
          \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/sparse.py\"\
          , line 162, in forward\n    return F.embedding(\n  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/functional.py\"\
          , line 2238, in embedding\n    return torch.embedding(weight, input, padding_idx,\
          \ scale_grad_by_freq, sparse)\nRuntimeError: Placeholder storage has not\
          \ been allocated on MPS device!\n</code></pre>\n"
        raw: "I'm trying to run this on a Apple M1 Max..\nthe code I use is this:\n\
          ```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport\
          \ transformers\nimport torch\nmodel = \"./falcon-7b\"\ndevice_name = 'cpu'\n\
          device = torch.device(device_name)\nrrmodel = AutoModelForCausalLM.from_pretrained(model,\n\
          \    trust_remote_code=True,\n    device_map=\"auto\")\nrrmodel = rrmodel.to(device)\n\
          tokenizer = AutoTokenizer.from_pretrained(model)\n\ninput_text = \"Once\
          \ upon a time\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\n\
          input_ids = input_ids.to(device)\nattention_mask = torch.ones(input_ids.shape)\n\
          attention_mask = attention_mask.to(device)\n\noutput = rrmodel.generate(input_ids,\n\
          \            attention_mask=attention_mask,\n            max_length=200,\n\
          \            do_sample=True,\n            top_k=10,\n            num_return_sequences=1,\n\
          \            eos_token_id=tokenizer.eos_token_id,)\n\noutput_text = tokenizer.decode(output[0],\
          \ skip_special_tokens=True)\n\nprint(output_text)\n```\n\nUsing device_name\
          \ = 'cpu' this take 5m 50s mins to run.\n\nI try to use device_name = 'mps'\
          \ for acceleration on the m1 chip.\nBut I get this error:\n\n```\nTraceback\
          \ (most recent call last):\n  File \"/Users/mario/Downloads/main.py\", line\
          \ 19, in <module>\n    output = rrmodel.generate(input_ids,\n  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 1565, in generate\n    return self.sample(\n  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 2612, in sample\n    outputs = self(\n  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/Users/mario/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py\"\
          , line 753, in forward\n    transformer_outputs = self.transformer(\n  File\
          \ \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/Users/mario/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py\"\
          , line 590, in forward\n    inputs_embeds = self.word_embeddings(input_ids)\n\
          \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/sparse.py\"\
          , line 162, in forward\n    return F.embedding(\n  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/functional.py\"\
          , line 2238, in embedding\n    return torch.embedding(weight, input, padding_idx,\
          \ scale_grad_by_freq, sparse)\nRuntimeError: Placeholder storage has not\
          \ been allocated on MPS device!\n```"
        updatedAt: '2023-06-02T15:24:27.725Z'
      numEdits: 0
      reactions: []
    id: 647a09abf518a860fbcbdf20
    type: comment
  author: BliepBlop
  content: "I'm trying to run this on a Apple M1 Max..\nthe code I use is this:\n\
    ```\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
    import torch\nmodel = \"./falcon-7b\"\ndevice_name = 'cpu'\ndevice = torch.device(device_name)\n\
    rrmodel = AutoModelForCausalLM.from_pretrained(model,\n    trust_remote_code=True,\n\
    \    device_map=\"auto\")\nrrmodel = rrmodel.to(device)\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
    \ninput_text = \"Once upon a time\"\ninput_ids = tokenizer.encode(input_text,\
    \ return_tensors='pt')\ninput_ids = input_ids.to(device)\nattention_mask = torch.ones(input_ids.shape)\n\
    attention_mask = attention_mask.to(device)\n\noutput = rrmodel.generate(input_ids,\n\
    \            attention_mask=attention_mask,\n            max_length=200,\n   \
    \         do_sample=True,\n            top_k=10,\n            num_return_sequences=1,\n\
    \            eos_token_id=tokenizer.eos_token_id,)\n\noutput_text = tokenizer.decode(output[0],\
    \ skip_special_tokens=True)\n\nprint(output_text)\n```\n\nUsing device_name =\
    \ 'cpu' this take 5m 50s mins to run.\n\nI try to use device_name = 'mps' for\
    \ acceleration on the m1 chip.\nBut I get this error:\n\n```\nTraceback (most\
    \ recent call last):\n  File \"/Users/mario/Downloads/main.py\", line 19, in <module>\n\
    \    output = rrmodel.generate(input_ids,\n  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/transformers/generation/utils.py\"\
    , line 1565, in generate\n    return self.sample(\n  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/transformers/generation/utils.py\"\
    , line 2612, in sample\n    outputs = self(\n  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
    \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File\
    \ \"/Users/mario/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py\"\
    , line 753, in forward\n    transformer_outputs = self.transformer(\n  File \"\
    /Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
    \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File\
    \ \"/Users/mario/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py\"\
    , line 590, in forward\n    inputs_embeds = self.word_embeddings(input_ids)\n\
    \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1502, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
    \  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1511, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File\
    \ \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/sparse.py\"\
    , line 162, in forward\n    return F.embedding(\n  File \"/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/functional.py\"\
    , line 2238, in embedding\n    return torch.embedding(weight, input, padding_idx,\
    \ scale_grad_by_freq, sparse)\nRuntimeError: Placeholder storage has not been\
    \ allocated on MPS device!\n```"
  created_at: 2023-06-02 14:24:27+00:00
  edited: false
  hidden: false
  id: 647a09abf518a860fbcbdf20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5dbd54dc1994d45683b19e49848cc7fc.svg
      fullname: Ishani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivyas
      type: user
    createdAt: '2023-06-02T17:35:24.000Z'
    data:
      edited: true
      editors:
      - ivyas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5dbd54dc1994d45683b19e49848cc7fc.svg
          fullname: Ishani
          isHf: false
          isPro: false
          name: ivyas
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;BliepBlop&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/BliepBlop\">@<span class=\"\
          underline\">BliepBlop</span></a></span>\n\n\t</span></span> Did you encounter\
          \ this issue when you run above code?</p>\n<p>ValueError: The current <code>device_map</code>\
          \ had weights offloaded to the disk. Please provide an <code>offload_folder</code>\
          \ for them. Alternatively, make sure you have <code>safetensors</code> installed\
          \ if the model you are using offers the weights in this format.</p>\n<p>I\
          \ installed <code>safetensors</code>.</p>\n"
        raw: '@BliepBlop Did you encounter this issue when you run above code?


          ValueError: The current `device_map` had weights offloaded to the disk.
          Please provide an `offload_folder` for them. Alternatively, make sure you
          have `safetensors` installed if the model you are using offers the weights
          in this format.


          I installed `safetensors`.'
        updatedAt: '2023-06-02T17:35:46.348Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F614"
        users:
        - zonkei
        - papaponcho
        - wilofice
    id: 647a285c3a17d5e00ad67c4a
    type: comment
  author: ivyas
  content: '@BliepBlop Did you encounter this issue when you run above code?


    ValueError: The current `device_map` had weights offloaded to the disk. Please
    provide an `offload_folder` for them. Alternatively, make sure you have `safetensors`
    installed if the model you are using offers the weights in this format.


    I installed `safetensors`.'
  created_at: 2023-06-02 16:35:24+00:00
  edited: true
  hidden: false
  id: 647a285c3a17d5e00ad67c4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:03:40.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8959549069404602
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>In principle the model should need at least 16GB of memory to run--but
          generation on CPU is bound to be slow. </p>

          <p>We also recommend having a look at this <a href="https://huggingface.co/blog/falcon">blog
          post</a> for more info on finetuning &amp; inference of Falcon.</p>

          '
        raw: "In principle the model should need at least 16GB of memory to run--but\
          \ generation on CPU is bound to be slow. \n\nWe also recommend having a\
          \ look at this [blog post](https://huggingface.co/blog/falcon) for more\
          \ info on finetuning & inference of Falcon."
        updatedAt: '2023-06-09T14:03:40.438Z'
      numEdits: 0
      reactions: []
    id: 6483313c0cbfa04ebb2d618b
    type: comment
  author: FalconLLM
  content: "In principle the model should need at least 16GB of memory to run--but\
    \ generation on CPU is bound to be slow. \n\nWe also recommend having a look at\
    \ this [blog post](https://huggingface.co/blog/falcon) for more info on finetuning\
    \ & inference of Falcon."
  created_at: 2023-06-09 13:03:40+00:00
  edited: false
  hidden: false
  id: 6483313c0cbfa04ebb2d618b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ff983b88f31127d538affca08e16916.svg
      fullname: Bryan Greenway
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rbgreenway
      type: user
    createdAt: '2023-06-13T17:08:18.000Z'
    data:
      edited: true
      editors:
      - rbgreenway
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8892771601676941
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ff983b88f31127d538affca08e16916.svg
          fullname: Bryan Greenway
          isHf: false
          isPro: false
          name: rbgreenway
          type: user
        html: '<p>Excellent post.  Thanks for providing this.<br>All worked for me
          on a 4090, Ubuntu 20.04</p>

          '
        raw: 'Excellent post.  Thanks for providing this.

          All worked for me on a 4090, Ubuntu 20.04'
        updatedAt: '2023-06-13T17:09:35.433Z'
      numEdits: 1
      reactions: []
    id: 6488a282a4bd45793cd0e0e1
    type: comment
  author: rbgreenway
  content: 'Excellent post.  Thanks for providing this.

    All worked for me on a 4090, Ubuntu 20.04'
  created_at: 2023-06-13 16:08:18+00:00
  edited: true
  hidden: false
  id: 6488a282a4bd45793cd0e0e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-06-28T23:10:35.000Z'
    data:
      edited: true
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8810217976570129
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<p>Alternative: <a rel="nofollow" href="https://github.com/cmp-nct/ggllm.cpp/blob/master/README.md">https://github.com/cmp-nct/ggllm.cpp/blob/master/README.md</a><br>Includes
          a video how to compile it on windows, does not need a complex conda/python
          backend and runs with just a few GB or RAM (or VRAM) 10+ times faster than
          with python<br>Also includes exe binary release for windows (for cpu and
          cuda) if you don''t want to get into development frameworks</p>

          '
        raw: "Alternative: https://github.com/cmp-nct/ggllm.cpp/blob/master/README.md\n\
          Includes a video how to compile it on windows, does not need a complex conda/python\
          \ backend and runs with just a few GB or RAM (or VRAM) 10+ times faster\
          \ than with python \nAlso includes exe binary release for windows (for cpu\
          \ and cuda) if you don't want to get into development frameworks"
        updatedAt: '2023-07-13T12:28:59.236Z'
      numEdits: 1
      reactions: []
    id: 649cbdeb6209253a7685da71
    type: comment
  author: cmp-nct
  content: "Alternative: https://github.com/cmp-nct/ggllm.cpp/blob/master/README.md\n\
    Includes a video how to compile it on windows, does not need a complex conda/python\
    \ backend and runs with just a few GB or RAM (or VRAM) 10+ times faster than with\
    \ python \nAlso includes exe binary release for windows (for cpu and cuda) if\
    \ you don't want to get into development frameworks"
  created_at: 2023-06-28 22:10:35+00:00
  edited: true
  hidden: false
  id: 649cbdeb6209253a7685da71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aeb99bfe03303b3af452e6ef29b88b5c.svg
      fullname: wasim maliik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wasiim
      type: user
    createdAt: '2023-07-13T10:46:24.000Z'
    data:
      edited: false
      editors:
      - wasiim
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9487377405166626
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aeb99bfe03303b3af452e6ef29b88b5c.svg
          fullname: wasim maliik
          isHf: false
          isPro: false
          name: wasiim
          type: user
        html: '<p>can anyone help me please<br>i have the text data stored in .txt
          the text data is simple information about a technology<br>i want to fine
          tune the falcon model and the i want to ask the question to the falcon model
          according to that .txt file </p>

          '
        raw: "can anyone help me please \ni have the text data stored in .txt the\
          \ text data is simple information about a technology \ni want to fine tune\
          \ the falcon model and the i want to ask the question to the falcon model\
          \ according to that .txt file "
        updatedAt: '2023-07-13T10:46:24.661Z'
      numEdits: 0
      reactions: []
    id: 64afd60003ad68015aeb1111
    type: comment
  author: wasiim
  content: "can anyone help me please \ni have the text data stored in .txt the text\
    \ data is simple information about a technology \ni want to fine tune the falcon\
    \ model and the i want to ask the question to the falcon model according to that\
    \ .txt file "
  created_at: 2023-07-13 09:46:24+00:00
  edited: false
  hidden: false
  id: 64afd60003ad68015aeb1111
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
      fullname: John
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmp-nct
      type: user
    createdAt: '2023-07-13T12:28:05.000Z'
    data:
      edited: false
      editors:
      - cmp-nct
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.94412761926651
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70bfddcf585e2f191ba24f47274c9e94.svg
          fullname: John
          isHf: false
          isPro: false
          name: cmp-nct
          type: user
        html: '<blockquote>

          <p>can anyone help me please<br>i have the text data stored in .txt the
          text data is simple information about a technology<br>i want to fine tune
          the falcon model and the i want to ask the question to the falcon model
          according to that .txt file</p>

          </blockquote>

          <p>Fine tuning typically involves a clean set of inputs and outputs, not
          a text with simple information.<br>You can look into fine tune projects
          for falcon and how their input data looks like, it will need an elaborate
          effort to transform your text into good input and output.</p>

          <p>The more likely solution is to just prompt Falcon with your text and
          ask it to use it as information source. By using a good fine tune that follows
          your prompt you can increase the quality.</p>

          '
        raw: "> can anyone help me please \n> i have the text data stored in .txt\
          \ the text data is simple information about a technology \n> i want to fine\
          \ tune the falcon model and the i want to ask the question to the falcon\
          \ model according to that .txt file\n\nFine tuning typically involves a\
          \ clean set of inputs and outputs, not a text with simple information. \n\
          You can look into fine tune projects for falcon and how their input data\
          \ looks like, it will need an elaborate effort to transform your text into\
          \ good input and output.\n\nThe more likely solution is to just prompt Falcon\
          \ with your text and ask it to use it as information source. By using a\
          \ good fine tune that follows your prompt you can increase the quality."
        updatedAt: '2023-07-13T12:28:05.881Z'
      numEdits: 0
      reactions: []
    id: 64afedd565af747da6314899
    type: comment
  author: cmp-nct
  content: "> can anyone help me please \n> i have the text data stored in .txt the\
    \ text data is simple information about a technology \n> i want to fine tune the\
    \ falcon model and the i want to ask the question to the falcon model according\
    \ to that .txt file\n\nFine tuning typically involves a clean set of inputs and\
    \ outputs, not a text with simple information. \nYou can look into fine tune projects\
    \ for falcon and how their input data looks like, it will need an elaborate effort\
    \ to transform your text into good input and output.\n\nThe more likely solution\
    \ is to just prompt Falcon with your text and ask it to use it as information\
    \ source. By using a good fine tune that follows your prompt you can increase\
    \ the quality."
  created_at: 2023-07-13 11:28:05+00:00
  edited: false
  hidden: false
  id: 64afedd565af747da6314899
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bfb3d7c0f24821cd09e7d7756d3d6c3.svg
      fullname: Jason Richards
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Raven78
      type: user
    createdAt: '2023-07-27T05:44:53.000Z'
    data:
      edited: false
      editors:
      - Raven78
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8637489080429077
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bfb3d7c0f24821cd09e7d7756d3d6c3.svg
          fullname: Jason Richards
          isHf: false
          isPro: false
          name: Raven78
          type: user
        html: "<p>Thank you so much. worked a treat!</p>\n<p>also if anyone is getting\
          \ the ERROR :<br>RuntimeError: Expected all tensors to be on the same device,\
          \ but found at least two devices, cuda:0 and cpu!</p>\n<p>just add the line:\
          \    input_ids = input_ids.to('cuda')</p>\n<p>thanks again <span data-props=\"\
          {&quot;user&quot;:&quot;Sloba&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Sloba\">@<span class=\"underline\">Sloba</span></a></span>\n\
          \n\t</span></span>  :)</p>\n"
        raw: 'Thank you so much. worked a treat!


          also if anyone is getting the ERROR :

          RuntimeError: Expected all tensors to be on the same device, but found at
          least two devices, cuda:0 and cpu!


          just add the line:    input_ids = input_ids.to(''cuda'')


          thanks again @Sloba  :)'
        updatedAt: '2023-07-27T05:44:53.393Z'
      numEdits: 0
      reactions: []
    id: 64c20455b005aab93d64b641
    type: comment
  author: Raven78
  content: 'Thank you so much. worked a treat!


    also if anyone is getting the ERROR :

    RuntimeError: Expected all tensors to be on the same device, but found at least
    two devices, cuda:0 and cpu!


    just add the line:    input_ids = input_ids.to(''cuda'')


    thanks again @Sloba  :)'
  created_at: 2023-07-27 04:44:53+00:00
  edited: false
  hidden: false
  id: 64c20455b005aab93d64b641
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: How to make it work for less experienced AI whisperers
