!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mrhimanshu
conflicting_files: null
created_at: 2023-05-29 07:00:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5704334d953cd1980b07e1d4b9fa0b50.svg
      fullname: Himanshu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mrhimanshu
      type: user
    createdAt: '2023-05-29T08:00:07.000Z'
    data:
      edited: false
      editors:
      - mrhimanshu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5704334d953cd1980b07e1d4b9fa0b50.svg
          fullname: Himanshu
          isHf: false
          isPro: false
          name: mrhimanshu
          type: user
        html: '<p>I''m trying to convert this model into 4bit but somehow it''s falling
          while getting a response.</p>

          <p>ValueError: The following <code>model_kwargs</code> are not used by the
          model: [''token_type_ids''] (note: typos in the generate<br>arguments will
          also show up in this list)</p>

          <p>The code is below :</p>

          <p>from transformers import AutoModelForCausalLM, AutoTokenizer</p>

          <p>model_id = "tiiuae/falcon-7b"</p>

          <p>model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True,
          device_map="auto", trust_remote_code=True)<br>tokenizer = AutoTokenizer.from_pretrained(model_id)</p>

          <p>text = "Hello my name is"<br>device = "cuda:0"</p>

          <p>inputs = tokenizer(text, return_tensors="pt").to(device)<br>outputs =
          model.generate(**inputs, max_length=60)<br>print(tokenizer.decode(outputs[0],
          skip_special_tokens=True))</p>

          '
        raw: "I'm trying to convert this model into 4bit but somehow it's falling\
          \ while getting a response.\r\n\r\nValueError: The following `model_kwargs`\
          \ are not used by the model: ['token_type_ids'] (note: typos in the generate\r\
          \narguments will also show up in this list)\r\n\r\nThe code is below :\r\
          \n\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\
          \nmodel_id = \"tiiuae/falcon-7b\"\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\
          \ load_in_4bit=True, device_map=\"auto\", trust_remote_code=True)\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_id)\r\n\r\ntext = \"Hello my name\
          \ is\"\r\ndevice = \"cuda:0\"\r\n\r\ninputs = tokenizer(text, return_tensors=\"\
          pt\").to(device)\r\noutputs = model.generate(**inputs, max_length=60)\r\n\
          print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
        updatedAt: '2023-05-29T08:00:07.686Z'
      numEdits: 0
      reactions: []
    id: 64745b8733192631baccb198
    type: comment
  author: mrhimanshu
  content: "I'm trying to convert this model into 4bit but somehow it's falling while\
    \ getting a response.\r\n\r\nValueError: The following `model_kwargs` are not\
    \ used by the model: ['token_type_ids'] (note: typos in the generate\r\narguments\
    \ will also show up in this list)\r\n\r\nThe code is below :\r\n\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel_id = \"tiiuae/falcon-7b\"\
    \r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True,\
    \ device_map=\"auto\", trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\
    \n\r\ntext = \"Hello my name is\"\r\ndevice = \"cuda:0\"\r\n\r\ninputs = tokenizer(text,\
    \ return_tensors=\"pt\").to(device)\r\noutputs = model.generate(**inputs, max_length=60)\r\
    \nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
  created_at: 2023-05-29 07:00:07+00:00
  edited: false
  hidden: false
  id: 64745b8733192631baccb198
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:07:25.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6143968105316162
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: "<p>See <a href=\"https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/2\"\
          >this discussion</a> for a solution \U0001F44D</p>\n"
        raw: "See [this discussion](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/2)\
          \ for a solution \U0001F44D"
        updatedAt: '2023-06-09T14:07:25.962Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6483321d4f25746570a69280
    id: 6483321d4f25746570a6927b
    type: comment
  author: FalconLLM
  content: "See [this discussion](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/2)\
    \ for a solution \U0001F44D"
  created_at: 2023-06-09 13:07:25+00:00
  edited: false
  hidden: false
  id: 6483321d4f25746570a6927b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-06-09T14:07:25.000Z'
    data:
      status: closed
    id: 6483321d4f25746570a69280
    type: status-change
  author: FalconLLM
  created_at: 2023-06-09 13:07:25+00:00
  id: 6483321d4f25746570a69280
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: tiiuae/falcon-7b
repo_type: model
status: closed
target_branch: null
title: How to quantize this model using QLoRA ?
