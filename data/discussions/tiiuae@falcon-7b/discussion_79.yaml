!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ConorVanek
conflicting_files: null
created_at: 2023-09-08 20:04:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2af7a5596fc9c48aab97bdee2dd36801.svg
      fullname: Conor Vanek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ConorVanek
      type: user
    createdAt: '2023-09-08T21:04:51.000Z'
    data:
      edited: false
      editors:
      - ConorVanek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4970552325248718
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2af7a5596fc9c48aab97bdee2dd36801.svg
          fullname: Conor Vanek
          isHf: false
          isPro: false
          name: ConorVanek
          type: user
        html: "<p>Getting the following error when I try to copy/paste and run the\
          \ model using the code on the model card:</p>\n<p>Loading checkpoint shards:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:19&lt;00:00,  9.83s/it]<br>/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:1417:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see <a href=\"https://huggingface.co/docs/transformers/main_classes/text_generation\"\
          >https://huggingface.co/docs/transformers/main_classes/text_generation</a>\
          \ )<br>  warnings.warn(<br>Setting <code>pad_token_id</code> to <code>eos_token_id</code>:11\
          \ for open-end generation.<br>Traceback (most recent call last):<br>  File\
          \ \"/home/anthony/huggingface/main.py\", line 16, in <br>    sequences =\
          \ pipeline(<br>  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py\"\
          , line 205, in <strong>call</strong><br>    return super().<strong>call</strong>(text_inputs,\
          \ **kwargs)<br>  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
          , line 1140, in <strong>call</strong><br>    return self.run_single(inputs,\
          \ preprocess_params, forward_params, postprocess_params)<br>  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
          , line 1147, in run_single<br>    model_outputs = self.forward(model_inputs,\
          \ **forward_params)<br>  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
          , line 1046, in forward<br>    model_outputs = self._forward(model_inputs,\
          \ **forward_params)<br>  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py\"\
          , line 268, in _forward<br>    generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=attention_mask, **generate_kwargs)<br>  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context<br>    return func(*args, **kwargs)<br>\
          \  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 1648, in generate<br>    return self.sample(<br>  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 2730, in sample<br>    outputs = self(<br>  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>\
          \  File \"/home/anthony/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/f7796529e36b2d49094450fb038cc7c4c86afa44/modelling_RW.py\"\
          , line 753, in forward<br>    transformer_outputs = self.transformer(<br>\
          \  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/home/anthony/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/f7796529e36b2d49094450fb038cc7c4c86afa44/modelling_RW.py\"\
          , line 648, in forward<br>    outputs = block(<br>  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>\
          \  File \"/home/anthony/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/f7796529e36b2d49094450fb038cc7c4c86afa44/modelling_RW.py\"\
          , line 385, in forward<br>    attn_outputs = self.self_attention(<br>  File\
          \ \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward<br>    output = old_forward(*args, **kwargs)<br>\
          \  File \"/home/anthony/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/f7796529e36b2d49094450fb038cc7c4c86afa44/modelling_RW.py\"\
          , line 279, in forward<br>    attn_output = F.scaled_dot_product_attention(<br>RuntimeError:\
          \ CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling <code>cublasGemmStridedBatchedExFix(handle,\
          \ opa, opb, (int)m, (int)n, (int)k, (void*)&amp;falpha, a, CUDA_R_16BF,\
          \ (int)lda, stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&amp;fbeta,\
          \ c, CUDA_R_16BF, (int)ldc, stridec, (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)</code></p>\n\
          <p>I am using an Anaconda environment on Ubuntu 22.04.3 LTS and here are\
          \ the packages I have installed:</p>\n<p>Package                  Version</p>\n\
          <hr>\n<p>accelerate               0.22.0<br>accelerator              2023.7.18.dev1<br>bottle\
          \                   0.12.25<br>certifi                  2023.7.22<br>charset-normalizer\
          \       3.2.0<br>cmake                    3.27.4.1<br>einops           \
          \        0.6.1<br>filelock                 3.12.3<br>fsspec            \
          \       2023.9.0<br>huggingface-hub          0.16.4<br>idna            \
          \         3.4<br>inquirerpy               0.3.4<br>Jinja2              \
          \     3.1.2<br>lit                      16.0.6<br>MarkupSafe           \
          \    2.1.3<br>mpmath                   1.3.0<br>networkx               \
          \  3.1<br>numpy                    1.25.2<br>nvidia-cublas-cu11       11.10.3.66<br>nvidia-cuda-cupti-cu11\
          \   11.7.101<br>nvidia-cuda-nvrtc-cu11   11.7.99<br>nvidia-cuda-runtime-cu11\
          \ 11.7.99<br>nvidia-cudnn-cu11        8.5.0.96<br>nvidia-cufft-cu11    \
          \    10.9.0.58<br>nvidia-curand-cu11       10.2.10.91<br>nvidia-cusolver-cu11\
          \     11.4.0.1<br>nvidia-cusparse-cu11     11.7.4.91<br>nvidia-nccl-cu11\
          \         2.14.3<br>nvidia-nvtx-cu11         11.7.91<br>packaging      \
          \          23.1<br>pfzy                     0.3.4<br>Pillow            \
          \       10.0.0<br>pip                      23.2.1<br>prompt-toolkit    \
          \       3.0.39<br>psutil                   5.9.5<br>PyYAML             \
          \      6.0.1<br>regex                    2023.8.8<br>requests          \
          \       2.31.0<br>safetensors              0.3.3<br>setproctitle       \
          \      1.3.2<br>setuptools               68.0.0<br>sympy               \
          \     1.12<br>tokenizers               0.13.3<br>torch                 \
          \   2.0.1<br>torchaudio               2.0.2<br>torchvision             \
          \ 0.15.2<br>tqdm                     4.66.1<br>transformers            \
          \ 4.34.0.dev0<br>triton                   2.0.0<br>typing_extensions   \
          \     4.7.1<br>urllib3                  2.0.4<br>waitress              \
          \   2.1.2<br>wcwidth                  0.2.6<br>wheel                   \
          \ 0.38.4</p>\n<p>Any help would be greatly appreciated thank you.</p>\n"
        raw: "Getting the following error when I try to copy/paste and run the model\
          \ using the code on the model card:\r\n\r\nLoading checkpoint shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 2/2 [00:19<00:00,  9.83s/it]\r\n/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:1417:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see https://huggingface.co/docs/transformers/main_classes/text_generation\
          \ )\r\n  warnings.warn(\r\nSetting `pad_token_id` to `eos_token_id`:11 for\
          \ open-end generation.\r\nTraceback (most recent call last):\r\n  File \"\
          /home/anthony/huggingface/main.py\", line 16, in <module>\r\n    sequences\
          \ = pipeline(\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py\"\
          , line 205, in __call__\r\n    return super().__call__(text_inputs, **kwargs)\r\
          \n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
          , line 1140, in __call__\r\n    return self.run_single(inputs, preprocess_params,\
          \ forward_params, postprocess_params)\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
          , line 1147, in run_single\r\n    model_outputs = self.forward(model_inputs,\
          \ **forward_params)\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
          , line 1046, in forward\r\n    model_outputs = self._forward(model_inputs,\
          \ **forward_params)\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py\"\
          , line 268, in _forward\r\n    generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=attention_mask, **generate_kwargs)\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 1648, in generate\r\n    return self.sample(\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 2730, in sample\r\n    outputs = self(\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/home/anthony/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/f7796529e36b2d49094450fb038cc7c4c86afa44/modelling_RW.py\"\
          , line 753, in forward\r\n    transformer_outputs = self.transformer(\r\n\
          \  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/anthony/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/f7796529e36b2d49094450fb038cc7c4c86afa44/modelling_RW.py\"\
          , line 648, in forward\r\n    outputs = block(\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/home/anthony/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/f7796529e36b2d49094450fb038cc7c4c86afa44/modelling_RW.py\"\
          , line 385, in forward\r\n    attn_outputs = self.self_attention(\r\n  File\
          \ \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/home/anthony/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/f7796529e36b2d49094450fb038cc7c4c86afa44/modelling_RW.py\"\
          , line 279, in forward\r\n    attn_output = F.scaled_dot_product_attention(\r\
          \nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasGemmStridedBatchedExFix(handle,\
          \ opa, opb, (int)m, (int)n, (int)k, (void*)&falpha, a, CUDA_R_16BF, (int)lda,\
          \ stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&fbeta, c, CUDA_R_16BF,\
          \ (int)ldc, stridec, (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\r\
          \n\r\nI am using an Anaconda environment on Ubuntu 22.04.3 LTS and here\
          \ are the packages I have installed:\r\n\r\nPackage                  Version\r\
          \n------------------------ --------------\r\naccelerate               0.22.0\r\
          \naccelerator              2023.7.18.dev1\r\nbottle                   0.12.25\r\
          \ncertifi                  2023.7.22\r\ncharset-normalizer       3.2.0\r\
          \ncmake                    3.27.4.1\r\neinops                   0.6.1\r\n\
          filelock                 3.12.3\r\nfsspec                   2023.9.0\r\n\
          huggingface-hub          0.16.4\r\nidna                     3.4\r\ninquirerpy\
          \               0.3.4\r\nJinja2                   3.1.2\r\nlit         \
          \             16.0.6\r\nMarkupSafe               2.1.3\r\nmpmath       \
          \            1.3.0\r\nnetworkx                 3.1\r\nnumpy            \
          \        1.25.2\r\nnvidia-cublas-cu11       11.10.3.66\r\nnvidia-cuda-cupti-cu11\
          \   11.7.101\r\nnvidia-cuda-nvrtc-cu11   11.7.99\r\nnvidia-cuda-runtime-cu11\
          \ 11.7.99\r\nnvidia-cudnn-cu11        8.5.0.96\r\nnvidia-cufft-cu11    \
          \    10.9.0.58\r\nnvidia-curand-cu11       10.2.10.91\r\nnvidia-cusolver-cu11\
          \     11.4.0.1\r\nnvidia-cusparse-cu11     11.7.4.91\r\nnvidia-nccl-cu11\
          \         2.14.3\r\nnvidia-nvtx-cu11         11.7.91\r\npackaging      \
          \          23.1\r\npfzy                     0.3.4\r\nPillow            \
          \       10.0.0\r\npip                      23.2.1\r\nprompt-toolkit    \
          \       3.0.39\r\npsutil                   5.9.5\r\nPyYAML             \
          \      6.0.1\r\nregex                    2023.8.8\r\nrequests          \
          \       2.31.0\r\nsafetensors              0.3.3\r\nsetproctitle       \
          \      1.3.2\r\nsetuptools               68.0.0\r\nsympy               \
          \     1.12\r\ntokenizers               0.13.3\r\ntorch                 \
          \   2.0.1\r\ntorchaudio               2.0.2\r\ntorchvision             \
          \ 0.15.2\r\ntqdm                     4.66.1\r\ntransformers            \
          \ 4.34.0.dev0\r\ntriton                   2.0.0\r\ntyping_extensions   \
          \     4.7.1\r\nurllib3                  2.0.4\r\nwaitress              \
          \   2.1.2\r\nwcwidth                  0.2.6\r\nwheel                   \
          \ 0.38.4\r\n\r\n\r\nAny help would be greatly appreciated thank you."
        updatedAt: '2023-09-08T21:04:51.753Z'
      numEdits: 0
      reactions: []
    id: 64fb8c7331a82e0d4032bcdc
    type: comment
  author: ConorVanek
  content: "Getting the following error when I try to copy/paste and run the model\
    \ using the code on the model card:\r\n\r\nLoading checkpoint shards: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 2/2 [00:19<00:00,  9.83s/it]\r\n/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py:1417:\
    \ UserWarning: You have modified the pretrained model configuration to control\
    \ generation. This is a deprecated strategy to control generation and will be\
    \ removed soon, in a future version. Please use a generation configuration file\
    \ (see https://huggingface.co/docs/transformers/main_classes/text_generation )\r\
    \n  warnings.warn(\r\nSetting `pad_token_id` to `eos_token_id`:11 for open-end\
    \ generation.\r\nTraceback (most recent call last):\r\n  File \"/home/anthony/huggingface/main.py\"\
    , line 16, in <module>\r\n    sequences = pipeline(\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py\"\
    , line 205, in __call__\r\n    return super().__call__(text_inputs, **kwargs)\r\
    \n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
    , line 1140, in __call__\r\n    return self.run_single(inputs, preprocess_params,\
    \ forward_params, postprocess_params)\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
    , line 1147, in run_single\r\n    model_outputs = self.forward(model_inputs, **forward_params)\r\
    \n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/base.py\"\
    , line 1046, in forward\r\n    model_outputs = self._forward(model_inputs, **forward_params)\r\
    \n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/pipelines/text_generation.py\"\
    , line 268, in _forward\r\n    generated_sequence = self.model.generate(input_ids=input_ids,\
    \ attention_mask=attention_mask, **generate_kwargs)\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py\"\
    , line 1648, in generate\r\n    return self.sample(\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/utils.py\"\
    , line 2730, in sample\r\n    outputs = self(\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/home/anthony/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/f7796529e36b2d49094450fb038cc7c4c86afa44/modelling_RW.py\"\
    , line 753, in forward\r\n    transformer_outputs = self.transformer(\r\n  File\
    \ \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/anthony/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/f7796529e36b2d49094450fb038cc7c4c86afa44/modelling_RW.py\"\
    , line 648, in forward\r\n    outputs = block(\r\n  File \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/home/anthony/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/f7796529e36b2d49094450fb038cc7c4c86afa44/modelling_RW.py\"\
    , line 385, in forward\r\n    attn_outputs = self.self_attention(\r\n  File \"\
    /home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/anthony/anaconda3/envs/huggingface/lib/python3.9/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/home/anthony/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/f7796529e36b2d49094450fb038cc7c4c86afa44/modelling_RW.py\"\
    , line 279, in forward\r\n    attn_output = F.scaled_dot_product_attention(\r\n\
    RuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasGemmStridedBatchedExFix(handle,\
    \ opa, opb, (int)m, (int)n, (int)k, (void*)&falpha, a, CUDA_R_16BF, (int)lda,\
    \ stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&fbeta, c, CUDA_R_16BF, (int)ldc,\
    \ stridec, (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\r\n\r\
    \nI am using an Anaconda environment on Ubuntu 22.04.3 LTS and here are the packages\
    \ I have installed:\r\n\r\nPackage                  Version\r\n------------------------\
    \ --------------\r\naccelerate               0.22.0\r\naccelerator           \
    \   2023.7.18.dev1\r\nbottle                   0.12.25\r\ncertifi            \
    \      2023.7.22\r\ncharset-normalizer       3.2.0\r\ncmake                  \
    \  3.27.4.1\r\neinops                   0.6.1\r\nfilelock                 3.12.3\r\
    \nfsspec                   2023.9.0\r\nhuggingface-hub          0.16.4\r\nidna\
    \                     3.4\r\ninquirerpy               0.3.4\r\nJinja2        \
    \           3.1.2\r\nlit                      16.0.6\r\nMarkupSafe           \
    \    2.1.3\r\nmpmath                   1.3.0\r\nnetworkx                 3.1\r\
    \nnumpy                    1.25.2\r\nnvidia-cublas-cu11       11.10.3.66\r\nnvidia-cuda-cupti-cu11\
    \   11.7.101\r\nnvidia-cuda-nvrtc-cu11   11.7.99\r\nnvidia-cuda-runtime-cu11 11.7.99\r\
    \nnvidia-cudnn-cu11        8.5.0.96\r\nnvidia-cufft-cu11        10.9.0.58\r\n\
    nvidia-curand-cu11       10.2.10.91\r\nnvidia-cusolver-cu11     11.4.0.1\r\nnvidia-cusparse-cu11\
    \     11.7.4.91\r\nnvidia-nccl-cu11         2.14.3\r\nnvidia-nvtx-cu11       \
    \  11.7.91\r\npackaging                23.1\r\npfzy                     0.3.4\r\
    \nPillow                   10.0.0\r\npip                      23.2.1\r\nprompt-toolkit\
    \           3.0.39\r\npsutil                   5.9.5\r\nPyYAML               \
    \    6.0.1\r\nregex                    2023.8.8\r\nrequests                 2.31.0\r\
    \nsafetensors              0.3.3\r\nsetproctitle             1.3.2\r\nsetuptools\
    \               68.0.0\r\nsympy                    1.12\r\ntokenizers        \
    \       0.13.3\r\ntorch                    2.0.1\r\ntorchaudio               2.0.2\r\
    \ntorchvision              0.15.2\r\ntqdm                     4.66.1\r\ntransformers\
    \             4.34.0.dev0\r\ntriton                   2.0.0\r\ntyping_extensions\
    \        4.7.1\r\nurllib3                  2.0.4\r\nwaitress                 2.1.2\r\
    \nwcwidth                  0.2.6\r\nwheel                    0.38.4\r\n\r\n\r\n\
    Any help would be greatly appreciated thank you."
  created_at: 2023-09-08 20:04:51+00:00
  edited: false
  hidden: false
  id: 64fb8c7331a82e0d4032bcdc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 79
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED'
