!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rahulseetharaman
conflicting_files: null
created_at: 2023-11-01 20:35:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4ac23b7ef93848232b46896add4f19a2.svg
      fullname: Rahul Seetharaman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rahulseetharaman
      type: user
    createdAt: '2023-11-01T21:35:03.000Z'
    data:
      edited: false
      editors:
      - rahulseetharaman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.456515908241272
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4ac23b7ef93848232b46896add4f19a2.svg
          fullname: Rahul Seetharaman
          isHf: false
          isPro: false
          name: rahulseetharaman
          type: user
        html: "<p>I am trying to generate text using Falcon-7B model. This is the\
          \ name of the model checkpoint on HF<br>rnosov/WizardLM-Uncensored-Falcon-7b-sharded</p>\n\
          <p>I am getting the following error </p>\n<pre><code>RuntimeError      \
          \                        Traceback (most recent call last)\n19 frames\n\
          ~/.cache/huggingface/modules/transformers_modules/ehartford/WizardLM-Uncensored-Falcon-7b/a95d8a001ec405c7d33baf704a190066949f2072/modelling_RW.py\
          \ in forward(self, hidden_states, alibi, attention_mask, layer_past, head_mask,\
          \ use_cache, output_attentions)\n    277             value_layer_ = value_layer.reshape(batch_size,\
          \ self.num_kv, -1, self.head_dim)\n    278 \n--&gt; 279             attn_output\
          \ = F.scaled_dot_product_attention(\n    280                 query_layer_,\
          \ key_layer_, value_layer_, None, 0.0, is_causal=True\n    281         \
          \    )\n\nRuntimeError: Expected query, key, and value to have the same\
          \ dtype, but got query.dtype: float key.dtype: float and value.dtype: c10::Half\
          \ instead.\n</code></pre>\n<p>This is how I invoke the code. </p>\n<pre><code>model\
          \ = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_8bit=True,\n\
          \    trust_remote_code=True\n)\nmodel.config.use_cache = False\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name, return_token_type_ids=False)\n\
          \ndef generate_text(prompt, prefix, max_new_tokens=100, num_beams=3, temperature=0.7,\
          \ num_return_sequences = 3):\n  model_inputs = tokenizer(prompt + prefix,\
          \ return_tensors='pt', return_token_type_ids=False)\n  model_output = model.generate(**model_inputs,\n\
          \                                max_new_tokens=max_new_tokens,\n      \
          \                          num_beams=num_beams,\n                      \
          \          do_sample=True,\n                                temperature=temperature,\n\
          \                                num_return_sequences=num_return_sequences)\n\
          \  output_text = tokenizer.batch_decode(model_output, skip_special_tokens=True)\n\
          \  return output_text\n</code></pre>\n<p>How do I resolve this issue ? Any\
          \ help in debugging this is appreciated, thanks!</p>\n"
        raw: "I am trying to generate text using Falcon-7B model. This is the name\
          \ of the model checkpoint on HF\r\nrnosov/WizardLM-Uncensored-Falcon-7b-sharded\r\
          \n\r\nI am getting the following error \r\n\r\n```\r\nRuntimeError     \
          \                         Traceback (most recent call last)\r\n19 frames\r\
          \n~/.cache/huggingface/modules/transformers_modules/ehartford/WizardLM-Uncensored-Falcon-7b/a95d8a001ec405c7d33baf704a190066949f2072/modelling_RW.py\
          \ in forward(self, hidden_states, alibi, attention_mask, layer_past, head_mask,\
          \ use_cache, output_attentions)\r\n    277             value_layer_ = value_layer.reshape(batch_size,\
          \ self.num_kv, -1, self.head_dim)\r\n    278 \r\n--> 279             attn_output\
          \ = F.scaled_dot_product_attention(\r\n    280                 query_layer_,\
          \ key_layer_, value_layer_, None, 0.0, is_causal=True\r\n    281       \
          \      )\r\n\r\nRuntimeError: Expected query, key, and value to have the\
          \ same dtype, but got query.dtype: float key.dtype: float and value.dtype:\
          \ c10::Half instead.\r\n```\r\n\r\nThis is how I invoke the code. \r\n\r\
          \n```\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_name,\r\
          \n    load_in_8bit=True,\r\n    trust_remote_code=True\r\n)\r\nmodel.config.use_cache\
          \ = False\r\ntokenizer = AutoTokenizer.from_pretrained(model_name, return_token_type_ids=False)\r\
          \n\r\ndef generate_text(prompt, prefix, max_new_tokens=100, num_beams=3,\
          \ temperature=0.7, num_return_sequences = 3):\r\n  model_inputs = tokenizer(prompt\
          \ + prefix, return_tensors='pt', return_token_type_ids=False)\r\n  model_output\
          \ = model.generate(**model_inputs,\r\n                                max_new_tokens=max_new_tokens,\r\
          \n                                num_beams=num_beams,\r\n             \
          \                   do_sample=True,\r\n                                temperature=temperature,\r\
          \n                                num_return_sequences=num_return_sequences)\r\
          \n  output_text = tokenizer.batch_decode(model_output, skip_special_tokens=True)\r\
          \n  return output_text\r\n```\r\n\r\nHow do I resolve this issue ? Any help\
          \ in debugging this is appreciated, thanks!"
        updatedAt: '2023-11-01T21:35:03.102Z'
      numEdits: 0
      reactions: []
    id: 6542c4872cc14ec6fd9741ea
    type: comment
  author: rahulseetharaman
  content: "I am trying to generate text using Falcon-7B model. This is the name of\
    \ the model checkpoint on HF\r\nrnosov/WizardLM-Uncensored-Falcon-7b-sharded\r\
    \n\r\nI am getting the following error \r\n\r\n```\r\nRuntimeError           \
    \                   Traceback (most recent call last)\r\n19 frames\r\n~/.cache/huggingface/modules/transformers_modules/ehartford/WizardLM-Uncensored-Falcon-7b/a95d8a001ec405c7d33baf704a190066949f2072/modelling_RW.py\
    \ in forward(self, hidden_states, alibi, attention_mask, layer_past, head_mask,\
    \ use_cache, output_attentions)\r\n    277             value_layer_ = value_layer.reshape(batch_size,\
    \ self.num_kv, -1, self.head_dim)\r\n    278 \r\n--> 279             attn_output\
    \ = F.scaled_dot_product_attention(\r\n    280                 query_layer_, key_layer_,\
    \ value_layer_, None, 0.0, is_causal=True\r\n    281             )\r\n\r\nRuntimeError:\
    \ Expected query, key, and value to have the same dtype, but got query.dtype:\
    \ float key.dtype: float and value.dtype: c10::Half instead.\r\n```\r\n\r\nThis\
    \ is how I invoke the code. \r\n\r\n```\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\
    \n    model_name,\r\n    load_in_8bit=True,\r\n    trust_remote_code=True\r\n\
    )\r\nmodel.config.use_cache = False\r\ntokenizer = AutoTokenizer.from_pretrained(model_name,\
    \ return_token_type_ids=False)\r\n\r\ndef generate_text(prompt, prefix, max_new_tokens=100,\
    \ num_beams=3, temperature=0.7, num_return_sequences = 3):\r\n  model_inputs =\
    \ tokenizer(prompt + prefix, return_tensors='pt', return_token_type_ids=False)\r\
    \n  model_output = model.generate(**model_inputs,\r\n                        \
    \        max_new_tokens=max_new_tokens,\r\n                                num_beams=num_beams,\r\
    \n                                do_sample=True,\r\n                        \
    \        temperature=temperature,\r\n                                num_return_sequences=num_return_sequences)\r\
    \n  output_text = tokenizer.batch_decode(model_output, skip_special_tokens=True)\r\
    \n  return output_text\r\n```\r\n\r\nHow do I resolve this issue ? Any help in\
    \ debugging this is appreciated, thanks!"
  created_at: 2023-11-01 20:35:03+00:00
  edited: false
  hidden: false
  id: 6542c4872cc14ec6fd9741ea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 90
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: Falcon-7B decoding error
