!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BliepBlop
conflicting_files: null
created_at: 2023-06-06 15:20:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d7f0bfd71b62f66af23a406c0b2441c.svg
      fullname: Mario VANHECKE
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BliepBlop
      type: user
    createdAt: '2023-06-06T16:20:34.000Z'
    data:
      edited: false
      editors:
      - BliepBlop
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3999367654323578
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d7f0bfd71b62f66af23a406c0b2441c.svg
          fullname: Mario VANHECKE
          isHf: false
          isPro: false
          name: BliepBlop
          type: user
        html: "<p>Hi I am trying to finetune this model.</p>\n<p>I downloaded a dataset\
          \ from :<br><a href=\"https://huggingface.co/datasets/deepmind/code_contests\"\
          >https://huggingface.co/datasets/deepmind/code_contests</a></p>\n<p>Now\
          \ I want to finetune this model using a script I build, now I get this error</p>\n\
          <pre><code>from datasets import load_dataset\nfrom trl import SFTTrainer\n\
          from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers\
          \ import TrainingArguments\n\nmodel_id = \"./\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\
          model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n\
          dataset = load_dataset(\"parquet\", data_files='/Users/mario/Downloads/code_contests/data/*.parquet',\
          \ split='train')\n\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token':\
          \ '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))\n\n# Preprocess\
          \ your dataset\ndef formatting_func(example):\n    text = f\"### Question:\
          \ {example['description']}\\n ### Answer: {example['solutions'][0]['solution']}\"\
          \n    return text\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\
          \          # output directory\n    num_train_epochs=3,              # total\
          \ number of training epochs\n    per_device_train_batch_size=16,  # batch\
          \ size per device during training\n    per_device_eval_batch_size=64,  \
          \ # batch size for evaluation\n    warmup_steps=500,                # number\
          \ of warmup steps for learning rate scheduler\n    weight_decay=0.01,  \
          \             # strength of weight decay\n    logging_dir='./logs',    \
          \        # directory for storing logs\n    logging_steps=10,\n    evaluation_strategy=\"\
          steps\",     # evaluation is done at each logging step\n    save_strategy=\"\
          steps\",           # model checkpoints are saved at each logging step\n\
          \    eval_steps=10,                   # evaluation and checkpoint saving\
          \ is done every 10 steps\n    load_best_model_at_end=True,     # the best\
          \ model is loaded at the end of training\n    metric_for_best_model=\"loss\"\
          ,    # use loss to determine the best model\n    greater_is_better=False,\
          \         # lower loss is better\n)\n\ntrainer = SFTTrainer(\n    model,\n\
          \    args=training_args,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n\
          \    formatting_func=formatting_func\n)\n\ntrainer.train()\n</code></pre>\n\
          <p>The error I get:</p>\n<pre><code>Loading checkpoint shards: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:22&lt;00:00, 11.05s/it]\n\
          Resolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 41/41 [00:00&lt;00:00, 33019.67it/s]\nFound cached dataset parquet (/Users/mario/.cache/huggingface/datasets/parquet/default-f2feb2edba9ed25e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n\
          Using pad_token, but it is not set yet.\n/opt/homebrew/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:165:\
          \ UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer,\
          \ this will default to 1024\n  warnings.warn(\nLoading cached processed\
          \ dataset at /Users/mario/.cache/huggingface/datasets/parquet/default-f2feb2edba9ed25e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-12ea81fe9ef7fc4c.arrow\n\
          /opt/homebrew/lib/python3.11/site-packages/transformers/optimization.py:407:\
          \ FutureWarning: This implementation of AdamW is deprecated and will be\
          \ removed in a future version. Use the PyTorch implementation torch.optim.AdamW\
          \ instead, or set `no_deprecation_warning=True` to disable this warning\n\
          \  warnings.warn(\n  0%|                                               \
          \                                                                      \
          \                                                                      \
          \                        | 0/3 [00:00&lt;?, ?it/s]You're using a PreTrainedTokenizerFast\
          \ tokenizer. Please note that with a fast tokenizer, using the `__call__`\
          \ method is faster than using a method to encode the text followed by a\
          \ call to the `pad` method to get a padded encoding.\nTraceback (most recent\
          \ call last):\n  File \"/Users/mario/Downloads/falcon-7b/finetune2.py\"\
          , line 46, in &lt;module&gt;\n    trainer.train()\n  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 1664, in train\n    return inner_training_loop(\n           ^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 1940, in _inner_training_loop\n    tr_loss_step = self.training_step(model,\
          \ inputs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \"/opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 2735, in training_step\n    loss = self.compute_loss(model, inputs)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 2767, in compute_loss\n    outputs = model(**inputs)\n          \
          \    ^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mario/.cache/huggingface/modules/transformers_modules/modelling_RW.py\"\
          , line 753, in forward\n    transformer_outputs = self.transformer(\n  \
          \                        ^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mario/.cache/huggingface/modules/transformers_modules/modelling_RW.py\"\
          , line 574, in forward\n    batch_size, seq_length = input_ids.shape\n \
          \   ^^^^^^^^^^^^^^^^^^^^^^\nValueError: not enough values to unpack (expected\
          \ 2, got 1)\n</code></pre>\n"
        raw: "Hi I am trying to finetune this model.\r\n\r\nI downloaded a dataset\
          \ from :\r\nhttps://huggingface.co/datasets/deepmind/code_contests\r\n\r\
          \nNow I want to finetune this model using a script I build, now I get this\
          \ error\r\n\r\n```\r\nfrom datasets import load_dataset\r\nfrom trl import\
          \ SFTTrainer\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
          \nfrom transformers import TrainingArguments\r\n\r\nmodel_id = \"./\"\r\n\
          \r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\
          \ trust_remote_code=True)\r\ndataset = load_dataset(\"parquet\", data_files='/Users/mario/Downloads/code_contests/data/*.parquet',\
          \ split='train')\r\n\r\nif tokenizer.pad_token is None:\r\n    tokenizer.add_special_tokens({'pad_token':\
          \ '[PAD]'})\r\n    model.resize_token_embeddings(len(tokenizer))\r\n\r\n\
          # Preprocess your dataset\r\ndef formatting_func(example):\r\n    text =\
          \ f\"### Question: {example['description']}\\n ### Answer: {example['solutions'][0]['solution']}\"\
          \r\n    return text\r\n\r\ntraining_args = TrainingArguments(\r\n    output_dir='./results',\
          \          # output directory\r\n    num_train_epochs=3,              #\
          \ total number of training epochs\r\n    per_device_train_batch_size=16,\
          \  # batch size per device during training\r\n    per_device_eval_batch_size=64,\
          \   # batch size for evaluation\r\n    warmup_steps=500,               \
          \ # number of warmup steps for learning rate scheduler\r\n    weight_decay=0.01,\
          \               # strength of weight decay\r\n    logging_dir='./logs',\
          \            # directory for storing logs\r\n    logging_steps=10,\r\n \
          \   evaluation_strategy=\"steps\",     # evaluation is done at each logging\
          \ step\r\n    save_strategy=\"steps\",           # model checkpoints are\
          \ saved at each logging step\r\n    eval_steps=10,                   # evaluation\
          \ and checkpoint saving is done every 10 steps\r\n    load_best_model_at_end=True,\
          \     # the best model is loaded at the end of training\r\n    metric_for_best_model=\"\
          loss\",    # use loss to determine the best model\r\n    greater_is_better=False,\
          \         # lower loss is better\r\n)\r\n\r\ntrainer = SFTTrainer(\r\n \
          \   model,\r\n    args=training_args,\r\n    tokenizer=tokenizer,\r\n  \
          \  train_dataset=dataset,\r\n    formatting_func=formatting_func\r\n)\r\n\
          \r\ntrainer.train()\r\n```\r\n\r\n\r\nThe error I get:\r\n```\r\nLoading\
          \ checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 2/2 [00:22<00:00, 11.05s/it]\r\nResolving data files: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 41/41 [00:00<00:00, 33019.67it/s]\r\
          \nFound cached dataset parquet (/Users/mario/.cache/huggingface/datasets/parquet/default-f2feb2edba9ed25e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\r\
          \nUsing pad_token, but it is not set yet.\r\n/opt/homebrew/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:165:\
          \ UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer,\
          \ this will default to 1024\r\n  warnings.warn(\r\nLoading cached processed\
          \ dataset at /Users/mario/.cache/huggingface/datasets/parquet/default-f2feb2edba9ed25e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-12ea81fe9ef7fc4c.arrow\r\
          \n/opt/homebrew/lib/python3.11/site-packages/transformers/optimization.py:407:\
          \ FutureWarning: This implementation of AdamW is deprecated and will be\
          \ removed in a future version. Use the PyTorch implementation torch.optim.AdamW\
          \ instead, or set `no_deprecation_warning=True` to disable this warning\r\
          \n  warnings.warn(\r\n  0%|                                            \
          \                                                                      \
          \                                                                      \
          \                           | 0/3 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast\
          \ tokenizer. Please note that with a fast tokenizer, using the `__call__`\
          \ method is faster than using a method to encode the text followed by a\
          \ call to the `pad` method to get a padded encoding.\r\nTraceback (most\
          \ recent call last):\r\n  File \"/Users/mario/Downloads/falcon-7b/finetune2.py\"\
          , line 46, in <module>\r\n    trainer.train()\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 1664, in train\r\n    return inner_training_loop(\r\n           ^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 1940, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model,\
          \ inputs)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\
          \ \"/opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 2735, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py\"\
          , line 2767, in compute_loss\r\n    outputs = model(**inputs)\r\n      \
          \        ^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/mario/.cache/huggingface/modules/transformers_modules/modelling_RW.py\"\
          , line 753, in forward\r\n    transformer_outputs = self.transformer(\r\n\
          \                          ^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/mario/.cache/huggingface/modules/transformers_modules/modelling_RW.py\"\
          , line 574, in forward\r\n    batch_size, seq_length = input_ids.shape\r\
          \n    ^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: not enough values to unpack\
          \ (expected 2, got 1)\r\n```"
        updatedAt: '2023-06-06T16:20:34.627Z'
      numEdits: 0
      reactions: []
    id: 647f5cd2e9c81260ff87a5cb
    type: comment
  author: BliepBlop
  content: "Hi I am trying to finetune this model.\r\n\r\nI downloaded a dataset from\
    \ :\r\nhttps://huggingface.co/datasets/deepmind/code_contests\r\n\r\nNow I want\
    \ to finetune this model using a script I build, now I get this error\r\n\r\n\
    ```\r\nfrom datasets import load_dataset\r\nfrom trl import SFTTrainer\r\nfrom\
    \ transformers import AutoTokenizer, AutoModelForCausalLM\r\nfrom transformers\
    \ import TrainingArguments\r\n\r\nmodel_id = \"./\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\r\
    \ndataset = load_dataset(\"parquet\", data_files='/Users/mario/Downloads/code_contests/data/*.parquet',\
    \ split='train')\r\n\r\nif tokenizer.pad_token is None:\r\n    tokenizer.add_special_tokens({'pad_token':\
    \ '[PAD]'})\r\n    model.resize_token_embeddings(len(tokenizer))\r\n\r\n# Preprocess\
    \ your dataset\r\ndef formatting_func(example):\r\n    text = f\"### Question:\
    \ {example['description']}\\n ### Answer: {example['solutions'][0]['solution']}\"\
    \r\n    return text\r\n\r\ntraining_args = TrainingArguments(\r\n    output_dir='./results',\
    \          # output directory\r\n    num_train_epochs=3,              # total\
    \ number of training epochs\r\n    per_device_train_batch_size=16,  # batch size\
    \ per device during training\r\n    per_device_eval_batch_size=64,   # batch size\
    \ for evaluation\r\n    warmup_steps=500,                # number of warmup steps\
    \ for learning rate scheduler\r\n    weight_decay=0.01,               # strength\
    \ of weight decay\r\n    logging_dir='./logs',            # directory for storing\
    \ logs\r\n    logging_steps=10,\r\n    evaluation_strategy=\"steps\",     # evaluation\
    \ is done at each logging step\r\n    save_strategy=\"steps\",           # model\
    \ checkpoints are saved at each logging step\r\n    eval_steps=10,           \
    \        # evaluation and checkpoint saving is done every 10 steps\r\n    load_best_model_at_end=True,\
    \     # the best model is loaded at the end of training\r\n    metric_for_best_model=\"\
    loss\",    # use loss to determine the best model\r\n    greater_is_better=False,\
    \         # lower loss is better\r\n)\r\n\r\ntrainer = SFTTrainer(\r\n    model,\r\
    \n    args=training_args,\r\n    tokenizer=tokenizer,\r\n    train_dataset=dataset,\r\
    \n    formatting_func=formatting_func\r\n)\r\n\r\ntrainer.train()\r\n```\r\n\r\
    \n\r\nThe error I get:\r\n```\r\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 2/2 [00:22<00:00, 11.05s/it]\r\nResolving data files:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 41/41 [00:00<00:00, 33019.67it/s]\r\
    \nFound cached dataset parquet (/Users/mario/.cache/huggingface/datasets/parquet/default-f2feb2edba9ed25e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\r\
    \nUsing pad_token, but it is not set yet.\r\n/opt/homebrew/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:165:\
    \ UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer,\
    \ this will default to 1024\r\n  warnings.warn(\r\nLoading cached processed dataset\
    \ at /Users/mario/.cache/huggingface/datasets/parquet/default-f2feb2edba9ed25e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-12ea81fe9ef7fc4c.arrow\r\
    \n/opt/homebrew/lib/python3.11/site-packages/transformers/optimization.py:407:\
    \ FutureWarning: This implementation of AdamW is deprecated and will be removed\
    \ in a future version. Use the PyTorch implementation torch.optim.AdamW instead,\
    \ or set `no_deprecation_warning=True` to disable this warning\r\n  warnings.warn(\r\
    \n  0%|                                                                      \
    \                                                                            \
    \                                                                 | 0/3 [00:00<?,\
    \ ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with\
    \ a fast tokenizer, using the `__call__` method is faster than using a method\
    \ to encode the text followed by a call to the `pad` method to get a padded encoding.\r\
    \nTraceback (most recent call last):\r\n  File \"/Users/mario/Downloads/falcon-7b/finetune2.py\"\
    , line 46, in <module>\r\n    trainer.train()\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py\"\
    , line 1664, in train\r\n    return inner_training_loop(\r\n           ^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py\"\
    , line 1940, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model,\
    \ inputs)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
    /opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py\", line 2735,\
    \ in training_step\r\n    loss = self.compute_loss(model, inputs)\r\n        \
    \   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py\"\
    , line 2767, in compute_loss\r\n    outputs = model(**inputs)\r\n            \
    \  ^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n   \
    \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/mario/.cache/huggingface/modules/transformers_modules/modelling_RW.py\"\
    , line 753, in forward\r\n    transformer_outputs = self.transformer(\r\n    \
    \                      ^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n   \
    \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/mario/.cache/huggingface/modules/transformers_modules/modelling_RW.py\"\
    , line 574, in forward\r\n    batch_size, seq_length = input_ids.shape\r\n   \
    \ ^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: not enough values to unpack (expected\
    \ 2, got 1)\r\n```"
  created_at: 2023-06-06 15:20:34+00:00
  edited: false
  hidden: false
  id: 647f5cd2e9c81260ff87a5cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6d7f0bfd71b62f66af23a406c0b2441c.svg
      fullname: Mario VANHECKE
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BliepBlop
      type: user
    createdAt: '2023-06-07T15:20:17.000Z'
    data:
      status: closed
    id: 6480a031e1421e205fda2574
    type: status-change
  author: BliepBlop
  created_at: 2023-06-07 14:20:17+00:00
  id: 6480a031e1421e205fda2574
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: tiiuae/falcon-7b
repo_type: model
status: closed
target_branch: null
title: Trying to finetune ... need help
