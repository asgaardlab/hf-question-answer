!!python/object:huggingface_hub.community.DiscussionWithDetails
author: romainbarraud
conflicting_files: null
created_at: 2023-05-29 11:51:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678713660385-614bcf2096e9836800aa6e01.jpeg?w=200&h=200&f=face
      fullname: Romain BARRAUD
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: romainbarraud
      type: user
    createdAt: '2023-05-29T12:51:38.000Z'
    data:
      edited: false
      editors:
      - romainbarraud
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678713660385-614bcf2096e9836800aa6e01.jpeg?w=200&h=200&f=face
          fullname: Romain BARRAUD
          isHf: false
          isPro: false
          name: romainbarraud
          type: user
        html: '<p>Hi. The model (as well as the instruct version) fails with the following
          error:</p>

          <p>The model ''RWForCausalLM'' is not supported for text-generation. Supported
          models are [''BartForCausalLM'', ''BertLMHeadModel'', ''BertGenerationDecoder'',
          ''BigBirdForCausalLM'', ''BigBirdPegasusForCausalLM'', ''BioGptForCausalLM'',
          ''BlenderbotForCausalLM'', ''BlenderbotSmallForCausalLM'', ''BloomForCausalLM'',
          ''CamembertForCausalLM'', ''CodeGenForCausalLM'', ''CpmAntForCausalLM'',
          ''CTRLLMHeadModel'', ''Data2VecTextForCausalLM'', ''ElectraForCausalLM'',
          ''ErnieForCausalLM'', ''GitForCausalLM'', ''GPT2LMHeadModel'', ''GPT2LMHeadModel'',
          ''GPTBigCodeForCausalLM'', ''GPTNeoForCausalLM'', ''GPTNeoXForCausalLM'',
          ''GPTNeoXJapaneseForCausalLM'', ''GPTJForCausalLM'', ''LlamaForCausalLM'',
          ''MarianForCausalLM'', ''MBartForCausalLM'', ''MegaForCausalLM'', ''MegatronBertForCausalLM'',
          ''MvpForCausalLM'', ''OpenLlamaForCausalLM'', ''OpenAIGPTLMHeadModel'',
          ''OPTForCausalLM'', ''PegasusForCausalLM'', ''PLBartForCausalLM'', ''ProphetNetForCausalLM'',
          ''QDQBertLMHeadModel'', ''ReformerModelWithLMHead'', ''RemBertForCausalLM'',
          ''RobertaForCausalLM'', ''RobertaPreLayerNormForCausalLM'', ''RoCBertForCausalLM'',
          ''RoFormerForCausalLM'', ''RwkvForCausalLM'', ''Speech2Text2ForCausalLM'',
          ''TransfoXLLMHeadModel'', ''TrOCRForCausalLM'', ''XGLMForCausalLM'', ''XLMWithLMHeadModel'',
          ''XLMProphetNetForCausalLM'', ''XLMRobertaForCausalLM'', ''XLMRobertaXLForCausalLM'',
          ''XLNetLMHeadModel'', ''XmodForCausalLM''].<br>Setting <code>pad_token_id</code>
          to <code>eos_token_id</code>:11 for open-end generation.</p>

          '
        raw: "Hi. The model (as well as the instruct version) fails with the following\
          \ error:\r\n\r\nThe model 'RWForCausalLM' is not supported for text-generation.\
          \ Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
          \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM',\
          \ 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM',\
          \ 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel',\
          \ 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM',\
          \ 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM',\
          \ 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
          \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
          \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
          \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
          \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
          \ 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
          \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
          \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel',\
          \ 'XmodForCausalLM'].\r\nSetting `pad_token_id` to `eos_token_id`:11 for\
          \ open-end generation."
        updatedAt: '2023-05-29T12:51:38.264Z'
      numEdits: 0
      reactions: []
    id: 64749fdaa855203d8feaf364
    type: comment
  author: romainbarraud
  content: "Hi. The model (as well as the instruct version) fails with the following\
    \ error:\r\n\r\nThe model 'RWForCausalLM' is not supported for text-generation.\
    \ Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
    \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM',\
    \ 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM',\
    \ 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM',\
    \ 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel',\
    \ 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM',\
    \ 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM',\
    \ 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM',\
    \ 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM',\
    \ 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
    \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
    \ 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
    \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
    \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\r\
    \nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation."
  created_at: 2023-05-29 11:51:38+00:00
  edited: false
  hidden: false
  id: 64749fdaa855203d8feaf364
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T06:54:48.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>Sorry about the delay, the <code>The model ''RWForCausalLM'' is
          not supported for text-generation</code> comes from the model not being
          integrated into the core part of the transformers library yet. It''s just
          a warning, and generation should follow afterwards.  See for example: <a
          rel="nofollow" href="https://twitter.com/camenduru/status/1662225039352283137?s=20">https://twitter.com/camenduru/status/1662225039352283137?s=20</a>
          of a video where it is working correctly. </p>

          <p>It will take a little bit of time to integrate the model fully into the
          transformers library, but hopefully in a couple of weeks this warning will
          go away.</p>

          '
        raw: "Sorry about the delay, the `The model 'RWForCausalLM' is not supported\
          \ for text-generation` comes from the model not being integrated into the\
          \ core part of the transformers library yet. It's just a warning, and generation\
          \ should follow afterwards.  See for example: https://twitter.com/camenduru/status/1662225039352283137?s=20\
          \ of a video where it is working correctly. \n\nIt will take a little bit\
          \ of time to integrate the model fully into the transformers library, but\
          \ hopefully in a couple of weeks this warning will go away."
        updatedAt: '2023-05-30T06:54:48.454Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64759db809e773226332716b
    id: 64759db809e773226332716a
    type: comment
  author: FalconLLM
  content: "Sorry about the delay, the `The model 'RWForCausalLM' is not supported\
    \ for text-generation` comes from the model not being integrated into the core\
    \ part of the transformers library yet. It's just a warning, and generation should\
    \ follow afterwards.  See for example: https://twitter.com/camenduru/status/1662225039352283137?s=20\
    \ of a video where it is working correctly. \n\nIt will take a little bit of time\
    \ to integrate the model fully into the transformers library, but hopefully in\
    \ a couple of weeks this warning will go away."
  created_at: 2023-05-30 05:54:48+00:00
  edited: false
  hidden: false
  id: 64759db809e773226332716a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T06:54:48.000Z'
    data:
      status: closed
    id: 64759db809e773226332716b
    type: status-change
  author: FalconLLM
  created_at: 2023-05-30 05:54:48+00:00
  id: 64759db809e773226332716b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: tiiuae/falcon-7b
repo_type: model
status: closed
target_branch: null
title: Colab fails
