!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Talha
conflicting_files: null
created_at: 2023-05-28 09:02:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7d285f327fa48775d2f504f5b9dd04b.svg
      fullname: Talha Anwar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Talha
      type: user
    createdAt: '2023-05-28T10:02:58.000Z'
    data:
      edited: false
      editors:
      - Talha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7d285f327fa48775d2f504f5b9dd04b.svg
          fullname: Talha Anwar
          isHf: false
          isPro: false
          name: Talha
          type: user
        html: "<p>I am trying to use falcon with langhcain<br>this is what I am doing<br><strong>model</strong></p>\n\
          <pre><code>model_id='falcon-7b'\ntokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True\
          \ )\nmodel = AutoModelForCausalLM.from_pretrained(model_id,trust_remote_code=True,\n\
          \n                                             quantization_config = BitsAndBytesConfig(load_in_4bit=True,\
          \ \n                                                                   \
          \     bnb_4bit_compute_dtype=torch.bfloat16),\n                        \
          \                     device_map='auto')\npipe = pipeline(\n        \"text-generation\"\
          ,\n        model=model, \n        tokenizer=tokenizer, \n        max_new_tokens=256\n\
          \    )\n\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\n</code></pre>\n\
          <p><strong>embeddings</strong></p>\n<pre><code>hfemb = HuggingFaceEmbeddings()\n\
          retriever = vector_db.as_retriever()\n</code></pre>\n<p><strong>memory</strong></p>\n\
          <pre><code>memory = ConversationBufferMemory(memory_key=\"chat_history\"\
          , return_messages=True,output_key='answer')\nchain = ConversationalRetrievalChain.from_llm(local_llm,\n\
          \                                           retriever=retriever, memory=memory,\n\
          \                                          chain_type=\"map_reduce\", #\n\
          \                                          return_source_documents=True)\n\
          </code></pre>\n<p>The code worked with flan-t5 but give error with falcon,</p>\n\
          <p>Here is full error</p>\n<pre><code>\u2502 [Errno 2] No such file or directory:\
          \ '/tmp/ipykernel_67349/1702757523.py'                        \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:140\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   137 \u2502   \u2502   \u2502   )                            \
          \                                                  \u2502\n\u2502   138\
          \ \u2502   \u2502   except (KeyboardInterrupt, Exception) as e:        \
          \                                \u2502\n\u2502   139 \u2502   \u2502  \
          \ \u2502   run_manager.on_chain_error(e)                               \
          \                   \u2502\n\u2502 \u2771 140 \u2502   \u2502   \u2502 \
          \  raise e                                                             \
          \           \u2502\n\u2502   141 \u2502   \u2502   run_manager.on_chain_end(outputs)\
          \                                                  \u2502\n\u2502   142\
          \ \u2502   \u2502   return self.prep_outputs(inputs, outputs, return_only_outputs)\
          \                     \u2502\n\u2502   143                             \
          \                                                               \u2502\n\
          \u2502                                                                 \
          \                                 \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:134\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   131 \u2502   \u2502   )                                     \
          \                                             \u2502\n\u2502   132 \u2502\
          \   \u2502   try:                                                      \
          \                         \u2502\n\u2502   133 \u2502   \u2502   \u2502\
          \   outputs = (                                                        \
          \            \u2502\n\u2502 \u2771 134 \u2502   \u2502   \u2502   \u2502\
          \   self._call(inputs, run_manager=run_manager)                        \
          \        \u2502\n\u2502   135 \u2502   \u2502   \u2502   \u2502   if new_arg_supported\
          \                                                       \u2502\n\u2502 \
          \  136 \u2502   \u2502   \u2502   \u2502   else self._call(inputs)     \
          \                                               \u2502\n\u2502   137 \u2502\
          \   \u2502   \u2502   )                                                \
          \                              \u2502\n\u2502                          \
          \                                                                      \
          \  \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:\
          \ \u2502\n\u2502 110 in _call                                          \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   107 \u2502   \u2502   new_inputs = inputs.copy()\
          \                                                         \u2502\n\u2502\
          \   108 \u2502   \u2502   new_inputs[\"question\"] = new_question      \
          \                                        \u2502\n\u2502   109 \u2502   \u2502\
          \   new_inputs[\"chat_history\"] = chat_history_str                    \
          \                  \u2502\n\u2502 \u2771 110 \u2502   \u2502   answer =\
          \ self.combine_docs_chain.run(                                         \
          \     \u2502\n\u2502   111 \u2502   \u2502   \u2502   input_documents=docs,\
          \ callbacks=_run_manager.get_child(), **new_inputs         \u2502\n\u2502\
          \   112 \u2502   \u2502   )                                            \
          \                                      \u2502\n\u2502   113 \u2502   \u2502\
          \   if self.return_source_documents:                                   \
          \                \u2502\n\u2502                                        \
          \                                                          \u2502\n\u2502\
          \ /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:239\
          \ in run                \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   236 \u2502   \u2502   \u2502   return self(args[0], callbacks=callbacks)[self.output_keys[0]]\
          \                 \u2502\n\u2502   237 \u2502   \u2502                 \
          \                                                                     \u2502\
          \n\u2502   238 \u2502   \u2502   if kwargs and not args:               \
          \                                             \u2502\n\u2502 \u2771 239\
          \ \u2502   \u2502   \u2502   return self(kwargs, callbacks=callbacks)[self.output_keys[0]]\
          \                  \u2502\n\u2502   240 \u2502   \u2502                \
          \                                                                      \u2502\
          \n\u2502   241 \u2502   \u2502   if not kwargs and not args:           \
          \                                             \u2502\n\u2502   242 \u2502\
          \   \u2502   \u2502   raise ValueError(                                \
          \                              \u2502\n\u2502                          \
          \                                                                      \
          \  \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:140\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   137 \u2502   \u2502   \u2502   )                            \
          \                                                  \u2502\n\u2502   138\
          \ \u2502   \u2502   except (KeyboardInterrupt, Exception) as e:        \
          \                                \u2502\n\u2502   139 \u2502   \u2502  \
          \ \u2502   run_manager.on_chain_error(e)                               \
          \                   \u2502\n\u2502 \u2771 140 \u2502   \u2502   \u2502 \
          \  raise e                                                             \
          \           \u2502\n\u2502   141 \u2502   \u2502   run_manager.on_chain_end(outputs)\
          \                                                  \u2502\n\u2502   142\
          \ \u2502   \u2502   return self.prep_outputs(inputs, outputs, return_only_outputs)\
          \                     \u2502\n\u2502   143                             \
          \                                                               \u2502\n\
          \u2502                                                                 \
          \                                 \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:134\
          \ in __call__           \u2502\n\u2502                                 \
          \                                                                 \u2502\
          \n\u2502   131 \u2502   \u2502   )                                     \
          \                                             \u2502\n\u2502   132 \u2502\
          \   \u2502   try:                                                      \
          \                         \u2502\n\u2502   133 \u2502   \u2502   \u2502\
          \   outputs = (                                                        \
          \            \u2502\n\u2502 \u2771 134 \u2502   \u2502   \u2502   \u2502\
          \   self._call(inputs, run_manager=run_manager)                        \
          \        \u2502\n\u2502   135 \u2502   \u2502   \u2502   \u2502   if new_arg_supported\
          \                                                       \u2502\n\u2502 \
          \  136 \u2502   \u2502   \u2502   \u2502   else self._call(inputs)     \
          \                                               \u2502\n\u2502   137 \u2502\
          \   \u2502   \u2502   )                                                \
          \                              \u2502\n\u2502                          \
          \                                                                      \
          \  \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\
          \ in   \u2502\n\u2502 _call                                            \
          \                                                \u2502\n\u2502        \
          \                                                                      \
          \                    \u2502\n\u2502    81 \u2502   \u2502   docs = inputs[self.input_key]\
          \                                                      \u2502\n\u2502  \
          \  82 \u2502   \u2502   # Other keys are assumed to be needed for LLM prediction\
          \                           \u2502\n\u2502    83 \u2502   \u2502   other_keys\
          \ = {k: v for k, v in inputs.items() if k != self.input_key}           \
          \   \u2502\n\u2502 \u2771  84 \u2502   \u2502   output, extra_return_dict\
          \ = self.combine_docs(                                     \u2502\n\u2502\
          \    85 \u2502   \u2502   \u2502   docs, callbacks=_run_manager.get_child(),\
          \ **other_keys                         \u2502\n\u2502    86 \u2502   \u2502\
          \   )                                                                  \
          \                \u2502\n\u2502    87 \u2502   \u2502   extra_return_dict[self.output_key]\
          \ = output                                        \u2502\n\u2502       \
          \                                                                      \
          \                     \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:1\
          \ \u2502\n\u2502 44 in combine_docs                                    \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   141 \u2502   \u2502   Combine by mapping\
          \ first chain over all documents, then reducing the results.      \u2502\
          \n\u2502   142 \u2502   \u2502   This reducing can be done recursively if\
          \ needed (if there are many documents).     \u2502\n\u2502   143 \u2502\
          \   \u2502   \"\"\"                                                    \
          \                            \u2502\n\u2502 \u2771 144 \u2502   \u2502 \
          \  results = self.llm_chain.apply(                                     \
          \               \u2502\n\u2502   145 \u2502   \u2502   \u2502   # FYI -\
          \ this is parallelized and so it is fast.                              \
          \  \u2502\n\u2502   146 \u2502   \u2502   \u2502   [{self.document_variable_name:\
          \ d.page_content, **kwargs} for d in docs],       \u2502\n\u2502   147 \u2502\
          \   \u2502   \u2502   callbacks=callbacks,                             \
          \                              \u2502\n\u2502                          \
          \                                                                      \
          \  \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/llm.py:157\
          \ in apply               \u2502\n\u2502                                \
          \                                                                  \u2502\
          \n\u2502   154 \u2502   \u2502   \u2502   response = self.generate(input_list,\
          \ run_manager=run_manager)                  \u2502\n\u2502   155 \u2502\
          \   \u2502   except (KeyboardInterrupt, Exception) as e:               \
          \                         \u2502\n\u2502   156 \u2502   \u2502   \u2502\
          \   run_manager.on_chain_error(e)                                      \
          \            \u2502\n\u2502 \u2771 157 \u2502   \u2502   \u2502   raise\
          \ e                                                                    \
          \    \u2502\n\u2502   158 \u2502   \u2502   outputs = self.create_outputs(response)\
          \                                            \u2502\n\u2502   159 \u2502\
          \   \u2502   run_manager.on_chain_end({\"outputs\": outputs})          \
          \                           \u2502\n\u2502   160 \u2502   \u2502   return\
          \ outputs                                                              \
          \       \u2502\n\u2502                                                 \
          \                                                 \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/llm.py:154\
          \ in apply               \u2502\n\u2502                                \
          \                                                                  \u2502\
          \n\u2502   151 \u2502   \u2502   \u2502   {\"input_list\": input_list},\
          \                                                    \u2502\n\u2502   152\
          \ \u2502   \u2502   )                                                  \
          \                                \u2502\n\u2502   153 \u2502   \u2502  \
          \ try:                                                                 \
          \              \u2502\n\u2502 \u2771 154 \u2502   \u2502   \u2502   response\
          \ = self.generate(input_list, run_manager=run_manager)                 \
          \ \u2502\n\u2502   155 \u2502   \u2502   except (KeyboardInterrupt, Exception)\
          \ as e:                                        \u2502\n\u2502   156 \u2502\
          \   \u2502   \u2502   run_manager.on_chain_error(e)                    \
          \                              \u2502\n\u2502   157 \u2502   \u2502   \u2502\
          \   raise e                                                            \
          \            \u2502\n\u2502                                            \
          \                                                      \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/llm.py:79\
          \ in generate             \u2502\n\u2502                               \
          \                                                                   \u2502\
          \n\u2502    76 \u2502   ) -&gt; LLMResult:                             \
          \                                           \u2502\n\u2502    77 \u2502\
          \   \u2502   \"\"\"Generate LLM result from inputs.\"\"\"              \
          \                               \u2502\n\u2502    78 \u2502   \u2502   prompts,\
          \ stop = self.prep_prompts(input_list, run_manager=run_manager)        \
          \     \u2502\n\u2502 \u2771  79 \u2502   \u2502   return self.llm.generate_prompt(\
          \                                                   \u2502\n\u2502    80\
          \ \u2502   \u2502   \u2502   prompts, stop, callbacks=run_manager.get_child()\
          \ if run_manager else None      \u2502\n\u2502    81 \u2502   \u2502   )\
          \                                                                      \
          \            \u2502\n\u2502    82                                      \
          \                                                      \u2502\n\u2502  \
          \                                                                      \
          \                          \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/base.py:134\
          \ in generate_prompt      \u2502\n\u2502                               \
          \                                                                   \u2502\
          \n\u2502   131 \u2502   \u2502   callbacks: Callbacks = None,          \
          \                                             \u2502\n\u2502   132 \u2502\
          \   ) -&gt; LLMResult:                                                 \
          \                       \u2502\n\u2502   133 \u2502   \u2502   prompt_strings\
          \ = [p.to_string() for p in prompts]                                  \u2502\
          \n\u2502 \u2771 134 \u2502   \u2502   return self.generate(prompt_strings,\
          \ stop=stop, callbacks=callbacks)               \u2502\n\u2502   135 \u2502\
          \                                                                      \
          \                    \u2502\n\u2502   136 \u2502   async def agenerate_prompt(\
          \                                                            \u2502\n\u2502\
          \   137 \u2502   \u2502   self,                                        \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/base.py:191\
          \ in generate             \u2502\n\u2502                               \
          \                                                                   \u2502\
          \n\u2502   188 \u2502   \u2502   \u2502   \u2502   )                   \
          \                                                       \u2502\n\u2502 \
          \  189 \u2502   \u2502   \u2502   except (KeyboardInterrupt, Exception)\
          \ as e:                                    \u2502\n\u2502   190 \u2502 \
          \  \u2502   \u2502   \u2502   run_manager.on_llm_error(e)              \
          \                                  \u2502\n\u2502 \u2771 191 \u2502   \u2502\
          \   \u2502   \u2502   raise e                                          \
          \                          \u2502\n\u2502   192 \u2502   \u2502   \u2502\
          \   run_manager.on_llm_end(output)                                     \
          \            \u2502\n\u2502   193 \u2502   \u2502   \u2502   return output\
          \                                                                  \u2502\
          \n\u2502   194 \u2502   \u2502   if len(missing_prompts) &gt; 0:       \
          \                                                \u2502\n\u2502        \
          \                                                                      \
          \                    \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/base.py:185\
          \ in generate             \u2502\n\u2502                               \
          \                                                                   \u2502\
          \n\u2502   182 \u2502   \u2502   \u2502   )                            \
          \                                                  \u2502\n\u2502   183\
          \ \u2502   \u2502   \u2502   try:                                      \
          \                                     \u2502\n\u2502   184 \u2502   \u2502\
          \   \u2502   \u2502   output = (                                       \
          \                          \u2502\n\u2502 \u2771 185 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   self._generate(prompts, stop=stop, run_manager=run_manager)\
          \            \u2502\n\u2502   186 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   if new_arg_supported                                               \
          \    \u2502\n\u2502   187 \u2502   \u2502   \u2502   \u2502   \u2502   else\
          \ self._generate(prompts, stop=stop)                                \u2502\
          \n\u2502   188 \u2502   \u2502   \u2502   \u2502   )                   \
          \                                                       \u2502\n\u2502 \
          \                                                                      \
          \                           \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/base.py:436\
          \ in _generate            \u2502\n\u2502                               \
          \                                                                   \u2502\
          \n\u2502   433 \u2502   \u2502   new_arg_supported = inspect.signature(self._call).parameters.get(\"\
          run_manager\")    \u2502\n\u2502   434 \u2502   \u2502   for prompt in prompts:\
          \                                                             \u2502\n\u2502\
          \   435 \u2502   \u2502   \u2502   text = (                            \
          \                                           \u2502\n\u2502 \u2771 436 \u2502\
          \   \u2502   \u2502   \u2502   self._call(prompt, stop=stop, run_manager=run_manager)\
          \                     \u2502\n\u2502   437 \u2502   \u2502   \u2502   \u2502\
          \   if new_arg_supported                                               \
          \        \u2502\n\u2502   438 \u2502   \u2502   \u2502   \u2502   else self._call(prompt,\
          \ stop=stop)                                         \u2502\n\u2502   439\
          \ \u2502   \u2502   \u2502   )                                         \
          \                                     \u2502\n\u2502                   \
          \                                                                      \
          \         \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/huggingface_pipeline.py:159\
          \ in      \u2502\n\u2502 _call                                         \
          \                                                   \u2502\n\u2502     \
          \                                                                      \
          \                       \u2502\n\u2502   156 \u2502   \u2502   stop: Optional[List[str]]\
          \ = None,                                                  \u2502\n\u2502\
          \   157 \u2502   \u2502   run_manager: Optional[CallbackManagerForLLMRun]\
          \ = None,                            \u2502\n\u2502   158 \u2502   ) -&gt;\
          \ str:                                                                 \
          \             \u2502\n\u2502 \u2771 159 \u2502   \u2502   response = self.pipeline(prompt)\
          \                                                   \u2502\n\u2502   160\
          \ \u2502   \u2502   if self.pipeline.task == \"text-generation\":      \
          \                                  \u2502\n\u2502   161 \u2502   \u2502\
          \   \u2502   # Text generation return includes the starter text.       \
          \                     \u2502\n\u2502   162 \u2502   \u2502   \u2502   text\
          \ = response[0][\"generated_text\"][len(prompt) :]                     \
          \       \u2502\n\u2502                                                 \
          \                                                 \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:201\
          \ in   \u2502\n\u2502 __call__                                         \
          \                                                \u2502\n\u2502        \
          \                                                                      \
          \                    \u2502\n\u2502   198 \u2502   \u2502   \u2502   - **generated_token_ids**\
          \ (`torch.Tensor` or `tf.Tensor`, present when `retu   \u2502\n\u2502  \
          \ 199 \u2502   \u2502   \u2502     ids of the generated text.          \
          \                                         \u2502\n\u2502   200 \u2502  \
          \ \u2502   \"\"\"                                                      \
          \                          \u2502\n\u2502 \u2771 201 \u2502   \u2502   return\
          \ super().__call__(text_inputs, **kwargs)                              \
          \       \u2502\n\u2502   202 \u2502                                    \
          \                                                      \u2502\n\u2502  \
          \ 203 \u2502   def preprocess(self, prompt_text, prefix=\"\", handle_long_generation=None,\
          \ **generate   \u2502\n\u2502   204 \u2502   \u2502   inputs = self.tokenizer(\
          \                                                           \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1118\
          \ in __call__    \u2502\n\u2502                                        \
          \                                                          \u2502\n\u2502\
          \   1115 \u2502   \u2502   \u2502   \u2502   )                         \
          \                                                \u2502\n\u2502   1116 \u2502\
          \   \u2502   \u2502   )                                                \
          \                             \u2502\n\u2502   1117 \u2502   \u2502   else:\
          \                                                                      \
          \       \u2502\n\u2502 \u2771 1118 \u2502   \u2502   \u2502   return self.run_single(inputs,\
          \ preprocess_params, forward_params, postproces  \u2502\n\u2502   1119 \u2502\
          \                                                                      \
          \                   \u2502\n\u2502   1120 \u2502   def run_multi(self, inputs,\
          \ preprocess_params, forward_params, postprocess_params):   \u2502\n\u2502\
          \   1121 \u2502   \u2502   return [self.run_single(item, preprocess_params,\
          \ forward_params, postprocess_par  \u2502\n\u2502                      \
          \                                                                      \
          \      \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1125\
          \ in run_single  \u2502\n\u2502                                        \
          \                                                          \u2502\n\u2502\
          \   1122 \u2502                                                        \
          \                                 \u2502\n\u2502   1123 \u2502   def run_single(self,\
          \ inputs, preprocess_params, forward_params, postprocess_params):  \u2502\
          \n\u2502   1124 \u2502   \u2502   model_inputs = self.preprocess(inputs,\
          \ **preprocess_params)                       \u2502\n\u2502 \u2771 1125\
          \ \u2502   \u2502   model_outputs = self.forward(model_inputs, **forward_params)\
          \                      \u2502\n\u2502   1126 \u2502   \u2502   outputs =\
          \ self.postprocess(model_outputs, **postprocess_params)                \
          \   \u2502\n\u2502   1127 \u2502   \u2502   return outputs             \
          \                                                       \u2502\n\u2502 \
          \  1128                                                                \
          \                           \u2502\n\u2502                             \
          \                                                                     \u2502\
          \n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1024\
          \ in forward     \u2502\n\u2502                                        \
          \                                                          \u2502\n\u2502\
          \   1021 \u2502   \u2502   \u2502   \u2502   inference_context = self.get_inference_context()\
          \                          \u2502\n\u2502   1022 \u2502   \u2502   \u2502\
          \   \u2502   with inference_context():                                 \
          \                \u2502\n\u2502   1023 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   model_inputs = self._ensure_tensor_on_device(model_inputs,\
          \ device=se  \u2502\n\u2502 \u2771 1024 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   model_outputs = self._forward(model_inputs, **forward_params)\
          \         \u2502\n\u2502   1025 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   model_outputs = self._ensure_tensor_on_device(model_outputs, device=\
          \  \u2502\n\u2502   1026 \u2502   \u2502   \u2502   else:              \
          \                                                           \u2502\n\u2502\
          \   1027 \u2502   \u2502   \u2502   \u2502   raise ValueError(f\"Framework\
          \ {self.framework} is not supported\")          \u2502\n\u2502         \
          \                                                                      \
          \                   \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:263\
          \ in   \u2502\n\u2502 _forward                                         \
          \                                                \u2502\n\u2502        \
          \                                                                      \
          \                    \u2502\n\u2502   260 \u2502   \u2502   \u2502   \u2502\
          \   generate_kwargs[\"min_length\"] += prefix_length                   \
          \          \u2502\n\u2502   261 \u2502   \u2502                        \
          \                                                              \u2502\n\u2502\
          \   262 \u2502   \u2502   # BS x SL                                    \
          \                                      \u2502\n\u2502 \u2771 263 \u2502\
          \   \u2502   generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=att   \u2502\n\u2502   264 \u2502   \u2502   out_b = generated_sequence.shape[0]\
          \                                                \u2502\n\u2502   265 \u2502\
          \   \u2502   if self.framework == \"pt\":                              \
          \                           \u2502\n\u2502   266 \u2502   \u2502   \u2502\
          \   generated_sequence = generated_sequence.reshape(in_b, out_b // in_b,\
          \ *genera   \u2502\n\u2502                                             \
          \                                                     \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\
          \ in decorate_context \u2502\n\u2502                                   \
          \                                                               \u2502\n\
          \u2502   112 \u2502   @functools.wraps(func)                           \
          \                                      \u2502\n\u2502   113 \u2502   def\
          \ decorate_context(*args, **kwargs):                                   \
          \              \u2502\n\u2502   114 \u2502   \u2502   with ctx_factory():\
          \                                                                \u2502\n\
          \u2502 \u2771 115 \u2502   \u2502   \u2502   return func(*args, **kwargs)\
          \                                                   \u2502\n\u2502   116\
          \ \u2502                                                               \
          \                           \u2502\n\u2502   117 \u2502   return decorate_context\
          \                                                                \u2502\n\
          \u2502   118                                                           \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1518\
          \ in generate  \u2502\n\u2502                                          \
          \                                                        \u2502\n\u2502\
          \   1515 \u2502   \u2502   \u2502   \u2502   )                         \
          \                                                \u2502\n\u2502   1516 \u2502\
          \   \u2502   \u2502                                                    \
          \                             \u2502\n\u2502   1517 \u2502   \u2502   \u2502\
          \   # 11. run greedy search                                            \
          \           \u2502\n\u2502 \u2771 1518 \u2502   \u2502   \u2502   return\
          \ self.greedy_search(                                                  \
          \  \u2502\n\u2502   1519 \u2502   \u2502   \u2502   \u2502   input_ids,\
          \                                                                \u2502\n\
          \u2502   1520 \u2502   \u2502   \u2502   \u2502   logits_processor=logits_processor,\
          \                                        \u2502\n\u2502   1521 \u2502  \
          \ \u2502   \u2502   \u2502   stopping_criteria=stopping_criteria,      \
          \                                \u2502\n\u2502                        \
          \                                                                      \
          \    \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/generation/utils.py:2335\
          \ in           \u2502\n\u2502 greedy_search                            \
          \                                                        \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502   2332 \u2502   \u2502   \u2502\
          \   model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\
          \  \u2502\n\u2502   2333 \u2502   \u2502   \u2502                      \
          \                                                           \u2502\n\u2502\
          \   2334 \u2502   \u2502   \u2502   # forward pass to get next token   \
          \                                           \u2502\n\u2502 \u2771 2335 \u2502\
          \   \u2502   \u2502   outputs = self(                                  \
          \                             \u2502\n\u2502   2336 \u2502   \u2502   \u2502\
          \   \u2502   **model_inputs,                                           \
          \                \u2502\n\u2502   2337 \u2502   \u2502   \u2502   \u2502\
          \   return_dict=True,                                                  \
          \       \u2502\n\u2502   2338 \u2502   \u2502   \u2502   \u2502   output_attentions=output_attentions,\
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
          \ in _call_impl      \u2502\n\u2502                                    \
          \                                                              \u2502\n\u2502\
          \   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks   \u2502\n\u2502   1499 \u2502   \u2502   \u2502\
          \   \u2502   or _global_backward_pre_hooks or _global_backward_hooks   \
          \                \u2502\n\u2502   1500 \u2502   \u2502   \u2502   \u2502\
          \   or _global_forward_hooks or _global_forward_pre_hooks):            \
          \       \u2502\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args,\
          \ **kwargs)                                          \u2502\n\u2502   1502\
          \ \u2502   \u2502   # Do not call functions when jit is used           \
          \                               \u2502\n\u2502   1503 \u2502   \u2502  \
          \ full_backward_hooks, non_full_backward_hooks = [], []                \
          \             \u2502\n\u2502   1504 \u2502   \u2502   backward_pre_hooks\
          \ = []                                                           \u2502\n\
          \u2502                                                                 \
          \                                 \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\
          \ in new_forward             \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502   162 \u2502   \u2502   \u2502   with torch.no_grad():        \
          \                                                  \u2502\n\u2502   163\
          \ \u2502   \u2502   \u2502   \u2502   output = old_forward(*args, **kwargs)\
          \                                      \u2502\n\u2502   164 \u2502   \u2502\
          \   else:                                                              \
          \                \u2502\n\u2502 \u2771 165 \u2502   \u2502   \u2502   output\
          \ = old_forward(*args, **kwargs)                                       \
          \   \u2502\n\u2502   166 \u2502   \u2502   return module._hf_hook.post_forward(module,\
          \ output)                                \u2502\n\u2502   167 \u2502   \
          \                                                                      \
          \                 \u2502\n\u2502   168 \u2502   module.forward = new_forward\
          \                                                           \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /home/talha/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py:753\
          \ in     \u2502\n\u2502 forward                                        \
          \                                                  \u2502\n\u2502      \
          \                                                                      \
          \                      \u2502\n\u2502    750 \u2502   \u2502           \
          \                                                                      \
          \    \u2502\n\u2502    751 \u2502   \u2502   return_dict = return_dict if\
          \ return_dict is not None else self.config.use_return  \u2502\n\u2502  \
          \  752 \u2502   \u2502                                                 \
          \                                    \u2502\n\u2502 \u2771  753 \u2502 \
          \  \u2502   transformer_outputs = self.transformer(                    \
          \                       \u2502\n\u2502    754 \u2502   \u2502   \u2502 \
          \  input_ids,                                                          \
          \          \u2502\n\u2502    755 \u2502   \u2502   \u2502   past_key_values=past_key_values,\
          \                                              \u2502\n\u2502    756 \u2502\
          \   \u2502   \u2502   attention_mask=attention_mask,                   \
          \                             \u2502\n\u2502                           \
          \                                                                      \
          \ \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
          \ in _call_impl      \u2502\n\u2502                                    \
          \                                                              \u2502\n\u2502\
          \   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks   \u2502\n\u2502   1499 \u2502   \u2502   \u2502\
          \   \u2502   or _global_backward_pre_hooks or _global_backward_hooks   \
          \                \u2502\n\u2502   1500 \u2502   \u2502   \u2502   \u2502\
          \   or _global_forward_hooks or _global_forward_pre_hooks):            \
          \       \u2502\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args,\
          \ **kwargs)                                          \u2502\n\u2502   1502\
          \ \u2502   \u2502   # Do not call functions when jit is used           \
          \                               \u2502\n\u2502   1503 \u2502   \u2502  \
          \ full_backward_hooks, non_full_backward_hooks = [], []                \
          \             \u2502\n\u2502   1504 \u2502   \u2502   backward_pre_hooks\
          \ = []                                                           \u2502\n\
          \u2502                                                                 \
          \                                 \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\
          \ in new_forward             \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502   162 \u2502   \u2502   \u2502   with torch.no_grad():        \
          \                                                  \u2502\n\u2502   163\
          \ \u2502   \u2502   \u2502   \u2502   output = old_forward(*args, **kwargs)\
          \                                      \u2502\n\u2502   164 \u2502   \u2502\
          \   else:                                                              \
          \                \u2502\n\u2502 \u2771 165 \u2502   \u2502   \u2502   output\
          \ = old_forward(*args, **kwargs)                                       \
          \   \u2502\n\u2502   166 \u2502   \u2502   return module._hf_hook.post_forward(module,\
          \ output)                                \u2502\n\u2502   167 \u2502   \
          \                                                                      \
          \                 \u2502\n\u2502   168 \u2502   module.forward = new_forward\
          \                                                           \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /home/talha/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py:648\
          \ in     \u2502\n\u2502 forward                                        \
          \                                                  \u2502\n\u2502      \
          \                                                                      \
          \                      \u2502\n\u2502    645 \u2502   \u2502   \u2502  \
          \ \u2502   \u2502   head_mask[i],                                      \
          \                   \u2502\n\u2502    646 \u2502   \u2502   \u2502   \u2502\
          \   )                                                                  \
          \       \u2502\n\u2502    647 \u2502   \u2502   \u2502   else:         \
          \                                                                \u2502\n\
          \u2502 \u2771  648 \u2502   \u2502   \u2502   \u2502   outputs = block(\
          \                                                          \u2502\n\u2502\
          \    649 \u2502   \u2502   \u2502   \u2502   \u2502   hidden_states,   \
          \                                                     \u2502\n\u2502   \
          \ 650 \u2502   \u2502   \u2502   \u2502   \u2502   layer_past=layer_past,\
          \                                                \u2502\n\u2502    651 \u2502\
          \   \u2502   \u2502   \u2502   \u2502   attention_mask=causal_mask,    \
          \                                       \u2502\n\u2502                 \
          \                                                                      \
          \           \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
          \ in _call_impl      \u2502\n\u2502                                    \
          \                                                              \u2502\n\u2502\
          \   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks   \u2502\n\u2502   1499 \u2502   \u2502   \u2502\
          \   \u2502   or _global_backward_pre_hooks or _global_backward_hooks   \
          \                \u2502\n\u2502   1500 \u2502   \u2502   \u2502   \u2502\
          \   or _global_forward_hooks or _global_forward_pre_hooks):            \
          \       \u2502\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args,\
          \ **kwargs)                                          \u2502\n\u2502   1502\
          \ \u2502   \u2502   # Do not call functions when jit is used           \
          \                               \u2502\n\u2502   1503 \u2502   \u2502  \
          \ full_backward_hooks, non_full_backward_hooks = [], []                \
          \             \u2502\n\u2502   1504 \u2502   \u2502   backward_pre_hooks\
          \ = []                                                           \u2502\n\
          \u2502                                                                 \
          \                                 \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\
          \ in new_forward             \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502   162 \u2502   \u2502   \u2502   with torch.no_grad():        \
          \                                                  \u2502\n\u2502   163\
          \ \u2502   \u2502   \u2502   \u2502   output = old_forward(*args, **kwargs)\
          \                                      \u2502\n\u2502   164 \u2502   \u2502\
          \   else:                                                              \
          \                \u2502\n\u2502 \u2771 165 \u2502   \u2502   \u2502   output\
          \ = old_forward(*args, **kwargs)                                       \
          \   \u2502\n\u2502   166 \u2502   \u2502   return module._hf_hook.post_forward(module,\
          \ output)                                \u2502\n\u2502   167 \u2502   \
          \                                                                      \
          \                 \u2502\n\u2502   168 \u2502   module.forward = new_forward\
          \                                                           \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /home/talha/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py:385\
          \ in     \u2502\n\u2502 forward                                        \
          \                                                  \u2502\n\u2502      \
          \                                                                      \
          \                      \u2502\n\u2502    382 \u2502   \u2502   residual\
          \ = hidden_states                                                      \
          \    \u2502\n\u2502    383 \u2502   \u2502                             \
          \                                                        \u2502\n\u2502\
          \    384 \u2502   \u2502   # Self attention.                           \
          \                                      \u2502\n\u2502 \u2771  385 \u2502\
          \   \u2502   attn_outputs = self.self_attention(                       \
          \                        \u2502\n\u2502    386 \u2502   \u2502   \u2502\
          \   layernorm_output,                                                  \
          \           \u2502\n\u2502    387 \u2502   \u2502   \u2502   layer_past=layer_past,\
          \                                                        \u2502\n\u2502\
          \    388 \u2502   \u2502   \u2502   attention_mask=attention_mask,     \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
          \ in _call_impl      \u2502\n\u2502                                    \
          \                                                              \u2502\n\u2502\
          \   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks   \u2502\n\u2502   1499 \u2502   \u2502   \u2502\
          \   \u2502   or _global_backward_pre_hooks or _global_backward_hooks   \
          \                \u2502\n\u2502   1500 \u2502   \u2502   \u2502   \u2502\
          \   or _global_forward_hooks or _global_forward_pre_hooks):            \
          \       \u2502\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args,\
          \ **kwargs)                                          \u2502\n\u2502   1502\
          \ \u2502   \u2502   # Do not call functions when jit is used           \
          \                               \u2502\n\u2502   1503 \u2502   \u2502  \
          \ full_backward_hooks, non_full_backward_hooks = [], []                \
          \             \u2502\n\u2502   1504 \u2502   \u2502   backward_pre_hooks\
          \ = []                                                           \u2502\n\
          \u2502                                                                 \
          \                                 \u2502\n\u2502 /home/talha/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\
          \ in new_forward             \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502   162 \u2502   \u2502   \u2502   with torch.no_grad():        \
          \                                                  \u2502\n\u2502   163\
          \ \u2502   \u2502   \u2502   \u2502   output = old_forward(*args, **kwargs)\
          \                                      \u2502\n\u2502   164 \u2502   \u2502\
          \   else:                                                              \
          \                \u2502\n\u2502 \u2771 165 \u2502   \u2502   \u2502   output\
          \ = old_forward(*args, **kwargs)                                       \
          \   \u2502\n\u2502   166 \u2502   \u2502   return module._hf_hook.post_forward(module,\
          \ output)                                \u2502\n\u2502   167 \u2502   \
          \                                                                      \
          \                 \u2502\n\u2502   168 \u2502   module.forward = new_forward\
          \                                                           \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /home/talha/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py:279\
          \ in     \u2502\n\u2502 forward                                        \
          \                                                  \u2502\n\u2502      \
          \                                                                      \
          \                      \u2502\n\u2502    276 \u2502   \u2502   \u2502  \
          \ key_layer_ = key_layer.reshape(batch_size, self.num_kv, -1, self.head_dim)\
          \    \u2502\n\u2502    277 \u2502   \u2502   \u2502   value_layer_ = value_layer.reshape(batch_size,\
          \ self.num_kv, -1, self.head_di  \u2502\n\u2502    278 \u2502   \u2502 \
          \  \u2502                                                              \
          \                   \u2502\n\u2502 \u2771  279 \u2502   \u2502   \u2502\
          \   attn_output = F.scaled_dot_product_attention(                      \
          \           \u2502\n\u2502    280 \u2502   \u2502   \u2502   \u2502   query_layer_,\
          \ key_layer_, value_layer_, None, 0.0, is_causal=True         \u2502\n\u2502\
          \    281 \u2502   \u2502   \u2502   )                                  \
          \                                           \u2502\n\u2502    282      \
          \                                                                      \
          \               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\n</code></pre>\n"
        raw: "I am trying to use falcon with langhcain\r\nthis is what I am doing\r\
          \n**model**\r\n```\r\nmodel_id='falcon-7b'\r\ntokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True\
          \ )\r\nmodel = AutoModelForCausalLM.from_pretrained(model_id,trust_remote_code=True,\r\
          \n\r\n                                             quantization_config =\
          \ BitsAndBytesConfig(load_in_4bit=True, \r\n                           \
          \                                             bnb_4bit_compute_dtype=torch.bfloat16),\r\
          \n                                             device_map='auto')\r\npipe\
          \ = pipeline(\r\n        \"text-generation\",\r\n        model=model, \r\
          \n        tokenizer=tokenizer, \r\n        max_new_tokens=256\r\n    )\r\
          \n\r\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\r\n\r\n```\r\n**embeddings**\r\
          \n```\r\nhfemb = HuggingFaceEmbeddings()\r\nretriever = vector_db.as_retriever()\r\
          \n```\r\n**memory**\r\n```\r\nmemory = ConversationBufferMemory(memory_key=\"\
          chat_history\", return_messages=True,output_key='answer')\r\nchain = ConversationalRetrievalChain.from_llm(local_llm,\r\
          \n                                           retriever=retriever, memory=memory,\r\
          \n                                          chain_type=\"map_reduce\", #\r\
          \n                                          return_source_documents=True)\r\
          \n```\r\n\r\nThe code worked with flan-t5 but give error with falcon,\r\n\
          \r\nHere is full error\r\n```\r\n\u2502 [Errno 2] No such file or directory:\
          \ '/tmp/ipykernel_67349/1702757523.py'                        \u2502\r\n\
          \u2502                                                                 \
          \                                 \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:140\
          \ in __call__           \u2502\r\n\u2502                               \
          \                                                                   \u2502\
          \r\n\u2502   137 \u2502   \u2502   \u2502   )                          \
          \                                                    \u2502\r\n\u2502  \
          \ 138 \u2502   \u2502   except (KeyboardInterrupt, Exception) as e:    \
          \                                    \u2502\r\n\u2502   139 \u2502   \u2502\
          \   \u2502   run_manager.on_chain_error(e)                             \
          \                     \u2502\r\n\u2502 \u2771 140 \u2502   \u2502   \u2502\
          \   raise e                                                            \
          \            \u2502\r\n\u2502   141 \u2502   \u2502   run_manager.on_chain_end(outputs)\
          \                                                  \u2502\r\n\u2502   142\
          \ \u2502   \u2502   return self.prep_outputs(inputs, outputs, return_only_outputs)\
          \                     \u2502\r\n\u2502   143                           \
          \                                                                 \u2502\
          \r\n\u2502                                                             \
          \                                     \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:134\
          \ in __call__           \u2502\r\n\u2502                               \
          \                                                                   \u2502\
          \r\n\u2502   131 \u2502   \u2502   )                                   \
          \                                               \u2502\r\n\u2502   132 \u2502\
          \   \u2502   try:                                                      \
          \                         \u2502\r\n\u2502   133 \u2502   \u2502   \u2502\
          \   outputs = (                                                        \
          \            \u2502\r\n\u2502 \u2771 134 \u2502   \u2502   \u2502   \u2502\
          \   self._call(inputs, run_manager=run_manager)                        \
          \        \u2502\r\n\u2502   135 \u2502   \u2502   \u2502   \u2502   if new_arg_supported\
          \                                                       \u2502\r\n\u2502\
          \   136 \u2502   \u2502   \u2502   \u2502   else self._call(inputs)    \
          \                                                \u2502\r\n\u2502   137\
          \ \u2502   \u2502   \u2502   )                                         \
          \                                     \u2502\r\n\u2502                 \
          \                                                                      \
          \           \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:\
          \ \u2502\r\n\u2502 110 in _call                                        \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   107 \u2502   \u2502   new_inputs\
          \ = inputs.copy()                                                      \
          \   \u2502\r\n\u2502   108 \u2502   \u2502   new_inputs[\"question\"] =\
          \ new_question                                              \u2502\r\n\u2502\
          \   109 \u2502   \u2502   new_inputs[\"chat_history\"] = chat_history_str\
          \                                      \u2502\r\n\u2502 \u2771 110 \u2502\
          \   \u2502   answer = self.combine_docs_chain.run(                     \
          \                         \u2502\r\n\u2502   111 \u2502   \u2502   \u2502\
          \   input_documents=docs, callbacks=_run_manager.get_child(), **new_inputs\
          \         \u2502\r\n\u2502   112 \u2502   \u2502   )                   \
          \                                                               \u2502\r\
          \n\u2502   113 \u2502   \u2502   if self.return_source_documents:      \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:239\
          \ in run                \u2502\r\n\u2502                               \
          \                                                                   \u2502\
          \r\n\u2502   236 \u2502   \u2502   \u2502   return self(args[0], callbacks=callbacks)[self.output_keys[0]]\
          \                 \u2502\r\n\u2502   237 \u2502   \u2502               \
          \                                                                      \
          \ \u2502\r\n\u2502   238 \u2502   \u2502   if kwargs and not args:     \
          \                                                       \u2502\r\n\u2502\
          \ \u2771 239 \u2502   \u2502   \u2502   return self(kwargs, callbacks=callbacks)[self.output_keys[0]]\
          \                  \u2502\r\n\u2502   240 \u2502   \u2502              \
          \                                                                      \
          \  \u2502\r\n\u2502   241 \u2502   \u2502   if not kwargs and not args:\
          \                                                        \u2502\r\n\u2502\
          \   242 \u2502   \u2502   \u2502   raise ValueError(                   \
          \                                           \u2502\r\n\u2502           \
          \                                                                      \
          \                 \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:140\
          \ in __call__           \u2502\r\n\u2502                               \
          \                                                                   \u2502\
          \r\n\u2502   137 \u2502   \u2502   \u2502   )                          \
          \                                                    \u2502\r\n\u2502  \
          \ 138 \u2502   \u2502   except (KeyboardInterrupt, Exception) as e:    \
          \                                    \u2502\r\n\u2502   139 \u2502   \u2502\
          \   \u2502   run_manager.on_chain_error(e)                             \
          \                     \u2502\r\n\u2502 \u2771 140 \u2502   \u2502   \u2502\
          \   raise e                                                            \
          \            \u2502\r\n\u2502   141 \u2502   \u2502   run_manager.on_chain_end(outputs)\
          \                                                  \u2502\r\n\u2502   142\
          \ \u2502   \u2502   return self.prep_outputs(inputs, outputs, return_only_outputs)\
          \                     \u2502\r\n\u2502   143                           \
          \                                                                 \u2502\
          \r\n\u2502                                                             \
          \                                     \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:134\
          \ in __call__           \u2502\r\n\u2502                               \
          \                                                                   \u2502\
          \r\n\u2502   131 \u2502   \u2502   )                                   \
          \                                               \u2502\r\n\u2502   132 \u2502\
          \   \u2502   try:                                                      \
          \                         \u2502\r\n\u2502   133 \u2502   \u2502   \u2502\
          \   outputs = (                                                        \
          \            \u2502\r\n\u2502 \u2771 134 \u2502   \u2502   \u2502   \u2502\
          \   self._call(inputs, run_manager=run_manager)                        \
          \        \u2502\r\n\u2502   135 \u2502   \u2502   \u2502   \u2502   if new_arg_supported\
          \                                                       \u2502\r\n\u2502\
          \   136 \u2502   \u2502   \u2502   \u2502   else self._call(inputs)    \
          \                                                \u2502\r\n\u2502   137\
          \ \u2502   \u2502   \u2502   )                                         \
          \                                     \u2502\r\n\u2502                 \
          \                                                                      \
          \           \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\
          \ in   \u2502\r\n\u2502 _call                                          \
          \                                                  \u2502\r\n\u2502    \
          \                                                                      \
          \                        \u2502\r\n\u2502    81 \u2502   \u2502   docs =\
          \ inputs[self.input_key]                                               \
          \       \u2502\r\n\u2502    82 \u2502   \u2502   # Other keys are assumed\
          \ to be needed for LLM prediction                           \u2502\r\n\u2502\
          \    83 \u2502   \u2502   other_keys = {k: v for k, v in inputs.items()\
          \ if k != self.input_key}              \u2502\r\n\u2502 \u2771  84 \u2502\
          \   \u2502   output, extra_return_dict = self.combine_docs(            \
          \                         \u2502\r\n\u2502    85 \u2502   \u2502   \u2502\
          \   docs, callbacks=_run_manager.get_child(), **other_keys             \
          \            \u2502\r\n\u2502    86 \u2502   \u2502   )                \
          \                                                                  \u2502\
          \r\n\u2502    87 \u2502   \u2502   extra_return_dict[self.output_key] =\
          \ output                                        \u2502\r\n\u2502       \
          \                                                                      \
          \                     \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:1\
          \ \u2502\r\n\u2502 44 in combine_docs                                  \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   141 \u2502   \u2502   Combine by\
          \ mapping first chain over all documents, then reducing the results.   \
          \   \u2502\r\n\u2502   142 \u2502   \u2502   This reducing can be done recursively\
          \ if needed (if there are many documents).     \u2502\r\n\u2502   143 \u2502\
          \   \u2502   \"\"\"                                                    \
          \                            \u2502\r\n\u2502 \u2771 144 \u2502   \u2502\
          \   results = self.llm_chain.apply(                                    \
          \                \u2502\r\n\u2502   145 \u2502   \u2502   \u2502   # FYI\
          \ - this is parallelized and so it is fast.                            \
          \    \u2502\r\n\u2502   146 \u2502   \u2502   \u2502   [{self.document_variable_name:\
          \ d.page_content, **kwargs} for d in docs],       \u2502\r\n\u2502   147\
          \ \u2502   \u2502   \u2502   callbacks=callbacks,                      \
          \                                     \u2502\r\n\u2502                 \
          \                                                                      \
          \           \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/llm.py:157\
          \ in apply               \u2502\r\n\u2502                              \
          \                                                                    \u2502\
          \r\n\u2502   154 \u2502   \u2502   \u2502   response = self.generate(input_list,\
          \ run_manager=run_manager)                  \u2502\r\n\u2502   155 \u2502\
          \   \u2502   except (KeyboardInterrupt, Exception) as e:               \
          \                         \u2502\r\n\u2502   156 \u2502   \u2502   \u2502\
          \   run_manager.on_chain_error(e)                                      \
          \            \u2502\r\n\u2502 \u2771 157 \u2502   \u2502   \u2502   raise\
          \ e                                                                    \
          \    \u2502\r\n\u2502   158 \u2502   \u2502   outputs = self.create_outputs(response)\
          \                                            \u2502\r\n\u2502   159 \u2502\
          \   \u2502   run_manager.on_chain_end({\"outputs\": outputs})          \
          \                           \u2502\r\n\u2502   160 \u2502   \u2502   return\
          \ outputs                                                              \
          \       \u2502\r\n\u2502                                               \
          \                                                   \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/llm.py:154\
          \ in apply               \u2502\r\n\u2502                              \
          \                                                                    \u2502\
          \r\n\u2502   151 \u2502   \u2502   \u2502   {\"input_list\": input_list},\
          \                                                    \u2502\r\n\u2502  \
          \ 152 \u2502   \u2502   )                                              \
          \                                    \u2502\r\n\u2502   153 \u2502   \u2502\
          \   try:                                                               \
          \                \u2502\r\n\u2502 \u2771 154 \u2502   \u2502   \u2502  \
          \ response = self.generate(input_list, run_manager=run_manager)        \
          \          \u2502\r\n\u2502   155 \u2502   \u2502   except (KeyboardInterrupt,\
          \ Exception) as e:                                        \u2502\r\n\u2502\
          \   156 \u2502   \u2502   \u2502   run_manager.on_chain_error(e)       \
          \                                           \u2502\r\n\u2502   157 \u2502\
          \   \u2502   \u2502   raise e                                          \
          \                              \u2502\r\n\u2502                        \
          \                                                                      \
          \    \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/llm.py:79\
          \ in generate             \u2502\r\n\u2502                             \
          \                                                                     \u2502\
          \r\n\u2502    76 \u2502   ) -> LLMResult:                              \
          \                                          \u2502\r\n\u2502    77 \u2502\
          \   \u2502   \"\"\"Generate LLM result from inputs.\"\"\"              \
          \                               \u2502\r\n\u2502    78 \u2502   \u2502 \
          \  prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)\
          \             \u2502\r\n\u2502 \u2771  79 \u2502   \u2502   return self.llm.generate_prompt(\
          \                                                   \u2502\r\n\u2502   \
          \ 80 \u2502   \u2502   \u2502   prompts, stop, callbacks=run_manager.get_child()\
          \ if run_manager else None      \u2502\r\n\u2502    81 \u2502   \u2502 \
          \  )                                                                   \
          \               \u2502\r\n\u2502    82                                 \
          \                                                           \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/base.py:134\
          \ in generate_prompt      \u2502\r\n\u2502                             \
          \                                                                     \u2502\
          \r\n\u2502   131 \u2502   \u2502   callbacks: Callbacks = None,        \
          \                                               \u2502\r\n\u2502   132 \u2502\
          \   ) -> LLMResult:                                                    \
          \                    \u2502\r\n\u2502   133 \u2502   \u2502   prompt_strings\
          \ = [p.to_string() for p in prompts]                                  \u2502\
          \r\n\u2502 \u2771 134 \u2502   \u2502   return self.generate(prompt_strings,\
          \ stop=stop, callbacks=callbacks)               \u2502\r\n\u2502   135 \u2502\
          \                                                                      \
          \                    \u2502\r\n\u2502   136 \u2502   async def agenerate_prompt(\
          \                                                            \u2502\r\n\u2502\
          \   137 \u2502   \u2502   self,                                        \
          \                                      \u2502\r\n\u2502                \
          \                                                                      \
          \            \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/base.py:191\
          \ in generate             \u2502\r\n\u2502                             \
          \                                                                     \u2502\
          \r\n\u2502   188 \u2502   \u2502   \u2502   \u2502   )                 \
          \                                                         \u2502\r\n\u2502\
          \   189 \u2502   \u2502   \u2502   except (KeyboardInterrupt, Exception)\
          \ as e:                                    \u2502\r\n\u2502   190 \u2502\
          \   \u2502   \u2502   \u2502   run_manager.on_llm_error(e)             \
          \                                   \u2502\r\n\u2502 \u2771 191 \u2502 \
          \  \u2502   \u2502   \u2502   raise e                                  \
          \                                  \u2502\r\n\u2502   192 \u2502   \u2502\
          \   \u2502   run_manager.on_llm_end(output)                            \
          \                     \u2502\r\n\u2502   193 \u2502   \u2502   \u2502  \
          \ return output                                                        \
          \          \u2502\r\n\u2502   194 \u2502   \u2502   if len(missing_prompts)\
          \ > 0:                                                       \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/base.py:185\
          \ in generate             \u2502\r\n\u2502                             \
          \                                                                     \u2502\
          \r\n\u2502   182 \u2502   \u2502   \u2502   )                          \
          \                                                    \u2502\r\n\u2502  \
          \ 183 \u2502   \u2502   \u2502   try:                                  \
          \                                         \u2502\r\n\u2502   184 \u2502\
          \   \u2502   \u2502   \u2502   output = (                              \
          \                                   \u2502\r\n\u2502 \u2771 185 \u2502 \
          \  \u2502   \u2502   \u2502   \u2502   self._generate(prompts, stop=stop,\
          \ run_manager=run_manager)            \u2502\r\n\u2502   186 \u2502   \u2502\
          \   \u2502   \u2502   \u2502   if new_arg_supported                    \
          \                               \u2502\r\n\u2502   187 \u2502   \u2502 \
          \  \u2502   \u2502   \u2502   else self._generate(prompts, stop=stop)  \
          \                              \u2502\r\n\u2502   188 \u2502   \u2502  \
          \ \u2502   \u2502   )                                                  \
          \                        \u2502\r\n\u2502                              \
          \                                                                    \u2502\
          \r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/base.py:436\
          \ in _generate            \u2502\r\n\u2502                             \
          \                                                                     \u2502\
          \r\n\u2502   433 \u2502   \u2502   new_arg_supported = inspect.signature(self._call).parameters.get(\"\
          run_manager\")    \u2502\r\n\u2502   434 \u2502   \u2502   for prompt in\
          \ prompts:                                                             \u2502\
          \r\n\u2502   435 \u2502   \u2502   \u2502   text = (                   \
          \                                                    \u2502\r\n\u2502 \u2771\
          \ 436 \u2502   \u2502   \u2502   \u2502   self._call(prompt, stop=stop,\
          \ run_manager=run_manager)                     \u2502\r\n\u2502   437 \u2502\
          \   \u2502   \u2502   \u2502   if new_arg_supported                    \
          \                                   \u2502\r\n\u2502   438 \u2502   \u2502\
          \   \u2502   \u2502   else self._call(prompt, stop=stop)               \
          \                          \u2502\r\n\u2502   439 \u2502   \u2502   \u2502\
          \   )                                                                  \
          \            \u2502\r\n\u2502                                          \
          \                                                        \u2502\r\n\u2502\
          \ /home/talha/venv/lib/python3.10/site-packages/langchain/llms/huggingface_pipeline.py:159\
          \ in      \u2502\r\n\u2502 _call                                       \
          \                                                     \u2502\r\n\u2502 \
          \                                                                      \
          \                           \u2502\r\n\u2502   156 \u2502   \u2502   stop:\
          \ Optional[List[str]] = None,                                          \
          \        \u2502\r\n\u2502   157 \u2502   \u2502   run_manager: Optional[CallbackManagerForLLMRun]\
          \ = None,                            \u2502\r\n\u2502   158 \u2502   ) ->\
          \ str:                                                                 \
          \             \u2502\r\n\u2502 \u2771 159 \u2502   \u2502   response = self.pipeline(prompt)\
          \                                                   \u2502\r\n\u2502   160\
          \ \u2502   \u2502   if self.pipeline.task == \"text-generation\":      \
          \                                  \u2502\r\n\u2502   161 \u2502   \u2502\
          \   \u2502   # Text generation return includes the starter text.       \
          \                     \u2502\r\n\u2502   162 \u2502   \u2502   \u2502  \
          \ text = response[0][\"generated_text\"][len(prompt) :]                \
          \            \u2502\r\n\u2502                                          \
          \                                                        \u2502\r\n\u2502\
          \ /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:201\
          \ in   \u2502\r\n\u2502 __call__                                       \
          \                                                  \u2502\r\n\u2502    \
          \                                                                      \
          \                        \u2502\r\n\u2502   198 \u2502   \u2502   \u2502\
          \   - **generated_token_ids** (`torch.Tensor` or `tf.Tensor`, present when\
          \ `retu   \u2502\r\n\u2502   199 \u2502   \u2502   \u2502     ids of the\
          \ generated text.                                                   \u2502\
          \r\n\u2502   200 \u2502   \u2502   \"\"\"                              \
          \                                                  \u2502\r\n\u2502 \u2771\
          \ 201 \u2502   \u2502   return super().__call__(text_inputs, **kwargs) \
          \                                    \u2502\r\n\u2502   202 \u2502     \
          \                                                                      \
          \               \u2502\r\n\u2502   203 \u2502   def preprocess(self, prompt_text,\
          \ prefix=\"\", handle_long_generation=None, **generate   \u2502\r\n\u2502\
          \   204 \u2502   \u2502   inputs = self.tokenizer(                     \
          \                                      \u2502\r\n\u2502                \
          \                                                                      \
          \            \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1118\
          \ in __call__    \u2502\r\n\u2502                                      \
          \                                                            \u2502\r\n\u2502\
          \   1115 \u2502   \u2502   \u2502   \u2502   )                         \
          \                                                \u2502\r\n\u2502   1116\
          \ \u2502   \u2502   \u2502   )                                         \
          \                                    \u2502\r\n\u2502   1117 \u2502   \u2502\
          \   else:                                                              \
          \               \u2502\r\n\u2502 \u2771 1118 \u2502   \u2502   \u2502  \
          \ return self.run_single(inputs, preprocess_params, forward_params, postproces\
          \  \u2502\r\n\u2502   1119 \u2502                                      \
          \                                                   \u2502\r\n\u2502   1120\
          \ \u2502   def run_multi(self, inputs, preprocess_params, forward_params,\
          \ postprocess_params):   \u2502\r\n\u2502   1121 \u2502   \u2502   return\
          \ [self.run_single(item, preprocess_params, forward_params, postprocess_par\
          \  \u2502\r\n\u2502                                                    \
          \                                              \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1125\
          \ in run_single  \u2502\r\n\u2502                                      \
          \                                                            \u2502\r\n\u2502\
          \   1122 \u2502                                                        \
          \                                 \u2502\r\n\u2502   1123 \u2502   def run_single(self,\
          \ inputs, preprocess_params, forward_params, postprocess_params):  \u2502\
          \r\n\u2502   1124 \u2502   \u2502   model_inputs = self.preprocess(inputs,\
          \ **preprocess_params)                       \u2502\r\n\u2502 \u2771 1125\
          \ \u2502   \u2502   model_outputs = self.forward(model_inputs, **forward_params)\
          \                      \u2502\r\n\u2502   1126 \u2502   \u2502   outputs\
          \ = self.postprocess(model_outputs, **postprocess_params)              \
          \     \u2502\r\n\u2502   1127 \u2502   \u2502   return outputs         \
          \                                                           \u2502\r\n\u2502\
          \   1128                                                               \
          \                            \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1024\
          \ in forward     \u2502\r\n\u2502                                      \
          \                                                            \u2502\r\n\u2502\
          \   1021 \u2502   \u2502   \u2502   \u2502   inference_context = self.get_inference_context()\
          \                          \u2502\r\n\u2502   1022 \u2502   \u2502   \u2502\
          \   \u2502   with inference_context():                                 \
          \                \u2502\r\n\u2502   1023 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   model_inputs = self._ensure_tensor_on_device(model_inputs,\
          \ device=se  \u2502\r\n\u2502 \u2771 1024 \u2502   \u2502   \u2502   \u2502\
          \   \u2502   model_outputs = self._forward(model_inputs, **forward_params)\
          \         \u2502\r\n\u2502   1025 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   model_outputs = self._ensure_tensor_on_device(model_outputs, device=\
          \  \u2502\r\n\u2502   1026 \u2502   \u2502   \u2502   else:            \
          \                                                             \u2502\r\n\
          \u2502   1027 \u2502   \u2502   \u2502   \u2502   raise ValueError(f\"Framework\
          \ {self.framework} is not supported\")          \u2502\r\n\u2502       \
          \                                                                      \
          \                     \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:263\
          \ in   \u2502\r\n\u2502 _forward                                       \
          \                                                  \u2502\r\n\u2502    \
          \                                                                      \
          \                        \u2502\r\n\u2502   260 \u2502   \u2502   \u2502\
          \   \u2502   generate_kwargs[\"min_length\"] += prefix_length          \
          \                   \u2502\r\n\u2502   261 \u2502   \u2502             \
          \                                                                      \
          \   \u2502\r\n\u2502   262 \u2502   \u2502   # BS x SL                 \
          \                                                         \u2502\r\n\u2502\
          \ \u2771 263 \u2502   \u2502   generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=att   \u2502\r\n\u2502   264 \u2502   \u2502   out_b =\
          \ generated_sequence.shape[0]                                          \
          \      \u2502\r\n\u2502   265 \u2502   \u2502   if self.framework == \"\
          pt\":                                                         \u2502\r\n\
          \u2502   266 \u2502   \u2502   \u2502   generated_sequence = generated_sequence.reshape(in_b,\
          \ out_b // in_b, *genera   \u2502\r\n\u2502                            \
          \                                                                      \u2502\
          \r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\
          \ in decorate_context \u2502\r\n\u2502                                 \
          \                                                                 \u2502\
          \r\n\u2502   112 \u2502   @functools.wraps(func)                       \
          \                                          \u2502\r\n\u2502   113 \u2502\
          \   def decorate_context(*args, **kwargs):                             \
          \                    \u2502\r\n\u2502   114 \u2502   \u2502   with ctx_factory():\
          \                                                                \u2502\r\
          \n\u2502 \u2771 115 \u2502   \u2502   \u2502   return func(*args, **kwargs)\
          \                                                   \u2502\r\n\u2502   116\
          \ \u2502                                                               \
          \                           \u2502\r\n\u2502   117 \u2502   return decorate_context\
          \                                                                \u2502\r\
          \n\u2502   118                                                         \
          \                                   \u2502\r\n\u2502                   \
          \                                                                      \
          \         \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1518\
          \ in generate  \u2502\r\n\u2502                                        \
          \                                                          \u2502\r\n\u2502\
          \   1515 \u2502   \u2502   \u2502   \u2502   )                         \
          \                                                \u2502\r\n\u2502   1516\
          \ \u2502   \u2502   \u2502                                             \
          \                                    \u2502\r\n\u2502   1517 \u2502   \u2502\
          \   \u2502   # 11. run greedy search                                   \
          \                    \u2502\r\n\u2502 \u2771 1518 \u2502   \u2502   \u2502\
          \   return self.greedy_search(                                         \
          \           \u2502\r\n\u2502   1519 \u2502   \u2502   \u2502   \u2502  \
          \ input_ids,                                                           \
          \     \u2502\r\n\u2502   1520 \u2502   \u2502   \u2502   \u2502   logits_processor=logits_processor,\
          \                                        \u2502\r\n\u2502   1521 \u2502\
          \   \u2502   \u2502   \u2502   stopping_criteria=stopping_criteria,    \
          \                                  \u2502\r\n\u2502                    \
          \                                                                      \
          \        \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/generation/utils.py:2335\
          \ in           \u2502\r\n\u2502 greedy_search                          \
          \                                                          \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502   2332 \u2502   \u2502   \u2502\
          \   model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\
          \  \u2502\r\n\u2502   2333 \u2502   \u2502   \u2502                    \
          \                                                             \u2502\r\n\
          \u2502   2334 \u2502   \u2502   \u2502   # forward pass to get next token\
          \                                              \u2502\r\n\u2502 \u2771 2335\
          \ \u2502   \u2502   \u2502   outputs = self(                           \
          \                                    \u2502\r\n\u2502   2336 \u2502   \u2502\
          \   \u2502   \u2502   **model_inputs,                                  \
          \                         \u2502\r\n\u2502   2337 \u2502   \u2502   \u2502\
          \   \u2502   return_dict=True,                                         \
          \                \u2502\r\n\u2502   2338 \u2502   \u2502   \u2502   \u2502\
          \   output_attentions=output_attentions,                               \
          \       \u2502\r\n\u2502                                               \
          \                                                   \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
          \ in _call_impl      \u2502\r\n\u2502                                  \
          \                                                                \u2502\r\
          \n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502\
          \   \u2502   or _global_backward_pre_hooks or _global_backward_hooks   \
          \                \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502\
          \   or _global_forward_hooks or _global_forward_pre_hooks):            \
          \       \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args,\
          \ **kwargs)                                          \u2502\r\n\u2502  \
          \ 1502 \u2502   \u2502   # Do not call functions when jit is used      \
          \                                    \u2502\r\n\u2502   1503 \u2502   \u2502\
          \   full_backward_hooks, non_full_backward_hooks = [], []              \
          \               \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks\
          \ = []                                                           \u2502\r\
          \n\u2502                                                               \
          \                                   \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\
          \ in new_forward             \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502   162 \u2502   \u2502   \u2502   with torch.no_grad():\
          \                                                          \u2502\r\n\u2502\
          \   163 \u2502   \u2502   \u2502   \u2502   output = old_forward(*args,\
          \ **kwargs)                                      \u2502\r\n\u2502   164\
          \ \u2502   \u2502   else:                                              \
          \                                \u2502\r\n\u2502 \u2771 165 \u2502   \u2502\
          \   \u2502   output = old_forward(*args, **kwargs)                     \
          \                     \u2502\r\n\u2502   166 \u2502   \u2502   return module._hf_hook.post_forward(module,\
          \ output)                                \u2502\r\n\u2502   167 \u2502 \
          \                                                                      \
          \                   \u2502\r\n\u2502   168 \u2502   module.forward = new_forward\
          \                                                           \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /home/talha/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py:753\
          \ in     \u2502\r\n\u2502 forward                                      \
          \                                                    \u2502\r\n\u2502  \
          \                                                                      \
          \                          \u2502\r\n\u2502    750 \u2502   \u2502     \
          \                                                                      \
          \          \u2502\r\n\u2502    751 \u2502   \u2502   return_dict = return_dict\
          \ if return_dict is not None else self.config.use_return  \u2502\r\n\u2502\
          \    752 \u2502   \u2502                                               \
          \                                      \u2502\r\n\u2502 \u2771  753 \u2502\
          \   \u2502   transformer_outputs = self.transformer(                   \
          \                        \u2502\r\n\u2502    754 \u2502   \u2502   \u2502\
          \   input_ids,                                                         \
          \           \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   past_key_values=past_key_values,\
          \                                              \u2502\r\n\u2502    756 \u2502\
          \   \u2502   \u2502   attention_mask=attention_mask,                   \
          \                             \u2502\r\n\u2502                         \
          \                                                                      \
          \   \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
          \ in _call_impl      \u2502\r\n\u2502                                  \
          \                                                                \u2502\r\
          \n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502\
          \   \u2502   or _global_backward_pre_hooks or _global_backward_hooks   \
          \                \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502\
          \   or _global_forward_hooks or _global_forward_pre_hooks):            \
          \       \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args,\
          \ **kwargs)                                          \u2502\r\n\u2502  \
          \ 1502 \u2502   \u2502   # Do not call functions when jit is used      \
          \                                    \u2502\r\n\u2502   1503 \u2502   \u2502\
          \   full_backward_hooks, non_full_backward_hooks = [], []              \
          \               \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks\
          \ = []                                                           \u2502\r\
          \n\u2502                                                               \
          \                                   \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\
          \ in new_forward             \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502   162 \u2502   \u2502   \u2502   with torch.no_grad():\
          \                                                          \u2502\r\n\u2502\
          \   163 \u2502   \u2502   \u2502   \u2502   output = old_forward(*args,\
          \ **kwargs)                                      \u2502\r\n\u2502   164\
          \ \u2502   \u2502   else:                                              \
          \                                \u2502\r\n\u2502 \u2771 165 \u2502   \u2502\
          \   \u2502   output = old_forward(*args, **kwargs)                     \
          \                     \u2502\r\n\u2502   166 \u2502   \u2502   return module._hf_hook.post_forward(module,\
          \ output)                                \u2502\r\n\u2502   167 \u2502 \
          \                                                                      \
          \                   \u2502\r\n\u2502   168 \u2502   module.forward = new_forward\
          \                                                           \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /home/talha/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py:648\
          \ in     \u2502\r\n\u2502 forward                                      \
          \                                                    \u2502\r\n\u2502  \
          \                                                                      \
          \                          \u2502\r\n\u2502    645 \u2502   \u2502   \u2502\
          \   \u2502   \u2502   head_mask[i],                                    \
          \                     \u2502\r\n\u2502    646 \u2502   \u2502   \u2502 \
          \  \u2502   )                                                          \
          \               \u2502\r\n\u2502    647 \u2502   \u2502   \u2502   else:\
          \                                                                      \
          \   \u2502\r\n\u2502 \u2771  648 \u2502   \u2502   \u2502   \u2502   outputs\
          \ = block(                                                          \u2502\
          \r\n\u2502    649 \u2502   \u2502   \u2502   \u2502   \u2502   hidden_states,\
          \                                                        \u2502\r\n\u2502\
          \    650 \u2502   \u2502   \u2502   \u2502   \u2502   layer_past=layer_past,\
          \                                                \u2502\r\n\u2502    651\
          \ \u2502   \u2502   \u2502   \u2502   \u2502   attention_mask=causal_mask,\
          \                                           \u2502\r\n\u2502           \
          \                                                                      \
          \                 \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
          \ in _call_impl      \u2502\r\n\u2502                                  \
          \                                                                \u2502\r\
          \n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502\
          \   \u2502   or _global_backward_pre_hooks or _global_backward_hooks   \
          \                \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502\
          \   or _global_forward_hooks or _global_forward_pre_hooks):            \
          \       \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args,\
          \ **kwargs)                                          \u2502\r\n\u2502  \
          \ 1502 \u2502   \u2502   # Do not call functions when jit is used      \
          \                                    \u2502\r\n\u2502   1503 \u2502   \u2502\
          \   full_backward_hooks, non_full_backward_hooks = [], []              \
          \               \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks\
          \ = []                                                           \u2502\r\
          \n\u2502                                                               \
          \                                   \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\
          \ in new_forward             \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502   162 \u2502   \u2502   \u2502   with torch.no_grad():\
          \                                                          \u2502\r\n\u2502\
          \   163 \u2502   \u2502   \u2502   \u2502   output = old_forward(*args,\
          \ **kwargs)                                      \u2502\r\n\u2502   164\
          \ \u2502   \u2502   else:                                              \
          \                                \u2502\r\n\u2502 \u2771 165 \u2502   \u2502\
          \   \u2502   output = old_forward(*args, **kwargs)                     \
          \                     \u2502\r\n\u2502   166 \u2502   \u2502   return module._hf_hook.post_forward(module,\
          \ output)                                \u2502\r\n\u2502   167 \u2502 \
          \                                                                      \
          \                   \u2502\r\n\u2502   168 \u2502   module.forward = new_forward\
          \                                                           \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /home/talha/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py:385\
          \ in     \u2502\r\n\u2502 forward                                      \
          \                                                    \u2502\r\n\u2502  \
          \                                                                      \
          \                          \u2502\r\n\u2502    382 \u2502   \u2502   residual\
          \ = hidden_states                                                      \
          \    \u2502\r\n\u2502    383 \u2502   \u2502                           \
          \                                                          \u2502\r\n\u2502\
          \    384 \u2502   \u2502   # Self attention.                           \
          \                                      \u2502\r\n\u2502 \u2771  385 \u2502\
          \   \u2502   attn_outputs = self.self_attention(                       \
          \                        \u2502\r\n\u2502    386 \u2502   \u2502   \u2502\
          \   layernorm_output,                                                  \
          \           \u2502\r\n\u2502    387 \u2502   \u2502   \u2502   layer_past=layer_past,\
          \                                                        \u2502\r\n\u2502\
          \    388 \u2502   \u2502   \u2502   attention_mask=attention_mask,     \
          \                                           \u2502\r\n\u2502           \
          \                                                                      \
          \                 \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
          \ in _call_impl      \u2502\r\n\u2502                                  \
          \                                                                \u2502\r\
          \n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
          \ or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502\
          \   \u2502   or _global_backward_pre_hooks or _global_backward_hooks   \
          \                \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502\
          \   or _global_forward_hooks or _global_forward_pre_hooks):            \
          \       \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args,\
          \ **kwargs)                                          \u2502\r\n\u2502  \
          \ 1502 \u2502   \u2502   # Do not call functions when jit is used      \
          \                                    \u2502\r\n\u2502   1503 \u2502   \u2502\
          \   full_backward_hooks, non_full_backward_hooks = [], []              \
          \               \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks\
          \ = []                                                           \u2502\r\
          \n\u2502                                                               \
          \                                   \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\
          \ in new_forward             \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502   162 \u2502   \u2502   \u2502   with torch.no_grad():\
          \                                                          \u2502\r\n\u2502\
          \   163 \u2502   \u2502   \u2502   \u2502   output = old_forward(*args,\
          \ **kwargs)                                      \u2502\r\n\u2502   164\
          \ \u2502   \u2502   else:                                              \
          \                                \u2502\r\n\u2502 \u2771 165 \u2502   \u2502\
          \   \u2502   output = old_forward(*args, **kwargs)                     \
          \                     \u2502\r\n\u2502   166 \u2502   \u2502   return module._hf_hook.post_forward(module,\
          \ output)                                \u2502\r\n\u2502   167 \u2502 \
          \                                                                      \
          \                   \u2502\r\n\u2502   168 \u2502   module.forward = new_forward\
          \                                                           \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /home/talha/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py:279\
          \ in     \u2502\r\n\u2502 forward                                      \
          \                                                    \u2502\r\n\u2502  \
          \                                                                      \
          \                          \u2502\r\n\u2502    276 \u2502   \u2502   \u2502\
          \   key_layer_ = key_layer.reshape(batch_size, self.num_kv, -1, self.head_dim)\
          \    \u2502\r\n\u2502    277 \u2502   \u2502   \u2502   value_layer_ = value_layer.reshape(batch_size,\
          \ self.num_kv, -1, self.head_di  \u2502\r\n\u2502    278 \u2502   \u2502\
          \   \u2502                                                             \
          \                    \u2502\r\n\u2502 \u2771  279 \u2502   \u2502   \u2502\
          \   attn_output = F.scaled_dot_product_attention(                      \
          \           \u2502\r\n\u2502    280 \u2502   \u2502   \u2502   \u2502  \
          \ query_layer_, key_layer_, value_layer_, None, 0.0, is_causal=True    \
          \     \u2502\r\n\u2502    281 \u2502   \u2502   \u2502   )             \
          \                                                                \u2502\r\
          \n\u2502    282                                                        \
          \                                   \u2502\r\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\r\n```"
        updatedAt: '2023-05-28T10:02:58.228Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Aspik101
        - mrhimanshu
        - disham993
    id: 647326d27be71eb8b1b5456d
    type: comment
  author: Talha
  content: "I am trying to use falcon with langhcain\r\nthis is what I am doing\r\n\
    **model**\r\n```\r\nmodel_id='falcon-7b'\r\ntokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True\
    \ )\r\nmodel = AutoModelForCausalLM.from_pretrained(model_id,trust_remote_code=True,\r\
    \n\r\n                                             quantization_config = BitsAndBytesConfig(load_in_4bit=True,\
    \ \r\n                                                                       \
    \ bnb_4bit_compute_dtype=torch.bfloat16),\r\n                                \
    \             device_map='auto')\r\npipe = pipeline(\r\n        \"text-generation\"\
    ,\r\n        model=model, \r\n        tokenizer=tokenizer, \r\n        max_new_tokens=256\r\
    \n    )\r\n\r\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\r\n\r\n```\r\n**embeddings**\r\
    \n```\r\nhfemb = HuggingFaceEmbeddings()\r\nretriever = vector_db.as_retriever()\r\
    \n```\r\n**memory**\r\n```\r\nmemory = ConversationBufferMemory(memory_key=\"\
    chat_history\", return_messages=True,output_key='answer')\r\nchain = ConversationalRetrievalChain.from_llm(local_llm,\r\
    \n                                           retriever=retriever, memory=memory,\r\
    \n                                          chain_type=\"map_reduce\", #\r\n \
    \                                         return_source_documents=True)\r\n```\r\
    \n\r\nThe code worked with flan-t5 but give error with falcon,\r\n\r\nHere is\
    \ full error\r\n```\r\n\u2502 [Errno 2] No such file or directory: '/tmp/ipykernel_67349/1702757523.py'\
    \                        \u2502\r\n\u2502                                    \
    \                                                              \u2502\r\n\u2502\
    \ /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:140 in\
    \ __call__           \u2502\r\n\u2502                                        \
    \                                                          \u2502\r\n\u2502  \
    \ 137 \u2502   \u2502   \u2502   )                                           \
    \                                   \u2502\r\n\u2502   138 \u2502   \u2502   except\
    \ (KeyboardInterrupt, Exception) as e:                                       \
    \ \u2502\r\n\u2502   139 \u2502   \u2502   \u2502   run_manager.on_chain_error(e)\
    \                                                  \u2502\r\n\u2502 \u2771 140\
    \ \u2502   \u2502   \u2502   raise e                                         \
    \                               \u2502\r\n\u2502   141 \u2502   \u2502   run_manager.on_chain_end(outputs)\
    \                                                  \u2502\r\n\u2502   142 \u2502\
    \   \u2502   return self.prep_outputs(inputs, outputs, return_only_outputs)  \
    \                   \u2502\r\n\u2502   143                                   \
    \                                                         \u2502\r\n\u2502   \
    \                                                                            \
    \                   \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:134\
    \ in __call__           \u2502\r\n\u2502                                     \
    \                                                             \u2502\r\n\u2502\
    \   131 \u2502   \u2502   )                                                  \
    \                                \u2502\r\n\u2502   132 \u2502   \u2502   try:\
    \                                                                            \
    \   \u2502\r\n\u2502   133 \u2502   \u2502   \u2502   outputs = (            \
    \                                                        \u2502\r\n\u2502 \u2771\
    \ 134 \u2502   \u2502   \u2502   \u2502   self._call(inputs, run_manager=run_manager)\
    \                                \u2502\r\n\u2502   135 \u2502   \u2502   \u2502\
    \   \u2502   if new_arg_supported                                            \
    \           \u2502\r\n\u2502   136 \u2502   \u2502   \u2502   \u2502   else self._call(inputs)\
    \                                                    \u2502\r\n\u2502   137 \u2502\
    \   \u2502   \u2502   )                                                      \
    \                        \u2502\r\n\u2502                                    \
    \                                                              \u2502\r\n\u2502\
    \ /home/talha/venv/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:\
    \ \u2502\r\n\u2502 110 in _call                                              \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   107 \u2502   \u2502   new_inputs = inputs.copy()        \
    \                                                 \u2502\r\n\u2502   108 \u2502\
    \   \u2502   new_inputs[\"question\"] = new_question                         \
    \                     \u2502\r\n\u2502   109 \u2502   \u2502   new_inputs[\"chat_history\"\
    ] = chat_history_str                                      \u2502\r\n\u2502 \u2771\
    \ 110 \u2502   \u2502   answer = self.combine_docs_chain.run(                \
    \                              \u2502\r\n\u2502   111 \u2502   \u2502   \u2502\
    \   input_documents=docs, callbacks=_run_manager.get_child(), **new_inputs   \
    \      \u2502\r\n\u2502   112 \u2502   \u2502   )                            \
    \                                                      \u2502\r\n\u2502   113\
    \ \u2502   \u2502   if self.return_source_documents:                         \
    \                          \u2502\r\n\u2502                                  \
    \                                                                \u2502\r\n\u2502\
    \ /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:239 in\
    \ run                \u2502\r\n\u2502                                        \
    \                                                          \u2502\r\n\u2502  \
    \ 236 \u2502   \u2502   \u2502   return self(args[0], callbacks=callbacks)[self.output_keys[0]]\
    \                 \u2502\r\n\u2502   237 \u2502   \u2502                     \
    \                                                                 \u2502\r\n\u2502\
    \   238 \u2502   \u2502   if kwargs and not args:                            \
    \                                \u2502\r\n\u2502 \u2771 239 \u2502   \u2502 \
    \  \u2502   return self(kwargs, callbacks=callbacks)[self.output_keys[0]]    \
    \              \u2502\r\n\u2502   240 \u2502   \u2502                        \
    \                                                              \u2502\r\n\u2502\
    \   241 \u2502   \u2502   if not kwargs and not args:                        \
    \                                \u2502\r\n\u2502   242 \u2502   \u2502   \u2502\
    \   raise ValueError(                                                        \
    \      \u2502\r\n\u2502                                                      \
    \                                            \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:140\
    \ in __call__           \u2502\r\n\u2502                                     \
    \                                                             \u2502\r\n\u2502\
    \   137 \u2502   \u2502   \u2502   )                                         \
    \                                     \u2502\r\n\u2502   138 \u2502   \u2502 \
    \  except (KeyboardInterrupt, Exception) as e:                               \
    \         \u2502\r\n\u2502   139 \u2502   \u2502   \u2502   run_manager.on_chain_error(e)\
    \                                                  \u2502\r\n\u2502 \u2771 140\
    \ \u2502   \u2502   \u2502   raise e                                         \
    \                               \u2502\r\n\u2502   141 \u2502   \u2502   run_manager.on_chain_end(outputs)\
    \                                                  \u2502\r\n\u2502   142 \u2502\
    \   \u2502   return self.prep_outputs(inputs, outputs, return_only_outputs)  \
    \                   \u2502\r\n\u2502   143                                   \
    \                                                         \u2502\r\n\u2502   \
    \                                                                            \
    \                   \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/base.py:134\
    \ in __call__           \u2502\r\n\u2502                                     \
    \                                                             \u2502\r\n\u2502\
    \   131 \u2502   \u2502   )                                                  \
    \                                \u2502\r\n\u2502   132 \u2502   \u2502   try:\
    \                                                                            \
    \   \u2502\r\n\u2502   133 \u2502   \u2502   \u2502   outputs = (            \
    \                                                        \u2502\r\n\u2502 \u2771\
    \ 134 \u2502   \u2502   \u2502   \u2502   self._call(inputs, run_manager=run_manager)\
    \                                \u2502\r\n\u2502   135 \u2502   \u2502   \u2502\
    \   \u2502   if new_arg_supported                                            \
    \           \u2502\r\n\u2502   136 \u2502   \u2502   \u2502   \u2502   else self._call(inputs)\
    \                                                    \u2502\r\n\u2502   137 \u2502\
    \   \u2502   \u2502   )                                                      \
    \                        \u2502\r\n\u2502                                    \
    \                                                              \u2502\r\n\u2502\
    \ /home/talha/venv/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\
    \ in   \u2502\r\n\u2502 _call                                                \
    \                                            \u2502\r\n\u2502                \
    \                                                                            \
    \      \u2502\r\n\u2502    81 \u2502   \u2502   docs = inputs[self.input_key]\
    \                                                      \u2502\r\n\u2502    82\
    \ \u2502   \u2502   # Other keys are assumed to be needed for LLM prediction \
    \                          \u2502\r\n\u2502    83 \u2502   \u2502   other_keys\
    \ = {k: v for k, v in inputs.items() if k != self.input_key}              \u2502\
    \r\n\u2502 \u2771  84 \u2502   \u2502   output, extra_return_dict = self.combine_docs(\
    \                                     \u2502\r\n\u2502    85 \u2502   \u2502 \
    \  \u2502   docs, callbacks=_run_manager.get_child(), **other_keys           \
    \              \u2502\r\n\u2502    86 \u2502   \u2502   )                    \
    \                                                              \u2502\r\n\u2502\
    \    87 \u2502   \u2502   extra_return_dict[self.output_key] = output        \
    \                                \u2502\r\n\u2502                            \
    \                                                                      \u2502\r\
    \n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:1\
    \ \u2502\r\n\u2502 44 in combine_docs                                        \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   141 \u2502   \u2502   Combine by mapping first chain over\
    \ all documents, then reducing the results.      \u2502\r\n\u2502   142 \u2502\
    \   \u2502   This reducing can be done recursively if needed (if there are many\
    \ documents).     \u2502\r\n\u2502   143 \u2502   \u2502   \"\"\"            \
    \                                                                    \u2502\r\n\
    \u2502 \u2771 144 \u2502   \u2502   results = self.llm_chain.apply(          \
    \                                          \u2502\r\n\u2502   145 \u2502   \u2502\
    \   \u2502   # FYI - this is parallelized and so it is fast.                 \
    \               \u2502\r\n\u2502   146 \u2502   \u2502   \u2502   [{self.document_variable_name:\
    \ d.page_content, **kwargs} for d in docs],       \u2502\r\n\u2502   147 \u2502\
    \   \u2502   \u2502   callbacks=callbacks,                                   \
    \                        \u2502\r\n\u2502                                    \
    \                                                              \u2502\r\n\u2502\
    \ /home/talha/venv/lib/python3.10/site-packages/langchain/chains/llm.py:157 in\
    \ apply               \u2502\r\n\u2502                                       \
    \                                                           \u2502\r\n\u2502 \
    \  154 \u2502   \u2502   \u2502   response = self.generate(input_list, run_manager=run_manager)\
    \                  \u2502\r\n\u2502   155 \u2502   \u2502   except (KeyboardInterrupt,\
    \ Exception) as e:                                        \u2502\r\n\u2502   156\
    \ \u2502   \u2502   \u2502   run_manager.on_chain_error(e)                   \
    \                               \u2502\r\n\u2502 \u2771 157 \u2502   \u2502  \
    \ \u2502   raise e                                                           \
    \             \u2502\r\n\u2502   158 \u2502   \u2502   outputs = self.create_outputs(response)\
    \                                            \u2502\r\n\u2502   159 \u2502   \u2502\
    \   run_manager.on_chain_end({\"outputs\": outputs})                         \
    \            \u2502\r\n\u2502   160 \u2502   \u2502   return outputs         \
    \                                                            \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/llm.py:154\
    \ in apply               \u2502\r\n\u2502                                    \
    \                                                              \u2502\r\n\u2502\
    \   151 \u2502   \u2502   \u2502   {\"input_list\": input_list},             \
    \                                       \u2502\r\n\u2502   152 \u2502   \u2502\
    \   )                                                                        \
    \          \u2502\r\n\u2502   153 \u2502   \u2502   try:                     \
    \                                                          \u2502\r\n\u2502 \u2771\
    \ 154 \u2502   \u2502   \u2502   response = self.generate(input_list, run_manager=run_manager)\
    \                  \u2502\r\n\u2502   155 \u2502   \u2502   except (KeyboardInterrupt,\
    \ Exception) as e:                                        \u2502\r\n\u2502   156\
    \ \u2502   \u2502   \u2502   run_manager.on_chain_error(e)                   \
    \                               \u2502\r\n\u2502   157 \u2502   \u2502   \u2502\
    \   raise e                                                                  \
    \      \u2502\r\n\u2502                                                      \
    \                                            \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/chains/llm.py:79\
    \ in generate             \u2502\r\n\u2502                                   \
    \                                                               \u2502\r\n\u2502\
    \    76 \u2502   ) -> LLMResult:                                             \
    \                           \u2502\r\n\u2502    77 \u2502   \u2502   \"\"\"Generate\
    \ LLM result from inputs.\"\"\"                                             \u2502\
    \r\n\u2502    78 \u2502   \u2502   prompts, stop = self.prep_prompts(input_list,\
    \ run_manager=run_manager)             \u2502\r\n\u2502 \u2771  79 \u2502   \u2502\
    \   return self.llm.generate_prompt(                                         \
    \          \u2502\r\n\u2502    80 \u2502   \u2502   \u2502   prompts, stop, callbacks=run_manager.get_child()\
    \ if run_manager else None      \u2502\r\n\u2502    81 \u2502   \u2502   )   \
    \                                                                            \
    \   \u2502\r\n\u2502    82                                                   \
    \                                         \u2502\r\n\u2502                   \
    \                                                                            \
    \   \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/base.py:134\
    \ in generate_prompt      \u2502\r\n\u2502                                   \
    \                                                               \u2502\r\n\u2502\
    \   131 \u2502   \u2502   callbacks: Callbacks = None,                       \
    \                                \u2502\r\n\u2502   132 \u2502   ) -> LLMResult:\
    \                                                                        \u2502\
    \r\n\u2502   133 \u2502   \u2502   prompt_strings = [p.to_string() for p in prompts]\
    \                                  \u2502\r\n\u2502 \u2771 134 \u2502   \u2502\
    \   return self.generate(prompt_strings, stop=stop, callbacks=callbacks)     \
    \          \u2502\r\n\u2502   135 \u2502                                     \
    \                                                     \u2502\r\n\u2502   136 \u2502\
    \   async def agenerate_prompt(                                              \
    \              \u2502\r\n\u2502   137 \u2502   \u2502   self,                \
    \                                                              \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/base.py:191\
    \ in generate             \u2502\r\n\u2502                                   \
    \                                                               \u2502\r\n\u2502\
    \   188 \u2502   \u2502   \u2502   \u2502   )                                \
    \                                          \u2502\r\n\u2502   189 \u2502   \u2502\
    \   \u2502   except (KeyboardInterrupt, Exception) as e:                     \
    \               \u2502\r\n\u2502   190 \u2502   \u2502   \u2502   \u2502   run_manager.on_llm_error(e)\
    \                                                \u2502\r\n\u2502 \u2771 191 \u2502\
    \   \u2502   \u2502   \u2502   raise e                                       \
    \                             \u2502\r\n\u2502   192 \u2502   \u2502   \u2502\
    \   run_manager.on_llm_end(output)                                           \
    \      \u2502\r\n\u2502   193 \u2502   \u2502   \u2502   return output       \
    \                                                           \u2502\r\n\u2502 \
    \  194 \u2502   \u2502   if len(missing_prompts) > 0:                        \
    \                               \u2502\r\n\u2502                             \
    \                                                                     \u2502\r\
    \n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/base.py:185\
    \ in generate             \u2502\r\n\u2502                                   \
    \                                                               \u2502\r\n\u2502\
    \   182 \u2502   \u2502   \u2502   )                                         \
    \                                     \u2502\r\n\u2502   183 \u2502   \u2502 \
    \  \u2502   try:                                                             \
    \              \u2502\r\n\u2502   184 \u2502   \u2502   \u2502   \u2502   output\
    \ = (                                                                 \u2502\r\
    \n\u2502 \u2771 185 \u2502   \u2502   \u2502   \u2502   \u2502   self._generate(prompts,\
    \ stop=stop, run_manager=run_manager)            \u2502\r\n\u2502   186 \u2502\
    \   \u2502   \u2502   \u2502   \u2502   if new_arg_supported                 \
    \                                  \u2502\r\n\u2502   187 \u2502   \u2502   \u2502\
    \   \u2502   \u2502   else self._generate(prompts, stop=stop)                \
    \                \u2502\r\n\u2502   188 \u2502   \u2502   \u2502   \u2502   )\
    \                                                                          \u2502\
    \r\n\u2502                                                                   \
    \                               \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/langchain/llms/base.py:436\
    \ in _generate            \u2502\r\n\u2502                                   \
    \                                                               \u2502\r\n\u2502\
    \   433 \u2502   \u2502   new_arg_supported = inspect.signature(self._call).parameters.get(\"\
    run_manager\")    \u2502\r\n\u2502   434 \u2502   \u2502   for prompt in prompts:\
    \                                                             \u2502\r\n\u2502\
    \   435 \u2502   \u2502   \u2502   text = (                                  \
    \                                     \u2502\r\n\u2502 \u2771 436 \u2502   \u2502\
    \   \u2502   \u2502   self._call(prompt, stop=stop, run_manager=run_manager) \
    \                    \u2502\r\n\u2502   437 \u2502   \u2502   \u2502   \u2502\
    \   if new_arg_supported                                                     \
    \  \u2502\r\n\u2502   438 \u2502   \u2502   \u2502   \u2502   else self._call(prompt,\
    \ stop=stop)                                         \u2502\r\n\u2502   439 \u2502\
    \   \u2502   \u2502   )                                                      \
    \                        \u2502\r\n\u2502                                    \
    \                                                              \u2502\r\n\u2502\
    \ /home/talha/venv/lib/python3.10/site-packages/langchain/llms/huggingface_pipeline.py:159\
    \ in      \u2502\r\n\u2502 _call                                             \
    \                                               \u2502\r\n\u2502             \
    \                                                                            \
    \         \u2502\r\n\u2502   156 \u2502   \u2502   stop: Optional[List[str]] =\
    \ None,                                                  \u2502\r\n\u2502   157\
    \ \u2502   \u2502   run_manager: Optional[CallbackManagerForLLMRun] = None,  \
    \                          \u2502\r\n\u2502   158 \u2502   ) -> str:         \
    \                                                                     \u2502\r\
    \n\u2502 \u2771 159 \u2502   \u2502   response = self.pipeline(prompt)       \
    \                                            \u2502\r\n\u2502   160 \u2502   \u2502\
    \   if self.pipeline.task == \"text-generation\":                            \
    \            \u2502\r\n\u2502   161 \u2502   \u2502   \u2502   # Text generation\
    \ return includes the starter text.                            \u2502\r\n\u2502\
    \   162 \u2502   \u2502   \u2502   text = response[0][\"generated_text\"][len(prompt)\
    \ :]                            \u2502\r\n\u2502                             \
    \                                                                     \u2502\r\
    \n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:201\
    \ in   \u2502\r\n\u2502 __call__                                             \
    \                                            \u2502\r\n\u2502                \
    \                                                                            \
    \      \u2502\r\n\u2502   198 \u2502   \u2502   \u2502   - **generated_token_ids**\
    \ (`torch.Tensor` or `tf.Tensor`, present when `retu   \u2502\r\n\u2502   199\
    \ \u2502   \u2502   \u2502     ids of the generated text.                    \
    \                               \u2502\r\n\u2502   200 \u2502   \u2502   \"\"\"\
    \                                                                            \
    \    \u2502\r\n\u2502 \u2771 201 \u2502   \u2502   return super().__call__(text_inputs,\
    \ **kwargs)                                     \u2502\r\n\u2502   202 \u2502\
    \                                                                            \
    \              \u2502\r\n\u2502   203 \u2502   def preprocess(self, prompt_text,\
    \ prefix=\"\", handle_long_generation=None, **generate   \u2502\r\n\u2502   204\
    \ \u2502   \u2502   inputs = self.tokenizer(                                 \
    \                          \u2502\r\n\u2502                                  \
    \                                                                \u2502\r\n\u2502\
    \ /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1118\
    \ in __call__    \u2502\r\n\u2502                                            \
    \                                                      \u2502\r\n\u2502   1115\
    \ \u2502   \u2502   \u2502   \u2502   )                                      \
    \                                   \u2502\r\n\u2502   1116 \u2502   \u2502  \
    \ \u2502   )                                                                 \
    \            \u2502\r\n\u2502   1117 \u2502   \u2502   else:                 \
    \                                                            \u2502\r\n\u2502\
    \ \u2771 1118 \u2502   \u2502   \u2502   return self.run_single(inputs, preprocess_params,\
    \ forward_params, postproces  \u2502\r\n\u2502   1119 \u2502                 \
    \                                                                        \u2502\
    \r\n\u2502   1120 \u2502   def run_multi(self, inputs, preprocess_params, forward_params,\
    \ postprocess_params):   \u2502\r\n\u2502   1121 \u2502   \u2502   return [self.run_single(item,\
    \ preprocess_params, forward_params, postprocess_par  \u2502\r\n\u2502       \
    \                                                                            \
    \               \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1125\
    \ in run_single  \u2502\r\n\u2502                                            \
    \                                                      \u2502\r\n\u2502   1122\
    \ \u2502                                                                     \
    \                    \u2502\r\n\u2502   1123 \u2502   def run_single(self, inputs,\
    \ preprocess_params, forward_params, postprocess_params):  \u2502\r\n\u2502  \
    \ 1124 \u2502   \u2502   model_inputs = self.preprocess(inputs, **preprocess_params)\
    \                       \u2502\r\n\u2502 \u2771 1125 \u2502   \u2502   model_outputs\
    \ = self.forward(model_inputs, **forward_params)                      \u2502\r\
    \n\u2502   1126 \u2502   \u2502   outputs = self.postprocess(model_outputs, **postprocess_params)\
    \                   \u2502\r\n\u2502   1127 \u2502   \u2502   return outputs \
    \                                                                   \u2502\r\n\
    \u2502   1128                                                                \
    \                           \u2502\r\n\u2502                                 \
    \                                                                 \u2502\r\n\u2502\
    \ /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1024\
    \ in forward     \u2502\r\n\u2502                                            \
    \                                                      \u2502\r\n\u2502   1021\
    \ \u2502   \u2502   \u2502   \u2502   inference_context = self.get_inference_context()\
    \                          \u2502\r\n\u2502   1022 \u2502   \u2502   \u2502  \
    \ \u2502   with inference_context():                                         \
    \        \u2502\r\n\u2502   1023 \u2502   \u2502   \u2502   \u2502   \u2502  \
    \ model_inputs = self._ensure_tensor_on_device(model_inputs, device=se  \u2502\
    \r\n\u2502 \u2771 1024 \u2502   \u2502   \u2502   \u2502   \u2502   model_outputs\
    \ = self._forward(model_inputs, **forward_params)         \u2502\r\n\u2502   1025\
    \ \u2502   \u2502   \u2502   \u2502   \u2502   model_outputs = self._ensure_tensor_on_device(model_outputs,\
    \ device=  \u2502\r\n\u2502   1026 \u2502   \u2502   \u2502   else:          \
    \                                                               \u2502\r\n\u2502\
    \   1027 \u2502   \u2502   \u2502   \u2502   raise ValueError(f\"Framework {self.framework}\
    \ is not supported\")          \u2502\r\n\u2502                              \
    \                                                                    \u2502\r\n\
    \u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:263\
    \ in   \u2502\r\n\u2502 _forward                                             \
    \                                            \u2502\r\n\u2502                \
    \                                                                            \
    \      \u2502\r\n\u2502   260 \u2502   \u2502   \u2502   \u2502   generate_kwargs[\"\
    min_length\"] += prefix_length                             \u2502\r\n\u2502  \
    \ 261 \u2502   \u2502                                                        \
    \                              \u2502\r\n\u2502   262 \u2502   \u2502   # BS x\
    \ SL                                                                         \
    \ \u2502\r\n\u2502 \u2771 263 \u2502   \u2502   generated_sequence = self.model.generate(input_ids=input_ids,\
    \ attention_mask=att   \u2502\r\n\u2502   264 \u2502   \u2502   out_b = generated_sequence.shape[0]\
    \                                                \u2502\r\n\u2502   265 \u2502\
    \   \u2502   if self.framework == \"pt\":                                    \
    \                     \u2502\r\n\u2502   266 \u2502   \u2502   \u2502   generated_sequence\
    \ = generated_sequence.reshape(in_b, out_b // in_b, *genera   \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\
    \ in decorate_context \u2502\r\n\u2502                                       \
    \                                                           \u2502\r\n\u2502 \
    \  112 \u2502   @functools.wraps(func)                                       \
    \                          \u2502\r\n\u2502   113 \u2502   def decorate_context(*args,\
    \ **kwargs):                                                 \u2502\r\n\u2502\
    \   114 \u2502   \u2502   with ctx_factory():                                \
    \                                \u2502\r\n\u2502 \u2771 115 \u2502   \u2502 \
    \  \u2502   return func(*args, **kwargs)                                     \
    \              \u2502\r\n\u2502   116 \u2502                                 \
    \                                                         \u2502\r\n\u2502   117\
    \ \u2502   return decorate_context                                           \
    \                     \u2502\r\n\u2502   118                                 \
    \                                                           \u2502\r\n\u2502 \
    \                                                                            \
    \                     \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1518\
    \ in generate  \u2502\r\n\u2502                                              \
    \                                                    \u2502\r\n\u2502   1515 \u2502\
    \   \u2502   \u2502   \u2502   )                                             \
    \                            \u2502\r\n\u2502   1516 \u2502   \u2502   \u2502\
    \                                                                            \
    \     \u2502\r\n\u2502   1517 \u2502   \u2502   \u2502   # 11. run greedy search\
    \                                                       \u2502\r\n\u2502 \u2771\
    \ 1518 \u2502   \u2502   \u2502   return self.greedy_search(                 \
    \                                   \u2502\r\n\u2502   1519 \u2502   \u2502  \
    \ \u2502   \u2502   input_ids,                                               \
    \                 \u2502\r\n\u2502   1520 \u2502   \u2502   \u2502   \u2502  \
    \ logits_processor=logits_processor,                                        \u2502\
    \r\n\u2502   1521 \u2502   \u2502   \u2502   \u2502   stopping_criteria=stopping_criteria,\
    \                                      \u2502\r\n\u2502                      \
    \                                                                            \u2502\
    \r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/transformers/generation/utils.py:2335\
    \ in           \u2502\r\n\u2502 greedy_search                                \
    \                                                    \u2502\r\n\u2502        \
    \                                                                            \
    \              \u2502\r\n\u2502   2332 \u2502   \u2502   \u2502   model_inputs\
    \ = self.prepare_inputs_for_generation(input_ids, **model_kwargs)  \u2502\r\n\u2502\
    \   2333 \u2502   \u2502   \u2502                                            \
    \                                     \u2502\r\n\u2502   2334 \u2502   \u2502\
    \   \u2502   # forward pass to get next token                                \
    \              \u2502\r\n\u2502 \u2771 2335 \u2502   \u2502   \u2502   outputs\
    \ = self(                                                               \u2502\
    \r\n\u2502   2336 \u2502   \u2502   \u2502   \u2502   **model_inputs,        \
    \                                                   \u2502\r\n\u2502   2337 \u2502\
    \   \u2502   \u2502   \u2502   return_dict=True,                             \
    \                            \u2502\r\n\u2502   2338 \u2502   \u2502   \u2502\
    \   \u2502   output_attentions=output_attentions,                            \
    \          \u2502\r\n\u2502                                                  \
    \                                                \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
    \ in _call_impl      \u2502\r\n\u2502                                        \
    \                                                          \u2502\r\n\u2502  \
    \ 1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502  \
    \ \u2502   or _global_backward_pre_hooks or _global_backward_hooks           \
    \        \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501\
    \ \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)            \
    \                              \u2502\r\n\u2502   1502 \u2502   \u2502   # Do\
    \ not call functions when jit is used                                        \
    \  \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
    \ = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502\
    \   backward_pre_hooks = []                                                  \
    \         \u2502\r\n\u2502                                                   \
    \                                               \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\
    \ in new_forward             \u2502\r\n\u2502                                \
    \                                                                  \u2502\r\n\u2502\
    \   162 \u2502   \u2502   \u2502   with torch.no_grad():                     \
    \                                     \u2502\r\n\u2502   163 \u2502   \u2502 \
    \  \u2502   \u2502   output = old_forward(*args, **kwargs)                   \
    \                   \u2502\r\n\u2502   164 \u2502   \u2502   else:           \
    \                                                                   \u2502\r\n\
    \u2502 \u2771 165 \u2502   \u2502   \u2502   output = old_forward(*args, **kwargs)\
    \                                          \u2502\r\n\u2502   166 \u2502   \u2502\
    \   return module._hf_hook.post_forward(module, output)                      \
    \          \u2502\r\n\u2502   167 \u2502                                     \
    \                                                     \u2502\r\n\u2502   168 \u2502\
    \   module.forward = new_forward                                             \
    \              \u2502\r\n\u2502                                              \
    \                                                    \u2502\r\n\u2502 /home/talha/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py:753\
    \ in     \u2502\r\n\u2502 forward                                            \
    \                                              \u2502\r\n\u2502              \
    \                                                                            \
    \        \u2502\r\n\u2502    750 \u2502   \u2502                             \
    \                                                        \u2502\r\n\u2502    751\
    \ \u2502   \u2502   return_dict = return_dict if return_dict is not None else\
    \ self.config.use_return  \u2502\r\n\u2502    752 \u2502   \u2502            \
    \                                                                         \u2502\
    \r\n\u2502 \u2771  753 \u2502   \u2502   transformer_outputs = self.transformer(\
    \                                           \u2502\r\n\u2502    754 \u2502   \u2502\
    \   \u2502   input_ids,                                                      \
    \              \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   past_key_values=past_key_values,\
    \                                              \u2502\r\n\u2502    756 \u2502\
    \   \u2502   \u2502   attention_mask=attention_mask,                         \
    \                       \u2502\r\n\u2502                                     \
    \                                                             \u2502\r\n\u2502\
    \ /home/talha/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
    \ in _call_impl      \u2502\r\n\u2502                                        \
    \                                                          \u2502\r\n\u2502  \
    \ 1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502  \
    \ \u2502   or _global_backward_pre_hooks or _global_backward_hooks           \
    \        \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501\
    \ \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)            \
    \                              \u2502\r\n\u2502   1502 \u2502   \u2502   # Do\
    \ not call functions when jit is used                                        \
    \  \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
    \ = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502\
    \   backward_pre_hooks = []                                                  \
    \         \u2502\r\n\u2502                                                   \
    \                                               \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\
    \ in new_forward             \u2502\r\n\u2502                                \
    \                                                                  \u2502\r\n\u2502\
    \   162 \u2502   \u2502   \u2502   with torch.no_grad():                     \
    \                                     \u2502\r\n\u2502   163 \u2502   \u2502 \
    \  \u2502   \u2502   output = old_forward(*args, **kwargs)                   \
    \                   \u2502\r\n\u2502   164 \u2502   \u2502   else:           \
    \                                                                   \u2502\r\n\
    \u2502 \u2771 165 \u2502   \u2502   \u2502   output = old_forward(*args, **kwargs)\
    \                                          \u2502\r\n\u2502   166 \u2502   \u2502\
    \   return module._hf_hook.post_forward(module, output)                      \
    \          \u2502\r\n\u2502   167 \u2502                                     \
    \                                                     \u2502\r\n\u2502   168 \u2502\
    \   module.forward = new_forward                                             \
    \              \u2502\r\n\u2502                                              \
    \                                                    \u2502\r\n\u2502 /home/talha/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py:648\
    \ in     \u2502\r\n\u2502 forward                                            \
    \                                              \u2502\r\n\u2502              \
    \                                                                            \
    \        \u2502\r\n\u2502    645 \u2502   \u2502   \u2502   \u2502   \u2502  \
    \ head_mask[i],                                                         \u2502\
    \r\n\u2502    646 \u2502   \u2502   \u2502   \u2502   )                      \
    \                                                   \u2502\r\n\u2502    647 \u2502\
    \   \u2502   \u2502   else:                                                  \
    \                       \u2502\r\n\u2502 \u2771  648 \u2502   \u2502   \u2502\
    \   \u2502   outputs = block(                                                \
    \          \u2502\r\n\u2502    649 \u2502   \u2502   \u2502   \u2502   \u2502\
    \   hidden_states,                                                        \u2502\
    \r\n\u2502    650 \u2502   \u2502   \u2502   \u2502   \u2502   layer_past=layer_past,\
    \                                                \u2502\r\n\u2502    651 \u2502\
    \   \u2502   \u2502   \u2502   \u2502   attention_mask=causal_mask,          \
    \                                 \u2502\r\n\u2502                           \
    \                                                                       \u2502\
    \r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
    \ in _call_impl      \u2502\r\n\u2502                                        \
    \                                                          \u2502\r\n\u2502  \
    \ 1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502  \
    \ \u2502   or _global_backward_pre_hooks or _global_backward_hooks           \
    \        \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501\
    \ \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)            \
    \                              \u2502\r\n\u2502   1502 \u2502   \u2502   # Do\
    \ not call functions when jit is used                                        \
    \  \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
    \ = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502\
    \   backward_pre_hooks = []                                                  \
    \         \u2502\r\n\u2502                                                   \
    \                                               \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\
    \ in new_forward             \u2502\r\n\u2502                                \
    \                                                                  \u2502\r\n\u2502\
    \   162 \u2502   \u2502   \u2502   with torch.no_grad():                     \
    \                                     \u2502\r\n\u2502   163 \u2502   \u2502 \
    \  \u2502   \u2502   output = old_forward(*args, **kwargs)                   \
    \                   \u2502\r\n\u2502   164 \u2502   \u2502   else:           \
    \                                                                   \u2502\r\n\
    \u2502 \u2771 165 \u2502   \u2502   \u2502   output = old_forward(*args, **kwargs)\
    \                                          \u2502\r\n\u2502   166 \u2502   \u2502\
    \   return module._hf_hook.post_forward(module, output)                      \
    \          \u2502\r\n\u2502   167 \u2502                                     \
    \                                                     \u2502\r\n\u2502   168 \u2502\
    \   module.forward = new_forward                                             \
    \              \u2502\r\n\u2502                                              \
    \                                                    \u2502\r\n\u2502 /home/talha/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py:385\
    \ in     \u2502\r\n\u2502 forward                                            \
    \                                              \u2502\r\n\u2502              \
    \                                                                            \
    \        \u2502\r\n\u2502    382 \u2502   \u2502   residual = hidden_states  \
    \                                                        \u2502\r\n\u2502    383\
    \ \u2502   \u2502                                                            \
    \                         \u2502\r\n\u2502    384 \u2502   \u2502   # Self attention.\
    \                                                                 \u2502\r\n\u2502\
    \ \u2771  385 \u2502   \u2502   attn_outputs = self.self_attention(          \
    \                                     \u2502\r\n\u2502    386 \u2502   \u2502\
    \   \u2502   layernorm_output,                                               \
    \              \u2502\r\n\u2502    387 \u2502   \u2502   \u2502   layer_past=layer_past,\
    \                                                        \u2502\r\n\u2502    388\
    \ \u2502   \u2502   \u2502   attention_mask=attention_mask,                  \
    \                              \u2502\r\n\u2502                              \
    \                                                                    \u2502\r\n\
    \u2502 /home/talha/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\
    \ in _call_impl      \u2502\r\n\u2502                                        \
    \                                                          \u2502\r\n\u2502  \
    \ 1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502  \
    \ \u2502   or _global_backward_pre_hooks or _global_backward_hooks           \
    \        \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks\
    \ or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501\
    \ \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)            \
    \                              \u2502\r\n\u2502   1502 \u2502   \u2502   # Do\
    \ not call functions when jit is used                                        \
    \  \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks\
    \ = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502\
    \   backward_pre_hooks = []                                                  \
    \         \u2502\r\n\u2502                                                   \
    \                                               \u2502\r\n\u2502 /home/talha/venv/lib/python3.10/site-packages/accelerate/hooks.py:165\
    \ in new_forward             \u2502\r\n\u2502                                \
    \                                                                  \u2502\r\n\u2502\
    \   162 \u2502   \u2502   \u2502   with torch.no_grad():                     \
    \                                     \u2502\r\n\u2502   163 \u2502   \u2502 \
    \  \u2502   \u2502   output = old_forward(*args, **kwargs)                   \
    \                   \u2502\r\n\u2502   164 \u2502   \u2502   else:           \
    \                                                                   \u2502\r\n\
    \u2502 \u2771 165 \u2502   \u2502   \u2502   output = old_forward(*args, **kwargs)\
    \                                          \u2502\r\n\u2502   166 \u2502   \u2502\
    \   return module._hf_hook.post_forward(module, output)                      \
    \          \u2502\r\n\u2502   167 \u2502                                     \
    \                                                     \u2502\r\n\u2502   168 \u2502\
    \   module.forward = new_forward                                             \
    \              \u2502\r\n\u2502                                              \
    \                                                    \u2502\r\n\u2502 /home/talha/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py:279\
    \ in     \u2502\r\n\u2502 forward                                            \
    \                                              \u2502\r\n\u2502              \
    \                                                                            \
    \        \u2502\r\n\u2502    276 \u2502   \u2502   \u2502   key_layer_ = key_layer.reshape(batch_size,\
    \ self.num_kv, -1, self.head_dim)    \u2502\r\n\u2502    277 \u2502   \u2502 \
    \  \u2502   value_layer_ = value_layer.reshape(batch_size, self.num_kv, -1, self.head_di\
    \  \u2502\r\n\u2502    278 \u2502   \u2502   \u2502                          \
    \                                                       \u2502\r\n\u2502 \u2771\
    \  279 \u2502   \u2502   \u2502   attn_output = F.scaled_dot_product_attention(\
    \                                 \u2502\r\n\u2502    280 \u2502   \u2502   \u2502\
    \   \u2502   query_layer_, key_layer_, value_layer_, None, 0.0, is_causal=True\
    \         \u2502\r\n\u2502    281 \u2502   \u2502   \u2502   )               \
    \                                                              \u2502\r\n\u2502\
    \    282                                                                     \
    \                      \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n```"
  created_at: 2023-05-28 09:02:58+00:00
  edited: false
  hidden: false
  id: 647326d27be71eb8b1b5456d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T06:56:21.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: '<p>Thanks for the report, should be resolved with: <a href="https://huggingface.co/tiiuae/falcon-7b/commit/1ba2370c784b56f8b31afc66d5234e8fb40a7209">https://huggingface.co/tiiuae/falcon-7b/commit/1ba2370c784b56f8b31afc66d5234e8fb40a7209</a>.<br>Though
          note that inference with datatypes other than bfloat16 has not been fully
          validated and may incur some model degradation. </p>

          <p>Let us know if you have any more issues.</p>

          '
        raw: "Thanks for the report, should be resolved with: https://huggingface.co/tiiuae/falcon-7b/commit/1ba2370c784b56f8b31afc66d5234e8fb40a7209.\n\
          Though note that inference with datatypes other than bfloat16 has not been\
          \ fully validated and may incur some model degradation. \n\n\nLet us know\
          \ if you have any more issues."
        updatedAt: '2023-05-30T06:56:21.289Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64759e15ab17a37d0b15e352
    id: 64759e15ab17a37d0b15e351
    type: comment
  author: FalconLLM
  content: "Thanks for the report, should be resolved with: https://huggingface.co/tiiuae/falcon-7b/commit/1ba2370c784b56f8b31afc66d5234e8fb40a7209.\n\
    Though note that inference with datatypes other than bfloat16 has not been fully\
    \ validated and may incur some model degradation. \n\n\nLet us know if you have\
    \ any more issues."
  created_at: 2023-05-30 05:56:21+00:00
  edited: false
  hidden: false
  id: 64759e15ab17a37d0b15e351
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T06:56:21.000Z'
    data:
      status: closed
    id: 64759e15ab17a37d0b15e352
    type: status-change
  author: FalconLLM
  created_at: 2023-05-30 05:56:21+00:00
  id: 64759e15ab17a37d0b15e352
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: tiiuae/falcon-7b
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: Expected query, key, and value to have the same dtype, but got
  query.dtype: float key.dtype: float  and value.dtype: c10::Half instead.'
