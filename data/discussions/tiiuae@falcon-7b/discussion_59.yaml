!!python/object:huggingface_hub.community.DiscussionWithDetails
author: amnasher
conflicting_files: null
created_at: 2023-07-12 10:54:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76a4df67bfc77200bc25d9a798fcef6a.svg
      fullname: Amna sher afal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: amnasher
      type: user
    createdAt: '2023-07-12T11:54:18.000Z'
    data:
      edited: true
      editors:
      - amnasher
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5113819241523743
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76a4df67bfc77200bc25d9a798fcef6a.svg
          fullname: Amna sher afal
          isHf: false
          isPro: false
          name: amnasher
          type: user
        html: '<p>Hello I have fintuned the falcon " ybelkada/falcon-7b-sharded-bf16
          " but during inference it is taking too much time like it took 28 minutes
          on a single prompt when i assigned a token size of 700 how can I resolve
          this issue?</p>

          <p>generation_config = model.generation_config<br>generation_config.max_new_tokens
          = 200<br>generation_config.temperature= 0.7<br>generation_config.top_p =
          1.0<br>generation_config.num_return_sequences = 1<br>generation_config.pad_token_id
          = tokenizer.eos_token_id<br>generation_config.eos_token_id = tokenizer.eos_token_id</p>

          <p>This is my code.</p>

          <p>prompt= f"""</p>

          <p>Human : Query<br>Assistant :<br>"""<br>encoding = tokenizer(prompt, return_tensors
          = ''pt'').to(DEVICE)<br>with torch.inference_mode():<br>outputs = model.generate(<br>input_ids
          = encoding.input_ids,<br>attention_mask = encoding.attention_mask,<br>generation_config
          = generation_config,<br>)</p>

          <p>print(tokenizer.decode(outputs[0], skip_special_tokens = True))</p>

          '
        raw: 'Hello I have fintuned the falcon " ybelkada/falcon-7b-sharded-bf16 "
          but during inference it is taking too much time like it took 28 minutes
          on a single prompt when i assigned a token size of 700 how can I resolve
          this issue?


          generation_config = model.generation_config

          generation_config.max_new_tokens = 200

          generation_config.temperature= 0.7

          generation_config.top_p = 1.0

          generation_config.num_return_sequences = 1

          generation_config.pad_token_id = tokenizer.eos_token_id

          generation_config.eos_token_id = tokenizer.eos_token_id


          This is my code.


          prompt= f"""


          Human : Query

          Assistant :

          """

          encoding = tokenizer(prompt, return_tensors = ''pt'').to(DEVICE)

          with torch.inference_mode():

          outputs = model.generate(

          input_ids = encoding.input_ids,

          attention_mask = encoding.attention_mask,

          generation_config = generation_config,

          )


          print(tokenizer.decode(outputs[0], skip_special_tokens = True))'
        updatedAt: '2023-07-12T11:54:35.529Z'
      numEdits: 1
      reactions: []
    id: 64ae946a098720e214e5c341
    type: comment
  author: amnasher
  content: 'Hello I have fintuned the falcon " ybelkada/falcon-7b-sharded-bf16 " but
    during inference it is taking too much time like it took 28 minutes on a single
    prompt when i assigned a token size of 700 how can I resolve this issue?


    generation_config = model.generation_config

    generation_config.max_new_tokens = 200

    generation_config.temperature= 0.7

    generation_config.top_p = 1.0

    generation_config.num_return_sequences = 1

    generation_config.pad_token_id = tokenizer.eos_token_id

    generation_config.eos_token_id = tokenizer.eos_token_id


    This is my code.


    prompt= f"""


    Human : Query

    Assistant :

    """

    encoding = tokenizer(prompt, return_tensors = ''pt'').to(DEVICE)

    with torch.inference_mode():

    outputs = model.generate(

    input_ids = encoding.input_ids,

    attention_mask = encoding.attention_mask,

    generation_config = generation_config,

    )


    print(tokenizer.decode(outputs[0], skip_special_tokens = True))'
  created_at: 2023-07-12 10:54:18+00:00
  edited: true
  hidden: false
  id: 64ae946a098720e214e5c341
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 59
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: Inference time issue
