!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tdicommons
conflicting_files: null
created_at: 2023-07-06 14:10:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448fe6a3adf50d864080b53/sZM9IFD1v-j9Vmn2A0FJb.png?w=200&h=200&f=face
      fullname: tdi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tdicommons
      type: user
    createdAt: '2023-07-06T15:10:11.000Z'
    data:
      edited: false
      editors:
      - tdicommons
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4295758903026581
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448fe6a3adf50d864080b53/sZM9IFD1v-j9Vmn2A0FJb.png?w=200&h=200&f=face
          fullname: tdi
          isHf: false
          isPro: false
          name: tdicommons
          type: user
        html: '<p>I have trained falcon-7b and pushed the finetuned model to hub,
          now when i am trying to use Langchain with my new model- which is saved
          at "tdicommons/falcon_7b_6_27_2023".<br>I am using following code to proceed
          with my objective:</p>

          <p>```from langchain.llms import HuggingFacePipeline<br>import torch<br>from
          transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM</p>

          <p>model_id = "tdicommons/falcon_7b_6_27_2023" # our finetuned model from
          HF here.<br>tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)<br>model
          = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True)</p>

          <p>pipe = pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    max_length=100<br>)</p>

          <p>local_llm = HuggingFacePipeline(pipeline=pipe)```</p>

          <p>I am getting an error, which is something i can''t understand - can anyone
          help me on this ?? </p>

          <p>Error: ---------------------------------------------------------------------------<br>ValueError                                Traceback
          (most recent call last)<br> in &lt;cell line: 8&gt;()<br>      6<br>      7
          model_id = "tdicommons/falcon_7b_6_27_2023" # our finetuned model from HF
          here.<br>----&gt; 8 tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)<br>      9
          model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True)<br>     10
          </p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py
          in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)<br>    717                     )<br>    718<br>--&gt;
          719         raise ValueError(<br>    720             f"Unrecognized configuration
          class {config.<strong>class</strong>} to build an AutoTokenizer.\n"<br>    721             f"Model
          type should be one of {'', ''.join(c.<strong>name</strong> for c in TOKENIZER_MAPPING.keys())}."</p>

          <p>ValueError: Unrecognized configuration class &lt;class ''transformers_modules.tiiuae.falcon-7b.2f5c3cd4eace6be6c0f12981f377fb35e5bf6ee5.configuration_RW.RWConfig''&gt;
          to build an AutoTokenizer.<br>Model type should be one of AlbertConfig,
          AlignConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig,
          BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig,
          BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, CamembertConfig,
          CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig,
          CodeGenConfig, ConvBertConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig,
          DebertaConfig, DebertaV2Config, DistilBertConfig, DPRConfig, ElectraConfig,
          ErnieConfig, ErnieMConfig, EsmConfig, FlaubertConfig, FNetConfig, FSMTConfig,
          FunnelConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig,
          GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig,
          GroupViTConfig, HubertConfig, IBertConfig, JukeboxConfig, LayoutLMConfig,
          LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig,
          LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config,
          MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MgpstrConfig,
          MobileBertConfig, MPNetConfig, MT5Config, MvpConfig, NezhaConfig, NllbMoeConfig,
          NystromformerConfig, OneFormerConfig, OpenAIGPTConfig, OPTConfig, OwlViTConfig,
          PegasusConfig, PegasusXConfig, PerceiverConfig, Pix2StructConfig, PLBartConfig,
          ProphetNetConfig, QDQBertConfig, RagConfig, RealmConfig, ReformerConfig,
          RemBertConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig,
          RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2TextConfig, Speech2Text2Config,
          SpeechT5Confi...</p>

          '
        raw: "I have trained falcon-7b and pushed the finetuned model to hub, now\
          \ when i am trying to use Langchain with my new model- which is saved at\
          \ \"tdicommons/falcon_7b_6_27_2023\". \r\nI am using following code to proceed\
          \ with my objective:\r\n\r\n\r\n```from langchain.llms import HuggingFacePipeline\r\
          \nimport torch\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
          \ pipeline, AutoModelForSeq2SeqLM\r\n\r\nmodel_id = \"tdicommons/falcon_7b_6_27_2023\"\
          \ # our finetuned model from HF here.\r\ntokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\r\
          \nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True)\r\
          \n\r\npipe = pipeline(\r\n    \"text-generation\",\r\n    model=model, \r\
          \n    tokenizer=tokenizer, \r\n    max_length=100\r\n)\r\n\r\nlocal_llm\
          \ = HuggingFacePipeline(pipeline=pipe)```\r\n\r\nI am getting an error,\
          \ which is something i can't understand - can anyone help me on this ??\
          \ \r\n\r\nError: ---------------------------------------------------------------------------\r\
          \nValueError                                Traceback (most recent call\
          \ last)\r\n<ipython-input-19-1462851e614d> in <cell line: 8>()\r\n     \
          \ 6 \r\n      7 model_id = \"tdicommons/falcon_7b_6_27_2023\" # our finetuned\
          \ model from HF here.\r\n----> 8 tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\r\
          \n      9 model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True)\r\
          \n     10 \r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\r\
          \n    717                     )\r\n    718 \r\n--> 719         raise ValueError(\r\
          \n    720             f\"Unrecognized configuration class {config.__class__}\
          \ to build an AutoTokenizer.\\n\"\r\n    721             f\"Model type should\
          \ be one of {', '.join(c.__name__ for c in TOKENIZER_MAPPING.keys())}.\"\
          \r\n\r\nValueError: Unrecognized configuration class <class 'transformers_modules.tiiuae.falcon-7b.2f5c3cd4eace6be6c0f12981f377fb35e5bf6ee5.configuration_RW.RWConfig'>\
          \ to build an AutoTokenizer.\r\nModel type should be one of AlbertConfig,\
          \ AlignConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig,\
          \ BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig,\
          \ BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, CamembertConfig,\
          \ CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig,\
          \ CodeGenConfig, ConvBertConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig,\
          \ DebertaConfig, DebertaV2Config, DistilBertConfig, DPRConfig, ElectraConfig,\
          \ ErnieConfig, ErnieMConfig, EsmConfig, FlaubertConfig, FNetConfig, FSMTConfig,\
          \ FunnelConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig,\
          \ GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig,\
          \ GroupViTConfig, HubertConfig, IBertConfig, JukeboxConfig, LayoutLMConfig,\
          \ LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig,\
          \ LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config,\
          \ MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MgpstrConfig,\
          \ MobileBertConfig, MPNetConfig, MT5Config, MvpConfig, NezhaConfig, NllbMoeConfig,\
          \ NystromformerConfig, OneFormerConfig, OpenAIGPTConfig, OPTConfig, OwlViTConfig,\
          \ PegasusConfig, PegasusXConfig, PerceiverConfig, Pix2StructConfig, PLBartConfig,\
          \ ProphetNetConfig, QDQBertConfig, RagConfig, RealmConfig, ReformerConfig,\
          \ RemBertConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig,\
          \ RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2TextConfig, Speech2Text2Config,\
          \ SpeechT5Confi..."
        updatedAt: '2023-07-06T15:10:11.884Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - maxolotl
    id: 64a6d9538385661ebafd090b
    type: comment
  author: tdicommons
  content: "I have trained falcon-7b and pushed the finetuned model to hub, now when\
    \ i am trying to use Langchain with my new model- which is saved at \"tdicommons/falcon_7b_6_27_2023\"\
    . \r\nI am using following code to proceed with my objective:\r\n\r\n\r\n```from\
    \ langchain.llms import HuggingFacePipeline\r\nimport torch\r\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\r\
    \n\r\nmodel_id = \"tdicommons/falcon_7b_6_27_2023\" # our finetuned model from\
    \ HF here.\r\ntokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\r\
    \nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True)\r\
    \n\r\npipe = pipeline(\r\n    \"text-generation\",\r\n    model=model, \r\n  \
    \  tokenizer=tokenizer, \r\n    max_length=100\r\n)\r\n\r\nlocal_llm = HuggingFacePipeline(pipeline=pipe)```\r\
    \n\r\nI am getting an error, which is something i can't understand - can anyone\
    \ help me on this ?? \r\n\r\nError: ---------------------------------------------------------------------------\r\
    \nValueError                                Traceback (most recent call last)\r\
    \n<ipython-input-19-1462851e614d> in <cell line: 8>()\r\n      6 \r\n      7 model_id\
    \ = \"tdicommons/falcon_7b_6_27_2023\" # our finetuned model from HF here.\r\n\
    ----> 8 tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\r\
    \n      9 model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True)\r\
    \n     10 \r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\r\n\
    \    717                     )\r\n    718 \r\n--> 719         raise ValueError(\r\
    \n    720             f\"Unrecognized configuration class {config.__class__} to\
    \ build an AutoTokenizer.\\n\"\r\n    721             f\"Model type should be\
    \ one of {', '.join(c.__name__ for c in TOKENIZER_MAPPING.keys())}.\"\r\n\r\n\
    ValueError: Unrecognized configuration class <class 'transformers_modules.tiiuae.falcon-7b.2f5c3cd4eace6be6c0f12981f377fb35e5bf6ee5.configuration_RW.RWConfig'>\
    \ to build an AutoTokenizer.\r\nModel type should be one of AlbertConfig, AlignConfig,\
    \ BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig,\
    \ BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config,\
    \ BloomConfig, BridgeTowerConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig,\
    \ ClapConfig, CLIPConfig, CLIPSegConfig, CodeGenConfig, ConvBertConfig, CpmAntConfig,\
    \ CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig,\
    \ DPRConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, FlaubertConfig,\
    \ FNetConfig, FSMTConfig, FunnelConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig,\
    \ GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig,\
    \ GroupViTConfig, HubertConfig, IBertConfig, JukeboxConfig, LayoutLMConfig, LayoutLMv2Config,\
    \ LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config,\
    \ LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MBartConfig, MegaConfig,\
    \ MegatronBertConfig, MgpstrConfig, MobileBertConfig, MPNetConfig, MT5Config,\
    \ MvpConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig,\
    \ OpenAIGPTConfig, OPTConfig, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig,\
    \ Pix2StructConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, RagConfig,\
    \ RealmConfig, ReformerConfig, RemBertConfig, RetriBertConfig, RobertaConfig,\
    \ RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2TextConfig,\
    \ Speech2Text2Config, SpeechT5Confi..."
  created_at: 2023-07-06 14:10:11+00:00
  edited: false
  hidden: false
  id: 64a6d9538385661ebafd090b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/babd4398298c35f431ceed4b518bd6b9.svg
      fullname: Max
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: maxolotl
      type: user
    createdAt: '2023-09-01T06:33:45.000Z'
    data:
      edited: false
      editors:
      - maxolotl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9916205406188965
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/babd4398298c35f431ceed4b518bd6b9.svg
          fullname: Max
          isHf: false
          isPro: false
          name: maxolotl
          type: user
        html: '<p>Also having this issue, have any luck solving it?</p>

          '
        raw: Also having this issue, have any luck solving it?
        updatedAt: '2023-09-01T06:33:45.521Z'
      numEdits: 0
      reactions: []
    id: 64f185c998ba1724c9119454
    type: comment
  author: maxolotl
  content: Also having this issue, have any luck solving it?
  created_at: 2023-09-01 05:33:45+00:00
  edited: false
  hidden: false
  id: 64f185c998ba1724c9119454
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448fe6a3adf50d864080b53/sZM9IFD1v-j9Vmn2A0FJb.png?w=200&h=200&f=face
      fullname: tdi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tdicommons
      type: user
    createdAt: '2023-09-02T19:34:00.000Z'
    data:
      edited: false
      editors:
      - tdicommons
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5276539325714111
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448fe6a3adf50d864080b53/sZM9IFD1v-j9Vmn2A0FJb.png?w=200&h=200&f=face
          fullname: tdi
          isHf: false
          isPro: false
          name: tdicommons
          type: user
        html: '<p>Yes, you need to use right Tokenizer,<br> model_id = "tiiuae/falcon-7b"
          # our finetuned model from HF here.<br>----&gt; 8 tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)</p>

          <p>2- You need to use right version of Transformers, we have used<br>pip
          install -U git+<a rel="nofollow" href="https://github.com/huggingface/transformers.git@e03a9cc">https://github.com/huggingface/transformers.git@e03a9cc</a>
          &amp;&amp; <br>    pip install -U git+<a rel="nofollow" href="https://github.com/huggingface/peft.git@42a184f">https://github.com/huggingface/peft.git@42a184f</a>
          &amp;&amp; <br>    pip install -U git+<a rel="nofollow" href="https://github.com/huggingface/accelerate.git@c9fbb71">https://github.com/huggingface/accelerate.git@c9fbb71</a>
          &amp;&amp; <br>    pip install einops==0.6.1 &amp;&amp; <br>    pip install
          torch==2.0.1 &amp;&amp; <br>    pip install bitsandbytes==0.39.0 &amp;&amp;
          <br>    pip install scipy &amp;&amp; <br>    pip install loralib==0.1.1
          &amp;&amp; \</p>

          '
        raw: "Yes, you need to use right Tokenizer, \n model_id = \"tiiuae/falcon-7b\"\
          \ # our finetuned model from HF here.\n----> 8 tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\n\
          \n2- You need to use right version of Transformers, we have used \npip install\
          \ -U git+https://github.com/huggingface/transformers.git@e03a9cc && \\\n\
          \    pip install -U git+https://github.com/huggingface/peft.git@42a184f\
          \ && \\\n    pip install -U git+https://github.com/huggingface/accelerate.git@c9fbb71\
          \ && \\\n    pip install einops==0.6.1 && \\\n    pip install torch==2.0.1\
          \ && \\\n    pip install bitsandbytes==0.39.0 && \\\n    pip install scipy\
          \ && \\\n    pip install loralib==0.1.1 && \\"
        updatedAt: '2023-09-02T19:34:00.963Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64f38e2982fc091e71f7d3be
    id: 64f38e2882fc091e71f7d3bb
    type: comment
  author: tdicommons
  content: "Yes, you need to use right Tokenizer, \n model_id = \"tiiuae/falcon-7b\"\
    \ # our finetuned model from HF here.\n----> 8 tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\n\
    \n2- You need to use right version of Transformers, we have used \npip install\
    \ -U git+https://github.com/huggingface/transformers.git@e03a9cc && \\\n    pip\
    \ install -U git+https://github.com/huggingface/peft.git@42a184f && \\\n    pip\
    \ install -U git+https://github.com/huggingface/accelerate.git@c9fbb71 && \\\n\
    \    pip install einops==0.6.1 && \\\n    pip install torch==2.0.1 && \\\n   \
    \ pip install bitsandbytes==0.39.0 && \\\n    pip install scipy && \\\n    pip\
    \ install loralib==0.1.1 && \\"
  created_at: 2023-09-02 18:34:00+00:00
  edited: false
  hidden: false
  id: 64f38e2882fc091e71f7d3bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448fe6a3adf50d864080b53/sZM9IFD1v-j9Vmn2A0FJb.png?w=200&h=200&f=face
      fullname: tdi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tdicommons
      type: user
    createdAt: '2023-09-02T19:34:01.000Z'
    data:
      status: closed
    id: 64f38e2982fc091e71f7d3be
    type: status-change
  author: tdicommons
  created_at: 2023-09-02 18:34:01+00:00
  id: 64f38e2982fc091e71f7d3be
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 56
repo_id: tiiuae/falcon-7b
repo_type: model
status: closed
target_branch: null
title: 'ValueError: Unrecognized configuration class <class ''transformers_modules.tiiuae.falcon-7b.2f5c3cd4eace6be6c0f12981f377fb35e5bf6ee5.configuration_RW.RWConfig''>
  to build an AutoTokenizer.'
