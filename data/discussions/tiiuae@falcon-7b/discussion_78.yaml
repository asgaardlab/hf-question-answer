!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aimananees
conflicting_files: null
created_at: 2023-09-03 20:23:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/454f7922467c831581b97a350a404a7d.svg
      fullname: Aiman Abdullah Anees
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aimananees
      type: user
    createdAt: '2023-09-03T21:23:38.000Z'
    data:
      edited: false
      editors:
      - aimananees
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7102148532867432
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/454f7922467c831581b97a350a404a7d.svg
          fullname: Aiman Abdullah Anees
          isHf: false
          isPro: false
          name: aimananees
          type: user
        html: "<p>Hi, I'm trying to load a model using quantization but it constantly\
          \ errors out. I have pip installed all the needed libraries which includes\
          \ <code>accelerate</code> and <code>bitsandbytes</code>. I have tried multiple\
          \ times but no luck. Is anyone facing the same issue?</p>\n<p>Here's my\
          \ code snippet:</p>\n<pre><code>import torch\nfrom transformers import BitsAndBytesConfig\n\
          from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\
          \nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n  \
          \  bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\"\
          ,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel_id = \"tiiuae/falcon-7b\"\
          \n\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\n        model_id,\
          \ \n        device_map=\"auto\",\n        quantization_config=quantization_config,\n\
          \        )\n\n</code></pre>\n"
        raw: "Hi, I'm trying to load a model using quantization but it constantly\
          \ errors out. I have pip installed all the needed libraries which includes\
          \ `accelerate` and `bitsandbytes`. I have tried multiple times but no luck.\
          \ Is anyone facing the same issue?\r\n\r\nHere's my code snippet:\r\n```\r\
          \nimport torch\r\nfrom transformers import BitsAndBytesConfig\r\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer, pipeline\r\n\r\nquantization_config\
          \ = BitsAndBytesConfig(\r\n    load_in_4bit=True,\r\n    bnb_4bit_compute_dtype=torch.float16,\r\
          \n    bnb_4bit_quant_type=\"nf4\",\r\n    bnb_4bit_use_double_quant=True,\r\
          \n)\r\n\r\nmodel_id = \"tiiuae/falcon-7b\"\r\n\r\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\r\
          \n        model_id, \r\n        device_map=\"auto\",\r\n        quantization_config=quantization_config,\r\
          \n        )\r\n\r\n\r\n```"
        updatedAt: '2023-09-03T21:23:38.089Z'
      numEdits: 0
      reactions: []
    id: 64f4f95ab56de37d9634ada7
    type: comment
  author: aimananees
  content: "Hi, I'm trying to load a model using quantization but it constantly errors\
    \ out. I have pip installed all the needed libraries which includes `accelerate`\
    \ and `bitsandbytes`. I have tried multiple times but no luck. Is anyone facing\
    \ the same issue?\r\n\r\nHere's my code snippet:\r\n```\r\nimport torch\r\nfrom\
    \ transformers import BitsAndBytesConfig\r\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer, pipeline\r\n\r\nquantization_config = BitsAndBytesConfig(\r\n\
    \    load_in_4bit=True,\r\n    bnb_4bit_compute_dtype=torch.float16,\r\n    bnb_4bit_quant_type=\"\
    nf4\",\r\n    bnb_4bit_use_double_quant=True,\r\n)\r\n\r\nmodel_id = \"tiiuae/falcon-7b\"\
    \r\n\r\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\r\n        model_id,\
    \ \r\n        device_map=\"auto\",\r\n        quantization_config=quantization_config,\r\
    \n        )\r\n\r\n\r\n```"
  created_at: 2023-09-03 20:23:38+00:00
  edited: false
  hidden: false
  id: 64f4f95ab56de37d9634ada7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/61bab76e80023f48169118acc181a56a.svg
      fullname: Warade
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yugal5
      type: user
    createdAt: '2023-09-06T12:58:38.000Z'
    data:
      edited: false
      editors:
      - Yugal5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8525514602661133
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/61bab76e80023f48169118acc181a56a.svg
          fullname: Warade
          isHf: false
          isPro: false
          name: Yugal5
          type: user
        html: '<p>Yeah, I am also facing same issue. Tried all options available on
          HF, Stackoverflow ,etc</p>

          '
        raw: 'Yeah, I am also facing same issue. Tried all options available on HF,
          Stackoverflow ,etc

          '
        updatedAt: '2023-09-06T12:58:38.181Z'
      numEdits: 0
      reactions: []
    id: 64f8777e7565a69eb6ad569c
    type: comment
  author: Yugal5
  content: 'Yeah, I am also facing same issue. Tried all options available on HF,
    Stackoverflow ,etc

    '
  created_at: 2023-09-06 11:58:38+00:00
  edited: false
  hidden: false
  id: 64f8777e7565a69eb6ad569c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
      fullname: Stephan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codegood
      type: user
    createdAt: '2023-09-29T01:06:00.000Z'
    data:
      edited: false
      editors:
      - codegood
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7365613579750061
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
          fullname: Stephan
          isHf: false
          isPro: false
          name: codegood
          type: user
        html: '<p>Use previous version of transformers.<br>!pip install transformers==4.30</p>

          '
        raw: 'Use previous version of transformers.

          !pip install transformers==4.30'
        updatedAt: '2023-09-29T01:06:00.214Z'
      numEdits: 0
      reactions: []
    id: 651622f85da02ec184dc5956
    type: comment
  author: codegood
  content: 'Use previous version of transformers.

    !pip install transformers==4.30'
  created_at: 2023-09-29 00:06:00+00:00
  edited: false
  hidden: false
  id: 651622f85da02ec184dc5956
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94d8d1969f6aa1bf7764936541dc0818.svg
      fullname: Thilini Wijesiriwardene
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thiliniiw
      type: user
    createdAt: '2023-09-30T19:54:12.000Z'
    data:
      edited: true
      editors:
      - Thiliniiw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8675979375839233
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94d8d1969f6aa1bf7764936541dc0818.svg
          fullname: Thilini Wijesiriwardene
          isHf: false
          isPro: false
          name: Thiliniiw
          type: user
        html: '<p>If you are in a notebook, restarting the session worked for me.
          See below.<br><a rel="nofollow" href="https://github.com/huggingface/transformers/issues/23323#issuecomment-1568464656">https://github.com/huggingface/transformers/issues/23323#issuecomment-1568464656</a></p>

          '
        raw: 'If you are in a notebook, restarting the session worked for me. See
          below.

          https://github.com/huggingface/transformers/issues/23323#issuecomment-1568464656'
        updatedAt: '2023-09-30T19:55:28.401Z'
      numEdits: 1
      reactions: []
    id: 65187ce43fe8cc63b4bbb48e
    type: comment
  author: Thiliniiw
  content: 'If you are in a notebook, restarting the session worked for me. See below.

    https://github.com/huggingface/transformers/issues/23323#issuecomment-1568464656'
  created_at: 2023-09-30 18:54:12+00:00
  edited: true
  hidden: false
  id: 65187ce43fe8cc63b4bbb48e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 78
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: 'ImportError: Using `load_in_8bit=True` requires Accelerate'
