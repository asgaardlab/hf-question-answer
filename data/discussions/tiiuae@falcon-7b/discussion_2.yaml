!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bilelm
conflicting_files: null
created_at: 2023-05-26 14:09:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/114e6991a581a3e18d64d2a0e334b26b.svg
      fullname: Bilel Moulahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bilelm
      type: user
    createdAt: '2023-05-26T15:09:15.000Z'
    data:
      edited: false
      editors:
      - bilelm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/114e6991a581a3e18d64d2a0e334b26b.svg
          fullname: Bilel Moulahi
          isHf: false
          isPro: false
          name: bilelm
          type: user
        html: '<p>Hello,<br>Thank you for sharing this model!<br>Could you specify
          the minimum requirements needed to run this model for inference ? </p>

          '
        raw: "Hello, \r\nThank you for sharing this model! \r\nCould you specify the\
          \ minimum requirements needed to run this model for inference ? "
        updatedAt: '2023-05-26T15:09:15.124Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pdakin
    id: 6470cb9bd9360cd9d8e82ac3
    type: comment
  author: bilelm
  content: "Hello, \r\nThank you for sharing this model! \r\nCould you specify the\
    \ minimum requirements needed to run this model for inference ? "
  created_at: 2023-05-26 14:09:15+00:00
  edited: false
  hidden: false
  id: 6470cb9bd9360cd9d8e82ac3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654022901910-61a8d1aac664736898ffc84f.jpeg?w=200&h=200&f=face
      fullname: Daniel Hesslow
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DanielHesslow
      type: user
    createdAt: '2023-05-26T16:21:14.000Z'
    data:
      edited: false
      editors:
      - DanielHesslow
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654022901910-61a8d1aac664736898ffc84f.jpeg?w=200&h=200&f=face
          fullname: Daniel Hesslow
          isHf: false
          isPro: false
          name: DanielHesslow
          type: user
        html: '<p>It requires at least 14GB of ram, smallest I''ve tried is an A10
          which works well. </p>

          <p>It''s also trained with bf16 which is only available on Ampere and later,
          I would expect some performance degradation if running it in fp16 instead.</p>

          '
        raw: "It requires at least 14GB of ram, smallest I've tried is an A10 which\
          \ works well. \n\nIt's also trained with bf16 which is only available on\
          \ Ampere and later, I would expect some performance degradation if running\
          \ it in fp16 instead."
        updatedAt: '2023-05-26T16:21:14.526Z'
      numEdits: 0
      reactions: []
    id: 6470dc7a1f0e7ee7fb1e1d99
    type: comment
  author: DanielHesslow
  content: "It requires at least 14GB of ram, smallest I've tried is an A10 which\
    \ works well. \n\nIt's also trained with bf16 which is only available on Ampere\
    \ and later, I would expect some performance degradation if running it in fp16\
    \ instead."
  created_at: 2023-05-26 15:21:14+00:00
  edited: false
  hidden: false
  id: 6470dc7a1f0e7ee7fb1e1d99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/114e6991a581a3e18d64d2a0e334b26b.svg
      fullname: Bilel Moulahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bilelm
      type: user
    createdAt: '2023-05-26T20:02:53.000Z'
    data:
      edited: false
      editors:
      - bilelm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/114e6991a581a3e18d64d2a0e334b26b.svg
          fullname: Bilel Moulahi
          isHf: false
          isPro: false
          name: bilelm
          type: user
        html: '<p>Thank you!<br>I will try it on a A5000 24Go, hope it''s ok.</p>

          '
        raw: 'Thank you!

          I will try it on a A5000 24Go, hope it''s ok.'
        updatedAt: '2023-05-26T20:02:53.627Z'
      numEdits: 0
      reactions: []
    id: 6471106d1c2bfd5b7bf8ef58
    type: comment
  author: bilelm
  content: 'Thank you!

    I will try it on a A5000 24Go, hope it''s ok.'
  created_at: 2023-05-26 19:02:53+00:00
  edited: false
  hidden: false
  id: 6471106d1c2bfd5b7bf8ef58
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/528891da9d97043cda200559d5c8d591.svg
      fullname: Nick relvan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zynos
      type: user
    createdAt: '2023-05-27T09:20:36.000Z'
    data:
      edited: false
      editors:
      - zynos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/528891da9d97043cda200559d5c8d591.svg
          fullname: Nick relvan
          isHf: false
          isPro: false
          name: zynos
          type: user
        html: '<p>I am running the given code on Windows (CPU with 32 GB RAM) but
          it keeps running for 2 hours+ without printing the results?<br>Someone have
          an idea on how to solve this?</p>

          '
        raw: 'I am running the given code on Windows (CPU with 32 GB RAM) but it keeps
          running for 2 hours+ without printing the results?

          Someone have an idea on how to solve this?'
        updatedAt: '2023-05-27T09:20:36.934Z'
      numEdits: 0
      reactions: []
    id: 6471cb640211f85270f9ec08
    type: comment
  author: zynos
  content: 'I am running the given code on Windows (CPU with 32 GB RAM) but it keeps
    running for 2 hours+ without printing the results?

    Someone have an idea on how to solve this?'
  created_at: 2023-05-27 08:20:36+00:00
  edited: false
  hidden: false
  id: 6471cb640211f85270f9ec08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf57e3c6d0a9fa281c4a982b0c93558a.svg
      fullname: Kamalraj M M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kamaljp
      type: user
    createdAt: '2023-05-27T12:57:14.000Z'
    data:
      edited: false
      editors:
      - Kamaljp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf57e3c6d0a9fa281c4a982b0c93558a.svg
          fullname: Kamalraj M M
          isHf: false
          isPro: false
          name: Kamaljp
          type: user
        html: '<p>I am trying to load this model on Colab, but it doesn''t load in
          the GPU.<br>What am I missing? I am using the code provided in the model
          card and installing the transformers library. Still, model is not loading
          in GPU.</p>

          '
        raw: "I am trying to load this model on Colab, but it doesn't load in the\
          \ GPU. \nWhat am I missing? I am using the code provided in the model card\
          \ and installing the transformers library. Still, model is not loading in\
          \ GPU."
        updatedAt: '2023-05-27T12:57:14.544Z'
      numEdits: 0
      reactions: []
    id: 6471fe2a97a75cc77aab43d3
    type: comment
  author: Kamaljp
  content: "I am trying to load this model on Colab, but it doesn't load in the GPU.\
    \ \nWhat am I missing? I am using the code provided in the model card and installing\
    \ the transformers library. Still, model is not loading in GPU."
  created_at: 2023-05-27 11:57:14+00:00
  edited: false
  hidden: false
  id: 6471fe2a97a75cc77aab43d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5bd26c0b59efc800107253c2f6c5832f.svg
      fullname: collectiv
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: akashcollectiv
      type: user
    createdAt: '2023-05-28T20:06:58.000Z'
    data:
      edited: true
      editors:
      - akashcollectiv
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5bd26c0b59efc800107253c2f6c5832f.svg
          fullname: collectiv
          isHf: false
          isPro: true
          name: akashcollectiv
          type: user
        html: '<p>i Tried in 40G A100 , worked well , but slow , took about 10min
          for single input ,<br>got a 80G A100 , after loading check point , its crashed<br>return
          (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)<br>```torch.cuda.OutOfMemoryError:
          CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.32 GiB total capacity;
          77.15 GiB already allocated; 832.00 KiB free; 78.13 GiB reserved in total
          by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF``<br>what am i missing ?</p>

          '
        raw: "i Tried in 40G A100 , worked well , but slow , took about 10min for\
          \ single input , \ngot a 80G A100 , after loading check point , its crashed\n\
          return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) *\
          \ sin)\n```torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 2.00 MiB (GPU 0; 79.32 GiB total capacity; 77.15 GiB already allocated;\
          \ 832.00 KiB free; 78.13 GiB reserved in total by PyTorch) If reserved memory\
          \ is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF``\n\
          what am i missing ?"
        updatedAt: '2023-05-28T20:07:39.537Z'
      numEdits: 1
      reactions: []
    id: 6473b46263001a0002d00f0b
    type: comment
  author: akashcollectiv
  content: "i Tried in 40G A100 , worked well , but slow , took about 10min for single\
    \ input , \ngot a 80G A100 , after loading check point , its crashed\nreturn (q\
    \ * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n```torch.cuda.OutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.32 GiB total capacity;\
    \ 77.15 GiB already allocated; 832.00 KiB free; 78.13 GiB reserved in total by\
    \ PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
    \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF``\n\
    what am i missing ?"
  created_at: 2023-05-28 19:06:58+00:00
  edited: true
  hidden: false
  id: 6473b46263001a0002d00f0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T14:51:23.000Z'
    data:
      edited: false
      editors:
      - FalconLLM
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
          fullname: Falcon LLM TII UAE
          isHf: false
          isPro: false
          name: FalconLLM
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;zynos&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zynos\">@<span class=\"\
          underline\">zynos</span></a></span>\n\n\t</span></span> it's unlikely you\
          \ will get anything in a reasonable time on CPU, you really need a GPU for\
          \ this sort of model. </p>\n<p><span data-props=\"{&quot;user&quot;:&quot;akashcollectiv&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/akashcollectiv\"\
          >@<span class=\"underline\">akashcollectiv</span></a></span>\n\n\t</span></span>\
          \ are you sure you are not trying to load Falcon-40B instead? The 7B should\
          \ fit fine on an A100 80GB. </p>\n"
        raw: "@zynos it's unlikely you will get anything in a reasonable time on CPU,\
          \ you really need a GPU for this sort of model. \n\n@akashcollectiv are\
          \ you sure you are not trying to load Falcon-40B instead? The 7B should\
          \ fit fine on an A100 80GB. "
        updatedAt: '2023-05-30T14:51:23.030Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64760d6b400d8a9ea630efbd
    id: 64760d6b400d8a9ea630efbc
    type: comment
  author: FalconLLM
  content: "@zynos it's unlikely you will get anything in a reasonable time on CPU,\
    \ you really need a GPU for this sort of model. \n\n@akashcollectiv are you sure\
    \ you are not trying to load Falcon-40B instead? The 7B should fit fine on an\
    \ A100 80GB. "
  created_at: 2023-05-30 13:51:23+00:00
  edited: false
  hidden: false
  id: 64760d6b400d8a9ea630efbc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471c2c76facfb01d8ac3278/ii7e_5o4jBoK3pS8WMaWK.png?w=200&h=200&f=face
      fullname: Falcon LLM TII UAE
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: FalconLLM
      type: user
    createdAt: '2023-05-30T14:51:23.000Z'
    data:
      status: closed
    id: 64760d6b400d8a9ea630efbd
    type: status-change
  author: FalconLLM
  created_at: 2023-05-30 13:51:23+00:00
  id: 64760d6b400d8a9ea630efbd
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668524739440-6373a9542d4eccfa6f90e97c.jpeg?w=200&h=200&f=face
      fullname: Rachel Shalom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rachelshalom
      type: user
    createdAt: '2023-06-04T07:54:47.000Z'
    data:
      edited: false
      editors:
      - rachelshalom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9603450298309326
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668524739440-6373a9542d4eccfa6f90e97c.jpeg?w=200&h=200&f=face
          fullname: Rachel Shalom
          isHf: false
          isPro: false
          name: rachelshalom
          type: user
        html: '<p>all of you are working with pytorch2?</p>

          '
        raw: all of you are working with pytorch2?
        updatedAt: '2023-06-04T07:54:47.341Z'
      numEdits: 0
      reactions: []
    id: 647c434783c62f3249113340
    type: comment
  author: rachelshalom
  content: all of you are working with pytorch2?
  created_at: 2023-06-04 06:54:47+00:00
  edited: false
  hidden: false
  id: 647c434783c62f3249113340
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6bd5cfdc21506ace6176d00a2973d8e5.svg
      fullname: BenfengXu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SpiketheCowboy
      type: user
    createdAt: '2023-06-06T09:48:35.000Z'
    data:
      edited: true
      editors:
      - SpiketheCowboy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7420409917831421
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6bd5cfdc21506ace6176d00a2973d8e5.svg
          fullname: BenfengXu
          isHf: false
          isPro: false
          name: SpiketheCowboy
          type: user
        html: '<p>me too, getting OOM when sequence length exceeds 1200+<br>using
          A100 80GB, bf16, and inference only (no_grad) for 7B falcon model<br>and
          yes, I''m using pytorch 2.0</p>

          <blockquote>

          <p>i Tried in 40G A100 , worked well , but slow , took about 10min for single
          input ,<br>got a 80G A100 , after loading check point , its crashed<br>return
          (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)<br>```torch.cuda.OutOfMemoryError:
          CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.32 GiB total capacity;
          77.15 GiB already allocated; 832.00 KiB free; 78.13 GiB reserved in total
          by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF``<br>what am i missing ?</p>

          </blockquote>

          '
        raw: "me too, getting OOM when sequence length exceeds 1200+\nusing A100 80GB,\
          \ bf16, and inference only (no_grad) for 7B falcon model\nand yes, I'm using\
          \ pytorch 2.0\n\n> i Tried in 40G A100 , worked well , but slow , took about\
          \ 10min for single input , \n> got a 80G A100 , after loading check point\
          \ , its crashed\n> return (q * cos) + (rotate_half(q) * sin), (k * cos)\
          \ + (rotate_half(k) * sin)\n> ```torch.cuda.OutOfMemoryError: CUDA out of\
          \ memory. Tried to allocate 2.00 MiB (GPU 0; 79.32 GiB total capacity; 77.15\
          \ GiB already allocated; 832.00 KiB free; 78.13 GiB reserved in total by\
          \ PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF``\n\
          > what am i missing ?"
        updatedAt: '2023-06-06T09:49:40.592Z'
      numEdits: 2
      reactions: []
    id: 647f00f31a446a624a39a295
    type: comment
  author: SpiketheCowboy
  content: "me too, getting OOM when sequence length exceeds 1200+\nusing A100 80GB,\
    \ bf16, and inference only (no_grad) for 7B falcon model\nand yes, I'm using pytorch\
    \ 2.0\n\n> i Tried in 40G A100 , worked well , but slow , took about 10min for\
    \ single input , \n> got a 80G A100 , after loading check point , its crashed\n\
    > return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n\
    > ```torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB\
    \ (GPU 0; 79.32 GiB total capacity; 77.15 GiB already allocated; 832.00 KiB free;\
    \ 78.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory\
    \ try setting max_split_size_mb to avoid fragmentation.  See documentation for\
    \ Memory Management and PYTORCH_CUDA_ALLOC_CONF``\n> what am i missing ?"
  created_at: 2023-06-06 08:48:35+00:00
  edited: true
  hidden: false
  id: 647f00f31a446a624a39a295
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf57e3c6d0a9fa281c4a982b0c93558a.svg
      fullname: Kamalraj M M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kamaljp
      type: user
    createdAt: '2023-06-06T14:58:35.000Z'
    data:
      edited: false
      editors:
      - Kamaljp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9916301965713501
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf57e3c6d0a9fa281c4a982b0c93558a.svg
          fullname: Kamalraj M M
          isHf: false
          isPro: false
          name: Kamaljp
          type: user
        html: '<p>Have you played with batch size? Halving the batch size seems to
          help</p>

          '
        raw: Have you played with batch size? Halving the batch size seems to help
        updatedAt: '2023-06-06T14:58:35.829Z'
      numEdits: 0
      reactions: []
    id: 647f499b2a7bcaa307a7428e
    type: comment
  author: Kamaljp
  content: Have you played with batch size? Halving the batch size seems to help
  created_at: 2023-06-06 13:58:35+00:00
  edited: false
  hidden: false
  id: 647f499b2a7bcaa307a7428e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6bd5cfdc21506ace6176d00a2973d8e5.svg
      fullname: BenfengXu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SpiketheCowboy
      type: user
    createdAt: '2023-06-06T15:30:48.000Z'
    data:
      edited: false
      editors:
      - SpiketheCowboy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9864215850830078
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6bd5cfdc21506ace6176d00a2973d8e5.svg
          fullname: BenfengXu
          isHf: false
          isPro: false
          name: SpiketheCowboy
          type: user
        html: '<blockquote>

          <p>Have you played with batch size? Halving the batch size seems to help</p>

          </blockquote>

          <p>The batch size I run with is 1.</p>

          '
        raw: '> Have you played with batch size? Halving the batch size seems to help


          The batch size I run with is 1.'
        updatedAt: '2023-06-06T15:30:48.523Z'
      numEdits: 0
      reactions: []
    id: 647f51289c31024457a78c8d
    type: comment
  author: SpiketheCowboy
  content: '> Have you played with batch size? Halving the batch size seems to help


    The batch size I run with is 1.'
  created_at: 2023-06-06 14:30:48+00:00
  edited: false
  hidden: false
  id: 647f51289c31024457a78c8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd8e591d0a7827f741ec535ad26b355d.svg
      fullname: konze
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: konze
      type: user
    createdAt: '2023-06-07T11:33:50.000Z'
    data:
      edited: false
      editors:
      - konze
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8398865461349487
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd8e591d0a7827f741ec535ad26b355d.svg
          fullname: konze
          isHf: false
          isPro: false
          name: konze
          type: user
        html: '<p>anyone tell me minimum hardware requirements for the falcon-7b-instruct,
          i want use if for question answering with given context/Documents data.</p>

          '
        raw: anyone tell me minimum hardware requirements for the falcon-7b-instruct,
          i want use if for question answering with given context/Documents data.
        updatedAt: '2023-06-07T11:33:50.956Z'
      numEdits: 0
      reactions: []
    id: 64806b1ebb25a636c9d91d39
    type: comment
  author: konze
  content: anyone tell me minimum hardware requirements for the falcon-7b-instruct,
    i want use if for question answering with given context/Documents data.
  created_at: 2023-06-07 10:33:50+00:00
  edited: false
  hidden: false
  id: 64806b1ebb25a636c9d91d39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ffdc16df87b66666d789482c630d08ca.svg
      fullname: Philip Dakin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pdakin
      type: user
    createdAt: '2023-06-09T19:37:04.000Z'
    data:
      edited: false
      editors:
      - pdakin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6317436695098877
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ffdc16df87b66666d789482c630d08ca.svg
          fullname: Philip Dakin
          isHf: false
          isPro: false
          name: pdakin
          type: user
        html: "<p>On Colab with model card inference call:</p>\n<pre><code>  sequences\
          \ = pipeline(\n    \"Girafatron is obsessed with giraffes, the most glorious\
          \ animal on the face of this Earth. Giraftron believes all other animals\
          \ are irrelevant when compared to the glorious majesty of the giraffe.\\\
          nDaniel: Hello, Girafatron!\\nGirafatron:\",\n      max_length=200,\n  \
          \    do_sample=True,\n      top_k=10,\n      num_return_sequences=1,\n \
          \     eos_token_id=tokenizer.eos_token_id,\n  )\n</code></pre>\n<p>A100\
          \ - 6966.189ms<br>V100 - 44117.912ms</p>\n"
        raw: "On Colab with model card inference call:\n\n```\n  sequences = pipeline(\n\
          \    \"Girafatron is obsessed with giraffes, the most glorious animal on\
          \ the face of this Earth. Giraftron believes all other animals are irrelevant\
          \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello,\
          \ Girafatron!\\nGirafatron:\",\n      max_length=200,\n      do_sample=True,\n\
          \      top_k=10,\n      num_return_sequences=1,\n      eos_token_id=tokenizer.eos_token_id,\n\
          \  )\n```\n\nA100 - 6966.189ms\nV100 - 44117.912ms"
        updatedAt: '2023-06-09T19:37:04.843Z'
      numEdits: 0
      reactions: []
    id: 64837f60206a4ce1bc4134a7
    type: comment
  author: pdakin
  content: "On Colab with model card inference call:\n\n```\n  sequences = pipeline(\n\
    \    \"Girafatron is obsessed with giraffes, the most glorious animal on the face\
    \ of this Earth. Giraftron believes all other animals are irrelevant when compared\
    \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\"\
    ,\n      max_length=200,\n      do_sample=True,\n      top_k=10,\n      num_return_sequences=1,\n\
    \      eos_token_id=tokenizer.eos_token_id,\n  )\n```\n\nA100 - 6966.189ms\nV100\
    \ - 44117.912ms"
  created_at: 2023-06-09 18:37:04+00:00
  edited: false
  hidden: false
  id: 64837f60206a4ce1bc4134a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c91373a1b7e389ab8b340ce484d2d0f1.svg
      fullname: Sebastian Sergnese
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Seb83
      type: user
    createdAt: '2023-06-13T15:58:57.000Z'
    data:
      edited: false
      editors:
      - Seb83
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8972087502479553
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c91373a1b7e389ab8b340ce484d2d0f1.svg
          fullname: Sebastian Sergnese
          isHf: false
          isPro: false
          name: Seb83
          type: user
        html: "<blockquote>\n<p>I am trying to load this model on Colab, but it doesn't\
          \ load in the GPU.<br>What am I missing? I am using the code provided in\
          \ the model card and installing the transformers library. Still, model is\
          \ not loading in GPU.</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;Kamaljp&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Kamaljp\"\
          >@<span class=\"underline\">Kamaljp</span></a></span>\n\n\t</span></span>\
          \ Runtime (heading in top toolbar between insert and tools) -&gt; change\
          \ runtime type -&gt; select GPU under hardware accelerator</p>\n"
        raw: "> I am trying to load this model on Colab, but it doesn't load in the\
          \ GPU. \n> What am I missing? I am using the code provided in the model\
          \ card and installing the transformers library. Still, model is not loading\
          \ in GPU.\n\n@Kamaljp Runtime (heading in top toolbar between insert and\
          \ tools) -> change runtime type -> select GPU under hardware accelerator"
        updatedAt: '2023-06-13T15:58:57.621Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Kamaljp
    id: 648892415cf73a16e76cb33c
    type: comment
  author: Seb83
  content: "> I am trying to load this model on Colab, but it doesn't load in the\
    \ GPU. \n> What am I missing? I am using the code provided in the model card and\
    \ installing the transformers library. Still, model is not loading in GPU.\n\n\
    @Kamaljp Runtime (heading in top toolbar between insert and tools) -> change runtime\
    \ type -> select GPU under hardware accelerator"
  created_at: 2023-06-13 14:58:57+00:00
  edited: false
  hidden: false
  id: 648892415cf73a16e76cb33c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: tiiuae/falcon-7b
repo_type: model
status: closed
target_branch: null
title: Minimum requirements for inference
