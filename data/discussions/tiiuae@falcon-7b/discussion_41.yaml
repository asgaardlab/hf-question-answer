!!python/object:huggingface_hub.community.DiscussionWithDetails
author: medmac01
conflicting_files: null
created_at: 2023-06-20 14:48:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640603e2c3ab325efa94bc4a/jBLC7JH2dBAkDHYzFXZmr.jpeg?w=200&h=200&f=face
      fullname: Mohammed Machrouh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: medmac01
      type: user
    createdAt: '2023-06-20T15:48:07.000Z'
    data:
      edited: false
      editors:
      - medmac01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6065106391906738
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640603e2c3ab325efa94bc4a/jBLC7JH2dBAkDHYzFXZmr.jpeg?w=200&h=200&f=face
          fullname: Mohammed Machrouh
          isHf: false
          isPro: false
          name: medmac01
          type: user
        html: "<p>Hi there,<br>I successfully finetuned the falcon-7b and pushed it\
          \ to the hub. However, the HF inference API isn't working.. I get this error\
          \ \"Loading medmac01/moroccan-qa-falcon-7b requires you to execute the configuration\
          \ file in that repo on your local machine. Make sure you have read the code\
          \ there to avoid malicious use, then set the option <code>trust_remote_code=True</code>\
          \ to remove this error.\"</p>\n<p>I tried to load the model locally while\
          \ setting trust_remote_code=True, and it works normal in the notebook, but\
          \ i'm unable to get it running on the inference API.</p>\n<pre><code>import\
          \ torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nbnb_config\
          \ = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n\
          \    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16,\n\
          \    )\n\npeft_model_id = \"medmac01/moroccan-qa-falcon-7b\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n\
          model = AutoModelForCausalLM.from_pretrained(\n    pretrained_model_name_or_path\
          \ =  config.base_model_name_or_path,\n    return_dict=True, \n    load_in_8bit=True,\
          \ \n    device_map={\"\":0},\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n\
          )\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,trust_remote_code=True)\n\
          tokenizer.pad_token = tokenizer.eos_token\n\n# Load the Lora model\nmodel\
          \ = PeftModel.from_pretrained(model, peft_model_id)\n</code></pre>\n"
        raw: "Hi there,\r\nI successfully finetuned the falcon-7b and pushed it to\
          \ the hub. However, the HF inference API isn't working.. I get this error\
          \ \"Loading medmac01/moroccan-qa-falcon-7b requires you to execute the configuration\
          \ file in that repo on your local machine. Make sure you have read the code\
          \ there to avoid malicious use, then set the option `trust_remote_code=True`\
          \ to remove this error.\"\r\n\r\nI tried to load the model locally while\
          \ setting trust_remote_code=True, and it works normal in the notebook, but\
          \ i'm unable to get it running on the inference API.\r\n\r\n```\r\nimport\
          \ torch\r\nfrom peft import PeftModel, PeftConfig\r\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\r\n\r\nbnb_config\
          \ = BitsAndBytesConfig(\r\n    load_in_4bit=True,\r\n    bnb_4bit_use_double_quant=True,\r\
          \n    bnb_4bit_quant_type='nf4',\r\n    bnb_4bit_compute_dtype=torch.bfloat16,\r\
          \n    )\r\n\r\npeft_model_id = \"medmac01/moroccan-qa-falcon-7b\"\r\nconfig\
          \ = PeftConfig.from_pretrained(peft_model_id)\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\
          \n    pretrained_model_name_or_path =  config.base_model_name_or_path,\r\
          \n    return_dict=True, \r\n    load_in_8bit=True, \r\n    device_map={\"\
          \":0},\r\n    quantization_config=bnb_config,\r\n    trust_remote_code=True,\r\
          \n)\r\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,trust_remote_code=True)\r\
          \ntokenizer.pad_token = tokenizer.eos_token\r\n\r\n# Load the Lora model\r\
          \nmodel = PeftModel.from_pretrained(model, peft_model_id)\r\n```"
        updatedAt: '2023-06-20T15:48:07.754Z'
      numEdits: 0
      reactions: []
    id: 6491ca371e1343a1a4ce6be7
    type: comment
  author: medmac01
  content: "Hi there,\r\nI successfully finetuned the falcon-7b and pushed it to the\
    \ hub. However, the HF inference API isn't working.. I get this error \"Loading\
    \ medmac01/moroccan-qa-falcon-7b requires you to execute the configuration file\
    \ in that repo on your local machine. Make sure you have read the code there to\
    \ avoid malicious use, then set the option `trust_remote_code=True` to remove\
    \ this error.\"\r\n\r\nI tried to load the model locally while setting trust_remote_code=True,\
    \ and it works normal in the notebook, but i'm unable to get it running on the\
    \ inference API.\r\n\r\n```\r\nimport torch\r\nfrom peft import PeftModel, PeftConfig\r\
    \nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\r\
    \n\r\nbnb_config = BitsAndBytesConfig(\r\n    load_in_4bit=True,\r\n    bnb_4bit_use_double_quant=True,\r\
    \n    bnb_4bit_quant_type='nf4',\r\n    bnb_4bit_compute_dtype=torch.bfloat16,\r\
    \n    )\r\n\r\npeft_model_id = \"medmac01/moroccan-qa-falcon-7b\"\r\nconfig =\
    \ PeftConfig.from_pretrained(peft_model_id)\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\
    \n    pretrained_model_name_or_path =  config.base_model_name_or_path,\r\n   \
    \ return_dict=True, \r\n    load_in_8bit=True, \r\n    device_map={\"\":0},\r\n\
    \    quantization_config=bnb_config,\r\n    trust_remote_code=True,\r\n)\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(config.base_model_name_or_path,trust_remote_code=True)\r\
    \ntokenizer.pad_token = tokenizer.eos_token\r\n\r\n# Load the Lora model\r\nmodel\
    \ = PeftModel.from_pretrained(model, peft_model_id)\r\n```"
  created_at: 2023-06-20 14:48:07+00:00
  edited: false
  hidden: false
  id: 6491ca371e1343a1a4ce6be7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 41
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: Unable to get inference API working, trust_remote_code needs to be True
