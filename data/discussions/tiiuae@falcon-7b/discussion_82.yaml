!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vsrinivas
conflicting_files: null
created_at: 2023-09-23 12:12:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/939ae045d03d680fc31bbd61a6f8c963.svg
      fullname: Valmeti Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vsrinivas
      type: user
    createdAt: '2023-09-23T13:12:15.000Z'
    data:
      edited: true
      editors:
      - vsrinivas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9101588129997253
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/939ae045d03d680fc31bbd61a6f8c963.svg
          fullname: Valmeti Srinivas
          isHf: false
          isPro: false
          name: vsrinivas
          type: user
        html: '<p>Please note that I did not encounter the problems that are explained
          here, with many other LLM I tried. I am trying to host an app using this
          model (in fact I tried 40b and instruct models as well).  When the container
          is being built, it runs into some memory/ storage related issues related
          to HF Spaces free account. </p>

          <ol>

          <li><p>The first problem is you get the error -"ValueError: The current
          <code>device_map</code> had weights offloaded to the disk. Please provide
          an <code>offload_folder</code> for them. Alternatively, make sure you have
          <code>safetensors</code> installed if the model you are using offers the
          weights in this format."</p>

          </li>

          <li><p>So accordingly after installing ''safetensors'', I tried again. Still
          the problem persists. So, I assume Falcon models are not safetensors (hope
          someone can confirm). When I pass the ''offload_folder="offload"'' parameter
          to ''AutoModelForCausalLM.from_pretrained'', it does seem to be working
          but runs into a memory issue, shown below, while loading checkpoint shards.</p>

          </li>

          </ol>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62e24dd7003eac16191b203c/DvTifThNmvRIL4_y32nAN.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/62e24dd7003eac16191b203c/DvTifThNmvRIL4_y32nAN.png"></a></p>

          <ol start="3">

          <li>While performing the above step with 40B model, it actually runs out
          of 50G storage space limit.</li>

          </ol>

          <p>Appreciate if someone can help with some suggestions here.</p>

          '
        raw: "Please note that I did not encounter the problems that are explained\
          \ here, with many other LLM I tried. I am trying to host an app using this\
          \ model (in fact I tried 40b and instruct models as well).  When the container\
          \ is being built, it runs into some memory/ storage related issues related\
          \ to HF Spaces free account. \n\n1. The first problem is you get the error\
          \ -\"ValueError: The current `device_map` had weights offloaded to the disk.\
          \ Please provide an `offload_folder` for them. Alternatively, make sure\
          \ you have `safetensors` installed if the model you are using offers the\
          \ weights in this format.\"\n\n2. So accordingly after installing 'safetensors',\
          \ I tried again. Still the problem persists. So, I assume Falcon models\
          \ are not safetensors (hope someone can confirm). When I pass the 'offload_folder=\"\
          offload\"' parameter to 'AutoModelForCausalLM.from_pretrained', it does\
          \ seem to be working but runs into a memory issue, shown below, while loading\
          \ checkpoint shards.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62e24dd7003eac16191b203c/DvTifThNmvRIL4_y32nAN.png)\n\
          \n3. While performing the above step with 40B model, it actually runs out\
          \ of 50G storage space limit.\n\nAppreciate if someone can help with some\
          \ suggestions here."
        updatedAt: '2023-09-23T13:12:53.793Z'
      numEdits: 1
      reactions: []
    id: 650ee42fbfb7dd98bbc0515e
    type: comment
  author: vsrinivas
  content: "Please note that I did not encounter the problems that are explained here,\
    \ with many other LLM I tried. I am trying to host an app using this model (in\
    \ fact I tried 40b and instruct models as well).  When the container is being\
    \ built, it runs into some memory/ storage related issues related to HF Spaces\
    \ free account. \n\n1. The first problem is you get the error -\"ValueError: The\
    \ current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\
    \ for them. Alternatively, make sure you have `safetensors` installed if the model\
    \ you are using offers the weights in this format.\"\n\n2. So accordingly after\
    \ installing 'safetensors', I tried again. Still the problem persists. So, I assume\
    \ Falcon models are not safetensors (hope someone can confirm). When I pass the\
    \ 'offload_folder=\"offload\"' parameter to 'AutoModelForCausalLM.from_pretrained',\
    \ it does seem to be working but runs into a memory issue, shown below, while\
    \ loading checkpoint shards.\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62e24dd7003eac16191b203c/DvTifThNmvRIL4_y32nAN.png)\n\
    \n3. While performing the above step with 40B model, it actually runs out of 50G\
    \ storage space limit.\n\nAppreciate if someone can help with some suggestions\
    \ here."
  created_at: 2023-09-23 12:12:15+00:00
  edited: true
  hidden: false
  id: 650ee42fbfb7dd98bbc0515e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5a27854dc24ac7c4e3d2fea9923c71b.svg
      fullname: Acrobatix
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Acrobatix
      type: user
    createdAt: '2023-09-23T20:50:42.000Z'
    data:
      edited: false
      editors:
      - Acrobatix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6163108348846436
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5a27854dc24ac7c4e3d2fea9923c71b.svg
          fullname: Acrobatix
          isHf: false
          isPro: false
          name: Acrobatix
          type: user
        html: '<p>You might want to load the model this way:</p>

          <pre><code>model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto",
          offload_folder="offload")

          </code></pre>

          '
        raw: 'You might want to load the model this way:

          ```

          model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto",
          offload_folder="offload")

          ```'
        updatedAt: '2023-09-23T20:50:42.416Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - juan-abia
    id: 650f4fa211f3210cf7af7a68
    type: comment
  author: Acrobatix
  content: 'You might want to load the model this way:

    ```

    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", offload_folder="offload")

    ```'
  created_at: 2023-09-23 19:50:42+00:00
  edited: false
  hidden: false
  id: 650f4fa211f3210cf7af7a68
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/939ae045d03d680fc31bbd61a6f8c963.svg
      fullname: Valmeti Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vsrinivas
      type: user
    createdAt: '2023-09-24T03:00:32.000Z'
    data:
      edited: true
      editors:
      - vsrinivas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7180761694908142
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/939ae045d03d680fc31bbd61a6f8c963.svg
          fullname: Valmeti Srinivas
          isHf: false
          isPro: false
          name: vsrinivas
          type: user
        html: '<p>That is how I have loaded the model already. </p>

          <p><code>model = AutoModelForCausalLM.from_pretrained(     checkpoint, device_map="auto",
          offload_folder="offload", trust_remote_code=True,)</code></p>

          '
        raw: "That is how I have loaded the model already. \n```model = AutoModelForCausalLM.from_pretrained(\n\
          \    checkpoint, device_map=\"auto\", offload_folder=\"offload\", trust_remote_code=True,)```"
        updatedAt: '2023-09-24T03:02:39.035Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vinayaru
    id: 650fa6509060fe79010ff968
    type: comment
  author: vsrinivas
  content: "That is how I have loaded the model already. \n```model = AutoModelForCausalLM.from_pretrained(\n\
    \    checkpoint, device_map=\"auto\", offload_folder=\"offload\", trust_remote_code=True,)```"
  created_at: 2023-09-24 02:00:32+00:00
  edited: true
  hidden: false
  id: 650fa6509060fe79010ff968
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/939ae045d03d680fc31bbd61a6f8c963.svg
      fullname: Valmeti Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vsrinivas
      type: user
    createdAt: '2023-09-24T03:03:12.000Z'
    data:
      status: closed
    id: 650fa6f0f141bc34f930d731
    type: status-change
  author: vsrinivas
  created_at: 2023-09-24 02:03:12+00:00
  id: 650fa6f0f141bc34f930d731
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/939ae045d03d680fc31bbd61a6f8c963.svg
      fullname: Valmeti Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vsrinivas
      type: user
    createdAt: '2023-09-24T03:03:20.000Z'
    data:
      status: open
    id: 650fa6f8007ea5dd9d00f6d5
    type: status-change
  author: vsrinivas
  created_at: 2023-09-24 02:03:20+00:00
  id: 650fa6f8007ea5dd9d00f6d5
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8fa41b1249d74819265a696aed78310e.svg
      fullname: Vinay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vinayaru
      type: user
    createdAt: '2023-12-26T16:33:19.000Z'
    data:
      edited: false
      editors:
      - vinayaru
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9594259858131409
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8fa41b1249d74819265a696aed78310e.svg
          fullname: Vinay
          isHf: false
          isPro: false
          name: vinayaru
          type: user
        html: '<p>Any solution to this problem please?</p>

          '
        raw: Any solution to this problem please?
        updatedAt: '2023-12-26T16:33:19.864Z'
      numEdits: 0
      reactions: []
    id: 658b004f5c6fb5d5e30de694
    type: comment
  author: vinayaru
  content: Any solution to this problem please?
  created_at: 2023-12-26 16:33:19+00:00
  edited: false
  hidden: false
  id: 658b004f5c6fb5d5e30de694
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 82
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: How to avoid running into memory/ storage problems associated with HF Spaces
  while using tiiuae/falcon-7b 0r 40b etc.,
