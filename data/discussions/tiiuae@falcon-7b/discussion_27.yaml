!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xtliu
conflicting_files: null
created_at: 2023-06-09 21:22:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf3736091ca276bc43e48e26701b061d.svg
      fullname: Xingtong Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xtliu
      type: user
    createdAt: '2023-06-09T22:22:47.000Z'
    data:
      edited: false
      editors:
      - xtliu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.607729971408844
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf3736091ca276bc43e48e26701b061d.svg
          fullname: Xingtong Liu
          isHf: false
          isPro: false
          name: xtliu
          type: user
        html: "<p>For some reason, the class \"Attention\" module produces a shape\
          \ mismatch error when it goes to this branch as below. It complains that\
          \ self.num_heads is not equal to self.num_kv, which will happen because\
          \ the model config default value for num_kv is 1.</p>\n<pre><code>    if\
          \ alibi is None:\n        query_layer_ = query_layer.reshape(batch_size,\
          \ self.num_heads, -1, self.head_dim)\n        key_layer_ = key_layer.reshape(batch_size,\
          \ self.num_kv, -1, self.head_dim)\n        value_layer_ = value_layer.reshape(batch_size,\
          \ self.num_kv, -1, self.head_dim).expand(-1, self.num_heads, -1, -1)\n \
          \        attn_output = F.scaled_dot_product_attention(\n            query_layer_,\
          \ key_layer_, value_layer_, None, 0.0, is_causal=True\n        )\n</code></pre>\n"
        raw: "For some reason, the class \"Attention\" module produces a shape mismatch\
          \ error when it goes to this branch as below. It complains that self.num_heads\
          \ is not equal to self.num_kv, which will happen because the model config\
          \ default value for num_kv is 1.\r\n\r\n\r\n        if alibi is None:\r\n\
          \            query_layer_ = query_layer.reshape(batch_size, self.num_heads,\
          \ -1, self.head_dim)\r\n            key_layer_ = key_layer.reshape(batch_size,\
          \ self.num_kv, -1, self.head_dim)\r\n            value_layer_ = value_layer.reshape(batch_size,\
          \ self.num_kv, -1, self.head_dim).expand(-1, self.num_heads, -1, -1)\r\n\
          \             attn_output = F.scaled_dot_product_attention(\r\n        \
          \        query_layer_, key_layer_, value_layer_, None, 0.0, is_causal=True\r\
          \n            )\r\n"
        updatedAt: '2023-06-09T22:22:47.754Z'
      numEdits: 0
      reactions: []
    id: 6483a6373df9c1d2100129c9
    type: comment
  author: xtliu
  content: "For some reason, the class \"Attention\" module produces a shape mismatch\
    \ error when it goes to this branch as below. It complains that self.num_heads\
    \ is not equal to self.num_kv, which will happen because the model config default\
    \ value for num_kv is 1.\r\n\r\n\r\n        if alibi is None:\r\n            query_layer_\
    \ = query_layer.reshape(batch_size, self.num_heads, -1, self.head_dim)\r\n   \
    \         key_layer_ = key_layer.reshape(batch_size, self.num_kv, -1, self.head_dim)\r\
    \n            value_layer_ = value_layer.reshape(batch_size, self.num_kv, -1,\
    \ self.head_dim).expand(-1, self.num_heads, -1, -1)\r\n             attn_output\
    \ = F.scaled_dot_product_attention(\r\n                query_layer_, key_layer_,\
    \ value_layer_, None, 0.0, is_causal=True\r\n            )\r\n"
  created_at: 2023-06-09 21:22:47+00:00
  edited: false
  hidden: false
  id: 6483a6373df9c1d2100129c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ceac8a9164879932a001d18bc0d070e.svg
      fullname: vivi c
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vivixzc
      type: user
    createdAt: '2023-07-26T05:53:55.000Z'
    data:
      edited: false
      editors:
      - vivixzc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9923530220985413
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ceac8a9164879932a001d18bc0d070e.svg
          fullname: vivi c
          isHf: false
          isPro: false
          name: vivixzc
          type: user
        html: '<p>I got the same issue, did you find a way to solve this?</p>

          '
        raw: I got the same issue, did you find a way to solve this?
        updatedAt: '2023-07-26T05:53:55.609Z'
      numEdits: 0
      reactions: []
    id: 64c0b4f3e175dd56a57019ac
    type: comment
  author: vivixzc
  content: I got the same issue, did you find a way to solve this?
  created_at: 2023-07-26 04:53:55+00:00
  edited: false
  hidden: false
  id: 64c0b4f3e175dd56a57019ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf3736091ca276bc43e48e26701b061d.svg
      fullname: Xingtong Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xtliu
      type: user
    createdAt: '2023-07-27T17:19:22.000Z'
    data:
      edited: false
      editors:
      - xtliu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.886908233165741
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf3736091ca276bc43e48e26701b061d.svg
          fullname: Xingtong Liu
          isHf: false
          isPro: false
          name: xtliu
          type: user
        html: '<p>I just use expand operation in the code piece above to make it the
          same shape</p>

          '
        raw: 'I just use expand operation in the code piece above to make it the same
          shape

          '
        updatedAt: '2023-07-27T17:19:22.199Z'
      numEdits: 0
      reactions: []
    id: 64c2a71a3578a3ba8192ee05
    type: comment
  author: xtliu
  content: 'I just use expand operation in the code piece above to make it the same
    shape

    '
  created_at: 2023-07-27 16:19:22+00:00
  edited: false
  hidden: false
  id: 64c2a71a3578a3ba8192ee05
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: Shape mismatch error for F.scaled_dot_product_attention in modelling_RW.py
