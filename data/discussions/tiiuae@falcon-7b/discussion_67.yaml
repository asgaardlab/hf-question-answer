!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Akash1267a
conflicting_files: null
created_at: 2023-07-13 12:37:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1d8620f52cc1d841731185a509319377.svg
      fullname: Akash Mhaisdhune
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Akash1267a
      type: user
    createdAt: '2023-07-13T13:37:56.000Z'
    data:
      edited: false
      editors:
      - Akash1267a
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.16586805880069733
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1d8620f52cc1d841731185a509319377.svg
          fullname: Akash Mhaisdhune
          isHf: false
          isPro: false
          name: Akash1267a
          type: user
        html: "<p>from transformers import AutoModelForCausalLM, AutoTokenizer</p>\n\
          <p>model_path = \"D:\\Program\\Falcon-7b\"<br>tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ trust_remote_code=True)<br>model = AutoModelForCausalLM.from_pretrained(model_path,\
          \ trust_remote_code=True, from_tf=True, revision=\"main\")</p>\n<p>template\
          \ = \"Question: {question}\"<br>prompt = PromptTemplate(template=template,\
          \ input_variables=[\"question\"])<br>llm_chain = LLMChain(prompt=prompt,\
          \ llm=model)</p>\n<p>question = \"What is the meaning of life?\"<br>output\
          \ = llm_chain.run(question)<br>print(output)</p>\n<hr>\n<p>\u2502 d:\\Program\\\
          zcode\\test4.py:8 in                                                   \
          \        \u2502<br>\u2502                                              \
          \                                                    \u2502<br>\u2502  \
          \  5                                                                   \
          \                          \u2502<br>\u2502    6 model_path = \"D:\\Program\\\
          Falcon-7b\"                                                         \u2502\
          <br>\u2502    7 tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\
          \               \u2502<br>\u2502 \u2771  8 model = AutoModelForCausalLM.from_pretrained(model_path,\
          \ trust_remote_code=True, from_tf    \u2502<br>\u2502    9             \
          \                                                                      \
          \          \u2502<br>\u2502   10 template = \"Question: {question}\"   \
          \                                                        \u2502<br>\u2502\
          \   11 prompt = PromptTemplate(template=template, input_variables=[\"question\"\
          ])                    \u2502<br>\u2502                                 \
          \                                                                 \u2502\
          <br>\u2502 D:\\Program\\venv\\Lib\\site-packages\\transformers\\models\\\
          auto\\auto_factory.py:466 in                \u2502<br>\u2502 from_pretrained\
          \                                                                      \
          \            \u2502<br>\u2502                                          \
          \                                                        \u2502<br>\u2502\
          \   463 \u2502   \u2502   \u2502   \u2502   pretrained_model_name_or_path,\
          \ module_file + \".py\", class_name, **hub_kw   \u2502<br>\u2502   464 \u2502\
          \   \u2502   \u2502   )                                                \
          \                              \u2502<br>\u2502   465 \u2502   \u2502  \
          \ \u2502   model_class.register_for_auto_class(cls.<strong>name</strong>)\
          \                              \u2502<br>\u2502 \u2771 466 \u2502   \u2502\
          \   \u2502   return model_class.from_pretrained(                       \
          \                     \u2502<br>\u2502   467 \u2502   \u2502   \u2502  \
          \ \u2502   pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
          \   \u2502<br>\u2502   468 \u2502   \u2502   \u2502   )                \
          \                                                              \u2502<br>\u2502\
          \   469 \u2502   \u2502   elif type(config) in cls._model_mapping.keys():\
          \                                    \u2502<br>\u2502                  \
          \                                                                      \
          \          \u2502<br>\u2502 D:\\Program\\venv\\Lib\\site-packages\\transformers\\\
          modeling_utils.py:2753 in from_pretrained         \u2502<br>\u2502     \
          \                                                                      \
          \                       \u2502<br>\u2502   2750 \u2502   \u2502   \u2502\
          \   \u2502   del device_map_without_lm_head                            \
          \                \u2502<br>\u2502   2751 \u2502   \u2502               \
          \                                                                      \u2502\
          <br>\u2502   2752 \u2502   \u2502   if from_tf:                        \
          \                                               \u2502<br>\u2502 \u2771\
          \ 2753 \u2502   \u2502   \u2502   if resolved_archive_file.endswith(\".index\"\
          ):                                  \u2502<br>\u2502   2754 \u2502   \u2502\
          \   \u2502   \u2502   # Load from a TensorFlow 1.X checkpoint - provided\
          \ by original authors    \u2502<br>\u2502   2755 \u2502   \u2502   \u2502\
          \   \u2502   model = cls.load_tf_weights(model, config, resolved_archive_file[:-6])\
          \    \u2502<br>\u2502   2756 \u2502   \u2502   \u2502   else:          \
          \                                                               \u2502<br>\u2570\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256F<br>AttributeError: 'list' object has no attribute 'endswith'</p>\n"
        raw: "from transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\n\
          model_path = \"D:\\Program\\Falcon-7b\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
          \ trust_remote_code=True, from_tf=True, revision=\"main\")\r\n\r\ntemplate\
          \ = \"Question: {question}\"\r\nprompt = PromptTemplate(template=template,\
          \ input_variables=[\"question\"])\r\nllm_chain = LLMChain(prompt=prompt,\
          \ llm=model)\r\n\r\nquestion = \"What is the meaning of life?\"\r\noutput\
          \ = llm_chain.run(question)\r\nprint(output)\r\n\r\n----------------------------------------------------------------------------------------------------------------------\r\
          \n\u2502 d:\\Program\\zcode\\test4.py:8 in <module>                    \
          \                                      \u2502\r\n\u2502                \
          \                                                                      \
          \            \u2502\r\n\u2502    5                                     \
          \                                                        \u2502\r\n\u2502\
          \    6 model_path = \"D:\\Program\\Falcon-7b\"                         \
          \                                \u2502\r\n\u2502    7 tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ trust_remote_code=True)               \u2502\r\n\u2502 \u2771  8 model\
          \ = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,\
          \ from_tf    \u2502\r\n\u2502    9                                     \
          \                                                        \u2502\r\n\u2502\
          \   10 template = \"Question: {question}\"                             \
          \                              \u2502\r\n\u2502   11 prompt = PromptTemplate(template=template,\
          \ input_variables=[\"question\"])                    \u2502\r\n\u2502  \
          \                                                                      \
          \                          \u2502\r\n\u2502 D:\\Program\\venv\\Lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py:466 in                \u2502\
          \r\n\u2502 from_pretrained                                             \
          \                                     \u2502\r\n\u2502                 \
          \                                                                      \
          \           \u2502\r\n\u2502   463 \u2502   \u2502   \u2502   \u2502   pretrained_model_name_or_path,\
          \ module_file + \".py\", class_name, **hub_kw   \u2502\r\n\u2502   464 \u2502\
          \   \u2502   \u2502   )                                                \
          \                              \u2502\r\n\u2502   465 \u2502   \u2502  \
          \ \u2502   model_class.register_for_auto_class(cls.__name__)           \
          \                   \u2502\r\n\u2502 \u2771 466 \u2502   \u2502   \u2502\
          \   return model_class.from_pretrained(                                \
          \            \u2502\r\n\u2502   467 \u2502   \u2502   \u2502   \u2502  \
          \ pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
          \   \u2502\r\n\u2502   468 \u2502   \u2502   \u2502   )                \
          \                                                              \u2502\r\n\
          \u2502   469 \u2502   \u2502   elif type(config) in cls._model_mapping.keys():\
          \                                    \u2502\r\n\u2502                  \
          \                                                                      \
          \          \u2502\r\n\u2502 D:\\Program\\venv\\Lib\\site-packages\\transformers\\\
          modeling_utils.py:2753 in from_pretrained         \u2502\r\n\u2502     \
          \                                                                      \
          \                       \u2502\r\n\u2502   2750 \u2502   \u2502   \u2502\
          \   \u2502   del device_map_without_lm_head                            \
          \                \u2502\r\n\u2502   2751 \u2502   \u2502               \
          \                                                                      \u2502\
          \r\n\u2502   2752 \u2502   \u2502   if from_tf:                        \
          \                                               \u2502\r\n\u2502 \u2771\
          \ 2753 \u2502   \u2502   \u2502   if resolved_archive_file.endswith(\".index\"\
          ):                                  \u2502\r\n\u2502   2754 \u2502   \u2502\
          \   \u2502   \u2502   # Load from a TensorFlow 1.X checkpoint - provided\
          \ by original authors    \u2502\r\n\u2502   2755 \u2502   \u2502   \u2502\
          \   \u2502   model = cls.load_tf_weights(model, config, resolved_archive_file[:-6])\
          \    \u2502\r\n\u2502   2756 \u2502   \u2502   \u2502   else:          \
          \                                                               \u2502\r\
          \n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u256F\r\nAttributeError: 'list' object has no attribute\
          \ 'endswith'"
        updatedAt: '2023-07-13T13:37:56.340Z'
      numEdits: 0
      reactions: []
    id: 64affe3452349201f96ee10c
    type: comment
  author: Akash1267a
  content: "from transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel_path\
    \ = \"D:\\Program\\Falcon-7b\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_path,\
    \ trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
    \ trust_remote_code=True, from_tf=True, revision=\"main\")\r\n\r\ntemplate = \"\
    Question: {question}\"\r\nprompt = PromptTemplate(template=template, input_variables=[\"\
    question\"])\r\nllm_chain = LLMChain(prompt=prompt, llm=model)\r\n\r\nquestion\
    \ = \"What is the meaning of life?\"\r\noutput = llm_chain.run(question)\r\nprint(output)\r\
    \n\r\n----------------------------------------------------------------------------------------------------------------------\r\
    \n\u2502 d:\\Program\\zcode\\test4.py:8 in <module>                          \
    \                                \u2502\r\n\u2502                            \
    \                                                                      \u2502\r\
    \n\u2502    5                                                                \
    \                             \u2502\r\n\u2502    6 model_path = \"D:\\Program\\\
    Falcon-7b\"                                                         \u2502\r\n\
    \u2502    7 tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\
    \               \u2502\r\n\u2502 \u2771  8 model = AutoModelForCausalLM.from_pretrained(model_path,\
    \ trust_remote_code=True, from_tf    \u2502\r\n\u2502    9                   \
    \                                                                          \u2502\
    \r\n\u2502   10 template = \"Question: {question}\"                          \
    \                                 \u2502\r\n\u2502   11 prompt = PromptTemplate(template=template,\
    \ input_variables=[\"question\"])                    \u2502\r\n\u2502        \
    \                                                                            \
    \              \u2502\r\n\u2502 D:\\Program\\venv\\Lib\\site-packages\\transformers\\\
    models\\auto\\auto_factory.py:466 in                \u2502\r\n\u2502 from_pretrained\
    \                                                                            \
    \      \u2502\r\n\u2502                                                      \
    \                                            \u2502\r\n\u2502   463 \u2502   \u2502\
    \   \u2502   \u2502   pretrained_model_name_or_path, module_file + \".py\", class_name,\
    \ **hub_kw   \u2502\r\n\u2502   464 \u2502   \u2502   \u2502   )             \
    \                                                                 \u2502\r\n\u2502\
    \   465 \u2502   \u2502   \u2502   model_class.register_for_auto_class(cls.__name__)\
    \                              \u2502\r\n\u2502 \u2771 466 \u2502   \u2502   \u2502\
    \   return model_class.from_pretrained(                                      \
    \      \u2502\r\n\u2502   467 \u2502   \u2502   \u2502   \u2502   pretrained_model_name_or_path,\
    \ *model_args, config=config, **hub_kwargs,   \u2502\r\n\u2502   468 \u2502  \
    \ \u2502   \u2502   )                                                        \
    \                      \u2502\r\n\u2502   469 \u2502   \u2502   elif type(config)\
    \ in cls._model_mapping.keys():                                    \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502 D:\\Program\\venv\\Lib\\site-packages\\\
    transformers\\modeling_utils.py:2753 in from_pretrained         \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502   2750 \u2502   \u2502   \u2502   \u2502\
    \   del device_map_without_lm_head                                           \
    \ \u2502\r\n\u2502   2751 \u2502   \u2502                                    \
    \                                                 \u2502\r\n\u2502   2752 \u2502\
    \   \u2502   if from_tf:                                                     \
    \                  \u2502\r\n\u2502 \u2771 2753 \u2502   \u2502   \u2502   if\
    \ resolved_archive_file.endswith(\".index\"):                                \
    \  \u2502\r\n\u2502   2754 \u2502   \u2502   \u2502   \u2502   # Load from a TensorFlow\
    \ 1.X checkpoint - provided by original authors    \u2502\r\n\u2502   2755 \u2502\
    \   \u2502   \u2502   \u2502   model = cls.load_tf_weights(model, config, resolved_archive_file[:-6])\
    \    \u2502\r\n\u2502   2756 \u2502   \u2502   \u2502   else:                \
    \                                                         \u2502\r\n\u2570\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nAttributeError: 'list' object has\
    \ no attribute 'endswith'"
  created_at: 2023-07-13 12:37:56+00:00
  edited: false
  hidden: false
  id: 64affe3452349201f96ee10c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/SdIbbDIuuSbHR46ULmFuf.png?w=200&h=200&f=face
      fullname: Adam Englander
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adam-zettafi
      type: user
    createdAt: '2023-07-14T20:35:31.000Z'
    data:
      edited: false
      editors:
      - adam-zettafi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9071676731109619
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/SdIbbDIuuSbHR46ULmFuf.png?w=200&h=200&f=face
          fullname: Adam Englander
          isHf: false
          isPro: false
          name: adam-zettafi
          type: user
        html: '<p>Are you attempting to open a tensor flow version of the model? The
          error leads me to believe you are not. Try removing <code>from_tf=True</code>.</p>

          '
        raw: Are you attempting to open a tensor flow version of the model? The error
          leads me to believe you are not. Try removing `from_tf=True`.
        updatedAt: '2023-07-14T20:35:31.459Z'
      numEdits: 0
      reactions: []
    id: 64b1b193094a60c78bd831cd
    type: comment
  author: adam-zettafi
  content: Are you attempting to open a tensor flow version of the model? The error
    leads me to believe you are not. Try removing `from_tf=True`.
  created_at: 2023-07-14 19:35:31+00:00
  edited: false
  hidden: false
  id: 64b1b193094a60c78bd831cd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 67
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: 'getting error '
