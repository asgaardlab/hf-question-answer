!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jashsayani
conflicting_files: null
created_at: 2023-11-06 04:40:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64784fb2c43296134a43135d/ZQYeP21wlw_onpn5UPbHu.jpeg?w=200&h=200&f=face
      fullname: Jash Sayani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jashsayani
      type: user
    createdAt: '2023-11-06T04:40:19.000Z'
    data:
      edited: false
      editors:
      - jashsayani
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9023755788803101
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64784fb2c43296134a43135d/ZQYeP21wlw_onpn5UPbHu.jpeg?w=200&h=200&f=face
          fullname: Jash Sayani
          isHf: false
          isPro: false
          name: jashsayani
          type: user
        html: "<p>I am trying out this Falcon-7b model on my M1 Pro mac and noticed\
          \ that it does not stop generating text. It keeps going till it reaches\
          \ max_length. Is this an issue with the model? or some config that I am\
          \ not setting correctly?</p>\n<p>My pipeline (shows it generated the response\
          \ twice; also includes prompt question in response but I think thats just\
          \ how its going to generate response):<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64784fb2c43296134a43135d/uDtSlpC45g9jD81hSYjgr.png\"\
          ><img alt=\"Screenshot 2023-11-05 at 8.34.05\u202FPM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64784fb2c43296134a43135d/uDtSlpC45g9jD81hSYjgr.png\"\
          ></a></p>\n"
        raw: "I am trying out this Falcon-7b model on my M1 Pro mac and noticed that\
          \ it does not stop generating text. It keeps going till it reaches max_length.\
          \ Is this an issue with the model? or some config that I am not setting\
          \ correctly?\r\n\r\nMy pipeline (shows it generated the response twice;\
          \ also includes prompt question in response but I think thats just how its\
          \ going to generate response):\r\n![Screenshot 2023-11-05 at 8.34.05\u202F\
          PM.png](https://cdn-uploads.huggingface.co/production/uploads/64784fb2c43296134a43135d/uDtSlpC45g9jD81hSYjgr.png)\r\
          \n"
        updatedAt: '2023-11-06T04:40:19.573Z'
      numEdits: 0
      reactions: []
    id: 65486e33b3a7efb93912c267
    type: comment
  author: jashsayani
  content: "I am trying out this Falcon-7b model on my M1 Pro mac and noticed that\
    \ it does not stop generating text. It keeps going till it reaches max_length.\
    \ Is this an issue with the model? or some config that I am not setting correctly?\r\
    \n\r\nMy pipeline (shows it generated the response twice; also includes prompt\
    \ question in response but I think thats just how its going to generate response):\r\
    \n![Screenshot 2023-11-05 at 8.34.05\u202FPM.png](https://cdn-uploads.huggingface.co/production/uploads/64784fb2c43296134a43135d/uDtSlpC45g9jD81hSYjgr.png)\r\
    \n"
  created_at: 2023-11-06 04:40:19+00:00
  edited: false
  hidden: false
  id: 65486e33b3a7efb93912c267
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 93
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: Model does not know when to stop generating text?
