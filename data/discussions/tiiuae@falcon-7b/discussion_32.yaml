!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rbgreenway
conflicting_files: null
created_at: 2023-06-13 16:21:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ff983b88f31127d538affca08e16916.svg
      fullname: Bryan Greenway
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rbgreenway
      type: user
    createdAt: '2023-06-13T17:21:04.000Z'
    data:
      edited: false
      editors:
      - rbgreenway
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9264384508132935
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ff983b88f31127d538affca08e16916.svg
          fullname: Bryan Greenway
          isHf: false
          isPro: false
          name: rbgreenway
          type: user
        html: '<p>I''m very much still learning my way around LLMs, but I see where
          some LLMs (e.g. CustomGPT, IngestAI) allow you to fine tune simply by providing
          raw, unlabeled text.  For example, providing training manuals for operating
          some complex system, supposedly can be used to fine tune an LLM around giving
          advice related to operating that system.  Maybe I have the training manuals
          for flying a small airplane... :-) </p>

          <p>Question: Can falcon-7b and/or 40b be fine tuned using this kind of raw,
          unlabeled data?  If so, where can I find instructions or an example of doing
          so?</p>

          <p>I apologize for the rudimentary question.<br>Thanks in advance for any
          suggestions that anyone may have.</p>

          '
        raw: "I'm very much still learning my way around LLMs, but I see where some\
          \ LLMs (e.g. CustomGPT, IngestAI) allow you to fine tune simply by providing\
          \ raw, unlabeled text.  For example, providing training manuals for operating\
          \ some complex system, supposedly can be used to fine tune an LLM around\
          \ giving advice related to operating that system.  Maybe I have the training\
          \ manuals for flying a small airplane... :-) \r\n\r\nQuestion: Can falcon-7b\
          \ and/or 40b be fine tuned using this kind of raw, unlabeled data?  If so,\
          \ where can I find instructions or an example of doing so?\r\n\r\nI apologize\
          \ for the rudimentary question.\r\nThanks in advance for any suggestions\
          \ that anyone may have."
        updatedAt: '2023-06-13T17:21:04.233Z'
      numEdits: 0
      reactions: []
    id: 6488a5809d6109a4dd05e952
    type: comment
  author: rbgreenway
  content: "I'm very much still learning my way around LLMs, but I see where some\
    \ LLMs (e.g. CustomGPT, IngestAI) allow you to fine tune simply by providing raw,\
    \ unlabeled text.  For example, providing training manuals for operating some\
    \ complex system, supposedly can be used to fine tune an LLM around giving advice\
    \ related to operating that system.  Maybe I have the training manuals for flying\
    \ a small airplane... :-) \r\n\r\nQuestion: Can falcon-7b and/or 40b be fine tuned\
    \ using this kind of raw, unlabeled data?  If so, where can I find instructions\
    \ or an example of doing so?\r\n\r\nI apologize for the rudimentary question.\r\
    \nThanks in advance for any suggestions that anyone may have."
  created_at: 2023-06-13 16:21:04+00:00
  edited: false
  hidden: false
  id: 6488a5809d6109a4dd05e952
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ecae328e126a3cb7ee854c00d66a0aef.svg
      fullname: Shubham Dawande
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: morpheus73
      type: user
    createdAt: '2023-06-15T08:01:50.000Z'
    data:
      edited: false
      editors:
      - morpheus73
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8984178900718689
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ecae328e126a3cb7ee854c00d66a0aef.svg
          fullname: Shubham Dawande
          isHf: false
          isPro: false
          name: morpheus73
          type: user
        html: '<p>I think we can fine-tune the model using masked/causal language
          modelling techniques given unlabelled text. Here the labels would be the
          next word token or masked tokens. I am trying to do the same for one of
          the pretrained models but haven''t gotten sufficient enough memorisation
          of training data. So not sure how well it works.</p>

          '
        raw: I think we can fine-tune the model using masked/causal language modelling
          techniques given unlabelled text. Here the labels would be the next word
          token or masked tokens. I am trying to do the same for one of the pretrained
          models but haven't gotten sufficient enough memorisation of training data.
          So not sure how well it works.
        updatedAt: '2023-06-15T08:01:50.106Z'
      numEdits: 0
      reactions: []
    id: 648ac56e308bc55017f78f1d
    type: comment
  author: morpheus73
  content: I think we can fine-tune the model using masked/causal language modelling
    techniques given unlabelled text. Here the labels would be the next word token
    or masked tokens. I am trying to do the same for one of the pretrained models
    but haven't gotten sufficient enough memorisation of training data. So not sure
    how well it works.
  created_at: 2023-06-15 07:01:50+00:00
  edited: false
  hidden: false
  id: 648ac56e308bc55017f78f1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3890822b1e742130e8b414d4d642616.svg
      fullname: Michael Mollel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mollel
      type: user
    createdAt: '2023-06-21T21:44:16.000Z'
    data:
      edited: false
      editors:
      - Mollel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8248088359832764
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3890822b1e742130e8b414d4d642616.svg
          fullname: Michael Mollel
          isHf: false
          isPro: false
          name: Mollel
          type: user
        html: '<p>Any solution?</p>

          '
        raw: 'Any solution?

          '
        updatedAt: '2023-06-21T21:44:16.208Z'
      numEdits: 0
      reactions: []
    id: 64936f3083f6d9c70851f701
    type: comment
  author: Mollel
  content: 'Any solution?

    '
  created_at: 2023-06-21 20:44:16+00:00
  edited: false
  hidden: false
  id: 64936f3083f6d9c70851f701
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ff983b88f31127d538affca08e16916.svg
      fullname: Bryan Greenway
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rbgreenway
      type: user
    createdAt: '2023-06-21T22:40:56.000Z'
    data:
      edited: false
      editors:
      - rbgreenway
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.972658634185791
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ff983b88f31127d538affca08e16916.svg
          fullname: Bryan Greenway
          isHf: false
          isPro: false
          name: rbgreenway
          type: user
        html: '<blockquote>

          <p>Any solution?</p>

          </blockquote>

          <p>Not by me.  I wonder if I''ve got a square peg/round hole problem here.  Everywhere
          I look, fine tuning on these LLMs is done with labeled data.  I''m very
          new to all this, so I''m not sure if I''m even asking good questions.  I''ve
          got another potential application for falcon that I''m banging my head on,
          too.  This stuff is incredible, but not for the faint at heart.</p>

          '
        raw: '> Any solution?


          Not by me.  I wonder if I''ve got a square peg/round hole problem here.  Everywhere
          I look, fine tuning on these LLMs is done with labeled data.  I''m very
          new to all this, so I''m not sure if I''m even asking good questions.  I''ve
          got another potential application for falcon that I''m banging my head on,
          too.  This stuff is incredible, but not for the faint at heart.

          '
        updatedAt: '2023-06-21T22:40:56.416Z'
      numEdits: 0
      reactions: []
    id: 64937c788ef138d2bc76666d
    type: comment
  author: rbgreenway
  content: '> Any solution?


    Not by me.  I wonder if I''ve got a square peg/round hole problem here.  Everywhere
    I look, fine tuning on these LLMs is done with labeled data.  I''m very new to
    all this, so I''m not sure if I''m even asking good questions.  I''ve got another
    potential application for falcon that I''m banging my head on, too.  This stuff
    is incredible, but not for the faint at heart.

    '
  created_at: 2023-06-21 21:40:56+00:00
  edited: false
  hidden: false
  id: 64937c788ef138d2bc76666d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3890822b1e742130e8b414d4d642616.svg
      fullname: Michael Mollel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mollel
      type: user
    createdAt: '2023-06-22T11:27:13.000Z'
    data:
      edited: false
      editors:
      - Mollel
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9126014113426208
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3890822b1e742130e8b414d4d642616.svg
          fullname: Michael Mollel
          isHf: false
          isPro: false
          name: Mollel
          type: user
        html: '<p>So far, the only solution I have found is for older models like
          mT5 and BERT. You can find more information and examples in the following
          link:</p>

          <p><a rel="nofollow" href="https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/README.md">GitHub
          - Hugging Face Transformers Examples</a></p>

          <p>If anyone has a link for unsupervised training specifically for Falcon,
          please let us know. I would greatly appreciate it!</p>

          '
        raw: 'So far, the only solution I have found is for older models like mT5
          and BERT. You can find more information and examples in the following link:


          [GitHub - Hugging Face Transformers Examples](https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/README.md)


          If anyone has a link for unsupervised training specifically for Falcon,
          please let us know. I would greatly appreciate it!'
        updatedAt: '2023-06-22T11:27:13.478Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - rbgreenway
    id: 649430114b687633b5676335
    type: comment
  author: Mollel
  content: 'So far, the only solution I have found is for older models like mT5 and
    BERT. You can find more information and examples in the following link:


    [GitHub - Hugging Face Transformers Examples](https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/README.md)


    If anyone has a link for unsupervised training specifically for Falcon, please
    let us know. I would greatly appreciate it!'
  created_at: 2023-06-22 10:27:13+00:00
  edited: false
  hidden: false
  id: 649430114b687633b5676335
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 32
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: Unsupervised learning with unlabled data
