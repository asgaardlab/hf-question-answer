!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hadifar
conflicting_files: null
created_at: 2023-07-13 15:09:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/74c722820ae9019997a0d1d6d29962ec.svg
      fullname: Amir Hadifar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hadifar
      type: user
    createdAt: '2023-07-13T16:09:30.000Z'
    data:
      edited: true
      editors:
      - hadifar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4592481255531311
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/74c722820ae9019997a0d1d6d29962ec.svg
          fullname: Amir Hadifar
          isHf: false
          isPro: false
          name: hadifar
          type: user
        html: "<p>When I try to run <code>falcon-7b</code> it throws the following\
          \ error:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\
          import transformers\nimport torch\n\nmodel = \"tiiuae/falcon-7b\"\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"\
          auto\",\n)\n</code></pre>\n<p>Is it related to the Cuda version (my Cuda\
          \ version is 12.1)? The code runs perfectly on google-collab.</p>\n<pre><code>raise\
          \ HFValidationError(\nhuggingface_hub.utils._validators.HFValidationError:\
          \ Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\
          \ forbidden, '-' and '.' cannot start or end the name, max length is 96:\
          \ 'RWForCausalLM(\n  (transformer): RWModel(\n    (word_embeddings): Embedding(65024,\
          \ 4544)\n    (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n      \
          \  (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n\
          \        (self_attention): Attention(\n          (maybe_rotary): RotaryEmbedding()\n\
          \          (query_key_value): Linear(in_features=4544, out_features=4672,\
          \ bias=False)\n          (dense): Linear(in_features=4544, out_features=4544,\
          \ bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n\
          \        )\n        (mlp): MLP(\n          (dense_h_to_4h): Linear(in_features=4544,\
          \ out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n\
          \          (dense_4h_to_h): Linear(in_features=18176, out_features=4544,\
          \ bias=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4544,\
          \ out_features=65024, bias=False)\n)'.\n</code></pre>\n"
        raw: "When I try to run `falcon-7b` it throws the following error:\n\n```\n\
          from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
          import torch\n\nmodel = \"tiiuae/falcon-7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
          \    device_map=\"auto\",\n)\n```\n\nIs it related to the Cuda version (my\
          \ Cuda version is 12.1)? The code runs perfectly on google-collab.\n   \n\
          \n```\nraise HFValidationError(\nhuggingface_hub.utils._validators.HFValidationError:\
          \ Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\
          \ forbidden, '-' and '.' cannot start or end the name, max length is 96:\
          \ 'RWForCausalLM(\n  (transformer): RWModel(\n    (word_embeddings): Embedding(65024,\
          \ 4544)\n    (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n      \
          \  (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n\
          \        (self_attention): Attention(\n          (maybe_rotary): RotaryEmbedding()\n\
          \          (query_key_value): Linear(in_features=4544, out_features=4672,\
          \ bias=False)\n          (dense): Linear(in_features=4544, out_features=4544,\
          \ bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n\
          \        )\n        (mlp): MLP(\n          (dense_h_to_4h): Linear(in_features=4544,\
          \ out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n\
          \          (dense_4h_to_h): Linear(in_features=18176, out_features=4544,\
          \ bias=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4544,\
          \ out_features=65024, bias=False)\n)'.\n```"
        updatedAt: '2023-07-13T16:11:38.440Z'
      numEdits: 2
      reactions: []
    id: 64b021ba919714d7dcd079de
    type: comment
  author: hadifar
  content: "When I try to run `falcon-7b` it throws the following error:\n\n```\n\
    from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
    import torch\n\nmodel = \"tiiuae/falcon-7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
    pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
    \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n\
    \    device_map=\"auto\",\n)\n```\n\nIs it related to the Cuda version (my Cuda\
    \ version is 12.1)? The code runs perfectly on google-collab.\n   \n\n```\nraise\
    \ HFValidationError(\nhuggingface_hub.utils._validators.HFValidationError: Repo\
    \ id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden,\
    \ '-' and '.' cannot start or end the name, max length is 96: 'RWForCausalLM(\n\
    \  (transformer): RWModel(\n    (word_embeddings): Embedding(65024, 4544)\n  \
    \  (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n        (input_layernorm):\
    \ LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n        (self_attention):\
    \ Attention(\n          (maybe_rotary): RotaryEmbedding()\n          (query_key_value):\
    \ Linear(in_features=4544, out_features=4672, bias=False)\n          (dense):\
    \ Linear(in_features=4544, out_features=4544, bias=False)\n          (attention_dropout):\
    \ Dropout(p=0.0, inplace=False)\n        )\n        (mlp): MLP(\n          (dense_h_to_4h):\
    \ Linear(in_features=4544, out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n\
    \          (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)\n\
    \        )\n      )\n    )\n    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n\
    \  )\n  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n\
    )'.\n```"
  created_at: 2023-07-13 15:09:30+00:00
  edited: true
  hidden: false
  id: 64b021ba919714d7dcd079de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/74c722820ae9019997a0d1d6d29962ec.svg
      fullname: Amir Hadifar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hadifar
      type: user
    createdAt: '2023-07-13T16:29:31.000Z'
    data:
      status: closed
    id: 64b0266bf9cb06c1dc5efc93
    type: status-change
  author: hadifar
  created_at: 2023-07-13 15:29:31+00:00
  id: 64b0266bf9cb06c1dc5efc93
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 68
repo_id: tiiuae/falcon-7b
repo_type: model
status: closed
target_branch: null
title: ' Repo id must use alphanumeric chars or ''-'', ''_'', ''.'', ''--'' and ''..''
  are forbidden, ''-'' and ''.'' cannot start or end the name, max length is 96'
