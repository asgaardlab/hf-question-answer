!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dwojcik
conflicting_files: null
created_at: 2023-12-14 09:26:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/959ee13b7ddff5604cc8dfcc44ffbf6a.svg
      fullname: "Dariusz W\xF3jcik"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dwojcik
      type: user
    createdAt: '2023-12-14T09:26:16.000Z'
    data:
      edited: false
      editors:
      - dwojcik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5948691368103027
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/959ee13b7ddff5604cc8dfcc44ffbf6a.svg
          fullname: "Dariusz W\xF3jcik"
          isHf: false
          isPro: false
          name: dwojcik
          type: user
        html: "<p>The code:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ AutoTokenizer, AutoModelForCausalLM\n<span class=\"hljs-keyword\">import</span>\
          \ transformers\n<span class=\"hljs-keyword\">import</span> torch\n\nmodel\
          \ = <span class=\"hljs-string\">\"tiiuae/falcon-7b\"</span>\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
          \    <span class=\"hljs-string\">\"text-generation\"</span>,\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>,\n    device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>,\n)\nsequences = pipeline(\n   <span class=\"hljs-string\"\
          >\"Girafatron is obsessed with giraffes, the most glorious animal on the\
          \ face of this Earth. Giraftron believes all other animals are irrelevant\
          \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello,\
          \ Girafatron!\\nGirafatron:\"</span>,\n    max_length=<span class=\"hljs-number\"\
          >200</span>,\n    do_sample=<span class=\"hljs-literal\">True</span>,\n\
          \    top_k=<span class=\"hljs-number\">10</span>,\n    num_return_sequences=<span\
          \ class=\"hljs-number\">1</span>,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\n<span class=\"hljs-keyword\">for</span> seq <span class=\"hljs-keyword\"\
          >in</span> sequences:\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"Result: <span class=\"hljs-subst\">{seq[<span\
          \ class=\"hljs-string\">'generated_text'</span>]}</span>\"</span>)\n</code></pre>\n\
          <p>produces:</p>\n<pre><code class=\"language-shell\">OSError: tiiuae/falcon-7b\
          \ does not appear to have a file named config.json. Checkout 'https://huggingface.co/tiiuae/falcon-7b/None'\
          \ for available files.\n</code></pre>\n<pre><code>transformers.__version__\
          \ = '4.37.0.dev0'\n</code></pre>\n"
        raw: "The code:\r\n```python\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
          \nimport transformers\r\nimport torch\r\n\r\nmodel = \"tiiuae/falcon-7b\"\
          \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\npipeline = transformers.pipeline(\r\
          \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\
          \n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n   \
          \ device_map=\"auto\",\r\n)\r\nsequences = pipeline(\r\n   \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",\r\n    max_length=200,\r\n    do_sample=True,\r\n    top_k=10,\r\
          \n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
          \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
          )\r\n```\r\nproduces:\r\n```shell\r\nOSError: tiiuae/falcon-7b does not\
          \ appear to have a file named config.json. Checkout 'https://huggingface.co/tiiuae/falcon-7b/None'\
          \ for available files.\r\n```\r\n\r\n```\r\ntransformers.__version__ = '4.37.0.dev0'\r\
          \n```"
        updatedAt: '2023-12-14T09:26:16.961Z'
      numEdits: 0
      reactions: []
    id: 657aca38c51417d4fe6064f8
    type: comment
  author: dwojcik
  content: "The code:\r\n```python\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
    \nimport transformers\r\nimport torch\r\n\r\nmodel = \"tiiuae/falcon-7b\"\r\n\r\
    \ntokenizer = AutoTokenizer.from_pretrained(model)\r\npipeline = transformers.pipeline(\r\
    \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n\
    \    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n    device_map=\"\
    auto\",\r\n)\r\nsequences = pipeline(\r\n   \"Girafatron is obsessed with giraffes,\
    \ the most glorious animal on the face of this Earth. Giraftron believes all other\
    \ animals are irrelevant when compared to the glorious majesty of the giraffe.\\\
    nDaniel: Hello, Girafatron!\\nGirafatron:\",\r\n    max_length=200,\r\n    do_sample=True,\r\
    \n    top_k=10,\r\n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
    \n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\"\
    )\r\n```\r\nproduces:\r\n```shell\r\nOSError: tiiuae/falcon-7b does not appear\
    \ to have a file named config.json. Checkout 'https://huggingface.co/tiiuae/falcon-7b/None'\
    \ for available files.\r\n```\r\n\r\n```\r\ntransformers.__version__ = '4.37.0.dev0'\r\
    \n```"
  created_at: 2023-12-14 09:26:16+00:00
  edited: false
  hidden: false
  id: 657aca38c51417d4fe6064f8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 96
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: Error while trying to load model
