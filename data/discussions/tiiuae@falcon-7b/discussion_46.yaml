!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MorphzZ
conflicting_files: null
created_at: 2023-06-24 00:15:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
      fullname: MORPHEUS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MorphzZ
      type: user
    createdAt: '2023-06-24T01:15:51.000Z'
    data:
      edited: false
      editors:
      - MorphzZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.583664059638977
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e9ea993fc4094f5aa1a0308247b330f.svg
          fullname: MORPHEUS
          isHf: false
          isPro: false
          name: MorphzZ
          type: user
        html: "<p>Hello,</p>\n<p>I tried to use falcon like <a href=\"https://huggingface.co/tiiuae/falcon-7b\"\
          >this</a>:</p>\n<pre><code>&gt;&gt;&gt; pipeline = transformers.pipeline(\n\
          ...     \"text-generation\",\n...     model=model,\n...     tokenizer=tokenizer,\n\
          ...     torch_dtype=torch.bfloat16,\n...     trust_remote_code=True,\n...\
          \     device_map=\"auto\",\n... )\n</code></pre>\n<p>This code gives me\
          \ following message:</p>\n<pre><code>The model 'RWForCausalLM' is not supported\
          \ for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel',\
          \ 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
          \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
          \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
          \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
          \ 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM',\
          \ 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM',\
          \ 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM',\
          \ 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM',\
          \ 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM',\
          \ 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead',\
          \ 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM',\
          \ 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM',\
          \ 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel',\
          \ 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM',\
          \ 'XLNetLMHeadModel', 'XmodForCausalLM'].\n</code></pre>\n<p>But the funny\
          \ thing is that I can still run the model:</p>\n<pre><code>&gt;&gt;&gt;\
          \ sequences = pipeline(\n...    \"Girafatron is obsessed with giraffes,\
          \ the most glorious animal on the face of this Earth. Giraftron believes\
          \ all other animals are irrelevant when compared to the glorious majesty\
          \ of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n...   \
          \  max_length=200,\n...     do_sample=True,\n...     top_k=10,\n...    \
          \ num_return_sequences=1,\n...     eos_token_id=tokenizer.eos_token_id,\n\
          ... )\n&gt;&gt;&gt; for seq in sequences:\n...     print(f\"Result: {seq['generated_text']}\"\
          )\n...\nResult: Girafatron is obsessed with giraffes, the most glorious\
          \ animal on the face of this Earth. Giraftron believes all other animals\
          \ are irrelevant when compared to the glorious majesty of the giraffe.\n\
          Daniel: Hello, Girafatron!\nGirafatron: Hi! Daniel, you look great today.\n\
          Daniel: Thank you, Girafatron.\nGirafatron: So do you know what I'm obsessed\
          \ with?\nDaniel: Giraffes, obviously.\nGirafatron: No, I am obsessed with\
          \ giraffes, the most glorious animal on this Earth.\nDaniel: Giraffes are\
          \ the best animals in the entire world.\nGirafatron: I'm glad you share\
          \ my opinion.\nDaniel: I'm also glad that there's a giraffes in this room.\n\
          Girafatron: Yes, I know.\nDaniel: I\n</code></pre>\n<p>How is this possible?</p>\n\
          <p>When I try to look what kind of model is falcon I see this:</p>\n<pre><code>&gt;&gt;&gt;\
          \ pipeline.model\nRWForCausalLM(\n  (transformer): RWModel(\n    (word_embeddings):\
          \ Embedding(65024, 4544)\n    (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n\
          \        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n\
          \        (self_attention): Attention(\n          (maybe_rotary): RotaryEmbedding()\n\
          \          (query_key_value): Linear(in_features=4544, out_features=4672,\
          \ bias=False)\n          (dense): Linear(in_features=4544, out_features=4544,\
          \ bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n\
          \        )\n        (mlp): MLP(\n          (dense_h_to_4h): Linear(in_features=4544,\
          \ out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n\
          \          (dense_4h_to_h): Linear(in_features=18176, out_features=4544,\
          \ bias=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4544,\
          \ out_features=65024, bias=False)\n)\n</code></pre>\n<p>But there is no\
          \ model named <code>RWModel</code> in <code>site-packages/transformers/models</code>.\
          \ How does it work then? Can someone explain me where can I find source\
          \ code of the model under <code>transformers</code> package?</p>\n"
        raw: "Hello,\r\n\r\nI tried to use falcon like [this](https://huggingface.co/tiiuae/falcon-7b):\r\
          \n\r\n```\r\n>>> pipeline = transformers.pipeline(\r\n...     \"text-generation\"\
          ,\r\n...     model=model,\r\n...     tokenizer=tokenizer,\r\n...     torch_dtype=torch.bfloat16,\r\
          \n...     trust_remote_code=True,\r\n...     device_map=\"auto\",\r\n...\
          \ )\r\n```\r\n\r\nThis code gives me following message:\r\n\r\n```\r\nThe\
          \ model 'RWForCausalLM' is not supported for text-generation. Supported\
          \ models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
          \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM',\
          \ 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM',\
          \ 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel',\
          \ 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM',\
          \ 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM',\
          \ 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
          \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
          \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
          \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM',\
          \ 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM',\
          \ 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel',\
          \ 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM',\
          \ 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel',\
          \ 'XmodForCausalLM'].\r\n```\r\n\r\nBut the funny thing is that I can still\
          \ run the model:\r\n\r\n```\r\n>>> sequences = pipeline(\r\n...    \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",\r\n...     max_length=200,\r\n...     do_sample=True,\r\n\
          ...     top_k=10,\r\n...     num_return_sequences=1,\r\n...     eos_token_id=tokenizer.eos_token_id,\r\
          \n... )\r\n>>> for seq in sequences:\r\n...     print(f\"Result: {seq['generated_text']}\"\
          )\r\n...\r\nResult: Girafatron is obsessed with giraffes, the most glorious\
          \ animal on the face of this Earth. Giraftron believes all other animals\
          \ are irrelevant when compared to the glorious majesty of the giraffe.\r\
          \nDaniel: Hello, Girafatron!\r\nGirafatron: Hi! Daniel, you look great today.\r\
          \nDaniel: Thank you, Girafatron.\r\nGirafatron: So do you know what I'm\
          \ obsessed with?\r\nDaniel: Giraffes, obviously.\r\nGirafatron: No, I am\
          \ obsessed with giraffes, the most glorious animal on this Earth.\r\nDaniel:\
          \ Giraffes are the best animals in the entire world.\r\nGirafatron: I'm\
          \ glad you share my opinion.\r\nDaniel: I'm also glad that there's a giraffes\
          \ in this room.\r\nGirafatron: Yes, I know.\r\nDaniel: I\r\n```\r\n\r\n\
          How is this possible?\r\n\r\nWhen I try to look what kind of model is falcon\
          \ I see this:\r\n\r\n```\r\n>>> pipeline.model\r\nRWForCausalLM(\r\n  (transformer):\
          \ RWModel(\r\n    (word_embeddings): Embedding(65024, 4544)\r\n    (h):\
          \ ModuleList(\r\n      (0-31): 32 x DecoderLayer(\r\n        (input_layernorm):\
          \ LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\r\n        (self_attention):\
          \ Attention(\r\n          (maybe_rotary): RotaryEmbedding()\r\n        \
          \  (query_key_value): Linear(in_features=4544, out_features=4672, bias=False)\r\
          \n          (dense): Linear(in_features=4544, out_features=4544, bias=False)\r\
          \n          (attention_dropout): Dropout(p=0.0, inplace=False)\r\n     \
          \   )\r\n        (mlp): MLP(\r\n          (dense_h_to_4h): Linear(in_features=4544,\
          \ out_features=18176, bias=False)\r\n          (act): GELU(approximate='none')\r\
          \n          (dense_4h_to_h): Linear(in_features=18176, out_features=4544,\
          \ bias=False)\r\n        )\r\n      )\r\n    )\r\n    (ln_f): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (lm_head): Linear(in_features=4544,\
          \ out_features=65024, bias=False)\r\n)\r\n```\r\n\r\nBut there is no model\
          \ named `RWModel` in `site-packages/transformers/models`. How does it work\
          \ then? Can someone explain me where can I find source code of the model\
          \ under `transformers` package?"
        updatedAt: '2023-06-24T01:15:51.489Z'
      numEdits: 0
      reactions: []
    id: 649643c7a002386736eac19f
    type: comment
  author: MorphzZ
  content: "Hello,\r\n\r\nI tried to use falcon like [this](https://huggingface.co/tiiuae/falcon-7b):\r\
    \n\r\n```\r\n>>> pipeline = transformers.pipeline(\r\n...     \"text-generation\"\
    ,\r\n...     model=model,\r\n...     tokenizer=tokenizer,\r\n...     torch_dtype=torch.bfloat16,\r\
    \n...     trust_remote_code=True,\r\n...     device_map=\"auto\",\r\n... )\r\n\
    ```\r\n\r\nThis code gives me following message:\r\n\r\n```\r\nThe model 'RWForCausalLM'\
    \ is not supported for text-generation. Supported models are ['BartForCausalLM',\
    \ 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM',\
    \ 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
    \ 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
    \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
    \ 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM',\
    \ 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM',\
    \ 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM',\
    \ 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
    \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM',\
    \ 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM',\
    \ 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM',\
    \ 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM',\
    \ 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM',\
    \ 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\r\n```\r\n\
    \r\nBut the funny thing is that I can still run the model:\r\n\r\n```\r\n>>> sequences\
    \ = pipeline(\r\n...    \"Girafatron is obsessed with giraffes, the most glorious\
    \ animal on the face of this Earth. Giraftron believes all other animals are irrelevant\
    \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
    nGirafatron:\",\r\n...     max_length=200,\r\n...     do_sample=True,\r\n... \
    \    top_k=10,\r\n...     num_return_sequences=1,\r\n...     eos_token_id=tokenizer.eos_token_id,\r\
    \n... )\r\n>>> for seq in sequences:\r\n...     print(f\"Result: {seq['generated_text']}\"\
    )\r\n...\r\nResult: Girafatron is obsessed with giraffes, the most glorious animal\
    \ on the face of this Earth. Giraftron believes all other animals are irrelevant\
    \ when compared to the glorious majesty of the giraffe.\r\nDaniel: Hello, Girafatron!\r\
    \nGirafatron: Hi! Daniel, you look great today.\r\nDaniel: Thank you, Girafatron.\r\
    \nGirafatron: So do you know what I'm obsessed with?\r\nDaniel: Giraffes, obviously.\r\
    \nGirafatron: No, I am obsessed with giraffes, the most glorious animal on this\
    \ Earth.\r\nDaniel: Giraffes are the best animals in the entire world.\r\nGirafatron:\
    \ I'm glad you share my opinion.\r\nDaniel: I'm also glad that there's a giraffes\
    \ in this room.\r\nGirafatron: Yes, I know.\r\nDaniel: I\r\n```\r\n\r\nHow is\
    \ this possible?\r\n\r\nWhen I try to look what kind of model is falcon I see\
    \ this:\r\n\r\n```\r\n>>> pipeline.model\r\nRWForCausalLM(\r\n  (transformer):\
    \ RWModel(\r\n    (word_embeddings): Embedding(65024, 4544)\r\n    (h): ModuleList(\r\
    \n      (0-31): 32 x DecoderLayer(\r\n        (input_layernorm): LayerNorm((4544,),\
    \ eps=1e-05, elementwise_affine=True)\r\n        (self_attention): Attention(\r\
    \n          (maybe_rotary): RotaryEmbedding()\r\n          (query_key_value):\
    \ Linear(in_features=4544, out_features=4672, bias=False)\r\n          (dense):\
    \ Linear(in_features=4544, out_features=4544, bias=False)\r\n          (attention_dropout):\
    \ Dropout(p=0.0, inplace=False)\r\n        )\r\n        (mlp): MLP(\r\n      \
    \    (dense_h_to_4h): Linear(in_features=4544, out_features=18176, bias=False)\r\
    \n          (act): GELU(approximate='none')\r\n          (dense_4h_to_h): Linear(in_features=18176,\
    \ out_features=4544, bias=False)\r\n        )\r\n      )\r\n    )\r\n    (ln_f):\
    \ LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (lm_head):\
    \ Linear(in_features=4544, out_features=65024, bias=False)\r\n)\r\n```\r\n\r\n\
    But there is no model named `RWModel` in `site-packages/transformers/models`.\
    \ How does it work then? Can someone explain me where can I find source code of\
    \ the model under `transformers` package?"
  created_at: 2023-06-24 00:15:51+00:00
  edited: false
  hidden: false
  id: 649643c7a002386736eac19f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
      fullname: Saptarshi Sengupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saptarshi7
      type: user
    createdAt: '2023-09-12T18:46:54.000Z'
    data:
      edited: false
      editors:
      - Saptarshi7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9950065016746521
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19be33f195de9ead8285d997659a1bf6.svg
          fullname: Saptarshi Sengupta
          isHf: false
          isPro: false
          name: Saptarshi7
          type: user
        html: '<p>Yes, I''m very curious about this as well. I don''t really know
          what RWmodel is apart from Refined-Web corpus model. However, I did see
          in one files'' comments that Falcon is perhaps a BLOOM checkpoint.</p>

          '
        raw: Yes, I'm very curious about this as well. I don't really know what RWmodel
          is apart from Refined-Web corpus model. However, I did see in one files'
          comments that Falcon is perhaps a BLOOM checkpoint.
        updatedAt: '2023-09-12T18:46:54.532Z'
      numEdits: 0
      reactions: []
    id: 6500b21e0339dae3dbb72ec6
    type: comment
  author: Saptarshi7
  content: Yes, I'm very curious about this as well. I don't really know what RWmodel
    is apart from Refined-Web corpus model. However, I did see in one files' comments
    that Falcon is perhaps a BLOOM checkpoint.
  created_at: 2023-09-12 17:46:54+00:00
  edited: false
  hidden: false
  id: 6500b21e0339dae3dbb72ec6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 46
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: Trying to understand internals of falcon
