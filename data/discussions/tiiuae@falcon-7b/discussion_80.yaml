!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Inoob
conflicting_files: null
created_at: 2023-09-15 11:44:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6280861626ddc5b95930990e1426da39.svg
      fullname: Ivan He
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Inoob
      type: user
    createdAt: '2023-09-15T12:44:34.000Z'
    data:
      edited: false
      editors:
      - Inoob
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5659853219985962
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6280861626ddc5b95930990e1426da39.svg
          fullname: Ivan He
          isHf: false
          isPro: false
          name: Inoob
          type: user
        html: "<p>when I downloaded falcon 7b using git clone into the dir tiiuae/falcon_7b\
          \ and run it with the following code:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, AutoModelForCausalLM\n<span class=\"hljs-keyword\"\
          >import</span> transformers\n<span class=\"hljs-keyword\">import</span>\
          \ torch\n\nmodel = <span class=\"hljs-string\">\"tiiuae/falcon-7b\"</span>\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n\
          \    <span class=\"hljs-string\">\"text-generation\"</span>,\n    model=model,\n\
          \    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>,\n    device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>,\n)\nsequences = pipeline(\n   <span class=\"hljs-string\"\
          >\"Girafatron is obsessed with giraffes, the most glorious animal on the\
          \ face of this Earth. Giraftron believes all other animals are irrelevant\
          \ when compared to the glorious majesty of the giraffe.\\nDaniel: Hello,\
          \ Girafatron!\\nGirafatron:\"</span>,\n    max_length=<span class=\"hljs-number\"\
          >200</span>,\n    do_sample=<span class=\"hljs-literal\">True</span>,\n\
          \    top_k=<span class=\"hljs-number\">10</span>,\n    num_return_sequences=<span\
          \ class=\"hljs-number\">1</span>,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\n<span class=\"hljs-keyword\">for</span> seq <span class=\"hljs-keyword\"\
          >in</span> sequences:\n    <span class=\"hljs-built_in\">print</span>(seq[<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n</code></pre>\n<p>But\
          \ it throws and error:<br>Traceback (most recent call last):<br>  File \"\
          /opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 3508, in run_code<br>    exec(code_obj, self.user_global_ns, self.user_ns)<br>\
          \  File \"/tmp/ipykernel_28/3467703408.py\", line 8, in <br>    pipeline\
          \ = transformers.pipeline(<br>  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/<strong>init</strong>.py\"\
          , line 824, in pipeline<br>    image_processor is None<br>  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 282, in infer_framework_load_model<br>ValueError: Could not load\
          \ model tiiuae/falcon-7b with any of the following classes: (&lt;class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;).\
          \ See the original errors:</p>\n<p>while loading with AutoModelForCausalLM,\
          \ an error is thrown:<br>Traceback (most recent call last):<br>  File \"\
          /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 488, in load_state_dict<br>    loaded_keys = [k.replace(f\"{module_name}.\"\
          , \"\") for k in state_dict_keys if k.startswith(f\"{module_name}.\")]<br>\
          \  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 797, in load<br>    with _open_zipfile_reader(opened_file) as opened_zipfile:<br>\
          \  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 283, in <strong>init</strong><br>    super().<strong>init</strong>(torch._C.PyTorchFileReader(name_or_buffer))<br>RuntimeError:\
          \ PytorchStreamReader failed reading zip archive: failed finding central\
          \ directory</p>\n<p>During handling of the above exception, another exception\
          \ occurred:</p>\n<p>Traceback (most recent call last):<br>  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 492, in load_state_dict<br>  File \"/opt/conda/lib/python3.10/codecs.py\"\
          , line 322, in decode<br>    (result, consumed) = self._buffer_decode(data,\
          \ self.errors, final)<br>UnicodeDecodeError: 'utf-8' codec can't decode\
          \ byte 0x80 in position 128: invalid start byte</p>\n<p>During handling\
          \ of the above exception, another exception occurred:</p>\n<p>Traceback\
          \ (most recent call last):<br>  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model<br>    model = model_class.from_pretrained(model,\
          \ **kwargs)<br>  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 558, in from_pretrained<br>    from_pretrained.<strong>doc</strong>\
          \ = from_pretrained_docstring<br>  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3175, in from_pretrained<br>    # This should always be a list but,\
          \ just to be sure.<br>  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3548, in _load_pretrained_model<br>    return x<br>  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 504, in load_state_dict<br>    old_keys.append(key)<br>OSError: Unable\
          \ to load weights from pytorch checkpoint file for 'tiiuae/falcon-7b/pytorch_model-00001-of-00002.bin'\
          \ at 'tiiuae/falcon-7b/pytorch_model-00001-of-00002.bin'. If you tried to\
          \ load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.</p>\n\
          <p>while loading with TFAutoModelForCausalLM, an error is thrown:<br>Traceback\
          \ (most recent call last):<br>  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model<br>    model = model_class.from_pretrained(model,\
          \ **kwargs)<br>  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 566, in from_pretrained<br>    for model in model_mapping.values():<br>ValueError:\
          \ Unrecognized configuration class &lt;class 'transformers_modules.falcon-7b.configuration_RW.RWConfig'&gt;\
          \ for this kind of AutoModel: TFAutoModelForCausalLM.<br>Model type should\
          \ be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config,\
          \ GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig,\
          \ RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig,\
          \ XLMConfig, XLMRobertaConfig, XLNetConfig.</p>\n<p>During handling of the\
          \ above exception, another exception occurred:</p>\n<p>Traceback (most recent\
          \ call last):<br>  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 2105, in showtraceback<br>    stb = self.InteractiveTB.structured_traceback(<br>\
          \  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1428, in structured_traceback<br>    return FormattedTB.structured_traceback(<br>\
          \  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1319, in structured_traceback<br>    return VerboseTB.structured_traceback(<br>\
          \  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1172, in structured_traceback<br>    formatted_exception = self.format_exception_as_a_whole(etype,\
          \ evalue, etb, number_of_lines_of_context,<br>  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1087, in format_exception_as_a_whole<br>    frames.append(self.format_record(record))<br>\
          \  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 969, in format_record<br>    frame_info.lines, Colors, self.has_colors,\
          \ lvals<br>  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 792, in lines<br>    return self._sd.lines<br>  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\"\
          , line 144, in cached_property_wrapper<br>    value = obj.<strong>dict</strong>[self.func.<strong>name</strong>]\
          \ = self.func(obj)<br>  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\"\
          , line 734, in lines<br>    pieces = self.included_pieces<br>  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\"\
          , line 144, in cached_property_wrapper<br>    value = obj.<strong>dict</strong>[self.func.<strong>name</strong>]\
          \ = self.func(obj)<br>  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\"\
          , line 681, in included_pieces<br>    pos = scope_pieces.index(self.executing_piece)<br>\
          \  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\"\
          , line 144, in cached_property_wrapper<br>    value = obj.<strong>dict</strong>[self.func.<strong>name</strong>]\
          \ = self.func(obj)<br>  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\"\
          , line 660, in executing_piece<br>    return only(<br>  File \"/opt/conda/lib/python3.10/site-packages/executing/executing.py\"\
          , line 190, in only<br>    raise NotOneValueFound('Expected one value, found\
          \ 0')<br>executing.executing.NotOneValueFound: Expected one value, found\
          \ 0</p>\n<p>How to fix this?<br>I already tried this, doesn't work:<br>pip\
          \ install -U git+<a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers.git@e03a9cc\"\
          >https://github.com/huggingface/transformers.git@e03a9cc</a> &amp;&amp;<br>pip\
          \ install -U git+<a rel=\"nofollow\" href=\"https://github.com/huggingface/peft.git@42a184f\"\
          >https://github.com/huggingface/peft.git@42a184f</a> &amp;&amp;<br>pip install\
          \ -U git+<a rel=\"nofollow\" href=\"https://github.com/huggingface/accelerate.git@c9fbb71\"\
          >https://github.com/huggingface/accelerate.git@c9fbb71</a> &amp;&amp;<br>pip\
          \ install einops==0.6.1 &amp;&amp;<br>pip install torch==2.0.1 &amp;&amp;<br>pip\
          \ install bitsandbytes==0.39.0 &amp;&amp;<br>pip install scipy &amp;&amp;<br>pip\
          \ install loralib==0.1.1 &amp;&amp; \\</p>\n"
        raw: "when I downloaded falcon 7b using git clone into the dir tiiuae/falcon_7b\
          \ and run it with the following code:\r\n```python\r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\nimport transformers\r\n\
          import torch\r\n\r\nmodel = \"tiiuae/falcon-7b\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\
          \npipeline = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
          \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\
          \n    device_map=\"auto\",\r\n)\r\nsequences = pipeline(\r\n   \"Girafatron\
          \ is obsessed with giraffes, the most glorious animal on the face of this\
          \ Earth. Giraftron believes all other animals are irrelevant when compared\
          \ to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\\
          nGirafatron:\",\r\n    max_length=200,\r\n    do_sample=True,\r\n    top_k=10,\r\
          \n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
          \n)\r\nfor seq in sequences:\r\n    print(seq['generated_text'])\r\n```\r\
          \nBut it throws and error:\r\nTraceback (most recent call last):\r\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 3508, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\
          \n  File \"/tmp/ipykernel_28/3467703408.py\", line 8, in <module>\r\n  \
          \  pipeline = transformers.pipeline(\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py\"\
          , line 824, in pipeline\r\n    image_processor is None\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 282, in infer_framework_load_model\r\nValueError: Could not load\
          \ model tiiuae/falcon-7b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\
          \ See the original errors:\r\n\r\nwhile loading with AutoModelForCausalLM,\
          \ an error is thrown:\r\nTraceback (most recent call last):\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 488, in load_state_dict\r\n    loaded_keys = [k.replace(f\"{module_name}.\"\
          , \"\") for k in state_dict_keys if k.startswith(f\"{module_name}.\")]\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 797, in load\r\n    with _open_zipfile_reader(opened_file) as opened_zipfile:\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 283, in __init__\r\n    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\r\
          \nRuntimeError: PytorchStreamReader failed reading zip archive: failed finding\
          \ central directory\r\n\r\nDuring handling of the above exception, another\
          \ exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 492, in load_state_dict\r\n  File \"/opt/conda/lib/python3.10/codecs.py\"\
          , line 322, in decode\r\n    (result, consumed) = self._buffer_decode(data,\
          \ self.errors, final)\r\nUnicodeDecodeError: 'utf-8' codec can't decode\
          \ byte 0x80 in position 128: invalid start byte\r\n\r\nDuring handling of\
          \ the above exception, another exception occurred:\r\n\r\nTraceback (most\
          \ recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model\r\n    model = model_class.from_pretrained(model,\
          \ **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 558, in from_pretrained\r\n    from_pretrained.__doc__ = from_pretrained_docstring\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3175, in from_pretrained\r\n    # This should always be a list but,\
          \ just to be sure.\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3548, in _load_pretrained_model\r\n    return x\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 504, in load_state_dict\r\n    old_keys.append(key)\r\nOSError: Unable\
          \ to load weights from pytorch checkpoint file for 'tiiuae/falcon-7b/pytorch_model-00001-of-00002.bin'\
          \ at 'tiiuae/falcon-7b/pytorch_model-00001-of-00002.bin'. If you tried to\
          \ load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\
          \n\r\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\r\n\
          Traceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model\r\n    model = model_class.from_pretrained(model,\
          \ **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 566, in from_pretrained\r\n    for model in model_mapping.values():\r\
          \nValueError: Unrecognized configuration class <class 'transformers_modules.falcon-7b.configuration_RW.RWConfig'>\
          \ for this kind of AutoModel: TFAutoModelForCausalLM.\r\nModel type should\
          \ be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config,\
          \ GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig,\
          \ RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig,\
          \ XLMConfig, XLMRobertaConfig, XLNetConfig.\r\n\r\n\r\n\r\n\r\nDuring handling\
          \ of the above exception, another exception occurred:\r\n\r\nTraceback (most\
          \ recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 2105, in showtraceback\r\n    stb = self.InteractiveTB.structured_traceback(\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1428, in structured_traceback\r\n    return FormattedTB.structured_traceback(\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1319, in structured_traceback\r\n    return VerboseTB.structured_traceback(\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1172, in structured_traceback\r\n    formatted_exception = self.format_exception_as_a_whole(etype,\
          \ evalue, etb, number_of_lines_of_context,\r\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 1087, in format_exception_as_a_whole\r\n    frames.append(self.format_record(record))\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 969, in format_record\r\n    frame_info.lines, Colors, self.has_colors,\
          \ lvals\r\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
          , line 792, in lines\r\n    return self._sd.lines\r\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\"\
          , line 144, in cached_property_wrapper\r\n    value = obj.__dict__[self.func.__name__]\
          \ = self.func(obj)\r\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\"\
          , line 734, in lines\r\n    pieces = self.included_pieces\r\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\"\
          , line 144, in cached_property_wrapper\r\n    value = obj.__dict__[self.func.__name__]\
          \ = self.func(obj)\r\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\"\
          , line 681, in included_pieces\r\n    pos = scope_pieces.index(self.executing_piece)\r\
          \n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\"\
          , line 144, in cached_property_wrapper\r\n    value = obj.__dict__[self.func.__name__]\
          \ = self.func(obj)\r\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\"\
          , line 660, in executing_piece\r\n    return only(\r\n  File \"/opt/conda/lib/python3.10/site-packages/executing/executing.py\"\
          , line 190, in only\r\n    raise NotOneValueFound('Expected one value, found\
          \ 0')\r\nexecuting.executing.NotOneValueFound: Expected one value, found\
          \ 0\r\n\r\nHow to fix this?\r\nI already tried this, doesn't work:\r\npip\
          \ install -U git+https://github.com/huggingface/transformers.git@e03a9cc\
          \ &&\r\npip install -U git+https://github.com/huggingface/peft.git@42a184f\
          \ &&\r\npip install -U git+https://github.com/huggingface/accelerate.git@c9fbb71\
          \ &&\r\npip install einops==0.6.1 &&\r\npip install torch==2.0.1 &&\r\n\
          pip install bitsandbytes==0.39.0 &&\r\npip install scipy &&\r\npip install\
          \ loralib==0.1.1 && \\"
        updatedAt: '2023-09-15T12:44:34.338Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - HormyAJP
    id: 650451b257cc1e5d7536cd05
    type: comment
  author: Inoob
  content: "when I downloaded falcon 7b using git clone into the dir tiiuae/falcon_7b\
    \ and run it with the following code:\r\n```python\r\nfrom transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\r\nimport transformers\r\nimport torch\r\
    \n\r\nmodel = \"tiiuae/falcon-7b\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\
    \npipeline = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
    \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\
    \n    device_map=\"auto\",\r\n)\r\nsequences = pipeline(\r\n   \"Girafatron is\
    \ obsessed with giraffes, the most glorious animal on the face of this Earth.\
    \ Giraftron believes all other animals are irrelevant when compared to the glorious\
    \ majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\r\n  \
    \  max_length=200,\r\n    do_sample=True,\r\n    top_k=10,\r\n    num_return_sequences=1,\r\
    \n    eos_token_id=tokenizer.eos_token_id,\r\n)\r\nfor seq in sequences:\r\n \
    \   print(seq['generated_text'])\r\n```\r\nBut it throws and error:\r\nTraceback\
    \ (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
    , line 3508, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\
    \n  File \"/tmp/ipykernel_28/3467703408.py\", line 8, in <module>\r\n    pipeline\
    \ = transformers.pipeline(\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py\"\
    , line 824, in pipeline\r\n    image_processor is None\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 282, in infer_framework_load_model\r\nValueError: Could not load model\
    \ tiiuae/falcon-7b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
    \ <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\
    \ See the original errors:\r\n\r\nwhile loading with AutoModelForCausalLM, an\
    \ error is thrown:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 488, in load_state_dict\r\n    loaded_keys = [k.replace(f\"{module_name}.\"\
    , \"\") for k in state_dict_keys if k.startswith(f\"{module_name}.\")]\r\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 797,\
    \ in load\r\n    with _open_zipfile_reader(opened_file) as opened_zipfile:\r\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line\
    \ 283, in __init__\r\n    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\r\
    \nRuntimeError: PytorchStreamReader failed reading zip archive: failed finding\
    \ central directory\r\n\r\nDuring handling of the above exception, another exception\
    \ occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 492, in load_state_dict\r\n  File \"/opt/conda/lib/python3.10/codecs.py\"\
    , line 322, in decode\r\n    (result, consumed) = self._buffer_decode(data, self.errors,\
    \ final)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position\
    \ 128: invalid start byte\r\n\r\nDuring handling of the above exception, another\
    \ exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 269, in infer_framework_load_model\r\n    model = model_class.from_pretrained(model,\
    \ **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 558, in from_pretrained\r\n    from_pretrained.__doc__ = from_pretrained_docstring\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 3175, in from_pretrained\r\n    # This should always be a list but, just\
    \ to be sure.\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 3548, in _load_pretrained_model\r\n    return x\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 504, in load_state_dict\r\n    old_keys.append(key)\r\nOSError: Unable\
    \ to load weights from pytorch checkpoint file for 'tiiuae/falcon-7b/pytorch_model-00001-of-00002.bin'\
    \ at 'tiiuae/falcon-7b/pytorch_model-00001-of-00002.bin'. If you tried to load\
    \ a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\r\n\r\nwhile\
    \ loading with TFAutoModelForCausalLM, an error is thrown:\r\nTraceback (most\
    \ recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 269, in infer_framework_load_model\r\n    model = model_class.from_pretrained(model,\
    \ **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 566, in from_pretrained\r\n    for model in model_mapping.values():\r\n\
    ValueError: Unrecognized configuration class <class 'transformers_modules.falcon-7b.configuration_RW.RWConfig'>\
    \ for this kind of AutoModel: TFAutoModelForCausalLM.\r\nModel type should be\
    \ one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig,\
    \ OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig,\
    \ RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\r\
    \n\r\n\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\
    \n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
    , line 2105, in showtraceback\r\n    stb = self.InteractiveTB.structured_traceback(\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\",\
    \ line 1428, in structured_traceback\r\n    return FormattedTB.structured_traceback(\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\",\
    \ line 1319, in structured_traceback\r\n    return VerboseTB.structured_traceback(\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\",\
    \ line 1172, in structured_traceback\r\n    formatted_exception = self.format_exception_as_a_whole(etype,\
    \ evalue, etb, number_of_lines_of_context,\r\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
    , line 1087, in format_exception_as_a_whole\r\n    frames.append(self.format_record(record))\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\",\
    \ line 969, in format_record\r\n    frame_info.lines, Colors, self.has_colors,\
    \ lvals\r\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\"\
    , line 792, in lines\r\n    return self._sd.lines\r\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\"\
    , line 144, in cached_property_wrapper\r\n    value = obj.__dict__[self.func.__name__]\
    \ = self.func(obj)\r\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\"\
    , line 734, in lines\r\n    pieces = self.included_pieces\r\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\"\
    , line 144, in cached_property_wrapper\r\n    value = obj.__dict__[self.func.__name__]\
    \ = self.func(obj)\r\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\"\
    , line 681, in included_pieces\r\n    pos = scope_pieces.index(self.executing_piece)\r\
    \n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line\
    \ 144, in cached_property_wrapper\r\n    value = obj.__dict__[self.func.__name__]\
    \ = self.func(obj)\r\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\"\
    , line 660, in executing_piece\r\n    return only(\r\n  File \"/opt/conda/lib/python3.10/site-packages/executing/executing.py\"\
    , line 190, in only\r\n    raise NotOneValueFound('Expected one value, found 0')\r\
    \nexecuting.executing.NotOneValueFound: Expected one value, found 0\r\n\r\nHow\
    \ to fix this?\r\nI already tried this, doesn't work:\r\npip install -U git+https://github.com/huggingface/transformers.git@e03a9cc\
    \ &&\r\npip install -U git+https://github.com/huggingface/peft.git@42a184f &&\r\
    \npip install -U git+https://github.com/huggingface/accelerate.git@c9fbb71 &&\r\
    \npip install einops==0.6.1 &&\r\npip install torch==2.0.1 &&\r\npip install bitsandbytes==0.39.0\
    \ &&\r\npip install scipy &&\r\npip install loralib==0.1.1 && \\"
  created_at: 2023-09-15 11:44:34+00:00
  edited: false
  hidden: false
  id: 650451b257cc1e5d7536cd05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e2d63b72054c55c7b41119af5f6e1c9.svg
      fullname: Andrew
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HormyAJP
      type: user
    createdAt: '2023-09-18T02:37:37.000Z'
    data:
      edited: false
      editors:
      - HormyAJP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6426425576210022
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e2d63b72054c55c7b41119af5f6e1c9.svg
          fullname: Andrew
          isHf: false
          isPro: false
          name: HormyAJP
          type: user
        html: "<p>I'm also having similar problems. I just used the example code from:\
          \ <a href=\"https://huggingface.co/tiiuae/falcon-7b\">https://huggingface.co/tiiuae/falcon-7b</a>.\
          \ I get the following error:</p>\n<pre><code>2023-09-18 10:34:37.337197:\
          \ I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary\
          \ is optimized to use available CPU instructions in performance-critical\
          \ operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI\
          \ FMA, in other operations, rebuild TensorFlow with the appropriate compiler\
          \ flags.\nTraceback (most recent call last):\n  File \"/Users/badger/dev/transformers/falcoln7b.py\"\
          , line 8, in &lt;module&gt;\n    pipeline = transformers.pipeline(\n  File\
          \ \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/__init__.py\"\
          , line 824, in pipeline\n    framework, model = infer_framework_load_model(\n\
          \  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 282, in infer_framework_load_model\n    raise ValueError(\nValueError:\
          \ Could not load model tiiuae/falcon-7b with any of the following classes:\
          \ (&lt;class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'&gt;).\
          \ See the original errors:\n\nwhile loading with AutoModelForCausalLM, an\
          \ error is thrown:\nTraceback (most recent call last):\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model,\
          \ **kwargs)\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 558, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3175, in from_pretrained\n    ) = cls._load_pretrained_model(\n \
          \ File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3296, in _load_pretrained_model\n    raise ValueError(\nValueError:\
          \ The current `device_map` had weights offloaded to the disk. Please provide\
          \ an `offload_folder` for them. Alternatively, make sure you have `safetensors`\
          \ installed if the model you are using offers the weights in this format.\n\
          \nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback\
          \ (most recent call last):\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model,\
          \ **kwargs)\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized\
          \ configuration class &lt;class 'transformers_modules.tiiuae.falcon-7b.f7796529e36b2d49094450fb038cc7c4c86afa44.configuration_RW.RWConfig'&gt;\
          \ for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should\
          \ be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config,\
          \ GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig,\
          \ RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig,\
          \ XLMConfig, XLMRobertaConfig, XLNetConfig.\n</code></pre>\n<p>Note that\
          \ I've added <code>offload_folder=\"offload\"</code> to the <code>AutoTokenizer.from_pretrained</code>\
          \ call and that doesn't get rid of the <code>offload_folder</code> error.\
          \ </p>\n<p>It's hard to tell from the error which issue is causing which.\
          \ I suspect one of the errors is likely spurious and is simply a knock on\
          \ effect of the other error.</p>\n<p>Anyone any ideas?</p>\n"
        raw: "I'm also having similar problems. I just used the example code from:\
          \ https://huggingface.co/tiiuae/falcon-7b. I get the following error:\n\n\
          ```\n2023-09-18 10:34:37.337197: I tensorflow/core/platform/cpu_feature_guard.cc:182]\
          \ This TensorFlow binary is optimized to use available CPU instructions\
          \ in performance-critical operations.\nTo enable the following instructions:\
          \ AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow\
          \ with the appropriate compiler flags.\nTraceback (most recent call last):\n\
          \  File \"/Users/badger/dev/transformers/falcoln7b.py\", line 8, in <module>\n\
          \    pipeline = transformers.pipeline(\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/__init__.py\"\
          , line 824, in pipeline\n    framework, model = infer_framework_load_model(\n\
          \  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 282, in infer_framework_load_model\n    raise ValueError(\nValueError:\
          \ Could not load model tiiuae/falcon-7b with any of the following classes:\
          \ (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\
          \ See the original errors:\n\nwhile loading with AutoModelForCausalLM, an\
          \ error is thrown:\nTraceback (most recent call last):\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model,\
          \ **kwargs)\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 558, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3175, in from_pretrained\n    ) = cls._load_pretrained_model(\n \
          \ File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3296, in _load_pretrained_model\n    raise ValueError(\nValueError:\
          \ The current `device_map` had weights offloaded to the disk. Please provide\
          \ an `offload_folder` for them. Alternatively, make sure you have `safetensors`\
          \ installed if the model you are using offers the weights in this format.\n\
          \nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback\
          \ (most recent call last):\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model,\
          \ **kwargs)\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized\
          \ configuration class <class 'transformers_modules.tiiuae.falcon-7b.f7796529e36b2d49094450fb038cc7c4c86afa44.configuration_RW.RWConfig'>\
          \ for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should\
          \ be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config,\
          \ GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig,\
          \ RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig,\
          \ XLMConfig, XLMRobertaConfig, XLNetConfig.\n```\n\nNote that I've added\
          \ `offload_folder=\"offload\"` to the `AutoTokenizer.from_pretrained` call\
          \ and that doesn't get rid of the `offload_folder` error. \n\nIt's hard\
          \ to tell from the error which issue is causing which. I suspect one of\
          \ the errors is likely spurious and is simply a knock on effect of the other\
          \ error.\n\nAnyone any ideas?"
        updatedAt: '2023-09-18T02:37:37.024Z'
      numEdits: 0
      reactions: []
    id: 6507b7f19db6e2495cb32cd3
    type: comment
  author: HormyAJP
  content: "I'm also having similar problems. I just used the example code from: https://huggingface.co/tiiuae/falcon-7b.\
    \ I get the following error:\n\n```\n2023-09-18 10:34:37.337197: I tensorflow/core/platform/cpu_feature_guard.cc:182]\
    \ This TensorFlow binary is optimized to use available CPU instructions in performance-critical\
    \ operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI\
    \ FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\
    Traceback (most recent call last):\n  File \"/Users/badger/dev/transformers/falcoln7b.py\"\
    , line 8, in <module>\n    pipeline = transformers.pipeline(\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/__init__.py\"\
    , line 824, in pipeline\n    framework, model = infer_framework_load_model(\n\
    \  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 282, in infer_framework_load_model\n    raise ValueError(\nValueError:\
    \ Could not load model tiiuae/falcon-7b with any of the following classes: (<class\
    \ 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).\
    \ See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error\
    \ is thrown:\nTraceback (most recent call last):\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model,\
    \ **kwargs)\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 558, in from_pretrained\n    return model_class.from_pretrained(\n  File\
    \ \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 3175, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"\
    /Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 3296, in _load_pretrained_model\n    raise ValueError(\nValueError: The\
    \ current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\
    \ for them. Alternatively, make sure you have `safetensors` installed if the model\
    \ you are using offers the weights in this format.\n\nwhile loading with TFAutoModelForCausalLM,\
    \ an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model,\
    \ **kwargs)\n  File \"/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized\
    \ configuration class <class 'transformers_modules.tiiuae.falcon-7b.f7796529e36b2d49094450fb038cc7c4c86afa44.configuration_RW.RWConfig'>\
    \ for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one\
    \ of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig,\
    \ OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig,\
    \ RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\
    ```\n\nNote that I've added `offload_folder=\"offload\"` to the `AutoTokenizer.from_pretrained`\
    \ call and that doesn't get rid of the `offload_folder` error. \n\nIt's hard to\
    \ tell from the error which issue is causing which. I suspect one of the errors\
    \ is likely spurious and is simply a knock on effect of the other error.\n\nAnyone\
    \ any ideas?"
  created_at: 2023-09-18 01:37:37+00:00
  edited: false
  hidden: false
  id: 6507b7f19db6e2495cb32cd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e2d63b72054c55c7b41119af5f6e1c9.svg
      fullname: Andrew
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HormyAJP
      type: user
    createdAt: '2023-09-18T03:23:25.000Z'
    data:
      edited: false
      editors:
      - HormyAJP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7385011315345764
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e2d63b72054c55c7b41119af5f6e1c9.svg
          fullname: Andrew
          isHf: false
          isPro: false
          name: HormyAJP
          type: user
        html: '<p>I think the fix is simple. Just put <code>model = AutoModelForCausalLM.from_pretrained(model)</code>
          after <code>tokenizer = AutoTokenizer.from_pretrained(model, offload_folder="offload",
          torch_dtype=torch.bfloat16)</code> and that should fix it. </p>

          '
        raw: 'I think the fix is simple. Just put `model = AutoModelForCausalLM.from_pretrained(model)`
          after `tokenizer = AutoTokenizer.from_pretrained(model, offload_folder="offload",
          torch_dtype=torch.bfloat16)` and that should fix it. '
        updatedAt: '2023-09-18T03:23:25.691Z'
      numEdits: 0
      reactions: []
    id: 6507c2ad5aa2ad0324f2b1fe
    type: comment
  author: HormyAJP
  content: 'I think the fix is simple. Just put `model = AutoModelForCausalLM.from_pretrained(model)`
    after `tokenizer = AutoTokenizer.from_pretrained(model, offload_folder="offload",
    torch_dtype=torch.bfloat16)` and that should fix it. '
  created_at: 2023-09-18 02:23:25+00:00
  edited: false
  hidden: false
  id: 6507c2ad5aa2ad0324f2b1fe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 80
repo_id: tiiuae/falcon-7b
repo_type: model
status: open
target_branch: null
title: 'ValueError: Unrecognized configuration class <class ''transformers_modules.falcon-7b.configuration_RW.RWConfig''>
  for this kind of AutoModel....'
