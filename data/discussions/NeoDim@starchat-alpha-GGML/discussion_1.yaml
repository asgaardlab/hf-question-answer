!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MohamedRashad
conflicting_files: null
created_at: 2023-05-16 23:31:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628885133347-6116d0584ef9fdfbf45dc4d9.jpeg?w=200&h=200&f=face
      fullname: Mohamed Rashad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MohamedRashad
      type: user
    createdAt: '2023-05-17T00:31:55.000Z'
    data:
      edited: false
      editors:
      - MohamedRashad
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628885133347-6116d0584ef9fdfbf45dc4d9.jpeg?w=200&h=200&f=face
          fullname: Mohamed Rashad
          isHf: false
          isPro: false
          name: MohamedRashad
          type: user
        html: '<p>Is there a way to load this model into gpu and use the acceleration
          benefit ?</p>

          '
        raw: Is there a way to load this model into gpu and use the acceleration benefit
          ?
        updatedAt: '2023-05-17T00:31:55.504Z'
      numEdits: 0
      reactions: []
    id: 6464207b884f2e3e1cebd38c
    type: comment
  author: MohamedRashad
  content: Is there a way to load this model into gpu and use the acceleration benefit
    ?
  created_at: 2023-05-16 23:31:55+00:00
  edited: false
  hidden: false
  id: 6464207b884f2e3e1cebd38c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-05-17T04:38:02.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>yes</p>

          '
        raw: 'yes'
        updatedAt: '2023-05-17T04:38:02.908Z'
      numEdits: 0
      reactions: []
    id: 64645a2adfacc4d097be9cac
    type: comment
  author: ehartford
  content: 'yes'
  created_at: 2023-05-17 03:38:02+00:00
  edited: false
  hidden: false
  id: 64645a2adfacc4d097be9cac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628885133347-6116d0584ef9fdfbf45dc4d9.jpeg?w=200&h=200&f=face
      fullname: Mohamed Rashad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MohamedRashad
      type: user
    createdAt: '2023-05-17T10:01:46.000Z'
    data:
      edited: false
      editors:
      - MohamedRashad
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628885133347-6116d0584ef9fdfbf45dc4d9.jpeg?w=200&h=200&f=face
          fullname: Mohamed Rashad
          isHf: false
          isPro: false
          name: MohamedRashad
          type: user
        html: '<p>How ?<br>And can i use it with something like Nvidia triton ?</p>

          '
        raw: 'How ?

          And can i use it with something like Nvidia triton ?'
        updatedAt: '2023-05-17T10:01:46.693Z'
      numEdits: 0
      reactions: []
    id: 6464a60a4855e06b9501ad62
    type: comment
  author: MohamedRashad
  content: 'How ?

    And can i use it with something like Nvidia triton ?'
  created_at: 2023-05-17 09:01:46+00:00
  edited: false
  hidden: false
  id: 6464a60a4855e06b9501ad62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6fb4c41a5080aaf44d130e0de5a2df1.svg
      fullname: Neo Dim
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: NeoDim
      type: user
    createdAt: '2023-05-17T17:59:29.000Z'
    data:
      edited: false
      editors:
      - NeoDim
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6fb4c41a5080aaf44d130e0de5a2df1.svg
          fullname: Neo Dim
          isHf: false
          isPro: false
          name: NeoDim
          type: user
        html: '<p>Looks like the inference binary should be compiled using CUDA for
          this - <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp#blas-build">https://github.com/ggerganov/llama.cpp#blas-build</a><br>But
          maybe it''s better to quantized for nvidia gpus version of this model -
          something like starchat-alpha-GPTQ. I don''t have Nvidia GPU, so I don''t
          know if this version exists or how to create it.</p>

          '
        raw: 'Looks like the inference binary should be compiled using CUDA for this
          - https://github.com/ggerganov/llama.cpp#blas-build

          But maybe it''s better to quantized for nvidia gpus version of this model
          - something like starchat-alpha-GPTQ. I don''t have Nvidia GPU, so I don''t
          know if this version exists or how to create it.'
        updatedAt: '2023-05-17T17:59:29.741Z'
      numEdits: 0
      reactions: []
    id: 646516016ceebdc7fd8c6b8a
    type: comment
  author: NeoDim
  content: 'Looks like the inference binary should be compiled using CUDA for this
    - https://github.com/ggerganov/llama.cpp#blas-build

    But maybe it''s better to quantized for nvidia gpus version of this model - something
    like starchat-alpha-GPTQ. I don''t have Nvidia GPU, so I don''t know if this version
    exists or how to create it.'
  created_at: 2023-05-17 16:59:29+00:00
  edited: false
  hidden: false
  id: 646516016ceebdc7fd8c6b8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-05-17T18:31:35.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>You can run this ggml model in llama.cpp with GPU.</p>

          '
        raw: You can run this ggml model in llama.cpp with GPU.
        updatedAt: '2023-05-17T18:31:35.852Z'
      numEdits: 0
      reactions: []
    id: 64651d876ceebdc7fd8ce4c0
    type: comment
  author: ehartford
  content: You can run this ggml model in llama.cpp with GPU.
  created_at: 2023-05-17 17:31:35+00:00
  edited: false
  hidden: false
  id: 64651d876ceebdc7fd8ce4c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6fb4c41a5080aaf44d130e0de5a2df1.svg
      fullname: Neo Dim
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: NeoDim
      type: user
    createdAt: '2023-05-17T18:48:01.000Z'
    data:
      edited: true
      editors:
      - NeoDim
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6fb4c41a5080aaf44d130e0de5a2df1.svg
          fullname: Neo Dim
          isHf: false
          isPro: false
          name: NeoDim
          type: user
        html: '<p>I don''t think this model can be run by llama.cpp just yet - <a
          rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/1441">https://github.com/ggerganov/llama.cpp/issues/1441</a></p>

          <p>For now there is only example code here - <a rel="nofollow" href="https://github.com/ggerganov/ggml/tree/master/examples/starcoder">https://github.com/ggerganov/ggml/tree/master/examples/starcoder</a></p>

          <p>This code works, but not very useful: it loads model, generates reply
          to single prompt and shutting down. Now I keep experimenting with this code
          to get conversation loop, but have troubles with it - looks like I didn''t
          get how to correctly manage memory. It breaks after single iteration of
          loop with "not enough memory in context". Will see if I can do better.</p>

          '
        raw: 'I don''t think this model can be run by llama.cpp just yet - https://github.com/ggerganov/llama.cpp/issues/1441


          For now there is only example code here - https://github.com/ggerganov/ggml/tree/master/examples/starcoder


          This code works, but not very useful: it loads model, generates reply to
          single prompt and shutting down. Now I keep experimenting with this code
          to get conversation loop, but have troubles with it - looks like I didn''t
          get how to correctly manage memory. It breaks after single iteration of
          loop with "not enough memory in context". Will see if I can do better.'
        updatedAt: '2023-05-17T18:48:38.847Z'
      numEdits: 1
      reactions: []
    id: 64652161e8e31202cb504da8
    type: comment
  author: NeoDim
  content: 'I don''t think this model can be run by llama.cpp just yet - https://github.com/ggerganov/llama.cpp/issues/1441


    For now there is only example code here - https://github.com/ggerganov/ggml/tree/master/examples/starcoder


    This code works, but not very useful: it loads model, generates reply to single
    prompt and shutting down. Now I keep experimenting with this code to get conversation
    loop, but have troubles with it - looks like I didn''t get how to correctly manage
    memory. It breaks after single iteration of loop with "not enough memory in context".
    Will see if I can do better.'
  created_at: 2023-05-17 17:48:01+00:00
  edited: true
  hidden: false
  id: 64652161e8e31202cb504da8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6fb4c41a5080aaf44d130e0de5a2df1.svg
      fullname: Neo Dim
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: NeoDim
      type: user
    createdAt: '2023-05-17T19:52:37.000Z'
    data:
      edited: false
      editors:
      - NeoDim
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6fb4c41a5080aaf44d130e0de5a2df1.svg
          fullname: Neo Dim
          isHf: false
          isPro: false
          name: NeoDim
          type: user
        html: '<p>Also relates - <a rel="nofollow" href="https://github.com/LostRuins/koboldcpp/issues/181">https://github.com/LostRuins/koboldcpp/issues/181</a></p>

          '
        raw: Also relates - https://github.com/LostRuins/koboldcpp/issues/181
        updatedAt: '2023-05-17T19:52:37.501Z'
      numEdits: 0
      reactions: []
    id: 64653085e8e31202cb511e3d
    type: comment
  author: NeoDim
  content: Also relates - https://github.com/LostRuins/koboldcpp/issues/181
  created_at: 2023-05-17 18:52:37+00:00
  edited: false
  hidden: false
  id: 64653085e8e31202cb511e3d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: NeoDim/starchat-alpha-GGML
repo_type: model
status: open
target_branch: null
title: Can the quantized model be loaded in gpu to have faster inference ?
