!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hartleyterw
conflicting_files: null
created_at: 2023-10-26 10:25:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2b71c92eb17af8d5edebb0f6d55f06c6.svg
      fullname: Hakurei Reimu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hartleyterw
      type: user
    createdAt: '2023-10-26T11:25:33.000Z'
    data:
      edited: false
      editors:
      - hartleyterw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6005532741546631
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2b71c92eb17af8d5edebb0f6d55f06c6.svg
          fullname: Hakurei Reimu
          isHf: false
          isPro: false
          name: hartleyterw
          type: user
        html: '<p>/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py in forward(self,
          x)<br>    815             result = F.linear(x, transpose(self.weight, self.fan_in_fan_out),
          bias=self.bias)<br>    816         elif self.r[self.active_adapter] &gt;
          0 and not self.merged:<br>--&gt; 817             result = F.linear(x, transpose(self.weight,
          self.fan_in_fan_out), bias=self.bias)<br>    818<br>    819             x
          = x.to(self.lora_A[self.active_adapter].weight.dtype)</p>

          <p>RuntimeError: self and mat2 must have the same dtype, but got Half and
          Int</p>

          <p>Using qlora and peft 0.4<br>Can''t finetune the original model because
          colab OOMs.<br>Is there any way to fix this?</p>

          '
        raw: "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py in forward(self,\
          \ x)\r\n    815             result = F.linear(x, transpose(self.weight,\
          \ self.fan_in_fan_out), bias=self.bias)\r\n    816         elif self.r[self.active_adapter]\
          \ > 0 and not self.merged:\r\n--> 817             result = F.linear(x, transpose(self.weight,\
          \ self.fan_in_fan_out), bias=self.bias)\r\n    818 \r\n    819         \
          \    x = x.to(self.lora_A[self.active_adapter].weight.dtype)\r\n\r\nRuntimeError:\
          \ self and mat2 must have the same dtype, but got Half and Int\r\n\r\nUsing\
          \ qlora and peft 0.4\r\nCan't finetune the original model because colab\
          \ OOMs.\r\nIs there any way to fix this?"
        updatedAt: '2023-10-26T11:25:33.009Z'
      numEdits: 0
      reactions: []
    id: 653a4cadacdeea08424fa6fe
    type: comment
  author: hartleyterw
  content: "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora.py in forward(self,\
    \ x)\r\n    815             result = F.linear(x, transpose(self.weight, self.fan_in_fan_out),\
    \ bias=self.bias)\r\n    816         elif self.r[self.active_adapter] > 0 and\
    \ not self.merged:\r\n--> 817             result = F.linear(x, transpose(self.weight,\
    \ self.fan_in_fan_out), bias=self.bias)\r\n    818 \r\n    819             x =\
    \ x.to(self.lora_A[self.active_adapter].weight.dtype)\r\n\r\nRuntimeError: self\
    \ and mat2 must have the same dtype, but got Half and Int\r\n\r\nUsing qlora and\
    \ peft 0.4\r\nCan't finetune the original model because colab OOMs.\r\nIs there\
    \ any way to fix this?"
  created_at: 2023-10-26 10:25:33+00:00
  edited: false
  hidden: false
  id: 653a4cadacdeea08424fa6fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8ecf5b0316733b9f7d4bf8031fef56e.svg
      fullname: goichi harada
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dahara1
      type: user
    createdAt: '2023-10-26T15:27:20.000Z'
    data:
      edited: true
      editors:
      - dahara1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9317416548728943
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8ecf5b0316733b9f7d4bf8031fef56e.svg
          fullname: goichi harada
          isHf: false
          isPro: false
          name: dahara1
          type: user
        html: '<p>Hello.</p>

          <p>It should work if I use the sample below, but for some reason it doesn''t
          work with this model.<br>I don''t know the cause yet.<br><a href="https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ/tree/main/finetune_sample">https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ/tree/main/finetune_sample</a></p>

          <p>ALMA-7B-Ja-V2 is scheduled to be released soon, so I would like to reconsider
          the quantization method at that time.</p>

          <p>Until then try the gguf version.<br><a rel="nofollow" href="https://github.com/webbigdata-jp/python_sample/blob/main/ALMA_7B_Ja_gguf_Free_Colab_sample.ipynb">https://github.com/webbigdata-jp/python_sample/blob/main/ALMA_7B_Ja_gguf_Free_Colab_sample.ipynb</a></p>

          '
        raw: 'Hello.


          It should work if I use the sample below, but for some reason it doesn''t
          work with this model.

          I don''t know the cause yet.

          https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ/tree/main/finetune_sample


          ALMA-7B-Ja-V2 is scheduled to be released soon, so I would like to reconsider
          the quantization method at that time.


          Until then try the gguf version.

          https://github.com/webbigdata-jp/python_sample/blob/main/ALMA_7B_Ja_gguf_Free_Colab_sample.ipynb'
        updatedAt: '2023-10-26T15:30:28.349Z'
      numEdits: 1
      reactions: []
    id: 653a8558b0f7bd82e0d76393
    type: comment
  author: dahara1
  content: 'Hello.


    It should work if I use the sample below, but for some reason it doesn''t work
    with this model.

    I don''t know the cause yet.

    https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ/tree/main/finetune_sample


    ALMA-7B-Ja-V2 is scheduled to be released soon, so I would like to reconsider
    the quantization method at that time.


    Until then try the gguf version.

    https://github.com/webbigdata-jp/python_sample/blob/main/ALMA_7B_Ja_gguf_Free_Colab_sample.ipynb'
  created_at: 2023-10-26 14:27:20+00:00
  edited: true
  hidden: false
  id: 653a8558b0f7bd82e0d76393
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2b71c92eb17af8d5edebb0f6d55f06c6.svg
      fullname: Hakurei Reimu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hartleyterw
      type: user
    createdAt: '2023-10-26T16:32:28.000Z'
    data:
      edited: false
      editors:
      - hartleyterw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9609358310699463
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2b71c92eb17af8d5edebb0f6d55f06c6.svg
          fullname: Hakurei Reimu
          isHf: false
          isPro: false
          name: hartleyterw
          type: user
        html: '<p>Training on cpu... I''d rather wait for the new release... :''D
          Thank you for your work.</p>

          '
        raw: Training on cpu... I'd rather wait for the new release... :'D Thank you
          for your work.
        updatedAt: '2023-10-26T16:32:28.627Z'
      numEdits: 0
      reactions: []
    id: 653a949c33574f20f7ed111b
    type: comment
  author: hartleyterw
  content: Training on cpu... I'd rather wait for the new release... :'D Thank you
    for your work.
  created_at: 2023-10-26 15:32:28+00:00
  edited: false
  hidden: false
  id: 653a949c33574f20f7ed111b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8ecf5b0316733b9f7d4bf8031fef56e.svg
      fullname: goichi harada
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dahara1
      type: user
    createdAt: '2023-11-05T09:20:38.000Z'
    data:
      edited: false
      editors:
      - dahara1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9581543803215027
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8ecf5b0316733b9f7d4bf8031fef56e.svg
          fullname: goichi harada
          isHf: false
          isPro: false
          name: dahara1
          type: user
        html: '<p>Hello.<br>It''s taken longer than I expected because I''ve remade
          it several times, but I think I''ll be able to release V2 tomorrow.</p>

          <p>As far as I can tell, it is still possible to create LoRA for GPTQ quantized
          models using axolotl.</p>

          <p>However, I have encountered a phenomenon where when I merge the created
          LoRA with the original model, the file size becomes the same as the model
          before GPTQ quantization.<br><a rel="nofollow" href="https://github.com/OpenAccess-AI-Collective/axolotl/issues/829">https://github.com/OpenAccess-AI-Collective/axolotl/issues/829</a></p>

          <p>Using huggingface/peft directly increased the file size as well, so this
          may be a current limitation on the peft side.</p>

          '
        raw: 'Hello.

          It''s taken longer than I expected because I''ve remade it several times,
          but I think I''ll be able to release V2 tomorrow.


          As far as I can tell, it is still possible to create LoRA for GPTQ quantized
          models using axolotl.


          However, I have encountered a phenomenon where when I merge the created
          LoRA with the original model, the file size becomes the same as the model
          before GPTQ quantization.

          https://github.com/OpenAccess-AI-Collective/axolotl/issues/829


          Using huggingface/peft directly increased the file size as well, so this
          may be a current limitation on the peft side.

          '
        updatedAt: '2023-11-05T09:20:38.166Z'
      numEdits: 0
      reactions: []
    id: 65475e6696c46859c2fd05fe
    type: comment
  author: dahara1
  content: 'Hello.

    It''s taken longer than I expected because I''ve remade it several times, but
    I think I''ll be able to release V2 tomorrow.


    As far as I can tell, it is still possible to create LoRA for GPTQ quantized models
    using axolotl.


    However, I have encountered a phenomenon where when I merge the created LoRA with
    the original model, the file size becomes the same as the model before GPTQ quantization.

    https://github.com/OpenAccess-AI-Collective/axolotl/issues/829


    Using huggingface/peft directly increased the file size as well, so this may be
    a current limitation on the peft side.

    '
  created_at: 2023-11-05 09:20:38+00:00
  edited: false
  hidden: false
  id: 65475e6696c46859c2fd05fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8ecf5b0316733b9f7d4bf8031fef56e.svg
      fullname: goichi harada
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dahara1
      type: user
    createdAt: '2023-12-11T02:52:28.000Z'
    data:
      edited: false
      editors:
      - dahara1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9418169260025024
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8ecf5b0316733b9f7d4bf8031fef56e.svg
          fullname: goichi harada
          isHf: false
          isPro: false
          name: dahara1
          type: user
        html: '<p>Hi.</p>

          <p>According this thread.<br><a rel="nofollow" href="https://github.com/OpenAccess-AI-Collective/axolotl/issues/829">https://github.com/OpenAccess-AI-Collective/axolotl/issues/829</a></p>

          <p>Lora fine-tuning -&gt; merge into one file -&gt; GPTQ -&gt; OK<br>GPTQ
          -&gt; fine-tuning -&gt; I can, but merging into one file is not supported.</p>

          <p>And we''re training a new version now.</p>

          '
        raw: 'Hi.


          According this thread.

          https://github.com/OpenAccess-AI-Collective/axolotl/issues/829


          Lora fine-tuning -> merge into one file -> GPTQ -> OK

          GPTQ -> fine-tuning -> I can, but merging into one file is not supported.


          And we''re training a new version now.'
        updatedAt: '2023-12-11T02:52:28.429Z'
      numEdits: 0
      reactions: []
    id: 6576796cd40e6ed3266c964b
    type: comment
  author: dahara1
  content: 'Hi.


    According this thread.

    https://github.com/OpenAccess-AI-Collective/axolotl/issues/829


    Lora fine-tuning -> merge into one file -> GPTQ -> OK

    GPTQ -> fine-tuning -> I can, but merging into one file is not supported.


    And we''re training a new version now.'
  created_at: 2023-12-11 02:52:28+00:00
  edited: false
  hidden: false
  id: 6576796cd40e6ed3266c964b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2b71c92eb17af8d5edebb0f6d55f06c6.svg
      fullname: Hakurei Reimu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hartleyterw
      type: user
    createdAt: '2023-12-12T11:58:33.000Z'
    data:
      edited: false
      editors:
      - hartleyterw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8879108428955078
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2b71c92eb17af8d5edebb0f6d55f06c6.svg
          fullname: Hakurei Reimu
          isHf: false
          isPro: false
          name: hartleyterw
          type: user
        html: '<p>Thanks for your reply.<br>I''m trying to use axolotl with the same
          config as you except I disabled bf32 and different dataset (not tokenized).
          But it''s giving me the error <code>ValueError: model_config.quantization_config
          is not set or quant_method is not set to gptq. Please make sure to point
          to a GPTQ model.</code></p>

          <p><code>!accelerate launch -m axolotl.cli.train /content/gptq.yml</code></p>

          <p>I tried with the V2 version as well, same error</p>

          '
        raw: "Thanks for your reply. \nI'm trying to use axolotl with the same config\
          \ as you except I disabled bf32 and different dataset (not tokenized). But\
          \ it's giving me the error `ValueError: model_config.quantization_config\
          \ is not set or quant_method is not set to gptq. Please make sure to point\
          \ to a GPTQ model.`\n\n`!accelerate launch -m axolotl.cli.train /content/gptq.yml`\n\
          \nI tried with the V2 version as well, same error"
        updatedAt: '2023-12-12T11:58:33.043Z'
      numEdits: 0
      reactions: []
    id: 65784ae99999746238a21414
    type: comment
  author: hartleyterw
  content: "Thanks for your reply. \nI'm trying to use axolotl with the same config\
    \ as you except I disabled bf32 and different dataset (not tokenized). But it's\
    \ giving me the error `ValueError: model_config.quantization_config is not set\
    \ or quant_method is not set to gptq. Please make sure to point to a GPTQ model.`\n\
    \n`!accelerate launch -m axolotl.cli.train /content/gptq.yml`\n\nI tried with\
    \ the V2 version as well, same error"
  created_at: 2023-12-12 11:58:33+00:00
  edited: false
  hidden: false
  id: 65784ae99999746238a21414
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8ecf5b0316733b9f7d4bf8031fef56e.svg
      fullname: goichi harada
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dahara1
      type: user
    createdAt: '2023-12-13T01:55:52.000Z'
    data:
      edited: false
      editors:
      - dahara1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9610866904258728
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8ecf5b0316733b9f7d4bf8031fef56e.svg
          fullname: goichi harada
          isHf: false
          isPro: false
          name: dahara1
          type: user
        html: '<p>Hmmm, I don''t see any errors in my environment.<br>Well, the quantize_config.json
          is included in the repository, so it could be that the model is failing
          to download for some reason.</p>

          <p>Check to see if you''re getting any out of memory or other errors prior
          to that error.</p>

          '
        raw: 'Hmmm, I don''t see any errors in my environment.

          Well, the quantize_config.json is included in the repository, so it could
          be that the model is failing to download for some reason.


          Check to see if you''re getting any out of memory or other errors prior
          to that error.'
        updatedAt: '2023-12-13T01:55:52.207Z'
      numEdits: 0
      reactions: []
    id: 65790f28e390cfd409c25cc6
    type: comment
  author: dahara1
  content: 'Hmmm, I don''t see any errors in my environment.

    Well, the quantize_config.json is included in the repository, so it could be that
    the model is failing to download for some reason.


    Check to see if you''re getting any out of memory or other errors prior to that
    error.'
  created_at: 2023-12-13 01:55:52+00:00
  edited: false
  hidden: false
  id: 65790f28e390cfd409c25cc6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: webbigdata/ALMA-7B-Ja-GPTQ-Ja-En
repo_type: model
status: open
target_branch: null
title: Error when finetuning this - fp16 vs int8?
