!!python/object:huggingface_hub.community.DiscussionWithDetails
author: whatever1983
conflicting_files: null
created_at: 2023-07-10 10:41:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b628b91320688dcf2e954c4b22d4a630.svg
      fullname: TS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: whatever1983
      type: user
    createdAt: '2023-07-10T11:41:53.000Z'
    data:
      edited: false
      editors:
      - whatever1983
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8222354650497437
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b628b91320688dcf2e954c4b22d4a630.svg
          fullname: TS
          isHf: false
          isPro: false
          name: whatever1983
          type: user
        html: '<p>I mean, metal.so is only targeted to x86 Mac OS+ Radeon GPU users.  Usability
          is low.  If you are an Apple OSX user, the accelerate framework used by
          llama.cpp and Koboldcpp is a better code path now.</p>

          <p>Windows Vulkan.so is less import, since Koboldcpp takes care of combined
          CPU+GPU inference using GGML 4bit even in a single .exe file.</p>

          <p>Linux Vulkan.so  is important.   For AMD iGPU users (7840U , where AMD''s
          official website won''t even offer windows 11 drivers, let along ROCm OpenCL
          drivers  useful for Linux Koboldcpp inference)  So MLC-LLM Vulkan path is
          the only way for 7840U(nice 8TF iGPU)</p>

          '
        raw: "I mean, metal.so is only targeted to x86 Mac OS+ Radeon GPU users. \
          \ Usability is low.  If you are an Apple OSX user, the accelerate framework\
          \ used by llama.cpp and Koboldcpp is a better code path now.\r\n\r\nWindows\
          \ Vulkan.so is less import, since Koboldcpp takes care of combined CPU+GPU\
          \ inference using GGML 4bit even in a single .exe file.\r\n\r\nLinux Vulkan.so\
          \  is important.   For AMD iGPU users (7840U , where AMD's official website\
          \ won't even offer windows 11 drivers, let along ROCm OpenCL drivers  useful\
          \ for Linux Koboldcpp inference)  So MLC-LLM Vulkan path is the only way\
          \ for 7840U(nice 8TF iGPU)"
        updatedAt: '2023-07-10T11:41:53.067Z'
      numEdits: 0
      reactions: []
    id: 64abee81141e3362ea99feb2
    type: comment
  author: whatever1983
  content: "I mean, metal.so is only targeted to x86 Mac OS+ Radeon GPU users.  Usability\
    \ is low.  If you are an Apple OSX user, the accelerate framework used by llama.cpp\
    \ and Koboldcpp is a better code path now.\r\n\r\nWindows Vulkan.so is less import,\
    \ since Koboldcpp takes care of combined CPU+GPU inference using GGML 4bit even\
    \ in a single .exe file.\r\n\r\nLinux Vulkan.so  is important.   For AMD iGPU\
    \ users (7840U , where AMD's official website won't even offer windows 11 drivers,\
    \ let along ROCm OpenCL drivers  useful for Linux Koboldcpp inference)  So MLC-LLM\
    \ Vulkan path is the only way for 7840U(nice 8TF iGPU)"
  created_at: 2023-07-10 10:41:53+00:00
  edited: false
  hidden: false
  id: 64abee81141e3362ea99feb2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b628b91320688dcf2e954c4b22d4a630.svg
      fullname: TS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: whatever1983
      type: user
    createdAt: '2023-07-10T11:53:24.000Z'
    data:
      edited: false
      editors:
      - whatever1983
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9161301255226135
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b628b91320688dcf2e954c4b22d4a630.svg
          fullname: TS
          isHf: false
          isPro: false
          name: whatever1983
          type: user
        html: '<p>I am not opposed to building Vulkan.so myself.</p>

          <p>Do you have scripts or commands that enabled you to build this ?  MLC-LLM
          documentation isn''t that great.  Want to build just the vulkan.so file,
          without requantize and reshard your model.</p>

          '
        raw: 'I am not opposed to building Vulkan.so myself.


          Do you have scripts or commands that enabled you to build this ?  MLC-LLM
          documentation isn''t that great.  Want to build just the vulkan.so file,
          without requantize and reshard your model.'
        updatedAt: '2023-07-10T11:53:24.862Z'
      numEdits: 0
      reactions: []
    id: 64abf134a934d2fc9a00d05f
    type: comment
  author: whatever1983
  content: 'I am not opposed to building Vulkan.so myself.


    Do you have scripts or commands that enabled you to build this ?  MLC-LLM documentation
    isn''t that great.  Want to build just the vulkan.so file, without requantize
    and reshard your model.'
  created_at: 2023-07-10 10:53:24+00:00
  edited: false
  hidden: false
  id: 64abf134a934d2fc9a00d05f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8e9377ae7a8010d72cd193782ceb57c9.svg
      fullname: Matt Hoffner
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: matthoffner
      type: user
    createdAt: '2023-07-10T16:29:51.000Z'
    data:
      edited: false
      editors:
      - matthoffner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5881738066673279
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8e9377ae7a8010d72cd193782ceb57c9.svg
          fullname: Matt Hoffner
          isHf: false
          isPro: true
          name: matthoffner
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;whatever1983&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/whatever1983\"\
          >@<span class=\"underline\">whatever1983</span></a></span>\n\n\t</span></span>\
          \ I'm happy to help here, but let me provide you the command. </p>\n<pre><code>python3\
          \ build.py --hf-path bigcode/gpt_bigcode-santacoder --target vulkan --quantization\
          \ q4f16_0\n</code></pre>\n<p>I followed this and guide: <a rel=\"nofollow\"\
          \ href=\"https://mlc.ai/mlc-llm/docs/compilation/compile_models.html\">https://mlc.ai/mlc-llm/docs/compilation/compile_models.html</a></p>\n"
        raw: "Hey @whatever1983 I'm happy to help here, but let me provide you the\
          \ command. \n\n```\npython3 build.py --hf-path bigcode/gpt_bigcode-santacoder\
          \ --target vulkan --quantization q4f16_0\n```\n\nI followed this and guide:\
          \ https://mlc.ai/mlc-llm/docs/compilation/compile_models.html"
        updatedAt: '2023-07-10T16:29:51.739Z'
      numEdits: 0
      reactions: []
    id: 64ac31ff5870653ec8e75405
    type: comment
  author: matthoffner
  content: "Hey @whatever1983 I'm happy to help here, but let me provide you the command.\
    \ \n\n```\npython3 build.py --hf-path bigcode/gpt_bigcode-santacoder --target\
    \ vulkan --quantization q4f16_0\n```\n\nI followed this and guide: https://mlc.ai/mlc-llm/docs/compilation/compile_models.html"
  created_at: 2023-07-10 15:29:51+00:00
  edited: false
  hidden: false
  id: 64ac31ff5870653ec8e75405
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: matthoffner/WizardCoder-mlc-llm
repo_type: model
status: open
target_branch: null
title: Very nice build, but can you also include the Linux Vulkan.so in the build?
