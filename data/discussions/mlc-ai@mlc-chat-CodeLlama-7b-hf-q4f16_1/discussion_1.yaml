!!python/object:huggingface_hub.community.DiscussionWithDetails
author: danielfl
conflicting_files: null
created_at: 2023-09-28 05:57:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a14c4cfaf025cd29bb0844/ldFRnKZpl4UDh-C22uAH5.png?w=200&h=200&f=face
      fullname: Daniel Lima
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danielfl
      type: user
    createdAt: '2023-09-28T06:57:03.000Z'
    data:
      edited: false
      editors:
      - danielfl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4270526170730591
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a14c4cfaf025cd29bb0844/ldFRnKZpl4UDh-C22uAH5.png?w=200&h=200&f=face
          fullname: Daniel Lima
          isHf: false
          isPro: false
          name: danielfl
          type: user
        html: '<p>When I try to run this model I receive:<br>$ mlc_chat_cli  --model
          mlc-chat-CodeLlama-7b-hf-q4f16_1</p>

          <p>relax_vm/builtin.cc:101: Check failed: input_shape[i] == reg (32016 vs.
          32000) : RuntimeError: ErrorContext(fn=decode, loc=param[3], param=params,
          annotation=R.Tuple(R.Tensor((32000, 512),<br>[.....]<br> shape[0] mismatch
          to specified constant.<br>Stack trace:<br>[..]</p>

          '
        raw: "When I try to run this model I receive:\r\n$ mlc_chat_cli  --model mlc-chat-CodeLlama-7b-hf-q4f16_1\r\
          \n\r\nrelax_vm/builtin.cc:101: Check failed: input_shape[i] == reg (32016\
          \ vs. 32000) : RuntimeError: ErrorContext(fn=decode, loc=param[3], param=params,\
          \ annotation=R.Tuple(R.Tensor((32000, 512), \r\n[.....]\r\n shape[0] mismatch\
          \ to specified constant.\r\nStack trace:\r\n[..]"
        updatedAt: '2023-09-28T06:57:03.862Z'
      numEdits: 0
      reactions: []
    id: 651523bfe9a10ac2479329a5
    type: comment
  author: danielfl
  content: "When I try to run this model I receive:\r\n$ mlc_chat_cli  --model mlc-chat-CodeLlama-7b-hf-q4f16_1\r\
    \n\r\nrelax_vm/builtin.cc:101: Check failed: input_shape[i] == reg (32016 vs.\
    \ 32000) : RuntimeError: ErrorContext(fn=decode, loc=param[3], param=params, annotation=R.Tuple(R.Tensor((32000,\
    \ 512), \r\n[.....]\r\n shape[0] mismatch to specified constant.\r\nStack trace:\r\
    \n[..]"
  created_at: 2023-09-28 05:57:03+00:00
  edited: false
  hidden: false
  id: 651523bfe9a10ac2479329a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57567fa8f741e2f177dbbd3a5ac8ff32.svg
      fullname: Charlie Ruan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: CharlieFRuan
      type: user
    createdAt: '2023-10-02T04:24:24.000Z'
    data:
      edited: false
      editors:
      - CharlieFRuan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8130922913551331
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57567fa8f741e2f177dbbd3a5ac8ff32.svg
          fullname: Charlie Ruan
          isHf: false
          isPro: false
          name: CharlieFRuan
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;danielfl&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/danielfl\"\
          >@<span class=\"underline\">danielfl</span></a></span>\n\n\t</span></span>\
          \ ! This issue should be fixed after the new mode libraries. Could you pull\
          \ from <a rel=\"nofollow\" href=\"https://github.com/mlc-ai/binary-mlc-llm-libs\"\
          >https://github.com/mlc-ai/binary-mlc-llm-libs</a> again?</p>\n"
        raw: Hi @danielfl ! This issue should be fixed after the new mode libraries.
          Could you pull from https://github.com/mlc-ai/binary-mlc-llm-libs again?
        updatedAt: '2023-10-02T04:24:24.398Z'
      numEdits: 0
      reactions: []
    id: 651a45f8ed0bef1df022f088
    type: comment
  author: CharlieFRuan
  content: Hi @danielfl ! This issue should be fixed after the new mode libraries.
    Could you pull from https://github.com/mlc-ai/binary-mlc-llm-libs again?
  created_at: 2023-10-02 03:24:24+00:00
  edited: false
  hidden: false
  id: 651a45f8ed0bef1df022f088
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mlc-ai/mlc-chat-CodeLlama-7b-hf-q4f16_1
repo_type: model
status: open
target_branch: null
title: 'Check failed: input_shape'
