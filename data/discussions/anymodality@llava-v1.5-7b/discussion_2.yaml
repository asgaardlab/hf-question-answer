!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MetaSkills
conflicting_files: null
created_at: 2023-10-25 03:14:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1uhbC02C61vRVZ6boso6w.jpeg?w=200&h=200&f=face
      fullname: Ken Collins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MetaSkills
      type: user
    createdAt: '2023-10-25T04:14:38.000Z'
    data:
      edited: true
      editors:
      - MetaSkills
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6897773742675781
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1uhbC02C61vRVZ6boso6w.jpeg?w=200&h=200&f=face
          fullname: Ken Collins
          isHf: false
          isPro: false
          name: MetaSkills
          type: user
        html: "<p>First, thank you so much for your work here. Really looking forward\
          \ to using this as a GPT-4 Vision competitor. </p>\n<p>I am trying to test\
          \ my SageMaker Endpoint with JavaScript, code below. But no matter what\
          \ I do I always get back basic stuff like \"man standing in front of a building\"\
          \ for your test image. Code below. Thoughts?</p>\n<pre><code class=\"language-javascript\"\
          ><span class=\"hljs-keyword\">import</span> { \n  <span class=\"hljs-title\
          \ class_\">SageMakerRuntimeClient</span>, \n  <span class=\"hljs-title class_\"\
          >InvokeEndpointCommand</span> \n} <span class=\"hljs-keyword\">from</span>\
          \ <span class=\"hljs-string\">\"@aws-sdk/client-sagemaker-runtime\"</span>;\n\
          <span class=\"hljs-keyword\">const</span> client = <span class=\"hljs-keyword\"\
          >new</span> <span class=\"hljs-title class_\">SageMakerRuntimeClient</span>({\
          \ <span class=\"hljs-attr\">region</span>: <span class=\"hljs-string\">\"\
          us-east-1\"</span> });\n<span class=\"hljs-keyword\">const</span> command\
          \ = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\"\
          >InvokeEndpointCommand</span>({\n  <span class=\"hljs-title class_\">Body</span>:\
          \ <span class=\"hljs-title class_\">JSON</span>.<span class=\"hljs-title\
          \ function_\">stringify</span>({\n    <span class=\"hljs-string\">\"image\"\
          </span> : <span class=\"hljs-string\">\"https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png\"\
          </span>, \n    <span class=\"hljs-string\">\"question\"</span> : <span class=\"\
          hljs-string\">\"Describe this image\"</span>,\n  }),\n  <span class=\"hljs-title\
          \ class_\">ContentType</span>: <span class=\"hljs-string\">'application/json'</span>,\n\
          \  <span class=\"hljs-title class_\">EndpointName</span>: <span class=\"\
          hljs-string\">'huggingface-pytorch-inference-2023-10-24-22-53-21-123'</span>,\n\
          \  <span class=\"hljs-title class_\">Accept</span>: <span class=\"hljs-string\"\
          >'application/json'</span>\n});\n<span class=\"hljs-keyword\">const</span>\
          \ data = <span class=\"hljs-keyword\">await</span> client.<span class=\"\
          hljs-title function_\">send</span>(command);\n<span class=\"hljs-keyword\"\
          >const</span> decoder = <span class=\"hljs-keyword\">new</span> <span class=\"\
          hljs-title class_\">TextDecoder</span>(<span class=\"hljs-string\">'utf-8'</span>);\n\
          <span class=\"hljs-variable language_\">console</span>.<span class=\"hljs-title\
          \ function_\">log</span>(decoder.<span class=\"hljs-title function_\">decode</span>(data.<span\
          \ class=\"hljs-property\">Body</span>));\n</code></pre>\n"
        raw: "First, thank you so much for your work here. Really looking forward\
          \ to using this as a GPT-4 Vision competitor. \n\nI am trying to test my\
          \ SageMaker Endpoint with JavaScript, code below. But no matter what I do\
          \ I always get back basic stuff like \"man standing in front of a building\"\
          \ for your test image. Code below. Thoughts?\n\n```javascript\nimport {\
          \ \n  SageMakerRuntimeClient, \n  InvokeEndpointCommand \n} from \"@aws-sdk/client-sagemaker-runtime\"\
          ;\nconst client = new SageMakerRuntimeClient({ region: \"us-east-1\" });\n\
          const command = new InvokeEndpointCommand({\n  Body: JSON.stringify({\n\
          \    \"image\" : \"https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png\"\
          , \n    \"question\" : \"Describe this image\",\n  }),\n  ContentType: 'application/json',\n\
          \  EndpointName: 'huggingface-pytorch-inference-2023-10-24-22-53-21-123',\n\
          \  Accept: 'application/json'\n});\nconst data = await client.send(command);\n\
          const decoder = new TextDecoder('utf-8');\nconsole.log(decoder.decode(data.Body));\n\
          ```\n"
        updatedAt: '2023-10-25T11:24:57.365Z'
      numEdits: 1
      reactions: []
    id: 6538962e7ec35db3420d9171
    type: comment
  author: MetaSkills
  content: "First, thank you so much for your work here. Really looking forward to\
    \ using this as a GPT-4 Vision competitor. \n\nI am trying to test my SageMaker\
    \ Endpoint with JavaScript, code below. But no matter what I do I always get back\
    \ basic stuff like \"man standing in front of a building\" for your test image.\
    \ Code below. Thoughts?\n\n```javascript\nimport { \n  SageMakerRuntimeClient,\
    \ \n  InvokeEndpointCommand \n} from \"@aws-sdk/client-sagemaker-runtime\";\n\
    const client = new SageMakerRuntimeClient({ region: \"us-east-1\" });\nconst command\
    \ = new InvokeEndpointCommand({\n  Body: JSON.stringify({\n    \"image\" : \"\
    https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png\"\
    , \n    \"question\" : \"Describe this image\",\n  }),\n  ContentType: 'application/json',\n\
    \  EndpointName: 'huggingface-pytorch-inference-2023-10-24-22-53-21-123',\n  Accept:\
    \ 'application/json'\n});\nconst data = await client.send(command);\nconst decoder\
    \ = new TextDecoder('utf-8');\nconsole.log(decoder.decode(data.Body));\n```\n"
  created_at: 2023-10-25 03:14:38+00:00
  edited: true
  hidden: false
  id: 6538962e7ec35db3420d9171
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c97adf2ca56230f0c3766606a7d05bb.svg
      fullname: Tom Gou
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: liltom-eth
      type: user
    createdAt: '2023-10-25T07:25:51.000Z'
    data:
      edited: false
      editors:
      - liltom-eth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6332006454467773
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c97adf2ca56230f0c3766606a7d05bb.svg
          fullname: Tom Gou
          isHf: false
          isPro: false
          name: liltom-eth
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;MetaSkills&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/MetaSkills\">@<span class=\"\
          underline\">MetaSkills</span></a></span>\n\n\t</span></span> Thanks for\
          \ testing in javascript.<br><a href=\"https://huggingface.co/anymodality/llava-v1.5-7b/discussions/1\"\
          >https://huggingface.co/anymodality/llava-v1.5-7b/discussions/1</a> also\
          \ mentioned this issue.<br>The solution is try this from <code>deploy_llava.ipynb</code></p>\n\
          <pre><code>from llava.conversation import conv_templates, SeparatorStyle\n\
          from llava.constants import (\nDEFAULT_IMAGE_TOKEN,\nDEFAULT_IM_START_TOKEN,\n\
          DEFAULT_IM_END_TOKEN,\n)\ndef get_prompt(raw_prompt):\n    conv_mode = \"\
          llava_v1\"\n    conv = conv_templates[conv_mode].copy()\n    roles = conv.roles\n\
          \    inp = f\"{roles[0]}: {raw_prompt}\"\n    inp = (\n        DEFAULT_IM_START_TOKEN\
          \ + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + inp\n    )\n\
          \    conv.append_message(conv.roles[0], inp)\n    conv.append_message(conv.roles[1],\
          \ None)\n    prompt = conv.get_prompt()\n    stop_str = conv.sep if conv.sep_style\
          \ != SeparatorStyle.TWO else conv.sep2\n    return prompt, stop_str\n\n\
          raw_prompt = \"Describe the image and color details.\"\nprompt, stop_str\
          \ = get_prompt(raw_prompt)\nimage_path = \"https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png\"\
          \ndata = {\"image\" : image_path, \"question\" : prompt, \"stop_str\" :\
          \ stop_str}\noutput = predictor.predict(data)\nprint(output)\n# The image\
          \ features a red toy animal, possibly a horse or a donkey, with a pair of\
          \ glasses on its face.\n</code></pre>\n<p>This helps processing input raw\
          \ prompt to llava format. And results looks good to me.</p>\n<p>Since you\
          \ are using JavaScript, a solution is to move <code>get_prompt()</code>\
          \ into <code>predict_fn() </code> from <code>code/inference.py</code> when\
          \ deploying the model.  Feel free to commit this change to the repo. I will\
          \ update the code later when I have time.</p>\n"
        raw: "@MetaSkills Thanks for testing in javascript.\nhttps://huggingface.co/anymodality/llava-v1.5-7b/discussions/1\
          \ also mentioned this issue.\nThe solution is try this from `deploy_llava.ipynb`\n\
          ```\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom\
          \ llava.constants import (\nDEFAULT_IMAGE_TOKEN,\nDEFAULT_IM_START_TOKEN,\n\
          DEFAULT_IM_END_TOKEN,\n)\ndef get_prompt(raw_prompt):\n    conv_mode = \"\
          llava_v1\"\n    conv = conv_templates[conv_mode].copy()\n    roles = conv.roles\n\
          \    inp = f\"{roles[0]}: {raw_prompt}\"\n    inp = (\n        DEFAULT_IM_START_TOKEN\
          \ + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + inp\n    )\n\
          \    conv.append_message(conv.roles[0], inp)\n    conv.append_message(conv.roles[1],\
          \ None)\n    prompt = conv.get_prompt()\n    stop_str = conv.sep if conv.sep_style\
          \ != SeparatorStyle.TWO else conv.sep2\n    return prompt, stop_str\n\n\
          raw_prompt = \"Describe the image and color details.\"\nprompt, stop_str\
          \ = get_prompt(raw_prompt)\nimage_path = \"https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png\"\
          \ndata = {\"image\" : image_path, \"question\" : prompt, \"stop_str\" :\
          \ stop_str}\noutput = predictor.predict(data)\nprint(output)\n# The image\
          \ features a red toy animal, possibly a horse or a donkey, with a pair of\
          \ glasses on its face.\n```\nThis helps processing input raw prompt to llava\
          \ format. And results looks good to me.\n\nSince you are using JavaScript,\
          \ a solution is to move `get_prompt()` into `predict_fn() ` from `code/inference.py`\
          \ when deploying the model.  Feel free to commit this change to the repo.\
          \ I will update the code later when I have time."
        updatedAt: '2023-10-25T07:25:51.577Z'
      numEdits: 0
      reactions: []
    id: 6538c2ffb81c2790a3e2e287
    type: comment
  author: liltom-eth
  content: "@MetaSkills Thanks for testing in javascript.\nhttps://huggingface.co/anymodality/llava-v1.5-7b/discussions/1\
    \ also mentioned this issue.\nThe solution is try this from `deploy_llava.ipynb`\n\
    ```\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.constants\
    \ import (\nDEFAULT_IMAGE_TOKEN,\nDEFAULT_IM_START_TOKEN,\nDEFAULT_IM_END_TOKEN,\n\
    )\ndef get_prompt(raw_prompt):\n    conv_mode = \"llava_v1\"\n    conv = conv_templates[conv_mode].copy()\n\
    \    roles = conv.roles\n    inp = f\"{roles[0]}: {raw_prompt}\"\n    inp = (\n\
    \        DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN +\
    \ \"\\n\" + inp\n    )\n    conv.append_message(conv.roles[0], inp)\n    conv.append_message(conv.roles[1],\
    \ None)\n    prompt = conv.get_prompt()\n    stop_str = conv.sep if conv.sep_style\
    \ != SeparatorStyle.TWO else conv.sep2\n    return prompt, stop_str\n\nraw_prompt\
    \ = \"Describe the image and color details.\"\nprompt, stop_str = get_prompt(raw_prompt)\n\
    image_path = \"https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png\"\
    \ndata = {\"image\" : image_path, \"question\" : prompt, \"stop_str\" : stop_str}\n\
    output = predictor.predict(data)\nprint(output)\n# The image features a red toy\
    \ animal, possibly a horse or a donkey, with a pair of glasses on its face.\n\
    ```\nThis helps processing input raw prompt to llava format. And results looks\
    \ good to me.\n\nSince you are using JavaScript, a solution is to move `get_prompt()`\
    \ into `predict_fn() ` from `code/inference.py` when deploying the model.  Feel\
    \ free to commit this change to the repo. I will update the code later when I\
    \ have time."
  created_at: 2023-10-25 06:25:51+00:00
  edited: false
  hidden: false
  id: 6538c2ffb81c2790a3e2e287
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1uhbC02C61vRVZ6boso6w.jpeg?w=200&h=200&f=face
      fullname: Ken Collins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MetaSkills
      type: user
    createdAt: '2023-10-26T02:29:16.000Z'
    data:
      edited: false
      editors:
      - MetaSkills
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9438234567642212
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1uhbC02C61vRVZ6boso6w.jpeg?w=200&h=200&f=face
          fullname: Ken Collins
          isHf: false
          isPro: false
          name: MetaSkills
          type: user
        html: '<p>Right, the idea is I want to hit this SageMaker Endpoint via some
          other workload, Lambda, EC2, K8s, etc. So could you share what the final
          <code>code/inference.py</code> would look like? Not sue what you mean by
          move.</p>

          '
        raw: Right, the idea is I want to hit this SageMaker Endpoint via some other
          workload, Lambda, EC2, K8s, etc. So could you share what the final `code/inference.py`
          would look like? Not sue what you mean by move.
        updatedAt: '2023-10-26T02:29:16.537Z'
      numEdits: 0
      reactions: []
    id: 6539cefc7ba797097a918f2b
    type: comment
  author: MetaSkills
  content: Right, the idea is I want to hit this SageMaker Endpoint via some other
    workload, Lambda, EC2, K8s, etc. So could you share what the final `code/inference.py`
    would look like? Not sue what you mean by move.
  created_at: 2023-10-26 01:29:16+00:00
  edited: false
  hidden: false
  id: 6539cefc7ba797097a918f2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1uhbC02C61vRVZ6boso6w.jpeg?w=200&h=200&f=face
      fullname: Ken Collins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MetaSkills
      type: user
    createdAt: '2023-10-26T20:27:39.000Z'
    data:
      edited: false
      editors:
      - MetaSkills
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.37150144577026367
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1uhbC02C61vRVZ6boso6w.jpeg?w=200&h=200&f=face
          fullname: Ken Collins
          isHf: false
          isPro: false
          name: MetaSkills
          type: user
        html: "<p>Ended up with a <code>code/inference.py</code> that looks like this\
          \ below. But working thru this error now:</p>\n<p>RuntimeError: Internal:\
          \ src/sentencepiece_processor.cc(1101) [model_proto-&gt;ParseFromArray(serialized.data(),\
          \ serialized.size())] </p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> requests\n<span class=\"hljs-keyword\">from</span>\
          \ PIL <span class=\"hljs-keyword\">import</span> Image\n<span class=\"hljs-keyword\"\
          >from</span> io <span class=\"hljs-keyword\">import</span> BytesIO\n<span\
          \ class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\n\
          \n<span class=\"hljs-keyword\">from</span> llava.model <span class=\"hljs-keyword\"\
          >import</span> LlavaLlamaForCausalLM\n<span class=\"hljs-keyword\">from</span>\
          \ llava.utils <span class=\"hljs-keyword\">import</span> disable_torch_init\n\
          <span class=\"hljs-keyword\">from</span> llava.mm_utils <span class=\"hljs-keyword\"\
          >import</span> tokenizer_image_token, KeywordsStoppingCriteria\n\n<span\
          \ class=\"hljs-keyword\">from</span> llava.conversation <span class=\"hljs-keyword\"\
          >import</span> conv_templates, SeparatorStyle\n<span class=\"hljs-keyword\"\
          >from</span> llava.constants <span class=\"hljs-keyword\">import</span>\
          \ (\n    IMAGE_TOKEN_INDEX,\n    DEFAULT_IMAGE_TOKEN,\n    DEFAULT_IM_START_TOKEN,\n\
          \    DEFAULT_IM_END_TOKEN,\n)\n\n\n<span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">model_fn</span>(<span class=\"hljs-params\"\
          >model_dir</span>):\n    kwargs = {<span class=\"hljs-string\">\"device_map\"\
          </span>: <span class=\"hljs-string\">\"auto\"</span>}\n    kwargs[<span\
          \ class=\"hljs-string\">\"torch_dtype\"</span>] = torch.float16\n    model\
          \ = LlavaLlamaForCausalLM.from_pretrained(\n        model_dir, low_cpu_mem_usage=<span\
          \ class=\"hljs-literal\">True</span>, **kwargs\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_dir,\
          \ use_fast=<span class=\"hljs-literal\">False</span>)\n\n    vision_tower\
          \ = model.get_vision_tower()\n    <span class=\"hljs-keyword\">if</span>\
          \ <span class=\"hljs-keyword\">not</span> vision_tower.is_loaded:\n    \
          \    vision_tower.load_model()\n    vision_tower.to(device=<span class=\"\
          hljs-string\">\"cuda\"</span>, dtype=torch.float16)\n    image_processor\
          \ = vision_tower.image_processor\n    <span class=\"hljs-keyword\">return</span>\
          \ model, tokenizer, image_processor\n\n\n<span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">predict_fn</span>(<span class=\"\
          hljs-params\">data, model_and_tokenizer</span>):\n    <span class=\"hljs-comment\"\
          ># unpack model and tokenizer</span>\n    model, tokenizer, image_processor\
          \ = model_and_tokenizer\n\n    <span class=\"hljs-comment\"># get prompt\
          \ &amp; parameters</span>\n    image_file = data.pop(<span class=\"hljs-string\"\
          >\"image\"</span>, data)\n    raw_prompt = data.pop(<span class=\"hljs-string\"\
          >\"question\"</span>, data)\n    max_new_tokens = data.pop(<span class=\"\
          hljs-string\">\"max_new_tokens\"</span>, <span class=\"hljs-number\">1024</span>)\n\
          \    temperature = data.pop(<span class=\"hljs-string\">\"temperature\"\
          </span>, <span class=\"hljs-number\">0.2</span>)\n\n    conv_mode = <span\
          \ class=\"hljs-string\">\"llava_v1\"</span>\n    conv = conv_templates[conv_mode].copy()\n\
          \    roles = conv.roles\n    inp = <span class=\"hljs-string\">f\"<span\
          \ class=\"hljs-subst\">{roles[<span class=\"hljs-number\">0</span>]}</span>:\
          \ <span class=\"hljs-subst\">{raw_prompt}</span>\"</span>\n    inp = (\n\
          \        DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\
          \ + <span class=\"hljs-string\">\"\\n\"</span> + inp\n    )\n    conv.append_message(conv.roles[<span\
          \ class=\"hljs-number\">0</span>], inp)\n    conv.append_message(conv.roles[<span\
          \ class=\"hljs-number\">1</span>], <span class=\"hljs-literal\">None</span>)\n\
          \    prompt = conv.get_prompt()\n    stop_str = conv.sep <span class=\"\
          hljs-keyword\">if</span> conv.sep_style != SeparatorStyle.TWO <span class=\"\
          hljs-keyword\">else</span> conv.sep2\n\n    <span class=\"hljs-keyword\"\
          >if</span> image_file.startswith(<span class=\"hljs-string\">\"http\"</span>)\
          \ <span class=\"hljs-keyword\">or</span> image_file.startswith(<span class=\"\
          hljs-string\">\"https\"</span>):\n        response = requests.get(image_file)\n\
          \        image = Image.<span class=\"hljs-built_in\">open</span>(BytesIO(response.content)).convert(<span\
          \ class=\"hljs-string\">\"RGB\"</span>)\n    <span class=\"hljs-keyword\"\
          >else</span>:\n        image = Image.<span class=\"hljs-built_in\">open</span>(image_file).convert(<span\
          \ class=\"hljs-string\">\"RGB\"</span>)\n\n    disable_torch_init()\n  \
          \  image_tensor = (\n        image_processor.preprocess(image, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>)[<span class=\"hljs-string\">\"pixel_values\"\
          </span>]\n        .half()\n        .cuda()\n    )\n\n    keywords = [stop_str]\n\
          \    input_ids = (\n        tokenizer_image_token(\n            prompt,\
          \ tokenizer, IMAGE_TOKEN_INDEX, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>)\n        .unsqueeze(<span class=\"hljs-number\">0</span>)\n\
          \        .cuda()\n    )\n    stopping_criteria = KeywordsStoppingCriteria(\n\
          \        keywords, tokenizer, input_ids)\n    <span class=\"hljs-keyword\"\
          >with</span> torch.inference_mode():\n        output_ids = model.generate(\n\
          \            input_ids,\n            images=image_tensor,\n            do_sample=<span\
          \ class=\"hljs-literal\">True</span>,\n            temperature=temperature,\n\
          \            max_new_tokens=max_new_tokens,\n            use_cache=<span\
          \ class=\"hljs-literal\">True</span>,\n            stopping_criteria=[stopping_criteria],\n\
          \        )\n    outputs = tokenizer.decode(\n        output_ids[<span class=\"\
          hljs-number\">0</span>, input_ids.shape[<span class=\"hljs-number\">1</span>]:],\
          \ skip_special_tokens=<span class=\"hljs-literal\">True</span>\n    ).strip()\n\
          \    <span class=\"hljs-keyword\">return</span> outputs\n</code></pre>\n"
        raw: "Ended up with a `code/inference.py` that looks like this below. But\
          \ working thru this error now:\n\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101)\
          \ [model_proto->ParseFromArray(serialized.data(), serialized.size())] \n\
          \n```python\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\
          import torch\nfrom transformers import AutoTokenizer\n\nfrom llava.model\
          \ import LlavaLlamaForCausalLM\nfrom llava.utils import disable_torch_init\n\
          from llava.mm_utils import tokenizer_image_token, KeywordsStoppingCriteria\n\
          \nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.constants\
          \ import (\n    IMAGE_TOKEN_INDEX,\n    DEFAULT_IMAGE_TOKEN,\n    DEFAULT_IM_START_TOKEN,\n\
          \    DEFAULT_IM_END_TOKEN,\n)\n\n\ndef model_fn(model_dir):\n    kwargs\
          \ = {\"device_map\": \"auto\"}\n    kwargs[\"torch_dtype\"] = torch.float16\n\
          \    model = LlavaLlamaForCausalLM.from_pretrained(\n        model_dir,\
          \ low_cpu_mem_usage=True, **kwargs\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_dir,\
          \ use_fast=False)\n\n    vision_tower = model.get_vision_tower()\n    if\
          \ not vision_tower.is_loaded:\n        vision_tower.load_model()\n    vision_tower.to(device=\"\
          cuda\", dtype=torch.float16)\n    image_processor = vision_tower.image_processor\n\
          \    return model, tokenizer, image_processor\n\n\ndef predict_fn(data,\
          \ model_and_tokenizer):\n    # unpack model and tokenizer\n    model, tokenizer,\
          \ image_processor = model_and_tokenizer\n\n    # get prompt & parameters\n\
          \    image_file = data.pop(\"image\", data)\n    raw_prompt = data.pop(\"\
          question\", data)\n    max_new_tokens = data.pop(\"max_new_tokens\", 1024)\n\
          \    temperature = data.pop(\"temperature\", 0.2)\n\n    conv_mode = \"\
          llava_v1\"\n    conv = conv_templates[conv_mode].copy()\n    roles = conv.roles\n\
          \    inp = f\"{roles[0]}: {raw_prompt}\"\n    inp = (\n        DEFAULT_IM_START_TOKEN\
          \ + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + inp\n    )\n\
          \    conv.append_message(conv.roles[0], inp)\n    conv.append_message(conv.roles[1],\
          \ None)\n    prompt = conv.get_prompt()\n    stop_str = conv.sep if conv.sep_style\
          \ != SeparatorStyle.TWO else conv.sep2\n\n    if image_file.startswith(\"\
          http\") or image_file.startswith(\"https\"):\n        response = requests.get(image_file)\n\
          \        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\
          \    else:\n        image = Image.open(image_file).convert(\"RGB\")\n\n\
          \    disable_torch_init()\n    image_tensor = (\n        image_processor.preprocess(image,\
          \ return_tensors=\"pt\")[\"pixel_values\"]\n        .half()\n        .cuda()\n\
          \    )\n\n    keywords = [stop_str]\n    input_ids = (\n        tokenizer_image_token(\n\
          \            prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\"\
          )\n        .unsqueeze(0)\n        .cuda()\n    )\n    stopping_criteria\
          \ = KeywordsStoppingCriteria(\n        keywords, tokenizer, input_ids)\n\
          \    with torch.inference_mode():\n        output_ids = model.generate(\n\
          \            input_ids,\n            images=image_tensor,\n            do_sample=True,\n\
          \            temperature=temperature,\n            max_new_tokens=max_new_tokens,\n\
          \            use_cache=True,\n            stopping_criteria=[stopping_criteria],\n\
          \        )\n    outputs = tokenizer.decode(\n        output_ids[0, input_ids.shape[1]:],\
          \ skip_special_tokens=True\n    ).strip()\n    return outputs\n```"
        updatedAt: '2023-10-26T20:27:39.994Z'
      numEdits: 0
      reactions: []
    id: 653acbbb9a2aeec4a2e29984
    type: comment
  author: MetaSkills
  content: "Ended up with a `code/inference.py` that looks like this below. But working\
    \ thru this error now:\n\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101)\
    \ [model_proto->ParseFromArray(serialized.data(), serialized.size())] \n\n```python\n\
    import requests\nfrom PIL import Image\nfrom io import BytesIO\nimport torch\n\
    from transformers import AutoTokenizer\n\nfrom llava.model import LlavaLlamaForCausalLM\n\
    from llava.utils import disable_torch_init\nfrom llava.mm_utils import tokenizer_image_token,\
    \ KeywordsStoppingCriteria\n\nfrom llava.conversation import conv_templates, SeparatorStyle\n\
    from llava.constants import (\n    IMAGE_TOKEN_INDEX,\n    DEFAULT_IMAGE_TOKEN,\n\
    \    DEFAULT_IM_START_TOKEN,\n    DEFAULT_IM_END_TOKEN,\n)\n\n\ndef model_fn(model_dir):\n\
    \    kwargs = {\"device_map\": \"auto\"}\n    kwargs[\"torch_dtype\"] = torch.float16\n\
    \    model = LlavaLlamaForCausalLM.from_pretrained(\n        model_dir, low_cpu_mem_usage=True,\
    \ **kwargs\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)\n\
    \n    vision_tower = model.get_vision_tower()\n    if not vision_tower.is_loaded:\n\
    \        vision_tower.load_model()\n    vision_tower.to(device=\"cuda\", dtype=torch.float16)\n\
    \    image_processor = vision_tower.image_processor\n    return model, tokenizer,\
    \ image_processor\n\n\ndef predict_fn(data, model_and_tokenizer):\n    # unpack\
    \ model and tokenizer\n    model, tokenizer, image_processor = model_and_tokenizer\n\
    \n    # get prompt & parameters\n    image_file = data.pop(\"image\", data)\n\
    \    raw_prompt = data.pop(\"question\", data)\n    max_new_tokens = data.pop(\"\
    max_new_tokens\", 1024)\n    temperature = data.pop(\"temperature\", 0.2)\n\n\
    \    conv_mode = \"llava_v1\"\n    conv = conv_templates[conv_mode].copy()\n \
    \   roles = conv.roles\n    inp = f\"{roles[0]}: {raw_prompt}\"\n    inp = (\n\
    \        DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN +\
    \ \"\\n\" + inp\n    )\n    conv.append_message(conv.roles[0], inp)\n    conv.append_message(conv.roles[1],\
    \ None)\n    prompt = conv.get_prompt()\n    stop_str = conv.sep if conv.sep_style\
    \ != SeparatorStyle.TWO else conv.sep2\n\n    if image_file.startswith(\"http\"\
    ) or image_file.startswith(\"https\"):\n        response = requests.get(image_file)\n\
    \        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n    else:\n\
    \        image = Image.open(image_file).convert(\"RGB\")\n\n    disable_torch_init()\n\
    \    image_tensor = (\n        image_processor.preprocess(image, return_tensors=\"\
    pt\")[\"pixel_values\"]\n        .half()\n        .cuda()\n    )\n\n    keywords\
    \ = [stop_str]\n    input_ids = (\n        tokenizer_image_token(\n          \
    \  prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n        .unsqueeze(0)\n\
    \        .cuda()\n    )\n    stopping_criteria = KeywordsStoppingCriteria(\n \
    \       keywords, tokenizer, input_ids)\n    with torch.inference_mode():\n  \
    \      output_ids = model.generate(\n            input_ids,\n            images=image_tensor,\n\
    \            do_sample=True,\n            temperature=temperature,\n         \
    \   max_new_tokens=max_new_tokens,\n            use_cache=True,\n            stopping_criteria=[stopping_criteria],\n\
    \        )\n    outputs = tokenizer.decode(\n        output_ids[0, input_ids.shape[1]:],\
    \ skip_special_tokens=True\n    ).strip()\n    return outputs\n```"
  created_at: 2023-10-26 19:27:39+00:00
  edited: false
  hidden: false
  id: 653acbbb9a2aeec4a2e29984
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1uhbC02C61vRVZ6boso6w.jpeg?w=200&h=200&f=face
      fullname: Ken Collins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MetaSkills
      type: user
    createdAt: '2023-10-26T22:12:21.000Z'
    data:
      edited: false
      editors:
      - MetaSkills
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9328957796096802
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1uhbC02C61vRVZ6boso6w.jpeg?w=200&h=200&f=face
          fullname: Ken Collins
          isHf: false
          isPro: false
          name: MetaSkills
          type: user
        html: '<p>Got this working! My last error was due to the fact I had the git
          lfs pointer for the <code>tokenizer.model</code> file vs the actual file
          itself. The above <code>code/inference.py</code> is working great. Thanks
          for your help and amazing work!!!</p>

          '
        raw: Got this working! My last error was due to the fact I had the git lfs
          pointer for the `tokenizer.model` file vs the actual file itself. The above
          `code/inference.py` is working great. Thanks for your help and amazing work!!!
        updatedAt: '2023-10-26T22:12:21.452Z'
      numEdits: 0
      reactions: []
      relatedEventId: 653ae4459430762a5cb7eb52
    id: 653ae4459430762a5cb7eb4e
    type: comment
  author: MetaSkills
  content: Got this working! My last error was due to the fact I had the git lfs pointer
    for the `tokenizer.model` file vs the actual file itself. The above `code/inference.py`
    is working great. Thanks for your help and amazing work!!!
  created_at: 2023-10-26 21:12:21+00:00
  edited: false
  hidden: false
  id: 653ae4459430762a5cb7eb4e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/1uhbC02C61vRVZ6boso6w.jpeg?w=200&h=200&f=face
      fullname: Ken Collins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MetaSkills
      type: user
    createdAt: '2023-10-26T22:12:21.000Z'
    data:
      status: closed
    id: 653ae4459430762a5cb7eb52
    type: status-change
  author: MetaSkills
  created_at: 2023-10-26 21:12:21+00:00
  id: 653ae4459430762a5cb7eb52
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c97adf2ca56230f0c3766606a7d05bb.svg
      fullname: Tom Gou
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: liltom-eth
      type: user
    createdAt: '2023-10-29T02:59:56.000Z'
    data:
      edited: false
      editors:
      - liltom-eth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.750939667224884
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c97adf2ca56230f0c3766606a7d05bb.svg
          fullname: Tom Gou
          isHf: false
          isPro: false
          name: liltom-eth
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;MetaSkills&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/MetaSkills\">@<span class=\"\
          underline\">MetaSkills</span></a></span>\n\n\t</span></span> updated <code>code/inference.py</code>\
          \ and is ready for deployment!</p>\n"
        raw: '@MetaSkills updated `code/inference.py` and is ready for deployment!'
        updatedAt: '2023-10-29T02:59:56.830Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - MetaSkills
    id: 653dcaacb424289c5f591166
    type: comment
  author: liltom-eth
  content: '@MetaSkills updated `code/inference.py` and is ready for deployment!'
  created_at: 2023-10-29 01:59:56+00:00
  edited: false
  hidden: false
  id: 653dcaacb424289c5f591166
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: anymodality/llava-v1.5-7b
repo_type: model
status: closed
target_branch: null
title: Testing SageMaker Endpoint. Odd Results.
