!!python/object:huggingface_hub.community.DiscussionWithDetails
author: saransha
conflicting_files: null
created_at: 2023-10-24 18:00:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/929b6d7e700899c05845ac10e4309dfd.svg
      fullname: Saransh Agarwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saransha
      type: user
    createdAt: '2023-10-24T19:00:23.000Z'
    data:
      edited: false
      editors:
      - saransha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9295797944068909
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/929b6d7e700899c05845ac10e4309dfd.svg
          fullname: Saransh Agarwal
          isHf: false
          isPro: false
          name: saransha
          type: user
        html: '<p>Hey there,</p>

          <p>Thank you so much for creating this fork and writing these scripts for
          deployment. Super easy to follow and clean. I was able to successfully deploy
          the model however the output does not ever seem related to my image. Im
          attaching one screenshot with the worst output,  but other times it usually
          identifies a man or something along those lines.<br> I am wondering if anyone
          else has encountered this .Thank you :)</p>

          <p>Additionally, i did try to deploy the 13b parameter model to try to debug
          this. Currently getting "RuntimeError: GET was unable to find an engine
          to execute this computation" when invoking predictions but i will post updates
          here. Two possible reasons are outdated transformer on sagemaker(does not
          support 1.31.0) or my chosen instance type is too small (instance_type="ml.g5.xlarge").
          </p>

          <p>Thank you in advance!</p>

          '
        raw: "Hey there,\r\n\r\nThank you so much for creating this fork and writing\
          \ these scripts for deployment. Super easy to follow and clean. I was able\
          \ to successfully deploy the model however the output does not ever seem\
          \ related to my image. Im attaching one screenshot with the worst output,\
          \  but other times it usually identifies a man or something along those\
          \ lines.\r\n I am wondering if anyone else has encountered this .Thank you\
          \ :)\r\n\r\nAdditionally, i did try to deploy the 13b parameter model to\
          \ try to debug this. Currently getting \"RuntimeError: GET was unable to\
          \ find an engine to execute this computation\" when invoking predictions\
          \ but i will post updates here. Two possible reasons are outdated transformer\
          \ on sagemaker(does not support 1.31.0) or my chosen instance type is too\
          \ small (instance_type=\"ml.g5.xlarge\"). \r\n\r\nThank you in advance!"
        updatedAt: '2023-10-24T19:00:23.751Z'
      numEdits: 0
      reactions: []
    id: 65381447ffe3e051310be8c6
    type: comment
  author: saransha
  content: "Hey there,\r\n\r\nThank you so much for creating this fork and writing\
    \ these scripts for deployment. Super easy to follow and clean. I was able to\
    \ successfully deploy the model however the output does not ever seem related\
    \ to my image. Im attaching one screenshot with the worst output,  but other times\
    \ it usually identifies a man or something along those lines.\r\n I am wondering\
    \ if anyone else has encountered this .Thank you :)\r\n\r\nAdditionally, i did\
    \ try to deploy the 13b parameter model to try to debug this. Currently getting\
    \ \"RuntimeError: GET was unable to find an engine to execute this computation\"\
    \ when invoking predictions but i will post updates here. Two possible reasons\
    \ are outdated transformer on sagemaker(does not support 1.31.0) or my chosen\
    \ instance type is too small (instance_type=\"ml.g5.xlarge\"). \r\n\r\nThank you\
    \ in advance!"
  created_at: 2023-10-24 18:00:23+00:00
  edited: false
  hidden: false
  id: 65381447ffe3e051310be8c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c97adf2ca56230f0c3766606a7d05bb.svg
      fullname: Tom Gou
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: liltom-eth
      type: user
    createdAt: '2023-10-24T19:04:22.000Z'
    data:
      edited: true
      editors:
      - liltom-eth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46266594529151917
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c97adf2ca56230f0c3766606a7d05bb.svg
          fullname: Tom Gou
          isHf: false
          isPro: false
          name: liltom-eth
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;saransha&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/saransha\">@<span class=\"\
          underline\">saransha</span></a></span>\n\n\t</span></span> Hi thanks for\
          \ testing. To get meaningful results, you can try this from <code>deploy_llava.ipynb</code></p>\n\
          <pre><code>from llava.conversation import conv_templates, SeparatorStyle\n\
          from llava.constants import (\nDEFAULT_IMAGE_TOKEN,\nDEFAULT_IM_START_TOKEN,\n\
          DEFAULT_IM_END_TOKEN,\n)\ndef get_prompt(raw_prompt):\n    conv_mode = \"\
          llava_v1\"\n    conv = conv_templates[conv_mode].copy()\n    roles = conv.roles\n\
          \    inp = f\"{roles[0]}: {raw_prompt}\"\n    inp = (\n        DEFAULT_IM_START_TOKEN\
          \ + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + inp\n    )\n\
          \    conv.append_message(conv.roles[0], inp)\n    conv.append_message(conv.roles[1],\
          \ None)\n    prompt = conv.get_prompt()\n    stop_str = conv.sep if conv.sep_style\
          \ != SeparatorStyle.TWO else conv.sep2\n    return prompt, stop_str\n\n\
          raw_prompt = \"Describe the image and color details.\"\nprompt, stop_str\
          \ = get_prompt(raw_prompt)\nimage_path = \"https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png\"\
          \ndata = {\"image\" : image_path, \"question\" : prompt, \"stop_str\" :\
          \ stop_str}\noutput = predictor.predict(data)\nprint(output)\n# The image\
          \ features a red toy animal, possibly a horse or a donkey, with a pair of\
          \ glasses on its face.\n</code></pre>\n<p>This helps processing input raw\
          \ prompt to llava format. And results looks good to me.</p>\n<p>Also 13b\
          \ model need some larger instance with more GPU memory. </p>\n"
        raw: "@saransha Hi thanks for testing. To get meaningful results, you can\
          \ try this from `deploy_llava.ipynb`\n```\nfrom llava.conversation import\
          \ conv_templates, SeparatorStyle\nfrom llava.constants import (\nDEFAULT_IMAGE_TOKEN,\n\
          DEFAULT_IM_START_TOKEN,\nDEFAULT_IM_END_TOKEN,\n)\ndef get_prompt(raw_prompt):\n\
          \    conv_mode = \"llava_v1\"\n    conv = conv_templates[conv_mode].copy()\n\
          \    roles = conv.roles\n    inp = f\"{roles[0]}: {raw_prompt}\"\n    inp\
          \ = (\n        DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\
          \ + \"\\n\" + inp\n    )\n    conv.append_message(conv.roles[0], inp)\n\
          \    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\
          \    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n\
          \    return prompt, stop_str\n\nraw_prompt = \"Describe the image and color\
          \ details.\"\nprompt, stop_str = get_prompt(raw_prompt)\nimage_path = \"\
          https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png\"\
          \ndata = {\"image\" : image_path, \"question\" : prompt, \"stop_str\" :\
          \ stop_str}\noutput = predictor.predict(data)\nprint(output)\n# The image\
          \ features a red toy animal, possibly a horse or a donkey, with a pair of\
          \ glasses on its face.\n```\nThis helps processing input raw prompt to llava\
          \ format. And results looks good to me.\n\nAlso 13b model need some larger\
          \ instance with more GPU memory. "
        updatedAt: '2023-10-24T19:05:36.204Z'
      numEdits: 1
      reactions: []
    id: 65381536cfc110ff796fd64c
    type: comment
  author: liltom-eth
  content: "@saransha Hi thanks for testing. To get meaningful results, you can try\
    \ this from `deploy_llava.ipynb`\n```\nfrom llava.conversation import conv_templates,\
    \ SeparatorStyle\nfrom llava.constants import (\nDEFAULT_IMAGE_TOKEN,\nDEFAULT_IM_START_TOKEN,\n\
    DEFAULT_IM_END_TOKEN,\n)\ndef get_prompt(raw_prompt):\n    conv_mode = \"llava_v1\"\
    \n    conv = conv_templates[conv_mode].copy()\n    roles = conv.roles\n    inp\
    \ = f\"{roles[0]}: {raw_prompt}\"\n    inp = (\n        DEFAULT_IM_START_TOKEN\
    \ + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + inp\n    )\n    conv.append_message(conv.roles[0],\
    \ inp)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\
    \    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n\
    \    return prompt, stop_str\n\nraw_prompt = \"Describe the image and color details.\"\
    \nprompt, stop_str = get_prompt(raw_prompt)\nimage_path = \"https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png\"\
    \ndata = {\"image\" : image_path, \"question\" : prompt, \"stop_str\" : stop_str}\n\
    output = predictor.predict(data)\nprint(output)\n# The image features a red toy\
    \ animal, possibly a horse or a donkey, with a pair of glasses on its face.\n\
    ```\nThis helps processing input raw prompt to llava format. And results looks\
    \ good to me.\n\nAlso 13b model need some larger instance with more GPU memory. "
  created_at: 2023-10-24 18:04:22+00:00
  edited: true
  hidden: false
  id: 65381536cfc110ff796fd64c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/929b6d7e700899c05845ac10e4309dfd.svg
      fullname: Saransh Agarwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saransha
      type: user
    createdAt: '2023-10-24T21:17:41.000Z'
    data:
      edited: false
      editors:
      - saransha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9190524816513062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/929b6d7e700899c05845ac10e4309dfd.svg
          fullname: Saransh Agarwal
          isHf: false
          isPro: false
          name: saransha
          type: user
        html: '<p>Thank you for the quick reply. Yes the output using this function
          makes complete sense!! </p>

          <p>I will post here if i am able to deploy 13b model. Since its hard to
          find a bigger single GPU, currently crashing on multiple devices found by
          cuda errors!</p>

          '
        raw: "Thank you for the quick reply. Yes the output using this function makes\
          \ complete sense!! \n\nI will post here if i am able to deploy 13b model.\
          \ Since its hard to find a bigger single GPU, currently crashing on multiple\
          \ devices found by cuda errors!"
        updatedAt: '2023-10-24T21:17:41.496Z'
      numEdits: 0
      reactions: []
      relatedEventId: 653834757ec35db342fe03d3
    id: 653834757ec35db342fe03d1
    type: comment
  author: saransha
  content: "Thank you for the quick reply. Yes the output using this function makes\
    \ complete sense!! \n\nI will post here if i am able to deploy 13b model. Since\
    \ its hard to find a bigger single GPU, currently crashing on multiple devices\
    \ found by cuda errors!"
  created_at: 2023-10-24 20:17:41+00:00
  edited: false
  hidden: false
  id: 653834757ec35db342fe03d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/929b6d7e700899c05845ac10e4309dfd.svg
      fullname: Saransh Agarwal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: saransha
      type: user
    createdAt: '2023-10-24T21:17:41.000Z'
    data:
      status: closed
    id: 653834757ec35db342fe03d3
    type: status-change
  author: saransha
  created_at: 2023-10-24 20:17:41+00:00
  id: 653834757ec35db342fe03d3
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c97adf2ca56230f0c3766606a7d05bb.svg
      fullname: Tom Gou
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: liltom-eth
      type: user
    createdAt: '2023-10-29T03:05:49.000Z'
    data:
      edited: false
      editors:
      - liltom-eth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47648340463638306
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c97adf2ca56230f0c3766606a7d05bb.svg
          fullname: Tom Gou
          isHf: false
          isPro: false
          name: liltom-eth
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;saransha&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/saransha\">@<span class=\"\
          underline\">saransha</span></a></span>\n\n\t</span></span> now get_prompt()\
          \ inside the predict_fn() when deployment. No need to call get_prompt()\
          \ when inference.</p>\n"
        raw: '@saransha now get_prompt() inside the predict_fn() when deployment.
          No need to call get_prompt() when inference.'
        updatedAt: '2023-10-29T03:05:49.718Z'
      numEdits: 0
      reactions: []
    id: 653dcc0d6d28265c85c11165
    type: comment
  author: liltom-eth
  content: '@saransha now get_prompt() inside the predict_fn() when deployment. No
    need to call get_prompt() when inference.'
  created_at: 2023-10-29 02:05:49+00:00
  edited: false
  hidden: false
  id: 653dcc0d6d28265c85c11165
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: anymodality/llava-v1.5-7b
repo_type: model
status: closed
target_branch: null
title: Unexpected Output when testing deployment
