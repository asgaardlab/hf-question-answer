!!python/object:huggingface_hub.community.DiscussionWithDetails
author: smjain
conflicting_files: null
created_at: 2023-03-18 01:11:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
      fullname: SHashank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smjain
      type: user
    createdAt: '2023-03-18T02:11:08.000Z'
    data:
      edited: false
      editors:
      - smjain
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
          fullname: SHashank
          isHf: false
          isPro: false
          name: smjain
          type: user
        html: '<p>/usr/local/lib/python3.9/dist-packages/bitsandbytes/functional.py
          in transform(A, to_order, from_order, out, transpose, state, ld)<br>   1696<br>   1697
          def transform(A, to_order, from_order=''row'', out=None, transpose=False,
          state=None, ld=None):<br>-&gt; 1698     prev_device = pre_call(A.device)<br>   1699     if
          state is None: state = (A.shape, from_order)<br>   1700     else: from_order
          = state[1]</p>

          <p>AttributeError: ''NoneType'' object has no attribute ''device''<br>Can
          you please check.</p>

          '
        raw: "/usr/local/lib/python3.9/dist-packages/bitsandbytes/functional.py in\
          \ transform(A, to_order, from_order, out, transpose, state, ld)\r\n   1696\
          \ \r\n   1697 def transform(A, to_order, from_order='row', out=None, transpose=False,\
          \ state=None, ld=None):\r\n-> 1698     prev_device = pre_call(A.device)\r\
          \n   1699     if state is None: state = (A.shape, from_order)\r\n   1700\
          \     else: from_order = state[1]\r\n\r\nAttributeError: 'NoneType' object\
          \ has no attribute 'device'\r\nCan you please check."
        updatedAt: '2023-03-18T02:11:08.423Z'
      numEdits: 0
      reactions: []
    id: 64151dbc385a75d7790ea7fa
    type: comment
  author: smjain
  content: "/usr/local/lib/python3.9/dist-packages/bitsandbytes/functional.py in transform(A,\
    \ to_order, from_order, out, transpose, state, ld)\r\n   1696 \r\n   1697 def\
    \ transform(A, to_order, from_order='row', out=None, transpose=False, state=None,\
    \ ld=None):\r\n-> 1698     prev_device = pre_call(A.device)\r\n   1699     if\
    \ state is None: state = (A.shape, from_order)\r\n   1700     else: from_order\
    \ = state[1]\r\n\r\nAttributeError: 'NoneType' object has no attribute 'device'\r\
    \nCan you please check."
  created_at: 2023-03-18 01:11:08+00:00
  edited: false
  hidden: false
  id: 64151dbc385a75d7790ea7fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png?w=200&h=200&f=face
      fullname: Manuel Romero
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: mrm8488
      type: user
    createdAt: '2023-03-18T11:29:42.000Z'
    data:
      edited: true
      editors:
      - mrm8488
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png?w=200&h=200&f=face
          fullname: Manuel Romero
          isHf: false
          isPro: true
          name: mrm8488
          type: user
        html: "<p>Look here: <a href=\"https://huggingface.co/mrm8488/Alpacoom/discussions/3\"\
          >https://huggingface.co/mrm8488/Alpacoom/discussions/3</a> <span data-props=\"\
          {&quot;user&quot;:&quot;smjain&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/smjain\">@<span class=\"underline\">smjain</span></a></span>\n\
          \n\t</span></span></p>\n"
        raw: 'Look here: https://huggingface.co/mrm8488/Alpacoom/discussions/3 @smjain'
        updatedAt: '2023-03-18T11:41:53.505Z'
      numEdits: 1
      reactions: []
    id: 6415a0a687030915d5ac3232
    type: comment
  author: mrm8488
  content: 'Look here: https://huggingface.co/mrm8488/Alpacoom/discussions/3 @smjain'
  created_at: 2023-03-18 10:29:42+00:00
  edited: true
  hidden: false
  id: 6415a0a687030915d5ac3232
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
      fullname: SHashank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smjain
      type: user
    createdAt: '2023-03-18T11:56:50.000Z'
    data:
      edited: false
      editors:
      - smjain
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
          fullname: SHashank
          isHf: false
          isPro: false
          name: smjain
          type: user
        html: '<p>Thanks. I checked and got it working.</p>

          '
        raw: Thanks. I checked and got it working.
        updatedAt: '2023-03-18T11:56:50.586Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mrm8488
    id: 6415a702f0e3d47d0ea276f9
    type: comment
  author: smjain
  content: Thanks. I checked and got it working.
  created_at: 2023-03-18 10:56:50+00:00
  edited: false
  hidden: false
  id: 6415a702f0e3d47d0ea276f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
      fullname: SHashank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smjain
      type: user
    createdAt: '2023-03-18T13:01:00.000Z'
    data:
      edited: true
      editors:
      - smjain
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
          fullname: SHashank
          isHf: false
          isPro: false
          name: smjain
          type: user
        html: '<p>I see another issue. At each call of generate, the gpu memory keeps
          growing. Its not getting released.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1679144497462-62ac1143cae4462c0c8a1112.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/1679144497462-62ac1143cae4462c0c8a1112.png"></a></p>

          '
        raw: 'I see another issue. At each call of generate, the gpu memory keeps
          growing. Its not getting released.



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1679144497462-62ac1143cae4462c0c8a1112.png)'
        updatedAt: '2023-03-18T13:01:40.153Z'
      numEdits: 1
      reactions: []
    id: 6415b60cc2d99a3c553c6d59
    type: comment
  author: smjain
  content: 'I see another issue. At each call of generate, the gpu memory keeps growing.
    Its not getting released.



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/1679144497462-62ac1143cae4462c0c8a1112.png)'
  created_at: 2023-03-18 12:01:00+00:00
  edited: true
  hidden: false
  id: 6415b60cc2d99a3c553c6d59
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png?w=200&h=200&f=face
      fullname: Manuel Romero
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: mrm8488
      type: user
    createdAt: '2023-03-18T13:52:47.000Z'
    data:
      edited: false
      editors:
      - mrm8488
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png?w=200&h=200&f=face
          fullname: Manuel Romero
          isHf: false
          isPro: true
          name: mrm8488
          type: user
        html: "<p>cc: <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span></p>\n"
        raw: 'cc: @ybelkada'
        updatedAt: '2023-03-18T13:52:47.059Z'
      numEdits: 0
      reactions: []
    id: 6415c22f80c0e8120834eee6
    type: comment
  author: mrm8488
  content: 'cc: @ybelkada'
  created_at: 2023-03-18 12:52:47+00:00
  edited: false
  hidden: false
  id: 6415c22f80c0e8120834eee6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1616562070355-noauth.png?w=200&h=200&f=face
      fullname: spuun is trying
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spuun
      type: user
    createdAt: '2023-03-22T16:01:10.000Z'
    data:
      edited: false
      editors:
      - spuun
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1616562070355-noauth.png?w=200&h=200&f=face
          fullname: spuun is trying
          isHf: false
          isPro: false
          name: spuun
          type: user
        html: "<p>Hi!<br>I've been running this model for the past couple days, really\
          \ nice model, tysm for open-sourcing it! \U0001F60A<br>Anyway, currently\
          \ having the same issue with the VRAM usage, any development on this?<br>If\
          \ it's of any help, I don't see an increase on every call from the looks\
          \ of it, just occasionally.</p>\n"
        raw: "Hi! \nI've been running this model for the past couple days, really\
          \ nice model, tysm for open-sourcing it! \U0001F60A \nAnyway, currently\
          \ having the same issue with the VRAM usage, any development on this? \n\
          If it's of any help, I don't see an increase on every call from the looks\
          \ of it, just occasionally."
        updatedAt: '2023-03-22T16:01:10.306Z'
      numEdits: 0
      reactions: []
    id: 641b2646d42926275da3cf6b
    type: comment
  author: spuun
  content: "Hi! \nI've been running this model for the past couple days, really nice\
    \ model, tysm for open-sourcing it! \U0001F60A \nAnyway, currently having the\
    \ same issue with the VRAM usage, any development on this? \nIf it's of any help,\
    \ I don't see an increase on every call from the looks of it, just occasionally."
  created_at: 2023-03-22 15:01:10+00:00
  edited: false
  hidden: false
  id: 641b2646d42926275da3cf6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1616562070355-noauth.png?w=200&h=200&f=face
      fullname: spuun is trying
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spuun
      type: user
    createdAt: '2023-03-23T17:47:56.000Z'
    data:
      edited: false
      editors:
      - spuun
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1616562070355-noauth.png?w=200&h=200&f=face
          fullname: spuun is trying
          isHf: false
          isPro: false
          name: spuun
          type: user
        html: "<blockquote>\n<p>Hi!<br>I've been running this model for the past couple\
          \ days, really nice model, tysm for open-sourcing it! \U0001F60A<br>Anyway,\
          \ currently having the same issue with the VRAM usage, any development on\
          \ this?<br>If it's of any help, I don't see an increase on every call from\
          \ the looks of it, just occasionally.</p>\n</blockquote>\n<p>Messed around\
          \ with it today, seems like adding a </p>\n<pre><code class=\"language-python\"\
          >    torch.cuda.empty_cache()\n    <span class=\"hljs-keyword\">import</span>\
          \ gc; gc.collect()\n</code></pre>\n<p>to the generate() function helped!\
          \ :)</p>\n"
        raw: "> Hi! \n> I've been running this model for the past couple days, really\
          \ nice model, tysm for open-sourcing it! \U0001F60A \n> Anyway, currently\
          \ having the same issue with the VRAM usage, any development on this? \n\
          > If it's of any help, I don't see an increase on every call from the looks\
          \ of it, just occasionally.\n\nMessed around with it today, seems like adding\
          \ a \n```python\n    torch.cuda.empty_cache()\n    import gc; gc.collect()\n\
          ```\nto the generate() function helped! :)"
        updatedAt: '2023-03-23T17:47:56.051Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mrm8488
    id: 641c90cc3d67778aae25500c
    type: comment
  author: spuun
  content: "> Hi! \n> I've been running this model for the past couple days, really\
    \ nice model, tysm for open-sourcing it! \U0001F60A \n> Anyway, currently having\
    \ the same issue with the VRAM usage, any development on this? \n> If it's of\
    \ any help, I don't see an increase on every call from the looks of it, just occasionally.\n\
    \nMessed around with it today, seems like adding a \n```python\n    torch.cuda.empty_cache()\n\
    \    import gc; gc.collect()\n```\nto the generate() function helped! :)"
  created_at: 2023-03-23 16:47:56+00:00
  edited: false
  hidden: false
  id: 641c90cc3d67778aae25500c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png?w=200&h=200&f=face
      fullname: Manuel Romero
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: mrm8488
      type: user
    createdAt: '2023-03-23T19:00:20.000Z'
    data:
      edited: false
      editors:
      - mrm8488
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e4318d616b09a31220980d6/24rMJ_vPh3gW9ZEmj64xr.png?w=200&h=200&f=face
          fullname: Manuel Romero
          isHf: false
          isPro: true
          name: mrm8488
          type: user
        html: "<p>Thank you so much <span data-props=\"{&quot;user&quot;:&quot;spuun&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/spuun\"\
          >@<span class=\"underline\">spuun</span></a></span>\n\n\t</span></span>!\
          \ Could you share the code snippet here for the rest of folks?</p>\n"
        raw: Thank you so much @spuun! Could you share the code snippet here for the
          rest of folks?
        updatedAt: '2023-03-23T19:00:20.772Z'
      numEdits: 0
      reactions: []
    id: 641ca1c43d67778aae25ec31
    type: comment
  author: mrm8488
  content: Thank you so much @spuun! Could you share the code snippet here for the
    rest of folks?
  created_at: 2023-03-23 18:00:20+00:00
  edited: false
  hidden: false
  id: 641ca1c43d67778aae25ec31
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1616562070355-noauth.png?w=200&h=200&f=face
      fullname: spuun is trying
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spuun
      type: user
    createdAt: '2023-03-23T23:10:37.000Z'
    data:
      edited: true
      editors:
      - spuun
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1616562070355-noauth.png?w=200&h=200&f=face
          fullname: spuun is trying
          isHf: false
          isPro: false
          name: spuun
          type: user
        html: "<p>Sure! It's really just adding those calls into the function (idt\
          \ the place you put them matters tbh, they're just garbage collector calls,\
          \ added two of them just to make sure).</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >generate</span>(<span class=\"hljs-params\"></span>\n<span class=\"hljs-params\"\
          >        instruction,</span>\n<span class=\"hljs-params\">        <span\
          \ class=\"hljs-built_in\">input</span>=<span class=\"hljs-literal\">None</span>,</span>\n\
          <span class=\"hljs-params\">        temperature=<span class=\"hljs-number\"\
          >0.1</span>,</span>\n<span class=\"hljs-params\">        top_p=<span class=\"\
          hljs-number\">0.75</span>,</span>\n<span class=\"hljs-params\">        top_k=<span\
          \ class=\"hljs-number\">40</span>,</span>\n<span class=\"hljs-params\">\
          \        num_beams=<span class=\"hljs-number\">4</span>,</span>\n<span class=\"\
          hljs-params\">        **kwargs,</span>\n<span class=\"hljs-params\"></span>):\n\
          \    torch.cuda.empty_cache()\n    <span class=\"hljs-keyword\">import</span>\
          \ gc; gc.collect()\n    prompt = generate_prompt(instruction, <span class=\"\
          hljs-built_in\">input</span>)\n    inputs = tokenizer(prompt, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>)\n    input_ids = inputs[<span class=\"\
          hljs-string\">\"input_ids\"</span>].cuda()\n    generation_config = GenerationConfig(\n\
          \        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n\
          \        num_beams=num_beams,\n        **kwargs,\n    )\n    <span class=\"\
          hljs-keyword\">with</span> torch.no_grad():\n        generation_output =\
          \ model.generate(\n            input_ids=input_ids,\n            generation_config=generation_config,\n\
          \            return_dict_in_generate=<span class=\"hljs-literal\">True</span>,\n\
          \            output_scores=<span class=\"hljs-literal\">True</span>,\n \
          \           max_new_tokens=<span class=\"hljs-number\">256</span>,\n   \
          \     )\n    s = generation_output.sequences[<span class=\"hljs-number\"\
          >0</span>]\n    output = tokenizer.decode(s)\n    torch.cuda.empty_cache()\n\
          \    <span class=\"hljs-keyword\">import</span> gc; gc.collect()\n    <span\
          \ class=\"hljs-keyword\">return</span> output.split(<span class=\"hljs-string\"\
          >\"### Response:\"</span>)[<span class=\"hljs-number\">1</span>].strip().split(<span\
          \ class=\"hljs-string\">\"Below\"</span>)[<span class=\"hljs-number\">0</span>]\n\
          </code></pre>\n<p>Before:<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/605ac7c22bd7bebcc260c29e/JKQWymjSnhRJbyGHHuN97.jpeg\"\
          ><img alt=\"without-c.jpg\" src=\"https://cdn-uploads.huggingface.co/production/uploads/605ac7c22bd7bebcc260c29e/JKQWymjSnhRJbyGHHuN97.jpeg\"\
          ></a></p>\n<p>After:<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/605ac7c22bd7bebcc260c29e/T3gsUU6zy4-HeJyQw6Nvo.jpeg\"\
          ><img alt=\"with-c.jpg\" src=\"https://cdn-uploads.huggingface.co/production/uploads/605ac7c22bd7bebcc260c29e/T3gsUU6zy4-HeJyQw6Nvo.jpeg\"\
          ></a></p>\n<p>What I've found is that this seems to only occur for large\
          \ prompts, I'm not sure where the threshold is to trigger it,  but from\
          \ what I can tell the size of the prompt is really what did it.</p>\n"
        raw: "Sure! It's really just adding those calls into the function (idt the\
          \ place you put them matters tbh, they're just garbage collector calls,\
          \ added two of them just to make sure).\n```python\ndef generate(\n    \
          \    instruction,\n        input=None,\n        temperature=0.1,\n     \
          \   top_p=0.75,\n        top_k=40,\n        num_beams=4,\n        **kwargs,\n\
          ):\n    torch.cuda.empty_cache()\n    import gc; gc.collect()\n    prompt\
          \ = generate_prompt(instruction, input)\n    inputs = tokenizer(prompt,\
          \ return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].cuda()\n\
          \    generation_config = GenerationConfig(\n        temperature=temperature,\n\
          \        top_p=top_p,\n        top_k=top_k,\n        num_beams=num_beams,\n\
          \        **kwargs,\n    )\n    with torch.no_grad():\n        generation_output\
          \ = model.generate(\n            input_ids=input_ids,\n            generation_config=generation_config,\n\
          \            return_dict_in_generate=True,\n            output_scores=True,\n\
          \            max_new_tokens=256,\n        )\n    s = generation_output.sequences[0]\n\
          \    output = tokenizer.decode(s)\n    torch.cuda.empty_cache()\n    import\
          \ gc; gc.collect()\n    return output.split(\"### Response:\")[1].strip().split(\"\
          Below\")[0]\n```\n\n\nBefore:\n![without-c.jpg](https://cdn-uploads.huggingface.co/production/uploads/605ac7c22bd7bebcc260c29e/JKQWymjSnhRJbyGHHuN97.jpeg)\n\
          \nAfter:\n![with-c.jpg](https://cdn-uploads.huggingface.co/production/uploads/605ac7c22bd7bebcc260c29e/T3gsUU6zy4-HeJyQw6Nvo.jpeg)\n\
          \nWhat I've found is that this seems to only occur for large prompts, I'm\
          \ not sure where the threshold is to trigger it,  but from what I can tell\
          \ the size of the prompt is really what did it."
        updatedAt: '2023-03-24T00:19:16.700Z'
      numEdits: 1
      reactions: []
    id: 641cdc6d0d7b2cc2db0f1440
    type: comment
  author: spuun
  content: "Sure! It's really just adding those calls into the function (idt the place\
    \ you put them matters tbh, they're just garbage collector calls, added two of\
    \ them just to make sure).\n```python\ndef generate(\n        instruction,\n \
    \       input=None,\n        temperature=0.1,\n        top_p=0.75,\n        top_k=40,\n\
    \        num_beams=4,\n        **kwargs,\n):\n    torch.cuda.empty_cache()\n \
    \   import gc; gc.collect()\n    prompt = generate_prompt(instruction, input)\n\
    \    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"\
    input_ids\"].cuda()\n    generation_config = GenerationConfig(\n        temperature=temperature,\n\
    \        top_p=top_p,\n        top_k=top_k,\n        num_beams=num_beams,\n  \
    \      **kwargs,\n    )\n    with torch.no_grad():\n        generation_output\
    \ = model.generate(\n            input_ids=input_ids,\n            generation_config=generation_config,\n\
    \            return_dict_in_generate=True,\n            output_scores=True,\n\
    \            max_new_tokens=256,\n        )\n    s = generation_output.sequences[0]\n\
    \    output = tokenizer.decode(s)\n    torch.cuda.empty_cache()\n    import gc;\
    \ gc.collect()\n    return output.split(\"### Response:\")[1].strip().split(\"\
    Below\")[0]\n```\n\n\nBefore:\n![without-c.jpg](https://cdn-uploads.huggingface.co/production/uploads/605ac7c22bd7bebcc260c29e/JKQWymjSnhRJbyGHHuN97.jpeg)\n\
    \nAfter:\n![with-c.jpg](https://cdn-uploads.huggingface.co/production/uploads/605ac7c22bd7bebcc260c29e/T3gsUU6zy4-HeJyQw6Nvo.jpeg)\n\
    \nWhat I've found is that this seems to only occur for large prompts, I'm not\
    \ sure where the threshold is to trigger it,  but from what I can tell the size\
    \ of the prompt is really what did it."
  created_at: 2023-03-23 22:10:37+00:00
  edited: true
  hidden: false
  id: 641cdc6d0d7b2cc2db0f1440
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: mrm8488/Alpacoom
repo_type: model
status: open
target_branch: null
title: Great work. I had an issue running this in colab
