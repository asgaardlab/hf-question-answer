!!python/object:huggingface_hub.community.DiscussionWithDetails
author: deepak-banka
conflicting_files: null
created_at: 2023-04-08 09:11:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9104e165f5b549f468cb2ce3a5df7b4f.svg
      fullname: Deepak Banka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepak-banka
      type: user
    createdAt: '2023-04-08T10:11:24.000Z'
    data:
      edited: false
      editors:
      - deepak-banka
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9104e165f5b549f468cb2ce3a5df7b4f.svg
          fullname: Deepak Banka
          isHf: false
          isPro: false
          name: deepak-banka
          type: user
        html: '<p>RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:<br>    size
          mismatch for base_model.model.transformer.h.0.self_attention.query_key_value.lora_A.default.weight:
          copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape
          in current model is torch.Size([16, 4096]).<br>    size mismatch for base_model.model.transformer.h.0.self_attention.query_key_value.lora_B.default.weight:
          copying a param with shape torch.Size([8192, 16, 1]) from checkpoint, the
          shape in current model is torch.Size([12288, 16]).<br>    size mismatch
          for base_model.model.transformer.h.1.self_attention.query_key_value.lora_A.default.weight:
          copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape
          in current model is torch.Size([16, 4096]).<br>    size mismatch for base_model.model.transformer.h.1.self_attention.query_key_value.lora_B.default.weight:
          copying a param with shape torch.Size([8192, 16, 1]) from checkpoint, the
          shape in current model is torch.Size([12288, 16]).<br>    size mismatch
          for base_model.model.transformer.h.2.self_attention.query_key_value.lora_A.default.weight:
          copying a param </p>

          '
        raw: "RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:\r\
          \n\tsize mismatch for base_model.model.transformer.h.0.self_attention.query_key_value.lora_A.default.weight:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([16, 4096]).\r\n\tsize mismatch for\
          \ base_model.model.transformer.h.0.self_attention.query_key_value.lora_B.default.weight:\
          \ copying a param with shape torch.Size([8192, 16, 1]) from checkpoint,\
          \ the shape in current model is torch.Size([12288, 16]).\r\n\tsize mismatch\
          \ for base_model.model.transformer.h.1.self_attention.query_key_value.lora_A.default.weight:\
          \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the\
          \ shape in current model is torch.Size([16, 4096]).\r\n\tsize mismatch for\
          \ base_model.model.transformer.h.1.self_attention.query_key_value.lora_B.default.weight:\
          \ copying a param with shape torch.Size([8192, 16, 1]) from checkpoint,\
          \ the shape in current model is torch.Size([12288, 16]).\r\n\tsize mismatch\
          \ for base_model.model.transformer.h.2.self_attention.query_key_value.lora_A.default.weight:\
          \ copying a param "
        updatedAt: '2023-04-08T10:11:24.793Z'
      numEdits: 0
      reactions: []
    id: 64313dcc1e22a07ceb885bea
    type: comment
  author: deepak-banka
  content: "RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:\r\
    \n\tsize mismatch for base_model.model.transformer.h.0.self_attention.query_key_value.lora_A.default.weight:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([16, 4096]).\r\n\tsize mismatch for base_model.model.transformer.h.0.self_attention.query_key_value.lora_B.default.weight:\
    \ copying a param with shape torch.Size([8192, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([12288, 16]).\r\n\tsize mismatch for base_model.model.transformer.h.1.self_attention.query_key_value.lora_A.default.weight:\
    \ copying a param with shape torch.Size([32, 4096]) from checkpoint, the shape\
    \ in current model is torch.Size([16, 4096]).\r\n\tsize mismatch for base_model.model.transformer.h.1.self_attention.query_key_value.lora_B.default.weight:\
    \ copying a param with shape torch.Size([8192, 16, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([12288, 16]).\r\n\tsize mismatch for base_model.model.transformer.h.2.self_attention.query_key_value.lora_A.default.weight:\
    \ copying a param "
  created_at: 2023-04-08 09:11:24+00:00
  edited: false
  hidden: false
  id: 64313dcc1e22a07ceb885bea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9104e165f5b549f468cb2ce3a5df7b4f.svg
      fullname: Deepak Banka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepak-banka
      type: user
    createdAt: '2023-05-12T11:13:55.000Z'
    data:
      status: closed
    id: 645e1f73c641dfd1164893cb
    type: status-change
  author: deepak-banka
  created_at: 2023-05-12 10:13:55+00:00
  id: 645e1f73c641dfd1164893cb
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: mrm8488/Alpacoom
repo_type: model
status: closed
target_branch: null
title: Getting size mismatch error while loading the peft modal
