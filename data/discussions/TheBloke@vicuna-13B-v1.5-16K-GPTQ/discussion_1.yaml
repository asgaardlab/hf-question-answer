!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hypersniper
conflicting_files: null
created_at: 2023-08-03 13:11:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b229669d21227b914badbb/umr0ngvZ5L_Nv2nVlC-Zo.png?w=200&h=200&f=face
      fullname: Hypersniper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hypersniper
      type: user
    createdAt: '2023-08-03T14:11:05.000Z'
    data:
      edited: false
      editors:
      - Hypersniper
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.980615496635437
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b229669d21227b914badbb/umr0ngvZ5L_Nv2nVlC-Zo.png?w=200&h=200&f=face
          fullname: Hypersniper
          isHf: false
          isPro: false
          name: Hypersniper
          type: user
        html: '<p>Performance is amazing on v1.5 , even better than 1.3 with the added
          bonus of 16k limit! I''ve tried so many other models but the Vicunia seems
          to work better with tasks / chat .  Any thoughts?</p>

          '
        raw: Performance is amazing on v1.5 , even better than 1.3 with the added
          bonus of 16k limit! I've tried so many other models but the Vicunia seems
          to work better with tasks / chat .  Any thoughts?
        updatedAt: '2023-08-03T14:11:05.101Z'
      numEdits: 0
      reactions: []
    id: 64cbb5792e592905f9f9cf28
    type: comment
  author: Hypersniper
  content: Performance is amazing on v1.5 , even better than 1.3 with the added bonus
    of 16k limit! I've tried so many other models but the Vicunia seems to work better
    with tasks / chat .  Any thoughts?
  created_at: 2023-08-03 13:11:05+00:00
  edited: false
  hidden: false
  id: 64cbb5792e592905f9f9cf28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/728ef74dd1244d235386c53c5c5b1944.svg
      fullname: Louis Del Valle
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nero-dv
      type: user
    createdAt: '2023-08-10T02:56:23.000Z'
    data:
      edited: false
      editors:
      - nero-dv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9638842344284058
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/728ef74dd1244d235386c53c5c5b1944.svg
          fullname: Louis Del Valle
          isHf: false
          isPro: false
          name: nero-dv
          type: user
        html: '<p>It''s great! I''m running on an RTX 3090 with ExLamma, I''m getting
          about 33-35 tokens per second with the 4 bit quantized model using a group
          size of 32. So far the inferences have been coherent, but I''ve yet to really
          try to break it. Very good model to play with.</p>

          '
        raw: It's great! I'm running on an RTX 3090 with ExLamma, I'm getting about
          33-35 tokens per second with the 4 bit quantized model using a group size
          of 32. So far the inferences have been coherent, but I've yet to really
          try to break it. Very good model to play with.
        updatedAt: '2023-08-10T02:56:23.529Z'
      numEdits: 0
      reactions: []
    id: 64d451d76345a50bad08268f
    type: comment
  author: nero-dv
  content: It's great! I'm running on an RTX 3090 with ExLamma, I'm getting about
    33-35 tokens per second with the 4 bit quantized model using a group size of 32.
    So far the inferences have been coherent, but I've yet to really try to break
    it. Very good model to play with.
  created_at: 2023-08-10 01:56:23+00:00
  edited: false
  hidden: false
  id: 64d451d76345a50bad08268f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0af86ed23ddf6c148a5fd21932725838.svg
      fullname: Michal Kolodziej
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikolodz
      type: user
    createdAt: '2023-08-10T08:17:02.000Z'
    data:
      edited: false
      editors:
      - mikolodz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9335699677467346
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0af86ed23ddf6c148a5fd21932725838.svg
          fullname: Michal Kolodziej
          isHf: false
          isPro: false
          name: mikolodz
          type: user
        html: "<p>@Ermarrero <span data-props=\"{&quot;user&quot;:&quot;nero-dv&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nero-dv\"\
          >@<span class=\"underline\">nero-dv</span></a></span>\n\n\t</span></span>\
          \  What settings are you using for 16k tokens? I played with the ExLamma\
          \ settings, but I'm getting poor quality at higher token number. (rtx3090)</p>\n"
        raw: '@Ermarrero @nero-dv  What settings are you using for 16k tokens? I played
          with the ExLamma settings, but I''m getting poor quality at higher token
          number. (rtx3090)'
        updatedAt: '2023-08-10T08:17:02.976Z'
      numEdits: 0
      reactions: []
    id: 64d49cfe95d5127246aa22d3
    type: comment
  author: mikolodz
  content: '@Ermarrero @nero-dv  What settings are you using for 16k tokens? I played
    with the ExLamma settings, but I''m getting poor quality at higher token number.
    (rtx3090)'
  created_at: 2023-08-10 07:17:02+00:00
  edited: false
  hidden: false
  id: 64d49cfe95d5127246aa22d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e706fe7bb21a046f80895901704b204a.svg
      fullname: Jeffrey Gilbert
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Renegadesoffun
      type: user
    createdAt: '2023-08-10T08:53:35.000Z'
    data:
      edited: false
      editors:
      - Renegadesoffun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9744607210159302
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e706fe7bb21a046f80895901704b204a.svg
          fullname: Jeffrey Gilbert
          isHf: false
          isPro: false
          name: Renegadesoffun
          type: user
        html: '<p>looks like max tokens according to the card is 8k? should we stick
          to that max for better coherence?</p>

          '
        raw: 'looks like max tokens according to the card is 8k? should we stick to
          that max for better coherence?

          '
        updatedAt: '2023-08-10T08:53:35.356Z'
      numEdits: 0
      reactions: []
    id: 64d4a58fa9ba52fb71fdffe2
    type: comment
  author: Renegadesoffun
  content: 'looks like max tokens according to the card is 8k? should we stick to
    that max for better coherence?

    '
  created_at: 2023-08-10 07:53:35+00:00
  edited: false
  hidden: false
  id: 64d4a58fa9ba52fb71fdffe2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-10T09:21:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9772639870643616
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>looks like max tokens according to the card is 8k? should we stick to
          that max for better coherence?</p>

          </blockquote>

          <p>No the max is 16K.  The 8192 shown in my GPTQ table is the sequence length
          I quantised at.  I wasn''t able to quantise at 16K yet. </p>

          <p>That doesn''t mean the model is limited to 8K, it just means the accuracy
          at 16K with the quantized version won''t be quite as good as if I had been
          able to quantise at 16K.  But it will still be very usable.</p>

          <p>This is explained in the "Explanation of GPTQ parameters" section of
          the README.</p>

          '
        raw: "> looks like max tokens according to the card is 8k? should we stick\
          \ to that max for better coherence?\n\nNo the max is 16K.  The 8192 shown\
          \ in my GPTQ table is the sequence length I quantised at.  I wasn't able\
          \ to quantise at 16K yet. \n\nThat doesn't mean the model is limited to\
          \ 8K, it just means the accuracy at 16K with the quantized version won't\
          \ be quite as good as if I had been able to quantise at 16K.  But it will\
          \ still be very usable.\n\nThis is explained in the \"Explanation of GPTQ\
          \ parameters\" section of the README."
        updatedAt: '2023-08-10T09:21:33.826Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - mikolodz
        - cdgaete
    id: 64d4ac1ddc27f96d40581436
    type: comment
  author: TheBloke
  content: "> looks like max tokens according to the card is 8k? should we stick to\
    \ that max for better coherence?\n\nNo the max is 16K.  The 8192 shown in my GPTQ\
    \ table is the sequence length I quantised at.  I wasn't able to quantise at 16K\
    \ yet. \n\nThat doesn't mean the model is limited to 8K, it just means the accuracy\
    \ at 16K with the quantized version won't be quite as good as if I had been able\
    \ to quantise at 16K.  But it will still be very usable.\n\nThis is explained\
    \ in the \"Explanation of GPTQ parameters\" section of the README."
  created_at: 2023-08-10 08:21:33+00:00
  edited: false
  hidden: false
  id: 64d4ac1ddc27f96d40581436
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0af86ed23ddf6c148a5fd21932725838.svg
      fullname: Michal Kolodziej
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikolodz
      type: user
    createdAt: '2023-08-10T09:30:13.000Z'
    data:
      edited: false
      editors:
      - mikolodz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9335050582885742
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0af86ed23ddf6c148a5fd21932725838.svg
          fullname: Michal Kolodziej
          isHf: false
          isPro: false
          name: mikolodz
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span><br>Could\
          \ you possibly share the quantization command that you have used to quantize\
          \ this model?</p>\n"
        raw: "Thank you @TheBloke \nCould you possibly share the quantization command\
          \ that you have used to quantize this model?"
        updatedAt: '2023-08-10T09:30:13.037Z'
      numEdits: 0
      reactions: []
    id: 64d4ae258e7d8a2ef8609ccd
    type: comment
  author: mikolodz
  content: "Thank you @TheBloke \nCould you possibly share the quantization command\
    \ that you have used to quantize this model?"
  created_at: 2023-08-10 08:30:13+00:00
  edited: false
  hidden: false
  id: 64d4ae258e7d8a2ef8609ccd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-10T09:32:54.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7774704098701477
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I use this AutoGPTQ wrapper script which I wrote a while ago: <a
          rel="nofollow" href="https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py">https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py</a></p>

          <p>with the parameters shown in the README. To quantise at 8K, I needed
          to use <code>--cache-examples 0</code> which sets <code>cache_examples_on_gpu=False</code>
          in the AutoGPTQ <code>.quantize()</code> call. Otherwise 8K samples will
          cause out of VRAM on a 48GB GPU (which I used for these), and maybe even
          on an 80GB GPU.  This uses RAM instead of VRAM and slows the process down
          quite a bit, but allows me to quantize at a higher sequence length.</p>

          '
        raw: 'I use this AutoGPTQ wrapper script which I wrote a while ago: https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py


          with the parameters shown in the README. To quantise at 8K, I needed to
          use `--cache-examples 0` which sets `cache_examples_on_gpu=False` in the
          AutoGPTQ `.quantize()` call. Otherwise 8K samples will cause out of VRAM
          on a 48GB GPU (which I used for these), and maybe even on an 80GB GPU.  This
          uses RAM instead of VRAM and slows the process down quite a bit, but allows
          me to quantize at a higher sequence length.'
        updatedAt: '2023-08-10T09:40:58.939Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jlzhou
    id: 64d4aec6d020ad86bd435996
    type: comment
  author: TheBloke
  content: 'I use this AutoGPTQ wrapper script which I wrote a while ago: https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py


    with the parameters shown in the README. To quantise at 8K, I needed to use `--cache-examples
    0` which sets `cache_examples_on_gpu=False` in the AutoGPTQ `.quantize()` call.
    Otherwise 8K samples will cause out of VRAM on a 48GB GPU (which I used for these),
    and maybe even on an 80GB GPU.  This uses RAM instead of VRAM and slows the process
    down quite a bit, but allows me to quantize at a higher sequence length.'
  created_at: 2023-08-10 08:32:54+00:00
  edited: true
  hidden: false
  id: 64d4aec6d020ad86bd435996
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0af86ed23ddf6c148a5fd21932725838.svg
      fullname: Michal Kolodziej
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikolodz
      type: user
    createdAt: '2023-08-10T09:47:55.000Z'
    data:
      edited: false
      editors:
      - mikolodz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9737993478775024
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0af86ed23ddf6c148a5fd21932725838.svg
          fullname: Michal Kolodziej
          isHf: false
          isPro: false
          name: mikolodz
          type: user
        html: '<p>You are the best. May God bless you for your effort and commitment!</p>

          '
        raw: You are the best. May God bless you for your effort and commitment!
        updatedAt: '2023-08-10T09:47:55.573Z'
      numEdits: 0
      reactions: []
    id: 64d4b24be80e83c3e78281e5
    type: comment
  author: mikolodz
  content: You are the best. May God bless you for your effort and commitment!
  created_at: 2023-08-10 08:47:55+00:00
  edited: false
  hidden: false
  id: 64d4b24be80e83c3e78281e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7cf0183a6903f104a440cbb271baff64.svg
      fullname: Mert Yazan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mertyazan
      type: user
    createdAt: '2023-08-11T09:03:59.000Z'
    data:
      edited: false
      editors:
      - mertyazan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9184108376502991
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7cf0183a6903f104a440cbb271baff64.svg
          fullname: Mert Yazan
          isHf: false
          isPro: false
          name: mertyazan
          type: user
        html: "<p>I have a repetition problem, the same as the one mentioned here:\
          \ <a href=\"https://huggingface.co/TheBloke/vicuna-13B-v1.5-16K-GGML/discussions/1\"\
          >https://huggingface.co/TheBloke/vicuna-13B-v1.5-16K-GGML/discussions/1</a></p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> you mentioned\
          \ that this is related to rope parameters, is it the same for the GPTQ version?\
          \ How can we adjust it in this version?<br>(Thank you for the amazing work\
          \ btw, since I work with limited resources, your models are saving my life\
          \ :))</p>\n"
        raw: "I have a repetition problem, the same as the one mentioned here: https://huggingface.co/TheBloke/vicuna-13B-v1.5-16K-GGML/discussions/1\n\
          \n@TheBloke you mentioned that this is related to rope parameters, is it\
          \ the same for the GPTQ version? How can we adjust it in this version? \n\
          (Thank you for the amazing work btw, since I work with limited resources,\
          \ your models are saving my life :))"
        updatedAt: '2023-08-11T09:03:59.348Z'
      numEdits: 0
      reactions: []
    id: 64d5f97f089bc502ce903dde
    type: comment
  author: mertyazan
  content: "I have a repetition problem, the same as the one mentioned here: https://huggingface.co/TheBloke/vicuna-13B-v1.5-16K-GGML/discussions/1\n\
    \n@TheBloke you mentioned that this is related to rope parameters, is it the same\
    \ for the GPTQ version? How can we adjust it in this version? \n(Thank you for\
    \ the amazing work btw, since I work with limited resources, your models are saving\
    \ my life :))"
  created_at: 2023-08-11 08:03:59+00:00
  edited: false
  hidden: false
  id: 64d5f97f089bc502ce903dde
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/vicuna-13B-v1.5-16K-GPTQ
repo_type: model
status: open
target_branch: null
title: Performance
