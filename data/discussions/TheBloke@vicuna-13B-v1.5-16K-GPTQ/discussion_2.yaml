!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MeTRoPol7
conflicting_files: null
created_at: 2023-08-28 22:54:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef1c23bad2c843e7bf5e6d82e2922fe7.svg
      fullname: Taha Metro
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MeTRoPol7
      type: user
    createdAt: '2023-08-28T23:54:03.000Z'
    data:
      edited: false
      editors:
      - MeTRoPol7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9106764197349548
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef1c23bad2c843e7bf5e6d82e2922fe7.svg
          fullname: Taha Metro
          isHf: false
          isPro: false
          name: MeTRoPol7
          type: user
        html: '<p>exllama:</p>

          <p>Temperature:0.95<br>Top-K:off<br>Top-P:0.75<br>Min-P:off<br>Typical:0.25</p>

          <p>User:<br>Hello, How are you today?<br>Chatbot:<br>I am not sure how to
          answer that question because because because because because because because
          because because because because because because because because because
          because because because because because because because because because
          because because because because because because because because because
          because because because because because because because because because
          because because because because because because because because because
          because because because because because because because because because
          because because because because because because because because because
          because because because because because because because because because
          because because because because because because</p>

          <p>The problem does not happen in AutoGPTQ,</p>

          <p>Also I tried m-sys/FastChat with with GPTQ-for-LLaMa and this error This
          error appears:<br>File "C:\ProgramData\Miniconda\envs\cuda-env\lib\site-packages\torch\nn\modules\linear.py",
          line 114, in forward<br>    return F.linear(input, self.weight, self.bias)<br>RuntimeError:
          expected scalar type Float but found Half</p>

          <p>Any solution to set context in exllama or FastChat?</p>

          '
        raw: "exllama:\r\n\r\nTemperature:0.95\r\nTop-K:off\r\nTop-P:0.75\r\nMin-P:off\r\
          \nTypical:0.25\r\n\r\nUser:\r\nHello, How are you today?\r\nChatbot:\r\n\
          I am not sure how to answer that question because because because because\
          \ because because because because because because because because because\
          \ because because because because because because because because because\
          \ because because because because because because because because because\
          \ because because because because because because because because because\
          \ because because because because because because because because because\
          \ because because because because because because because because because\
          \ because because because because because because because because because\
          \ because because because because because because because because because\
          \ because because because because because because because because because\r\
          \n\r\nThe problem does not happen in AutoGPTQ,\r\n\r\nAlso I tried m-sys/FastChat\
          \ with with GPTQ-for-LLaMa and this error This error appears:\r\nFile \"\
          C:\\ProgramData\\Miniconda\\envs\\cuda-env\\lib\\site-packages\\torch\\\
          nn\\modules\\linear.py\", line 114, in forward\r\n    return F.linear(input,\
          \ self.weight, self.bias)\r\nRuntimeError: expected scalar type Float but\
          \ found Half\r\n\r\nAny solution to set context in exllama or FastChat?"
        updatedAt: '2023-08-28T23:54:03.275Z'
      numEdits: 0
      reactions: []
    id: 64ed339bff8f3a69b495458d
    type: comment
  author: MeTRoPol7
  content: "exllama:\r\n\r\nTemperature:0.95\r\nTop-K:off\r\nTop-P:0.75\r\nMin-P:off\r\
    \nTypical:0.25\r\n\r\nUser:\r\nHello, How are you today?\r\nChatbot:\r\nI am not\
    \ sure how to answer that question because because because because because because\
    \ because because because because because because because because because because\
    \ because because because because because because because because because because\
    \ because because because because because because because because because because\
    \ because because because because because because because because because because\
    \ because because because because because because because because because because\
    \ because because because because because because because because because because\
    \ because because because because because because because because because because\
    \ because because because because because because because because because\r\n\r\
    \nThe problem does not happen in AutoGPTQ,\r\n\r\nAlso I tried m-sys/FastChat\
    \ with with GPTQ-for-LLaMa and this error This error appears:\r\nFile \"C:\\ProgramData\\\
    Miniconda\\envs\\cuda-env\\lib\\site-packages\\torch\\nn\\modules\\linear.py\"\
    , line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\
    \nRuntimeError: expected scalar type Float but found Half\r\n\r\nAny solution\
    \ to set context in exllama or FastChat?"
  created_at: 2023-08-28 22:54:03+00:00
  edited: false
  hidden: false
  id: 64ed339bff8f3a69b495458d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ef1c23bad2c843e7bf5e6d82e2922fe7.svg
      fullname: Taha Metro
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MeTRoPol7
      type: user
    createdAt: '2023-08-29T14:49:40.000Z'
    data:
      edited: false
      editors:
      - MeTRoPol7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7938694357872009
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ef1c23bad2c843e7bf5e6d82e2922fe7.svg
          fullname: Taha Metro
          isHf: false
          isPro: false
          name: MeTRoPol7
          type: user
        html: '<p>I understand add " --alpha 4.0 " to exllama fix the problem, but
          I cant find any solution for FastChat.</p>

          '
        raw: I understand add " --alpha 4.0 " to exllama fix the problem, but I cant
          find any solution for FastChat.
        updatedAt: '2023-08-29T14:49:40.501Z'
      numEdits: 0
      reactions: []
    id: 64ee05841d7aadc1b14a7db1
    type: comment
  author: MeTRoPol7
  content: I understand add " --alpha 4.0 " to exllama fix the problem, but I cant
    find any solution for FastChat.
  created_at: 2023-08-29 13:49:40+00:00
  edited: false
  hidden: false
  id: 64ee05841d7aadc1b14a7db1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/vicuna-13B-v1.5-16K-GPTQ
repo_type: model
status: open
target_branch: null
title: Lopp at the end of sentences
