!!python/object:huggingface_hub.community.DiscussionWithDetails
author: littledot
conflicting_files: null
created_at: 2023-10-01 00:04:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9958489f224a55975d3670471c1eba91.svg
      fullname: Qiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littledot
      type: user
    createdAt: '2023-10-01T01:04:39.000Z'
    data:
      edited: false
      editors:
      - littledot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4812469482421875
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9958489f224a55975d3670471c1eba91.svg
          fullname: Qiang
          isHf: false
          isPro: false
          name: littledot
          type: user
        html: "<p>python3 -m fastchat.serve.cli --model-path /home/littledot/.cache/modelscope/hub/qwen/Qwen-14B-Chat-Int4</p>\n\
          <h2 id=\"error-info\">error info</h2>\n<p>littledot@aiserver:~$ python3\
          \ -m fastchat.serve.cli --model-path /home/littledot/.cache/modelscope/hub/qwen/Qwen-14B-Chat-Int4\
          \             Try importing flash-attention for faster inference...<br>Warning:\
          \ import flash_attn rotary fail, please install FlashAttention rotary to\
          \ get higher efficiency <a rel=\"nofollow\" href=\"https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\"\
          >https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary</a><br>Warning:\
          \ import flash_attn rms_norm fail, please install FlashAttention layer_norm\
          \ to get higher efficiency <a rel=\"nofollow\" href=\"https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\"\
          >https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm</a><br>Warning:\
          \ import flash_attn fail, please install FlashAttention to get higher efficiency\
          \ <a rel=\"nofollow\" href=\"https://github.com/Dao-AILab/flash-attention\"\
          >https://github.com/Dao-AILab/flash-attention</a><br>Loading checkpoint\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01&lt;00:00,  3.52it/s]<br>Traceback\
          \ (most recent call last):<br>  File \"/usr/lib/python3.10/runpy.py\", line\
          \ 196, in _run_module_as_main<br>    return _run_code(code, main_globals,\
          \ None,<br>  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code<br>\
          \    exec(code, run_globals)<br>  File \"/usr/local/lib/python3.10/dist-packages/fastchat/serve/cli.py\"\
          , line 283, in <br>    main(args)<br>  File \"/usr/local/lib/python3.10/dist-packages/fastchat/serve/cli.py\"\
          , line 208, in main<br>    chat_loop(<br>  File \"/usr/local/lib/python3.10/dist-packages/fastchat/serve/inference.py\"\
          , line 311, in chat_loop<br>    model, tokenizer = load_model(<br>  File\
          \ \"/usr/local/lib/python3.10/dist-packages/fastchat/model/model_adapter.py\"\
          , line 288, in load_model<br>    model, tokenizer = adapter.load_model(model_path,\
          \ kwargs)<br>  File \"/usr/local/lib/python3.10/dist-packages/fastchat/model/model_adapter.py\"\
          , line 1368, in load_model<br>    model = AutoModelForCausalLM.from_pretrained(<br>\
          \  File \"/home/littledot/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 558, in from_pretrained<br>    return model_class.from_pretrained(<br>\
          \  File \"/home/littledot/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3250, in from_pretrained<br>    model = quantizer.post_init_model(model)<br>\
          \  File \"/home/littledot/.local/lib/python3.10/site-packages/optimum/gptq/quantizer.py\"\
          , line 482, in post_init_model<br>    raise ValueError(<br>ValueError: Found\
          \ modules on cpu/disk. Using Exllama backend requires all the modules to\
          \ be on GPU.You can deactivate exllama backend by setting <code>disable_exllama=True</code>\
          \ in the quantization config object</p>\n"
        raw: "python3 -m fastchat.serve.cli --model-path /home/littledot/.cache/modelscope/hub/qwen/Qwen-14B-Chat-Int4\r\
          \n\r\nerror info\r\n---------------------------------------------------------------------------------------------------------------------------------------------------\r\
          \nlittledot@aiserver:~$ python3 -m fastchat.serve.cli --model-path /home/littledot/.cache/modelscope/hub/qwen/Qwen-14B-Chat-Int4\
          \             Try importing flash-attention for faster inference...\r\n\
          Warning: import flash_attn rotary fail, please install FlashAttention rotary\
          \ to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\r\
          \nWarning: import flash_attn rms_norm fail, please install FlashAttention\
          \ layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\r\
          \nWarning: import flash_attn fail, please install FlashAttention to get\
          \ higher efficiency https://github.com/Dao-AILab/flash-attention\r\nLoading\
          \ checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,\
          \  3.52it/s]\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\"\
          , line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals,\
          \ None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\
          \n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastchat/serve/cli.py\"\
          , line 283, in <module>\r\n    main(args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastchat/serve/cli.py\"\
          , line 208, in main\r\n    chat_loop(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastchat/serve/inference.py\"\
          , line 311, in chat_loop\r\n    model, tokenizer = load_model(\r\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/fastchat/model/model_adapter.py\"\
          , line 288, in load_model\r\n    model, tokenizer = adapter.load_model(model_path,\
          \ kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastchat/model/model_adapter.py\"\
          , line 1368, in load_model\r\n    model = AutoModelForCausalLM.from_pretrained(\r\
          \n  File \"/home/littledot/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 558, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n  File \"/home/littledot/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3250, in from_pretrained\r\n    model = quantizer.post_init_model(model)\r\
          \n  File \"/home/littledot/.local/lib/python3.10/site-packages/optimum/gptq/quantizer.py\"\
          , line 482, in post_init_model\r\n    raise ValueError(\r\nValueError: Found\
          \ modules on cpu/disk. Using Exllama backend requires all the modules to\
          \ be on GPU.You can deactivate exllama backend by setting `disable_exllama=True`\
          \ in the quantization config object\r\n"
        updatedAt: '2023-10-01T01:04:39.214Z'
      numEdits: 0
      reactions: []
    id: 6518c5a742097d8c591ba3e5
    type: comment
  author: littledot
  content: "python3 -m fastchat.serve.cli --model-path /home/littledot/.cache/modelscope/hub/qwen/Qwen-14B-Chat-Int4\r\
    \n\r\nerror info\r\n---------------------------------------------------------------------------------------------------------------------------------------------------\r\
    \nlittledot@aiserver:~$ python3 -m fastchat.serve.cli --model-path /home/littledot/.cache/modelscope/hub/qwen/Qwen-14B-Chat-Int4\
    \             Try importing flash-attention for faster inference...\r\nWarning:\
    \ import flash_attn rotary fail, please install FlashAttention rotary to get higher\
    \ efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\r\
    \nWarning: import flash_attn rms_norm fail, please install FlashAttention layer_norm\
    \ to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\r\
    \nWarning: import flash_attn fail, please install FlashAttention to get higher\
    \ efficiency https://github.com/Dao-AILab/flash-attention\r\nLoading checkpoint\
    \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 5/5 [00:01<00:00,  3.52it/s]\r\nTraceback (most recent call last):\r\n\
    \  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n\
    \    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\"\
    , line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastchat/serve/cli.py\"\
    , line 283, in <module>\r\n    main(args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastchat/serve/cli.py\"\
    , line 208, in main\r\n    chat_loop(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastchat/serve/inference.py\"\
    , line 311, in chat_loop\r\n    model, tokenizer = load_model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastchat/model/model_adapter.py\"\
    , line 288, in load_model\r\n    model, tokenizer = adapter.load_model(model_path,\
    \ kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastchat/model/model_adapter.py\"\
    , line 1368, in load_model\r\n    model = AutoModelForCausalLM.from_pretrained(\r\
    \n  File \"/home/littledot/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 558, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
    \  File \"/home/littledot/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 3250, in from_pretrained\r\n    model = quantizer.post_init_model(model)\r\
    \n  File \"/home/littledot/.local/lib/python3.10/site-packages/optimum/gptq/quantizer.py\"\
    , line 482, in post_init_model\r\n    raise ValueError(\r\nValueError: Found modules\
    \ on cpu/disk. Using Exllama backend requires all the modules to be on GPU.You\
    \ can deactivate exllama backend by setting `disable_exllama=True` in the quantization\
    \ config object\r\n"
  created_at: 2023-10-01 00:04:39+00:00
  edited: false
  hidden: false
  id: 6518c5a742097d8c591ba3e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9921ca8568f9d53c919681d8960ad802.svg
      fullname: jklj077
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jklj077
      type: user
    createdAt: '2023-10-09T10:39:52.000Z'
    data:
      edited: false
      editors:
      - jklj077
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9408180117607117
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9921ca8568f9d53c919681d8960ad802.svg
          fullname: jklj077
          isHf: false
          isPro: false
          name: jklj077
          type: user
        html: '<p>Please check this issue <a rel="nofollow" href="https://github.com/QwenLM/Qwen/issues/411">https://github.com/QwenLM/Qwen/issues/411</a>
          and see if the answers would help. (in short, int4 models are not supported
          on CPUs)</p>

          '
        raw: Please check this issue https://github.com/QwenLM/Qwen/issues/411 and
          see if the answers would help. (in short, int4 models are not supported
          on CPUs)
        updatedAt: '2023-10-09T10:39:52.151Z'
      numEdits: 0
      reactions: []
    id: 6523d878d6ca498dc37d337e
    type: comment
  author: jklj077
  content: Please check this issue https://github.com/QwenLM/Qwen/issues/411 and see
    if the answers would help. (in short, int4 models are not supported on CPUs)
  created_at: 2023-10-09 09:39:52+00:00
  edited: false
  hidden: false
  id: 6523d878d6ca498dc37d337e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9958489f224a55975d3670471c1eba91.svg
      fullname: Qiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littledot
      type: user
    createdAt: '2023-10-09T11:15:54.000Z'
    data:
      edited: false
      editors:
      - littledot
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.44350409507751465
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9958489f224a55975d3670471c1eba91.svg
          fullname: Qiang
          isHf: false
          isPro: false
          name: littledot
          type: user
        html: "<p>\u6211\u4F7F\u7528 GPU\u52A0\u8F7D\u7684\uFF0C2080 ti 22G\u663E\u5361\
          \u7684\u914D\u7F6E\u3002<br>\u8FD0\u884C\u7684\u8FC7\u7A0B\u4E2D\u51FA\u73B0\
          \u7684\u8FD9\u4E2A\u9519\u8BEF\u63D0\u793A\u3002</p>\n<p>I used the configuration\
          \ of a 2080 ti 22G graphics card loaded on a GPU.<br>This error message\
          \ appears during the operation.</p>\n"
        raw: "\u6211\u4F7F\u7528 GPU\u52A0\u8F7D\u7684\uFF0C2080 ti 22G\u663E\u5361\
          \u7684\u914D\u7F6E\u3002\n\u8FD0\u884C\u7684\u8FC7\u7A0B\u4E2D\u51FA\u73B0\
          \u7684\u8FD9\u4E2A\u9519\u8BEF\u63D0\u793A\u3002\n\nI used the configuration\
          \ of a 2080 ti 22G graphics card loaded on a GPU.\nThis error message appears\
          \ during the operation."
        updatedAt: '2023-10-09T11:15:54.638Z'
      numEdits: 0
      reactions: []
    id: 6523e0eac428cceeb71c437e
    type: comment
  author: littledot
  content: "\u6211\u4F7F\u7528 GPU\u52A0\u8F7D\u7684\uFF0C2080 ti 22G\u663E\u5361\u7684\
    \u914D\u7F6E\u3002\n\u8FD0\u884C\u7684\u8FC7\u7A0B\u4E2D\u51FA\u73B0\u7684\u8FD9\
    \u4E2A\u9519\u8BEF\u63D0\u793A\u3002\n\nI used the configuration of a 2080 ti\
    \ 22G graphics card loaded on a GPU.\nThis error message appears during the operation."
  created_at: 2023-10-09 10:15:54+00:00
  edited: false
  hidden: false
  id: 6523e0eac428cceeb71c437e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9921ca8568f9d53c919681d8960ad802.svg
      fullname: jklj077
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jklj077
      type: user
    createdAt: '2023-10-11T11:50:09.000Z'
    data:
      edited: false
      editors:
      - jklj077
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7435791492462158
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9921ca8568f9d53c919681d8960ad802.svg
          fullname: jklj077
          isHf: false
          isPro: false
          name: jklj077
          type: user
        html: "<p>The error message were saying that models are on CPUs. There may\
          \ be configuration (models were not loaded on GPUs) or environment (CUDA\
          \ not found) issues. I suggest you check those first.</p>\n<p>Those are\
          \ controled by fastchat and you could also try seek help from them. The\
          \ related files are here <a rel=\"nofollow\" href=\"https://github.com/lm-sys/FastChat/blob/0c37d989df96cd67464cfbb21fdbebe1bc59022a/fastchat/model/model_adapter.py#L376\"\
          >https://github.com/lm-sys/FastChat/blob/0c37d989df96cd67464cfbb21fdbebe1bc59022a/fastchat/model/model_adapter.py#L376</a></p>\n\
          <p>\u63D0\u4F9B\u7684\u62A5\u9519\u4FE1\u606F\u63D0\u793A\u6A21\u578B\u6CA1\
          \u6709\u5B8C\u5168\u8F7D\u5165GPU\u3002\u53EF\u80FD\u5B58\u5728\u6A21\u578B\
          \u914D\u7F6E\u6216\u8005\u73AF\u5883\u914D\u7F6E\u95EE\u9898\uFF0C\u4F8B\
          \u5982\u6A21\u578B\u6CA1\u6709\u914D\u7F6EGPU\u52A0\u8F7D\u6216\u8005\u73AF\
          \u5883\u4E2D\u65E0\u6CD5\u627E\u5230CUDA\u3002\u5EFA\u8BAE\u60A8\u68C0\u67E5\
          \u4E0B\u76F8\u5173\u914D\u7F6E\u3002</p>\n<p>\u8FD9\u4E9B\u914D\u7F6E\u7531\
          fastchat\u914D\u7F6E\uFF0C\u60A8\u4E5F\u53EF\u4EE5\u5411\u4ED6\u4EEC\u5BFB\
          \u6C42\u5E2E\u52A9\u3002\u76F8\u5173\u53C2\u6570\u914D\u7F6E\u6587\u4EF6\
          \u5728\u8FD9\u91CC <a rel=\"nofollow\" href=\"https://github.com/lm-sys/FastChat/blob/0c37d989df96cd67464cfbb21fdbebe1bc59022a/fastchat/model/model_adapter.py#L376\"\
          >https://github.com/lm-sys/FastChat/blob/0c37d989df96cd67464cfbb21fdbebe1bc59022a/fastchat/model/model_adapter.py#L376</a></p>\n"
        raw: "The error message were saying that models are on CPUs. There may be\
          \ configuration (models were not loaded on GPUs) or environment (CUDA not\
          \ found) issues. I suggest you check those first.\n\nThose are controled\
          \ by fastchat and you could also try seek help from them. The related files\
          \ are here https://github.com/lm-sys/FastChat/blob/0c37d989df96cd67464cfbb21fdbebe1bc59022a/fastchat/model/model_adapter.py#L376\n\
          \n\u63D0\u4F9B\u7684\u62A5\u9519\u4FE1\u606F\u63D0\u793A\u6A21\u578B\u6CA1\
          \u6709\u5B8C\u5168\u8F7D\u5165GPU\u3002\u53EF\u80FD\u5B58\u5728\u6A21\u578B\
          \u914D\u7F6E\u6216\u8005\u73AF\u5883\u914D\u7F6E\u95EE\u9898\uFF0C\u4F8B\
          \u5982\u6A21\u578B\u6CA1\u6709\u914D\u7F6EGPU\u52A0\u8F7D\u6216\u8005\u73AF\
          \u5883\u4E2D\u65E0\u6CD5\u627E\u5230CUDA\u3002\u5EFA\u8BAE\u60A8\u68C0\u67E5\
          \u4E0B\u76F8\u5173\u914D\u7F6E\u3002\n\n\u8FD9\u4E9B\u914D\u7F6E\u7531fastchat\u914D\
          \u7F6E\uFF0C\u60A8\u4E5F\u53EF\u4EE5\u5411\u4ED6\u4EEC\u5BFB\u6C42\u5E2E\
          \u52A9\u3002\u76F8\u5173\u53C2\u6570\u914D\u7F6E\u6587\u4EF6\u5728\u8FD9\
          \u91CC https://github.com/lm-sys/FastChat/blob/0c37d989df96cd67464cfbb21fdbebe1bc59022a/fastchat/model/model_adapter.py#L376"
        updatedAt: '2023-10-11T11:50:09.654Z'
      numEdits: 0
      reactions: []
    id: 65268bf13f98076c3ee08675
    type: comment
  author: jklj077
  content: "The error message were saying that models are on CPUs. There may be configuration\
    \ (models were not loaded on GPUs) or environment (CUDA not found) issues. I suggest\
    \ you check those first.\n\nThose are controled by fastchat and you could also\
    \ try seek help from them. The related files are here https://github.com/lm-sys/FastChat/blob/0c37d989df96cd67464cfbb21fdbebe1bc59022a/fastchat/model/model_adapter.py#L376\n\
    \n\u63D0\u4F9B\u7684\u62A5\u9519\u4FE1\u606F\u63D0\u793A\u6A21\u578B\u6CA1\u6709\
    \u5B8C\u5168\u8F7D\u5165GPU\u3002\u53EF\u80FD\u5B58\u5728\u6A21\u578B\u914D\u7F6E\
    \u6216\u8005\u73AF\u5883\u914D\u7F6E\u95EE\u9898\uFF0C\u4F8B\u5982\u6A21\u578B\
    \u6CA1\u6709\u914D\u7F6EGPU\u52A0\u8F7D\u6216\u8005\u73AF\u5883\u4E2D\u65E0\u6CD5\
    \u627E\u5230CUDA\u3002\u5EFA\u8BAE\u60A8\u68C0\u67E5\u4E0B\u76F8\u5173\u914D\u7F6E\
    \u3002\n\n\u8FD9\u4E9B\u914D\u7F6E\u7531fastchat\u914D\u7F6E\uFF0C\u60A8\u4E5F\
    \u53EF\u4EE5\u5411\u4ED6\u4EEC\u5BFB\u6C42\u5E2E\u52A9\u3002\u76F8\u5173\u53C2\
    \u6570\u914D\u7F6E\u6587\u4EF6\u5728\u8FD9\u91CC https://github.com/lm-sys/FastChat/blob/0c37d989df96cd67464cfbb21fdbebe1bc59022a/fastchat/model/model_adapter.py#L376"
  created_at: 2023-10-11 10:50:09+00:00
  edited: false
  hidden: false
  id: 65268bf13f98076c3ee08675
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9921ca8568f9d53c919681d8960ad802.svg
      fullname: jklj077
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jklj077
      type: user
    createdAt: '2023-10-11T12:01:24.000Z'
    data:
      edited: true
      editors:
      - jklj077
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8203946948051453
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9921ca8568f9d53c919681d8960ad802.svg
          fullname: jklj077
          isHf: false
          isPro: false
          name: jklj077
          type: user
        html: '<p>After some searching, I think FastChat does not support GPTQ yet
          (not sure). Qwen-14B-Chat-Int4 is quantized using AutoGPTQ.<br><a rel="nofollow"
          href="https://github.com/lm-sys/FastChat/issues/1671">https://github.com/lm-sys/FastChat/issues/1671</a><br><a
          rel="nofollow" href="https://github.com/lm-sys/FastChat/issues/1745">https://github.com/lm-sys/FastChat/issues/1745</a><br><a
          rel="nofollow" href="https://github.com/lm-sys/FastChat/issues/2215">https://github.com/lm-sys/FastChat/issues/2215</a><br><a
          rel="nofollow" href="https://github.com/lm-sys/FastChat/issues/2375">https://github.com/lm-sys/FastChat/issues/2375</a></p>

          <p>There is also a PR there. Seems to mention the same problem as yours.<br><a
          rel="nofollow" href="https://github.com/lm-sys/FastChat/pull/2365">https://github.com/lm-sys/FastChat/pull/2365</a></p>

          '
        raw: 'After some searching, I think FastChat does not support GPTQ yet (not
          sure). Qwen-14B-Chat-Int4 is quantized using AutoGPTQ.

          https://github.com/lm-sys/FastChat/issues/1671

          https://github.com/lm-sys/FastChat/issues/1745

          https://github.com/lm-sys/FastChat/issues/2215

          https://github.com/lm-sys/FastChat/issues/2375


          There is also a PR there. Seems to mention the same problem as yours.

          https://github.com/lm-sys/FastChat/pull/2365'
        updatedAt: '2023-10-11T12:03:24.025Z'
      numEdits: 1
      reactions: []
    id: 65268e94e67f1c6e153815ad
    type: comment
  author: jklj077
  content: 'After some searching, I think FastChat does not support GPTQ yet (not
    sure). Qwen-14B-Chat-Int4 is quantized using AutoGPTQ.

    https://github.com/lm-sys/FastChat/issues/1671

    https://github.com/lm-sys/FastChat/issues/1745

    https://github.com/lm-sys/FastChat/issues/2215

    https://github.com/lm-sys/FastChat/issues/2375


    There is also a PR there. Seems to mention the same problem as yours.

    https://github.com/lm-sys/FastChat/pull/2365'
  created_at: 2023-10-11 11:01:24+00:00
  edited: true
  hidden: false
  id: 65268e94e67f1c6e153815ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644682834896-620760a26e3b7210c2ff1943.jpeg?w=200&h=200&f=face
      fullname: Junyang Lin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: JustinLin610
      type: user
    createdAt: '2023-10-12T06:30:50.000Z'
    data:
      edited: true
      editors:
      - JustinLin610
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9537870287895203
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644682834896-620760a26e3b7210c2ff1943.jpeg?w=200&h=200&f=face
          fullname: Junyang Lin
          isHf: false
          isPro: false
          name: JustinLin610
          type: user
        html: '<p>I think you need to make changes to fastchat code. I just debugged
          it, and you need to turn low_cpu_mem_usage=True to device_map="auto" in
          the Qwen adapter inside model_adapter.py. We''ll shoot a PR to the official
          repo, but for now I advise you to modify yourself.</p>

          '
        raw: I think you need to make changes to fastchat code. I just debugged it,
          and you need to turn low_cpu_mem_usage=True to device_map="auto" in the
          Qwen adapter inside model_adapter.py. We'll shoot a PR to the official repo,
          but for now I advise you to modify yourself.
        updatedAt: '2023-10-12T06:32:37.549Z'
      numEdits: 1
      reactions: []
    id: 6527929a0ef49cfb78497d72
    type: comment
  author: JustinLin610
  content: I think you need to make changes to fastchat code. I just debugged it,
    and you need to turn low_cpu_mem_usage=True to device_map="auto" in the Qwen adapter
    inside model_adapter.py. We'll shoot a PR to the official repo, but for now I
    advise you to modify yourself.
  created_at: 2023-10-12 05:30:50+00:00
  edited: true
  hidden: false
  id: 6527929a0ef49cfb78497d72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9921ca8568f9d53c919681d8960ad802.svg
      fullname: jklj077
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jklj077
      type: user
    createdAt: '2023-12-21T13:25:19.000Z'
    data:
      status: closed
    id: 65843cbf8d32c176005887f3
    type: status-change
  author: jklj077
  created_at: 2023-12-21 13:25:19+00:00
  id: 65843cbf8d32c176005887f3
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Qwen/Qwen-14B-Chat-Int4
repo_type: model
status: closed
target_branch: null
title: FastChat Load Error
