!!python/object:huggingface_hub.community.DiscussionWithDetails
author: clevnumb
conflicting_files: null
created_at: 2023-11-17 02:14:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/550e36f3104051a9f97622a1ec5c8c50.svg
      fullname: Lance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clevnumb
      type: user
    createdAt: '2023-11-17T02:14:28.000Z'
    data:
      edited: false
      editors:
      - clevnumb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9624703526496887
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/550e36f3104051a9f97622a1ec5c8c50.svg
          fullname: Lance
          isHf: false
          isPro: false
          name: clevnumb
          type: user
        html: '<p>Can I fit a 5bpw or 6bpw on a single 4090? </p>

          <p>What is the highest one and what context ranges?  I can''t find this
          info anywhere.</p>

          <p>Thank you.</p>

          '
        raw: "Can I fit a 5bpw or 6bpw on a single 4090? \r\n\r\nWhat is the highest\
          \ one and what context ranges?  I can't find this info anywhere.\r\n\r\n\
          Thank you."
        updatedAt: '2023-11-17T02:14:28.501Z'
      numEdits: 0
      reactions: []
    id: 6556cc84f062c53d55634be2
    type: comment
  author: clevnumb
  content: "Can I fit a 5bpw or 6bpw on a single 4090? \r\n\r\nWhat is the highest\
    \ one and what context ranges?  I can't find this info anywhere.\r\n\r\nThank\
    \ you."
  created_at: 2023-11-17 02:14:28+00:00
  edited: false
  hidden: false
  id: 6556cc84f062c53d55634be2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-17T03:45:43.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9466186761856079
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>You can estimate the VRAM requirements by just looking at the size
          of the model files themselves.  The 4.0bpw models will comfortably fit on
          a single 4090.  5.0bpw will likely not fit, however, I just tried loading
          on an empty 4090 and it runs OOM. 5.0 - 8.0 bpw will need two cards.  I
          should probably generate a 4.65 bpw model to get slightly more bits on a
          model that fits in a single 4090.</p>

          '
        raw: You can estimate the VRAM requirements by just looking at the size of
          the model files themselves.  The 4.0bpw models will comfortably fit on a
          single 4090.  5.0bpw will likely not fit, however, I just tried loading
          on an empty 4090 and it runs OOM. 5.0 - 8.0 bpw will need two cards.  I
          should probably generate a 4.65 bpw model to get slightly more bits on a
          model that fits in a single 4090.
        updatedAt: '2023-11-17T03:45:43.798Z'
      numEdits: 0
      reactions: []
    id: 6556e1e747c93d37a886bfc6
    type: comment
  author: LoneStriker
  content: You can estimate the VRAM requirements by just looking at the size of the
    model files themselves.  The 4.0bpw models will comfortably fit on a single 4090.  5.0bpw
    will likely not fit, however, I just tried loading on an empty 4090 and it runs
    OOM. 5.0 - 8.0 bpw will need two cards.  I should probably generate a 4.65 bpw
    model to get slightly more bits on a model that fits in a single 4090.
  created_at: 2023-11-17 03:45:43+00:00
  edited: false
  hidden: false
  id: 6556e1e747c93d37a886bfc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/550e36f3104051a9f97622a1ec5c8c50.svg
      fullname: Lance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clevnumb
      type: user
    createdAt: '2023-11-17T03:54:16.000Z'
    data:
      edited: true
      editors:
      - clevnumb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9223889112472534
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/550e36f3104051a9f97622a1ec5c8c50.svg
          fullname: Lance
          isHf: false
          isPro: false
          name: clevnumb
          type: user
        html: '<p>That would be awesome if you could do that...</p>

          <p>Thank you for the detail. Is this model when used by such a system best
          loaded at a 4096 context?  1.75 alpha value?  (I have 96GB of RAM, not sure
          if that matters, beyond loading the model in)</p>

          '
        raw: 'That would be awesome if you could do that...


          Thank you for the detail. Is this model when used by such a system best
          loaded at a 4096 context?  1.75 alpha value?  (I have 96GB of RAM, not sure
          if that matters, beyond loading the model in)'
        updatedAt: '2023-11-17T03:54:43.655Z'
      numEdits: 1
      reactions: []
    id: 6556e3e88076124b8d80b1ee
    type: comment
  author: clevnumb
  content: 'That would be awesome if you could do that...


    Thank you for the detail. Is this model when used by such a system best loaded
    at a 4096 context?  1.75 alpha value?  (I have 96GB of RAM, not sure if that matters,
    beyond loading the model in)'
  created_at: 2023-11-17 03:54:16+00:00
  edited: true
  hidden: false
  id: 6556e3e88076124b8d80b1ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-17T04:06:25.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9413427114486694
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I have not used this model extensively. Your RAM won''t matter other
          than allowing you to load the model into your GPU''s VRAM. If you stick
          with the default 4096 context length, no alpha value needed. If you extend
          higher, you''ll need to start adjusting the alpha; there''s a formula for
          alpha, but the ooba slider may not actually let you set it high enough with
          default limits. This model supposedly supports up to 200K context, but I''ve
          not had great success with long-context models in the past.</p>

          '
        raw: I have not used this model extensively. Your RAM won't matter other than
          allowing you to load the model into your GPU's VRAM. If you stick with the
          default 4096 context length, no alpha value needed. If you extend higher,
          you'll need to start adjusting the alpha; there's a formula for alpha, but
          the ooba slider may not actually let you set it high enough with default
          limits. This model supposedly supports up to 200K context, but I've not
          had great success with long-context models in the past.
        updatedAt: '2023-11-17T04:06:25.841Z'
      numEdits: 0
      reactions: []
    id: 6556e6c1f062c53d556830f8
    type: comment
  author: LoneStriker
  content: I have not used this model extensively. Your RAM won't matter other than
    allowing you to load the model into your GPU's VRAM. If you stick with the default
    4096 context length, no alpha value needed. If you extend higher, you'll need
    to start adjusting the alpha; there's a formula for alpha, but the ooba slider
    may not actually let you set it high enough with default limits. This model supposedly
    supports up to 200K context, but I've not had great success with long-context
    models in the past.
  created_at: 2023-11-17 04:06:25+00:00
  edited: false
  hidden: false
  id: 6556e6c1f062c53d556830f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/550e36f3104051a9f97622a1ec5c8c50.svg
      fullname: Lance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clevnumb
      type: user
    createdAt: '2023-11-17T04:09:11.000Z'
    data:
      edited: false
      editors:
      - clevnumb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.989545464515686
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/550e36f3104051a9f97622a1ec5c8c50.svg
          fullname: Lance
          isHf: false
          isPro: false
          name: clevnumb
          type: user
        html: '<p>Thanks, I''ll play around with it. I''ve never gotten anything working
          well beyond 4096 at all...  :-\</p>

          '
        raw: Thanks, I'll play around with it. I've never gotten anything working
          well beyond 4096 at all...  :-\
        updatedAt: '2023-11-17T04:09:11.851Z'
      numEdits: 0
      reactions: []
    id: 6556e7677eaa0731c9184b2c
    type: comment
  author: clevnumb
  content: Thanks, I'll play around with it. I've never gotten anything working well
    beyond 4096 at all...  :-\
  created_at: 2023-11-17 04:09:11+00:00
  edited: false
  hidden: false
  id: 6556e7677eaa0731c9184b2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a37d2caacab433fdd17d232aa29371c.svg
      fullname: eepos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eepos
      type: user
    createdAt: '2023-11-17T12:18:46.000Z'
    data:
      edited: true
      editors:
      - eepos
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.812863290309906
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a37d2caacab433fdd17d232aa29371c.svg
          fullname: eepos
          isHf: false
          isPro: false
          name: eepos
          type: user
        html: '<p>I can load a 5.0bpw model on a 4090 at 4096 context length and it
          generates at good speed. This is on Windows 11 too so OS and browser are
          consuming more VRAM than on Linux (I assume). Note that 8-bit cache does
          wonders for VRAM consumption:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62f9558c77b722f1866448b8/QPpJ5Asd35ho5p8mvUtM-.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/62f9558c77b722f1866448b8/QPpJ5Asd35ho5p8mvUtM-.png"></a></p>

          <p>As you can see, it''s very close to VRAM limit. I have experienced a
          slowdown due to Nvidia driver directing the model to RAM slowing things
          down when I had too many browser tabs open... :) But in my example image
          this isn''t happening as I have no other tabs/programs consuming VRAM.</p>

          '
        raw: 'I can load a 5.0bpw model on a 4090 at 4096 context length and it generates
          at good speed. This is on Windows 11 too so OS and browser are consuming
          more VRAM than on Linux (I assume). Note that 8-bit cache does wonders for
          VRAM consumption:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62f9558c77b722f1866448b8/QPpJ5Asd35ho5p8mvUtM-.png)


          As you can see, it''s very close to VRAM limit. I have experienced a slowdown
          due to Nvidia driver directing the model to RAM slowing things down when
          I had too many browser tabs open... :) But in my example image this isn''t
          happening as I have no other tabs/programs consuming VRAM.

          '
        updatedAt: '2023-11-17T12:23:40.156Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - LoneStriker
    id: 65575a26eeced178a6879b28
    type: comment
  author: eepos
  content: 'I can load a 5.0bpw model on a 4090 at 4096 context length and it generates
    at good speed. This is on Windows 11 too so OS and browser are consuming more
    VRAM than on Linux (I assume). Note that 8-bit cache does wonders for VRAM consumption:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62f9558c77b722f1866448b8/QPpJ5Asd35ho5p8mvUtM-.png)


    As you can see, it''s very close to VRAM limit. I have experienced a slowdown
    due to Nvidia driver directing the model to RAM slowing things down when I had
    too many browser tabs open... :) But in my example image this isn''t happening
    as I have no other tabs/programs consuming VRAM.

    '
  created_at: 2023-11-17 12:18:46+00:00
  edited: true
  hidden: false
  id: 65575a26eeced178a6879b28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-17T18:01:15.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7055656909942627
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Thanks for the info.  For future 34B quants, I''ve added 4.65 bpw
          version, first one added here for my <a href="https://huggingface.co/LoneStriker?search_models=yi-34b-spicyboros-3.1-2">fine-tune
          of Yi-34B with Spicyboros dataset</a>.</p>

          '
        raw: Thanks for the info.  For future 34B quants, I've added 4.65 bpw version,
          first one added here for my [fine-tune of Yi-34B with Spicyboros dataset](https://huggingface.co/LoneStriker?search_models=yi-34b-spicyboros-3.1-2).
        updatedAt: '2023-11-17T18:01:15.335Z'
      numEdits: 0
      reactions: []
    id: 6557aa6be457dfd573547cf9
    type: comment
  author: LoneStriker
  content: Thanks for the info.  For future 34B quants, I've added 4.65 bpw version,
    first one added here for my [fine-tune of Yi-34B with Spicyboros dataset](https://huggingface.co/LoneStriker?search_models=yi-34b-spicyboros-3.1-2).
  created_at: 2023-11-17 18:01:15+00:00
  edited: false
  hidden: false
  id: 6557aa6be457dfd573547cf9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a37d2caacab433fdd17d232aa29371c.svg
      fullname: eepos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eepos
      type: user
    createdAt: '2023-11-17T18:19:16.000Z'
    data:
      edited: false
      editors:
      - eepos
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9778668880462646
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a37d2caacab433fdd17d232aa29371c.svg
          fullname: eepos
          isHf: false
          isPro: false
          name: eepos
          type: user
        html: '<p>Yep, 4.65bpw is good as it leaves a bit of wiggle room. Thanks for
          all the exl2 quants btw, have downloaded many!</p>

          '
        raw: Yep, 4.65bpw is good as it leaves a bit of wiggle room. Thanks for all
          the exl2 quants btw, have downloaded many!
        updatedAt: '2023-11-17T18:19:16.266Z'
      numEdits: 0
      reactions: []
    id: 6557aea4f2c0ae79c283e797
    type: comment
  author: eepos
  content: Yep, 4.65bpw is good as it leaves a bit of wiggle room. Thanks for all
    the exl2 quants btw, have downloaded many!
  created_at: 2023-11-17 18:19:16+00:00
  edited: false
  hidden: false
  id: 6557aea4f2c0ae79c283e797
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/550e36f3104051a9f97622a1ec5c8c50.svg
      fullname: Lance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clevnumb
      type: user
    createdAt: '2023-11-18T06:50:00.000Z'
    data:
      edited: false
      editors:
      - clevnumb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.942681610584259
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/550e36f3104051a9f97622a1ec5c8c50.svg
          fullname: Lance
          isHf: false
          isPro: false
          name: clevnumb
          type: user
        html: '<p>Yes thanks a lot for all the hard work!</p>

          <p>EEPOS:  Does setting this to 8-bit cache lower the quality of the output?
          I mean, any drawbacks to having that enabled?</p>

          '
        raw: 'Yes thanks a lot for all the hard work!


          EEPOS:  Does setting this to 8-bit cache lower the quality of the output?
          I mean, any drawbacks to having that enabled?'
        updatedAt: '2023-11-18T06:50:00.592Z'
      numEdits: 0
      reactions: []
    id: 65585e989d249b4ab31f224f
    type: comment
  author: clevnumb
  content: 'Yes thanks a lot for all the hard work!


    EEPOS:  Does setting this to 8-bit cache lower the quality of the output? I mean,
    any drawbacks to having that enabled?'
  created_at: 2023-11-18 06:50:00+00:00
  edited: false
  hidden: false
  id: 65585e989d249b4ab31f224f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-18T07:31:59.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.953648030757904
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>According to Turboderp, there''s no quality loss. You save VRAM
          and trade off a bit of inference speed.</p>

          '
        raw: According to Turboderp, there's no quality loss. You save VRAM and trade
          off a bit of inference speed.
        updatedAt: '2023-11-18T07:31:59.904Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - eepos
        - maxspaso
    id: 6558686f4a49906b2c2a3ca1
    type: comment
  author: LoneStriker
  content: According to Turboderp, there's no quality loss. You save VRAM and trade
    off a bit of inference speed.
  created_at: 2023-11-18 07:31:59+00:00
  edited: false
  hidden: false
  id: 6558686f4a49906b2c2a3ca1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/Nous-Capybara-34B-4.0bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: Which of these 34B model BPW will fit on a single 24GB card's (4090) VRAM?
