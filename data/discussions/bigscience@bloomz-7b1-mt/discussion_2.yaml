!!python/object:huggingface_hub.community.DiscussionWithDetails
author: raihan2345
conflicting_files: null
created_at: 2023-03-15 01:18:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/38f75a31ec4878a4c527097d33d732e6.svg
      fullname: muhammad raihan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: raihan2345
      type: user
    createdAt: '2023-03-15T02:18:42.000Z'
    data:
      edited: false
      editors:
      - raihan2345
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/38f75a31ec4878a4c527097d33d732e6.svg
          fullname: muhammad raihan
          isHf: false
          isPro: false
          name: raihan2345
          type: user
        html: '<p>I am interested in fine-tuning / prompt-tuning the Bloomz-7B-MT
          model provided by Hugging Face. However, I am unsure about the specific
          hardware requirements that are needed to perform this task efficiently.</p>

          <p>Some of the questions I have include:</p>

          <ul>

          <li>What is the recommended amount of RAM for fine-tuning / prompt-tuning
          Bloomz-7B-MT?</li>

          <li>What is the recommended GPU memory size?</li>

          <li>Are there any specific hardware requirements for training with mixed
          precision?</li>

          <li>Are there any other hardware considerations that are important for efficient
          fine-tuning / prompt-tuning of Bloomz-7B-MT?</li>

          </ul>

          <p>I would greatly appreciate any insights and advice</p>

          '
        raw: "I am interested in fine-tuning / prompt-tuning the Bloomz-7B-MT model\
          \ provided by Hugging Face. However, I am unsure about the specific hardware\
          \ requirements that are needed to perform this task efficiently.\r\n\r\n\
          Some of the questions I have include:\r\n\r\n- What is the recommended amount\
          \ of RAM for fine-tuning / prompt-tuning Bloomz-7B-MT?\r\n- What is the\
          \ recommended GPU memory size?\r\n- Are there any specific hardware requirements\
          \ for training with mixed precision?\r\n- Are there any other hardware considerations\
          \ that are important for efficient fine-tuning / prompt-tuning of Bloomz-7B-MT?\r\
          \n\r\nI would greatly appreciate any insights and advice"
        updatedAt: '2023-03-15T02:18:42.753Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - ArthurJochems
        - FgRegistr
        - YeungNLP
    id: 64112b02028e6f7153e02f0a
    type: comment
  author: raihan2345
  content: "I am interested in fine-tuning / prompt-tuning the Bloomz-7B-MT model\
    \ provided by Hugging Face. However, I am unsure about the specific hardware requirements\
    \ that are needed to perform this task efficiently.\r\n\r\nSome of the questions\
    \ I have include:\r\n\r\n- What is the recommended amount of RAM for fine-tuning\
    \ / prompt-tuning Bloomz-7B-MT?\r\n- What is the recommended GPU memory size?\r\
    \n- Are there any specific hardware requirements for training with mixed precision?\r\
    \n- Are there any other hardware considerations that are important for efficient\
    \ fine-tuning / prompt-tuning of Bloomz-7B-MT?\r\n\r\nI would greatly appreciate\
    \ any insights and advice"
  created_at: 2023-03-15 01:18:42+00:00
  edited: false
  hidden: false
  id: 64112b02028e6f7153e02f0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/329fcbe59b7d41470a9bcc87c0a8b4f2.svg
      fullname: Jochems
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurJochems
      type: user
    createdAt: '2023-03-28T14:46:52.000Z'
    data:
      edited: false
      editors:
      - ArthurJochems
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/329fcbe59b7d41470a9bcc87c0a8b4f2.svg
          fullname: Jochems
          isHf: false
          isPro: false
          name: ArthurJochems
          type: user
        html: '<p>Interested in exactly the same topic. Anyone working on this?</p>

          '
        raw: Interested in exactly the same topic. Anyone working on this?
        updatedAt: '2023-03-28T14:46:52.430Z'
      numEdits: 0
      reactions: []
    id: 6422fddcad875472716ebd05
    type: comment
  author: ArthurJochems
  content: Interested in exactly the same topic. Anyone working on this?
  created_at: 2023-03-28 13:46:52+00:00
  edited: false
  hidden: false
  id: 6422fddcad875472716ebd05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c336cd4cccc52ec2a7e85e66ae8efa9b.svg
      fullname: M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Davyd48
      type: user
    createdAt: '2023-07-20T06:43:56.000Z'
    data:
      edited: true
      editors:
      - Davyd48
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9406337141990662
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c336cd4cccc52ec2a7e85e66ae8efa9b.svg
          fullname: M
          isHf: false
          isPro: false
          name: Davyd48
          type: user
        html: '<p>Hello dear friends, I will also greatly appreciate any help in this
          question. Please tell me what is the minimum computer power required to
          run the retraining process for this model?</p>

          '
        raw: Hello dear friends, I will also greatly appreciate any help in this question.
          Please tell me what is the minimum computer power required to run the retraining
          process for this model?
        updatedAt: '2023-07-20T06:50:32.414Z'
      numEdits: 1
      reactions: []
    id: 64b8d7ac8c12f0008b92d973
    type: comment
  author: Davyd48
  content: Hello dear friends, I will also greatly appreciate any help in this question.
    Please tell me what is the minimum computer power required to run the retraining
    process for this model?
  created_at: 2023-07-20 05:43:56+00:00
  edited: true
  hidden: false
  id: 64b8d7ac8c12f0008b92d973
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-07-20T16:22:03.000Z'
    data:
      edited: true
      editors:
      - Muennighoff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9181736707687378
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: "<p>Sorry for the late response, not sure how helpful my answer is as\
          \ it depends on so many factors \U0001F605</p>\n<blockquote>\n<p>What is\
          \ the recommended amount of RAM for fine-tuning / prompt-tuning Bloomz-7B-MT?</p>\n\
          </blockquote>\n<p>Bloomz-7B-MT is ~14GB. If you have more CPU RAM than that,\
          \ it should be fine. You can even get away with less CPU RAM by transferring\
          \ the weights to GPU step by step.</p>\n<blockquote>\n<p>What is the recommended\
          \ GPU memory size?</p>\n</blockquote>\n<p>Bloomz-7B-MT is ~14GB.<br>Fine-tuning:\
          \ For naive fine-tuning of all parameters you will need to store ~2x the\
          \ weights in optimizer states etc. , so probably having 80GB would be sufficient.<br>Fine-tuning\
          \ w/ less memory: Using DeepSpeed / other things, you could probably do\
          \ full fine-tuning with 40GB<br>Fine-tuning some parameters: Using LLoRA\
          \ or other stuff, you can probably do fine-tuning with even less by just\
          \ changing a few parameters<br>Prompting: Depends on the length of your\
          \ prompt, you will need ~14GB + some amount for the data</p>\n<blockquote>\n\
          <p>Are there any specific hardware requirements for training with mixed\
          \ precision?</p>\n</blockquote>\n<p>Most modern GPUs support FP16 afaik,\
          \ i.e. Nvidia A100, AMD Mi250 etc should all work</p>\n"
        raw: "Sorry for the late response, not sure how helpful my answer is as it\
          \ depends on so many factors \U0001F605\n\n> What is the recommended amount\
          \ of RAM for fine-tuning / prompt-tuning Bloomz-7B-MT?\n\nBloomz-7B-MT is\
          \ ~14GB. If you have more CPU RAM than that, it should be fine. You can\
          \ even get away with less CPU RAM by transferring the weights to GPU step\
          \ by step.\n> What is the recommended GPU memory size?\n\nBloomz-7B-MT is\
          \ ~14GB.\nFine-tuning: For naive fine-tuning of all parameters you will\
          \ need to store ~2x the weights in optimizer states etc. , so probably having\
          \ 80GB would be sufficient.\nFine-tuning w/ less memory: Using DeepSpeed\
          \ / other things, you could probably do full fine-tuning with 40GB\nFine-tuning\
          \ some parameters: Using LLoRA or other stuff, you can probably do fine-tuning\
          \ with even less by just changing a few parameters\nPrompting: Depends on\
          \ the length of your prompt, you will need ~14GB + some amount for the data\n\
          > Are there any specific hardware requirements for training with mixed precision?\n\
          \nMost modern GPUs support FP16 afaik, i.e. Nvidia A100, AMD Mi250 etc should\
          \ all work\n"
        updatedAt: '2023-07-20T16:22:25.526Z'
      numEdits: 1
      reactions: []
    id: 64b95f2b047fa3db9436ea8a
    type: comment
  author: Muennighoff
  content: "Sorry for the late response, not sure how helpful my answer is as it depends\
    \ on so many factors \U0001F605\n\n> What is the recommended amount of RAM for\
    \ fine-tuning / prompt-tuning Bloomz-7B-MT?\n\nBloomz-7B-MT is ~14GB. If you have\
    \ more CPU RAM than that, it should be fine. You can even get away with less CPU\
    \ RAM by transferring the weights to GPU step by step.\n> What is the recommended\
    \ GPU memory size?\n\nBloomz-7B-MT is ~14GB.\nFine-tuning: For naive fine-tuning\
    \ of all parameters you will need to store ~2x the weights in optimizer states\
    \ etc. , so probably having 80GB would be sufficient.\nFine-tuning w/ less memory:\
    \ Using DeepSpeed / other things, you could probably do full fine-tuning with\
    \ 40GB\nFine-tuning some parameters: Using LLoRA or other stuff, you can probably\
    \ do fine-tuning with even less by just changing a few parameters\nPrompting:\
    \ Depends on the length of your prompt, you will need ~14GB + some amount for\
    \ the data\n> Are there any specific hardware requirements for training with mixed\
    \ precision?\n\nMost modern GPUs support FP16 afaik, i.e. Nvidia A100, AMD Mi250\
    \ etc should all work\n"
  created_at: 2023-07-20 15:22:03+00:00
  edited: true
  hidden: false
  id: 64b95f2b047fa3db9436ea8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653cb1e63bd61358050e9bc5/PCFSc1A9wVdbjRTK6NsSj.jpeg?w=200&h=200&f=face
      fullname: Rukaiya Hasan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rukaiyaaaah
      type: user
    createdAt: '2023-11-13T14:40:53.000Z'
    data:
      edited: true
      editors:
      - rukaiyaaaah
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9426005482673645
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653cb1e63bd61358050e9bc5/PCFSc1A9wVdbjRTK6NsSj.jpeg?w=200&h=200&f=face
          fullname: Rukaiya Hasan
          isHf: false
          isPro: false
          name: rukaiyaaaah
          type: user
        html: '<p>I want to use this version of the model and make it more ChatGPT
          like for my language Punjabi. Is there a way I can shred off tokenizers
          and embeddings for languages other than the one I want, since it can be
          done for mt5 which reduced the model size by more than half. Also, are there
          any smaller versions of this model coming soon since I dont have access
          to a cluster of GPUs.</p>

          <p>I have seen multiple tutorials on using the QLORA and PEFT techniques
          to fine-tune many 7B parameter models but they dont seem to work for this
          one here. I want to fine-tune it using a free version on colab and I dont
          want it to take much space, can anyone please help?</p>

          <p>Thanks in advance!</p>

          '
        raw: 'I want to use this version of the model and make it more ChatGPT like
          for my language Punjabi. Is there a way I can shred off tokenizers and embeddings
          for languages other than the one I want, since it can be done for mt5 which
          reduced the model size by more than half. Also, are there any smaller versions
          of this model coming soon since I dont have access to a cluster of GPUs.


          I have seen multiple tutorials on using the QLORA and PEFT techniques to
          fine-tune many 7B parameter models but they dont seem to work for this one
          here. I want to fine-tune it using a free version on colab and I dont want
          it to take much space, can anyone please help?


          Thanks in advance!'
        updatedAt: '2023-11-13T15:37:29.835Z'
      numEdits: 2
      reactions: []
    id: 655235757c71b44c30ed4c68
    type: comment
  author: rukaiyaaaah
  content: 'I want to use this version of the model and make it more ChatGPT like
    for my language Punjabi. Is there a way I can shred off tokenizers and embeddings
    for languages other than the one I want, since it can be done for mt5 which reduced
    the model size by more than half. Also, are there any smaller versions of this
    model coming soon since I dont have access to a cluster of GPUs.


    I have seen multiple tutorials on using the QLORA and PEFT techniques to fine-tune
    many 7B parameter models but they dont seem to work for this one here. I want
    to fine-tune it using a free version on colab and I dont want it to take much
    space, can anyone please help?


    Thanks in advance!'
  created_at: 2023-11-13 14:40:53+00:00
  edited: true
  hidden: false
  id: 655235757c71b44c30ed4c68
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-11-13T17:06:22.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6674124002456665
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>You can try e.g. <a href="https://huggingface.co/bigscience/bloomz-3b">https://huggingface.co/bigscience/bloomz-3b</a>
          or <a href="https://huggingface.co/bigscience/bloomz-3b">https://huggingface.co/bigscience/bloomz-3b</a>
          or <a href="https://huggingface.co/bigscience/bloomz-1b7">https://huggingface.co/bigscience/bloomz-1b7</a>
          which should be similar to bloomz-7b1-mt. For -mt, 7b1 is the smallest one
          though</p>

          '
        raw: You can try e.g. https://huggingface.co/bigscience/bloomz-3b or https://huggingface.co/bigscience/bloomz-3b
          or https://huggingface.co/bigscience/bloomz-1b7 which should be similar
          to bloomz-7b1-mt. For -mt, 7b1 is the smallest one though
        updatedAt: '2023-11-13T17:06:22.387Z'
      numEdits: 0
      reactions: []
    id: 6552578edeb9e5b4a0816ae1
    type: comment
  author: Muennighoff
  content: You can try e.g. https://huggingface.co/bigscience/bloomz-3b or https://huggingface.co/bigscience/bloomz-3b
    or https://huggingface.co/bigscience/bloomz-1b7 which should be similar to bloomz-7b1-mt.
    For -mt, 7b1 is the smallest one though
  created_at: 2023-11-13 17:06:22+00:00
  edited: false
  hidden: false
  id: 6552578edeb9e5b4a0816ae1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653cb1e63bd61358050e9bc5/PCFSc1A9wVdbjRTK6NsSj.jpeg?w=200&h=200&f=face
      fullname: Rukaiya Hasan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rukaiyaaaah
      type: user
    createdAt: '2023-11-13T18:16:50.000Z'
    data:
      edited: true
      editors:
      - rukaiyaaaah
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9252879023551941
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/653cb1e63bd61358050e9bc5/PCFSc1A9wVdbjRTK6NsSj.jpeg?w=200&h=200&f=face
          fullname: Rukaiya Hasan
          isHf: false
          isPro: false
          name: rukaiyaaaah
          type: user
        html: '<p>Thanks alot I am thinking of going ahead with fine-tuning the smallest
          version of bloom itself taking individual data files from the same xp3mt
          dataset, (because I found some cleaning issues with the data so I would
          want to give a try with the cleaned data), is there any way to remove the
          tokens for other languages, decreasing the tokenizer size though?</p>

          '
        raw: Thanks alot I am thinking of going ahead with fine-tuning the smallest
          version of bloom itself taking individual data files from the same xp3mt
          dataset, (because I found some cleaning issues with the data so I would
          want to give a try with the cleaned data), is there any way to remove the
          tokens for other languages, decreasing the tokenizer size though?
        updatedAt: '2023-11-13T18:17:56.113Z'
      numEdits: 1
      reactions: []
    id: 655268129e144c06dd07b3d6
    type: comment
  author: rukaiyaaaah
  content: Thanks alot I am thinking of going ahead with fine-tuning the smallest
    version of bloom itself taking individual data files from the same xp3mt dataset,
    (because I found some cleaning issues with the data so I would want to give a
    try with the cleaned data), is there any way to remove the tokens for other languages,
    decreasing the tokenizer size though?
  created_at: 2023-11-13 18:16:50+00:00
  edited: true
  hidden: false
  id: 655268129e144c06dd07b3d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-11-13T18:52:11.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9322625398635864
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Hm I have never tried that but it should work. The easiest one may
          be to remove all tokens with foreign characters that you do not intend to
          use (e.g. Chinese / Japanese / Korean etc)</p>

          '
        raw: Hm I have never tried that but it should work. The easiest one may be
          to remove all tokens with foreign characters that you do not intend to use
          (e.g. Chinese / Japanese / Korean etc)
        updatedAt: '2023-11-13T18:52:11.127Z'
      numEdits: 0
      reactions: []
    id: 6552705b09c73282c8bbb3cd
    type: comment
  author: Muennighoff
  content: Hm I have never tried that but it should work. The easiest one may be to
    remove all tokens with foreign characters that you do not intend to use (e.g.
    Chinese / Japanese / Korean etc)
  created_at: 2023-11-13 18:52:11+00:00
  edited: false
  hidden: false
  id: 6552705b09c73282c8bbb3cd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: bigscience/bloomz-7b1-mt
repo_type: model
status: open
target_branch: null
title: ' Hardware requirements for fine-tuning / prompt-tuning bloomz-7b1-mt'
