!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kurapika993
conflicting_files: null
created_at: 2023-05-15 19:47:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
      fullname: Mayukh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kurapika993
      type: user
    createdAt: '2023-05-15T20:47:38.000Z'
    data:
      edited: true
      editors:
      - Kurapika993
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
          fullname: Mayukh
          isHf: false
          isPro: false
          name: Kurapika993
          type: user
        html: '<p>While running: model = AutoGPTQForCausalLM.from_quantized(TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ,
          device="cuda:5", use_triton=False)</p>

          <p>I am getting this strange error, even if the quantiz.config file is present
          in the repo</p>

          <p>FileNotFoundError: [Errno 2] No such file or directory: ''TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/quantize_config.json''</p>

          '
        raw: 'While running: model = AutoGPTQForCausalLM.from_quantized(TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ,
          device="cuda:5", use_triton=False)


          I am getting this strange error, even if the quantiz.config file is present
          in the repo


          FileNotFoundError: [Errno 2] No such file or directory: ''TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/quantize_config.json'''
        updatedAt: '2023-05-15T20:48:14.011Z'
      numEdits: 1
      reactions: []
    id: 64629a6ad9b490218810b170
    type: comment
  author: Kurapika993
  content: 'While running: model = AutoGPTQForCausalLM.from_quantized(TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ,
    device="cuda:5", use_triton=False)


    I am getting this strange error, even if the quantiz.config file is present in
    the repo


    FileNotFoundError: [Errno 2] No such file or directory: ''TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/quantize_config.json'''
  created_at: 2023-05-15 19:47:38+00:00
  edited: true
  hidden: false
  id: 64629a6ad9b490218810b170
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
      fullname: Mayukh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kurapika993
      type: user
    createdAt: '2023-05-15T21:34:09.000Z'
    data:
      edited: false
      editors:
      - Kurapika993
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
          fullname: Mayukh
          isHf: false
          isPro: false
          name: Kurapika993
          type: user
        html: '<p>On cloning the model and running I am also getting this error</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(''/path/OpenAssistant-SFT-7-Llama-30B-GPTQ'',
          model_basename= ''OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit'', use_safetensors=True,
          device="cuda:5")</p>

          <p>SafetensorError                           Traceback (most recent call
          last)<br>Cell In[46], line 1<br>----&gt; 1 model = AutoGPTQForCausalLM.from_quantized(''/home/das/model/OpenAssistant-SFT-7-Llama-30B-GPTQ'',
          model_basename= ''OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit'', use_safetensors=True,
          device="cuda:5")</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:63,
          in AutoGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,
          use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)<br>     49
          @classmethod<br>     50 def from_quantized(<br>     51     cls,<br>   (...)<br>     60     trust_remote_code:
          bool = False<br>     61 ) -&gt; BaseGPTQForCausalLM:<br>     62     model_type
          = check_and_get_model_type(save_dir)<br>---&gt; 63     return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(<br>     64         save_dir=save_dir,<br>     65         device=device,<br>     66         use_safetensors=use_safetensors,<br>     67         use_triton=use_triton,<br>     68         max_memory=max_memory,<br>     69         device_map=device_map,<br>     70         quantize_config=quantize_config,<br>     71         model_basename=model_basename,<br>     72         trust_remote_code=trust_remote_code<br>     73     )</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:544,
          in BaseGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,
          use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)<br>    541
          if not max_memory and not device_map:<br>    542     device_map = {"": device}<br>--&gt;
          544 model = accelerate.load_checkpoint_and_dispatch(<br>    545     model,
          model_save_name, device_map, max_memory, no_split_module_classes=[cls.layer_type]<br>    546
          )<br>    548 model_config = model.config.to_dict()<br>    549 seq_len_keys
          = ["max_position_embeddings", "seq_length", "n_positions"]</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/big_modeling.py:479,
          in load_checkpoint_and_dispatch(model, checkpoint, device_map, max_memory,
          no_split_module_classes, offload_folder, offload_buffers, dtype, offload_state_dict,
          preload_module_classes)<br>    477 if offload_state_dict is None and device_map
          is not None and "disk" in device_map.values():<br>    478     offload_state_dict
          = True<br>--&gt; 479 load_checkpoint_in_model(<br>    480     model,<br>    481     checkpoint,<br>    482     device_map=device_map,<br>    483     offload_folder=offload_folder,<br>    484     dtype=dtype,<br>    485     offload_state_dict=offload_state_dict,<br>    486     offload_buffers=offload_buffers,<br>    487
          )<br>    488 if device_map is None:<br>    489     return model</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py:971,
          in load_checkpoint_in_model(model, checkpoint, device_map, offload_folder,
          dtype, offload_state_dict, offload_buffers)<br>    968 buffer_names = [name
          for name, _ in model.named_buffers()]<br>    970 for checkpoint_file in
          checkpoint_files:<br>--&gt; 971     checkpoint = load_state_dict(checkpoint_file,
          device_map=device_map)<br>    972     if device_map is None:<br>    973         model.load_state_dict(checkpoint,
          strict=False)</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py:807,
          in load_state_dict(checkpoint_file, device_map)<br>    803 if not is_safetensors_available():<br>    804     raise
          ImportError(<br>    805         f"To load {checkpoint_file}, the <code>safetensors</code>
          library is necessary <code>pip install safetensors</code>."<br>    806     )<br>--&gt;
          807 with safe_open(checkpoint_file, framework="pt") as f:<br>    808     metadata
          = f.metadata()<br>    809     weight_names = f.keys()</p>

          <p>SafetensorError: Error while deserializing header: HeaderTooLarge</p>

          '
        raw: "On cloning the model and running I am also getting this error\n\nmodel\
          \ = AutoGPTQForCausalLM.from_quantized('/path/OpenAssistant-SFT-7-Llama-30B-GPTQ',\
          \ model_basename= 'OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit', use_safetensors=True,\
          \ device=\"cuda:5\")\n\n\nSafetensorError                           Traceback\
          \ (most recent call last)\nCell In[46], line 1\n----> 1 model = AutoGPTQForCausalLM.from_quantized('/home/das/model/OpenAssistant-SFT-7-Llama-30B-GPTQ',\
          \ model_basename= 'OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit', use_safetensors=True,\
          \ device=\"cuda:5\")\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:63,\
          \ in AutoGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,\
          \ use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
          \     49 @classmethod\n     50 def from_quantized(\n     51     cls,\n \
          \  (...)\n     60     trust_remote_code: bool = False\n     61 ) -> BaseGPTQForCausalLM:\n\
          \     62     model_type = check_and_get_model_type(save_dir)\n---> 63  \
          \   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\n     64\
          \         save_dir=save_dir,\n     65         device=device,\n     66  \
          \       use_safetensors=use_safetensors,\n     67         use_triton=use_triton,\n\
          \     68         max_memory=max_memory,\n     69         device_map=device_map,\n\
          \     70         quantize_config=quantize_config,\n     71         model_basename=model_basename,\n\
          \     72         trust_remote_code=trust_remote_code\n     73     )\n\n\
          File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:544,\
          \ in BaseGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,\
          \ use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
          \    541 if not max_memory and not device_map:\n    542     device_map =\
          \ {\"\": device}\n--> 544 model = accelerate.load_checkpoint_and_dispatch(\n\
          \    545     model, model_save_name, device_map, max_memory, no_split_module_classes=[cls.layer_type]\n\
          \    546 )\n    548 model_config = model.config.to_dict()\n    549 seq_len_keys\
          \ = [\"max_position_embeddings\", \"seq_length\", \"n_positions\"]\n\nFile\
          \ ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/big_modeling.py:479,\
          \ in load_checkpoint_and_dispatch(model, checkpoint, device_map, max_memory,\
          \ no_split_module_classes, offload_folder, offload_buffers, dtype, offload_state_dict,\
          \ preload_module_classes)\n    477 if offload_state_dict is None and device_map\
          \ is not None and \"disk\" in device_map.values():\n    478     offload_state_dict\
          \ = True\n--> 479 load_checkpoint_in_model(\n    480     model,\n    481\
          \     checkpoint,\n    482     device_map=device_map,\n    483     offload_folder=offload_folder,\n\
          \    484     dtype=dtype,\n    485     offload_state_dict=offload_state_dict,\n\
          \    486     offload_buffers=offload_buffers,\n    487 )\n    488 if device_map\
          \ is None:\n    489     return model\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py:971,\
          \ in load_checkpoint_in_model(model, checkpoint, device_map, offload_folder,\
          \ dtype, offload_state_dict, offload_buffers)\n    968 buffer_names = [name\
          \ for name, _ in model.named_buffers()]\n    970 for checkpoint_file in\
          \ checkpoint_files:\n--> 971     checkpoint = load_state_dict(checkpoint_file,\
          \ device_map=device_map)\n    972     if device_map is None:\n    973  \
          \       model.load_state_dict(checkpoint, strict=False)\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py:807,\
          \ in load_state_dict(checkpoint_file, device_map)\n    803 if not is_safetensors_available():\n\
          \    804     raise ImportError(\n    805         f\"To load {checkpoint_file},\
          \ the `safetensors` library is necessary `pip install safetensors`.\"\n\
          \    806     )\n--> 807 with safe_open(checkpoint_file, framework=\"pt\"\
          ) as f:\n    808     metadata = f.metadata()\n    809     weight_names =\
          \ f.keys()\n\nSafetensorError: Error while deserializing header: HeaderTooLarge"
        updatedAt: '2023-05-15T21:34:09.750Z'
      numEdits: 0
      reactions: []
    id: 6462a55148e13890ea5577b8
    type: comment
  author: Kurapika993
  content: "On cloning the model and running I am also getting this error\n\nmodel\
    \ = AutoGPTQForCausalLM.from_quantized('/path/OpenAssistant-SFT-7-Llama-30B-GPTQ',\
    \ model_basename= 'OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit', use_safetensors=True,\
    \ device=\"cuda:5\")\n\n\nSafetensorError                           Traceback\
    \ (most recent call last)\nCell In[46], line 1\n----> 1 model = AutoGPTQForCausalLM.from_quantized('/home/das/model/OpenAssistant-SFT-7-Llama-30B-GPTQ',\
    \ model_basename= 'OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit', use_safetensors=True,\
    \ device=\"cuda:5\")\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:63,\
    \ in AutoGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,\
    \ use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
    \     49 @classmethod\n     50 def from_quantized(\n     51     cls,\n   (...)\n\
    \     60     trust_remote_code: bool = False\n     61 ) -> BaseGPTQForCausalLM:\n\
    \     62     model_type = check_and_get_model_type(save_dir)\n---> 63     return\
    \ GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\n     64         save_dir=save_dir,\n\
    \     65         device=device,\n     66         use_safetensors=use_safetensors,\n\
    \     67         use_triton=use_triton,\n     68         max_memory=max_memory,\n\
    \     69         device_map=device_map,\n     70         quantize_config=quantize_config,\n\
    \     71         model_basename=model_basename,\n     72         trust_remote_code=trust_remote_code\n\
    \     73     )\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:544,\
    \ in BaseGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,\
    \ use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
    \    541 if not max_memory and not device_map:\n    542     device_map = {\"\"\
    : device}\n--> 544 model = accelerate.load_checkpoint_and_dispatch(\n    545 \
    \    model, model_save_name, device_map, max_memory, no_split_module_classes=[cls.layer_type]\n\
    \    546 )\n    548 model_config = model.config.to_dict()\n    549 seq_len_keys\
    \ = [\"max_position_embeddings\", \"seq_length\", \"n_positions\"]\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/big_modeling.py:479,\
    \ in load_checkpoint_and_dispatch(model, checkpoint, device_map, max_memory, no_split_module_classes,\
    \ offload_folder, offload_buffers, dtype, offload_state_dict, preload_module_classes)\n\
    \    477 if offload_state_dict is None and device_map is not None and \"disk\"\
    \ in device_map.values():\n    478     offload_state_dict = True\n--> 479 load_checkpoint_in_model(\n\
    \    480     model,\n    481     checkpoint,\n    482     device_map=device_map,\n\
    \    483     offload_folder=offload_folder,\n    484     dtype=dtype,\n    485\
    \     offload_state_dict=offload_state_dict,\n    486     offload_buffers=offload_buffers,\n\
    \    487 )\n    488 if device_map is None:\n    489     return model\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py:971,\
    \ in load_checkpoint_in_model(model, checkpoint, device_map, offload_folder, dtype,\
    \ offload_state_dict, offload_buffers)\n    968 buffer_names = [name for name,\
    \ _ in model.named_buffers()]\n    970 for checkpoint_file in checkpoint_files:\n\
    --> 971     checkpoint = load_state_dict(checkpoint_file, device_map=device_map)\n\
    \    972     if device_map is None:\n    973         model.load_state_dict(checkpoint,\
    \ strict=False)\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py:807,\
    \ in load_state_dict(checkpoint_file, device_map)\n    803 if not is_safetensors_available():\n\
    \    804     raise ImportError(\n    805         f\"To load {checkpoint_file},\
    \ the `safetensors` library is necessary `pip install safetensors`.\"\n    806\
    \     )\n--> 807 with safe_open(checkpoint_file, framework=\"pt\") as f:\n   \
    \ 808     metadata = f.metadata()\n    809     weight_names = f.keys()\n\nSafetensorError:\
    \ Error while deserializing header: HeaderTooLarge"
  created_at: 2023-05-15 20:34:09+00:00
  edited: false
  hidden: false
  id: 6462a55148e13890ea5577b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-15T21:49:57.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>AutoGPTQ doesn't currently support cloning models direct from HF.\
          \ You need to download it first.  The ability to directly query HF models\
          \ is planned to be added in the near future.</p>\n<p>Also, you will need\
          \ to pass a <code>model_basename</code> parameter to tell it the name of\
          \ the model file to use.</p>\n<p>Here's an example of how to use it. I use\
          \ the <a rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/blob/main/download-model.py\"\
          >download-model.py script from text-generation-webui</a> for quick and easy\
          \ downloading of HF models.</p>\n<pre><code>root@1c6b80974469:/workspace/test#\
          \ python ~/text-generation-webui/download-model.py TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \ --threads 2\nDownloading the model to models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\n\
          ...\nroot@1c6b80974469:/workspace# ll models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/\n\
          total 16551713\ndrwxrwxrwx  2 root root     3001577 May 15 21:37 ./\ndrwxrwxrwx\
          \ 22 root root     3040539 May 15 21:37 ../\n-rw-rw-rw-  1 root root 16940554392\
          \ May 15 21:43 OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors\n-rw-rw-rw-\
          \  1 root root        7367 May 15 21:37 README.md\n-rw-rw-rw-  1 root root\
          \         133 May 15 21:37 added_tokens.json\n-rw-rw-rw-  1 root root  \
          \       568 May 15 21:37 config.json\n-rw-rw-rw-  1 root root         137\
          \ May 15 21:37 generation_config.json\n-rw-rw-rw-  1 root root         337\
          \ May 15 21:37 huggingface-metadata.txt\n-rw-rw-rw-  1 root root       \
          \  122 May 15 21:37 quantize_config.json\n-rw-rw-rw-  1 root root      \
          \   477 May 15 21:37 special_tokens_map.json\n-rw-rw-rw-  1 root root  \
          \   1843612 May 15 21:37 tokenizer.json\n-rw-rw-rw-  1 root root      499723\
          \ May 15 21:37 tokenizer.model\n-rw-rw-rw-  1 root root         715 May\
          \ 15 21:37 tokenizer_config.json\n</code></pre>\n<p>Now run this script:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ pipeline, logging\n<span class=\"hljs-keyword\">from</span> auto_gptq\
          \ <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          <span class=\"hljs-keyword\">import</span> argparse\n\nquantized_model_dir\
          \ = <span class=\"hljs-string\">\"/workspace/models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\"\
          </span>\n\nmodel_basename = <span class=\"hljs-string\">\"OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit\"\
          </span>\n\nuse_strict = <span class=\"hljs-literal\">False</span>\n\nuse_triton\
          \ = <span class=\"hljs-literal\">False</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"Loading tokenizer\"</span>)\n\
          tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=<span\
          \ class=\"hljs-literal\">True</span>)\n\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"Loading model\"</span>)\nmodel\
          \ = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n        use_safetensors=<span\
          \ class=\"hljs-literal\">True</span>,\n        strict=use_strict,\n    \
          \    model_basename=model_basename,\n        device=<span class=\"hljs-string\"\
          >\"cuda:0\"</span>,\n        use_triton=use_triton,\n        quantize_config=<span\
          \ class=\"hljs-literal\">None</span>) <span class=\"hljs-comment\"># quantize_config\
          \ will be loaded from the supplied quantize_config.json</span>\n\n<span\
          \ class=\"hljs-comment\"># Inference using model.pipeline()</span>\n\n<span\
          \ class=\"hljs-comment\"># Prevent printing spurious transformers error\
          \ when using pipeline with AutoGPTQ - known bug.</span>\nlogging.set_verbosity(logging.CRITICAL)\n\
          \nprompt = <span class=\"hljs-string\">\"Tell me about AI\"</span>\nprompt_template=<span\
          \ class=\"hljs-string\">f'''### Human: <span class=\"hljs-subst\">{prompt}</span></span>\n\
          <span class=\"hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"</span>)\npipe\
          \ = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span class=\"\
          hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n\n<span class=\"hljs-comment\"\
          ># Inference using model.generate()</span>\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n<p>Output:</p>\n<pre><code>root@1c6b80974469:/workspace#\
          \ python simple_gptq_openassistant.py\nLoading tokenizer\nLoading model\n\
          *** Pipeline:\n### Human: Tell me about AI\n### Assistant: Artificial Intelligence\
          \ (AI) is a field of computer science that focuses on creating machines\
          \ capable of performing tasks that would normally require human intelligence.\
          \ These tasks include understanding natural language, recognizing objects\
          \ and patterns, making decisions based on data, and learning from experience.\n\
          \nThe goal of AI research is to create intelligent systems that can work\
          \ alongside humans or replace them in certain tasks. This includes developing\
          \ algorithms and models that can learn from large amounts of data and make\
          \ predictions or take actions based on that information.\n\nThere are many\
          \ different types of AI, including machine learning, deep learning, natural\
          \ language processing, robotics, and more. Each type has its own strengths\
          \ and weaknesses, and they are used for a wide range of applications, such\
          \ as image recognition, speech recognition, autonomous vehicles, chatbots,\
          \ and much more.\n\n\n*** Generate:\n&lt;s&gt; ### Human: Tell me about\
          \ AI\n### Assistant: AI stands for Artificial Intelligence. It refers to\
          \ the development of computer systems that can perform tasks that would\
          \ normally require human intelligence, such as visual perception, speech\
          \ recognition, decision-making, and language translation. AI systems are\
          \ designed to learn from experience and adapt to new data, making them increasingly\
          \ effective at performing complex tasks over time. Some common applications\
          \ of AI include natural language processing, robotics, and machine learning.&lt;/s&gt;\n\
          root@1c6b80974469:/workspace#\n</code></pre>\n"
        raw: "AutoGPTQ doesn't currently support cloning models direct from HF. You\
          \ need to download it first.  The ability to directly query HF models is\
          \ planned to be added in the near future.\n\nAlso, you will need to pass\
          \ a `model_basename` parameter to tell it the name of the model file to\
          \ use.\n\nHere's an example of how to use it. I use the [download-model.py\
          \ script from text-generation-webui](https://github.com/oobabooga/text-generation-webui/blob/main/download-model.py)\
          \ for quick and easy downloading of HF models.\n```\nroot@1c6b80974469:/workspace/test#\
          \ python ~/text-generation-webui/download-model.py TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \ --threads 2\nDownloading the model to models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\n\
          ...\nroot@1c6b80974469:/workspace# ll models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/\n\
          total 16551713\ndrwxrwxrwx  2 root root     3001577 May 15 21:37 ./\ndrwxrwxrwx\
          \ 22 root root     3040539 May 15 21:37 ../\n-rw-rw-rw-  1 root root 16940554392\
          \ May 15 21:43 OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors\n-rw-rw-rw-\
          \  1 root root        7367 May 15 21:37 README.md\n-rw-rw-rw-  1 root root\
          \         133 May 15 21:37 added_tokens.json\n-rw-rw-rw-  1 root root  \
          \       568 May 15 21:37 config.json\n-rw-rw-rw-  1 root root         137\
          \ May 15 21:37 generation_config.json\n-rw-rw-rw-  1 root root         337\
          \ May 15 21:37 huggingface-metadata.txt\n-rw-rw-rw-  1 root root       \
          \  122 May 15 21:37 quantize_config.json\n-rw-rw-rw-  1 root root      \
          \   477 May 15 21:37 special_tokens_map.json\n-rw-rw-rw-  1 root root  \
          \   1843612 May 15 21:37 tokenizer.json\n-rw-rw-rw-  1 root root      499723\
          \ May 15 21:37 tokenizer.model\n-rw-rw-rw-  1 root root         715 May\
          \ 15 21:37 tokenizer_config.json\n```\n\nNow run this script:\n```python\n\
          from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\nquantized_model_dir\
          \ = \"/workspace/models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\"\n\n\
          model_basename = \"OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit\"\n\nuse_strict\
          \ = False\n\nuse_triton = False\n\nprint(\"Loading tokenizer\")\ntokenizer\
          \ = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)\n\n\
          \nprint(\"Loading model\")\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=True,\n        strict=use_strict,\n        model_basename=model_basename,\n\
          \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\
          \ # quantize_config will be loaded from the supplied quantize_config.json\n\
          \n# Inference using model.pipeline()\n\n# Prevent printing spurious transformers\
          \ error when using pipeline with AutoGPTQ - known bug.\nlogging.set_verbosity(logging.CRITICAL)\n\
          \nprompt = \"Tell me about AI\"\nprompt_template=f'''### Human: {prompt}\n\
          ### Assistant:'''\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"\
          text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n\
          \    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\
          \nprint(pipe(prompt_template)[0]['generated_text'])\n\n# Inference using\
          \ model.generate()\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\
          ```\n\nOutput:\n```\nroot@1c6b80974469:/workspace# python simple_gptq_openassistant.py\n\
          Loading tokenizer\nLoading model\n*** Pipeline:\n### Human: Tell me about\
          \ AI\n### Assistant: Artificial Intelligence (AI) is a field of computer\
          \ science that focuses on creating machines capable of performing tasks\
          \ that would normally require human intelligence. These tasks include understanding\
          \ natural language, recognizing objects and patterns, making decisions based\
          \ on data, and learning from experience.\n\nThe goal of AI research is to\
          \ create intelligent systems that can work alongside humans or replace them\
          \ in certain tasks. This includes developing algorithms and models that\
          \ can learn from large amounts of data and make predictions or take actions\
          \ based on that information.\n\nThere are many different types of AI, including\
          \ machine learning, deep learning, natural language processing, robotics,\
          \ and more. Each type has its own strengths and weaknesses, and they are\
          \ used for a wide range of applications, such as image recognition, speech\
          \ recognition, autonomous vehicles, chatbots, and much more.\n\n\n*** Generate:\n\
          <s> ### Human: Tell me about AI\n### Assistant: AI stands for Artificial\
          \ Intelligence. It refers to the development of computer systems that can\
          \ perform tasks that would normally require human intelligence, such as\
          \ visual perception, speech recognition, decision-making, and language translation.\
          \ AI systems are designed to learn from experience and adapt to new data,\
          \ making them increasingly effective at performing complex tasks over time.\
          \ Some common applications of AI include natural language processing, robotics,\
          \ and machine learning.</s>\nroot@1c6b80974469:/workspace#\n```"
        updatedAt: '2023-05-15T21:49:57.214Z'
      numEdits: 0
      reactions: []
    id: 6462a90557b29d859fa75a5d
    type: comment
  author: TheBloke
  content: "AutoGPTQ doesn't currently support cloning models direct from HF. You\
    \ need to download it first.  The ability to directly query HF models is planned\
    \ to be added in the near future.\n\nAlso, you will need to pass a `model_basename`\
    \ parameter to tell it the name of the model file to use.\n\nHere's an example\
    \ of how to use it. I use the [download-model.py script from text-generation-webui](https://github.com/oobabooga/text-generation-webui/blob/main/download-model.py)\
    \ for quick and easy downloading of HF models.\n```\nroot@1c6b80974469:/workspace/test#\
    \ python ~/text-generation-webui/download-model.py TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\
    \ --threads 2\nDownloading the model to models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\n\
    ...\nroot@1c6b80974469:/workspace# ll models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/\n\
    total 16551713\ndrwxrwxrwx  2 root root     3001577 May 15 21:37 ./\ndrwxrwxrwx\
    \ 22 root root     3040539 May 15 21:37 ../\n-rw-rw-rw-  1 root root 16940554392\
    \ May 15 21:43 OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors\n-rw-rw-rw-\
    \  1 root root        7367 May 15 21:37 README.md\n-rw-rw-rw-  1 root root   \
    \      133 May 15 21:37 added_tokens.json\n-rw-rw-rw-  1 root root         568\
    \ May 15 21:37 config.json\n-rw-rw-rw-  1 root root         137 May 15 21:37 generation_config.json\n\
    -rw-rw-rw-  1 root root         337 May 15 21:37 huggingface-metadata.txt\n-rw-rw-rw-\
    \  1 root root         122 May 15 21:37 quantize_config.json\n-rw-rw-rw-  1 root\
    \ root         477 May 15 21:37 special_tokens_map.json\n-rw-rw-rw-  1 root root\
    \     1843612 May 15 21:37 tokenizer.json\n-rw-rw-rw-  1 root root      499723\
    \ May 15 21:37 tokenizer.model\n-rw-rw-rw-  1 root root         715 May 15 21:37\
    \ tokenizer_config.json\n```\n\nNow run this script:\n```python\nfrom transformers\
    \ import AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
    \ BaseQuantizeConfig\nimport argparse\n\nquantized_model_dir = \"/workspace/models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\"\
    \n\nmodel_basename = \"OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit\"\n\nuse_strict\
    \ = False\n\nuse_triton = False\n\nprint(\"Loading tokenizer\")\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
    \ use_fast=True)\n\n\nprint(\"Loading model\")\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
    \        use_safetensors=True,\n        strict=use_strict,\n        model_basename=model_basename,\n\
    \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\
    \ # quantize_config will be loaded from the supplied quantize_config.json\n\n\
    # Inference using model.pipeline()\n\n# Prevent printing spurious transformers\
    \ error when using pipeline with AutoGPTQ - known bug.\nlogging.set_verbosity(logging.CRITICAL)\n\
    \nprompt = \"Tell me about AI\"\nprompt_template=f'''### Human: {prompt}\n###\
    \ Assistant:'''\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\"\
    ,\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.7,\n\
    \    top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    \n# Inference using model.generate()\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids\
    \ = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput =\
    \ model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\
    ```\n\nOutput:\n```\nroot@1c6b80974469:/workspace# python simple_gptq_openassistant.py\n\
    Loading tokenizer\nLoading model\n*** Pipeline:\n### Human: Tell me about AI\n\
    ### Assistant: Artificial Intelligence (AI) is a field of computer science that\
    \ focuses on creating machines capable of performing tasks that would normally\
    \ require human intelligence. These tasks include understanding natural language,\
    \ recognizing objects and patterns, making decisions based on data, and learning\
    \ from experience.\n\nThe goal of AI research is to create intelligent systems\
    \ that can work alongside humans or replace them in certain tasks. This includes\
    \ developing algorithms and models that can learn from large amounts of data and\
    \ make predictions or take actions based on that information.\n\nThere are many\
    \ different types of AI, including machine learning, deep learning, natural language\
    \ processing, robotics, and more. Each type has its own strengths and weaknesses,\
    \ and they are used for a wide range of applications, such as image recognition,\
    \ speech recognition, autonomous vehicles, chatbots, and much more.\n\n\n*** Generate:\n\
    <s> ### Human: Tell me about AI\n### Assistant: AI stands for Artificial Intelligence.\
    \ It refers to the development of computer systems that can perform tasks that\
    \ would normally require human intelligence, such as visual perception, speech\
    \ recognition, decision-making, and language translation. AI systems are designed\
    \ to learn from experience and adapt to new data, making them increasingly effective\
    \ at performing complex tasks over time. Some common applications of AI include\
    \ natural language processing, robotics, and machine learning.</s>\nroot@1c6b80974469:/workspace#\n\
    ```"
  created_at: 2023-05-15 20:49:57+00:00
  edited: false
  hidden: false
  id: 6462a90557b29d859fa75a5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
      fullname: Mayukh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kurapika993
      type: user
    createdAt: '2023-05-16T10:01:38.000Z'
    data:
      edited: false
      editors:
      - Kurapika993
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
          fullname: Mayukh
          isHf: false
          isPro: false
          name: Kurapika993
          type: user
        html: '<p>Can you give your environments requirement.txt</p>

          '
        raw: Can you give your environments requirement.txt
        updatedAt: '2023-05-16T10:01:38.601Z'
      numEdits: 0
      reactions: []
    id: 64635482c758f9420909f0bc
    type: comment
  author: Kurapika993
  content: Can you give your environments requirement.txt
  created_at: 2023-05-16 09:01:38+00:00
  edited: false
  hidden: false
  id: 64635482c758f9420909f0bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-16T10:03:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Just follow the instructions in the AutoGPTQ README.  You should
          be able to just do: <code>pip install auto-gptq</code>.  Or for the latest
          code:</p>

          <pre><code>git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          </code></pre>

          '
        raw: 'Just follow the instructions in the AutoGPTQ README.  You should be
          able to just do: `pip install auto-gptq`.  Or for the latest code:

          ```

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          ```'
        updatedAt: '2023-05-16T10:03:25.812Z'
      numEdits: 0
      reactions: []
    id: 646354ed95e64061c97a8bf3
    type: comment
  author: TheBloke
  content: 'Just follow the instructions in the AutoGPTQ README.  You should be able
    to just do: `pip install auto-gptq`.  Or for the latest code:

    ```

    git clone https://github.com/PanQiWei/AutoGPTQ

    cd AutoGPTQ

    pip install .

    ```'
  created_at: 2023-05-16 09:03:25+00:00
  edited: false
  hidden: false
  id: 646354ed95e64061c97a8bf3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
      fullname: Mayukh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kurapika993
      type: user
    createdAt: '2023-05-16T10:16:00.000Z'
    data:
      edited: false
      editors:
      - Kurapika993
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
          fullname: Mayukh
          isHf: false
          isPro: false
          name: Kurapika993
          type: user
        html: '<p>can you tell me about this error while loading the tokenizer, is
          there anything wrong with the model path ??</p>

          <h2 id="loading-tokenizer">Loading tokenizer</h2>

          <p>HFValidationError                         Traceback (most recent call
          last)<br>Cell In[3], line 14<br>     11 use_triton = False<br>     13 print("Loading
          tokenizer")<br>---&gt; 14 tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,
          use_fast=True)</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:642,
          in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,
          **kwargs)<br>    639     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,
          *inputs, **kwargs)<br>    641 # Next, let''s try to use the tokenizer_config
          file to get the tokenizer class.<br>--&gt; 642 tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path,
          **kwargs)<br>    643 if "_commit_hash" in tokenizer_config:<br>    644     kwargs["_commit_hash"]
          = tokenizer_config["_commit_hash"]</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:486,
          in get_tokenizer_config(pretrained_model_name_or_path, cache_dir, force_download,
          resume_download, proxies, use_auth_token, revision, local_files_only, subfolder,
          **kwargs)<br>    424 """<br>    425 Loads the tokenizer configuration from
          a pretrained model tokenizer configuration.<br>    426<br>   (...)<br>    483
          tokenizer_config = get_tokenizer_config("tokenizer-test")<br>    484 ```"""<br>    485
          commit_hash = kwargs.get("_commit_hash", None)<br>--&gt; 486 resolved_config_file
          = cached_file(<br>    487     pretrained_model_name_or_path,<br>    488     TOKENIZER_CONFIG_FILE,<br>    489     cache_dir=cache_dir,<br>    490     force_download=force_download,<br>    491     resume_download=resume_download,<br>    492     proxies=proxies,<br>    493     use_auth_token=use_auth_token,<br>    494     revision=revision,<br>    495     local_files_only=local_files_only,<br>    496     subfolder=subfolder,<br>    497     _raise_exceptions_for_missing_entries=False,<br>    498     _raise_exceptions_for_connection_errors=False,<br>    499     _commit_hash=commit_hash,<br>    500
          )<br>    501 if resolved_config_file is None:<br>    502     logger.info("Could
          not locate the tokenizer configuration file, will try to use the model config
          instead.")</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/utils/hub.py:409,
          in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,
          proxies, use_auth_token, revision, local_files_only, subfolder, user_agent,
          _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,
          _commit_hash)<br>    406 user_agent = http_user_agent(user_agent)<br>    407
          try:<br>    408     # Load from URL or cache if already cached<br>--&gt;
          409     resolved_file = hf_hub_download(<br>    410         path_or_repo_id,<br>    411         filename,<br>    412         subfolder=None
          if len(subfolder) == 0 else subfolder,<br>    413         revision=revision,<br>    414         cache_dir=cache_dir,<br>    415         user_agent=user_agent,<br>    416         force_download=force_download,<br>    417         proxies=proxies,<br>    418         resume_download=resume_download,<br>    419         use_auth_token=use_auth_token,<br>    420         local_files_only=local_files_only,<br>    421     )<br>    423
          except RepositoryNotFoundError:<br>    424     raise EnvironmentError(<br>    425         f"{path_or_repo_id}
          is not a local folder and is not a valid model identifier "<br>    426         "listed
          on ''<a href="https://huggingface.co/models''%5CnIf">https://huggingface.co/models''\nIf</a>
          this is a private repository, make sure to "<br>    427         "pass a
          token having permission to this repo with <code>use_auth_token</code> or
          log in with "<br>    428         "<code>huggingface-cli login</code> and
          pass <code>use_auth_token=True</code>."<br>    429     )</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:112,
          in validate_hf_hub_args.._inner_fn(*args, **kwargs)<br>    107 for arg_name,
          arg_value in chain(<br>    108     zip(signature.parameters, args),  # Args
          values<br>    109     kwargs.items(),  # Kwargs values<br>    110 ):<br>    111     if
          arg_name in ["repo_id", "from_id", "to_id"]:<br>--&gt; 112         validate_repo_id(arg_value)<br>    114     elif
          arg_name == "token" and arg_value is not None:<br>    115         has_token
          = True</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/<em>validators.py:160,
          in validate_repo_id(repo_id)<br>    157     raise HFValidationError(f"Repo
          id must be a string, not {type(repo_id)}: ''{repo_id}''.")<br>    159 if
          repo_id.count("/") &gt; 1:<br>--&gt; 160     raise HFValidationError(<br>    161         "Repo
          id must be in the form ''repo_name'' or ''namespace/repo_name'':"<br>    162         f"
          ''{repo_id}''. Use <code>repo_type</code> argument if needed."<br>    163     )<br>    165
          if not REPO_ID_REGEX.match(repo_id):<br>    166     raise HFValidationError(<br>    167         "Repo
          id must use alphanumeric chars or ''-'', ''</em>'', ''.'', ''--'' and ''..''
          are"<br>    168         " forbidden, ''-'' and ''.'' cannot start or end
          the name, max length is 96:"<br>    169         f" ''{repo_id}''."<br>    170     )</p>

          <p>HFValidationError: Repo id must be in the form ''repo_name'' or ''namespace/repo_name'':
          ''/workspace/models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ''. Use <code>repo_type</code>
          argument if neede</p>

          '
        raw: "can you tell me about this error while loading the tokenizer, is there\
          \ anything wrong with the model path ??\n\nLoading tokenizer\n---------------------------------------------------------------------------\n\
          HFValidationError                         Traceback (most recent call last)\n\
          Cell In[3], line 14\n     11 use_triton = False\n     13 print(\"Loading\
          \ tokenizer\")\n---> 14 tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:642,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\n    639     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\n    641 # Next, let's try to use the tokenizer_config\
          \ file to get the tokenizer class.\n--> 642 tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path,\
          \ **kwargs)\n    643 if \"_commit_hash\" in tokenizer_config:\n    644 \
          \    kwargs[\"_commit_hash\"] = tokenizer_config[\"_commit_hash\"]\n\nFile\
          \ ~/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:486,\
          \ in get_tokenizer_config(pretrained_model_name_or_path, cache_dir, force_download,\
          \ resume_download, proxies, use_auth_token, revision, local_files_only,\
          \ subfolder, **kwargs)\n    424 \"\"\"\n    425 Loads the tokenizer configuration\
          \ from a pretrained model tokenizer configuration.\n    426 \n   (...)\n\
          \    483 tokenizer_config = get_tokenizer_config(\"tokenizer-test\")\n \
          \   484 ```\"\"\"\n    485 commit_hash = kwargs.get(\"_commit_hash\", None)\n\
          --> 486 resolved_config_file = cached_file(\n    487     pretrained_model_name_or_path,\n\
          \    488     TOKENIZER_CONFIG_FILE,\n    489     cache_dir=cache_dir,\n\
          \    490     force_download=force_download,\n    491     resume_download=resume_download,\n\
          \    492     proxies=proxies,\n    493     use_auth_token=use_auth_token,\n\
          \    494     revision=revision,\n    495     local_files_only=local_files_only,\n\
          \    496     subfolder=subfolder,\n    497     _raise_exceptions_for_missing_entries=False,\n\
          \    498     _raise_exceptions_for_connection_errors=False,\n    499   \
          \  _commit_hash=commit_hash,\n    500 )\n    501 if resolved_config_file\
          \ is None:\n    502     logger.info(\"Could not locate the tokenizer configuration\
          \ file, will try to use the model config instead.\")\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/utils/hub.py:409,\
          \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
          \ proxies, use_auth_token, revision, local_files_only, subfolder, user_agent,\
          \ _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
          \ _commit_hash)\n    406 user_agent = http_user_agent(user_agent)\n    407\
          \ try:\n    408     # Load from URL or cache if already cached\n--> 409\
          \     resolved_file = hf_hub_download(\n    410         path_or_repo_id,\n\
          \    411         filename,\n    412         subfolder=None if len(subfolder)\
          \ == 0 else subfolder,\n    413         revision=revision,\n    414    \
          \     cache_dir=cache_dir,\n    415         user_agent=user_agent,\n   \
          \ 416         force_download=force_download,\n    417         proxies=proxies,\n\
          \    418         resume_download=resume_download,\n    419         use_auth_token=use_auth_token,\n\
          \    420         local_files_only=local_files_only,\n    421     )\n   \
          \ 423 except RepositoryNotFoundError:\n    424     raise EnvironmentError(\n\
          \    425         f\"{path_or_repo_id} is not a local folder and is not a\
          \ valid model identifier \"\n    426         \"listed on 'https://huggingface.co/models'\\\
          nIf this is a private repository, make sure to \"\n    427         \"pass\
          \ a token having permission to this repo with `use_auth_token` or log in\
          \ with \"\n    428         \"`huggingface-cli login` and pass `use_auth_token=True`.\"\
          \n    429     )\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:112,\
          \ in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)\n    107 for\
          \ arg_name, arg_value in chain(\n    108     zip(signature.parameters, args),\
          \  # Args values\n    109     kwargs.items(),  # Kwargs values\n    110\
          \ ):\n    111     if arg_name in [\"repo_id\", \"from_id\", \"to_id\"]:\n\
          --> 112         validate_repo_id(arg_value)\n    114     elif arg_name ==\
          \ \"token\" and arg_value is not None:\n    115         has_token = True\n\
          \nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:160,\
          \ in validate_repo_id(repo_id)\n    157     raise HFValidationError(f\"\
          Repo id must be a string, not {type(repo_id)}: '{repo_id}'.\")\n    159\
          \ if repo_id.count(\"/\") > 1:\n--> 160     raise HFValidationError(\n \
          \   161         \"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\
          \n    162         f\" '{repo_id}'. Use `repo_type` argument if needed.\"\
          \n    163     )\n    165 if not REPO_ID_REGEX.match(repo_id):\n    166 \
          \    raise HFValidationError(\n    167         \"Repo id must use alphanumeric\
          \ chars or '-', '_', '.', '--' and '..' are\"\n    168         \" forbidden,\
          \ '-' and '.' cannot start or end the name, max length is 96:\"\n    169\
          \         f\" '{repo_id}'.\"\n    170     )\n\nHFValidationError: Repo id\
          \ must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ'.\
          \ Use `repo_type` argument if neede"
        updatedAt: '2023-05-16T10:16:00.610Z'
      numEdits: 0
      reactions: []
    id: 646357e01ac9eb9637ce8c0c
    type: comment
  author: Kurapika993
  content: "can you tell me about this error while loading the tokenizer, is there\
    \ anything wrong with the model path ??\n\nLoading tokenizer\n---------------------------------------------------------------------------\n\
    HFValidationError                         Traceback (most recent call last)\n\
    Cell In[3], line 14\n     11 use_triton = False\n     13 print(\"Loading tokenizer\"\
    )\n---> 14 tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)\n\
    \nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:642,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\n    639     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\n    641 # Next, let's try to use the tokenizer_config file\
    \ to get the tokenizer class.\n--> 642 tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path,\
    \ **kwargs)\n    643 if \"_commit_hash\" in tokenizer_config:\n    644     kwargs[\"\
    _commit_hash\"] = tokenizer_config[\"_commit_hash\"]\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:486,\
    \ in get_tokenizer_config(pretrained_model_name_or_path, cache_dir, force_download,\
    \ resume_download, proxies, use_auth_token, revision, local_files_only, subfolder,\
    \ **kwargs)\n    424 \"\"\"\n    425 Loads the tokenizer configuration from a\
    \ pretrained model tokenizer configuration.\n    426 \n   (...)\n    483 tokenizer_config\
    \ = get_tokenizer_config(\"tokenizer-test\")\n    484 ```\"\"\"\n    485 commit_hash\
    \ = kwargs.get(\"_commit_hash\", None)\n--> 486 resolved_config_file = cached_file(\n\
    \    487     pretrained_model_name_or_path,\n    488     TOKENIZER_CONFIG_FILE,\n\
    \    489     cache_dir=cache_dir,\n    490     force_download=force_download,\n\
    \    491     resume_download=resume_download,\n    492     proxies=proxies,\n\
    \    493     use_auth_token=use_auth_token,\n    494     revision=revision,\n\
    \    495     local_files_only=local_files_only,\n    496     subfolder=subfolder,\n\
    \    497     _raise_exceptions_for_missing_entries=False,\n    498     _raise_exceptions_for_connection_errors=False,\n\
    \    499     _commit_hash=commit_hash,\n    500 )\n    501 if resolved_config_file\
    \ is None:\n    502     logger.info(\"Could not locate the tokenizer configuration\
    \ file, will try to use the model config instead.\")\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/utils/hub.py:409,\
    \ in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download,\
    \ proxies, use_auth_token, revision, local_files_only, subfolder, user_agent,\
    \ _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors,\
    \ _commit_hash)\n    406 user_agent = http_user_agent(user_agent)\n    407 try:\n\
    \    408     # Load from URL or cache if already cached\n--> 409     resolved_file\
    \ = hf_hub_download(\n    410         path_or_repo_id,\n    411         filename,\n\
    \    412         subfolder=None if len(subfolder) == 0 else subfolder,\n    413\
    \         revision=revision,\n    414         cache_dir=cache_dir,\n    415  \
    \       user_agent=user_agent,\n    416         force_download=force_download,\n\
    \    417         proxies=proxies,\n    418         resume_download=resume_download,\n\
    \    419         use_auth_token=use_auth_token,\n    420         local_files_only=local_files_only,\n\
    \    421     )\n    423 except RepositoryNotFoundError:\n    424     raise EnvironmentError(\n\
    \    425         f\"{path_or_repo_id} is not a local folder and is not a valid\
    \ model identifier \"\n    426         \"listed on 'https://huggingface.co/models'\\\
    nIf this is a private repository, make sure to \"\n    427         \"pass a token\
    \ having permission to this repo with `use_auth_token` or log in with \"\n   \
    \ 428         \"`huggingface-cli login` and pass `use_auth_token=True`.\"\n  \
    \  429     )\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:112,\
    \ in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)\n    107 for arg_name,\
    \ arg_value in chain(\n    108     zip(signature.parameters, args),  # Args values\n\
    \    109     kwargs.items(),  # Kwargs values\n    110 ):\n    111     if arg_name\
    \ in [\"repo_id\", \"from_id\", \"to_id\"]:\n--> 112         validate_repo_id(arg_value)\n\
    \    114     elif arg_name == \"token\" and arg_value is not None:\n    115  \
    \       has_token = True\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:160,\
    \ in validate_repo_id(repo_id)\n    157     raise HFValidationError(f\"Repo id\
    \ must be a string, not {type(repo_id)}: '{repo_id}'.\")\n    159 if repo_id.count(\"\
    /\") > 1:\n--> 160     raise HFValidationError(\n    161         \"Repo id must\
    \ be in the form 'repo_name' or 'namespace/repo_name':\"\n    162         f\"\
    \ '{repo_id}'. Use `repo_type` argument if needed.\"\n    163     )\n    165 if\
    \ not REPO_ID_REGEX.match(repo_id):\n    166     raise HFValidationError(\n  \
    \  167         \"Repo id must use alphanumeric chars or '-', '_', '.', '--' and\
    \ '..' are\"\n    168         \" forbidden, '-' and '.' cannot start or end the\
    \ name, max length is 96:\"\n    169         f\" '{repo_id}'.\"\n    170     )\n\
    \nHFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name':\
    \ '/workspace/models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ'. Use `repo_type`\
    \ argument if neede"
  created_at: 2023-05-16 09:16:00+00:00
  edited: false
  hidden: false
  id: 646357e01ac9eb9637ce8c0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-16T10:19:57.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Looks like you either didn''t download the model, or else didn''t
          update the model path correctly</p>

          '
        raw: Looks like you either didn't download the model, or else didn't update
          the model path correctly
        updatedAt: '2023-05-16T10:19:57.448Z'
      numEdits: 0
      reactions: []
    id: 646358cdab15db2fa566ef21
    type: comment
  author: TheBloke
  content: Looks like you either didn't download the model, or else didn't update
    the model path correctly
  created_at: 2023-05-16 09:19:57+00:00
  edited: false
  hidden: false
  id: 646358cdab15db2fa566ef21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
      fullname: Mayukh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kurapika993
      type: user
    createdAt: '2023-05-16T10:23:23.000Z'
    data:
      edited: false
      editors:
      - Kurapika993
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
          fullname: Mayukh
          isHf: false
          isPro: false
          name: Kurapika993
          type: user
        html: '<p>Oh sorry the it was the problem with the model path ..I  have fixed
          that</p>

          <p>I am getting this error while loading the model</p>

          <h2 id="the-safetensors-archive-passed-at-modelsthebloke_openassistant-sft-7-llama-30b-gptqopenassistant-sft-7-llama-30b-gptq-4bitsafetensors-does-not-contain-metadata-make-sure-to-save-your-model-with-the-save_pretrained-method-defaulting-to-pt-metadata">The
          safetensors archive passed at models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors
          does not contain metadata. Make sure to save your model with the <code>save_pretrained</code>
          method. Defaulting to ''pt'' metadata.</h2>

          <p>ValueError                                Traceback (most recent call
          last)<br>Cell In[8], line 2<br>      1 print("Loading model")<br>----&gt;
          2 model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,<br>      3         use_safetensors=True,<br>      4         model_basename=model_basename,<br>      5         device="cuda:5",<br>      6         quantize_config=None)</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:63,
          in AutoGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,
          use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)<br>     49
          @classmethod<br>     50 def from_quantized(<br>     51     cls,<br>   (...)<br>     60     trust_remote_code:
          bool = False<br>     61 ) -&gt; BaseGPTQForCausalLM:<br>     62     model_type
          = check_and_get_model_type(save_dir)<br>---&gt; 63     return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(<br>     64         save_dir=save_dir,<br>     65         device=device,<br>     66         use_safetensors=use_safetensors,<br>     67         use_triton=use_triton,<br>     68         max_memory=max_memory,<br>     69         device_map=device_map,<br>     70         quantize_config=quantize_config,<br>     71         model_basename=model_basename,<br>     72         trust_remote_code=trust_remote_code<br>     73     )</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:544,
          in BaseGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,
          use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)<br>    541
          if not max_memory and not device_map:<br>    542     device_map = {"": device}<br>--&gt;
          544 model = accelerate.load_checkpoint_and_dispatch(<br>    545     model,
          model_save_name, device_map, max_memory, no_split_module_classes=[cls.layer_type]<br>    546
          )<br>    548 model_config = model.config.to_dict()<br>    549 seq_len_keys
          = ["max_position_embeddings", "seq_length", "n_positions"]</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/big_modeling.py:479,
          in load_checkpoint_and_dispatch(model, checkpoint, device_map, max_memory,
          no_split_module_classes, offload_folder, offload_buffers, dtype, offload_state_dict,
          preload_module_classes)<br>    477 if offload_state_dict is None and device_map
          is not None and "disk" in device_map.values():<br>    478     offload_state_dict
          = True<br>--&gt; 479 load_checkpoint_in_model(<br>    480     model,<br>    481     checkpoint,<br>    482     device_map=device_map,<br>    483     offload_folder=offload_folder,<br>    484     dtype=dtype,<br>    485     offload_state_dict=offload_state_dict,<br>    486     offload_buffers=offload_buffers,<br>    487
          )<br>    488 if device_map is None:<br>    489     return model</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py:993,
          in load_checkpoint_in_model(model, checkpoint, device_map, offload_folder,
          dtype, offload_state_dict, offload_buffers)<br>    991             offload_weight(param,
          param_name, state_dict_folder, index=state_dict_index)<br>    992         else:<br>--&gt;
          993             set_module_tensor_to_device(model, param_name, param_device,
          value=param, dtype=dtype)<br>    995 # Force Python to clean up.<br>    996
          del checkpoint</p>

          <p>File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py:135,
          in set_module_tensor_to_device(module, tensor_name, device, value, dtype)<br>    132     tensor_name
          = splits[-1]<br>    134 if tensor_name not in module._parameters and tensor_name
          not in module._buffers:<br>--&gt; 135     raise ValueError(f"{module} does
          not have a parameter or a buffer named {tensor_name}.")<br>    136 is_buffer
          = tensor_name in module._buffers<br>    137 old_value = getattr(module,
          tensor_name)</p>

          <p>ValueError: QuantLinear() does not have a parameter or a buffer named
          bias.</p>

          '
        raw: "Oh sorry the it was the problem with the model path ..I  have fixed\
          \ that\n\n\nI am getting this error while loading the model\n\nThe safetensors\
          \ archive passed at models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\n---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[8], line 2\n      1 print(\"Loading model\")\n----> 2 model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \      3         use_safetensors=True,\n      4         model_basename=model_basename,\n\
          \      5         device=\"cuda:5\",\n      6         quantize_config=None)\n\
          \nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:63,\
          \ in AutoGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,\
          \ use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
          \     49 @classmethod\n     50 def from_quantized(\n     51     cls,\n \
          \  (...)\n     60     trust_remote_code: bool = False\n     61 ) -> BaseGPTQForCausalLM:\n\
          \     62     model_type = check_and_get_model_type(save_dir)\n---> 63  \
          \   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\n     64\
          \         save_dir=save_dir,\n     65         device=device,\n     66  \
          \       use_safetensors=use_safetensors,\n     67         use_triton=use_triton,\n\
          \     68         max_memory=max_memory,\n     69         device_map=device_map,\n\
          \     70         quantize_config=quantize_config,\n     71         model_basename=model_basename,\n\
          \     72         trust_remote_code=trust_remote_code\n     73     )\n\n\
          File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:544,\
          \ in BaseGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,\
          \ use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
          \    541 if not max_memory and not device_map:\n    542     device_map =\
          \ {\"\": device}\n--> 544 model = accelerate.load_checkpoint_and_dispatch(\n\
          \    545     model, model_save_name, device_map, max_memory, no_split_module_classes=[cls.layer_type]\n\
          \    546 )\n    548 model_config = model.config.to_dict()\n    549 seq_len_keys\
          \ = [\"max_position_embeddings\", \"seq_length\", \"n_positions\"]\n\nFile\
          \ ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/big_modeling.py:479,\
          \ in load_checkpoint_and_dispatch(model, checkpoint, device_map, max_memory,\
          \ no_split_module_classes, offload_folder, offload_buffers, dtype, offload_state_dict,\
          \ preload_module_classes)\n    477 if offload_state_dict is None and device_map\
          \ is not None and \"disk\" in device_map.values():\n    478     offload_state_dict\
          \ = True\n--> 479 load_checkpoint_in_model(\n    480     model,\n    481\
          \     checkpoint,\n    482     device_map=device_map,\n    483     offload_folder=offload_folder,\n\
          \    484     dtype=dtype,\n    485     offload_state_dict=offload_state_dict,\n\
          \    486     offload_buffers=offload_buffers,\n    487 )\n    488 if device_map\
          \ is None:\n    489     return model\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py:993,\
          \ in load_checkpoint_in_model(model, checkpoint, device_map, offload_folder,\
          \ dtype, offload_state_dict, offload_buffers)\n    991             offload_weight(param,\
          \ param_name, state_dict_folder, index=state_dict_index)\n    992      \
          \   else:\n--> 993             set_module_tensor_to_device(model, param_name,\
          \ param_device, value=param, dtype=dtype)\n    995 # Force Python to clean\
          \ up.\n    996 del checkpoint\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py:135,\
          \ in set_module_tensor_to_device(module, tensor_name, device, value, dtype)\n\
          \    132     tensor_name = splits[-1]\n    134 if tensor_name not in module._parameters\
          \ and tensor_name not in module._buffers:\n--> 135     raise ValueError(f\"\
          {module} does not have a parameter or a buffer named {tensor_name}.\")\n\
          \    136 is_buffer = tensor_name in module._buffers\n    137 old_value =\
          \ getattr(module, tensor_name)\n\nValueError: QuantLinear() does not have\
          \ a parameter or a buffer named bias."
        updatedAt: '2023-05-16T10:23:23.822Z'
      numEdits: 0
      reactions: []
    id: 6463599ba429ec3af0a438a5
    type: comment
  author: Kurapika993
  content: "Oh sorry the it was the problem with the model path ..I  have fixed that\n\
    \n\nI am getting this error while loading the model\n\nThe safetensors archive\
    \ passed at models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\n---------------------------------------------------------------------------\n\
    ValueError                                Traceback (most recent call last)\n\
    Cell In[8], line 2\n      1 print(\"Loading model\")\n----> 2 model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
    \      3         use_safetensors=True,\n      4         model_basename=model_basename,\n\
    \      5         device=\"cuda:5\",\n      6         quantize_config=None)\n\n\
    File ~/miniconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/auto.py:63,\
    \ in AutoGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,\
    \ use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
    \     49 @classmethod\n     50 def from_quantized(\n     51     cls,\n   (...)\n\
    \     60     trust_remote_code: bool = False\n     61 ) -> BaseGPTQForCausalLM:\n\
    \     62     model_type = check_and_get_model_type(save_dir)\n---> 63     return\
    \ GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\n     64         save_dir=save_dir,\n\
    \     65         device=device,\n     66         use_safetensors=use_safetensors,\n\
    \     67         use_triton=use_triton,\n     68         max_memory=max_memory,\n\
    \     69         device_map=device_map,\n     70         quantize_config=quantize_config,\n\
    \     71         model_basename=model_basename,\n     72         trust_remote_code=trust_remote_code\n\
    \     73     )\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:544,\
    \ in BaseGPTQForCausalLM.from_quantized(cls, save_dir, device, use_safetensors,\
    \ use_triton, max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
    \    541 if not max_memory and not device_map:\n    542     device_map = {\"\"\
    : device}\n--> 544 model = accelerate.load_checkpoint_and_dispatch(\n    545 \
    \    model, model_save_name, device_map, max_memory, no_split_module_classes=[cls.layer_type]\n\
    \    546 )\n    548 model_config = model.config.to_dict()\n    549 seq_len_keys\
    \ = [\"max_position_embeddings\", \"seq_length\", \"n_positions\"]\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/big_modeling.py:479,\
    \ in load_checkpoint_and_dispatch(model, checkpoint, device_map, max_memory, no_split_module_classes,\
    \ offload_folder, offload_buffers, dtype, offload_state_dict, preload_module_classes)\n\
    \    477 if offload_state_dict is None and device_map is not None and \"disk\"\
    \ in device_map.values():\n    478     offload_state_dict = True\n--> 479 load_checkpoint_in_model(\n\
    \    480     model,\n    481     checkpoint,\n    482     device_map=device_map,\n\
    \    483     offload_folder=offload_folder,\n    484     dtype=dtype,\n    485\
    \     offload_state_dict=offload_state_dict,\n    486     offload_buffers=offload_buffers,\n\
    \    487 )\n    488 if device_map is None:\n    489     return model\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py:993,\
    \ in load_checkpoint_in_model(model, checkpoint, device_map, offload_folder, dtype,\
    \ offload_state_dict, offload_buffers)\n    991             offload_weight(param,\
    \ param_name, state_dict_folder, index=state_dict_index)\n    992         else:\n\
    --> 993             set_module_tensor_to_device(model, param_name, param_device,\
    \ value=param, dtype=dtype)\n    995 # Force Python to clean up.\n    996 del\
    \ checkpoint\n\nFile ~/miniconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py:135,\
    \ in set_module_tensor_to_device(module, tensor_name, device, value, dtype)\n\
    \    132     tensor_name = splits[-1]\n    134 if tensor_name not in module._parameters\
    \ and tensor_name not in module._buffers:\n--> 135     raise ValueError(f\"{module}\
    \ does not have a parameter or a buffer named {tensor_name}.\")\n    136 is_buffer\
    \ = tensor_name in module._buffers\n    137 old_value = getattr(module, tensor_name)\n\
    \nValueError: QuantLinear() does not have a parameter or a buffer named bias."
  created_at: 2023-05-16 09:23:23+00:00
  edited: false
  hidden: false
  id: 6463599ba429ec3af0a438a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-16T10:25:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Are you changing the code I sent you?  You need to pass <code>strict=False</code></p>

          '
        raw: Are you changing the code I sent you?  You need to pass `strict=False`
        updatedAt: '2023-05-16T10:25:45.017Z'
      numEdits: 0
      reactions: []
    id: 64635a29a429ec3af0a43d29
    type: comment
  author: TheBloke
  content: Are you changing the code I sent you?  You need to pass `strict=False`
  created_at: 2023-05-16 09:25:45+00:00
  edited: false
  hidden: false
  id: 64635a29a429ec3af0a43d29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
      fullname: Mayukh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kurapika993
      type: user
    createdAt: '2023-05-16T10:30:12.000Z'
    data:
      edited: false
      editors:
      - Kurapika993
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
          fullname: Mayukh
          isHf: false
          isPro: false
          name: Kurapika993
          type: user
        html: '<p>Ya thats because I am getting unexpected keyword argument error  like
          this on running the following</p>

          <p>print("Loading model")<br>model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,<br>        use_safetensors=True,<br>        strict=False,<br>        model_basename=model_basename,<br>        device="cuda:5",<br>        use_triton=use_triton,<br>        quantize_config=None)</p>

          <h2 id="loading-model">Loading model</h2>

          <p>TypeError                                 Traceback (most recent call
          last)<br>Cell In[15], line 2<br>      1 print("Loading model")<br>----&gt;
          2 model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,<br>      3         use_safetensors=True,<br>      4         strict=False,<br>      5         model_basename=model_basename,<br>      6         device="cuda:5",<br>      7         use_triton=use_triton,<br>      8         quantize_config=None)</p>

          <p>TypeError: AutoGPTQForCausalLM.from_quantized() got an unexpected keyword
          argument ''strict''</p>

          '
        raw: "Ya thats because I am getting unexpected keyword argument error  like\
          \ this on running the following\n\nprint(\"Loading model\")\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=True,\n        strict=False,\n        model_basename=model_basename,\n\
          \        device=\"cuda:5\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          \n\n\n\nLoading model\n---------------------------------------------------------------------------\n\
          TypeError                                 Traceback (most recent call last)\n\
          Cell In[15], line 2\n      1 print(\"Loading model\")\n----> 2 model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \      3         use_safetensors=True,\n      4         strict=False,\n\
          \      5         model_basename=model_basename,\n      6         device=\"\
          cuda:5\",\n      7         use_triton=use_triton,\n      8         quantize_config=None)\n\
          \nTypeError: AutoGPTQForCausalLM.from_quantized() got an unexpected keyword\
          \ argument 'strict'"
        updatedAt: '2023-05-16T10:30:12.655Z'
      numEdits: 0
      reactions: []
    id: 64635b3412814d754178582f
    type: comment
  author: Kurapika993
  content: "Ya thats because I am getting unexpected keyword argument error  like\
    \ this on running the following\n\nprint(\"Loading model\")\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
    \        use_safetensors=True,\n        strict=False,\n        model_basename=model_basename,\n\
    \        device=\"cuda:5\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
    \n\n\n\nLoading model\n---------------------------------------------------------------------------\n\
    TypeError                                 Traceback (most recent call last)\n\
    Cell In[15], line 2\n      1 print(\"Loading model\")\n----> 2 model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
    \      3         use_safetensors=True,\n      4         strict=False,\n      5\
    \         model_basename=model_basename,\n      6         device=\"cuda:5\",\n\
    \      7         use_triton=use_triton,\n      8         quantize_config=None)\n\
    \nTypeError: AutoGPTQForCausalLM.from_quantized() got an unexpected keyword argument\
    \ 'strict'"
  created_at: 2023-05-16 09:30:12+00:00
  edited: false
  hidden: false
  id: 64635b3412814d754178582f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-16T10:31:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK please install latest auto-gptq from source and then try again
          with the code I gave you, changing only the model path</p>

          <pre><code>git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          </code></pre>

          '
        raw: 'OK please install latest auto-gptq from source and then try again with
          the code I gave you, changing only the model path

          ```

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip install .

          ```'
        updatedAt: '2023-05-16T10:31:33.542Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ThisUserNameWillNotExist
    id: 64635b8532317fa4806775ea
    type: comment
  author: TheBloke
  content: 'OK please install latest auto-gptq from source and then try again with
    the code I gave you, changing only the model path

    ```

    git clone https://github.com/PanQiWei/AutoGPTQ

    cd AutoGPTQ

    pip install .

    ```'
  created_at: 2023-05-16 09:31:33+00:00
  edited: false
  hidden: false
  id: 64635b8532317fa4806775ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
      fullname: Mayukh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kurapika993
      type: user
    createdAt: '2023-05-16T16:59:56.000Z'
    data:
      edited: false
      editors:
      - Kurapika993
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
          fullname: Mayukh
          isHf: false
          isPro: false
          name: Kurapika993
          type: user
        html: '<p>thanks</p>

          '
        raw: thanks
        updatedAt: '2023-05-16T16:59:56.815Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6463b68c6dad99445deabc09
    id: 6463b68c6dad99445deabc08
    type: comment
  author: Kurapika993
  content: thanks
  created_at: 2023-05-16 15:59:56+00:00
  edited: false
  hidden: false
  id: 6463b68c6dad99445deabc08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
      fullname: Mayukh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kurapika993
      type: user
    createdAt: '2023-05-16T16:59:56.000Z'
    data:
      status: closed
    id: 6463b68c6dad99445deabc09
    type: status-change
  author: Kurapika993
  created_at: 2023-05-16 15:59:56+00:00
  id: 6463b68c6dad99445deabc09
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ
repo_type: model
status: closed
target_branch: null
title: File not found in AutoGptq
