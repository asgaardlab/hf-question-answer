!!python/object:huggingface_hub.community.DiscussionWithDetails
author: phooney
conflicting_files: null
created_at: 2023-05-09 18:53:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aad204488134c26ea0b26d70e02f573d.svg
      fullname: Riki M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phooney
      type: user
    createdAt: '2023-05-09T19:53:19.000Z'
    data:
      edited: false
      editors:
      - phooney
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aad204488134c26ea0b26d70e02f573d.svg
          fullname: Riki M
          isHf: false
          isPro: false
          name: phooney
          type: user
        html: '<p>Hi there,</p>

          <p>I wouldn''t be surprised if this is just me having missed some setting
          I was supposed to put in, but I was wondering if somebody could help me.  I
          can load the OpenAssistant 30B 1028g version and it responds to a short
          prompts, but longer prompts give the error below.  Eventually, after a few
          short prompts, all requests result in this error, so it appears to be ''running
          out of room'' in some way.</p>

          <p>Traceback (most recent call last):<br>  File "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\text-generation-webui\modules\callbacks.py",
          line 73, in gentask<br>    ret = self.mfunc(callback=_callback, **self.kwargs)<br>  File
          "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\text-generation-webui\modules\text_generation.py",
          line 251, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 1485, in generate<br>    return self.sample(<br>  File "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 2524, in sample<br>    outputs = self(<br>  File "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 687, in forward<br>    outputs = self.model(<br>  File "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 577, in forward<br>    layer_outputs = decoder_layer(<br>  File "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 292, in forward<br>    hidden_states, self_attn_weights, present_key_value
          = self.self_attn(<br>  File "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 196, in forward<br>    query_states = self.q_proj(hidden_states).view(bsz,
          q_len, self.num_heads, self.head_dim).transpose(1, 2)<br>  File "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:*******\Oobabooga\oobabooga_windows\oobabooga_windows\text-generation-webui\repositories\GPTQ-for-LLaMa\quant.py",
          line 362, in forward<br>    weight = weight.reshape(-1, self.groupsize,
          weight.shape[2])<br>RuntimeError: shape ''[-1, 1024, 6656]'' is invalid
          for input of size 44302336<br>Output generated in 0.25 seconds (0.00 tokens/s,
          0 tokens, context 157, seed 162660829)</p>

          '
        raw: "Hi there,\r\n\r\nI wouldn't be surprised if this is just me having missed\
          \ some setting I was supposed to put in, but I was wondering if somebody\
          \ could help me.  I can load the OpenAssistant 30B 1028g version and it\
          \ responds to a short prompts, but longer prompts give the error below.\
          \  Eventually, after a few short prompts, all requests result in this error,\
          \ so it appears to be 'running out of room' in some way.\r\n\r\nTraceback\
          \ (most recent call last):\r\n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\\
          oobabooga_windows\\text-generation-webui\\modules\\callbacks.py\", line\
          \ 73, in gentask\r\n    ret = self.mfunc(callback=_callback, **self.kwargs)\r\
          \n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\text_generation.py\", line 251, in generate_with_callback\r\
          \n    shared.model.generate(**kwargs)\r\n  File \"C:\\*******\\Oobabooga\\\
          oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return\
          \ func(*args, **kwargs)\r\n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          generation\\utils.py\", line 1485, in generate\r\n    return self.sample(\r\
          \n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py\"\
          , line 2524, in sample\r\n    outputs = self(\r\n  File \"C:\\*******\\\
          Oobabooga\\oobabooga_windows\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\
          \n    return forward_call(*args, **kwargs)\r\n  File \"C:\\*******\\Oobabooga\\\
          oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 687, in forward\r\
          \n    outputs = self.model(\r\n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 577, in forward\r\n    layer_outputs = decoder_layer(\r\
          \n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 292, in forward\r\n    hidden_states, self_attn_weights,\
          \ present_key_value = self.self_attn(\r\n  File \"C:\\*******\\Oobabooga\\\
          oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return\
          \ forward_call(*args, **kwargs)\r\n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\", line 196, in forward\r\n    query_states\
          \ = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1,\
          \ 2)\r\n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\", line 362,\
          \ in forward\r\n    weight = weight.reshape(-1, self.groupsize, weight.shape[2])\r\
          \nRuntimeError: shape '[-1, 1024, 6656]' is invalid for input of size 44302336\r\
          \nOutput generated in 0.25 seconds (0.00 tokens/s, 0 tokens, context 157,\
          \ seed 162660829)"
        updatedAt: '2023-05-09T19:53:20.002Z'
      numEdits: 0
      reactions: []
    id: 645aa4af333fb183578581f4
    type: comment
  author: phooney
  content: "Hi there,\r\n\r\nI wouldn't be surprised if this is just me having missed\
    \ some setting I was supposed to put in, but I was wondering if somebody could\
    \ help me.  I can load the OpenAssistant 30B 1028g version and it responds to\
    \ a short prompts, but longer prompts give the error below.  Eventually, after\
    \ a few short prompts, all requests result in this error, so it appears to be\
    \ 'running out of room' in some way.\r\n\r\nTraceback (most recent call last):\r\
    \n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
    modules\\callbacks.py\", line 73, in gentask\r\n    ret = self.mfunc(callback=_callback,\
    \ **self.kwargs)\r\n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\\
    text-generation-webui\\modules\\text_generation.py\", line 251, in generate_with_callback\r\
    \n    shared.model.generate(**kwargs)\r\n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\\
    oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\utils\\_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1485, in generate\r\
    \n    return self.sample(\r\n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\\
    oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\generation\\\
    utils.py\", line 2524, in sample\r\n    outputs = self(\r\n  File \"C:\\*******\\\
    Oobabooga\\oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
    , line 687, in forward\r\n    outputs = self.model(\r\n  File \"C:\\*******\\\
    Oobabooga\\oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
    , line 577, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"C:\\\
    *******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"C:\\*******\\Oobabooga\\\
    oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 292, in forward\r\n   \
    \ hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File\
    \ \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"C:\\*******\\Oobabooga\\\
    oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 196, in forward\r\n   \
    \ query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1,\
    \ 2)\r\n  File \"C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"\
    C:\\*******\\Oobabooga\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
    repositories\\GPTQ-for-LLaMa\\quant.py\", line 362, in forward\r\n    weight =\
    \ weight.reshape(-1, self.groupsize, weight.shape[2])\r\nRuntimeError: shape '[-1,\
    \ 1024, 6656]' is invalid for input of size 44302336\r\nOutput generated in 0.25\
    \ seconds (0.00 tokens/s, 0 tokens, context 157, seed 162660829)"
  created_at: 2023-05-09 18:53:19+00:00
  edited: false
  hidden: false
  id: 645aa4af333fb183578581f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7076a9a25757cbcaa0653128ffc3084f.svg
      fullname: Ulymp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ulymp
      type: user
    createdAt: '2023-05-10T23:15:53.000Z'
    data:
      edited: false
      editors:
      - ulymp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7076a9a25757cbcaa0653128ffc3084f.svg
          fullname: Ulymp
          isHf: false
          isPro: false
          name: ulymp
          type: user
        html: '<p>Same error here with the exact same message (on Linux, that is).
          Happens after a few short prompts. Also have the 1024 version from the main
          branch.</p>

          '
        raw: Same error here with the exact same message (on Linux, that is). Happens
          after a few short prompts. Also have the 1024 version from the main branch.
        updatedAt: '2023-05-10T23:15:53.571Z'
      numEdits: 0
      reactions: []
    id: 645c25a9129cd39906e77559
    type: comment
  author: ulymp
  content: Same error here with the exact same message (on Linux, that is). Happens
    after a few short prompts. Also have the 1024 version from the main branch.
  created_at: 2023-05-10 22:15:53+00:00
  edited: false
  hidden: false
  id: 645c25a9129cd39906e77559
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-12T17:01:56.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Could you both check you''re running the latest version of text-generation-webui,
          and then re-save your GPTQ parameters for the model. There was a bug with
          saving GPTQ parameters in textgen, which was recently fixed.</p>

          <p>Are either of you using pre_layer/CPU offload, or multiple GPUs, or any
          special config like that?</p>

          <p>If you''re still getting the problem after updating, please let me know
          and I''ll test it myself.</p>

          '
        raw: 'Could you both check you''re running the latest version of text-generation-webui,
          and then re-save your GPTQ parameters for the model. There was a bug with
          saving GPTQ parameters in textgen, which was recently fixed.


          Are either of you using pre_layer/CPU offload, or multiple GPUs, or any
          special config like that?


          If you''re still getting the problem after updating, please let me know
          and I''ll test it myself.'
        updatedAt: '2023-05-12T17:01:56.392Z'
      numEdits: 0
      reactions: []
    id: 645e71046320b0efe409f90c
    type: comment
  author: TheBloke
  content: 'Could you both check you''re running the latest version of text-generation-webui,
    and then re-save your GPTQ parameters for the model. There was a bug with saving
    GPTQ parameters in textgen, which was recently fixed.


    Are either of you using pre_layer/CPU offload, or multiple GPUs, or any special
    config like that?


    If you''re still getting the problem after updating, please let me know and I''ll
    test it myself.'
  created_at: 2023-05-12 16:01:56+00:00
  edited: false
  hidden: false
  id: 645e71046320b0efe409f90c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aad204488134c26ea0b26d70e02f573d.svg
      fullname: Riki M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phooney
      type: user
    createdAt: '2023-05-13T04:31:08.000Z'
    data:
      edited: false
      editors:
      - phooney
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aad204488134c26ea0b26d70e02f573d.svg
          fullname: Riki M
          isHf: false
          isPro: false
          name: phooney
          type: user
        html: '<p>Hi there,</p>

          <p>I updated and got this error:</p>

          <p>ERROR: pip''s dependency resolver does not currently take into account
          all the packages that are installed. This behaviour is the source of the
          following dependency conflicts.<br>numba 0.56.4 requires numpy&lt;1.24,&gt;=1.18,
          but you have numpy 1.24.3 which is incompatible.</p>

          <p>Not sure how important that is, as the updater kept on going  and doing
          its thing after that, but thought I''d put it in in case it was relevant.</p>

          <p>Anyway, after that I am still getting the same error/behaviour as before.  It
          does appear to have some trouble saving the parameters.  I can save them
          (4 wbits, 1024 groupsize, llama) and reload the model no problem.  The settings
          appear to persist when I load a different model and then load OpenAssistant
          again.</p>

          <p>I''m using 1x RTX4090 and I have pre_layer at 0.  Nothing ticked out
          of auto-devices, disk, cpu, bf16 or load-in-8bit.  gpu-memory in MiB for
          device: 0.  cpu-memory in MiB 0.</p>

          '
        raw: 'Hi there,


          I updated and got this error:


          ERROR: pip''s dependency resolver does not currently take into account all
          the packages that are installed. This behaviour is the source of the following
          dependency conflicts.

          numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which
          is incompatible.


          Not sure how important that is, as the updater kept on going  and doing
          its thing after that, but thought I''d put it in in case it was relevant.


          Anyway, after that I am still getting the same error/behaviour as before.  It
          does appear to have some trouble saving the parameters.  I can save them
          (4 wbits, 1024 groupsize, llama) and reload the model no problem.  The settings
          appear to persist when I load a different model and then load OpenAssistant
          again.


          I''m using 1x RTX4090 and I have pre_layer at 0.  Nothing ticked out of
          auto-devices, disk, cpu, bf16 or load-in-8bit.  gpu-memory in MiB for device:
          0.  cpu-memory in MiB 0.'
        updatedAt: '2023-05-13T04:31:08.217Z'
      numEdits: 0
      reactions: []
    id: 645f128c6320b0efe40e4ad0
    type: comment
  author: phooney
  content: 'Hi there,


    I updated and got this error:


    ERROR: pip''s dependency resolver does not currently take into account all the
    packages that are installed. This behaviour is the source of the following dependency
    conflicts.

    numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.


    Not sure how important that is, as the updater kept on going  and doing its thing
    after that, but thought I''d put it in in case it was relevant.


    Anyway, after that I am still getting the same error/behaviour as before.  It
    does appear to have some trouble saving the parameters.  I can save them (4 wbits,
    1024 groupsize, llama) and reload the model no problem.  The settings appear to
    persist when I load a different model and then load OpenAssistant again.


    I''m using 1x RTX4090 and I have pre_layer at 0.  Nothing ticked out of auto-devices,
    disk, cpu, bf16 or load-in-8bit.  gpu-memory in MiB for device: 0.  cpu-memory
    in MiB 0.'
  created_at: 2023-05-13 03:31:08+00:00
  edited: false
  hidden: false
  id: 645f128c6320b0efe40e4ad0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T11:12:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK, I will test this again shortly and let you know</p>

          <p>What OS are you running?</p>

          '
        raw: 'OK, I will test this again shortly and let you know


          What OS are you running?'
        updatedAt: '2023-05-13T11:12:51.710Z'
      numEdits: 0
      reactions: []
    id: 645f70b3446a4fa46955fa6c
    type: comment
  author: TheBloke
  content: 'OK, I will test this again shortly and let you know


    What OS are you running?'
  created_at: 2023-05-13 10:12:51+00:00
  edited: false
  hidden: false
  id: 645f70b3446a4fa46955fa6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aad204488134c26ea0b26d70e02f573d.svg
      fullname: Riki M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phooney
      type: user
    createdAt: '2023-05-14T00:54:41.000Z'
    data:
      edited: false
      editors:
      - phooney
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aad204488134c26ea0b26d70e02f573d.svg
          fullname: Riki M
          isHf: false
          isPro: false
          name: phooney
          type: user
        html: '<p>Win 11 Pro for me.  Thank you for looking into it, much appreciated.</p>

          '
        raw: Win 11 Pro for me.  Thank you for looking into it, much appreciated.
        updatedAt: '2023-05-14T00:54:41.666Z'
      numEdits: 0
      reactions: []
    id: 6460315172397238b2323a04
    type: comment
  author: phooney
  content: Win 11 Pro for me.  Thank you for looking into it, much appreciated.
  created_at: 2023-05-13 23:54:41+00:00
  edited: false
  hidden: false
  id: 6460315172397238b2323a04
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-14T05:16:06.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<blockquote>

          <p>Same error here with the exact same message (on Linux, that is). Happens
          after a few short prompts. Also have the 1024 version from the main branch.</p>

          </blockquote>

          <p>Same here, the 1024 seems not supported by ooba? even with -groupsize
          1024 as input arg</p>

          '
        raw: '> Same error here with the exact same message (on Linux, that is). Happens
          after a few short prompts. Also have the 1024 version from the main branch.


          Same here, the 1024 seems not supported by ooba? even with -groupsize 1024
          as input arg'
        updatedAt: '2023-05-14T05:16:06.149Z'
      numEdits: 0
      reactions: []
    id: 64606e96a93c1779eb124667
    type: comment
  author: Yhyu13
  content: '> Same error here with the exact same message (on Linux, that is). Happens
    after a few short prompts. Also have the 1024 version from the main branch.


    Same here, the 1024 seems not supported by ooba? even with -groupsize 1024 as
    input arg'
  created_at: 2023-05-14 04:16:06+00:00
  edited: false
  hidden: false
  id: 64606e96a93c1779eb124667
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-14T07:49:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK I''m going to do a no groupsize model instead. That''s what I''m
          doing for new 30B models anyway, as otherwise it will OOM on long replies
          on 24GB cards</p>

          '
        raw: OK I'm going to do a no groupsize model instead. That's what I'm doing
          for new 30B models anyway, as otherwise it will OOM on long replies on 24GB
          cards
        updatedAt: '2023-05-14T07:49:52.981Z'
      numEdits: 0
      reactions: []
    id: 646092a0232a700f2657d99e
    type: comment
  author: TheBloke
  content: OK I'm going to do a no groupsize model instead. That's what I'm doing
    for new 30B models anyway, as otherwise it will OOM on long replies on 24GB cards
  created_at: 2023-05-14 06:49:52+00:00
  edited: false
  hidden: false
  id: 646092a0232a700f2657d99e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
      fullname: poisenbery
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: poisenbery
      type: user
    createdAt: '2023-05-14T11:56:09.000Z'
    data:
      edited: true
      editors:
      - poisenbery
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
          fullname: poisenbery
          isHf: false
          isPro: false
          name: poisenbery
          type: user
        html: '<p>I found out that this has something to do with the --chat argument
          with characters or something.<br>I installed linux and got everything working.</p>

          <p>As soon as I tried to run with --chat and the example character, I started
          getting this error again.</p>

          <p>Does this mean that we CANNOT use any type of character with this model?</p>

          '
        raw: 'I found out that this has something to do with the --chat argument with
          characters or something.

          I installed linux and got everything working.


          As soon as I tried to run with --chat and the example character, I started
          getting this error again.


          Does this mean that we CANNOT use any type of character with this model?'
        updatedAt: '2023-05-14T12:01:05.986Z'
      numEdits: 2
      reactions: []
    id: 6460cc59604bf892333289a6
    type: comment
  author: poisenbery
  content: 'I found out that this has something to do with the --chat argument with
    characters or something.

    I installed linux and got everything working.


    As soon as I tried to run with --chat and the example character, I started getting
    this error again.


    Does this mean that we CANNOT use any type of character with this model?'
  created_at: 2023-05-14 10:56:09+00:00
  edited: true
  hidden: false
  id: 6460cc59604bf892333289a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-14T12:01:54.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sounds like the --chat issue might be a text-gen-ui bug</p>

          <p>But I''m doing another model now, made with no groupsize. That will be
          better anyway, as it needs less VRAM.  It''ll be uploaded in 2-3 hours.</p>

          '
        raw: 'Sounds like the --chat issue might be a text-gen-ui bug


          But I''m doing another model now, made with no groupsize. That will be better
          anyway, as it needs less VRAM.  It''ll be uploaded in 2-3 hours.'
        updatedAt: '2023-05-14T12:01:54.076Z'
      numEdits: 0
      reactions: []
    id: 6460cdb2567598449e04ab67
    type: comment
  author: TheBloke
  content: 'Sounds like the --chat issue might be a text-gen-ui bug


    But I''m doing another model now, made with no groupsize. That will be better
    anyway, as it needs less VRAM.  It''ll be uploaded in 2-3 hours.'
  created_at: 2023-05-14 11:01:54+00:00
  edited: false
  hidden: false
  id: 6460cdb2567598449e04ab67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
      fullname: poisenbery
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: poisenbery
      type: user
    createdAt: '2023-05-14T12:09:35.000Z'
    data:
      edited: false
      editors:
      - poisenbery
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
          fullname: poisenbery
          isHf: false
          isPro: false
          name: poisenbery
          type: user
        html: '<p>Probably. The model works fine, but as soon as context is entered
          for a character, it gives the error.<br>I''ll submit a bug report on oobabooga
          since I finally know what is causing the issue now.</p>

          '
        raw: 'Probably. The model works fine, but as soon as context is entered for
          a character, it gives the error.

          I''ll submit a bug report on oobabooga since I finally know what is causing
          the issue now.'
        updatedAt: '2023-05-14T12:09:35.748Z'
      numEdits: 0
      reactions: []
    id: 6460cf7f7735f76a4a4edb4b
    type: comment
  author: poisenbery
  content: 'Probably. The model works fine, but as soon as context is entered for
    a character, it gives the error.

    I''ll submit a bug report on oobabooga since I finally know what is causing the
    issue now.'
  created_at: 2023-05-14 11:09:35+00:00
  edited: false
  hidden: false
  id: 6460cf7f7735f76a4a4edb4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-14T19:40:55.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>I just tested 1024g with my own 30b model. I converted it with AutoGPTQ.
          It also has this error since it loaded with old cuda. The only GPTQ solution
          that was able to run it was the 4bit lora autograd one. I had no problem
          there with the crash on FWD.. I ran out of memory instead. I thought this
          was supposed to help by using <em>less</em> vram.. instead it uses more.
          </p>

          <p>I think 1024g group size can be laid to rest.</p>

          '
        raw: "I just tested 1024g with my own 30b model. I converted it with AutoGPTQ.\
          \ It also has this error since it loaded with old cuda. The only GPTQ solution\
          \ that was able to run it was the 4bit lora autograd one. I had no problem\
          \ there with the crash on FWD.. I ran out of memory instead. I thought this\
          \ was supposed to help by using *less* vram.. instead it uses more. \n\n\
          I think 1024g group size can be laid to rest."
        updatedAt: '2023-05-14T19:40:55.048Z'
      numEdits: 0
      reactions: []
    id: 64613947604bf892333572f0
    type: comment
  author: autobots
  content: "I just tested 1024g with my own 30b model. I converted it with AutoGPTQ.\
    \ It also has this error since it loaded with old cuda. The only GPTQ solution\
    \ that was able to run it was the 4bit lora autograd one. I had no problem there\
    \ with the crash on FWD.. I ran out of memory instead. I thought this was supposed\
    \ to help by using *less* vram.. instead it uses more. \n\nI think 1024g group\
    \ size can be laid to rest."
  created_at: 2023-05-14 18:40:55+00:00
  edited: false
  hidden: false
  id: 64613947604bf892333572f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-14T19:43:12.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It uses less VRAM than 128g, but more than no groupsize at all</p>

          <p>The no groupsize model is taking absolutely forever to pack, but it''s
          nearly done. Be uploaded fairly soon.</p>

          '
        raw: 'It uses less VRAM than 128g, but more than no groupsize at all


          The no groupsize model is taking absolutely forever to pack, but it''s nearly
          done. Be uploaded fairly soon.'
        updatedAt: '2023-05-14T19:43:12.417Z'
      numEdits: 0
      reactions: []
    id: 646139d0604bf8923335764b
    type: comment
  author: TheBloke
  content: 'It uses less VRAM than 128g, but more than no groupsize at all


    The no groupsize model is taking absolutely forever to pack, but it''s nearly
    done. Be uploaded fairly soon.'
  created_at: 2023-05-14 18:43:12+00:00
  edited: false
  hidden: false
  id: 646139d0604bf8923335764b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-14T19:50:44.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>So no groups is still 100% best memory usage? Good to know. Need
          a shootout with 128g and act order by itself and see who wins perplexity
          and by how much.</p>

          '
        raw: So no groups is still 100% best memory usage? Good to know. Need a shootout
          with 128g and act order by itself and see who wins perplexity and by how
          much.
        updatedAt: '2023-05-14T19:50:44.117Z'
      numEdits: 0
      reactions: []
    id: 64613b94b2ae2983b107e2fc
    type: comment
  author: autobots
  content: So no groups is still 100% best memory usage? Good to know. Need a shootout
    with 128g and act order by itself and see who wins perplexity and by how much.
  created_at: 2023-05-14 18:50:44+00:00
  edited: false
  hidden: false
  id: 64613b94b2ae2983b107e2fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-14T21:48:33.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Model with <code>group_size = None</code> and <code>--act-order</code>
          is up in main.  Those of you having trouble with the 1024g file, please
          test this and report back.  </p>

          <p>The previous 1024g-compat file is moved to its own new branch.</p>

          '
        raw: "Model with `group_size = None` and `--act-order` is up in main.  Those\
          \ of you having trouble with the 1024g file, please test this and report\
          \ back.  \n\nThe previous 1024g-compat file is moved to its own new branch."
        updatedAt: '2023-05-15T07:30:21.468Z'
      numEdits: 2
      reactions: []
    id: 64615731567598449e08523e
    type: comment
  author: TheBloke
  content: "Model with `group_size = None` and `--act-order` is up in main.  Those\
    \ of you having trouble with the 1024g file, please test this and report back.\
    \  \n\nThe previous 1024g-compat file is moved to its own new branch."
  created_at: 2023-05-14 20:48:33+00:00
  edited: true
  hidden: false
  id: 64615731567598449e08523e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aad204488134c26ea0b26d70e02f573d.svg
      fullname: Riki M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phooney
      type: user
    createdAt: '2023-05-15T01:04:57.000Z'
    data:
      edited: false
      editors:
      - phooney
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aad204488134c26ea0b26d70e02f573d.svg
          fullname: Riki M
          isHf: false
          isPro: false
          name: phooney
          type: user
        html: '<p>New version works for me, thank you!</p>

          '
        raw: New version works for me, thank you!
        updatedAt: '2023-05-15T01:04:57.353Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 64618539604bf8923337a93e
    type: comment
  author: phooney
  content: New version works for me, thank you!
  created_at: 2023-05-15 00:04:57+00:00
  edited: false
  hidden: false
  id: 64618539604bf8923337a93e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: shape ''[-1, 1024, 6656]'' is invalid for input of size 44302336'
