!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GuruVirus
conflicting_files: null
created_at: 2023-05-06 21:08:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8517be9ecdcdbec2ff6805a4597c771.svg
      fullname: GuruVirus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GuruVirus
      type: user
    createdAt: '2023-05-06T22:08:30.000Z'
    data:
      edited: true
      editors:
      - GuruVirus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8517be9ecdcdbec2ff6805a4597c771.svg
          fullname: GuruVirus
          isHf: false
          isPro: false
          name: GuruVirus
          type: user
        html: '<p>Using Win11 w/ WLS Ubuntu 22.0.4.2<br>Model is in \home\username\text-generation-webui\models<br><a
          rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa.git">https://github.com/qwopqwop200/GPTQ-for-LLaMa.git</a>
          cloned into text-generation-webui/repositories/GPTQ-for-LLaMa/</p>

          <p>When launching with:<br>python server.py --model OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors<br>I
          get:<br>OSError: It looks like the config file at ''models/OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors''
          is not a valid JSON file.</p>

          <p>What am I doing wrong with this model?</p>

          <p>Edit: also tried loading the WebUI without the model parameter, got in,
          selected the model from the dropdown list and got the same traceback output.</p>

          <p>Edit2: also tried the steps in the Readme:<br>"If you see an error in
          the bottom right, ignore it - it''s temporary.<br>Fill out the GPTQ parameters
          on the right: Bits = 4, Groupsize = 1024, model_type = Llama<br>Click Save
          settings for this model in the top right.<br>Click Reload the Model in the
          top right."<br>Same error on load.</p>

          <p>Edit3: noticed the readme says:<br>python server.py --model OpenAssistant-SFT-7-Llama-30B-GPTQ<br>But
          this model card doesn''t have a file named OpenAssistant-SFT-7-Llama-30B-GPTQ</p>

          <p>Edit4: confirmed from another user that it requires a folder under /models.
          Created OpenAssistant-SFT-7-Llama-30B-GPTQ with all the files from this
          model card.<br>Made a little progress. New error:</p>

          <p>INFO:Gradio HTTP request redirected to localhost :)<br>bin /home/username/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so<br>INFO:Loading
          OpenAssistant-SFT-7-Llama-30B-GPTQ...<br>INFO:Found the following quantized
          model: models/OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors<br>Traceback
          (most recent call last):<br>  File "/home/username/text-generation-webui/server.py",
          line 872, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "/home/username/text-generation-webui/modules/models.py", line 159, in load_model<br>    model
          = load_quantized(model_name)<br>  File "/home/username/text-generation-webui/modules/GPTQ_loader.py",
          line 179, in load_quantized<br>    model = load_quant(str(path_to_model),
          str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)<br>  File
          "/home/username/text-generation-webui/modules/GPTQ_loader.py", line 77,
          in _load_quant<br>    model.load_state_dict(safe_load(checkpoint), strict=False)<br>  File
          "/home/username/miniconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 2041, in load_state_dict<br>    raise RuntimeError(''Error(s) in loading
          state_dict for {}:\n\t{}''.format(<br>RuntimeError: Error(s) in loading
          state_dict for LlamaForCausalLM:<br>        size mismatch for model.layers.0.self_attn.k_proj.qzeros:
          copying a param with shape torch.Size([7, 832]) from checkpoint, the shape
          in current model is torch.Size([52, 832]).<br>        size mismatch for
          model.layers.0.self_attn.k_proj.scales: copying a param with shape torch.Size([7,
          6656]) from checkpoint, the shape in current model is torch.Size([52, 6656]).<br>        size
          mismatch for model.layers.0.self_attn.o_proj.qzeros: copying a param with
          shape torch.Size([7, 832]) from checkpoint, the shape in current model is
          torch.Size([52, 832]).</p>

          <p>It then shows hundreds of size mismatch error lines.</p>

          '
        raw: "Using Win11 w/ WLS Ubuntu 22.0.4.2\nModel is in \\home\\username\\text-generation-webui\\\
          models\\\nhttps://github.com/qwopqwop200/GPTQ-for-LLaMa.git cloned into\
          \ text-generation-webui/repositories/GPTQ-for-LLaMa/\n\nWhen launching with:\n\
          python server.py --model OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors\n\
          I get:\nOSError: It looks like the config file at 'models/OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors'\
          \ is not a valid JSON file.\n\nWhat am I doing wrong with this model?\n\n\
          Edit: also tried loading the WebUI without the model parameter, got in,\
          \ selected the model from the dropdown list and got the same traceback output.\n\
          \nEdit2: also tried the steps in the Readme:\n\"If you see an error in the\
          \ bottom right, ignore it - it's temporary.\nFill out the GPTQ parameters\
          \ on the right: Bits = 4, Groupsize = 1024, model_type = Llama\nClick Save\
          \ settings for this model in the top right.\nClick Reload the Model in the\
          \ top right.\"\nSame error on load.\n\nEdit3: noticed the readme says:\n\
          python server.py --model OpenAssistant-SFT-7-Llama-30B-GPTQ\nBut this model\
          \ card doesn't have a file named OpenAssistant-SFT-7-Llama-30B-GPTQ\n\n\
          Edit4: confirmed from another user that it requires a folder under /models.\
          \ Created OpenAssistant-SFT-7-Llama-30B-GPTQ with all the files from this\
          \ model card.\nMade a little progress. New error:\n\nINFO:Gradio HTTP request\
          \ redirected to localhost :)\nbin /home/username/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n\
          INFO:Loading OpenAssistant-SFT-7-Llama-30B-GPTQ...\nINFO:Found the following\
          \ quantized model: models/OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors\n\
          Traceback (most recent call last):\n  File \"/home/username/text-generation-webui/server.py\"\
          , line 872, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"/home/username/text-generation-webui/modules/models.py\", line\
          \ 159, in load_model\n    model = load_quantized(model_name)\n  File \"\
          /home/username/text-generation-webui/modules/GPTQ_loader.py\", line 179,\
          \ in load_quantized\n    model = load_quant(str(path_to_model), str(pt_path),\
          \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          \  File \"/home/username/text-generation-webui/modules/GPTQ_loader.py\"\
          , line 77, in _load_quant\n    model.load_state_dict(safe_load(checkpoint),\
          \ strict=False)\n  File \"/home/username/miniconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM:\n        size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([7, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n        size mismatch for\
          \ model.layers.0.self_attn.k_proj.scales: copying a param with shape torch.Size([7,\
          \ 6656]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 6656]).\n        size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([7, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\nIt then shows hundreds of\
          \ size mismatch error lines."
        updatedAt: '2023-05-06T23:01:57.497Z'
      numEdits: 8
      reactions: []
    id: 6456cfdebfdf9c63ce320234
    type: comment
  author: GuruVirus
  content: "Using Win11 w/ WLS Ubuntu 22.0.4.2\nModel is in \\home\\username\\text-generation-webui\\\
    models\\\nhttps://github.com/qwopqwop200/GPTQ-for-LLaMa.git cloned into text-generation-webui/repositories/GPTQ-for-LLaMa/\n\
    \nWhen launching with:\npython server.py --model OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors\n\
    I get:\nOSError: It looks like the config file at 'models/OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors'\
    \ is not a valid JSON file.\n\nWhat am I doing wrong with this model?\n\nEdit:\
    \ also tried loading the WebUI without the model parameter, got in, selected the\
    \ model from the dropdown list and got the same traceback output.\n\nEdit2: also\
    \ tried the steps in the Readme:\n\"If you see an error in the bottom right, ignore\
    \ it - it's temporary.\nFill out the GPTQ parameters on the right: Bits = 4, Groupsize\
    \ = 1024, model_type = Llama\nClick Save settings for this model in the top right.\n\
    Click Reload the Model in the top right.\"\nSame error on load.\n\nEdit3: noticed\
    \ the readme says:\npython server.py --model OpenAssistant-SFT-7-Llama-30B-GPTQ\n\
    But this model card doesn't have a file named OpenAssistant-SFT-7-Llama-30B-GPTQ\n\
    \nEdit4: confirmed from another user that it requires a folder under /models.\
    \ Created OpenAssistant-SFT-7-Llama-30B-GPTQ with all the files from this model\
    \ card.\nMade a little progress. New error:\n\nINFO:Gradio HTTP request redirected\
    \ to localhost :)\nbin /home/username/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n\
    INFO:Loading OpenAssistant-SFT-7-Llama-30B-GPTQ...\nINFO:Found the following quantized\
    \ model: models/OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors\n\
    Traceback (most recent call last):\n  File \"/home/username/text-generation-webui/server.py\"\
    , line 872, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"/home/username/text-generation-webui/modules/models.py\", line 159,\
    \ in load_model\n    model = load_quantized(model_name)\n  File \"/home/username/text-generation-webui/modules/GPTQ_loader.py\"\
    , line 179, in load_quantized\n    model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
    \  File \"/home/username/text-generation-webui/modules/GPTQ_loader.py\", line\
    \ 77, in _load_quant\n    model.load_state_dict(safe_load(checkpoint), strict=False)\n\
    \  File \"/home/username/miniconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading state_dict\
    \ for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\n\
    \        size mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param\
    \ with shape torch.Size([7, 832]) from checkpoint, the shape in current model\
    \ is torch.Size([52, 832]).\n        size mismatch for model.layers.0.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([7, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n        size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([7, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\nIt then shows hundreds of size mismatch\
    \ error lines."
  created_at: 2023-05-06 21:08:30+00:00
  edited: true
  hidden: false
  id: 6456cfdebfdf9c63ce320234
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-07T06:38:05.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please check the sha256sum of the model you downloaded and compare
          it against the file on HF. That sounds like a corrupted file.</p>

          <p>For future reference, the instructions for easy install are listed in
          the README. You don''t need to mess around with manual downloading, making
          folders, or any of that</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/V9PE1yjgJa2ew4wjZaGbO.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/V9PE1yjgJa2ew4wjZaGbO.png"></a></p>

          '
        raw: 'Please check the sha256sum of the model you downloaded and compare it
          against the file on HF. That sounds like a corrupted file.


          For future reference, the instructions for easy install are listed in the
          README. You don''t need to mess around with manual downloading, making folders,
          or any of that


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/V9PE1yjgJa2ew4wjZaGbO.png)'
        updatedAt: '2023-05-07T06:38:05.334Z'
      numEdits: 0
      reactions: []
    id: 6457474d711ee86f6ee95f74
    type: comment
  author: TheBloke
  content: 'Please check the sha256sum of the model you downloaded and compare it
    against the file on HF. That sounds like a corrupted file.


    For future reference, the instructions for easy install are listed in the README.
    You don''t need to mess around with manual downloading, making folders, or any
    of that


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/V9PE1yjgJa2ew4wjZaGbO.png)'
  created_at: 2023-05-07 05:38:05+00:00
  edited: false
  hidden: false
  id: 6457474d711ee86f6ee95f74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8517be9ecdcdbec2ff6805a4597c771.svg
      fullname: GuruVirus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GuruVirus
      type: user
    createdAt: '2023-05-07T15:26:08.000Z'
    data:
      edited: false
      editors:
      - GuruVirus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8517be9ecdcdbec2ff6805a4597c771.svg
          fullname: GuruVirus
          isHf: false
          isPro: false
          name: GuruVirus
          type: user
        html: '<p>I''m used to diffusion models with a single file. It was not clear
          that I needed all of the other files.<br>With all of the other files in
          a folder, it now works.</p>

          '
        raw: 'I''m used to diffusion models with a single file. It was not clear that
          I needed all of the other files.

          With all of the other files in a folder, it now works.'
        updatedAt: '2023-05-07T15:26:08.724Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6457c310cf099a9dd1501abd
    id: 6457c310cf099a9dd1501abc
    type: comment
  author: GuruVirus
  content: 'I''m used to diffusion models with a single file. It was not clear that
    I needed all of the other files.

    With all of the other files in a folder, it now works.'
  created_at: 2023-05-07 14:26:08+00:00
  edited: false
  hidden: false
  id: 6457c310cf099a9dd1501abc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d8517be9ecdcdbec2ff6805a4597c771.svg
      fullname: GuruVirus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GuruVirus
      type: user
    createdAt: '2023-05-07T15:26:08.000Z'
    data:
      status: closed
    id: 6457c310cf099a9dd1501abd
    type: status-change
  author: GuruVirus
  created_at: 2023-05-07 14:26:08+00:00
  id: 6457c310cf099a9dd1501abd
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ
repo_type: model
status: closed
target_branch: null
title: '''<model>.safetensors'' is not a valid JSON file.'
