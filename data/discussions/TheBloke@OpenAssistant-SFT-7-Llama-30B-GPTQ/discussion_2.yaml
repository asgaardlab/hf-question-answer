!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DrSmurf
conflicting_files: null
created_at: 2023-04-30 16:30:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e94bb2eb82bfccf36a4469e4ca85893.svg
      fullname: Lukas Adelbrecht
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DrSmurf
      type: user
    createdAt: '2023-04-30T17:30:55.000Z'
    data:
      edited: false
      editors:
      - DrSmurf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e94bb2eb82bfccf36a4469e4ca85893.svg
          fullname: Lukas Adelbrecht
          isHf: false
          isPro: false
          name: DrSmurf
          type: user
        html: "<p>I get this error message when trying to load the model with oobabooga:</p>\n\
          <p>Traceback (most recent call last):<br>File \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 442, in load_state_dict<br>return torch.load(checkpoint_file, map_location=\u201C\
          cpu\u201D)<br>File \u201C/home/lukas/.local/lib/python3.10/site-packages/torch/serialization.py\u201D\
          , line 791, in load<br>with _open_file_like(f, \u2018rb\u2019) as opened_file:<br>File\
          \ \u201C/home/lukas/.local/lib/python3.10/site-packages/torch/serialization.py\u201D\
          , line 271, in _open_file_like<br>return _open_file(name_or_buffer, mode)<br>File\
          \ \u201C/home/lukas/.local/lib/python3.10/site-packages/torch/serialization.py\u201D\
          , line 252, in init<br>super().init(open(name, mode))<br>FileNotFoundError:\
          \ [Errno 2] No such file or directory: \u2018models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/pytorch_model-00001-of-00007.bin\u2019\
          </p>\n<p>During handling of the above exception, another exception occurred:</p>\n\
          <p>Traceback (most recent call last):<br>File \u201C/home/lukas/text-generation-webui/server.py\u201D\
          , line 102, in load_model_wrapper<br>shared.model, shared.tokenizer = load_model(shared.model_name)<br>File\
          \ \u201C/home/lukas/text-generation-webui/modules/models.py\u201D, line\
          \ 217, in load_model<br>model = LoaderClass.from_pretrained(checkpoint,\
          \ **params)<br>File \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u201D\
          , line 471, in from_pretrained<br>return model_class.from_pretrained(<br>File\
          \ \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 2795, in from_pretrained<br>) = cls._load_pretrained_model(<br>File\
          \ \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 3109, in _load_pretrained_model<br>state_dict = load_state_dict(shard_file)<br>File\
          \ \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 445, in load_state_dict<br>with open(checkpoint_file) as f:<br>FileNotFoundError:\
          \ [Errno 2] No such file or directory: \u2018models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/pytorch_model-00001-of-00007.bin\u2019\
          </p>\n<p>What do I have to change there?</p>\n"
        raw: "I get this error message when trying to load the model with oobabooga:\r\
          \n\r\nTraceback (most recent call last):\r\nFile \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 442, in load_state_dict\r\nreturn torch.load(checkpoint_file, map_location=\u201C\
          cpu\u201D)\r\nFile \u201C/home/lukas/.local/lib/python3.10/site-packages/torch/serialization.py\u201D\
          , line 791, in load\r\nwith _open_file_like(f, \u2018rb\u2019) as opened_file:\r\
          \nFile \u201C/home/lukas/.local/lib/python3.10/site-packages/torch/serialization.py\u201D\
          , line 271, in _open_file_like\r\nreturn _open_file(name_or_buffer, mode)\r\
          \nFile \u201C/home/lukas/.local/lib/python3.10/site-packages/torch/serialization.py\u201D\
          , line 252, in init\r\nsuper().init(open(name, mode))\r\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: \u2018models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/pytorch_model-00001-of-00007.bin\u2019\
          \r\n\r\nDuring handling of the above exception, another exception occurred:\r\
          \n\r\nTraceback (most recent call last):\r\nFile \u201C/home/lukas/text-generation-webui/server.py\u201D\
          , line 102, in load_model_wrapper\r\nshared.model, shared.tokenizer = load_model(shared.model_name)\r\
          \nFile \u201C/home/lukas/text-generation-webui/modules/models.py\u201D,\
          \ line 217, in load_model\r\nmodel = LoaderClass.from_pretrained(checkpoint,\
          \ **params)\r\nFile \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u201D\
          , line 471, in from_pretrained\r\nreturn model_class.from_pretrained(\r\n\
          File \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 2795, in from_pretrained\r\n) = cls._load_pretrained_model(\r\nFile\
          \ \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 3109, in _load_pretrained_model\r\nstate_dict = load_state_dict(shard_file)\r\
          \nFile \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
          , line 445, in load_state_dict\r\nwith open(checkpoint_file) as f:\r\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: \u2018models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/pytorch_model-00001-of-00007.bin\u2019\
          \r\n\r\nWhat do I have to change there?"
        updatedAt: '2023-04-30T17:30:55.592Z'
      numEdits: 0
      reactions: []
    id: 644ea5cfd6001776ed78035d
    type: comment
  author: DrSmurf
  content: "I get this error message when trying to load the model with oobabooga:\r\
    \n\r\nTraceback (most recent call last):\r\nFile \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
    , line 442, in load_state_dict\r\nreturn torch.load(checkpoint_file, map_location=\u201C\
    cpu\u201D)\r\nFile \u201C/home/lukas/.local/lib/python3.10/site-packages/torch/serialization.py\u201D\
    , line 791, in load\r\nwith _open_file_like(f, \u2018rb\u2019) as opened_file:\r\
    \nFile \u201C/home/lukas/.local/lib/python3.10/site-packages/torch/serialization.py\u201D\
    , line 271, in _open_file_like\r\nreturn _open_file(name_or_buffer, mode)\r\n\
    File \u201C/home/lukas/.local/lib/python3.10/site-packages/torch/serialization.py\u201D\
    , line 252, in init\r\nsuper().init(open(name, mode))\r\nFileNotFoundError: [Errno\
    \ 2] No such file or directory: \u2018models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/pytorch_model-00001-of-00007.bin\u2019\
    \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\
    \r\nTraceback (most recent call last):\r\nFile \u201C/home/lukas/text-generation-webui/server.py\u201D\
    , line 102, in load_model_wrapper\r\nshared.model, shared.tokenizer = load_model(shared.model_name)\r\
    \nFile \u201C/home/lukas/text-generation-webui/modules/models.py\u201D, line 217,\
    \ in load_model\r\nmodel = LoaderClass.from_pretrained(checkpoint, **params)\r\
    \nFile \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u201D\
    , line 471, in from_pretrained\r\nreturn model_class.from_pretrained(\r\nFile\
    \ \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
    , line 2795, in from_pretrained\r\n) = cls._load_pretrained_model(\r\nFile \u201C\
    /home/lukas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
    , line 3109, in _load_pretrained_model\r\nstate_dict = load_state_dict(shard_file)\r\
    \nFile \u201C/home/lukas/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u201D\
    , line 445, in load_state_dict\r\nwith open(checkpoint_file) as f:\r\nFileNotFoundError:\
    \ [Errno 2] No such file or directory: \u2018models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/pytorch_model-00001-of-00007.bin\u2019\
    \r\n\r\nWhat do I have to change there?"
  created_at: 2023-04-30 16:30:55+00:00
  edited: false
  hidden: false
  id: 644ea5cfd6001776ed78035d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-30T17:46:46.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This issue occurs when the GPTQ parameters are not set in the ooba
          UI.   Instructions are in the README for how to set these params.</p>

          <p>So I assume you''re using one of the 1024g models?</p>

          <p>Unfortunately there''s an issue at the moment - the UI won''t let you
          set a groupsize of 1024.  I''ve submitted some code to oobabooga to fix
          this (here: <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/pull/1660">https://github.com/oobabooga/text-generation-webui/pull/1660</a>)
          but it''s not been merged yet.</p>

          <p>Until this code is merged into text-generation-webui, you have three
          options:</p>

          <ol>

          <li>Use command line arguments to specify the correct GPTQ params. Launch
          the ooba UI with the following command line arguments: <code>--groupsize
          1024 --wbits 4 --model_type llama</code>, eg:</li>

          </ol>

          <pre><code> python server.py --wbits 4 --groupsize 1024 --model_type llama
          --quant_attn --warmup_autotune --fused_mlp  #plus any other command line
          args you want

          </code></pre>

          <p>(don''t add <code>--quant_attn --warmup_autotune --fused_mlp </code>
          unless you''re using Triton GPTQ-for-LLaMa - but hopefully you are using
          Triton as you''re on Linux)</p>

          <ol start="2">

          <li>If you don''t want to use different command line arguments for some
          reason, you could instead use a groupsize 128 model. I have two available
          in 128g-compat and 128g-latest branches:</li>

          </ol>

          <ul>

          <li><a href="https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/tree/128-compat">https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/tree/128-compat</a></li>

          <li><a href="https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/tree/128-latest">https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/tree/128-latest</a></li>

          </ul>

          <p>These have groupsize = 128 so you will be able to set groupsize = 128
          in the UI as detailed in the README.</p>

          <p>Note that 128g models use a little more VRAM and are therefore more likely
          to go Out of Memory or slow down. You can try CPU offloading to try and
          avoid that.</p>

          <ol start="3">

          <li>Or if you don''t want to use a 128g mode and you don''t want to change
          the command line arguments, you could manually edit the UI''s code to allow
          specifying groupsize 1024 , as described here:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/0LiUxwIewE5lTk-UX4N71.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/0LiUxwIewE5lTk-UX4N71.png"></a></li>

          </ol>

          <p>Note that if you do this you won''t be able to <code>git pull</code>
          new updates without first doing <code>git reset --hard</code></p>

          <p>I''d recommend option 1.</p>

          <p>Hopefully ooba will soon merge my PR and then the UI issue will be resolved.</p>

          '
        raw: "This issue occurs when the GPTQ parameters are not set in the ooba UI.\
          \   Instructions are in the README for how to set these params.\n\nSo I\
          \ assume you're using one of the 1024g models?\n\nUnfortunately there's\
          \ an issue at the moment - the UI won't let you set a groupsize of 1024.\
          \  I've submitted some code to oobabooga to fix this (here: https://github.com/oobabooga/text-generation-webui/pull/1660)\
          \ but it's not been merged yet.\n\nUntil this code is merged into text-generation-webui,\
          \ you have three options:\n1. Use command line arguments to specify the\
          \ correct GPTQ params. Launch the ooba UI with the following command line\
          \ arguments: `--groupsize 1024 --wbits 4 --model_type llama`, eg:\n```\n\
          \ python server.py --wbits 4 --groupsize 1024 --model_type llama --quant_attn\
          \ --warmup_autotune --fused_mlp  #plus any other command line args you want\n\
          ```\n(don't add `--quant_attn --warmup_autotune --fused_mlp ` unless you're\
          \ using Triton GPTQ-for-LLaMa - but hopefully you are using Triton as you're\
          \ on Linux)\n\n2. If you don't want to use different command line arguments\
          \ for some reason, you could instead use a groupsize 128 model. I have two\
          \ available in 128g-compat and 128g-latest branches:\n* https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/tree/128-compat\n\
          * https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/tree/128-latest\n\
          \nThese have groupsize = 128 so you will be able to set groupsize = 128\
          \ in the UI as detailed in the README.\n\nNote that 128g models use a little\
          \ more VRAM and are therefore more likely to go Out of Memory or slow down.\
          \ You can try CPU offloading to try and avoid that.\n\n3. Or if you don't\
          \ want to use a 128g mode and you don't want to change the command line\
          \ arguments, you could manually edit the UI's code to allow specifying groupsize\
          \ 1024 , as described here:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/0LiUxwIewE5lTk-UX4N71.png)\n\
          \nNote that if you do this you won't be able to `git pull` new updates without\
          \ first doing `git reset --hard`\n\nI'd recommend option 1.\n\nHopefully\
          \ ooba will soon merge my PR and then the UI issue will be resolved."
        updatedAt: '2023-04-30T17:47:18.997Z'
      numEdits: 1
      reactions: []
    id: 644ea986cf72e60a5b7ea761
    type: comment
  author: TheBloke
  content: "This issue occurs when the GPTQ parameters are not set in the ooba UI.\
    \   Instructions are in the README for how to set these params.\n\nSo I assume\
    \ you're using one of the 1024g models?\n\nUnfortunately there's an issue at the\
    \ moment - the UI won't let you set a groupsize of 1024.  I've submitted some\
    \ code to oobabooga to fix this (here: https://github.com/oobabooga/text-generation-webui/pull/1660)\
    \ but it's not been merged yet.\n\nUntil this code is merged into text-generation-webui,\
    \ you have three options:\n1. Use command line arguments to specify the correct\
    \ GPTQ params. Launch the ooba UI with the following command line arguments: `--groupsize\
    \ 1024 --wbits 4 --model_type llama`, eg:\n```\n python server.py --wbits 4 --groupsize\
    \ 1024 --model_type llama --quant_attn --warmup_autotune --fused_mlp  #plus any\
    \ other command line args you want\n```\n(don't add `--quant_attn --warmup_autotune\
    \ --fused_mlp ` unless you're using Triton GPTQ-for-LLaMa - but hopefully you\
    \ are using Triton as you're on Linux)\n\n2. If you don't want to use different\
    \ command line arguments for some reason, you could instead use a groupsize 128\
    \ model. I have two available in 128g-compat and 128g-latest branches:\n* https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/tree/128-compat\n\
    * https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ/tree/128-latest\n\
    \nThese have groupsize = 128 so you will be able to set groupsize = 128 in the\
    \ UI as detailed in the README.\n\nNote that 128g models use a little more VRAM\
    \ and are therefore more likely to go Out of Memory or slow down. You can try\
    \ CPU offloading to try and avoid that.\n\n3. Or if you don't want to use a 128g\
    \ mode and you don't want to change the command line arguments, you could manually\
    \ edit the UI's code to allow specifying groupsize 1024 , as described here:\n\
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/0LiUxwIewE5lTk-UX4N71.png)\n\
    \nNote that if you do this you won't be able to `git pull` new updates without\
    \ first doing `git reset --hard`\n\nI'd recommend option 1.\n\nHopefully ooba\
    \ will soon merge my PR and then the UI issue will be resolved."
  created_at: 2023-04-30 16:46:46+00:00
  edited: true
  hidden: false
  id: 644ea986cf72e60a5b7ea761
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e94bb2eb82bfccf36a4469e4ca85893.svg
      fullname: Lukas Adelbrecht
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DrSmurf
      type: user
    createdAt: '2023-04-30T18:46:14.000Z'
    data:
      edited: false
      editors:
      - DrSmurf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e94bb2eb82bfccf36a4469e4ca85893.svg
          fullname: Lukas Adelbrecht
          isHf: false
          isPro: false
          name: DrSmurf
          type: user
        html: '<p>Thank you very much for this great answer!<br>Yes I wanted to use
          the 1024g model as I have a P40 and fear that the vram usage might be to
          high with 128g.</p>

          '
        raw: "Thank you very much for this great answer! \nYes I wanted to use the\
          \ 1024g model as I have a P40 and fear that the vram usage might be to high\
          \ with 128g."
        updatedAt: '2023-04-30T18:46:14.787Z'
      numEdits: 0
      reactions: []
    id: 644eb776bf9683cba466738b
    type: comment
  author: DrSmurf
  content: "Thank you very much for this great answer! \nYes I wanted to use the 1024g\
    \ model as I have a P40 and fear that the vram usage might be to high with 128g."
  created_at: 2023-04-30 17:46:14+00:00
  edited: false
  hidden: false
  id: 644eb776bf9683cba466738b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-05-02T12:40:21.000Z'
    data:
      edited: false
      editors:
      - yehiaserag
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: '<p>Merged 8 hours ago</p>

          '
        raw: Merged 8 hours ago
        updatedAt: '2023-05-02T12:40:21.895Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - DrSmurf
        - TheBloke
    id: 645104b541f3c769b908fc67
    type: comment
  author: yehiaserag
  content: Merged 8 hours ago
  created_at: 2023-05-02 11:40:21+00:00
  edited: false
  hidden: false
  id: 645104b541f3c769b908fc67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
      fullname: poisenbery
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: poisenbery
      type: user
    createdAt: '2023-05-08T22:36:28.000Z'
    data:
      edited: false
      editors:
      - poisenbery
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
          fullname: poisenbery
          isHf: false
          isPro: false
          name: poisenbery
          type: user
        html: '<p>Hey sorry if this is a dumb question but I''m not really smart at
          this type of stuff.<br>I keep getting this error:</p>

          <p>weight = weight.reshape(-1, self.groupsize, weight.shape[2])<br>RuntimeError:
          shape ''[-1, 1024, 6656]'' is invalid for input of size 44302336</p>

          '
        raw: 'Hey sorry if this is a dumb question but I''m not really smart at this
          type of stuff.

          I keep getting this error:


          weight = weight.reshape(-1, self.groupsize, weight.shape[2])

          RuntimeError: shape ''[-1, 1024, 6656]'' is invalid for input of size 44302336'
        updatedAt: '2023-05-08T22:36:28.592Z'
      numEdits: 0
      reactions: []
    id: 6459796c232e5f0712bc19ae
    type: comment
  author: poisenbery
  content: 'Hey sorry if this is a dumb question but I''m not really smart at this
    type of stuff.

    I keep getting this error:


    weight = weight.reshape(-1, self.groupsize, weight.shape[2])

    RuntimeError: shape ''[-1, 1024, 6656]'' is invalid for input of size 44302336'
  created_at: 2023-05-08 21:36:28+00:00
  edited: false
  hidden: false
  id: 6459796c232e5f0712bc19ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-09T11:17:36.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Hey sorry if this is a dumb question but I''m not really smart at this
          type of stuff.<br>I keep getting this error:</p>

          <p>weight = weight.reshape(-1, self.groupsize, weight.shape[2])<br>RuntimeError:
          shape ''[-1, 1024, 6656]'' is invalid for input of size 44302336</p>

          </blockquote>

          <p>Firstly please check the sha256sum of the downloaded models and confirm
          it matches the ones listed in this HF repo.  This might be because the model
          hasn''t been downloaded properly.</p>

          '
        raw: "> Hey sorry if this is a dumb question but I'm not really smart at this\
          \ type of stuff.\n> I keep getting this error:\n> \n> weight = weight.reshape(-1,\
          \ self.groupsize, weight.shape[2])\n> RuntimeError: shape '[-1, 1024, 6656]'\
          \ is invalid for input of size 44302336\n\nFirstly please check the sha256sum\
          \ of the downloaded models and confirm it matches the ones listed in this\
          \ HF repo.  This might be because the model hasn't been downloaded properly."
        updatedAt: '2023-05-09T11:17:36.464Z'
      numEdits: 0
      reactions: []
    id: 645a2bd01b60cc62d6243d3f
    type: comment
  author: TheBloke
  content: "> Hey sorry if this is a dumb question but I'm not really smart at this\
    \ type of stuff.\n> I keep getting this error:\n> \n> weight = weight.reshape(-1,\
    \ self.groupsize, weight.shape[2])\n> RuntimeError: shape '[-1, 1024, 6656]' is\
    \ invalid for input of size 44302336\n\nFirstly please check the sha256sum of\
    \ the downloaded models and confirm it matches the ones listed in this HF repo.\
    \  This might be because the model hasn't been downloaded properly."
  created_at: 2023-05-09 10:17:36+00:00
  edited: false
  hidden: false
  id: 645a2bd01b60cc62d6243d3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-09T11:17:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Hey sorry if this is a dumb question but I''m not really smart at this
          type of stuff.<br>I keep getting this error:</p>

          <p>weight = weight.reshape(-1, self.groupsize, weight.shape[2])<br>RuntimeError:
          shape ''[-1, 1024, 6656]'' is invalid for input of size 44302336</p>

          </blockquote>

          <p>Firstly please check the sha256sum of the downloaded models and confirm
          it matches the ones listed in this HF repo.  This might be because the model
          hasn''t been downloaded properly.</p>

          '
        raw: "> Hey sorry if this is a dumb question but I'm not really smart at this\
          \ type of stuff.\n> I keep getting this error:\n> \n> weight = weight.reshape(-1,\
          \ self.groupsize, weight.shape[2])\n> RuntimeError: shape '[-1, 1024, 6656]'\
          \ is invalid for input of size 44302336\n\nFirstly please check the sha256sum\
          \ of the downloaded models and confirm it matches the ones listed in this\
          \ HF repo.  This might be because the model hasn't been downloaded properly."
        updatedAt: '2023-05-09T11:17:46.425Z'
      numEdits: 0
      reactions: []
    id: 645a2bda85dad0abdea1f72c
    type: comment
  author: TheBloke
  content: "> Hey sorry if this is a dumb question but I'm not really smart at this\
    \ type of stuff.\n> I keep getting this error:\n> \n> weight = weight.reshape(-1,\
    \ self.groupsize, weight.shape[2])\n> RuntimeError: shape '[-1, 1024, 6656]' is\
    \ invalid for input of size 44302336\n\nFirstly please check the sha256sum of\
    \ the downloaded models and confirm it matches the ones listed in this HF repo.\
    \  This might be because the model hasn't been downloaded properly."
  created_at: 2023-05-09 10:17:46+00:00
  edited: false
  hidden: false
  id: 645a2bda85dad0abdea1f72c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-09T11:18:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Hey sorry if this is a dumb question but I''m not really smart at this
          type of stuff.<br>I keep getting this error:</p>

          <p>weight = weight.reshape(-1, self.groupsize, weight.shape[2])<br>RuntimeError:
          shape ''[-1, 1024, 6656]'' is invalid for input of size 44302336</p>

          </blockquote>

          <p>Firstly please check the sha256sum of the downloaded models and confirm
          it matches the ones listed in this HF repo.  This might be because the model
          hasn''t been downloaded properly.</p>

          '
        raw: "> Hey sorry if this is a dumb question but I'm not really smart at this\
          \ type of stuff.\n> I keep getting this error:\n> \n> weight = weight.reshape(-1,\
          \ self.groupsize, weight.shape[2])\n> RuntimeError: shape '[-1, 1024, 6656]'\
          \ is invalid for input of size 44302336\n\nFirstly please check the sha256sum\
          \ of the downloaded models and confirm it matches the ones listed in this\
          \ HF repo.  This might be because the model hasn't been downloaded properly."
        updatedAt: '2023-05-09T11:18:06.934Z'
      numEdits: 0
      reactions: []
    id: 645a2bee1b60cc62d624417b
    type: comment
  author: TheBloke
  content: "> Hey sorry if this is a dumb question but I'm not really smart at this\
    \ type of stuff.\n> I keep getting this error:\n> \n> weight = weight.reshape(-1,\
    \ self.groupsize, weight.shape[2])\n> RuntimeError: shape '[-1, 1024, 6656]' is\
    \ invalid for input of size 44302336\n\nFirstly please check the sha256sum of\
    \ the downloaded models and confirm it matches the ones listed in this HF repo.\
    \  This might be because the model hasn't been downloaded properly."
  created_at: 2023-05-09 10:18:06+00:00
  edited: false
  hidden: false
  id: 645a2bee1b60cc62d624417b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-09T11:20:47.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Hey sorry if this is a dumb question but I''m not really smart at this
          type of stuff.<br>I keep getting this error:</p>

          <p>weight = weight.reshape(-1, self.groupsize, weight.shape[2])<br>RuntimeError:
          shape ''[-1, 1024, 6656]'' is invalid for input of size 44302336</p>

          </blockquote>

          <p>Firstly please check the sha256sum of the downloaded models and confirm
          it matches the ones listed in this HF repo.  This might be because the model
          hasn''t been downloaded properly.</p>

          '
        raw: "> Hey sorry if this is a dumb question but I'm not really smart at this\
          \ type of stuff.\n> I keep getting this error:\n> \n> weight = weight.reshape(-1,\
          \ self.groupsize, weight.shape[2])\n> RuntimeError: shape '[-1, 1024, 6656]'\
          \ is invalid for input of size 44302336\n\nFirstly please check the sha256sum\
          \ of the downloaded models and confirm it matches the ones listed in this\
          \ HF repo.  This might be because the model hasn't been downloaded properly."
        updatedAt: '2023-05-09T11:20:47.552Z'
      numEdits: 0
      reactions: []
    id: 645a2c8f85dad0abdea2124d
    type: comment
  author: TheBloke
  content: "> Hey sorry if this is a dumb question but I'm not really smart at this\
    \ type of stuff.\n> I keep getting this error:\n> \n> weight = weight.reshape(-1,\
    \ self.groupsize, weight.shape[2])\n> RuntimeError: shape '[-1, 1024, 6656]' is\
    \ invalid for input of size 44302336\n\nFirstly please check the sha256sum of\
    \ the downloaded models and confirm it matches the ones listed in this HF repo.\
    \  This might be because the model hasn't been downloaded properly."
  created_at: 2023-05-09 10:20:47+00:00
  edited: false
  hidden: false
  id: 645a2c8f85dad0abdea2124d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
      fullname: poisenbery
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: poisenbery
      type: user
    createdAt: '2023-05-09T23:44:31.000Z'
    data:
      edited: false
      editors:
      - poisenbery
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
          fullname: poisenbery
          isHf: false
          isPro: false
          name: poisenbery
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Hey sorry if this is a dumb question but I''m not really smart at this
          type of stuff.<br>I keep getting this error:</p>

          <p>weight = weight.reshape(-1, self.groupsize, weight.shape[2])<br>RuntimeError:
          shape ''[-1, 1024, 6656]'' is invalid for input of size 44302336</p>

          </blockquote>

          <p>Firstly please check the sha256sum of the downloaded models and confirm
          it matches the ones listed in this HF repo.  This might be because the model
          hasn''t been downloaded properly.</p>

          </blockquote>

          <p>V:\AI\oobabooga-windows\text-generation-webui\models\TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ&gt;CertUtil
          -hashfile V:\AI\oobabooga-windows\text-generation-webui\models\TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors
          SHA256<br>SHA256 hash of V:\AI\oobabooga-windows\text-generation-webui\models\TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors:<br>1fe1a29b637f46da5bc48d9b97deeb87020edbd0c3765fddf1b7a0b207754534<br>CertUtil:
          -hashfile command completed successfully.</p>

          <p>It''s the same. It will generate 1 response before error.<br>If I use
          any character file it will error without generating a response.</p>

          '
        raw: "> > Hey sorry if this is a dumb question but I'm not really smart at\
          \ this type of stuff.\n> > I keep getting this error:\n> > \n> > weight\
          \ = weight.reshape(-1, self.groupsize, weight.shape[2])\n> > RuntimeError:\
          \ shape '[-1, 1024, 6656]' is invalid for input of size 44302336\n> \n>\
          \ Firstly please check the sha256sum of the downloaded models and confirm\
          \ it matches the ones listed in this HF repo.  This might be because the\
          \ model hasn't been downloaded properly.\n\nV:\\AI\\oobabooga-windows\\\
          text-generation-webui\\models\\TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ>CertUtil\
          \ -hashfile V:\\AI\\oobabooga-windows\\text-generation-webui\\models\\TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\\\
          OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors\
          \ SHA256\nSHA256 hash of V:\\AI\\oobabooga-windows\\text-generation-webui\\\
          models\\TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\\OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors:\n\
          1fe1a29b637f46da5bc48d9b97deeb87020edbd0c3765fddf1b7a0b207754534\nCertUtil:\
          \ -hashfile command completed successfully.\n\n\n\nIt's the same. It will\
          \ generate 1 response before error.\nIf I use any character file it will\
          \ error without generating a response."
        updatedAt: '2023-05-09T23:44:31.602Z'
      numEdits: 0
      reactions: []
    id: 645adadfe505443f81997c14
    type: comment
  author: poisenbery
  content: "> > Hey sorry if this is a dumb question but I'm not really smart at this\
    \ type of stuff.\n> > I keep getting this error:\n> > \n> > weight = weight.reshape(-1,\
    \ self.groupsize, weight.shape[2])\n> > RuntimeError: shape '[-1, 1024, 6656]'\
    \ is invalid for input of size 44302336\n> \n> Firstly please check the sha256sum\
    \ of the downloaded models and confirm it matches the ones listed in this HF repo.\
    \  This might be because the model hasn't been downloaded properly.\n\nV:\\AI\\\
    oobabooga-windows\\text-generation-webui\\models\\TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ>CertUtil\
    \ -hashfile V:\\AI\\oobabooga-windows\\text-generation-webui\\models\\TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\\\
    OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors SHA256\n\
    SHA256 hash of V:\\AI\\oobabooga-windows\\text-generation-webui\\models\\TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\\\
    OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors:\n1fe1a29b637f46da5bc48d9b97deeb87020edbd0c3765fddf1b7a0b207754534\n\
    CertUtil: -hashfile command completed successfully.\n\n\n\nIt's the same. It will\
    \ generate 1 response before error.\nIf I use any character file it will error\
    \ without generating a response."
  created_at: 2023-05-09 22:44:31+00:00
  edited: false
  hidden: false
  id: 645adadfe505443f81997c14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-10T09:54:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK please double check you have the right GPTQ parameters for the
          model you''re using.  Apparently there''s a bug in text-gen-ui at the moment
          where these params can get reset, so try:</p>

          <ol>

          <li>Load model</li>

          <li>On models page, Set GPTQ params - bits = 4, model_type = llama, groupsize
          = appropriate one for the model you''re using (128 or 1024)</li>

          <li>Click Reload the model</li>

          <li>Test</li>

          </ol>

          <p>Let me know</p>

          '
        raw: 'OK please double check you have the right GPTQ parameters for the model
          you''re using.  Apparently there''s a bug in text-gen-ui at the moment where
          these params can get reset, so try:

          1. Load model

          2. On models page, Set GPTQ params - bits = 4, model_type = llama, groupsize
          = appropriate one for the model you''re using (128 or 1024)

          3. Click Reload the model

          4. Test


          Let me know'
        updatedAt: '2023-05-10T09:54:46.982Z'
      numEdits: 0
      reactions: []
    id: 645b69e6337b2ccf07f7bab8
    type: comment
  author: TheBloke
  content: 'OK please double check you have the right GPTQ parameters for the model
    you''re using.  Apparently there''s a bug in text-gen-ui at the moment where these
    params can get reset, so try:

    1. Load model

    2. On models page, Set GPTQ params - bits = 4, model_type = llama, groupsize =
    appropriate one for the model you''re using (128 or 1024)

    3. Click Reload the model

    4. Test


    Let me know'
  created_at: 2023-05-10 08:54:46+00:00
  edited: false
  hidden: false
  id: 645b69e6337b2ccf07f7bab8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
      fullname: poisenbery
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: poisenbery
      type: user
    createdAt: '2023-05-10T13:39:45.000Z'
    data:
      edited: true
      editors:
      - poisenbery
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
          fullname: poisenbery
          isHf: false
          isPro: false
          name: poisenbery
          type: user
        html: '<p>I do. I checked like...12 times to make sure that I wasn''t forgetting
          anything. I have the settings in the WebUI, I saved the parameters for that
          model, and I have the command line args.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6370c7a73434f2764ce43ba9/WyKUbteDmOSDGcvrHp5bV.png"><img
          alt="Screenshot (1495).png" src="https://cdn-uploads.huggingface.co/production/uploads/6370c7a73434f2764ce43ba9/WyKUbteDmOSDGcvrHp5bV.png"></a></p>

          <p>  File "V:\AI\oobabooga-windows\text-generation-webui\repositories\GPTQ-for-LLaMa\quant.py",
          line 362, in forward<br>    weight = weight.reshape(-1, self.groupsize,
          weight.shape[2])<br>RuntimeError: shape ''[-1, 1024, 6656]'' is invalid
          for input of size 44302336<br>Output generated in 0.40 seconds (0.00 tokens/s,
          0 tokens, context 231, seed 408650408)</p>

          <p>I''m using the 1click installer on windows.</p>

          <p>Before, it would generate 1 response and error.<br>Now, it''s just erroring.</p>

          '
        raw: "I do. I checked like...12 times to make sure that I wasn't forgetting\
          \ anything. I have the settings in the WebUI, I saved the parameters for\
          \ that model, and I have the command line args.\n![Screenshot (1495).png](https://cdn-uploads.huggingface.co/production/uploads/6370c7a73434f2764ce43ba9/WyKUbteDmOSDGcvrHp5bV.png)\n\
          \n\n  File \"V:\\AI\\oobabooga-windows\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\quant.py\", line 362, in forward\n    weight = weight.reshape(-1,\
          \ self.groupsize, weight.shape[2])\nRuntimeError: shape '[-1, 1024, 6656]'\
          \ is invalid for input of size 44302336\nOutput generated in 0.40 seconds\
          \ (0.00 tokens/s, 0 tokens, context 231, seed 408650408)\n\nI'm using the\
          \ 1click installer on windows.\n\n\nBefore, it would generate 1 response\
          \ and error.\nNow, it's just erroring."
        updatedAt: '2023-05-10T13:41:55.150Z'
      numEdits: 1
      reactions: []
    id: 645b9ea1337b2ccf07fa3c25
    type: comment
  author: poisenbery
  content: "I do. I checked like...12 times to make sure that I wasn't forgetting\
    \ anything. I have the settings in the WebUI, I saved the parameters for that\
    \ model, and I have the command line args.\n![Screenshot (1495).png](https://cdn-uploads.huggingface.co/production/uploads/6370c7a73434f2764ce43ba9/WyKUbteDmOSDGcvrHp5bV.png)\n\
    \n\n  File \"V:\\AI\\oobabooga-windows\\text-generation-webui\\repositories\\\
    GPTQ-for-LLaMa\\quant.py\", line 362, in forward\n    weight = weight.reshape(-1,\
    \ self.groupsize, weight.shape[2])\nRuntimeError: shape '[-1, 1024, 6656]' is\
    \ invalid for input of size 44302336\nOutput generated in 0.40 seconds (0.00 tokens/s,\
    \ 0 tokens, context 231, seed 408650408)\n\nI'm using the 1click installer on\
    \ windows.\n\n\nBefore, it would generate 1 response and error.\nNow, it's just\
    \ erroring."
  created_at: 2023-05-10 12:39:45+00:00
  edited: true
  hidden: false
  id: 645b9ea1337b2ccf07fa3c25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-10T14:00:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please try updating text-gen-ui using the update.bat in the 1-click
          installer.  There was a bug fix recently related to GPTQ parameters not
          saving</p>

          <p>So:</p>

          <ol>

          <li>Close text-gen-ui</li>

          <li>Apply the update</li>

          <li>Re-open the UI</li>

          <li>Load my model </li>

          <li>Set the GPTQ parameters again</li>

          <li>Click "Save settings for this model"</li>

          <li>Click Reload this model</li>

          <li>Test and let me know.</li>

          </ol>

          <p>Also, please show me a screenshot of the contents of your <code>models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ</code>
          folder</p>

          '
        raw: "Please try updating text-gen-ui using the update.bat in the 1-click\
          \ installer.  There was a bug fix recently related to GPTQ parameters not\
          \ saving\n\nSo:\n\n1. Close text-gen-ui\n2. Apply the update\n3. Re-open\
          \ the UI\n4. Load my model \n5. Set the GPTQ parameters again\n6. Click\
          \ \"Save settings for this model\"\n7. Click Reload this model\n8. Test\
          \ and let me know.\n\nAlso, please show me a screenshot of the contents\
          \ of your `models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ` folder"
        updatedAt: '2023-05-10T14:00:16.882Z'
      numEdits: 0
      reactions: []
    id: 645ba370b4e65f04f7f53376
    type: comment
  author: TheBloke
  content: "Please try updating text-gen-ui using the update.bat in the 1-click installer.\
    \  There was a bug fix recently related to GPTQ parameters not saving\n\nSo:\n\
    \n1. Close text-gen-ui\n2. Apply the update\n3. Re-open the UI\n4. Load my model\
    \ \n5. Set the GPTQ parameters again\n6. Click \"Save settings for this model\"\
    \n7. Click Reload this model\n8. Test and let me know.\n\nAlso, please show me\
    \ a screenshot of the contents of your `models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ`\
    \ folder"
  created_at: 2023-05-10 13:00:16+00:00
  edited: false
  hidden: false
  id: 645ba370b4e65f04f7f53376
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
      fullname: poisenbery
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: poisenbery
      type: user
    createdAt: '2023-05-10T18:35:42.000Z'
    data:
      edited: true
      editors:
      - poisenbery
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
          fullname: poisenbery
          isHf: false
          isPro: false
          name: poisenbery
          type: user
        html: '<p>I''ve already tried re-installing and updating again just to be
          safe.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6370c7a73434f2764ce43ba9/Ed8T_t24j8Jpf7ptbb-bh.png"><img
          alt="Screenshot (1497).png" src="https://cdn-uploads.huggingface.co/production/uploads/6370c7a73434f2764ce43ba9/Ed8T_t24j8Jpf7ptbb-bh.png"></a></p>

          <p>EDIT: I tried re-installing GPTQ and it''s not working, so I think that
          this is something wrong with my install. I''m gonna ask on oobabooga github
          because everything seems to be ok on the model side of things.</p>

          '
        raw: 'I''ve already tried re-installing and updating again just to be safe.



          ![Screenshot (1497).png](https://cdn-uploads.huggingface.co/production/uploads/6370c7a73434f2764ce43ba9/Ed8T_t24j8Jpf7ptbb-bh.png)



          EDIT: I tried re-installing GPTQ and it''s not working, so I think that
          this is something wrong with my install. I''m gonna ask on oobabooga github
          because everything seems to be ok on the model side of things.'
        updatedAt: '2023-05-10T19:00:35.149Z'
      numEdits: 1
      reactions: []
    id: 645be3feb396a40a8492f9a9
    type: comment
  author: poisenbery
  content: 'I''ve already tried re-installing and updating again just to be safe.



    ![Screenshot (1497).png](https://cdn-uploads.huggingface.co/production/uploads/6370c7a73434f2764ce43ba9/Ed8T_t24j8Jpf7ptbb-bh.png)



    EDIT: I tried re-installing GPTQ and it''s not working, so I think that this is
    something wrong with my install. I''m gonna ask on oobabooga github because everything
    seems to be ok on the model side of things.'
  created_at: 2023-05-10 17:35:42+00:00
  edited: true
  hidden: false
  id: 645be3feb396a40a8492f9a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-10T20:09:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah that all looks OK.  You can delete pytorch_model.bin.index.json
          by the way, that file shouldn''t be there.  But it shouldn''t break anything
          either.</p>

          '
        raw: Yeah that all looks OK.  You can delete pytorch_model.bin.index.json
          by the way, that file shouldn't be there.  But it shouldn't break anything
          either.
        updatedAt: '2023-05-10T20:09:20.942Z'
      numEdits: 0
      reactions: []
    id: 645bf9f08ac8cdceb38d74fb
    type: comment
  author: TheBloke
  content: Yeah that all looks OK.  You can delete pytorch_model.bin.index.json by
    the way, that file shouldn't be there.  But it shouldn't break anything either.
  created_at: 2023-05-10 19:09:20+00:00
  edited: false
  hidden: false
  id: 645bf9f08ac8cdceb38d74fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
      fullname: poisenbery
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: poisenbery
      type: user
    createdAt: '2023-05-10T20:21:05.000Z'
    data:
      edited: false
      editors:
      - poisenbery
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
          fullname: poisenbery
          isHf: false
          isPro: false
          name: poisenbery
          type: user
        html: '<p>Upon further research, it seems like I''m not the only person experiencing
          difficulties getting GPTQ to work on windows.</p>

          <p>I''m probably just going to install linux because it seems like it''s
          a better platform overall for this type of stuff.</p>

          <p>Thanks for the help. I know what I need to do now.</p>

          '
        raw: 'Upon further research, it seems like I''m not the only person experiencing
          difficulties getting GPTQ to work on windows.


          I''m probably just going to install linux because it seems like it''s a
          better platform overall for this type of stuff.


          Thanks for the help. I know what I need to do now.'
        updatedAt: '2023-05-10T20:21:05.256Z'
      numEdits: 0
      reactions: []
    id: 645bfcb10120c98d16ab55e9
    type: comment
  author: poisenbery
  content: 'Upon further research, it seems like I''m not the only person experiencing
    difficulties getting GPTQ to work on windows.


    I''m probably just going to install linux because it seems like it''s a better
    platform overall for this type of stuff.


    Thanks for the help. I know what I need to do now.'
  created_at: 2023-05-10 19:21:05+00:00
  edited: false
  hidden: false
  id: 645bfcb10120c98d16ab55e9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-10T20:22:03.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah it is a lot easier in Linux. You could install WSL2 on top
          of Windows, and then you don''t need to reboot. Your NVidia GPU will be
          supported. It works quite well I''m told.</p>

          <p><a rel="nofollow" href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html">https://docs.nvidia.com/cuda/wsl-user-guide/index.html</a></p>

          '
        raw: 'Yeah it is a lot easier in Linux. You could install WSL2 on top of Windows,
          and then you don''t need to reboot. Your NVidia GPU will be supported. It
          works quite well I''m told.


          https://docs.nvidia.com/cuda/wsl-user-guide/index.html'
        updatedAt: '2023-05-10T20:22:03.237Z'
      numEdits: 0
      reactions: []
    id: 645bfcebb396a40a8493dbae
    type: comment
  author: TheBloke
  content: 'Yeah it is a lot easier in Linux. You could install WSL2 on top of Windows,
    and then you don''t need to reboot. Your NVidia GPU will be supported. It works
    quite well I''m told.


    https://docs.nvidia.com/cuda/wsl-user-guide/index.html'
  created_at: 2023-05-10 19:22:03+00:00
  edited: false
  hidden: false
  id: 645bfcebb396a40a8493dbae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
      fullname: poisenbery
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: poisenbery
      type: user
    createdAt: '2023-05-14T11:32:13.000Z'
    data:
      edited: false
      editors:
      - poisenbery
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04ec5a75e54f46a4c2b60aee301d276a.svg
          fullname: poisenbery
          isHf: false
          isPro: false
          name: poisenbery
          type: user
        html: '<p>I installed PopOS!, did all of the things, and had the SAME issue.</p>

          <p>I found out that the --chat argument was the reason why it was showing
          that error.</p>

          <p>Do you know if there is a way to get this to work with the --chat argument?</p>

          <p>I don''t really know too much about how any of this stuff works, and
          was actually surprised that --chat was the reason why it wasn''t working.</p>

          '
        raw: 'I installed PopOS!, did all of the things, and had the SAME issue.


          I found out that the --chat argument was the reason why it was showing that
          error.


          Do you know if there is a way to get this to work with the --chat argument?


          I don''t really know too much about how any of this stuff works, and was
          actually surprised that --chat was the reason why it wasn''t working.'
        updatedAt: '2023-05-14T11:32:13.465Z'
      numEdits: 0
      reactions: []
    id: 6460c6bd232a700f26592f19
    type: comment
  author: poisenbery
  content: 'I installed PopOS!, did all of the things, and had the SAME issue.


    I found out that the --chat argument was the reason why it was showing that error.


    Do you know if there is a way to get this to work with the --chat argument?


    I don''t really know too much about how any of this stuff works, and was actually
    surprised that --chat was the reason why it wasn''t working.'
  created_at: 2023-05-14 10:32:13+00:00
  edited: false
  hidden: false
  id: 6460c6bd232a700f26592f19
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/1e94bb2eb82bfccf36a4469e4ca85893.svg
      fullname: Lukas Adelbrecht
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DrSmurf
      type: user
    createdAt: '2023-05-14T20:35:23.000Z'
    data:
      status: closed
    id: 6461460b604bf8923335c925
    type: status-change
  author: DrSmurf
  created_at: 2023-05-14 19:35:23+00:00
  id: 6461460b604bf8923335c925
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-15T07:40:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;poisenbery&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/poisenbery\">@<span class=\"\
          underline\">poisenbery</span></a></span>\n\n\t</span></span>  I recently\
          \ released a new model in the main branch that uses group_size = None and\
          \ should resolve these problems.</p>\n"
        raw: '@poisenbery  I recently released a new model in the main branch that
          uses group_size = None and should resolve these problems.'
        updatedAt: '2023-05-15T07:40:45.148Z'
      numEdits: 0
      reactions: []
    id: 6461e1fd76f92cf538fa0115
    type: comment
  author: TheBloke
  content: '@poisenbery  I recently released a new model in the main branch that uses
    group_size = None and should resolve these problems.'
  created_at: 2023-05-15 06:40:45+00:00
  edited: false
  hidden: false
  id: 6461e1fd76f92cf538fa0115
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ
repo_type: model
status: closed
target_branch: null
title: Error when loading with oobabooga
