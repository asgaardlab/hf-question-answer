!!python/object:huggingface_hub.community.DiscussionWithDetails
author: e-caste
conflicting_files: null
created_at: 2023-05-16 08:37:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
      fullname: e
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: e-caste
      type: user
    createdAt: '2023-05-16T09:37:16.000Z'
    data:
      edited: true
      editors:
      - e-caste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
          fullname: e
          isHf: false
          isPro: false
          name: e-caste
          type: user
        html: "<p>My main issue is the fact that the model often replies with the\
          \ invisible character \"\\u200b\" only, and I believe that could be strictly\
          \ related to:</p>\n<ol>\n<li>the prompt structure</li>\n<li>the <code>quant-cuda</code>\
          \ package version/implementation</li>\n</ol>\n<h2 id=\"notes\">Notes</h2>\n\
          <p>I am using the <code>text-generation-webui/models/config-user.yaml</code>\
          \ you provided:</p>\n<pre><code class=\"language-yaml\"><span class=\"hljs-string\"\
          >TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ$:</span>\n  <span class=\"\
          hljs-attr\">auto_devices:</span> <span class=\"hljs-literal\">false</span>\n\
          \  <span class=\"hljs-attr\">bf16:</span> <span class=\"hljs-literal\">false</span>\n\
          \  <span class=\"hljs-attr\">cpu:</span> <span class=\"hljs-literal\">false</span>\n\
          \  <span class=\"hljs-attr\">cpu_memory:</span> <span class=\"hljs-number\"\
          >0</span>\n  <span class=\"hljs-attr\">disk:</span> <span class=\"hljs-literal\"\
          >false</span>\n  <span class=\"hljs-attr\">gpu_memory_0:</span> <span class=\"\
          hljs-number\">0</span>\n  <span class=\"hljs-attr\">groupsize:</span> <span\
          \ class=\"hljs-string\">None</span>\n  <span class=\"hljs-attr\">load_in_8bit:</span>\
          \ <span class=\"hljs-literal\">false</span>\n  <span class=\"hljs-attr\"\
          >mlock:</span> <span class=\"hljs-literal\">false</span>\n  <span class=\"\
          hljs-attr\">model_type:</span> <span class=\"hljs-string\">llama</span>\n\
          \  <span class=\"hljs-attr\">n_batch:</span> <span class=\"hljs-number\"\
          >512</span>\n  <span class=\"hljs-attr\">n_gpu_layers:</span> <span class=\"\
          hljs-number\">0</span>\n  <span class=\"hljs-attr\">pre_layer:</span> <span\
          \ class=\"hljs-number\">0</span>\n  <span class=\"hljs-attr\">threads:</span>\
          \ <span class=\"hljs-number\">0</span>\n  <span class=\"hljs-attr\">wbits:</span>\
          \ <span class=\"hljs-string\">'4'</span>\n</code></pre>\n<p>I am using the\
          \ integrated API to interface with the model. Here is my request body (parameters\
          \ are subject to change, but the ones directly affecting the results are\
          \ these):</p>\n<pre><code class=\"language-py\">request = {\n        <span\
          \ class=\"hljs-string\">'prompt'</span>: context,\n        <span class=\"\
          hljs-string\">'max_new_tokens'</span>: <span class=\"hljs-number\">500</span>,\n\
          \        <span class=\"hljs-string\">'do_sample'</span>: <span class=\"\
          hljs-literal\">True</span>,\n        <span class=\"hljs-string\">'temperature'</span>:\
          \ <span class=\"hljs-number\">1.3</span>,\n        <span class=\"hljs-string\"\
          >'top_p'</span>: <span class=\"hljs-number\">0.1</span>,\n        <span\
          \ class=\"hljs-string\">'typical_p'</span>: <span class=\"hljs-number\"\
          >1</span>,\n        <span class=\"hljs-string\">'repetition_penalty'</span>:\
          \ <span class=\"hljs-number\">1.18</span>,\n        <span class=\"hljs-string\"\
          >'top_k'</span>: <span class=\"hljs-number\">40</span>,\n        <span class=\"\
          hljs-string\">'min_length'</span>: <span class=\"hljs-number\">10</span>,\n\
          \        <span class=\"hljs-string\">'no_repeat_ngram_size'</span>: <span\
          \ class=\"hljs-number\">3</span>,\n        <span class=\"hljs-string\">'num_beams'</span>:\
          \ <span class=\"hljs-number\">1</span>,\n        <span class=\"hljs-string\"\
          >'penalty_alpha'</span>: <span class=\"hljs-number\">0</span>,\n       \
          \ <span class=\"hljs-string\">'length_penalty'</span>: <span class=\"hljs-number\"\
          >1</span>,\n        <span class=\"hljs-string\">'early_stopping'</span>:\
          \ <span class=\"hljs-literal\">False</span>,\n        <span class=\"hljs-string\"\
          >'seed'</span>: -<span class=\"hljs-number\">1</span>,\n        <span class=\"\
          hljs-string\">'add_bos_token'</span>: <span class=\"hljs-literal\">False</span>,\n\
          \        <span class=\"hljs-string\">'truncation_length'</span>: <span class=\"\
          hljs-number\">2048</span>,\n        <span class=\"hljs-string\">'ban_eos_token'</span>:\
          \ <span class=\"hljs-literal\">False</span>,\n        <span class=\"hljs-string\"\
          >'skip_special_tokens'</span>: <span class=\"hljs-literal\">True</span>,\n\
          \        <span class=\"hljs-string\">'stopping_strings'</span>: [],\n  \
          \  }\n</code></pre>\n<h2 id=\"prompt-structure\">Prompt structure</h2>\n\
          <h3 id=\"my-pre-prompt\">My pre-prompt</h3>\n<p>(partially stolen from MPT-7B-Chat)</p>\n\
          <pre><code class=\"language-py\">SYSTEMTOKEN = <span class=\"hljs-string\"\
          >\"&lt;|system|&gt; \"</span>  <span class=\"hljs-comment\"># unofficial</span>\n\
          USERTOKEN = <span class=\"hljs-string\">\"&lt;|prompter|&gt; \"</span>\n\
          ASSISTANTTOKEN = <span class=\"hljs-string\">\"&lt;|assistant|&gt;: \"</span>\n\
          ENDTOKEN = <span class=\"hljs-string\">\"\\n\"</span>\nstart_message = <span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{USERTOKEN}</span>You\
          \ are a helpful chatbot. You answer questions. You are excited to be able\
          \ to help the user and you will help them any way you can. You are more\
          \ than an information source, you are also able to code, write poetry, write\
          \ short stories, summarize text, and make jokes. You always reply after\
          \ `<span class=\"hljs-subst\">{ASSISTANTTOKEN}</span>`. Say okay if you\
          \ understand.\\n<span class=\"hljs-subst\">{ASSISTANTTOKEN}</span>okay.<span\
          \ class=\"hljs-subst\">{ENDTOKEN}</span>\"</span>\n</code></pre>\n<h3 id=\"\
          my-prompt-structure\">My prompt structure</h3>\n<pre><code class=\"language-py\"\
          >&lt;|prompter|&gt; write a haiku about the Python programming language\\\
          n&lt;|assistant|&gt;: \\u200b\\n\n</code></pre>\n<h2 id=\"the-quant-cuda-package\"\
          >The quant-cuda package</h2>\n<p>Many combinations to try. I'll leave the\
          \ template below to fill.<br>Most importantly, in general, 1024-X and 128-X\
          \ variants are affected by the phantom character issue. Instead, the groupsize=None\
          \ variant is affected by another issue that is predicting text outside of\
          \  the sensible context, with what seems to be stolen from chatbot web pages\
          \ where the bot is talking to another user, e.g:<br>-- BEGIN EXAMPLE<br>user:\
          \ write a Python program to generate haikus</p>\n<p>assistant: ```python<br>import\
          \ random<br>from collections import deque</p>\n<p>def haiku_generator():<br>\
          \    words = ['cherry', 'blossom', 'petal','spring', 'breeze']<br>    syllables\
          \ = [1, 2, 3]</p>\n<pre><code>for _ in range(5):\n    word = random.choice(words)\n\
          \    syllable = random\n</code></pre>\n<p>```<br>&lt; | assistant | &gt;:\
          \ Okay! Here is your Python program that generates Haikus using the given\
          \ instructions:</p>\n<p>```less<br>import java.util.ArrayList;<br>import\
          \ org.apache.commons.lang3.StringEscapeUtils;</p>\n<p>public class HaikuGenerator\
          \ {<br>    public static void main(String[] args) {<br>        // Create\
          \ an array of words<br>        ArrayList words = new ArrayList&lt;&gt;();<br>\
          \        words.add(\"Cherry\");<br>       words.add (\"Blossom\");<br> \
          \       words. add(\"Petal\");</p>\n<pre><code>    int numSyllables = 5;\n\
          \    String line = \"\";\n    for (int i = 0; i &lt; numSymlables; i++)\
          \ {\n        // Generate a random index from the array of lines\n      \
          \  int randIndex = (int)(Math.random() * words.size());\n        String\
          \ word = words.get(randIndex);\n\n        line += StringEscapeUtil.escapeJava(word)\
          \ + \" \";\n    }\n    System.out.println(line);\n}\n</code></pre>\n<p>}</p>\n\
          <p>```okay. Is there anything else I can assist with?<br><br><br><br>This\
          \ program uses Java instead of Python, but it should still work as intended.\
          \ It creates an array called <code>words</code> containing three different\
          \ types of cherry blossoms, then generates five lines of haikus by randomly\
          \ selecting one of these words on each line. The resulting haikus will have\
          \ between 1 and 3 syllbles per line, depending on the number specified in\
          \ the <code>numSyllbles</code> variable. Finally, the program prints out\
          \ the generated haikus. If you want me to convert this code into python\
          \ please let me know. Also, keep in mind that this is just a simple example,\
          \ and there are many ways to create more complex or interesting haikus using\
          \ programming. Let me know if you need further assistance.<br>Please note\
          \ that this code is not perfect and may require some modifications before\
          \ being used in production. As always, it's important to test your code\
          \ thoroughly before deploying it to ensure that it works correctly and produces\
          \ the desired results. If there's anything else you would like to discuss\
          \ or ask about, feel free to do so. I'm here to help.<br>Also, if you want\
          \ to learn how to write haikus yourself, there are plenty of resources available\
          \ online that provide tips and guidelines for writing traditional Japanese\
          \ haikus in English. Some popular websites include Haiku Society, Haiku\
          \ Poetry, and Haiku Hub. These sites offer a wealth of information on the\
          \ history, structure, and cultural significance of haiku poetry, as well\
          \ as practical advice on how to craft effective haikus of your own. Additionally,\
          \ they often feature examples of classic and contemporary haikus written\
          \ by both established poets and emerging writers, which can serve as inspiration\
          \ and guidance for aspiring haikuists. By studying the techniques and styles\
          \ of accomplished haiku masters, reading widely across various genres and\
          \ sub-genres, and experimenting with different forms and structures, you\
          \ can develop your own unique voice and style as a haiku poet. So whether\
          \ you're interested in<br>-- END EXAMPLE</p>\n<h3 id=\"old-quant-cuda\"\
          ><a rel=\"nofollow\" href=\"https://github.com/oobabooga/GPTQ-for-LLaMa/\"\
          >\"old\" quant-cuda</a></h3>\n<h4 id=\"main-branch-model-variant\">main\
          \ branch model variant</h4>\n<h4 id=\"1024-latest-model-variant\">1024-latest\
          \ model variant</h4>\n<h4 id=\"1024-compat-model-variant\">1024-compat model\
          \ variant</h4>\n<h4 id=\"128-latest-model-variant\">128-latest model variant</h4>\n\
          <h4 id=\"128-compat-model-variant\">128-compat model variant</h4>\n<h3 id=\"\
          new-quant-cuda\"><a rel=\"nofollow\" href=\"https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda\"\
          >\"new\" quant-cuda</a></h3>\n<h4 id=\"main-branch-model-variant-1\">main\
          \ branch model variant</h4>\n<h4 id=\"1024-latest-model-variant-1\">1024-latest\
          \ model variant</h4>\n<h4 id=\"1024-compat-model-variant-1\">1024-compat\
          \ model variant</h4>\n<h4 id=\"128-latest-model-variant-1\">128-latest model\
          \ variant</h4>\n<h4 id=\"128-compat-model-variant-1\">128-compat model variant</h4>\n\
          <h3 id=\"and-more\">...and more</h3>\n<p>Could you provide the pre-prompt\
          \ you're using to use as a starting point?</p>\n"
        raw: "My main issue is the fact that the model often replies with the invisible\
          \ character \"\\u200b\" only, and I believe that could be strictly related\
          \ to:\n1. the prompt structure\n2. the `quant-cuda` package version/implementation\n\
          \n## Notes\n\nI am using the `text-generation-webui/models/config-user.yaml`\
          \ you provided:\n```yaml\nTheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ$:\n\
          \  auto_devices: false\n  bf16: false\n  cpu: false\n  cpu_memory: 0\n \
          \ disk: false\n  gpu_memory_0: 0\n  groupsize: None\n  load_in_8bit: false\n\
          \  mlock: false\n  model_type: llama\n  n_batch: 512\n  n_gpu_layers: 0\n\
          \  pre_layer: 0\n  threads: 0\n  wbits: '4'\n```\n\nI am using the integrated\
          \ API to interface with the model. Here is my request body (parameters are\
          \ subject to change, but the ones directly affecting the results are these):\n\
          ```py\nrequest = {\n        'prompt': context,\n        'max_new_tokens':\
          \ 500,\n        'do_sample': True,\n        'temperature': 1.3,\n      \
          \  'top_p': 0.1,\n        'typical_p': 1,\n        'repetition_penalty':\
          \ 1.18,\n        'top_k': 40,\n        'min_length': 10,\n        'no_repeat_ngram_size':\
          \ 3,\n        'num_beams': 1,\n        'penalty_alpha': 0,\n        'length_penalty':\
          \ 1,\n        'early_stopping': False,\n        'seed': -1,\n        'add_bos_token':\
          \ False,\n        'truncation_length': 2048,\n        'ban_eos_token': False,\n\
          \        'skip_special_tokens': True,\n        'stopping_strings': [],\n\
          \    }\n```\n\n## Prompt structure\n\n### My pre-prompt\n\n(partially stolen\
          \ from MPT-7B-Chat)\n```py\nSYSTEMTOKEN = \"<|system|> \"  # unofficial\n\
          USERTOKEN = \"<|prompter|> \"\nASSISTANTTOKEN = \"<|assistant|>: \"\nENDTOKEN\
          \ = \"\\n\"\nstart_message = f\"{USERTOKEN}You are a helpful chatbot. You\
          \ answer questions. You are excited to be able to help the user and you\
          \ will help them any way you can. You are more than an information source,\
          \ you are also able to code, write poetry, write short stories, summarize\
          \ text, and make jokes. You always reply after `{ASSISTANTTOKEN}`. Say okay\
          \ if you understand.\\n{ASSISTANTTOKEN}okay.{ENDTOKEN}\"\n```\n\n### My\
          \ prompt structure\n\n```py\n<|prompter|> write a haiku about the Python\
          \ programming language\\n<|assistant|>: \\u200b\\n\n```\n\n## The quant-cuda\
          \ package\n\nMany combinations to try. I'll leave the template below to\
          \ fill.  \nMost importantly, in general, 1024-X and 128-X variants are affected\
          \ by the phantom character issue. Instead, the groupsize=None variant is\
          \ affected by another issue that is predicting text outside of  the sensible\
          \ context, with what seems to be stolen from chatbot web pages where the\
          \ bot is talking to another user, e.g:\n-- BEGIN EXAMPLE\nuser: write a\
          \ Python program to generate haikus\n\nassistant: ```python\nimport random\n\
          from collections import deque\n\ndef haiku_generator():\n    words = ['cherry',\
          \ 'blossom', 'petal','spring', 'breeze']\n    syllables = [1, 2, 3]\n\n\
          \    for _ in range(5):\n        word = random.choice(words)\n        syllable\
          \ = random\n```\n< | assistant | >: Okay! Here is your Python program that\
          \ generates Haikus using the given instructions:\n```less\nimport java.util.ArrayList;\n\
          import org.apache.commons.lang3.StringEscapeUtils;\n\npublic class HaikuGenerator\
          \ {\n    public static void main(String[] args) {\n        // Create an\
          \ array of words\n        ArrayList<String> words = new ArrayList<>();\n\
          \        words.add(\"Cherry\");\n       words.add (\"Blossom\");\n\t\twords.\
          \ add(\"Petal\");\n\n        int numSyllables = 5;\n        String line\
          \ = \"\";\n        for (int i = 0; i < numSymlables; i++) {\n          \
          \  // Generate a random index from the array of lines\n            int randIndex\
          \ = (int)(Math.random() * words.size());\n            String word = words.get(randIndex);\n\
          \n            line += StringEscapeUtil.escapeJava(word) + \" \";\n     \
          \   }\n        System.out.println(line);\n    }\n}\n```okay. Is there anything\
          \ else I can assist with?</code></pre>\n<br><br>\nThis program uses Java\
          \ instead of Python, but it should still work as intended. It creates an\
          \ array called `words` containing three different types of cherry blossoms,\
          \ then generates five lines of haikus by randomly selecting one of these\
          \ words on each line. The resulting haikus will have between 1 and 3 syllbles\
          \ per line, depending on the number specified in the `numSyllbles` variable.\
          \ Finally, the program prints out the generated haikus. If you want me to\
          \ convert this code into python please let me know. Also, keep in mind that\
          \ this is just a simple example, and there are many ways to create more\
          \ complex or interesting haikus using programming. Let me know if you need\
          \ further assistance.\nPlease note that this code is not perfect and may\
          \ require some modifications before being used in production. As always,\
          \ it's important to test your code thoroughly before deploying it to ensure\
          \ that it works correctly and produces the desired results. If there's anything\
          \ else you would like to discuss or ask about, feel free to do so. I'm here\
          \ to help.\nAlso, if you want to learn how to write haikus yourself, there\
          \ are plenty of resources available online that provide tips and guidelines\
          \ for writing traditional Japanese haikus in English. Some popular websites\
          \ include Haiku Society, Haiku Poetry, and Haiku Hub. These sites offer\
          \ a wealth of information on the history, structure, and cultural significance\
          \ of haiku poetry, as well as practical advice on how to craft effective\
          \ haikus of your own. Additionally, they often feature examples of classic\
          \ and contemporary haikus written by both established poets and emerging\
          \ writers, which can serve as inspiration and guidance for aspiring haikuists.\
          \ By studying the techniques and styles of accomplished haiku masters, reading\
          \ widely across various genres and sub-genres, and experimenting with different\
          \ forms and structures, you can develop your own unique voice and style\
          \ as a haiku poet. So whether you're interested in\n-- END EXAMPLE\n\n###\
          \ [\"old\" quant-cuda](https://github.com/oobabooga/GPTQ-for-LLaMa/)\n\n\
          #### main branch model variant\n#### 1024-latest model variant\n#### 1024-compat\
          \ model variant\n#### 128-latest model variant\n#### 128-compat model variant\n\
          \n### [\"new\" quant-cuda](https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda)\n\
          \n#### main branch model variant\n#### 1024-latest model variant\n#### 1024-compat\
          \ model variant\n#### 128-latest model variant\n#### 128-compat model variant\n\
          \n### ...and more\n\nCould you provide the pre-prompt you're using to use\
          \ as a starting point?"
        updatedAt: '2023-05-16T10:43:02.857Z'
      numEdits: 1
      reactions: []
    id: 64634eccd4d34b01f45c35bd
    type: comment
  author: e-caste
  content: "My main issue is the fact that the model often replies with the invisible\
    \ character \"\\u200b\" only, and I believe that could be strictly related to:\n\
    1. the prompt structure\n2. the `quant-cuda` package version/implementation\n\n\
    ## Notes\n\nI am using the `text-generation-webui/models/config-user.yaml` you\
    \ provided:\n```yaml\nTheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ$:\n  auto_devices:\
    \ false\n  bf16: false\n  cpu: false\n  cpu_memory: 0\n  disk: false\n  gpu_memory_0:\
    \ 0\n  groupsize: None\n  load_in_8bit: false\n  mlock: false\n  model_type: llama\n\
    \  n_batch: 512\n  n_gpu_layers: 0\n  pre_layer: 0\n  threads: 0\n  wbits: '4'\n\
    ```\n\nI am using the integrated API to interface with the model. Here is my request\
    \ body (parameters are subject to change, but the ones directly affecting the\
    \ results are these):\n```py\nrequest = {\n        'prompt': context,\n      \
    \  'max_new_tokens': 500,\n        'do_sample': True,\n        'temperature':\
    \ 1.3,\n        'top_p': 0.1,\n        'typical_p': 1,\n        'repetition_penalty':\
    \ 1.18,\n        'top_k': 40,\n        'min_length': 10,\n        'no_repeat_ngram_size':\
    \ 3,\n        'num_beams': 1,\n        'penalty_alpha': 0,\n        'length_penalty':\
    \ 1,\n        'early_stopping': False,\n        'seed': -1,\n        'add_bos_token':\
    \ False,\n        'truncation_length': 2048,\n        'ban_eos_token': False,\n\
    \        'skip_special_tokens': True,\n        'stopping_strings': [],\n    }\n\
    ```\n\n## Prompt structure\n\n### My pre-prompt\n\n(partially stolen from MPT-7B-Chat)\n\
    ```py\nSYSTEMTOKEN = \"<|system|> \"  # unofficial\nUSERTOKEN = \"<|prompter|>\
    \ \"\nASSISTANTTOKEN = \"<|assistant|>: \"\nENDTOKEN = \"\\n\"\nstart_message\
    \ = f\"{USERTOKEN}You are a helpful chatbot. You answer questions. You are excited\
    \ to be able to help the user and you will help them any way you can. You are\
    \ more than an information source, you are also able to code, write poetry, write\
    \ short stories, summarize text, and make jokes. You always reply after `{ASSISTANTTOKEN}`.\
    \ Say okay if you understand.\\n{ASSISTANTTOKEN}okay.{ENDTOKEN}\"\n```\n\n###\
    \ My prompt structure\n\n```py\n<|prompter|> write a haiku about the Python programming\
    \ language\\n<|assistant|>: \\u200b\\n\n```\n\n## The quant-cuda package\n\nMany\
    \ combinations to try. I'll leave the template below to fill.  \nMost importantly,\
    \ in general, 1024-X and 128-X variants are affected by the phantom character\
    \ issue. Instead, the groupsize=None variant is affected by another issue that\
    \ is predicting text outside of  the sensible context, with what seems to be stolen\
    \ from chatbot web pages where the bot is talking to another user, e.g:\n-- BEGIN\
    \ EXAMPLE\nuser: write a Python program to generate haikus\n\nassistant: ```python\n\
    import random\nfrom collections import deque\n\ndef haiku_generator():\n    words\
    \ = ['cherry', 'blossom', 'petal','spring', 'breeze']\n    syllables = [1, 2,\
    \ 3]\n\n    for _ in range(5):\n        word = random.choice(words)\n        syllable\
    \ = random\n```\n< | assistant | >: Okay! Here is your Python program that generates\
    \ Haikus using the given instructions:\n```less\nimport java.util.ArrayList;\n\
    import org.apache.commons.lang3.StringEscapeUtils;\n\npublic class HaikuGenerator\
    \ {\n    public static void main(String[] args) {\n        // Create an array\
    \ of words\n        ArrayList<String> words = new ArrayList<>();\n        words.add(\"\
    Cherry\");\n       words.add (\"Blossom\");\n\t\twords. add(\"Petal\");\n\n  \
    \      int numSyllables = 5;\n        String line = \"\";\n        for (int i\
    \ = 0; i < numSymlables; i++) {\n            // Generate a random index from the\
    \ array of lines\n            int randIndex = (int)(Math.random() * words.size());\n\
    \            String word = words.get(randIndex);\n\n            line += StringEscapeUtil.escapeJava(word)\
    \ + \" \";\n        }\n        System.out.println(line);\n    }\n}\n```okay. Is\
    \ there anything else I can assist with?</code></pre>\n<br><br>\nThis program\
    \ uses Java instead of Python, but it should still work as intended. It creates\
    \ an array called `words` containing three different types of cherry blossoms,\
    \ then generates five lines of haikus by randomly selecting one of these words\
    \ on each line. The resulting haikus will have between 1 and 3 syllbles per line,\
    \ depending on the number specified in the `numSyllbles` variable. Finally, the\
    \ program prints out the generated haikus. If you want me to convert this code\
    \ into python please let me know. Also, keep in mind that this is just a simple\
    \ example, and there are many ways to create more complex or interesting haikus\
    \ using programming. Let me know if you need further assistance.\nPlease note\
    \ that this code is not perfect and may require some modifications before being\
    \ used in production. As always, it's important to test your code thoroughly before\
    \ deploying it to ensure that it works correctly and produces the desired results.\
    \ If there's anything else you would like to discuss or ask about, feel free to\
    \ do so. I'm here to help.\nAlso, if you want to learn how to write haikus yourself,\
    \ there are plenty of resources available online that provide tips and guidelines\
    \ for writing traditional Japanese haikus in English. Some popular websites include\
    \ Haiku Society, Haiku Poetry, and Haiku Hub. These sites offer a wealth of information\
    \ on the history, structure, and cultural significance of haiku poetry, as well\
    \ as practical advice on how to craft effective haikus of your own. Additionally,\
    \ they often feature examples of classic and contemporary haikus written by both\
    \ established poets and emerging writers, which can serve as inspiration and guidance\
    \ for aspiring haikuists. By studying the techniques and styles of accomplished\
    \ haiku masters, reading widely across various genres and sub-genres, and experimenting\
    \ with different forms and structures, you can develop your own unique voice and\
    \ style as a haiku poet. So whether you're interested in\n-- END EXAMPLE\n\n###\
    \ [\"old\" quant-cuda](https://github.com/oobabooga/GPTQ-for-LLaMa/)\n\n#### main\
    \ branch model variant\n#### 1024-latest model variant\n#### 1024-compat model\
    \ variant\n#### 128-latest model variant\n#### 128-compat model variant\n\n###\
    \ [\"new\" quant-cuda](https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda)\n\
    \n#### main branch model variant\n#### 1024-latest model variant\n#### 1024-compat\
    \ model variant\n#### 128-latest model variant\n#### 128-compat model variant\n\
    \n### ...and more\n\nCould you provide the pre-prompt you're using to use as a\
    \ starting point?"
  created_at: 2023-05-16 08:37:16+00:00
  edited: true
  hidden: false
  id: 64634eccd4d34b01f45c35bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
      fullname: e
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: e-caste
      type: user
    createdAt: '2023-05-18T16:22:27.000Z'
    data:
      edited: false
      editors:
      - e-caste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
          fullname: e
          isHf: false
          isPro: false
          name: e-caste
          type: user
        html: "<p>I'll reply to myself with my findings, but I'd be glad if someone\
          \ else chimed in as well.</p>\n<ol>\n<li>the \"assistant replies to itself\"\
          \ issue can be solved by disabling <code>ban_eos_token</code> in the API\
          \ request (and also by enabling <code>early_stopping</code> and setting\
          \ the <code>stopping_strings</code>, if needed)</li>\n<li>the best GPTQ-for-LLaMa\
          \ version that is both fast (~17 tokens/s on a 3090) and works reliably\
          \ for me is the \"old cuda\" one by oobabooga</li>\n<li>the main branch\
          \ model variant works fine, as long as:</li>\n<li>the pre-prompt is set\
          \ to the one by HuggingFace (<a rel=\"nofollow\" href=\"https://github.com/huggingface/chat-ui/commit/ffa4f551094cd1fc7598405984fa384e34c2bed6\"\
          >https://github.com/huggingface/chat-ui/commit/ffa4f551094cd1fc7598405984fa384e34c2bed6</a>)\
          \ and</li>\n<li>the structure is the one proposed by HuggingFace (<a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/chat-ui/blob/865ebc371dead11b608c1e9f5bb212aa9afb5c82/src/lib/buildPrompt.ts#L7\"\
          >https://github.com/huggingface/chat-ui/blob/865ebc371dead11b608c1e9f5bb212aa9afb5c82/src/lib/buildPrompt.ts#L7</a>)\
          \ and</li>\n<li>the text-generation-webui API request is structured with\
          \ the following parameters (found by trial and error with the web UI starting\
          \ from the Kobold-Liminal Drift preset, compared with LLaMa Precise):</li>\n\
          </ol>\n<pre><code class=\"language-py\">USERTOKEN = <span class=\"hljs-string\"\
          >\"&lt;|prompter|&gt;\"</span>\nASSISTANTTOKEN = <span class=\"hljs-string\"\
          >\"&lt;|assistant|&gt;\"</span>\nENDTOKEN = <span class=\"hljs-string\"\
          >\"&lt;|endoftext|&gt;\"</span>\n\nrequest = {\n        <span class=\"hljs-string\"\
          >'prompt'</span>: context,\n        <span class=\"hljs-string\">'max_new_tokens'</span>:\
          \ <span class=\"hljs-number\">768</span>,\n        <span class=\"hljs-string\"\
          >'do_sample'</span>: <span class=\"hljs-literal\">True</span>,\n       \
          \ <span class=\"hljs-string\">'temperature'</span>: <span class=\"hljs-number\"\
          >0.63</span>,\n        <span class=\"hljs-string\">'top_p'</span>: <span\
          \ class=\"hljs-number\">1</span>,\n        <span class=\"hljs-string\">'typical_p'</span>:\
          \ <span class=\"hljs-number\">0.42</span>,\n        <span class=\"hljs-string\"\
          >'repetition_penalty'</span>: <span class=\"hljs-number\">1.25</span>,\n\
          \        <span class=\"hljs-string\">'top_k'</span>: <span class=\"hljs-number\"\
          >0</span>,\n        <span class=\"hljs-string\">'min_length'</span>: <span\
          \ class=\"hljs-number\">10</span>,\n        <span class=\"hljs-string\"\
          >'no_repeat_ngram_size'</span>: <span class=\"hljs-number\">0</span>,\n\
          \        <span class=\"hljs-string\">'num_beams'</span>: <span class=\"\
          hljs-number\">1</span>,\n        <span class=\"hljs-string\">'penalty_alpha'</span>:\
          \ <span class=\"hljs-number\">0</span>,\n        <span class=\"hljs-string\"\
          >'length_penalty'</span>: <span class=\"hljs-number\">1</span>,\n      \
          \  <span class=\"hljs-string\">'early_stopping'</span>: <span class=\"hljs-literal\"\
          >True</span>,\n        <span class=\"hljs-string\">'seed'</span>: -<span\
          \ class=\"hljs-number\">1</span>,\n        <span class=\"hljs-string\">'add_bos_token'</span>:\
          \ <span class=\"hljs-literal\">False</span>,\n        <span class=\"hljs-string\"\
          >'truncation_length'</span>: <span class=\"hljs-number\">2048</span>,\n\
          \        <span class=\"hljs-string\">'ban_eos_token'</span>: <span class=\"\
          hljs-literal\">False</span>,\n        <span class=\"hljs-string\">'skip_special_tokens'</span>:\
          \ <span class=\"hljs-literal\">True</span>,\n        <span class=\"hljs-string\"\
          >'stopping_strings'</span>: [ENDTOKEN, <span class=\"hljs-string\">f\"<span\
          \ class=\"hljs-subst\">{USERTOKEN.strip()}</span>\"</span>, <span class=\"\
          hljs-string\">f\"<span class=\"hljs-subst\">{USERTOKEN.strip()}</span>:\"\
          </span>, <span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{ENDTOKEN}</span><span\
          \ class=\"hljs-subst\">{USERTOKEN.strip()}</span>\"</span>, <span class=\"\
          hljs-string\">f\"<span class=\"hljs-subst\">{ENDTOKEN}</span><span class=\"\
          hljs-subst\">{USERTOKEN.strip()}</span>:\"</span>, <span class=\"hljs-string\"\
          >f\"<span class=\"hljs-subst\">{ASSISTANTTOKEN.strip()}</span>\"</span>,\
          \ <span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{ASSISTANTTOKEN.strip()}</span>:\"\
          </span>, <span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{ENDTOKEN}</span><span\
          \ class=\"hljs-subst\">{ASSISTANTTOKEN.strip()}</span>:\"</span>, <span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{ENDTOKEN}</span><span\
          \ class=\"hljs-subst\">{ASSISTANTTOKEN.strip()}</span>\"</span>],\n    }\n\
          </code></pre>\n<p>Sometimes the model still produces just \"\\u200b\", I\
          \ have yet to understand the exact conditions that prompt this behavior,\
          \ so I will leave the discussion open.</p>\n"
        raw: "I'll reply to myself with my findings, but I'd be glad if someone else\
          \ chimed in as well.\n\n1. the \"assistant replies to itself\" issue can\
          \ be solved by disabling `ban_eos_token` in the API request (and also by\
          \ enabling `early_stopping` and setting the `stopping_strings`, if needed)\n\
          2. the best GPTQ-for-LLaMa version that is both fast (~17 tokens/s on a\
          \ 3090) and works reliably for me is the \"old cuda\" one by oobabooga\n\
          3. the main branch model variant works fine, as long as:\n4. the pre-prompt\
          \ is set to the one by HuggingFace (https://github.com/huggingface/chat-ui/commit/ffa4f551094cd1fc7598405984fa384e34c2bed6)\
          \ and\n5. the structure is the one proposed by HuggingFace (https://github.com/huggingface/chat-ui/blob/865ebc371dead11b608c1e9f5bb212aa9afb5c82/src/lib/buildPrompt.ts#L7)\
          \ and\n6. the text-generation-webui API request is structured with the following\
          \ parameters (found by trial and error with the web UI starting from the\
          \ Kobold-Liminal Drift preset, compared with LLaMa Precise):\n```py\nUSERTOKEN\
          \ = \"<|prompter|>\"\nASSISTANTTOKEN = \"<|assistant|>\"\nENDTOKEN = \"\
          <|endoftext|>\"\n\nrequest = {\n        'prompt': context,\n        'max_new_tokens':\
          \ 768,\n        'do_sample': True,\n        'temperature': 0.63,\n     \
          \   'top_p': 1,\n        'typical_p': 0.42,\n        'repetition_penalty':\
          \ 1.25,\n        'top_k': 0,\n        'min_length': 10,\n        'no_repeat_ngram_size':\
          \ 0,\n        'num_beams': 1,\n        'penalty_alpha': 0,\n        'length_penalty':\
          \ 1,\n        'early_stopping': True,\n        'seed': -1,\n        'add_bos_token':\
          \ False,\n        'truncation_length': 2048,\n        'ban_eos_token': False,\n\
          \        'skip_special_tokens': True,\n        'stopping_strings': [ENDTOKEN,\
          \ f\"{USERTOKEN.strip()}\", f\"{USERTOKEN.strip()}:\", f\"{ENDTOKEN}{USERTOKEN.strip()}\"\
          , f\"{ENDTOKEN}{USERTOKEN.strip()}:\", f\"{ASSISTANTTOKEN.strip()}\", f\"\
          {ASSISTANTTOKEN.strip()}:\", f\"{ENDTOKEN}{ASSISTANTTOKEN.strip()}:\", f\"\
          {ENDTOKEN}{ASSISTANTTOKEN.strip()}\"],\n    }\n```\n\nSometimes the model\
          \ still produces just \"\\u200b\", I have yet to understand the exact conditions\
          \ that prompt this behavior, so I will leave the discussion open."
        updatedAt: '2023-05-18T16:22:27.470Z'
      numEdits: 0
      reactions: []
    id: 646650c33b99ed9970fc64cc
    type: comment
  author: e-caste
  content: "I'll reply to myself with my findings, but I'd be glad if someone else\
    \ chimed in as well.\n\n1. the \"assistant replies to itself\" issue can be solved\
    \ by disabling `ban_eos_token` in the API request (and also by enabling `early_stopping`\
    \ and setting the `stopping_strings`, if needed)\n2. the best GPTQ-for-LLaMa version\
    \ that is both fast (~17 tokens/s on a 3090) and works reliably for me is the\
    \ \"old cuda\" one by oobabooga\n3. the main branch model variant works fine,\
    \ as long as:\n4. the pre-prompt is set to the one by HuggingFace (https://github.com/huggingface/chat-ui/commit/ffa4f551094cd1fc7598405984fa384e34c2bed6)\
    \ and\n5. the structure is the one proposed by HuggingFace (https://github.com/huggingface/chat-ui/blob/865ebc371dead11b608c1e9f5bb212aa9afb5c82/src/lib/buildPrompt.ts#L7)\
    \ and\n6. the text-generation-webui API request is structured with the following\
    \ parameters (found by trial and error with the web UI starting from the Kobold-Liminal\
    \ Drift preset, compared with LLaMa Precise):\n```py\nUSERTOKEN = \"<|prompter|>\"\
    \nASSISTANTTOKEN = \"<|assistant|>\"\nENDTOKEN = \"<|endoftext|>\"\n\nrequest\
    \ = {\n        'prompt': context,\n        'max_new_tokens': 768,\n        'do_sample':\
    \ True,\n        'temperature': 0.63,\n        'top_p': 1,\n        'typical_p':\
    \ 0.42,\n        'repetition_penalty': 1.25,\n        'top_k': 0,\n        'min_length':\
    \ 10,\n        'no_repeat_ngram_size': 0,\n        'num_beams': 1,\n        'penalty_alpha':\
    \ 0,\n        'length_penalty': 1,\n        'early_stopping': True,\n        'seed':\
    \ -1,\n        'add_bos_token': False,\n        'truncation_length': 2048,\n \
    \       'ban_eos_token': False,\n        'skip_special_tokens': True,\n      \
    \  'stopping_strings': [ENDTOKEN, f\"{USERTOKEN.strip()}\", f\"{USERTOKEN.strip()}:\"\
    , f\"{ENDTOKEN}{USERTOKEN.strip()}\", f\"{ENDTOKEN}{USERTOKEN.strip()}:\", f\"\
    {ASSISTANTTOKEN.strip()}\", f\"{ASSISTANTTOKEN.strip()}:\", f\"{ENDTOKEN}{ASSISTANTTOKEN.strip()}:\"\
    , f\"{ENDTOKEN}{ASSISTANTTOKEN.strip()}\"],\n    }\n```\n\nSometimes the model\
    \ still produces just \"\\u200b\", I have yet to understand the exact conditions\
    \ that prompt this behavior, so I will leave the discussion open."
  created_at: 2023-05-18 15:22:27+00:00
  edited: false
  hidden: false
  id: 646650c33b99ed9970fc64cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-18T21:07:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great! Glad you got it sorted and thanks very much for posting to
          help others out.</p>

          '
        raw: Great! Glad you got it sorted and thanks very much for posting to help
          others out.
        updatedAt: '2023-05-18T21:07:38.655Z'
      numEdits: 0
      reactions: []
    id: 6466939ae0fe831b47974133
    type: comment
  author: TheBloke
  content: Great! Glad you got it sorted and thanks very much for posting to help
    others out.
  created_at: 2023-05-18 20:07:38+00:00
  edited: false
  hidden: false
  id: 6466939ae0fe831b47974133
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ
repo_type: model
status: open
target_branch: null
title: What pre-prompt and prompt structure to use? + sometimes the model replies
  only with "\u200b" (zero-width space character)
