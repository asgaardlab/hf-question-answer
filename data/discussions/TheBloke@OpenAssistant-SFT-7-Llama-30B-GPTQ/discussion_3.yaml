!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pevogam
conflicting_files: null
created_at: 2023-05-04 15:16:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-05-04T16:16:12.000Z'
    data:
      edited: false
      editors:
      - pevogam
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
          fullname: Plamen Dimitrov
          isHf: false
          isPro: false
          name: pevogam
          type: user
        html: "<p>I tried this</p>\n<pre><code>#:/mnt/local/notebooks/text-generation-webui&gt;\
          \ python3.9 download-model.py TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \ --branch=128-compat\n\nDownloading the model to models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ_128-compat\n\
          ...\n</code></pre>\n<p>and it leads to a lot of errors (and ultimate nonzero\
          \ exit) like</p>\n<pre><code>text-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.55.mlp.down_proj.qzeros: copying a\
          \ param with shape torch.Size([140, 832]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 832]).\n</code></pre>\n<p>all of which\
          \ follow</p>\n<pre><code>text-generation-webui-text-generation-webui-1 \
          \ | Found the following quantized model: models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ_128-compat/OpenAssistant-30B-epoch7-GPTQ-4bit-128g.compat.no-act-order.safetensors\n\
          text-generation-webui-text-generation-webui-1  | Loading model ...\ntext-generation-webui-text-generation-webui-1\
          \  | Traceback (most recent call last):\ntext-generation-webui-text-generation-webui-1\
          \  |   File \"/app/server.py\", line 918, in &lt;module&gt;\ntext-generation-webui-text-generation-webui-1\
          \  |     shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          text-generation-webui-text-generation-webui-1  |   File \"/app/modules/models.py\"\
          , line 150, in load_model\ntext-generation-webui-text-generation-webui-1\
          \  |     model = load_quantized(model_name)\ntext-generation-webui-text-generation-webui-1\
          \  |   File \"/app/modules/GPTQ_loader.py\", line 176, in load_quantized\n\
          text-generation-webui-text-generation-webui-1  |     model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          text-generation-webui-text-generation-webui-1  |   File \"/app/modules/GPTQ_loader.py\"\
          , line 77, in _load_quant\ntext-generation-webui-text-generation-webui-1\
          \  |     model.load_state_dict(safe_load(checkpoint), strict=False)\ntext-generation-webui-text-generation-webui-1\
          \  |   File \"/app/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\ntext-generation-webui-text-generation-webui-1\
          \  |     raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\\
          t{}'.format(\ntext-generation-webui-text-generation-webui-1  | RuntimeError:\
          \ Error(s) in loading state_dict for LlamaForCausalLM:\n</code></pre>\n\
          <p>Am I doing something wrong in the way I download the model or the choice\
          \ of branch?</p>\n"
        raw: "I tried this\r\n```\r\n#:/mnt/local/notebooks/text-generation-webui>\
          \ python3.9 download-model.py TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \ --branch=128-compat\r\n\r\nDownloading the model to models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ_128-compat\r\
          \n...\r\n```\r\nand it leads to a lot of errors (and ultimate nonzero exit)\
          \ like\r\n```\r\ntext-generation-webui-text-generation-webui-1  | \tsize\
          \ mismatch for model.layers.55.mlp.down_proj.qzeros: copying a param with\
          \ shape torch.Size([140, 832]) from checkpoint, the shape in current model\
          \ is torch.Size([1, 832]).\r\n```\r\nall of which follow\r\n```\r\ntext-generation-webui-text-generation-webui-1\
          \  | Found the following quantized model: models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ_128-compat/OpenAssistant-30B-epoch7-GPTQ-4bit-128g.compat.no-act-order.safetensors\r\
          \ntext-generation-webui-text-generation-webui-1  | Loading model ...\r\n\
          text-generation-webui-text-generation-webui-1  | Traceback (most recent\
          \ call last):\r\ntext-generation-webui-text-generation-webui-1  |   File\
          \ \"/app/server.py\", line 918, in <module>\r\ntext-generation-webui-text-generation-webui-1\
          \  |     shared.model, shared.tokenizer = load_model(shared.model_name)\r\
          \ntext-generation-webui-text-generation-webui-1  |   File \"/app/modules/models.py\"\
          , line 150, in load_model\r\ntext-generation-webui-text-generation-webui-1\
          \  |     model = load_quantized(model_name)\r\ntext-generation-webui-text-generation-webui-1\
          \  |   File \"/app/modules/GPTQ_loader.py\", line 176, in load_quantized\r\
          \ntext-generation-webui-text-generation-webui-1  |     model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\r\
          \ntext-generation-webui-text-generation-webui-1  |   File \"/app/modules/GPTQ_loader.py\"\
          , line 77, in _load_quant\r\ntext-generation-webui-text-generation-webui-1\
          \  |     model.load_state_dict(safe_load(checkpoint), strict=False)\r\n\
          text-generation-webui-text-generation-webui-1  |   File \"/app/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\r\ntext-generation-webui-text-generation-webui-1\
          \  |     raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\\
          t{}'.format(\r\ntext-generation-webui-text-generation-webui-1  | RuntimeError:\
          \ Error(s) in loading state_dict for LlamaForCausalLM:\r\n```\r\nAm I doing\
          \ something wrong in the way I download the model or the choice of branch?"
        updatedAt: '2023-05-04T16:16:12.203Z'
      numEdits: 0
      reactions: []
    id: 6453da4c096d57ae122cd5ea
    type: comment
  author: pevogam
  content: "I tried this\r\n```\r\n#:/mnt/local/notebooks/text-generation-webui> python3.9\
    \ download-model.py TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ --branch=128-compat\r\
    \n\r\nDownloading the model to models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ_128-compat\r\
    \n...\r\n```\r\nand it leads to a lot of errors (and ultimate nonzero exit) like\r\
    \n```\r\ntext-generation-webui-text-generation-webui-1  | \tsize mismatch for\
    \ model.layers.55.mlp.down_proj.qzeros: copying a param with shape torch.Size([140,\
    \ 832]) from checkpoint, the shape in current model is torch.Size([1, 832]).\r\
    \n```\r\nall of which follow\r\n```\r\ntext-generation-webui-text-generation-webui-1\
    \  | Found the following quantized model: models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ_128-compat/OpenAssistant-30B-epoch7-GPTQ-4bit-128g.compat.no-act-order.safetensors\r\
    \ntext-generation-webui-text-generation-webui-1  | Loading model ...\r\ntext-generation-webui-text-generation-webui-1\
    \  | Traceback (most recent call last):\r\ntext-generation-webui-text-generation-webui-1\
    \  |   File \"/app/server.py\", line 918, in <module>\r\ntext-generation-webui-text-generation-webui-1\
    \  |     shared.model, shared.tokenizer = load_model(shared.model_name)\r\ntext-generation-webui-text-generation-webui-1\
    \  |   File \"/app/modules/models.py\", line 150, in load_model\r\ntext-generation-webui-text-generation-webui-1\
    \  |     model = load_quantized(model_name)\r\ntext-generation-webui-text-generation-webui-1\
    \  |   File \"/app/modules/GPTQ_loader.py\", line 176, in load_quantized\r\ntext-generation-webui-text-generation-webui-1\
    \  |     model = load_quant(str(path_to_model), str(pt_path), shared.args.wbits,\
    \ shared.args.groupsize, kernel_switch_threshold=threshold)\r\ntext-generation-webui-text-generation-webui-1\
    \  |   File \"/app/modules/GPTQ_loader.py\", line 77, in _load_quant\r\ntext-generation-webui-text-generation-webui-1\
    \  |     model.load_state_dict(safe_load(checkpoint), strict=False)\r\ntext-generation-webui-text-generation-webui-1\
    \  |   File \"/app/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 2041, in load_state_dict\r\ntext-generation-webui-text-generation-webui-1\
    \  |     raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\
    \ntext-generation-webui-text-generation-webui-1  | RuntimeError: Error(s) in loading\
    \ state_dict for LlamaForCausalLM:\r\n```\r\nAm I doing something wrong in the\
    \ way I download the model or the choice of branch?"
  created_at: 2023-05-04 15:16:12+00:00
  edited: false
  hidden: false
  id: 6453da4c096d57ae122cd5ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641aee24ec5b871c0bcb8322/nhfgY2ncnIh9RiqdCA5nO.png?w=200&h=200&f=face
      fullname: Electro Fried
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Electrofried
      type: user
    createdAt: '2023-05-05T00:41:58.000Z'
    data:
      edited: false
      editors:
      - Electrofried
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641aee24ec5b871c0bcb8322/nhfgY2ncnIh9RiqdCA5nO.png?w=200&h=200&f=face
          fullname: Electro Fried
          isHf: false
          isPro: false
          name: Electrofried
          type: user
        html: '<p>Getting a similar but slightly different error for the 1028 branch:</p>

          <p>size mismatch for model.layers.59.mlp.up_proj.scales: copying a param
          with shape torch.Size([7, 17920]) from checkpoint, the shape in current
          model is torch.Size([52, 17920]).<br>text-generation-webui-text-generation-webui-1
          exited with code 1</p>

          '
        raw: 'Getting a similar but slightly different error for the 1028 branch:


          size mismatch for model.layers.59.mlp.up_proj.scales: copying a param with
          shape torch.Size([7, 17920]) from checkpoint, the shape in current model
          is torch.Size([52, 17920]).

          text-generation-webui-text-generation-webui-1 exited with code 1'
        updatedAt: '2023-05-05T00:41:58.365Z'
      numEdits: 0
      reactions: []
    id: 645450d6ce1cc3ed5fe73fd0
    type: comment
  author: Electrofried
  content: 'Getting a similar but slightly different error for the 1028 branch:


    size mismatch for model.layers.59.mlp.up_proj.scales: copying a param with shape
    torch.Size([7, 17920]) from checkpoint, the shape in current model is torch.Size([52,
    17920]).

    text-generation-webui-text-generation-webui-1 exited with code 1'
  created_at: 2023-05-04 23:41:58+00:00
  edited: false
  hidden: false
  id: 645450d6ce1cc3ed5fe73fd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T10:20:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK I will run some tests and let you know</p>

          '
        raw: OK I will run some tests and let you know
        updatedAt: '2023-05-05T10:20:22.603Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Electrofried
        - pevogam
    id: 6454d866d55525a4fee11ee2
    type: comment
  author: TheBloke
  content: OK I will run some tests and let you know
  created_at: 2023-05-05 09:20:22+00:00
  edited: false
  hidden: false
  id: 6454d866d55525a4fee11ee2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
      fullname: e
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: e-caste
      type: user
    createdAt: '2023-05-15T16:30:58.000Z'
    data:
      edited: false
      editors:
      - e-caste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
          fullname: e
          isHf: false
          isPro: false
          name: e-caste
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> hi, thank you\
          \ for quantizing these model checkpoints! I also have a size mismatch error\
          \ when trying to instance the latest model, with no group size. I can launch\
          \ the server:</p>\n<ul>\n<li>either being very specific: <code>python server.py\
          \ --gpu-memory 22500MiB --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \ --groupsize -1 --wbits 4 --model_type llama --api --listen --listen-host=X.Y.Z.T</code></li>\n\
          <li>or being quite generic: <code>python server.py --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \ --api --listen --listen-host=X.Y.Z.T</code></li>\n</ul>\n<p>I have an\
          \ RTX 3090 and 48 GB of RAM, I have installed the recommended GPTQ-for-LLaMa\
          \ (<a rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md\"\
          >from the docs here</a>), but I have also experimented with the other variants.<br>What\
          \ command line arguments or tricks are you using to make it work?</p>\n\
          <p>Here are the error logs:</p>\n<pre><code>INFO:Loading TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ...\n\
          INFO:Found the following quantized model: models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors\n\
          Traceback (most recent call last):\n  File \"/home/XXX/text-generation-webui/server.py\"\
          , line 952, in &lt;module&gt;\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"/home/XXX/text-generation-webui/modules/models.py\", line 159,\
          \ in load_model\n    model = load_quantized(model_name)\n  File \"/home/XXX/text-generation-webui/modules/GPTQ_loader.py\"\
          , line 178, in load_quantized\n    model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          \  File \"/home/XXX/text-generation-webui/modules/GPTQ_loader.py\", line\
          \ 84, in _load_quant\n    model.load_state_dict(safe_load(checkpoint), strict=False)\n\
          \  File \"/home/XXX/text-generation-webui/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM:\n    size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.0.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.0.self_attn.o_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.0.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.0.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.0.self_attn.v_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.0.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.0.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.0.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.0.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.0.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.0.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.1.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.1.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.1.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.1.self_attn.o_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.1.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.1.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.1.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.1.self_attn.v_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.1.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.1.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.1.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.1.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.1.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.1.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.2.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.2.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.2.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.2.self_attn.o_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.2.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.2.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.2.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.2.self_attn.v_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.2.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.2.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.2.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.2.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.2.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.2.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.3.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.3.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.3.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.3.self_attn.o_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.3.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.3.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.3.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.3.self_attn.v_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.3.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.3.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.3.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.3.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.3.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.3.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.4.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.4.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.4.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.4.self_attn.o_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.4.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.4.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.4.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.4.self_attn.v_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.4.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.4.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.4.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.4.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.4.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.4.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.5.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.5.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.5.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.5.self_attn.o_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.5.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.5.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.5.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.5.self_attn.v_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.5.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.5.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.5.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.5.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.5.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.5.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.6.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.6.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.6.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.6.self_attn.o_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.6.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.6.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.6.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.6.self_attn.v_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.6.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.6.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.6.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.6.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.6.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.6.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.7.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.7.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.7.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.7.self_attn.o_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.7.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.7.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.7.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.7.self_attn.v_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.7.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.7.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.7.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.7.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.7.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.7.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.8.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.8.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.8.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.8.self_attn.o_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.8.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.8.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.8.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.8.self_attn.v_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.8.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.8.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.8.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.8.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.8.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.8.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.9.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.9.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.9.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.9.self_attn.o_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.9.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.9.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.9.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.9.self_attn.v_proj.scales: copying a\
          \ param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n    size mismatch for model.layers.9.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.9.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.9.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.9.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.9.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.9.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.10.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.10.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.10.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.10.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.10.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.10.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.10.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.10.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.10.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.10.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.10.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.10.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.10.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.10.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.11.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.11.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.11.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.11.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.11.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.11.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.11.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.11.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.11.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.11.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.11.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.11.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.11.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.11.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.12.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.12.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.12.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.12.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.12.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.12.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.12.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.12.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.12.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.12.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.12.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.12.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.12.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.12.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.13.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.13.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.13.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.13.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.13.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.13.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.13.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.13.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.13.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.13.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.13.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.13.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.13.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.13.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.14.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.14.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.14.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.14.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.14.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.14.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.14.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.14.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.14.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.14.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.14.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.14.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.14.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.14.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.15.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.15.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.15.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.15.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.15.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.15.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.15.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.15.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.15.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.15.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.15.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.15.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.15.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.15.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.16.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.16.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.16.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.16.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.16.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.16.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.16.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.16.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.16.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.16.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.16.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.16.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.16.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.16.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.17.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.17.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.17.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.17.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.17.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.17.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.17.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.17.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.17.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.17.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.17.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.17.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.17.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.17.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.18.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.18.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.18.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.18.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.18.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.18.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.18.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.18.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.18.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.18.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.18.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.18.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.18.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.18.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.19.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.19.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.19.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.19.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.19.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.19.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.19.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.19.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.19.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.19.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.19.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.19.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.19.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.19.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.20.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.20.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.20.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.20.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.20.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.20.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.20.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.20.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.20.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.20.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.20.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.20.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.20.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.20.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.21.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.21.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.21.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.21.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.21.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.21.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.21.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.21.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.21.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.21.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.21.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.21.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.21.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.21.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.22.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.22.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.22.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.22.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.22.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.22.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.22.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.22.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.22.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.22.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.22.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.22.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.22.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.22.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.23.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.23.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.23.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.23.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.23.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.23.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.23.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.23.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.23.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.23.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.23.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.23.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.23.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.23.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.24.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.24.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.24.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.24.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.24.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.24.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.24.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.24.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.24.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.24.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.24.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.24.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.24.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.24.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.25.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.25.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.25.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.25.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.25.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.25.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.25.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.25.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.25.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.25.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.25.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.25.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.25.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.25.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.26.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.26.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.26.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.26.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.26.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.26.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.26.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.26.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.26.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.26.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.26.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.26.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.26.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.26.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.27.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.27.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.27.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.27.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.27.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.27.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.27.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.27.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.27.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.27.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.27.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.27.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.27.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.27.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.28.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.28.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.28.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.28.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.28.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.28.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.28.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.28.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.28.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.28.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.28.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.28.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.28.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.28.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.29.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.29.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.29.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.29.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.29.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.29.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.29.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.29.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.29.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.29.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.29.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.29.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.29.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.29.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.30.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.30.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.30.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.30.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.30.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.30.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.30.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.30.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.30.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.30.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.30.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.30.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.30.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.30.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.31.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.31.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.31.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.31.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.31.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.31.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.31.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.31.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.31.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.31.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.31.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.31.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.31.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.31.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.32.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.32.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.32.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.32.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.32.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.32.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.32.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.32.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.32.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.32.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.32.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.32.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.32.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.32.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.33.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.33.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.33.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.33.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.33.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.33.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.33.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.33.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.33.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.33.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.33.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.33.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.33.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.33.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.34.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.34.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.34.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.34.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.34.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.34.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.34.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.34.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.34.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.34.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.34.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.34.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.34.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.34.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.35.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.35.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.35.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.35.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.35.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.35.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.35.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.35.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.35.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.35.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.35.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.35.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.35.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.35.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.36.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.36.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.36.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.36.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.36.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.36.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.36.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.36.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.36.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.36.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.36.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.36.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.36.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.36.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.37.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.37.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.37.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.37.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.37.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.37.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.37.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.37.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.37.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.37.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.37.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.37.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.37.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.37.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.38.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.38.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.38.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.38.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.38.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.38.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.38.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.38.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.38.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.38.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.38.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.38.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.38.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.38.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.39.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.39.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.39.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.39.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.39.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.39.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.39.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.39.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.39.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.39.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.39.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.39.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.39.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.39.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.40.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.40.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.40.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.40.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.40.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.40.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.40.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.40.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.40.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.40.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.40.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.40.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.40.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.40.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.41.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.41.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.41.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.41.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.41.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.41.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.41.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.41.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.41.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.41.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.41.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.41.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.41.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.41.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.42.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.42.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.42.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.42.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.42.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.42.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.42.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.42.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.42.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.42.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.42.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.42.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.42.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.42.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.43.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.43.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.43.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.43.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.43.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.43.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.43.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.43.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.43.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.43.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.43.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.43.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.43.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.43.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.44.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.44.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.44.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.44.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.44.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.44.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.44.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.44.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.44.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.44.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.44.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.44.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.44.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.44.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.45.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.45.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.45.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.45.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.45.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.45.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.45.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.45.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.45.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.45.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.45.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.45.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.45.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.45.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.46.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.46.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.46.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.46.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.46.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.46.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.46.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.46.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.46.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.46.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.46.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.46.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.46.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.46.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.47.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.47.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.47.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.47.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.47.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.47.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.47.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.47.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.47.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.47.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.47.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.47.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.47.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.47.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.48.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.48.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.48.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.48.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.48.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.48.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.48.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.48.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.48.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.48.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.48.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.48.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.48.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.48.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.49.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.49.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.49.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.49.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.49.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.49.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.49.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.49.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.49.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.49.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.49.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.49.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.49.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.49.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.50.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.50.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.50.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.50.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.50.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.50.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.50.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.50.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.50.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.50.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.50.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.50.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.50.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.50.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.51.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.51.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.51.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.51.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.51.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.51.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.51.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.51.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.51.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.51.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.51.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.51.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.51.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.51.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.52.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.52.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.52.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.52.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.52.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.52.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.52.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.52.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.52.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.52.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.52.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.52.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.52.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.52.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.53.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.53.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.53.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.53.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.53.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.53.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.53.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.53.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.53.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.53.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.53.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.53.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.53.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.53.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.54.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.54.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.54.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.54.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.54.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.54.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.54.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.54.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.54.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.54.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.54.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.54.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.54.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.54.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.55.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.55.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.55.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.55.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.55.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.55.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.55.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.55.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.55.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.55.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.55.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.55.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.55.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.55.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.56.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.56.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.56.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.56.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.56.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.56.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.56.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.56.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.56.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.56.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.56.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.56.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.56.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.56.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.57.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.57.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.57.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.57.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.57.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.57.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.57.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.57.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.57.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.57.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.57.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.57.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.57.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.57.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.58.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.58.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.58.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.58.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.58.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.58.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.58.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.58.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.58.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.58.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.58.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.58.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.58.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.58.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n    size mismatch for model.layers.59.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.59.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.59.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.59.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.59.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n    size mismatch for model.layers.59.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n    size mismatch for\
          \ model.layers.59.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \    size mismatch for model.layers.59.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 6656]).\n    size mismatch for model.layers.59.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n    size mismatch for model.layers.59.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n    size mismatch\
          \ for model.layers.59.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n    size mismatch for model.layers.59.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n    size mismatch for model.layers.59.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n    size mismatch for\
          \ model.layers.59.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n</code></pre>\n"
        raw: "@TheBloke hi, thank you for quantizing these model checkpoints! I also\
          \ have a size mismatch error when trying to instance the latest model, with\
          \ no group size. I can launch the server:\n- either being very specific:\
          \ `python server.py --gpu-memory 22500MiB --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \ --groupsize -1 --wbits 4 --model_type llama --api --listen --listen-host=X.Y.Z.T`\n\
          - or being quite generic: `python server.py --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \ --api --listen --listen-host=X.Y.Z.T`\n\nI have an RTX 3090 and 48 GB\
          \ of RAM, I have installed the recommended GPTQ-for-LLaMa ([from the docs\
          \ here](https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md)),\
          \ but I have also experimented with the other variants.  \nWhat command\
          \ line arguments or tricks are you using to make it work?\n\nHere are the\
          \ error logs:\n```\nINFO:Loading TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ...\n\
          INFO:Found the following quantized model: models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors\n\
          Traceback (most recent call last):\n  File \"/home/XXX/text-generation-webui/server.py\"\
          , line 952, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"/home/XXX/text-generation-webui/modules/models.py\", line 159,\
          \ in load_model\n    model = load_quantized(model_name)\n  File \"/home/XXX/text-generation-webui/modules/GPTQ_loader.py\"\
          , line 178, in load_quantized\n    model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          \  File \"/home/XXX/text-generation-webui/modules/GPTQ_loader.py\", line\
          \ 84, in _load_quant\n    model.load_state_dict(safe_load(checkpoint), strict=False)\n\
          \  File \"/home/XXX/text-generation-webui/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM:\n\tsize mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.0.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.0.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.0.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.0.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.0.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.0.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.0.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.0.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.0.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.0.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.0.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.1.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.1.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.1.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.1.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.1.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.1.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.1.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.1.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.1.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.1.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.2.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.2.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.2.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.2.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.2.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.2.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.2.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.2.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.2.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.2.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.2.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.2.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.3.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.3.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.3.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.3.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.3.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.3.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.3.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.3.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.3.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.3.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.3.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.3.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.4.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.4.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.4.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.4.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.4.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.4.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.4.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.4.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.4.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.4.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.4.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.4.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.5.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.5.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.5.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.5.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.5.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.5.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.5.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.5.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.5.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.5.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.5.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.5.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.6.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.6.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.6.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.6.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.6.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.6.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.6.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.6.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.6.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.6.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.6.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.6.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.7.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.7.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.7.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.7.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.7.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.7.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.7.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.7.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.7.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.7.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.7.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.7.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.8.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.8.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.8.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.8.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.8.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.8.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.8.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.8.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.8.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.8.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.8.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.8.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.9.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.9.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.9.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.9.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.9.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.9.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.9.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.9.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.9.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.9.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.9.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.9.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.10.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.10.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.10.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.10.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.10.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.10.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.10.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.10.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.10.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.10.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.11.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.11.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.11.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.11.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.11.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.11.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.11.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.11.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.11.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.11.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.12.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.12.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.12.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.12.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.12.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.12.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.12.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.12.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.12.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.12.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.13.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.13.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.13.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.13.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.13.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.13.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.13.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.13.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.13.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.13.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.14.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.14.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.14.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.14.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.14.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.14.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.14.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.14.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.14.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.14.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.15.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.15.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.15.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.15.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.15.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.15.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.15.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.15.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.15.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.15.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.16.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.16.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.16.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.16.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.16.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.16.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.16.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.16.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.16.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.16.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.17.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.17.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.17.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.17.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.17.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.17.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.17.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.17.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.17.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.17.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.18.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.18.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.18.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.18.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.18.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.18.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.18.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.18.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.18.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.18.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.19.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.19.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.19.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.19.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.19.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.19.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.19.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.19.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.19.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.19.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.20.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.20.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.20.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.20.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.20.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.20.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.20.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.20.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.20.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.20.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.20.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.20.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.21.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.21.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.21.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.21.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.21.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.21.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.21.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.21.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.21.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.21.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.21.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.21.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.22.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.22.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.22.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.22.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.22.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.22.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.22.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.22.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.22.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.22.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.22.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.22.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.23.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.23.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.23.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.23.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.23.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.23.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.23.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.23.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.23.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.23.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.23.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.23.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.24.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.24.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.24.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.24.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.24.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.24.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.24.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.24.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.24.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.24.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.24.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.24.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.25.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.25.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.25.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.25.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.25.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.25.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.25.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.25.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.25.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.25.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.25.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.25.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.26.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.26.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.26.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.26.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.26.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.26.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.26.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.26.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.26.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.26.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.26.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.26.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.27.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.27.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.27.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.27.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.27.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.27.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.27.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.27.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.27.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.27.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.27.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.27.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.28.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.28.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.28.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.28.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.28.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.28.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.28.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.28.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.28.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.28.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.28.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.28.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.28.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.28.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.29.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.29.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.29.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.29.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.29.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.29.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.29.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.29.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.29.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.29.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.29.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.29.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.29.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.29.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.30.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.30.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.30.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.30.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.30.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.30.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.30.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.30.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.30.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.30.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.30.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.30.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.30.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.30.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.31.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.31.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.31.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.31.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.31.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.31.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.31.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.31.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.31.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.31.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.31.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.31.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.31.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.31.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.32.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.32.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.32.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.32.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.32.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.32.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.32.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.32.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.32.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.32.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.32.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.32.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.32.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.32.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.33.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.33.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.33.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.33.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.33.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.33.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.33.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.33.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.33.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.33.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.33.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.33.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.33.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.33.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.34.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.34.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.34.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.34.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.34.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.34.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.34.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.34.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.34.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.34.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.34.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.34.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.34.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.34.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.35.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.35.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.35.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.35.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.35.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.35.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.35.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.35.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.35.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.35.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.35.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.35.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.35.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.35.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.36.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.36.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.36.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.36.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.36.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.36.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.36.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.36.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.36.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.36.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.36.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.36.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.36.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.36.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.37.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.37.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.37.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.37.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.37.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.37.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.37.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.37.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.37.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.37.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.37.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.37.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.37.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.37.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.38.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.38.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.38.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.38.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.38.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.38.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.38.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.38.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.38.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.38.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.38.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.38.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.38.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.38.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.39.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.39.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.39.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.39.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.39.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.39.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.39.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.39.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.39.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.39.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.39.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.39.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.39.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.39.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.40.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.40.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.40.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.40.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.40.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.40.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.40.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.40.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.40.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.40.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.40.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.40.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.40.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.40.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.41.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.41.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.41.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.41.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.41.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.41.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.41.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.41.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.41.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.41.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.41.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.41.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.41.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.41.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.42.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.42.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.42.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.42.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.42.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.42.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.42.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.42.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.42.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.42.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.42.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.42.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.42.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.42.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.43.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.43.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.43.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.43.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.43.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.43.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.43.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.43.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.43.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.43.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.43.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.43.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.43.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.43.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.44.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.44.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.44.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.44.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.44.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.44.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.44.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.44.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.44.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.44.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.44.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.44.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.44.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.44.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.45.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.45.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.45.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.45.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.45.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.45.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.45.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.45.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.45.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.45.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.45.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.45.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.45.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.45.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.46.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.46.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.46.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.46.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.46.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.46.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.46.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.46.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.46.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.46.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.46.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.46.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.46.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.46.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.47.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.47.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.47.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.47.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.47.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.47.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.47.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.47.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.47.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.47.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.47.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.47.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.47.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.47.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.48.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.48.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.48.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.48.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.48.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.48.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.48.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.48.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.48.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.48.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.48.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.48.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.48.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.48.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.49.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.49.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.49.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.49.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.49.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.49.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.49.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.49.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.49.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.49.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.49.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.49.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.49.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.49.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.50.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.50.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.50.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.50.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.50.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.50.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.50.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.50.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.50.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.50.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.50.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.50.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.50.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.50.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.51.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.51.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.51.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.51.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.51.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.51.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.51.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.51.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.51.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.51.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.51.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.51.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.51.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.51.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.52.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.52.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.52.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.52.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.52.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.52.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.52.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.52.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.52.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.52.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.52.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.52.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.52.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.52.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.53.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.53.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.53.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.53.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.53.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.53.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.53.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.53.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.53.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.53.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.53.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.53.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.53.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.53.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.54.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.54.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.54.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.54.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.54.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.54.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.54.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.54.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.54.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.54.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.54.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.54.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.54.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.54.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.55.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.55.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.55.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.55.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.55.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.55.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.55.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.55.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.55.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.55.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.55.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.55.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.55.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.55.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.56.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.56.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.56.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.56.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.56.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.56.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.56.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.56.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.56.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.56.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.56.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.56.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.56.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.56.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.57.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.57.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.57.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.57.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.57.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.57.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.57.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.57.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.57.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.57.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.57.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.57.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.57.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.57.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.58.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.58.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.58.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.58.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.58.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.58.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.58.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.58.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.58.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.58.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.58.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.58.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.58.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.58.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n\tsize mismatch for model.layers.59.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.59.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.59.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.59.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.59.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.59.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\n\tsize mismatch for\
          \ model.layers.59.self_attn.v_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\n\
          \tsize mismatch for model.layers.59.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.59.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.59.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([140, 6656]).\n\tsize mismatch for\
          \ model.layers.59.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 2240]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 2240]).\n\tsize mismatch for model.layers.59.mlp.gate_proj.scales: copying\
          \ a param with shape torch.Size([1, 17920]) from checkpoint, the shape in\
          \ current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.59.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 2240]).\n\tsize mismatch for\
          \ model.layers.59.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 17920]) from checkpoint, the shape in current model is torch.Size([52,\
          \ 17920]).\n```"
        updatedAt: '2023-05-15T16:30:58.237Z'
      numEdits: 0
      reactions: []
    id: 64625e4248e13890ea530d4c
    type: comment
  author: e-caste
  content: "@TheBloke hi, thank you for quantizing these model checkpoints! I also\
    \ have a size mismatch error when trying to instance the latest model, with no\
    \ group size. I can launch the server:\n- either being very specific: `python\
    \ server.py --gpu-memory 22500MiB --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
    \ --groupsize -1 --wbits 4 --model_type llama --api --listen --listen-host=X.Y.Z.T`\n\
    - or being quite generic: `python server.py --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
    \ --api --listen --listen-host=X.Y.Z.T`\n\nI have an RTX 3090 and 48 GB of RAM,\
    \ I have installed the recommended GPTQ-for-LLaMa ([from the docs here](https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md)),\
    \ but I have also experimented with the other variants.  \nWhat command line arguments\
    \ or tricks are you using to make it work?\n\nHere are the error logs:\n```\n\
    INFO:Loading TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ...\nINFO:Found the following\
    \ quantized model: models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors\n\
    Traceback (most recent call last):\n  File \"/home/XXX/text-generation-webui/server.py\"\
    , line 952, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"/home/XXX/text-generation-webui/modules/models.py\", line 159, in load_model\n\
    \    model = load_quantized(model_name)\n  File \"/home/XXX/text-generation-webui/modules/GPTQ_loader.py\"\
    , line 178, in load_quantized\n    model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
    \  File \"/home/XXX/text-generation-webui/modules/GPTQ_loader.py\", line 84, in\
    \ _load_quant\n    model.load_state_dict(safe_load(checkpoint), strict=False)\n\
    \  File \"/home/XXX/text-generation-webui/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading state_dict\
    \ for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\n\
    \tsize mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param with\
    \ shape torch.Size([1, 832]) from checkpoint, the shape in current model is torch.Size([52,\
    \ 832]).\n\tsize mismatch for model.layers.0.self_attn.k_proj.scales: copying\
    \ a param with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
    \ model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.0.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.0.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.0.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.0.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.0.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.0.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.0.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.0.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.0.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.0.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.0.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.1.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.1.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.1.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.1.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.1.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.1.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.1.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.1.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.2.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.2.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.2.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.2.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.2.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.2.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.2.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.2.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.2.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.2.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.2.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.2.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.3.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.3.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.3.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.3.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.3.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.3.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.3.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.3.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.3.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.3.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.3.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.3.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.4.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.4.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.4.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.4.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.4.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.4.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.4.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.4.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.4.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.4.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.4.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.4.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.5.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.5.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.5.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.5.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.5.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.5.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.5.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.5.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.5.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.5.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.5.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.5.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.6.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.6.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.6.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.6.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.6.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.6.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.6.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.6.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.6.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.6.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.6.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.6.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.7.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.7.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.7.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.7.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.7.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.7.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.7.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.7.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.7.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.7.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.7.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.7.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.8.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.8.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.8.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.8.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.8.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.8.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.8.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.8.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.8.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.8.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.8.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.8.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.9.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.9.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.9.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.9.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.9.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.9.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.9.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.9.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.9.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.9.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.9.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.9.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.10.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.10.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.10.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.10.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.10.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.10.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.10.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.10.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.11.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.11.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.11.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.11.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.11.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.11.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.11.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.11.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.12.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.12.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.12.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.12.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.12.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.12.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.12.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.12.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.13.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.13.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.13.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.13.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.13.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.13.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.13.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.13.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.14.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.14.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.14.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.14.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.14.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.14.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.14.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.14.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.15.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.15.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.15.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.15.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.15.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.15.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.15.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.15.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.16.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.16.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.16.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.16.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.16.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.16.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.16.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.16.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.17.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.17.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.17.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.17.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.17.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.17.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.17.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.17.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.18.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.18.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.18.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.18.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.18.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.18.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.18.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.18.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.19.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.19.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.19.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.19.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.19.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.19.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.19.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.19.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.20.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.20.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.20.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.20.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.20.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.20.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.20.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.20.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.20.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.20.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.20.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.20.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.21.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.21.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.21.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.21.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.21.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.21.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.21.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.21.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.21.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.21.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.21.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.21.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.22.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.22.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.22.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.22.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.22.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.22.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.22.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.22.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.22.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.22.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.22.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.22.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.23.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.23.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.23.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.23.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.23.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.23.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.23.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.23.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.23.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.23.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.23.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.23.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.24.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.24.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.24.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.24.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.24.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.24.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.24.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.24.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.24.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.24.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.24.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.24.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.25.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.25.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.25.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.25.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.25.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.25.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.25.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.25.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.25.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.25.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.25.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.25.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.26.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.26.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.26.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.26.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.26.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.26.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.26.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.26.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.26.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.26.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.26.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.26.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.27.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.27.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.27.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.27.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.27.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.27.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.27.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.27.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.27.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.27.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.27.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.27.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.28.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.28.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.28.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.28.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.28.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.28.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.28.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.28.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.28.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.28.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.28.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.28.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.28.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.28.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.29.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.29.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.29.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.29.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.29.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.29.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.29.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.29.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.29.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.29.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.29.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.29.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.29.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.29.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.30.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.30.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.30.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.30.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.30.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.30.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.30.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.30.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.30.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.30.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.30.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.30.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.30.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.30.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.31.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.31.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.31.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.31.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.31.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.31.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.31.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.31.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.31.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.31.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.31.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.31.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.31.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.31.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.32.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.32.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.32.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.32.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.32.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.32.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.32.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.32.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.32.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.32.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.32.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.32.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.32.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.32.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.33.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.33.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.33.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.33.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.33.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.33.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.33.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.33.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.33.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.33.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.33.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.33.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.33.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.33.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.34.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.34.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.34.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.34.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.34.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.34.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.34.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.34.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.34.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.34.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.34.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.34.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.34.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.34.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.35.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.35.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.35.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.35.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.35.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.35.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.35.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.35.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.35.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.35.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.35.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.35.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.35.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.35.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.36.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.36.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.36.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.36.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.36.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.36.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.36.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.36.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.36.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.36.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.36.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.36.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.36.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.36.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.37.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.37.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.37.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.37.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.37.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.37.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.37.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.37.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.37.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.37.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.37.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.37.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.37.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.37.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.38.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.38.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.38.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.38.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.38.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.38.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.38.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.38.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.38.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.38.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.38.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.38.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.38.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.38.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.39.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.39.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.39.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.39.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.39.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.39.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.39.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.39.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.39.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.39.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.39.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.39.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.39.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.39.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.40.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.40.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.40.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.40.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.40.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.40.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.40.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.40.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.40.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.40.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.40.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.40.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.40.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.40.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.41.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.41.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.41.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.41.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.41.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.41.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.41.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.41.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.41.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.41.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.41.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.41.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.41.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.41.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.42.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.42.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.42.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.42.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.42.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.42.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.42.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.42.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.42.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.42.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.42.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.42.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.42.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.42.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.43.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.43.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.43.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.43.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.43.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.43.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.43.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.43.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.43.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.43.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.43.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.43.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.43.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.43.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.44.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.44.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.44.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.44.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.44.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.44.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.44.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.44.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.44.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.44.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.44.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.44.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.44.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.44.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.45.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.45.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.45.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.45.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.45.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.45.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.45.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.45.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.45.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.45.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.45.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.45.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.45.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.45.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.46.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.46.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.46.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.46.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.46.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.46.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.46.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.46.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.46.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.46.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.46.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.46.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.46.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.46.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.47.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.47.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.47.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.47.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.47.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.47.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.47.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.47.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.47.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.47.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.47.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.47.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.47.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.47.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.48.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.48.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.48.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.48.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.48.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.48.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.48.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.48.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.48.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.48.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.48.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.48.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.48.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.48.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.49.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.49.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.49.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.49.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.49.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.49.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.49.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.49.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.49.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.49.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.49.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.49.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.49.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.49.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.50.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.50.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.50.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.50.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.50.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.50.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.50.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.50.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.50.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.50.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.50.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.50.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.50.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.50.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.51.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.51.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.51.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.51.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.51.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.51.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.51.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.51.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.51.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.51.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.51.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.51.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.51.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.51.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.52.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.52.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.52.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.52.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.52.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.52.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.52.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.52.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.52.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.52.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.52.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.52.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.52.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.52.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.53.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.53.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.53.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.53.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.53.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.53.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.53.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.53.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.53.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.53.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.53.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.53.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.53.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.53.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.54.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.54.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.54.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.54.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.54.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.54.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.54.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.54.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.54.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.54.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.54.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.54.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.54.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.54.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.55.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.55.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.55.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.55.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.55.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.55.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.55.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.55.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.55.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.55.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.55.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.55.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.55.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.55.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.56.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.56.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.56.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.56.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.56.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.56.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.56.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.56.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.56.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.56.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.56.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.56.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.56.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.56.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.57.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.57.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.57.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.57.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.57.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.57.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.57.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.57.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.57.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.57.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.57.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.57.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.57.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.57.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.58.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.58.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.58.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.58.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.58.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.58.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.58.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.58.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.58.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.58.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.58.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.58.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.58.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.58.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.59.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.59.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.59.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.59.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.59.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.59.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.59.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\n\tsize mismatch for model.layers.59.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\n\tsize mismatch for model.layers.59.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([140, 832]).\n\tsize mismatch for model.layers.59.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([140, 6656]).\n\tsize mismatch for model.layers.59.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.59.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n\tsize mismatch for model.layers.59.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2240]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 2240]).\n\tsize mismatch for model.layers.59.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 17920]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 17920]).\n```"
  created_at: 2023-05-15 15:30:58+00:00
  edited: false
  hidden: false
  id: 64625e4248e13890ea530d4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-15T16:50:11.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>ugh I can''t win with this model!  I can''t seem to make a model
          that will work for everyone.</p>

          <p>Please try either:</p>

          <ol>

          <li>One of the models in the other branches, eg 1024-latest  - it will require
          more VRAM though, and will possibly OOM on a 24GB GPU on longer responses;</li>

          <li>Or continuing to use the new no-group_size model, but with the ''old''
          GPTQ-for-LLaMa, like so:</li>

          </ol>

          <pre><code>cd text-generation-webui/repositories

          mv GPTQ-for-LLaMa latest.GPTQ-for-LLaMa

          git clone https://github.com/oobabooga/GPTQ-for-LLaMa

          cd GPTQ-for-LLaMa

          pip uninstall -y quant-cuda

          python setup_cuda.py install

          </code></pre>

          <p>This will downgrade you to the earlier GfL which is what I used to quantise
          the no-group_size model.  I used that old version because if I quantise
          the model with the latest GfL, people can''t do CPU offloading on the old
          GfL, and most people are using the old GfL because it''s faster with CUDA
          and is bundled with text-generation-webui.  But from what you''re saying,
          it appears that now the same problem also exists in reverse, and a GPTQ
          made with old GfL won''t work properly with the latest version!</p>

          <p>I hope this nightmare of incompatibility will improve in the nearish
          future when the community moves to AutoGPTQ.  But we''re not quite there
          yet.</p>

          '
        raw: 'ugh I can''t win with this model!  I can''t seem to make a model that
          will work for everyone.


          Please try either:

          1. One of the models in the other branches, eg 1024-latest  - it will require
          more VRAM though, and will possibly OOM on a 24GB GPU on longer responses;

          2. Or continuing to use the new no-group_size model, but with the ''old''
          GPTQ-for-LLaMa, like so:

          ```

          cd text-generation-webui/repositories

          mv GPTQ-for-LLaMa latest.GPTQ-for-LLaMa

          git clone https://github.com/oobabooga/GPTQ-for-LLaMa

          cd GPTQ-for-LLaMa

          pip uninstall -y quant-cuda

          python setup_cuda.py install

          ```


          This will downgrade you to the earlier GfL which is what I used to quantise
          the no-group_size model.  I used that old version because if I quantise
          the model with the latest GfL, people can''t do CPU offloading on the old
          GfL, and most people are using the old GfL because it''s faster with CUDA
          and is bundled with text-generation-webui.  But from what you''re saying,
          it appears that now the same problem also exists in reverse, and a GPTQ
          made with old GfL won''t work properly with the latest version!


          I hope this nightmare of incompatibility will improve in the nearish future
          when the community moves to AutoGPTQ.  But we''re not quite there yet.'
        updatedAt: '2023-05-15T16:51:56.700Z'
      numEdits: 2
      reactions: []
    id: 646262c348e13890ea5335eb
    type: comment
  author: TheBloke
  content: 'ugh I can''t win with this model!  I can''t seem to make a model that
    will work for everyone.


    Please try either:

    1. One of the models in the other branches, eg 1024-latest  - it will require
    more VRAM though, and will possibly OOM on a 24GB GPU on longer responses;

    2. Or continuing to use the new no-group_size model, but with the ''old'' GPTQ-for-LLaMa,
    like so:

    ```

    cd text-generation-webui/repositories

    mv GPTQ-for-LLaMa latest.GPTQ-for-LLaMa

    git clone https://github.com/oobabooga/GPTQ-for-LLaMa

    cd GPTQ-for-LLaMa

    pip uninstall -y quant-cuda

    python setup_cuda.py install

    ```


    This will downgrade you to the earlier GfL which is what I used to quantise the
    no-group_size model.  I used that old version because if I quantise the model
    with the latest GfL, people can''t do CPU offloading on the old GfL, and most
    people are using the old GfL because it''s faster with CUDA and is bundled with
    text-generation-webui.  But from what you''re saying, it appears that now the
    same problem also exists in reverse, and a GPTQ made with old GfL won''t work
    properly with the latest version!


    I hope this nightmare of incompatibility will improve in the nearish future when
    the community moves to AutoGPTQ.  But we''re not quite there yet.'
  created_at: 2023-05-15 15:50:11+00:00
  edited: true
  hidden: false
  id: 646262c348e13890ea5335eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
      fullname: e
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: e-caste
      type: user
    createdAt: '2023-05-16T07:28:58.000Z'
    data:
      edited: false
      editors:
      - e-caste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
          fullname: e
          isHf: false
          isPro: false
          name: e-caste
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> thanks for the\
          \ quick reply! To clarify, I am using the \"old\" GPTQ-for-LLaMa (which\
          \ is the recommended one in the docs) when I get this error. If I install\
          \ the \"new\" one with the following steps:</p>\n<ol>\n<li><code>pip uninstall\
          \ quant-cuda</code> (this is the old one)</li>\n<li><code>cd</code> into\
          \ dir cloned from <a rel=\"nofollow\" href=\"https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda\"\
          >https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda</a></li>\n<li><code>CC=gcc-10\
          \ CXX=g++-10 python setup_cuda.py install</code></li>\n</ol>\n<p>I still\
          \ get the same error as posted above.</p>\n"
        raw: '@TheBloke thanks for the quick reply! To clarify, I am using the "old"
          GPTQ-for-LLaMa (which is the recommended one in the docs) when I get this
          error. If I install the "new" one with the following steps:

          1. `pip uninstall quant-cuda` (this is the old one)

          2. `cd` into dir cloned from https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda

          3. `CC=gcc-10 CXX=g++-10 python setup_cuda.py install`


          I still get the same error as posted above.'
        updatedAt: '2023-05-16T07:28:58.833Z'
      numEdits: 0
      reactions: []
    id: 646330bac758f9420908875f
    type: comment
  author: e-caste
  content: '@TheBloke thanks for the quick reply! To clarify, I am using the "old"
    GPTQ-for-LLaMa (which is the recommended one in the docs) when I get this error.
    If I install the "new" one with the following steps:

    1. `pip uninstall quant-cuda` (this is the old one)

    2. `cd` into dir cloned from https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda

    3. `CC=gcc-10 CXX=g++-10 python setup_cuda.py install`


    I still get the same error as posted above.'
  created_at: 2023-05-16 06:28:58+00:00
  edited: false
  hidden: false
  id: 646330bac758f9420908875f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-16T07:32:15.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;e-caste&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/e-caste\">@<span class=\"\
          underline\">e-caste</span></a></span>\n\n\t</span></span> OK then I'm confused.\
          \  The model definitely works with 'old' ooba GPTQ for Llama. I tested it\
          \ myself, both normally and with --pre_layer, and several other people have\
          \ confirmed it works.</p>\n<p>Please show testing with ooba GfL <a rel=\"\
          nofollow\" href=\"https://github.com/oobabooga/GPTQ-for-LLaMa\">https://github.com/oobabooga/GPTQ-for-LLaMa</a>\
          \ and show how you launch text-gen-ui, and the errors you get.  </p>\n<p>Remember\
          \ to re-compile ooba GPTQ for LLaMA with <code>python setup_cuda.py install</code>\
          \ when switching back to it</p>\n<p>Also just to be sure, please double\
          \ check the SHA256SUM of the downloaded model to confirm it's downloaded\
          \ OK, and that you're using the new file in the main branch:</p>\n<p><a\
          \ rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/R27ntb7lehSEDVa89LUH6.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/R27ntb7lehSEDVa89LUH6.png\"\
          ></a></p>\n"
        raw: "@e-caste OK then I'm confused.  The model definitely works with 'old'\
          \ ooba GPTQ for Llama. I tested it myself, both normally and with --pre_layer,\
          \ and several other people have confirmed it works.\n\nPlease show testing\
          \ with ooba GfL https://github.com/oobabooga/GPTQ-for-LLaMa and show how\
          \ you launch text-gen-ui, and the errors you get.  \n\nRemember to re-compile\
          \ ooba GPTQ for LLaMA with `python setup_cuda.py install` when switching\
          \ back to it\n\nAlso just to be sure, please double check the SHA256SUM\
          \ of the downloaded model to confirm it's downloaded OK, and that you're\
          \ using the new file in the main branch:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/R27ntb7lehSEDVa89LUH6.png)"
        updatedAt: '2023-05-16T07:34:22.950Z'
      numEdits: 3
      reactions: []
    id: 6463317f589d58dbc798f32a
    type: comment
  author: TheBloke
  content: "@e-caste OK then I'm confused.  The model definitely works with 'old'\
    \ ooba GPTQ for Llama. I tested it myself, both normally and with --pre_layer,\
    \ and several other people have confirmed it works.\n\nPlease show testing with\
    \ ooba GfL https://github.com/oobabooga/GPTQ-for-LLaMa and show how you launch\
    \ text-gen-ui, and the errors you get.  \n\nRemember to re-compile ooba GPTQ for\
    \ LLaMA with `python setup_cuda.py install` when switching back to it\n\nAlso\
    \ just to be sure, please double check the SHA256SUM of the downloaded model to\
    \ confirm it's downloaded OK, and that you're using the new file in the main branch:\n\
    \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/R27ntb7lehSEDVa89LUH6.png)"
  created_at: 2023-05-16 06:32:15+00:00
  edited: true
  hidden: false
  id: 6463317f589d58dbc798f32a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
      fullname: e
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: e-caste
      type: user
    createdAt: '2023-05-16T08:05:09.000Z'
    data:
      edited: true
      editors:
      - e-caste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
          fullname: e
          isHf: false
          isPro: false
          name: e-caste
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> sorry for any\
          \ confusion, let's see if I can be more clear. Here are my steps:</p>\n\
          <ul>\n<li><code>git clone https://github.com/oobabooga/text-generation-webui</code></li>\n\
          <li><code>cd text-generation-webui</code></li>\n<li><code>python3.10.6 -m\
          \ venv venv</code></li>\n<li><code>source venv/bin/activate</code></li>\n\
          <li><code>pip install torch torchvision torchaudio</code> this install PyTorch\
          \ 2.0.1 with CUDA 11.7, even though I have 11.2 installed since I am on\
          \ Pop!_OS 22.04 (but my NVIDIA drivers version 525.105.17 are compatible\
          \ with CUDA up to 12.0) -- could this be the culprit?</li>\n<li><code>pip\
          \ install -r requirements.txt</code></li>\n<li><code>mkdir repositories</code></li>\n\
          <li><code>cd repositories</code></li>\n<li><code>git clone https://github.com/oobabooga/GPTQ-for-LLaMa\
          \ -b cuda GPTQ-for-LLaMa_oobabooga</code></li>\n<li><code>ln -s GPTQ-for-LLaMa_oobabooga\
          \ GPTQ-for-LLaMa</code> the symlink is to allow text-generation-webui to\
          \ find the repo dir</li>\n<li><code>cd GPTQ-for-LLaMa</code></li>\n<li><code>sudo\
          \ apt install gcc-10 g++-10</code> this is because CUDA 11.2 is only compatible\
          \ with g++ &lt; 11</li>\n<li><code>CC=gcc-10 CXX=g++-10 python setup_cuda.py\
          \ install</code> this installs the <code>quant-cuda</code> package, \"old\"\
          \ CUDA version, with the correct version of the C/C++ compilers</li>\n<li><code>cd\
          \ ../..</code></li>\n<li><code>python download-model.py oobabooga/llama-tokenizer</code>\
          \ following this line in the <a rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md\"\
          >docs</a>: \u26A0\uFE0F The tokenizer files in the sources above may be\
          \ outdated. Make sure to obtain the universal LLaMA tokenizer as described\
          \ <a rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/blob/main/docs/LLaMA-model.md#option-1-pre-converted-weights\"\
          >here</a>. -- could this be the culprit?</li>\n<li><code>python download-model.py\
          \ TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ --branch main</code></li>\n\
          <li><code>sha256sum models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors</code>\
          \ SHA-256 checksum is 3200d0314235e82313021303fd2be12e86a819deb6bf951d8179c84d7dd5acfd,\
          \ seems correct</li>\n<li><code>python server.py --gpu-memory 22500MiB --model\
          \ TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ --groupsize -1 --wbits 4 --model_type\
          \ llama --api --listen --listen-host=192.168.X.Y</code></li>\n</ul>\n<p>At\
          \ this point I get the size mismatch error logs.</p>\n<p>My relevant hardware\
          \ is: RTX 3090, 48 GB RAM.<br>My OS is: Linux Pop!_OS 22.04 LTS, kernel\
          \ version 6.2.6-76060206-generic.</p>\n"
        raw: "@TheBloke sorry for any confusion, let's see if I can be more clear.\
          \ Here are my steps:\n- `git clone https://github.com/oobabooga/text-generation-webui`\n\
          - `cd text-generation-webui`\n- `python3.10.6 -m venv venv`\n- `source venv/bin/activate`\n\
          - `pip install torch torchvision torchaudio` this install PyTorch 2.0.1\
          \ with CUDA 11.7, even though I have 11.2 installed since I am on Pop!_OS\
          \ 22.04 (but my NVIDIA drivers version 525.105.17 are compatible with CUDA\
          \ up to 12.0) -- could this be the culprit?\n- `pip install -r requirements.txt`\n\
          - `mkdir repositories`\n- `cd repositories`\n- `git clone https://github.com/oobabooga/GPTQ-for-LLaMa\
          \ -b cuda GPTQ-for-LLaMa_oobabooga`\n- `ln -s GPTQ-for-LLaMa_oobabooga GPTQ-for-LLaMa`\
          \ the symlink is to allow text-generation-webui to find the repo dir\n-\
          \ `cd GPTQ-for-LLaMa`\n- `sudo apt install gcc-10 g++-10` this is because\
          \ CUDA 11.2 is only compatible with g++ < 11\n- `CC=gcc-10 CXX=g++-10 python\
          \ setup_cuda.py install` this installs the `quant-cuda` package, \"old\"\
          \ CUDA version, with the correct version of the C/C++ compilers\n- `cd ../..`\n\
          - `python download-model.py oobabooga/llama-tokenizer` following this line\
          \ in the [docs](https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md):\
          \ \u26A0\uFE0F The tokenizer files in the sources above may be outdated.\
          \ Make sure to obtain the universal LLaMA tokenizer as described [here](https://github.com/oobabooga/text-generation-webui/blob/main/docs/LLaMA-model.md#option-1-pre-converted-weights).\
          \ -- could this be the culprit?\n- `python download-model.py TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \ --branch main`\n- `sha256sum models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors`\
          \ SHA-256 checksum is 3200d0314235e82313021303fd2be12e86a819deb6bf951d8179c84d7dd5acfd,\
          \ seems correct\n- `python server.py --gpu-memory 22500MiB --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \ --groupsize -1 --wbits 4 --model_type llama --api --listen --listen-host=192.168.X.Y`\n\
          \nAt this point I get the size mismatch error logs.\n\nMy relevant hardware\
          \ is: RTX 3090, 48 GB RAM.  \nMy OS is: Linux Pop!_OS 22.04 LTS, kernel\
          \ version 6.2.6-76060206-generic."
        updatedAt: '2023-05-16T08:12:35.726Z'
      numEdits: 1
      reactions: []
    id: 6463393512814d7541770ff7
    type: comment
  author: e-caste
  content: "@TheBloke sorry for any confusion, let's see if I can be more clear. Here\
    \ are my steps:\n- `git clone https://github.com/oobabooga/text-generation-webui`\n\
    - `cd text-generation-webui`\n- `python3.10.6 -m venv venv`\n- `source venv/bin/activate`\n\
    - `pip install torch torchvision torchaudio` this install PyTorch 2.0.1 with CUDA\
    \ 11.7, even though I have 11.2 installed since I am on Pop!_OS 22.04 (but my\
    \ NVIDIA drivers version 525.105.17 are compatible with CUDA up to 12.0) -- could\
    \ this be the culprit?\n- `pip install -r requirements.txt`\n- `mkdir repositories`\n\
    - `cd repositories`\n- `git clone https://github.com/oobabooga/GPTQ-for-LLaMa\
    \ -b cuda GPTQ-for-LLaMa_oobabooga`\n- `ln -s GPTQ-for-LLaMa_oobabooga GPTQ-for-LLaMa`\
    \ the symlink is to allow text-generation-webui to find the repo dir\n- `cd GPTQ-for-LLaMa`\n\
    - `sudo apt install gcc-10 g++-10` this is because CUDA 11.2 is only compatible\
    \ with g++ < 11\n- `CC=gcc-10 CXX=g++-10 python setup_cuda.py install` this installs\
    \ the `quant-cuda` package, \"old\" CUDA version, with the correct version of\
    \ the C/C++ compilers\n- `cd ../..`\n- `python download-model.py oobabooga/llama-tokenizer`\
    \ following this line in the [docs](https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md):\
    \ \u26A0\uFE0F The tokenizer files in the sources above may be outdated. Make\
    \ sure to obtain the universal LLaMA tokenizer as described [here](https://github.com/oobabooga/text-generation-webui/blob/main/docs/LLaMA-model.md#option-1-pre-converted-weights).\
    \ -- could this be the culprit?\n- `python download-model.py TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\
    \ --branch main`\n- `sha256sum models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors`\
    \ SHA-256 checksum is 3200d0314235e82313021303fd2be12e86a819deb6bf951d8179c84d7dd5acfd,\
    \ seems correct\n- `python server.py --gpu-memory 22500MiB --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
    \ --groupsize -1 --wbits 4 --model_type llama --api --listen --listen-host=192.168.X.Y`\n\
    \nAt this point I get the size mismatch error logs.\n\nMy relevant hardware is:\
    \ RTX 3090, 48 GB RAM.  \nMy OS is: Linux Pop!_OS 22.04 LTS, kernel version 6.2.6-76060206-generic."
  created_at: 2023-05-16 07:05:09+00:00
  edited: true
  hidden: false
  id: 6463393512814d7541770ff7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-16T08:06:42.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK thanks for the details.  Can you please try without --gpu-memory
          ?</p>

          '
        raw: OK thanks for the details.  Can you please try without --gpu-memory ?
        updatedAt: '2023-05-16T08:06:42.727Z'
      numEdits: 0
      reactions: []
    id: 64633992ab15db2fa565c0a1
    type: comment
  author: TheBloke
  content: OK thanks for the details.  Can you please try without --gpu-memory ?
  created_at: 2023-05-16 07:06:42+00:00
  edited: false
  hidden: false
  id: 64633992ab15db2fa565c0a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
      fullname: e
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: e-caste
      type: user
    createdAt: '2023-05-16T08:10:02.000Z'
    data:
      edited: false
      editors:
      - e-caste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
          fullname: e
          isHf: false
          isPro: false
          name: e-caste
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> I have tried\
          \ without <code>--gpu-memory 22500MiB</code> and after <code>rm -rf models/oobabooga_llama-tokenizer</code>,\
          \ I always get the same size mismatch error.</p>\n"
        raw: '@TheBloke I have tried without `--gpu-memory 22500MiB` and after `rm
          -rf models/oobabooga_llama-tokenizer`, I always get the same size mismatch
          error.'
        updatedAt: '2023-05-16T08:10:02.480Z'
      numEdits: 0
      reactions: []
    id: 64633a5ac758f9420908f03e
    type: comment
  author: e-caste
  content: '@TheBloke I have tried without `--gpu-memory 22500MiB` and after `rm -rf
    models/oobabooga_llama-tokenizer`, I always get the same size mismatch error.'
  created_at: 2023-05-16 07:10:02+00:00
  edited: false
  hidden: false
  id: 64633a5ac758f9420908f03e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-16T08:32:54.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah OK I see the same. Really not sure what''s going on here. I''ll
          get back to you.</p>

          '
        raw: Yeah OK I see the same. Really not sure what's going on here. I'll get
          back to you.
        updatedAt: '2023-05-16T08:32:54.061Z'
      numEdits: 0
      reactions: []
    id: 64633fb632317fa4806666cf
    type: comment
  author: TheBloke
  content: Yeah OK I see the same. Really not sure what's going on here. I'll get
    back to you.
  created_at: 2023-05-16 07:32:54+00:00
  edited: false
  hidden: false
  id: 64633fb632317fa4806666cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-16T08:48:11.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Ok, phew! I've figured it out and it's not the model's fault :)\
          \ I thought I was going crazy there. I've done this model so many times.</p>\n\
          <p>It's a bug in text-gen-ui. Not 100% figured out what's going on, but\
          \ for some reason it's not applying the model type correctly. It think's\
          \ it's not a Llama type model.</p>\n<p>Please create/edit <code>text-generation-webui/models/config-user.yaml</code>\
          \ and add the following:</p>\n<pre><code class=\"language-yaml\"><span class=\"\
          hljs-string\">TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ$:</span>\n  <span\
          \ class=\"hljs-attr\">auto_devices:</span> <span class=\"hljs-literal\"\
          >false</span>\n  <span class=\"hljs-attr\">bf16:</span> <span class=\"hljs-literal\"\
          >false</span>\n  <span class=\"hljs-attr\">cpu:</span> <span class=\"hljs-literal\"\
          >false</span>\n  <span class=\"hljs-attr\">cpu_memory:</span> <span class=\"\
          hljs-number\">0</span>\n  <span class=\"hljs-attr\">disk:</span> <span class=\"\
          hljs-literal\">false</span>\n  <span class=\"hljs-attr\">gpu_memory_0:</span>\
          \ <span class=\"hljs-number\">0</span>\n  <span class=\"hljs-attr\">groupsize:</span>\
          \ <span class=\"hljs-string\">None</span>\n  <span class=\"hljs-attr\">load_in_8bit:</span>\
          \ <span class=\"hljs-literal\">false</span>\n  <span class=\"hljs-attr\"\
          >mlock:</span> <span class=\"hljs-literal\">false</span>\n  <span class=\"\
          hljs-attr\">model_type:</span> <span class=\"hljs-string\">llama</span>\n\
          \  <span class=\"hljs-attr\">n_batch:</span> <span class=\"hljs-number\"\
          >512</span>\n  <span class=\"hljs-attr\">n_gpu_layers:</span> <span class=\"\
          hljs-number\">0</span>\n  <span class=\"hljs-attr\">pre_layer:</span> <span\
          \ class=\"hljs-number\">0</span>\n  <span class=\"hljs-attr\">threads:</span>\
          \ <span class=\"hljs-number\">0</span>\n  <span class=\"hljs-attr\">wbits:</span>\
          \ <span class=\"hljs-string\">'4'</span>\n</code></pre>\n<p>Save that file,\
          \ then close and re-open text-gen-ui with these args:  <code>python server.py\
          \  --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ  --api --listen</code>\
          \ (plus whatever other non-model-related args you want)</p>\n<pre><code>root@eb60333c3265:~/test/text-generation-webui#\
          \ python server.py  --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \  --api --listen\nINFO:Note: detected 256 virtual cores but NumExpr set\
          \ to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n\
          INFO:Note: NumExpr detected 256 cores but \"NUMEXPR_MAX_THREADS\" not set,\
          \ so enforcing safe limit of 8.\nINFO:NumExpr defaulting to 8 threads.\n\
          INFO:Gradio HTTP request redirected to localhost :)\nbin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n\
          INFO:Loading TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ...\nINFO:Found\
          \ the following quantized model: models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors\n\
          INFO:Loaded the model in 16.59 seconds.\n\nStarting streaming server at\
          \ ws://0.0.0.0:5005/api/v1/stream\nStarting API at http://0.0.0.0:5000/api\n\
          INFO:server listening on 0.0.0.0:5005\nRunning on local URL:  http://0.0.0.0:7860\n\
          \nTo create a public link, set `share=True` in `launch()`.\n</code></pre>\n"
        raw: "Ok, phew! I've figured it out and it's not the model's fault :) I thought\
          \ I was going crazy there. I've done this model so many times.\n\nIt's a\
          \ bug in text-gen-ui. Not 100% figured out what's going on, but for some\
          \ reason it's not applying the model type correctly. It think's it's not\
          \ a Llama type model.\n\nPlease create/edit `text-generation-webui/models/config-user.yaml`\
          \ and add the following:\n```yaml\nTheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ$:\n\
          \  auto_devices: false\n  bf16: false\n  cpu: false\n  cpu_memory: 0\n \
          \ disk: false\n  gpu_memory_0: 0\n  groupsize: None\n  load_in_8bit: false\n\
          \  mlock: false\n  model_type: llama\n  n_batch: 512\n  n_gpu_layers: 0\n\
          \  pre_layer: 0\n  threads: 0\n  wbits: '4'\n```\n\nSave that file, then\
          \ close and re-open text-gen-ui with these args:  `python server.py  --model\
          \ TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ  --api --listen` (plus whatever\
          \ other non-model-related args you want)\n\n```\nroot@eb60333c3265:~/test/text-generation-webui#\
          \ python server.py  --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \  --api --listen\nINFO:Note: detected 256 virtual cores but NumExpr set\
          \ to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n\
          INFO:Note: NumExpr detected 256 cores but \"NUMEXPR_MAX_THREADS\" not set,\
          \ so enforcing safe limit of 8.\nINFO:NumExpr defaulting to 8 threads.\n\
          INFO:Gradio HTTP request redirected to localhost :)\nbin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n\
          INFO:Loading TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ...\nINFO:Found\
          \ the following quantized model: models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors\n\
          INFO:Loaded the model in 16.59 seconds.\n\nStarting streaming server at\
          \ ws://0.0.0.0:5005/api/v1/stream\nStarting API at http://0.0.0.0:5000/api\n\
          INFO:server listening on 0.0.0.0:5005\nRunning on local URL:  http://0.0.0.0:7860\n\
          \nTo create a public link, set `share=True` in `launch()`.\n\n```"
        updatedAt: '2023-05-16T08:48:41.443Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - e-caste
    id: 6463434bab15db2fa5661b31
    type: comment
  author: TheBloke
  content: "Ok, phew! I've figured it out and it's not the model's fault :) I thought\
    \ I was going crazy there. I've done this model so many times.\n\nIt's a bug in\
    \ text-gen-ui. Not 100% figured out what's going on, but for some reason it's\
    \ not applying the model type correctly. It think's it's not a Llama type model.\n\
    \nPlease create/edit `text-generation-webui/models/config-user.yaml` and add the\
    \ following:\n```yaml\nTheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ$:\n  auto_devices:\
    \ false\n  bf16: false\n  cpu: false\n  cpu_memory: 0\n  disk: false\n  gpu_memory_0:\
    \ 0\n  groupsize: None\n  load_in_8bit: false\n  mlock: false\n  model_type: llama\n\
    \  n_batch: 512\n  n_gpu_layers: 0\n  pre_layer: 0\n  threads: 0\n  wbits: '4'\n\
    ```\n\nSave that file, then close and re-open text-gen-ui with these args:  `python\
    \ server.py  --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ  --api --listen`\
    \ (plus whatever other non-model-related args you want)\n\n```\nroot@eb60333c3265:~/test/text-generation-webui#\
    \ python server.py  --model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ  --api\
    \ --listen\nINFO:Note: detected 256 virtual cores but NumExpr set to maximum of\
    \ 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\nINFO:Note: NumExpr\
    \ detected 256 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit\
    \ of 8.\nINFO:NumExpr defaulting to 8 threads.\nINFO:Gradio HTTP request redirected\
    \ to localhost :)\nbin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n\
    INFO:Loading TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ...\nINFO:Found the following\
    \ quantized model: models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-SFT-7-Llama-30B-GPTQ-4bit.safetensors\n\
    INFO:Loaded the model in 16.59 seconds.\n\nStarting streaming server at ws://0.0.0.0:5005/api/v1/stream\n\
    Starting API at http://0.0.0.0:5000/api\nINFO:server listening on 0.0.0.0:5005\n\
    Running on local URL:  http://0.0.0.0:7860\n\nTo create a public link, set `share=True`\
    \ in `launch()`.\n\n```"
  created_at: 2023-05-16 07:48:11+00:00
  edited: true
  hidden: false
  id: 6463434bab15db2fa5661b31
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
      fullname: e
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: e-caste
      type: user
    createdAt: '2023-05-16T08:49:14.000Z'
    data:
      edited: false
      editors:
      - e-caste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
          fullname: e
          isHf: false
          isPro: false
          name: e-caste
          type: user
        html: "<p>I am also trying the 1024-latest model variant, both with and without\
          \ the custom oobabboga/llama-tokenizer, and I always get the maximum number\
          \ of tokens of gibberish back:</p>\n<p>user: hello!<br>assistant: blmerge\
          \ Thom Blatte Giblblbloussmergedemmergeblbl threads\u677Ebl Tamergebl Women],\
          \ blblmergebljetmergeblousmergeblmergeatoblmergemergebluousmergeblshipblbl),mergemergeivablbl.),blblboundblmerge\
          \ Howardblblmerge\u82E5blmergeBl'blblGiblbl womenblblateblmergeageblblattemergebl\
          \ Ratblbl HDmergeblinemergemergemerge Blblbl Gibl Womenmergebl Giremblmergeleaseblblatblbl\
          \ Blblmergeshipmergebl Blattemergemergeatoatteblblw\xE4rmergemerge.),mergebl)--blmerge\
          \ Heinrichblblinglyblbl Felblbl WomenattemergeattemergeIKblblInvokeblbl\
          \ Tablblouslyblbl\uFFFDblmergeattebl.),merge Furblbl--blbl Threadblmerge.),atteblmergepportblblshipmergeatteofblbl\
          \ MissourimergebloussblblThreadblblplanblblravblbl Girlsmergebl),blblatoblbl],blblakiblbl\
          \ rmblbl.--blbljetblbl Russianblbl Henriblblvblbl remainingblblaticallyblbl\
          \ rasblblVEblbldobblbl approblbl Rugbyblblremblblasyblblatablbl()),blblenneblblousblbl\
          \ ratblbl photographblbl `--blblatableblblptblbl \u0440blbllotblbl'blmergeoussblshipshipbl.),shipbl\
          \ Bl.),bl.),atoblshipatoblatomergeblattebl Bl Womenblblrablblityblbl Potblbl\xE9\
          eblbl)-- Blblatteshipmergeatomergeato Blblship),blatte.),blatte Blbl Blmergeblatoshipblshipremblatteatobl.),),blmerge\
          \ Giblmerge Felmergebl.),.),bl Gimergeblbound.),blatorembl Giboundblbl EDblbl\u677E\
          blblvoblbl fotografblbl SOblblientblbl\xEDkblbl LagblblveblblBlblbleltblbl\
          \ Georgesblbl $_blbl Thomblblmentblbl Eddblbl'_blbl OffblblDispatchblblineblbl\
          \ Primblbl dispatchmergemergeshipatomergemerge Felbl.),rembl Womenbl.),\
          \ Blbl Womenremblshipboundblatteremblato Blmergeato GiblshipIKmergeblatmergebl\
          \ Felmerge</p>\n<p>Instead the 128-latest version, on top of producing the\
          \ max amount of tokens set in the API, throws an error: <code>IndexError:\
          \ piece id is out of range.</code></p>\n<hr>\n<p>If I remove the \"old\"\
          \ quant-cuda, and replace it with the one by iwalton3 (<a rel=\"nofollow\"\
          \ href=\"https://github.com/iwalton3/GPTQ-for-LLaMa\">https://github.com/iwalton3/GPTQ-for-LLaMa</a>),\
          \ I can get <strong>some</strong> sensible output (e.g. user: hello! assistant:\
          \ \U0001F60A Hello there! How may I assist you today?), but I get a lot\
          \ of responses which consist of just the <code>\\u200b</code> zero-width\
          \ space character (e.g. user: write a haiku about the Python programming\
          \ language assistant: \\u200b user: please assistant: \U0001F331Python grows\
          \ In code it blooms and flows A garden of logic) -- it really likes emojis\
          \ apparently.</p>\n<p>If I try to run the checkpoint in the main branch,\
          \ it gives the same size mismatch errors as before.</p>\n<hr>\n<p>If I remove\
          \ the gptq-llama package and install the \"new\" quant-cuda, the model from\
          \ the main branch still crashes with the same error, and the 128-latest\
          \ version behaves as above.</p>\n<p>My preprompt (partially stolen from\
          \ MPT-7B-Chat) is:</p>\n<pre><code class=\"language-py\">USERTOKEN = <span\
          \ class=\"hljs-string\">\"&lt;|prompter|&gt; \"</span>\nASSISTANTTOKEN =\
          \ <span class=\"hljs-string\">\"&lt;|assistant|&gt;: \"</span>\nENDTOKEN\
          \ = <span class=\"hljs-string\">\"\\n\"</span>\nstart_message = <span class=\"\
          hljs-string\">f\"<span class=\"hljs-subst\">{USERTOKEN}</span>You are a\
          \ helpful chatbot. You answer questions. You are excited to be able to help\
          \ the user and you will help them any way you can. You are more than an\
          \ information source, you are also able to code, write poetry, write short\
          \ stories, summarize text, and make jokes. You always reply after `<span\
          \ class=\"hljs-subst\">{ASSISTANTTOKEN}</span>`. Say okay if you understand.\\\
          n<span class=\"hljs-subst\">{ASSISTANTTOKEN}</span>okay.<span class=\"hljs-subst\"\
          >{ENDTOKEN}</span>\"</span>\n</code></pre>\n<p>By the way, thanks a lot\
          \ for helping me. It would be extremely cool to run OpenAssistant locally!</p>\n"
        raw: "I am also trying the 1024-latest model variant, both with and without\
          \ the custom oobabboga/llama-tokenizer, and I always get the maximum number\
          \ of tokens of gibberish back:\n\nuser: hello!\nassistant: blmerge Thom\
          \ Blatte Giblblbloussmergedemmergeblbl threads\u677Ebl Tamergebl Women],\
          \ blblmergebljetmergeblousmergeblmergeatoblmergemergebluousmergeblshipblbl),mergemergeivablbl.),blblboundblmerge\
          \ Howardblblmerge\u82E5blmergeBl\\'blblGiblbl womenblblateblmergeageblblattemergebl\
          \ Ratblbl HDmergeblinemergemergemerge Blblbl Gibl Womenmergebl Giremblmergeleaseblblatblbl\
          \ Blblmergeshipmergebl Blattemergemergeatoatteblblw\xE4rmergemerge.),mergebl)--blmerge\
          \ Heinrichblblinglyblbl Felblbl WomenattemergeattemergeIKblblInvokeblbl\
          \ Tablblouslyblbl\uFFFDblmergeattebl.),merge Furblbl--blbl Threadblmerge.),atteblmergepportblblshipmergeatteofblbl\
          \ MissourimergebloussblblThreadblblplanblblravblbl Girlsmergebl),blblatoblbl],blblakiblbl\
          \ rmblbl.--blbljetblbl Russianblbl Henriblblvblbl remainingblblaticallyblbl\
          \ rasblblVEblbldobblbl approblbl Rugbyblblremblblasyblblatablbl()),blblenneblblousblbl\
          \ ratblbl photographblbl `--blblatableblblptblbl \u0440blbllotblbl\\'blmergeoussblshipshipbl.),shipbl\
          \ Bl.),bl.),atoblshipatoblatomergeblattebl Bl Womenblblrablblityblbl Potblbl\xE9\
          eblbl)-- Blblatteshipmergeatomergeato Blblship),blatte.),blatte Blbl Blmergeblatoshipblshipremblatteatobl.),),blmerge\
          \ Giblmerge Felmergebl.),.),bl Gimergeblbound.),blatorembl Giboundblbl EDblbl\u677E\
          blblvoblbl fotografblbl SOblblientblbl\xEDkblbl LagblblveblblBlblbleltblbl\
          \ Georgesblbl $_blbl Thomblblmentblbl Eddblbl'_blbl OffblblDispatchblblineblbl\
          \ Primblbl dispatchmergemergeshipatomergemerge Felbl.),rembl Womenbl.),\
          \ Blbl Womenremblshipboundblatteremblato Blmergeato GiblshipIKmergeblatmergebl\
          \ Felmerge\n\nInstead the 128-latest version, on top of producing the max\
          \ amount of tokens set in the API, throws an error: `IndexError: piece id\
          \ is out of range.`\n\n---\n\nIf I remove the \"old\" quant-cuda, and replace\
          \ it with the one by iwalton3 (https://github.com/iwalton3/GPTQ-for-LLaMa),\
          \ I can get __some__ sensible output (e.g. user: hello! assistant: \U0001F60A\
          \ Hello there! How may I assist you today?), but I get a lot of responses\
          \ which consist of just the `\\u200b` zero-width space character (e.g. user:\
          \ write a haiku about the Python programming language assistant: \\u200b\
          \ user: please assistant: \U0001F331Python grows In code it blooms and flows\
          \ A garden of logic) -- it really likes emojis apparently.\n\nIf I try to\
          \ run the checkpoint in the main branch, it gives the same size mismatch\
          \ errors as before.\n\n---\n\nIf I remove the gptq-llama package and install\
          \ the \"new\" quant-cuda, the model from the main branch still crashes with\
          \ the same error, and the 128-latest version behaves as above.\n\nMy preprompt\
          \ (partially stolen from MPT-7B-Chat) is:\n```py\nUSERTOKEN = \"<|prompter|>\
          \ \"\nASSISTANTTOKEN = \"<|assistant|>: \"\nENDTOKEN = \"\\n\"\nstart_message\
          \ = f\"{USERTOKEN}You are a helpful chatbot. You answer questions. You are\
          \ excited to be able to help the user and you will help them any way you\
          \ can. You are more than an information source, you are also able to code,\
          \ write poetry, write short stories, summarize text, and make jokes. You\
          \ always reply after `{ASSISTANTTOKEN}`. Say okay if you understand.\\n{ASSISTANTTOKEN}okay.{ENDTOKEN}\"\
          \n```\n\nBy the way, thanks a lot for helping me. It would be extremely\
          \ cool to run OpenAssistant locally!"
        updatedAt: '2023-05-16T08:49:14.293Z'
      numEdits: 0
      reactions: []
    id: 6463438a589d58dbc799a7b3
    type: comment
  author: e-caste
  content: "I am also trying the 1024-latest model variant, both with and without\
    \ the custom oobabboga/llama-tokenizer, and I always get the maximum number of\
    \ tokens of gibberish back:\n\nuser: hello!\nassistant: blmerge Thom Blatte Giblblbloussmergedemmergeblbl\
    \ threads\u677Ebl Tamergebl Women], blblmergebljetmergeblousmergeblmergeatoblmergemergebluousmergeblshipblbl),mergemergeivablbl.),blblboundblmerge\
    \ Howardblblmerge\u82E5blmergeBl\\'blblGiblbl womenblblateblmergeageblblattemergebl\
    \ Ratblbl HDmergeblinemergemergemerge Blblbl Gibl Womenmergebl Giremblmergeleaseblblatblbl\
    \ Blblmergeshipmergebl Blattemergemergeatoatteblblw\xE4rmergemerge.),mergebl)--blmerge\
    \ Heinrichblblinglyblbl Felblbl WomenattemergeattemergeIKblblInvokeblbl Tablblouslyblbl\uFFFD\
    blmergeattebl.),merge Furblbl--blbl Threadblmerge.),atteblmergepportblblshipmergeatteofblbl\
    \ MissourimergebloussblblThreadblblplanblblravblbl Girlsmergebl),blblatoblbl],blblakiblbl\
    \ rmblbl.--blbljetblbl Russianblbl Henriblblvblbl remainingblblaticallyblbl rasblblVEblbldobblbl\
    \ approblbl Rugbyblblremblblasyblblatablbl()),blblenneblblousblbl ratblbl photographblbl\
    \ `--blblatableblblptblbl \u0440blbllotblbl\\'blmergeoussblshipshipbl.),shipbl\
    \ Bl.),bl.),atoblshipatoblatomergeblattebl Bl Womenblblrablblityblbl Potblbl\xE9\
    eblbl)-- Blblatteshipmergeatomergeato Blblship),blatte.),blatte Blbl Blmergeblatoshipblshipremblatteatobl.),),blmerge\
    \ Giblmerge Felmergebl.),.),bl Gimergeblbound.),blatorembl Giboundblbl EDblbl\u677E\
    blblvoblbl fotografblbl SOblblientblbl\xEDkblbl LagblblveblblBlblbleltblbl Georgesblbl\
    \ $_blbl Thomblblmentblbl Eddblbl'_blbl OffblblDispatchblblineblbl Primblbl dispatchmergemergeshipatomergemerge\
    \ Felbl.),rembl Womenbl.), Blbl Womenremblshipboundblatteremblato Blmergeato GiblshipIKmergeblatmergebl\
    \ Felmerge\n\nInstead the 128-latest version, on top of producing the max amount\
    \ of tokens set in the API, throws an error: `IndexError: piece id is out of range.`\n\
    \n---\n\nIf I remove the \"old\" quant-cuda, and replace it with the one by iwalton3\
    \ (https://github.com/iwalton3/GPTQ-for-LLaMa), I can get __some__ sensible output\
    \ (e.g. user: hello! assistant: \U0001F60A Hello there! How may I assist you today?),\
    \ but I get a lot of responses which consist of just the `\\u200b` zero-width\
    \ space character (e.g. user: write a haiku about the Python programming language\
    \ assistant: \\u200b user: please assistant: \U0001F331Python grows In code it\
    \ blooms and flows A garden of logic) -- it really likes emojis apparently.\n\n\
    If I try to run the checkpoint in the main branch, it gives the same size mismatch\
    \ errors as before.\n\n---\n\nIf I remove the gptq-llama package and install the\
    \ \"new\" quant-cuda, the model from the main branch still crashes with the same\
    \ error, and the 128-latest version behaves as above.\n\nMy preprompt (partially\
    \ stolen from MPT-7B-Chat) is:\n```py\nUSERTOKEN = \"<|prompter|> \"\nASSISTANTTOKEN\
    \ = \"<|assistant|>: \"\nENDTOKEN = \"\\n\"\nstart_message = f\"{USERTOKEN}You\
    \ are a helpful chatbot. You answer questions. You are excited to be able to help\
    \ the user and you will help them any way you can. You are more than an information\
    \ source, you are also able to code, write poetry, write short stories, summarize\
    \ text, and make jokes. You always reply after `{ASSISTANTTOKEN}`. Say okay if\
    \ you understand.\\n{ASSISTANTTOKEN}okay.{ENDTOKEN}\"\n```\n\nBy the way, thanks\
    \ a lot for helping me. It would be extremely cool to run OpenAssistant locally!"
  created_at: 2023-05-16 07:49:14+00:00
  edited: false
  hidden: false
  id: 6463438a589d58dbc799a7b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
      fullname: e
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: e-caste
      type: user
    createdAt: '2023-05-16T08:55:05.000Z'
    data:
      edited: false
      editors:
      - e-caste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671291560403-noauth.png?w=200&h=200&f=face
          fullname: e
          isHf: false
          isPro: false
          name: e-caste
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> thanks a lot!\
          \ That worked! \U0001F389</p>\n"
        raw: "@TheBloke thanks a lot! That worked! \U0001F389"
        updatedAt: '2023-05-16T08:55:05.972Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 646344e91ac9eb9637cdcdc8
    type: comment
  author: e-caste
  content: "@TheBloke thanks a lot! That worked! \U0001F389"
  created_at: 2023-05-16 07:55:05+00:00
  edited: false
  hidden: false
  id: 646344e91ac9eb9637cdcdc8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-05-16T12:25:45.000Z'
    data:
      edited: false
      editors:
      - pevogam
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
          fullname: Plamen Dimitrov
          isHf: false
          isPro: false
          name: pevogam
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ unfortunately this did not change anything for me running on a docker\
          \ container like so:</p>\n<pre><code>pevogam@prime:/mnt/local/notebooks/text-generation-webui&gt;\
          \ head .env \n# by default the Dockerfile specifies these versions: 3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX\n\
          # however for me to work i had to specify the exact version for my card\
          \ ( 2060 ) it was 7.5\n# https://developer.nvidia.com/cuda-gpus you can\
          \ find the version for your card here\nTORCH_CUDA_ARCH_LIST=7.5\n\n# these\
          \ commands worked for me with roughly 4.5GB of vram\nCLI_ARGS=--model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \ --api --listen\n\npevogam@prime:/mnt/local/notebooks/text-generation-webui&gt;\
          \ cat models/config-user.yaml \nTheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ$:\n\
          \  auto_devices: false\n  bf16: false\n  cpu: false\n  cpu_memory: 0\n \
          \ disk: false\n  gpu_memory_0: 0\n  groupsize: None\n  load_in_8bit: false\n\
          \  mlock: false\n  model_type: llama\n  n_batch: 512\n  n_gpu_layers: 0\n\
          \  pre_layer: 0\n  threads: 0\n  wbits: '4'\npevogam@prime:/mnt/local/notebooks/text-generation-webui&gt;\
          \ docker compose up --build\n[+] Building 1.3s (37/37) FINISHED        \
          \                                                                      \
          \                                                                      \
          \                \n =&gt; [internal] load build definition from Dockerfile\
          \                                                                      \
          \                                                                      \
          \ 0.1s\n =&gt; =&gt; transferring dockerfile: 115B                     \
          \                                                                      \
          \                                                                  0.0s\n\
          \ =&gt; [internal] load .dockerignore                                  \
          \                                                                      \
          \                                                        0.0s\n =&gt; =&gt;\
          \ transferring context: 123B                                           \
          \                                                                      \
          \                                               0.0s\n =&gt; [internal]\
          \ load metadata for docker.io/nvidia/cuda:11.8.0-runtime-ubuntu22.04   \
          \                                                                      \
          \                                       0.9s\n =&gt; [internal] load metadata\
          \ for docker.io/nvidia/cuda:11.8.0-devel-ubuntu22.04                   \
          \                                                                      \
          \                         0.0s\n =&gt; [stage-1  1/24] FROM docker.io/nvidia/cuda:11.8.0-runtime-ubuntu22.04@sha256:9b9ce0e128463d147a58b5013255c60e7eb725141f37c197b1ddee5aeb7e4161\
          \                                                0.0s\n =&gt; [internal]\
          \ load build context                                                   \
          \                                                                      \
          \                                       0.0s\n =&gt; =&gt; transferring\
          \ context: 5.00kB                                                      \
          \                                                                      \
          \                                  0.0s\n =&gt; [builder 1/7] FROM docker.io/nvidia/cuda:11.8.0-devel-ubuntu22.04\
          \                                                                      \
          \                                                      0.0s\n =&gt; CACHED\
          \ [stage-1  2/24] RUN apt-get update &amp;&amp;     apt-get install --no-install-recommends\
          \ -y libportaudio2 libasound-dev git python3 python3-pip make g++ &amp;&amp;\
          \     rm -rf /var/lib/apt/lists/*  0.0s\n =&gt; CACHED [stage-1  3/24] RUN\
          \ --mount=type=cache,target=/root/.cache/pip pip3 install virtualenv   \
          \                                                                      \
          \                       0.0s\n =&gt; CACHED [stage-1  4/24] RUN mkdir /app\
          \                                                                      \
          \                                                                      \
          \            0.0s\n =&gt; CACHED [stage-1  5/24] WORKDIR /app          \
          \                                                                      \
          \                                                                      \
          \    0.0s\n =&gt; CACHED [stage-1  6/24] RUN test -n \"HEAD\" &amp;&amp;\
          \ git reset --hard HEAD || echo \"Using provided webui source\"        \
          \                                                                      \
          \       0.0s\n =&gt; CACHED [stage-1  7/24] RUN virtualenv /app/venv   \
          \                                                                      \
          \                                                                     0.0s\n\
          \ =&gt; CACHED [stage-1  8/24] RUN . /app/venv/bin/activate &amp;&amp; \
          \    pip3 install --upgrade pip setuptools &amp;&amp;     pip3 install torch\
          \ torchvision torchaudio                                            0.0s\n\
          \ =&gt; CACHED [builder 2/7] RUN apt-get update &amp;&amp;     apt-get install\
          \ --no-install-recommends -y git vim build-essential python3-dev python3-venv\
          \ &amp;&amp;     rm -rf /var/lib/apt/lists/*                0.0s\n =&gt;\
          \ CACHED [builder 3/7] RUN git clone https://github.com/oobabooga/GPTQ-for-LLaMa\
          \ /build                                                               \
          \                                         0.0s\n =&gt; CACHED [builder 4/7]\
          \ WORKDIR /build                                                       \
          \                                                                      \
          \                             0.0s\n =&gt; CACHED [builder 5/7] RUN python3\
          \ -m venv /build/venv                                                  \
          \                                                                      \
          \                 0.0s\n =&gt; CACHED [builder 6/7] RUN . /build/venv/bin/activate\
          \ &amp;&amp;     pip3 install --upgrade pip setuptools &amp;&amp;     pip3\
          \ install torch torchvision torchaudio &amp;&amp;     pip3 install -r requirements.txt\
          \    0.0s\n =&gt; CACHED [builder 7/7] RUN . /build/venv/bin/activate &amp;&amp;\
          \     python3 setup_cuda.py bdist_wheel -d .                           \
          \                                                                 0.0s\n\
          \ =&gt; CACHED [stage-1  9/24] COPY --from=builder /build /app/repositories/GPTQ-for-LLaMa\
          \                                                                      \
          \                                     0.0s\n =&gt; CACHED [stage-1 10/24]\
          \ RUN . /app/venv/bin/activate &amp;&amp;     pip3 install /app/repositories/GPTQ-for-LLaMa/*.whl\
          \                                                                      \
          \         0.0s\n =&gt; CACHED [stage-1 11/24] COPY extensions/api/requirements.txt\
          \ /app/extensions/api/requirements.txt                                 \
          \                                                            0.0s\n =&gt;\
          \ CACHED [stage-1 12/24] COPY extensions/elevenlabs_tts/requirements.txt\
          \ /app/extensions/elevenlabs_tts/requirements.txt                      \
          \                                                 0.0s\n =&gt; CACHED [stage-1\
          \ 13/24] COPY extensions/google_translate/requirements.txt /app/extensions/google_translate/requirements.txt\
          \                                                                   0.0s\n\
          \ =&gt; CACHED [stage-1 14/24] COPY extensions/silero_tts/requirements.txt\
          \ /app/extensions/silero_tts/requirements.txt                          \
          \                                                     0.0s\n =&gt; CACHED\
          \ [stage-1 15/24] COPY extensions/whisper_stt/requirements.txt /app/extensions/whisper_stt/requirements.txt\
          \                                                                      \
          \       0.0s\n =&gt; CACHED [stage-1 16/24] RUN --mount=type=cache,target=/root/.cache/pip\
          \ . /app/venv/bin/activate &amp;&amp; cd extensions/api &amp;&amp; pip3\
          \ install -r requirements.txt                                      0.0s\n\
          \ =&gt; CACHED [stage-1 17/24] RUN --mount=type=cache,target=/root/.cache/pip\
          \ . /app/venv/bin/activate &amp;&amp; cd extensions/elevenlabs_tts &amp;&amp;\
          \ pip3 install -r requirements.txt                           0.0s\n =&gt;\
          \ CACHED [stage-1 18/24] RUN --mount=type=cache,target=/root/.cache/pip\
          \ . /app/venv/bin/activate &amp;&amp; cd extensions/google_translate &amp;&amp;\
          \ pip3 install -r requirements.txt                         0.0s\n =&gt;\
          \ CACHED [stage-1 19/24] RUN --mount=type=cache,target=/root/.cache/pip\
          \ . /app/venv/bin/activate &amp;&amp; cd extensions/silero_tts &amp;&amp;\
          \ pip3 install -r requirements.txt                               0.0s\n\
          \ =&gt; CACHED [stage-1 20/24] RUN --mount=type=cache,target=/root/.cache/pip\
          \ . /app/venv/bin/activate &amp;&amp; cd extensions/whisper_stt &amp;&amp;\
          \ pip3 install -r requirements.txt                              0.0s\n =&gt;\
          \ CACHED [stage-1 21/24] COPY requirements.txt /app/requirements.txt   \
          \                                                                      \
          \                                                  0.0s\n =&gt; CACHED [stage-1\
          \ 22/24] RUN . /app/venv/bin/activate &amp;&amp;     pip3 install -r requirements.txt\
          \                                                                      \
          \                            0.0s\n =&gt; CACHED [stage-1 23/24] RUN cp\
          \ /app/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\
          \ /app/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\
          \      0.0s\n =&gt; CACHED [stage-1 24/24] COPY . /app/                \
          \                                                                      \
          \                                                                    0.0s\n\
          \ =&gt; exporting to image                                             \
          \                                                                      \
          \                                                        0.1s\n =&gt; =&gt;\
          \ exporting layers                                                     \
          \                                                                      \
          \                                               0.0s\n =&gt; =&gt; writing\
          \ image sha256:cebe69eb2dd3d8126d46913e0c05e48da6b2a152d4bd2e77382fdfdeaf3994e2\
          \                                                                      \
          \                               0.0s\n =&gt; =&gt; naming to docker.io/library/text-generation-webui-text-generation-webui\
          \                                                                      \
          \                                             0.0s\n[+] Running 1/1\n \u2714\
          \ Container text-generation-webui-text-generation-webui-1  Recreated   \
          \                                                                      \
          \                                                   0.1s \nAttaching to\
          \ text-generation-webui-text-generation-webui-1\ntext-generation-webui-text-generation-webui-1\
          \  | \ntext-generation-webui-text-generation-webui-1  | ==========\ntext-generation-webui-text-generation-webui-1\
          \  | == CUDA ==\ntext-generation-webui-text-generation-webui-1  | ==========\n\
          text-generation-webui-text-generation-webui-1  | \ntext-generation-webui-text-generation-webui-1\
          \  | CUDA Version 11.8.0\ntext-generation-webui-text-generation-webui-1\
          \  | \ntext-generation-webui-text-generation-webui-1  | Container image\
          \ Copyright (c) 2016-2022, NVIDIA CORPORATION &amp; AFFILIATES. All rights\
          \ reserved.\ntext-generation-webui-text-generation-webui-1  | \ntext-generation-webui-text-generation-webui-1\
          \  | This container image and its contents are governed by the NVIDIA Deep\
          \ Learning Container License.\ntext-generation-webui-text-generation-webui-1\
          \  | By pulling and using the container, you accept the terms and conditions\
          \ of this license:\ntext-generation-webui-text-generation-webui-1  | https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\
          text-generation-webui-text-generation-webui-1  | \ntext-generation-webui-text-generation-webui-1\
          \  | A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE\
          \ for your convenience.\ntext-generation-webui-text-generation-webui-1 \
          \ | \ntext-generation-webui-text-generation-webui-1  | Gradio HTTP request\
          \ redirected to localhost :)\ntext-generation-webui-text-generation-webui-1\
          \  | bin /app/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n\
          text-generation-webui-text-generation-webui-1  | Loading TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ...\n\
          text-generation-webui-text-generation-webui-1  | Found the following quantized\
          \ model: models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors\n\
          text-generation-webui-text-generation-webui-1  | Loading model ...\ntext-generation-webui-text-generation-webui-1\
          \  | Traceback (most recent call last):\ntext-generation-webui-text-generation-webui-1\
          \  |   File \"/app/server.py\", line 918, in &lt;module&gt;\ntext-generation-webui-text-generation-webui-1\
          \  |     shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          text-generation-webui-text-generation-webui-1  |   File \"/app/modules/models.py\"\
          , line 150, in load_model\ntext-generation-webui-text-generation-webui-1\
          \  |     model = load_quantized(model_name)\ntext-generation-webui-text-generation-webui-1\
          \  |   File \"/app/modules/GPTQ_loader.py\", line 176, in load_quantized\n\
          text-generation-webui-text-generation-webui-1  |     model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          text-generation-webui-text-generation-webui-1  |   File \"/app/modules/GPTQ_loader.py\"\
          , line 77, in _load_quant\ntext-generation-webui-text-generation-webui-1\
          \  |     model.load_state_dict(safe_load(checkpoint), strict=False)\ntext-generation-webui-text-generation-webui-1\
          \  |   File \"/app/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\ntext-generation-webui-text-generation-webui-1\
          \  |     raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\\
          t{}'.format(\ntext-generation-webui-text-generation-webui-1  | RuntimeError:\
          \ Error(s) in loading state_dict for LlamaForCausalLM:\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.k_proj.qzeros: copying\
          \ a param with shape torch.Size([7, 832]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([7, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 6656]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.o_proj.qzeros: copying\
          \ a param with shape torch.Size([7, 832]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([7, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 6656]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.q_proj.qzeros: copying\
          \ a param with shape torch.Size([7, 832]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([7, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 6656]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.v_proj.qzeros: copying\
          \ a param with shape torch.Size([7, 832]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([7, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 6656]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.mlp.down_proj.qzeros: copying a\
          \ param with shape torch.Size([18, 832]) from checkpoint, the shape in current\
          \ model is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.mlp.down_proj.scales: copying a\
          \ param with shape torch.Size([18, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 6656]).\n</code></pre>\n<p>Could it be\
          \ that the user config file does not take effect due to some reason?</p>\n"
        raw: "Hi @TheBloke unfortunately this did not change anything for me running\
          \ on a docker container like so:\n```\npevogam@prime:/mnt/local/notebooks/text-generation-webui>\
          \ head .env \n# by default the Dockerfile specifies these versions: 3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX\n\
          # however for me to work i had to specify the exact version for my card\
          \ ( 2060 ) it was 7.5\n# https://developer.nvidia.com/cuda-gpus you can\
          \ find the version for your card here\nTORCH_CUDA_ARCH_LIST=7.5\n\n# these\
          \ commands worked for me with roughly 4.5GB of vram\nCLI_ARGS=--model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
          \ --api --listen\n\npevogam@prime:/mnt/local/notebooks/text-generation-webui>\
          \ cat models/config-user.yaml \nTheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ$:\n\
          \  auto_devices: false\n  bf16: false\n  cpu: false\n  cpu_memory: 0\n \
          \ disk: false\n  gpu_memory_0: 0\n  groupsize: None\n  load_in_8bit: false\n\
          \  mlock: false\n  model_type: llama\n  n_batch: 512\n  n_gpu_layers: 0\n\
          \  pre_layer: 0\n  threads: 0\n  wbits: '4'\npevogam@prime:/mnt/local/notebooks/text-generation-webui>\
          \ docker compose up --build\n[+] Building 1.3s (37/37) FINISHED        \
          \                                                                      \
          \                                                                      \
          \                \n => [internal] load build definition from Dockerfile\
          \                                                                      \
          \                                                                      \
          \ 0.1s\n => => transferring dockerfile: 115B                           \
          \                                                                      \
          \                                                            0.0s\n => [internal]\
          \ load .dockerignore                                                   \
          \                                                                      \
          \                                       0.0s\n => => transferring context:\
          \ 123B                                                                 \
          \                                                                      \
          \                         0.0s\n => [internal] load metadata for docker.io/nvidia/cuda:11.8.0-runtime-ubuntu22.04\
          \                                                                      \
          \                                          0.9s\n => [internal] load metadata\
          \ for docker.io/nvidia/cuda:11.8.0-devel-ubuntu22.04                   \
          \                                                                      \
          \                         0.0s\n => [stage-1  1/24] FROM docker.io/nvidia/cuda:11.8.0-runtime-ubuntu22.04@sha256:9b9ce0e128463d147a58b5013255c60e7eb725141f37c197b1ddee5aeb7e4161\
          \                                                0.0s\n => [internal] load\
          \ build context                                                        \
          \                                                                      \
          \                                  0.0s\n => => transferring context: 5.00kB\
          \                                                                      \
          \                                                                      \
          \                  0.0s\n => [builder 1/7] FROM docker.io/nvidia/cuda:11.8.0-devel-ubuntu22.04\
          \                                                                      \
          \                                                      0.0s\n => CACHED\
          \ [stage-1  2/24] RUN apt-get update &&     apt-get install --no-install-recommends\
          \ -y libportaudio2 libasound-dev git python3 python3-pip make g++ &&   \
          \  rm -rf /var/lib/apt/lists/*  0.0s\n => CACHED [stage-1  3/24] RUN --mount=type=cache,target=/root/.cache/pip\
          \ pip3 install virtualenv                                              \
          \                                                  0.0s\n => CACHED [stage-1\
          \  4/24] RUN mkdir /app                                                \
          \                                                                      \
          \                                  0.0s\n => CACHED [stage-1  5/24] WORKDIR\
          \ /app                                                                 \
          \                                                                      \
          \                   0.0s\n => CACHED [stage-1  6/24] RUN test -n \"HEAD\"\
          \ && git reset --hard HEAD || echo \"Using provided webui source\"     \
          \                                                                      \
          \          0.0s\n => CACHED [stage-1  7/24] RUN virtualenv /app/venv   \
          \                                                                      \
          \                                                                     0.0s\n\
          \ => CACHED [stage-1  8/24] RUN . /app/venv/bin/activate &&     pip3 install\
          \ --upgrade pip setuptools &&     pip3 install torch torchvision torchaudio\
          \                                            0.0s\n => CACHED [builder 2/7]\
          \ RUN apt-get update &&     apt-get install --no-install-recommends -y git\
          \ vim build-essential python3-dev python3-venv &&     rm -rf /var/lib/apt/lists/*\
          \                0.0s\n => CACHED [builder 3/7] RUN git clone https://github.com/oobabooga/GPTQ-for-LLaMa\
          \ /build                                                               \
          \                                         0.0s\n => CACHED [builder 4/7]\
          \ WORKDIR /build                                                       \
          \                                                                      \
          \                             0.0s\n => CACHED [builder 5/7] RUN python3\
          \ -m venv /build/venv                                                  \
          \                                                                      \
          \                 0.0s\n => CACHED [builder 6/7] RUN . /build/venv/bin/activate\
          \ &&     pip3 install --upgrade pip setuptools &&     pip3 install torch\
          \ torchvision torchaudio &&     pip3 install -r requirements.txt    0.0s\n\
          \ => CACHED [builder 7/7] RUN . /build/venv/bin/activate &&     python3\
          \ setup_cuda.py bdist_wheel -d .                                       \
          \                                                     0.0s\n => CACHED [stage-1\
          \  9/24] COPY --from=builder /build /app/repositories/GPTQ-for-LLaMa   \
          \                                                                      \
          \                                  0.0s\n => CACHED [stage-1 10/24] RUN\
          \ . /app/venv/bin/activate &&     pip3 install /app/repositories/GPTQ-for-LLaMa/*.whl\
          \                                                                      \
          \         0.0s\n => CACHED [stage-1 11/24] COPY extensions/api/requirements.txt\
          \ /app/extensions/api/requirements.txt                                 \
          \                                                            0.0s\n => CACHED\
          \ [stage-1 12/24] COPY extensions/elevenlabs_tts/requirements.txt /app/extensions/elevenlabs_tts/requirements.txt\
          \                                                                      \
          \ 0.0s\n => CACHED [stage-1 13/24] COPY extensions/google_translate/requirements.txt\
          \ /app/extensions/google_translate/requirements.txt                    \
          \                                               0.0s\n => CACHED [stage-1\
          \ 14/24] COPY extensions/silero_tts/requirements.txt /app/extensions/silero_tts/requirements.txt\
          \                                                                      \
          \         0.0s\n => CACHED [stage-1 15/24] COPY extensions/whisper_stt/requirements.txt\
          \ /app/extensions/whisper_stt/requirements.txt                         \
          \                                                    0.0s\n => CACHED [stage-1\
          \ 16/24] RUN --mount=type=cache,target=/root/.cache/pip . /app/venv/bin/activate\
          \ && cd extensions/api && pip3 install -r requirements.txt             \
          \                         0.0s\n => CACHED [stage-1 17/24] RUN --mount=type=cache,target=/root/.cache/pip\
          \ . /app/venv/bin/activate && cd extensions/elevenlabs_tts && pip3 install\
          \ -r requirements.txt                           0.0s\n => CACHED [stage-1\
          \ 18/24] RUN --mount=type=cache,target=/root/.cache/pip . /app/venv/bin/activate\
          \ && cd extensions/google_translate && pip3 install -r requirements.txt\
          \                         0.0s\n => CACHED [stage-1 19/24] RUN --mount=type=cache,target=/root/.cache/pip\
          \ . /app/venv/bin/activate && cd extensions/silero_tts && pip3 install -r\
          \ requirements.txt                               0.0s\n => CACHED [stage-1\
          \ 20/24] RUN --mount=type=cache,target=/root/.cache/pip . /app/venv/bin/activate\
          \ && cd extensions/whisper_stt && pip3 install -r requirements.txt     \
          \                         0.0s\n => CACHED [stage-1 21/24] COPY requirements.txt\
          \ /app/requirements.txt                                                \
          \                                                                      \
          \     0.0s\n => CACHED [stage-1 22/24] RUN . /app/venv/bin/activate && \
          \    pip3 install -r requirements.txt                                  \
          \                                                                0.0s\n\
          \ => CACHED [stage-1 23/24] RUN cp /app/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\
          \ /app/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\
          \      0.0s\n => CACHED [stage-1 24/24] COPY . /app/                   \
          \                                                                      \
          \                                                                 0.0s\n\
          \ => exporting to image                                                \
          \                                                                      \
          \                                                     0.1s\n => => exporting\
          \ layers                                                               \
          \                                                                      \
          \                                     0.0s\n => => writing image sha256:cebe69eb2dd3d8126d46913e0c05e48da6b2a152d4bd2e77382fdfdeaf3994e2\
          \                                                                      \
          \                               0.0s\n => => naming to docker.io/library/text-generation-webui-text-generation-webui\
          \                                                                      \
          \                                             0.0s\n[+] Running 1/1\n \u2714\
          \ Container text-generation-webui-text-generation-webui-1  Recreated   \
          \                                                                      \
          \                                                   0.1s \nAttaching to\
          \ text-generation-webui-text-generation-webui-1\ntext-generation-webui-text-generation-webui-1\
          \  | \ntext-generation-webui-text-generation-webui-1  | ==========\ntext-generation-webui-text-generation-webui-1\
          \  | == CUDA ==\ntext-generation-webui-text-generation-webui-1  | ==========\n\
          text-generation-webui-text-generation-webui-1  | \ntext-generation-webui-text-generation-webui-1\
          \  | CUDA Version 11.8.0\ntext-generation-webui-text-generation-webui-1\
          \  | \ntext-generation-webui-text-generation-webui-1  | Container image\
          \ Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\
          text-generation-webui-text-generation-webui-1  | \ntext-generation-webui-text-generation-webui-1\
          \  | This container image and its contents are governed by the NVIDIA Deep\
          \ Learning Container License.\ntext-generation-webui-text-generation-webui-1\
          \  | By pulling and using the container, you accept the terms and conditions\
          \ of this license:\ntext-generation-webui-text-generation-webui-1  | https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\
          text-generation-webui-text-generation-webui-1  | \ntext-generation-webui-text-generation-webui-1\
          \  | A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE\
          \ for your convenience.\ntext-generation-webui-text-generation-webui-1 \
          \ | \ntext-generation-webui-text-generation-webui-1  | Gradio HTTP request\
          \ redirected to localhost :)\ntext-generation-webui-text-generation-webui-1\
          \  | bin /app/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n\
          text-generation-webui-text-generation-webui-1  | Loading TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ...\n\
          text-generation-webui-text-generation-webui-1  | Found the following quantized\
          \ model: models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors\n\
          text-generation-webui-text-generation-webui-1  | Loading model ...\ntext-generation-webui-text-generation-webui-1\
          \  | Traceback (most recent call last):\ntext-generation-webui-text-generation-webui-1\
          \  |   File \"/app/server.py\", line 918, in <module>\ntext-generation-webui-text-generation-webui-1\
          \  |     shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          text-generation-webui-text-generation-webui-1  |   File \"/app/modules/models.py\"\
          , line 150, in load_model\ntext-generation-webui-text-generation-webui-1\
          \  |     model = load_quantized(model_name)\ntext-generation-webui-text-generation-webui-1\
          \  |   File \"/app/modules/GPTQ_loader.py\", line 176, in load_quantized\n\
          text-generation-webui-text-generation-webui-1  |     model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          text-generation-webui-text-generation-webui-1  |   File \"/app/modules/GPTQ_loader.py\"\
          , line 77, in _load_quant\ntext-generation-webui-text-generation-webui-1\
          \  |     model.load_state_dict(safe_load(checkpoint), strict=False)\ntext-generation-webui-text-generation-webui-1\
          \  |   File \"/app/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\ntext-generation-webui-text-generation-webui-1\
          \  |     raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\\
          t{}'.format(\ntext-generation-webui-text-generation-webui-1  | RuntimeError:\
          \ Error(s) in loading state_dict for LlamaForCausalLM:\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.k_proj.qzeros: copying\
          \ a param with shape torch.Size([7, 832]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.k_proj.scales: copying\
          \ a param with shape torch.Size([7, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 6656]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.o_proj.qzeros: copying\
          \ a param with shape torch.Size([7, 832]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.o_proj.scales: copying\
          \ a param with shape torch.Size([7, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 6656]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.q_proj.qzeros: copying\
          \ a param with shape torch.Size([7, 832]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.q_proj.scales: copying\
          \ a param with shape torch.Size([7, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 6656]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.v_proj.qzeros: copying\
          \ a param with shape torch.Size([7, 832]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.self_attn.v_proj.scales: copying\
          \ a param with shape torch.Size([7, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 6656]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.mlp.down_proj.qzeros: copying a\
          \ param with shape torch.Size([18, 832]) from checkpoint, the shape in current\
          \ model is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1\
          \  | \tsize mismatch for model.layers.0.mlp.down_proj.scales: copying a\
          \ param with shape torch.Size([18, 6656]) from checkpoint, the shape in\
          \ current model is torch.Size([1, 6656]).\n```\nCould it be that the user\
          \ config file does not take effect due to some reason?"
        updatedAt: '2023-05-16T12:25:45.974Z'
      numEdits: 0
      reactions: []
    id: 646376491ac9eb9637cfa91f
    type: comment
  author: pevogam
  content: "Hi @TheBloke unfortunately this did not change anything for me running\
    \ on a docker container like so:\n```\npevogam@prime:/mnt/local/notebooks/text-generation-webui>\
    \ head .env \n# by default the Dockerfile specifies these versions: 3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX\n\
    # however for me to work i had to specify the exact version for my card ( 2060\
    \ ) it was 7.5\n# https://developer.nvidia.com/cuda-gpus you can find the version\
    \ for your card here\nTORCH_CUDA_ARCH_LIST=7.5\n\n# these commands worked for\
    \ me with roughly 4.5GB of vram\nCLI_ARGS=--model TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ\
    \ --api --listen\n\npevogam@prime:/mnt/local/notebooks/text-generation-webui>\
    \ cat models/config-user.yaml \nTheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ$:\n\
    \  auto_devices: false\n  bf16: false\n  cpu: false\n  cpu_memory: 0\n  disk:\
    \ false\n  gpu_memory_0: 0\n  groupsize: None\n  load_in_8bit: false\n  mlock:\
    \ false\n  model_type: llama\n  n_batch: 512\n  n_gpu_layers: 0\n  pre_layer:\
    \ 0\n  threads: 0\n  wbits: '4'\npevogam@prime:/mnt/local/notebooks/text-generation-webui>\
    \ docker compose up --build\n[+] Building 1.3s (37/37) FINISHED              \
    \                                                                            \
    \                                                                          \n\
    \ => [internal] load build definition from Dockerfile                        \
    \                                                                            \
    \                                         0.1s\n => => transferring dockerfile:\
    \ 115B                                                                       \
    \                                                                            \
    \          0.0s\n => [internal] load .dockerignore                           \
    \                                                                            \
    \                                                         0.0s\n => => transferring\
    \ context: 123B                                                              \
    \                                                                            \
    \                      0.0s\n => [internal] load metadata for docker.io/nvidia/cuda:11.8.0-runtime-ubuntu22.04\
    \                                                                            \
    \                                    0.9s\n => [internal] load metadata for docker.io/nvidia/cuda:11.8.0-devel-ubuntu22.04\
    \                                                                            \
    \                                      0.0s\n => [stage-1  1/24] FROM docker.io/nvidia/cuda:11.8.0-runtime-ubuntu22.04@sha256:9b9ce0e128463d147a58b5013255c60e7eb725141f37c197b1ddee5aeb7e4161\
    \                                                0.0s\n => [internal] load build\
    \ context                                                                    \
    \                                                                            \
    \                0.0s\n => => transferring context: 5.00kB                   \
    \                                                                            \
    \                                                               0.0s\n => [builder\
    \ 1/7] FROM docker.io/nvidia/cuda:11.8.0-devel-ubuntu22.04                   \
    \                                                                            \
    \                             0.0s\n => CACHED [stage-1  2/24] RUN apt-get update\
    \ &&     apt-get install --no-install-recommends -y libportaudio2 libasound-dev\
    \ git python3 python3-pip make g++ &&     rm -rf /var/lib/apt/lists/*  0.0s\n\
    \ => CACHED [stage-1  3/24] RUN --mount=type=cache,target=/root/.cache/pip pip3\
    \ install virtualenv                                                         \
    \                                       0.0s\n => CACHED [stage-1  4/24] RUN mkdir\
    \ /app                                                                       \
    \                                                                            \
    \     0.0s\n => CACHED [stage-1  5/24] WORKDIR /app                          \
    \                                                                            \
    \                                                    0.0s\n => CACHED [stage-1\
    \  6/24] RUN test -n \"HEAD\" && git reset --hard HEAD || echo \"Using provided\
    \ webui source\"                                                             \
    \                        0.0s\n => CACHED [stage-1  7/24] RUN virtualenv /app/venv\
    \                                                                            \
    \                                                                  0.0s\n => CACHED\
    \ [stage-1  8/24] RUN . /app/venv/bin/activate &&     pip3 install --upgrade pip\
    \ setuptools &&     pip3 install torch torchvision torchaudio                \
    \                            0.0s\n => CACHED [builder 2/7] RUN apt-get update\
    \ &&     apt-get install --no-install-recommends -y git vim build-essential python3-dev\
    \ python3-venv &&     rm -rf /var/lib/apt/lists/*                0.0s\n => CACHED\
    \ [builder 3/7] RUN git clone https://github.com/oobabooga/GPTQ-for-LLaMa /build\
    \                                                                            \
    \                            0.0s\n => CACHED [builder 4/7] WORKDIR /build   \
    \                                                                            \
    \                                                                           0.0s\n\
    \ => CACHED [builder 5/7] RUN python3 -m venv /build/venv                    \
    \                                                                            \
    \                                         0.0s\n => CACHED [builder 6/7] RUN .\
    \ /build/venv/bin/activate &&     pip3 install --upgrade pip setuptools &&   \
    \  pip3 install torch torchvision torchaudio &&     pip3 install -r requirements.txt\
    \    0.0s\n => CACHED [builder 7/7] RUN . /build/venv/bin/activate &&     python3\
    \ setup_cuda.py bdist_wheel -d .                                             \
    \                                               0.0s\n => CACHED [stage-1  9/24]\
    \ COPY --from=builder /build /app/repositories/GPTQ-for-LLaMa                \
    \                                                                            \
    \               0.0s\n => CACHED [stage-1 10/24] RUN . /app/venv/bin/activate\
    \ &&     pip3 install /app/repositories/GPTQ-for-LLaMa/*.whl                 \
    \                                                              0.0s\n => CACHED\
    \ [stage-1 11/24] COPY extensions/api/requirements.txt /app/extensions/api/requirements.txt\
    \                                                                            \
    \                 0.0s\n => CACHED [stage-1 12/24] COPY extensions/elevenlabs_tts/requirements.txt\
    \ /app/extensions/elevenlabs_tts/requirements.txt                            \
    \                                           0.0s\n => CACHED [stage-1 13/24] COPY\
    \ extensions/google_translate/requirements.txt /app/extensions/google_translate/requirements.txt\
    \                                                                   0.0s\n =>\
    \ CACHED [stage-1 14/24] COPY extensions/silero_tts/requirements.txt /app/extensions/silero_tts/requirements.txt\
    \                                                                            \
    \   0.0s\n => CACHED [stage-1 15/24] COPY extensions/whisper_stt/requirements.txt\
    \ /app/extensions/whisper_stt/requirements.txt                               \
    \                                              0.0s\n => CACHED [stage-1 16/24]\
    \ RUN --mount=type=cache,target=/root/.cache/pip . /app/venv/bin/activate && cd\
    \ extensions/api && pip3 install -r requirements.txt                         \
    \             0.0s\n => CACHED [stage-1 17/24] RUN --mount=type=cache,target=/root/.cache/pip\
    \ . /app/venv/bin/activate && cd extensions/elevenlabs_tts && pip3 install -r\
    \ requirements.txt                           0.0s\n => CACHED [stage-1 18/24]\
    \ RUN --mount=type=cache,target=/root/.cache/pip . /app/venv/bin/activate && cd\
    \ extensions/google_translate && pip3 install -r requirements.txt            \
    \             0.0s\n => CACHED [stage-1 19/24] RUN --mount=type=cache,target=/root/.cache/pip\
    \ . /app/venv/bin/activate && cd extensions/silero_tts && pip3 install -r requirements.txt\
    \                               0.0s\n => CACHED [stage-1 20/24] RUN --mount=type=cache,target=/root/.cache/pip\
    \ . /app/venv/bin/activate && cd extensions/whisper_stt && pip3 install -r requirements.txt\
    \                              0.0s\n => CACHED [stage-1 21/24] COPY requirements.txt\
    \ /app/requirements.txt                                                      \
    \                                                                     0.0s\n =>\
    \ CACHED [stage-1 22/24] RUN . /app/venv/bin/activate &&     pip3 install -r requirements.txt\
    \                                                                            \
    \                      0.0s\n => CACHED [stage-1 23/24] RUN cp /app/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\
    \ /app/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so \
    \     0.0s\n => CACHED [stage-1 24/24] COPY . /app/                          \
    \                                                                            \
    \                                                    0.0s\n => exporting to image\
    \                                                                            \
    \                                                                            \
    \                   0.1s\n => => exporting layers                            \
    \                                                                            \
    \                                                                  0.0s\n => =>\
    \ writing image sha256:cebe69eb2dd3d8126d46913e0c05e48da6b2a152d4bd2e77382fdfdeaf3994e2\
    \                                                                            \
    \                         0.0s\n => => naming to docker.io/library/text-generation-webui-text-generation-webui\
    \                                                                            \
    \                                       0.0s\n[+] Running 1/1\n \u2714 Container\
    \ text-generation-webui-text-generation-webui-1  Recreated                   \
    \                                                                            \
    \                             0.1s \nAttaching to text-generation-webui-text-generation-webui-1\n\
    text-generation-webui-text-generation-webui-1  | \ntext-generation-webui-text-generation-webui-1\
    \  | ==========\ntext-generation-webui-text-generation-webui-1  | == CUDA ==\n\
    text-generation-webui-text-generation-webui-1  | ==========\ntext-generation-webui-text-generation-webui-1\
    \  | \ntext-generation-webui-text-generation-webui-1  | CUDA Version 11.8.0\n\
    text-generation-webui-text-generation-webui-1  | \ntext-generation-webui-text-generation-webui-1\
    \  | Container image Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES.\
    \ All rights reserved.\ntext-generation-webui-text-generation-webui-1  | \ntext-generation-webui-text-generation-webui-1\
    \  | This container image and its contents are governed by the NVIDIA Deep Learning\
    \ Container License.\ntext-generation-webui-text-generation-webui-1  | By pulling\
    \ and using the container, you accept the terms and conditions of this license:\n\
    text-generation-webui-text-generation-webui-1  | https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\
    text-generation-webui-text-generation-webui-1  | \ntext-generation-webui-text-generation-webui-1\
    \  | A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE\
    \ for your convenience.\ntext-generation-webui-text-generation-webui-1  | \ntext-generation-webui-text-generation-webui-1\
    \  | Gradio HTTP request redirected to localhost :)\ntext-generation-webui-text-generation-webui-1\
    \  | bin /app/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n\
    text-generation-webui-text-generation-webui-1  | Loading TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ...\n\
    text-generation-webui-text-generation-webui-1  | Found the following quantized\
    \ model: models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ/OpenAssistant-30B-epoch7-GPTQ-4bit-1024g.compat.no-act-order.safetensors\n\
    text-generation-webui-text-generation-webui-1  | Loading model ...\ntext-generation-webui-text-generation-webui-1\
    \  | Traceback (most recent call last):\ntext-generation-webui-text-generation-webui-1\
    \  |   File \"/app/server.py\", line 918, in <module>\ntext-generation-webui-text-generation-webui-1\
    \  |     shared.model, shared.tokenizer = load_model(shared.model_name)\ntext-generation-webui-text-generation-webui-1\
    \  |   File \"/app/modules/models.py\", line 150, in load_model\ntext-generation-webui-text-generation-webui-1\
    \  |     model = load_quantized(model_name)\ntext-generation-webui-text-generation-webui-1\
    \  |   File \"/app/modules/GPTQ_loader.py\", line 176, in load_quantized\ntext-generation-webui-text-generation-webui-1\
    \  |     model = load_quant(str(path_to_model), str(pt_path), shared.args.wbits,\
    \ shared.args.groupsize, kernel_switch_threshold=threshold)\ntext-generation-webui-text-generation-webui-1\
    \  |   File \"/app/modules/GPTQ_loader.py\", line 77, in _load_quant\ntext-generation-webui-text-generation-webui-1\
    \  |     model.load_state_dict(safe_load(checkpoint), strict=False)\ntext-generation-webui-text-generation-webui-1\
    \  |   File \"/app/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 2041, in load_state_dict\ntext-generation-webui-text-generation-webui-1\
    \  |     raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\
    text-generation-webui-text-generation-webui-1  | RuntimeError: Error(s) in loading\
    \ state_dict for LlamaForCausalLM:\ntext-generation-webui-text-generation-webui-1\
    \  | \tsize mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param\
    \ with shape torch.Size([7, 832]) from checkpoint, the shape in current model\
    \ is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1  | \t\
    size mismatch for model.layers.0.self_attn.k_proj.scales: copying a param with\
    \ shape torch.Size([7, 6656]) from checkpoint, the shape in current model is torch.Size([1,\
    \ 6656]).\ntext-generation-webui-text-generation-webui-1  | \tsize mismatch for\
    \ model.layers.0.self_attn.o_proj.qzeros: copying a param with shape torch.Size([7,\
    \ 832]) from checkpoint, the shape in current model is torch.Size([1, 832]).\n\
    text-generation-webui-text-generation-webui-1  | \tsize mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([7, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 6656]).\ntext-generation-webui-text-generation-webui-1\
    \  | \tsize mismatch for model.layers.0.self_attn.q_proj.qzeros: copying a param\
    \ with shape torch.Size([7, 832]) from checkpoint, the shape in current model\
    \ is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1  | \t\
    size mismatch for model.layers.0.self_attn.q_proj.scales: copying a param with\
    \ shape torch.Size([7, 6656]) from checkpoint, the shape in current model is torch.Size([1,\
    \ 6656]).\ntext-generation-webui-text-generation-webui-1  | \tsize mismatch for\
    \ model.layers.0.self_attn.v_proj.qzeros: copying a param with shape torch.Size([7,\
    \ 832]) from checkpoint, the shape in current model is torch.Size([1, 832]).\n\
    text-generation-webui-text-generation-webui-1  | \tsize mismatch for model.layers.0.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([7, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 6656]).\ntext-generation-webui-text-generation-webui-1\
    \  | \tsize mismatch for model.layers.0.mlp.down_proj.qzeros: copying a param\
    \ with shape torch.Size([18, 832]) from checkpoint, the shape in current model\
    \ is torch.Size([1, 832]).\ntext-generation-webui-text-generation-webui-1  | \t\
    size mismatch for model.layers.0.mlp.down_proj.scales: copying a param with shape\
    \ torch.Size([18, 6656]) from checkpoint, the shape in current model is torch.Size([1,\
    \ 6656]).\n```\nCould it be that the user config file does not take effect due\
    \ to some reason?"
  created_at: 2023-05-16 11:25:45+00:00
  edited: false
  hidden: false
  id: 646376491ac9eb9637cfa91f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-16T12:28:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You''re running the 1024g model, so either change to using the model
          in <code>main</code>, or else change:</p>

          <pre><code> groupsize: None

          </code></pre>

          <p>to:</p>

          <pre><code> groupsize: 1024

          </code></pre>

          <p>in the .yaml file.</p>

          '
        raw: "You're running the 1024g model, so either change to using the model\
          \ in `main`, or else change:\n```\n groupsize: None\n```\nto:\n```\n groupsize:\
          \ 1024\n```\nin the .yaml file."
        updatedAt: '2023-05-16T12:28:20.162Z'
      numEdits: 0
      reactions: []
    id: 646376e412814d7541795d7b
    type: comment
  author: TheBloke
  content: "You're running the 1024g model, so either change to using the model in\
    \ `main`, or else change:\n```\n groupsize: None\n```\nto:\n```\n groupsize: 1024\n\
    ```\nin the .yaml file."
  created_at: 2023-05-16 11:28:20+00:00
  edited: false
  hidden: false
  id: 646376e412814d7541795d7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-05-16T13:14:39.000Z'
    data:
      edited: false
      editors:
      - pevogam
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
          fullname: Plamen Dimitrov
          isHf: false
          isPro: false
          name: pevogam
          type: user
        html: '<p>Ah good catch, thanks a lot! Problem is indeed fixed.</p>

          '
        raw: Ah good catch, thanks a lot! Problem is indeed fixed.
        updatedAt: '2023-05-16T13:14:39.443Z'
      numEdits: 0
      reactions: []
      relatedEventId: 646381bfab15db2fa5687a38
    id: 646381bfab15db2fa5687a37
    type: comment
  author: pevogam
  content: Ah good catch, thanks a lot! Problem is indeed fixed.
  created_at: 2023-05-16 12:14:39+00:00
  edited: false
  hidden: false
  id: 646381bfab15db2fa5687a37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-05-16T13:14:39.000Z'
    data:
      status: closed
    id: 646381bfab15db2fa5687a38
    type: status-change
  author: pevogam
  created_at: 2023-05-16 12:14:39+00:00
  id: 646381bfab15db2fa5687a38
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ
repo_type: model
status: closed
target_branch: null
title: Mismatch in tensor dimensions
