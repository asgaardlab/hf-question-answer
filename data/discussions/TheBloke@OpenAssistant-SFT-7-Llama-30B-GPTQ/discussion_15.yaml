!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pevogam
conflicting_files: null
created_at: 2023-10-03 20:13:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-10-03T21:13:33.000Z'
    data:
      edited: false
      editors:
      - pevogam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5089067220687866
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
          fullname: Plamen Dimitrov
          isHf: false
          isPro: false
          name: pevogam
          type: user
        html: "<p>Hi, the model has worked greatly before but I think a recent version\
          \ of the <code>transformers</code> library has a problem (possibly with\
          \ auto-gptq) that brings to infinite recursion:</p>\n<pre><code>  File \"\
          /usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1229, in unk_token_id\n    return self.convert_tokens_to_ids(self.unk_token)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 297, in convert_tokens_to_ids\n    return self._convert_token_to_id_with_added_voc(tokens)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
          /usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 304, in _convert_token_to_id_with_added_voc\n    return self.unk_token_id\n\
          \           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1229, in unk_token_id\n    return self.convert_tokens_to_ids(self.unk_token)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 297, in convert_tokens_to_ids\n    return self._convert_token_to_id_with_added_voc(tokens)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
          /usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 304, in _convert_token_to_id_with_added_voc\n    return self.unk_token_id\n\
          \           ^^^^^^^^^^^^^^^^^\n</code></pre>\n<p>It can be fixed if I add\
          \ the following arguments:</p>\n<pre><code class=\"language-diff\">quantized_model_dir\
          \ = \"/app/models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ_gptq-4bit-32g-actorder_True\"\
          \n<span class=\"hljs-deletion\">-tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)</span>\n<span class=\"hljs-addition\">+tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True,</span>\n<span class=\"hljs-addition\">+               \
          \                           unk_token=\"&lt;unk&gt;\", bos_token=\"&lt;s&gt;\"\
          , eos_token=\"&lt;/s&gt;\")</span>\n</code></pre>\n<p>I am using the most\
          \ recent version of all dependencies including auto-gptq:</p>\n<pre><code>root@dfe9ff3a5c15:/app#\
          \ pip show auto-gptq\nName: auto-gptq\nVersion: 0.4.2\nSummary: An easy-to-use\
          \ LLMs quantization package with user-friendly apis, based on GPTQ algorithm.\n\
          Home-page: https://github.com/PanQiWei/AutoGPTQ\nAuthor: PanQiWei\nAuthor-email:\
          \ \nLicense: \nLocation: /usr/local/lib/python3.11/site-packages\nRequires:\
          \ accelerate, datasets, numpy, peft, rouge, safetensors, torch, transformers\n\
          Required-by: \n</code></pre>\n<p>so I doubt it is caused by a choice of\
          \ wrong version.</p>\n"
        raw: "Hi, the model has worked greatly before but I think a recent version\
          \ of the `transformers` library has a problem (possibly with auto-gptq)\
          \ that brings to infinite recursion:\r\n```\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1229, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 297, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\
          \ \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 304, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\
          \n           ^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\"\
          , line 1229, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 297, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\
          \ \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\"\
          , line 304, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\
          \n           ^^^^^^^^^^^^^^^^^\r\n```\r\nIt can be fixed if I add the following\
          \ arguments:\r\n```diff\r\nquantized_model_dir = \"/app/models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ_gptq-4bit-32g-actorder_True\"\
          \r\n-tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)\r\
          \n+tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True,\r\
          \n+                                          unk_token=\"<unk>\", bos_token=\"\
          <s>\", eos_token=\"</s>\")\r\n```\r\nI am using the most recent version\
          \ of all dependencies including auto-gptq:\r\n```\r\nroot@dfe9ff3a5c15:/app#\
          \ pip show auto-gptq\r\nName: auto-gptq\r\nVersion: 0.4.2\r\nSummary: An\
          \ easy-to-use LLMs quantization package with user-friendly apis, based on\
          \ GPTQ algorithm.\r\nHome-page: https://github.com/PanQiWei/AutoGPTQ\r\n\
          Author: PanQiWei\r\nAuthor-email: \r\nLicense: \r\nLocation: /usr/local/lib/python3.11/site-packages\r\
          \nRequires: accelerate, datasets, numpy, peft, rouge, safetensors, torch,\
          \ transformers\r\nRequired-by: \r\n```\r\nso I doubt it is caused by a choice\
          \ of wrong version."
        updatedAt: '2023-10-03T21:13:33.451Z'
      numEdits: 0
      reactions: []
    id: 651c83fda82dd4695109d060
    type: comment
  author: pevogam
  content: "Hi, the model has worked greatly before but I think a recent version of\
    \ the `transformers` library has a problem (possibly with auto-gptq) that brings\
    \ to infinite recursion:\r\n```\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1229, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 297, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 304, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\
    \n           ^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\"\
    , line 1229, in unk_token_id\r\n    return self.convert_tokens_to_ids(self.unk_token)\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 297, in convert_tokens_to_ids\r\n    return self._convert_token_to_id_with_added_voc(tokens)\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\"\
    , line 304, in _convert_token_to_id_with_added_voc\r\n    return self.unk_token_id\r\
    \n           ^^^^^^^^^^^^^^^^^\r\n```\r\nIt can be fixed if I add the following\
    \ arguments:\r\n```diff\r\nquantized_model_dir = \"/app/models/TheBloke_OpenAssistant-SFT-7-Llama-30B-GPTQ_gptq-4bit-32g-actorder_True\"\
    \r\n-tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)\r\
    \n+tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True,\r\
    \n+                                          unk_token=\"<unk>\", bos_token=\"\
    <s>\", eos_token=\"</s>\")\r\n```\r\nI am using the most recent version of all\
    \ dependencies including auto-gptq:\r\n```\r\nroot@dfe9ff3a5c15:/app# pip show\
    \ auto-gptq\r\nName: auto-gptq\r\nVersion: 0.4.2\r\nSummary: An easy-to-use LLMs\
    \ quantization package with user-friendly apis, based on GPTQ algorithm.\r\nHome-page:\
    \ https://github.com/PanQiWei/AutoGPTQ\r\nAuthor: PanQiWei\r\nAuthor-email: \r\
    \nLicense: \r\nLocation: /usr/local/lib/python3.11/site-packages\r\nRequires:\
    \ accelerate, datasets, numpy, peft, rouge, safetensors, torch, transformers\r\
    \nRequired-by: \r\n```\r\nso I doubt it is caused by a choice of wrong version."
  created_at: 2023-10-03 20:13:33+00:00
  edited: false
  hidden: false
  id: 651c83fda82dd4695109d060
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-03T21:16:00.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7025653719902039
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ah yeah, this is a result of a Transformers change - it no longer
          accepts empty values in <code>special_tokens_map.json</code>.</p>

          <p>I just applied a fix, please re-download and test again.</p>

          '
        raw: 'Ah yeah, this is a result of a Transformers change - it no longer accepts
          empty values in `special_tokens_map.json`.


          I just applied a fix, please re-download and test again.'
        updatedAt: '2023-10-03T21:16:00.469Z'
      numEdits: 0
      reactions: []
    id: 651c8490830ddd7cb7249572
    type: comment
  author: TheBloke
  content: 'Ah yeah, this is a result of a Transformers change - it no longer accepts
    empty values in `special_tokens_map.json`.


    I just applied a fix, please re-download and test again.'
  created_at: 2023-10-03 20:16:00+00:00
  edited: false
  hidden: false
  id: 651c8490830ddd7cb7249572
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-10-07T08:16:50.000Z'
    data:
      edited: false
      editors:
      - pevogam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9908202886581421
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
          fullname: Plamen Dimitrov
          isHf: false
          isPro: false
          name: pevogam
          type: user
        html: '<p>Alright, I applied the same change as in the diff of your commit
          and it works now, thanks and closing this.</p>

          '
        raw: Alright, I applied the same change as in the diff of your commit and
          it works now, thanks and closing this.
        updatedAt: '2023-10-07T08:16:50.074Z'
      numEdits: 0
      reactions: []
      relatedEventId: 652113f2f6ceb915cc72bfb2
    id: 652113f2f6ceb915cc72bfaa
    type: comment
  author: pevogam
  content: Alright, I applied the same change as in the diff of your commit and it
    works now, thanks and closing this.
  created_at: 2023-10-07 07:16:50+00:00
  edited: false
  hidden: false
  id: 652113f2f6ceb915cc72bfaa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632804fcbfc72a1d59d03bf1/C2VfwbAnkikqUCgbc__Vd.jpeg?w=200&h=200&f=face
      fullname: Plamen Dimitrov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pevogam
      type: user
    createdAt: '2023-10-07T08:16:50.000Z'
    data:
      status: closed
    id: 652113f2f6ceb915cc72bfb2
    type: status-change
  author: pevogam
  created_at: 2023-10-07 07:16:50+00:00
  id: 652113f2f6ceb915cc72bfb2
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ
repo_type: model
status: closed
target_branch: null
title: Infinite recursion unless we provide some special tokens
