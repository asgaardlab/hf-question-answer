!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kurapika993
conflicting_files: null
created_at: 2023-05-10 16:45:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
      fullname: Mayukh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kurapika993
      type: user
    createdAt: '2023-05-10T17:45:54.000Z'
    data:
      edited: false
      editors:
      - Kurapika993
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
          fullname: Mayukh
          isHf: false
          isPro: false
          name: Kurapika993
          type: user
        html: '<p>On trying to download the model raises the below exception.. </p>

          <p>Exception                                 Traceback (most recent call
          last)<br>Input In [35], in &lt;cell line: 3&gt;()<br>      1 from transformers
          import AutoTokenizer, AutoModelForCausalLM<br>----&gt; 3 tokenizer = AutoTokenizer.from_pretrained("TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ")<br>      5
          model = AutoModelForCausalLM.from_pretrained("TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ")</p>

          <p>File ~/Environment/default/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:702,
          in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,
          **kwargs)<br>    698     if tokenizer_class is None:<br>    699         raise
          ValueError(<br>    700             f"Tokenizer class {tokenizer_class_candidate}
          does not exist or is not currently imported."<br>    701         )<br>--&gt;
          702     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,
          *inputs, **kwargs)<br>    704 # Otherwise we have to be creative.<br>    705
          # if model is an encoder decoder, the encoder tokenizer class is used by
          default<br>    706 if isinstance(config, EncoderDecoderConfig):</p>

          <p>File ~/Environment/default/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1811,
          in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,
          *init_inputs, **kwargs)<br>   1808     else:<br>   1809         logger.info(f"loading
          file {file_path} from cache at {resolved_vocab_files[file_id]}")<br>-&gt;
          1811 return cls._from_pretrained(<br>   1812     resolved_vocab_files,<br>   1813     pretrained_model_name_or_path,<br>   1814     init_configuration,<br>   1815     *init_inputs,<br>   1816     use_auth_token=use_auth_token,<br>   1817     cache_dir=cache_dir,<br>   1818     local_files_only=local_files_only,<br>   1819     _commit_hash=commit_hash,<br>   1820     **kwargs,<br>   1821
          )</p>

          <p>File ~/Environment/default/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1965,
          in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,
          init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,
          *init_inputs, **kwargs)<br>   1963 # Instantiate tokenizer.<br>   1964 try:<br>-&gt;
          1965     tokenizer = cls(*init_inputs, **init_kwargs)<br>   1966 except
          OSError:<br>   1967     raise OSError(<br>   1968         "Unable to load
          vocabulary from file. "<br>   1969         "Please check that the provided
          vocabulary is accessible and not corrupted."<br>   1970     )</p>

          <p>File ~/Environment/default/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py:89,
          in LlamaTokenizerFast.<strong>init</strong>(self, vocab_file, tokenizer_file,
          clean_up_tokenization_spaces, unk_token, bos_token, eos_token, **kwargs)<br>     79
          def <strong>init</strong>(<br>     80     self,<br>     81     vocab_file=None,<br>   (...)<br>     87     **kwargs,<br>     88
          ):<br>---&gt; 89     super().<strong>init</strong>(<br>     90         vocab_file=vocab_file,<br>     91         tokenizer_file=tokenizer_file,<br>     92         clean_up_tokenization_spaces=clean_up_tokenization_spaces,<br>     93         unk_token=unk_token,<br>     94         bos_token=bos_token,<br>     95         eos_token=eos_token,<br>     96         **kwargs,<br>     97     )<br>     99     self.vocab_file
          = vocab_file<br>    100     self.can_save_slow_tokenizer = False if not
          self.vocab_file else True</p>

          <p>File ~/Environment/default/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:111,
          in PreTrainedTokenizerFast.<strong>init</strong>(self, *args, **kwargs)<br>    108     fast_tokenizer
          = copy.deepcopy(tokenizer_object)<br>    109 elif fast_tokenizer_file is
          not None and not from_slow:<br>    110     # We have a serialization from
          tokenizers which let us directly build the backend<br>--&gt; 111     fast_tokenizer
          = TokenizerFast.from_file(fast_tokenizer_file)<br>    112 elif slow_tokenizer
          is not None:<br>    113     # We need to convert a slow tokenizer to build
          the backend<br>    114     fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)</p>

          <p>Exception: data did not match any variant of untagged enum PyNormalizerTypeWrapper
          at line 94 column 3</p>

          '
        raw: "On trying to download the model raises the below exception.. \r\n\r\n\
          \r\nException                                 Traceback (most recent call\
          \ last)\r\nInput In [35], in <cell line: 3>()\r\n      1 from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\n----> 3 tokenizer = AutoTokenizer.from_pretrained(\"\
          TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\")\r\n      5 model = AutoModelForCausalLM.from_pretrained(\"\
          TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\")\r\n\r\nFile ~/Environment/default/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:702,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\r\n    698     if tokenizer_class is None:\r\n    699      \
          \   raise ValueError(\r\n    700             f\"Tokenizer class {tokenizer_class_candidate}\
          \ does not exist or is not currently imported.\"\r\n    701         )\r\n\
          --> 702     return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    704 # Otherwise we have to be creative.\r\n\
          \    705 # if model is an encoder decoder, the encoder tokenizer class is\
          \ used by default\r\n    706 if isinstance(config, EncoderDecoderConfig):\r\
          \n\r\nFile ~/Environment/default/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1811,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *init_inputs, **kwargs)\r\n   1808     else:\r\n   1809         logger.info(f\"\
          loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\
          )\r\n-> 1811 return cls._from_pretrained(\r\n   1812     resolved_vocab_files,\r\
          \n   1813     pretrained_model_name_or_path,\r\n   1814     init_configuration,\r\
          \n   1815     *init_inputs,\r\n   1816     use_auth_token=use_auth_token,\r\
          \n   1817     cache_dir=cache_dir,\r\n   1818     local_files_only=local_files_only,\r\
          \n   1819     _commit_hash=commit_hash,\r\n   1820     **kwargs,\r\n   1821\
          \ )\r\n\r\nFile ~/Environment/default/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1965,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir,\
          \ local_files_only, _commit_hash, *init_inputs, **kwargs)\r\n   1963 # Instantiate\
          \ tokenizer.\r\n   1964 try:\r\n-> 1965     tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\r\n   1966 except OSError:\r\n   1967     raise OSError(\r\
          \n   1968         \"Unable to load vocabulary from file. \"\r\n   1969 \
          \        \"Please check that the provided vocabulary is accessible and not\
          \ corrupted.\"\r\n   1970     )\r\n\r\nFile ~/Environment/default/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py:89,\
          \ in LlamaTokenizerFast.__init__(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces,\
          \ unk_token, bos_token, eos_token, **kwargs)\r\n     79 def __init__(\r\n\
          \     80     self,\r\n     81     vocab_file=None,\r\n   (...)\r\n     87\
          \     **kwargs,\r\n     88 ):\r\n---> 89     super().__init__(\r\n     90\
          \         vocab_file=vocab_file,\r\n     91         tokenizer_file=tokenizer_file,\r\
          \n     92         clean_up_tokenization_spaces=clean_up_tokenization_spaces,\r\
          \n     93         unk_token=unk_token,\r\n     94         bos_token=bos_token,\r\
          \n     95         eos_token=eos_token,\r\n     96         **kwargs,\r\n\
          \     97     )\r\n     99     self.vocab_file = vocab_file\r\n    100  \
          \   self.can_save_slow_tokenizer = False if not self.vocab_file else True\r\
          \n\r\nFile ~/Environment/default/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:111,\
          \ in PreTrainedTokenizerFast.__init__(self, *args, **kwargs)\r\n    108\
          \     fast_tokenizer = copy.deepcopy(tokenizer_object)\r\n    109 elif fast_tokenizer_file\
          \ is not None and not from_slow:\r\n    110     # We have a serialization\
          \ from tokenizers which let us directly build the backend\r\n--> 111   \
          \  fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\r\n   \
          \ 112 elif slow_tokenizer is not None:\r\n    113     # We need to convert\
          \ a slow tokenizer to build the backend\r\n    114     fast_tokenizer =\
          \ convert_slow_tokenizer(slow_tokenizer)\r\n\r\nException: data did not\
          \ match any variant of untagged enum PyNormalizerTypeWrapper at line 94\
          \ column 3"
        updatedAt: '2023-05-10T17:45:54.922Z'
      numEdits: 0
      reactions: []
    id: 645bd852bc7518912e226721
    type: comment
  author: Kurapika993
  content: "On trying to download the model raises the below exception.. \r\n\r\n\r\
    \nException                                 Traceback (most recent call last)\r\
    \nInput In [35], in <cell line: 3>()\r\n      1 from transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\r\n----> 3 tokenizer = AutoTokenizer.from_pretrained(\"\
    TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\")\r\n      5 model = AutoModelForCausalLM.from_pretrained(\"\
    TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\")\r\n\r\nFile ~/Environment/default/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:702,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    698     if tokenizer_class is None:\r\n    699         raise\
    \ ValueError(\r\n    700             f\"Tokenizer class {tokenizer_class_candidate}\
    \ does not exist or is not currently imported.\"\r\n    701         )\r\n--> 702\
    \     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    704 # Otherwise we have to be creative.\r\n    705 # if model\
    \ is an encoder decoder, the encoder tokenizer class is used by default\r\n  \
    \  706 if isinstance(config, EncoderDecoderConfig):\r\n\r\nFile ~/Environment/default/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1811,\
    \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
    \ *init_inputs, **kwargs)\r\n   1808     else:\r\n   1809         logger.info(f\"\
    loading file {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n\
    -> 1811 return cls._from_pretrained(\r\n   1812     resolved_vocab_files,\r\n\
    \   1813     pretrained_model_name_or_path,\r\n   1814     init_configuration,\r\
    \n   1815     *init_inputs,\r\n   1816     use_auth_token=use_auth_token,\r\n\
    \   1817     cache_dir=cache_dir,\r\n   1818     local_files_only=local_files_only,\r\
    \n   1819     _commit_hash=commit_hash,\r\n   1820     **kwargs,\r\n   1821 )\r\
    \n\r\nFile ~/Environment/default/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1965,\
    \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
    \ *init_inputs, **kwargs)\r\n   1963 # Instantiate tokenizer.\r\n   1964 try:\r\
    \n-> 1965     tokenizer = cls(*init_inputs, **init_kwargs)\r\n   1966 except OSError:\r\
    \n   1967     raise OSError(\r\n   1968         \"Unable to load vocabulary from\
    \ file. \"\r\n   1969         \"Please check that the provided vocabulary is accessible\
    \ and not corrupted.\"\r\n   1970     )\r\n\r\nFile ~/Environment/default/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py:89,\
    \ in LlamaTokenizerFast.__init__(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces,\
    \ unk_token, bos_token, eos_token, **kwargs)\r\n     79 def __init__(\r\n    \
    \ 80     self,\r\n     81     vocab_file=None,\r\n   (...)\r\n     87     **kwargs,\r\
    \n     88 ):\r\n---> 89     super().__init__(\r\n     90         vocab_file=vocab_file,\r\
    \n     91         tokenizer_file=tokenizer_file,\r\n     92         clean_up_tokenization_spaces=clean_up_tokenization_spaces,\r\
    \n     93         unk_token=unk_token,\r\n     94         bos_token=bos_token,\r\
    \n     95         eos_token=eos_token,\r\n     96         **kwargs,\r\n     97\
    \     )\r\n     99     self.vocab_file = vocab_file\r\n    100     self.can_save_slow_tokenizer\
    \ = False if not self.vocab_file else True\r\n\r\nFile ~/Environment/default/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:111,\
    \ in PreTrainedTokenizerFast.__init__(self, *args, **kwargs)\r\n    108     fast_tokenizer\
    \ = copy.deepcopy(tokenizer_object)\r\n    109 elif fast_tokenizer_file is not\
    \ None and not from_slow:\r\n    110     # We have a serialization from tokenizers\
    \ which let us directly build the backend\r\n--> 111     fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\r\
    \n    112 elif slow_tokenizer is not None:\r\n    113     # We need to convert\
    \ a slow tokenizer to build the backend\r\n    114     fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\r\
    \n\r\nException: data did not match any variant of untagged enum PyNormalizerTypeWrapper\
    \ at line 94 column 3"
  created_at: 2023-05-10 16:45:54+00:00
  edited: false
  hidden: false
  id: 645bd852bc7518912e226721
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-10T20:14:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>That's a very strange error. I'm not sure what's going on.  The\
          \ tokenizer line should work fine, like so:</p>\n<pre><code>Python 3.10.11\
          \ (main, Apr  5 2023, 14:15:10) [GCC 9.4.0] on linux\nType \"help\", \"\
          copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\
          \ from transformers import AutoTokenizer\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"\
          TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\")\nDownloading (\u2026)okenizer_config.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 715/715 [00:00&lt;00:00, 3.42MB/s]\nDownloading tokenizer.model: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 500k/500k [00:00&lt;00:00, 3.88MB/s]\nDownloading\
          \ (\u2026)/main/tokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 1.84M/1.84M [00:00&lt;00:00, 3.96MB/s]\nDownloading (\u2026)in/added_tokens.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588| 133/133 [00:00&lt;00:00, 843kB/s]\nDownloading (\u2026)cial_tokens_map.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 477/477 [00:00&lt;00:00, 2.79MB/s]\n&gt;&gt;&gt; print(tokenizer)\nLlamaTokenizerFast(name_or_path='TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ',\
          \ vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True,\
          \ padding_side='left', truncation_side='right', special_tokens={'bos_token':\
          \ AddedToken(\"\", rstrip=False, lstrip=False, single_word=False, normalized=True),\
          \ 'eos_token': '&lt;/s&gt;', 'unk_token': AddedToken(\"\", rstrip=False,\
          \ lstrip=False, single_word=False, normalized=True), 'sep_token': '&lt;s&gt;',\
          \ 'pad_token': '&lt;/s&gt;', 'additional_special_tokens': ['&lt;|prompter|&gt;',\
          \ '&lt;|system|&gt;', '&lt;|prefix_begin|&gt;', '&lt;|prefix_end|&gt;',\
          \ '&lt;|assistant|&gt;']}, clean_up_tokenization_spaces=False)\n&gt;&gt;&gt;\n\
          </code></pre>\n<p>All I can suggest is to clear out your local Hugging Face\
          \ cache and try again - maybe the download failed or something.  If that\
          \ line continues to fail, then I suspect you have some local issue in your\
          \ setup.</p>\n<p>However the next line, <code>model = AutoModelForCausalLM.from_pretrained(\"\
          TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\")</code>, cannot work.  You\
          \ can't load a GPTQ model with AutoModelForCausalLM.</p>\n<p>Check out AutoGPTQ\
          \ for a way to load GPTQ models from Python code.  It's still in active\
          \ development and there are a few issues atm, but it should work.  And I'm\
          \ going to push an example command line client for GPTQ inference in the\
          \ next day or so.</p>\n"
        raw: "That's a very strange error. I'm not sure what's going on.  The tokenizer\
          \ line should work fine, like so:\n```\nPython 3.10.11 (main, Apr  5 2023,\
          \ 14:15:10) [GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\"\
          \ or \"license\" for more information.\n>>> from transformers import AutoTokenizer\n\
          >>> tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\"\
          )\nDownloading (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 715/715 [00:00<00:00, 3.42MB/s]\n\
          Downloading tokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500k/500k\
          \ [00:00<00:00, 3.88MB/s]\nDownloading (\u2026)/main/tokenizer.json: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 1.84M/1.84M [00:00<00:00, 3.96MB/s]\n\
          Downloading (\u2026)in/added_tokens.json: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133/133 [00:00<00:00, 843kB/s]\n\
          Downloading (\u2026)cial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 477/477 [00:00<00:00, 2.79MB/s]\n\
          >>> print(tokenizer)\nLlamaTokenizerFast(name_or_path='TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ',\
          \ vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True,\
          \ padding_side='left', truncation_side='right', special_tokens={'bos_token':\
          \ AddedToken(\"\", rstrip=False, lstrip=False, single_word=False, normalized=True),\
          \ 'eos_token': '</s>', 'unk_token': AddedToken(\"\", rstrip=False, lstrip=False,\
          \ single_word=False, normalized=True), 'sep_token': '<s>', 'pad_token':\
          \ '</s>', 'additional_special_tokens': ['<|prompter|>', '<|system|>', '<|prefix_begin|>',\
          \ '<|prefix_end|>', '<|assistant|>']}, clean_up_tokenization_spaces=False)\n\
          >>>\n\n```\n\nAll I can suggest is to clear out your local Hugging Face\
          \ cache and try again - maybe the download failed or something.  If that\
          \ line continues to fail, then I suspect you have some local issue in your\
          \ setup.\n\nHowever the next line, `model = AutoModelForCausalLM.from_pretrained(\"\
          TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\")`, cannot work.  You can't\
          \ load a GPTQ model with AutoModelForCausalLM.\n\nCheck out AutoGPTQ for\
          \ a way to load GPTQ models from Python code.  It's still in active development\
          \ and there are a few issues atm, but it should work.  And I'm going to\
          \ push an example command line client for GPTQ inference in the next day\
          \ or so."
        updatedAt: '2023-05-10T20:14:21.406Z'
      numEdits: 0
      reactions: []
    id: 645bfb1db396a40a8493cd34
    type: comment
  author: TheBloke
  content: "That's a very strange error. I'm not sure what's going on.  The tokenizer\
    \ line should work fine, like so:\n```\nPython 3.10.11 (main, Apr  5 2023, 14:15:10)\
    \ [GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\"\
    \ for more information.\n>>> from transformers import AutoTokenizer\n>>> tokenizer\
    \ = AutoTokenizer.from_pretrained(\"TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\"\
    )\nDownloading (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 715/715 [00:00<00:00,\
    \ 3.42MB/s]\nDownloading tokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 500k/500k [00:00<00:00, 3.88MB/s]\nDownloading (\u2026)/main/tokenizer.json:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.84M/1.84M\
    \ [00:00<00:00, 3.96MB/s]\nDownloading (\u2026)in/added_tokens.json: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588| 133/133 [00:00<00:00, 843kB/s]\nDownloading (\u2026)cial_tokens_map.json:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588| 477/477 [00:00<00:00, 2.79MB/s]\n>>> print(tokenizer)\nLlamaTokenizerFast(name_or_path='TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ',\
    \ vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True,\
    \ padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"\
    \", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token':\
    \ '</s>', 'unk_token': AddedToken(\"\", rstrip=False, lstrip=False, single_word=False,\
    \ normalized=True), 'sep_token': '<s>', 'pad_token': '</s>', 'additional_special_tokens':\
    \ ['<|prompter|>', '<|system|>', '<|prefix_begin|>', '<|prefix_end|>', '<|assistant|>']},\
    \ clean_up_tokenization_spaces=False)\n>>>\n\n```\n\nAll I can suggest is to clear\
    \ out your local Hugging Face cache and try again - maybe the download failed\
    \ or something.  If that line continues to fail, then I suspect you have some\
    \ local issue in your setup.\n\nHowever the next line, `model = AutoModelForCausalLM.from_pretrained(\"\
    TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\")`, cannot work.  You can't load\
    \ a GPTQ model with AutoModelForCausalLM.\n\nCheck out AutoGPTQ for a way to load\
    \ GPTQ models from Python code.  It's still in active development and there are\
    \ a few issues atm, but it should work.  And I'm going to push an example command\
    \ line client for GPTQ inference in the next day or so."
  created_at: 2023-05-10 19:14:21+00:00
  edited: false
  hidden: false
  id: 645bfb1db396a40a8493cd34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
      fullname: Mayukh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kurapika993
      type: user
    createdAt: '2023-05-15T15:55:39.000Z'
    data:
      status: closed
    id: 646255fb8e12f9ab99962a37
    type: status-change
  author: Kurapika993
  created_at: 2023-05-15 14:55:39+00:00
  id: 646255fb8e12f9ab99962a37
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/10a0d62af0788d6533722fd18d1d47c7.svg
      fullname: dawei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: carlosbdw
      type: user
    createdAt: '2023-06-01T08:23:05.000Z'
    data:
      edited: false
      editors:
      - carlosbdw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/10a0d62af0788d6533722fd18d1d47c7.svg
          fullname: dawei
          isHf: false
          isPro: false
          name: carlosbdw
          type: user
        html: '<p>same error , and I am using  AutoGPTQ :</p>

          <p>use_triton = False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)</p>

          <p>quantize_config = BaseQuantizeConfig(<br>        bits=4,<br>        group_size=128,<br>        desc_act=False<br>    )</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,<br>        use_safetensors=True,<br>        model_basename=model_basename,<br>        device="cuda:0",<br>        use_triton=use_triton,<br>        quantize_config=quantize_config)</p>

          '
        raw: "same error , and I am using  AutoGPTQ :\n\nuse_triton = False\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)\n\n\
          quantize_config = BaseQuantizeConfig(\n        bits=4,\n        group_size=128,\n\
          \        desc_act=False\n    )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=True,\n        model_basename=model_basename,\n\
          \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=quantize_config)"
        updatedAt: '2023-06-01T08:23:05.219Z'
      numEdits: 0
      reactions: []
    id: 647855699c1f42c1f4d9244f
    type: comment
  author: carlosbdw
  content: "same error , and I am using  AutoGPTQ :\n\nuse_triton = False\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)\n\nquantize_config\
    \ = BaseQuantizeConfig(\n        bits=4,\n        group_size=128,\n        desc_act=False\n\
    \    )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n  \
    \      use_safetensors=True,\n        model_basename=model_basename,\n       \
    \ device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=quantize_config)"
  created_at: 2023-06-01 07:23:05+00:00
  edited: false
  hidden: false
  id: 647855699c1f42c1f4d9244f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-01T08:24:37.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Can you show me the exact error <span data-props=\"{&quot;user&quot;:&quot;carlosbdw&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/carlosbdw\"\
          >@<span class=\"underline\">carlosbdw</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>And please put it in code tags - put  ``` before and after the\
          \ log</p>\n"
        raw: "Can you show me the exact error @carlosbdw \n\nAnd please put it in\
          \ code tags - put  \\`\\`\\` before and after the log"
        updatedAt: '2023-06-01T08:24:37.093Z'
      numEdits: 0
      reactions: []
    id: 647855c59c1f42c1f4d92c35
    type: comment
  author: TheBloke
  content: "Can you show me the exact error @carlosbdw \n\nAnd please put it in code\
    \ tags - put  \\`\\`\\` before and after the log"
  created_at: 2023-06-01 07:24:37+00:00
  edited: false
  hidden: false
  id: 647855c59c1f42c1f4d92c35
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63345a00495073b887034037/4JbX2qaqCT7YlypvoYaNY.png?w=200&h=200&f=face
      fullname: Mayukh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kurapika993
      type: user
    createdAt: '2023-06-01T08:35:40.000Z'
    data:
      status: open
    id: 6478585c159a889d001f5763
    type: status-change
  author: Kurapika993
  created_at: 2023-06-01 07:35:40+00:00
  id: 6478585c159a889d001f5763
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ
repo_type: model
status: open
target_branch: null
title: Exception on downloading the model
