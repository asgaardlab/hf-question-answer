!!python/object:huggingface_hub.community.DiscussionWithDetails
author: robert1602
conflicting_files: null
created_at: 2023-06-07 13:32:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b2f05f67b5182b48221daeef45d2d3f.svg
      fullname: Roberto Anzaldua
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robert1602
      type: user
    createdAt: '2023-06-07T14:32:36.000Z'
    data:
      edited: false
      editors:
      - robert1602
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4908125698566437
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b2f05f67b5182b48221daeef45d2d3f.svg
          fullname: Roberto Anzaldua
          isHf: false
          isPro: false
          name: robert1602
          type: user
        html: '<p>I am trying to convert Blip2 to ONNX, I know it''s not yet supported
          for the ''easiest'' way to do this, so I was trying the low-level option,
          but I got this error:</p>

          <p>  File "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py",
          line 981, in forward<br>    raise ValueError(f"You have to specify either
          {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")</p>

          <p>This is the code I use:</p>

          <p>torch.onnx.export(MODEL,<br>                  tuple(DUMMY_MODEL_INPUT.values()),<br>                  "{}.onnx".format(MODEL_NAME),<br>                  verbose=True,<br>                  input_names=[''pixel_values'',
          ''input_ids'', ''attention_mask''],<br>                  output_names=[''logits''],<br>                  dynamic_axes={<br>                                ''input_ids'':
          {0: ''batch_size'', 1: ''sequence''},<br>                                ''attention_mask'':
          {0: ''batch_size'', 1: ''sequence''},<br>                                ''logits'':
          {0: ''batch_size'', 1: ''sequence''}},<br>                  do_constant_folding=True,<br>                  opset_version=13,<br>                  )</p>

          '
        raw: "I am trying to convert Blip2 to ONNX, I know it's not yet supported\
          \ for the 'easiest' way to do this, so I was trying the low-level option,\
          \ but I got this error:\r\n\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\"\
          , line 981, in forward\r\n    raise ValueError(f\"You have to specify either\
          \ {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\r\n\r\n\
          This is the code I use:\r\n\r\ntorch.onnx.export(MODEL,\r\n            \
          \      tuple(DUMMY_MODEL_INPUT.values()),\r\n                  \"{}.onnx\"\
          .format(MODEL_NAME),\r\n                  verbose=True,\r\n            \
          \      input_names=['pixel_values', 'input_ids', 'attention_mask'],\r\n\
          \                  output_names=['logits'],\r\n                  dynamic_axes={\r\
          \n                                'input_ids': {0: 'batch_size', 1: 'sequence'},\r\
          \n                                'attention_mask': {0: 'batch_size', 1:\
          \ 'sequence'},\r\n                                'logits': {0: 'batch_size',\
          \ 1: 'sequence'}},\r\n                  do_constant_folding=True,\r\n  \
          \                opset_version=13,\r\n                  )"
        updatedAt: '2023-06-07T14:32:36.334Z'
      numEdits: 0
      reactions: []
    id: 648095049860cd75c25d63d6
    type: comment
  author: robert1602
  content: "I am trying to convert Blip2 to ONNX, I know it's not yet supported for\
    \ the 'easiest' way to do this, so I was trying the low-level option, but I got\
    \ this error:\r\n\r\n  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\"\
    , line 981, in forward\r\n    raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids\
    \ or {err_msg_prefix}inputs_embeds\")\r\n\r\nThis is the code I use:\r\n\r\ntorch.onnx.export(MODEL,\r\
    \n                  tuple(DUMMY_MODEL_INPUT.values()),\r\n                  \"\
    {}.onnx\".format(MODEL_NAME),\r\n                  verbose=True,\r\n         \
    \         input_names=['pixel_values', 'input_ids', 'attention_mask'],\r\n   \
    \               output_names=['logits'],\r\n                  dynamic_axes={\r\
    \n                                'input_ids': {0: 'batch_size', 1: 'sequence'},\r\
    \n                                'attention_mask': {0: 'batch_size', 1: 'sequence'},\r\
    \n                                'logits': {0: 'batch_size', 1: 'sequence'}},\r\
    \n                  do_constant_folding=True,\r\n                  opset_version=13,\r\
    \n                  )"
  created_at: 2023-06-07 13:32:36+00:00
  edited: false
  hidden: false
  id: 648095049860cd75c25d63d6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: Salesforce/blip2-flan-t5-xxl
repo_type: model
status: open
target_branch: null
title: Convert to ONNX
