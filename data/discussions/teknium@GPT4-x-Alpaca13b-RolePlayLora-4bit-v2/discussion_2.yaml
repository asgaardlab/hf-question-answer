!!python/object:huggingface_hub.community.DiscussionWithDetails
author: autobots
conflicting_files: null
created_at: 2023-04-23 03:15:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-04-23T04:15:55.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: "<p>This one? <a href=\"https://huggingface.co/ZeusLabs/gpt4-x-alpaca-rp-lora/tree/main/gpt-rp-instruct-2-all-layers\"\
          >https://huggingface.co/ZeusLabs/gpt4-x-alpaca-rp-lora/tree/main/gpt-rp-instruct-2-all-layers</a></p>\n\
          <p>I d/l this model and its a little bit crazy. Did bad generations mean\
          \ nonsensical numbers and words? After updating the tokenizer, the model\
          \ is coherent, it just gives weird answers.</p>\n<p>for example:</p>\n<p>To\
          \ cook four eggs in a pan, follow these steps:1. Fill a medium saucepan\
          \ with water and add enough salt so that it tastes like seawater when you\
          \ dip your finger into it.2. Place the pot over medium heat until it reaches\
          \ about 60\xB0C (160\xB0F).3. Crack one egg at a time into a bowl and gently\
          \ slip it into the hot water using chopsticks or tongs.4. Cook the egg for\
          \ three minutes before removing it onto a plate.5. Repeat this process for\
          \ all remaining eggs. Make sure not to let the water cool down too much\
          \ between eggs.6. When finished, remove the eggs from the pan and enjoy\
          \ your breakfast.</p>\n"
        raw: "This one? https://huggingface.co/ZeusLabs/gpt4-x-alpaca-rp-lora/tree/main/gpt-rp-instruct-2-all-layers\r\
          \n\r\nI d/l this model and its a little bit crazy. Did bad generations mean\
          \ nonsensical numbers and words? After updating the tokenizer, the model\
          \ is coherent, it just gives weird answers.\r\n\r\n\r\nfor example:\r\n\r\
          \nTo cook four eggs in a pan, follow these steps:1. Fill a medium saucepan\
          \ with water and add enough salt so that it tastes like seawater when you\
          \ dip your finger into it.2. Place the pot over medium heat until it reaches\
          \ about 60\xB0C (160\xB0F).3. Crack one egg at a time into a bowl and gently\
          \ slip it into the hot water using chopsticks or tongs.4. Cook the egg for\
          \ three minutes before removing it onto a plate.5. Repeat this process for\
          \ all remaining eggs. Make sure not to let the water cool down too much\
          \ between eggs.6. When finished, remove the eggs from the pan and enjoy\
          \ your breakfast."
        updatedAt: '2023-04-23T04:15:55.473Z'
      numEdits: 0
      reactions: []
    id: 6444b0fb79e7797ce71e8eeb
    type: comment
  author: autobots
  content: "This one? https://huggingface.co/ZeusLabs/gpt4-x-alpaca-rp-lora/tree/main/gpt-rp-instruct-2-all-layers\r\
    \n\r\nI d/l this model and its a little bit crazy. Did bad generations mean nonsensical\
    \ numbers and words? After updating the tokenizer, the model is coherent, it just\
    \ gives weird answers.\r\n\r\n\r\nfor example:\r\n\r\nTo cook four eggs in a pan,\
    \ follow these steps:1. Fill a medium saucepan with water and add enough salt\
    \ so that it tastes like seawater when you dip your finger into it.2. Place the\
    \ pot over medium heat until it reaches about 60\xB0C (160\xB0F).3. Crack one\
    \ egg at a time into a bowl and gently slip it into the hot water using chopsticks\
    \ or tongs.4. Cook the egg for three minutes before removing it onto a plate.5.\
    \ Repeat this process for all remaining eggs. Make sure not to let the water cool\
    \ down too much between eggs.6. When finished, remove the eggs from the pan and\
    \ enjoy your breakfast."
  created_at: 2023-04-23 03:15:55+00:00
  edited: false
  hidden: false
  id: 6444b0fb79e7797ce71e8eeb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-04-23T07:23:52.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>Like I state in the model page, it is worse than base gpt4-x-alpaca
          at blank instruct responses. It is tailor made for role playing. See model
          page for an example roleplaying instruction, and it should perform much
          better at those tasks. The only training done with the lora was for roleplaying,
          and it has lost some coherency without a roleplaying task for instructs,
          where it has gained in roleplaying skill.</p>

          '
        raw: Like I state in the model page, it is worse than base gpt4-x-alpaca at
          blank instruct responses. It is tailor made for role playing. See model
          page for an example roleplaying instruction, and it should perform much
          better at those tasks. The only training done with the lora was for roleplaying,
          and it has lost some coherency without a roleplaying task for instructs,
          where it has gained in roleplaying skill.
        updatedAt: '2023-04-23T07:23:52.432Z'
      numEdits: 0
      reactions: []
    id: 6444dd0853ecc52f50e6fd7e
    type: comment
  author: teknium
  content: Like I state in the model page, it is worse than base gpt4-x-alpaca at
    blank instruct responses. It is tailor made for role playing. See model page for
    an example roleplaying instruction, and it should perform much better at those
    tasks. The only training done with the lora was for roleplaying, and it has lost
    some coherency without a roleplaying task for instructs, where it has gained in
    roleplaying skill.
  created_at: 2023-04-23 06:23:52+00:00
  edited: false
  hidden: false
  id: 6444dd0853ecc52f50e6fd7e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-04-23T07:24:25.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<blockquote>

          <p>Like I state in the model page, it is worse than base gpt4-x-alpaca at
          blank instruct responses. It is tailor made for role playing. See model
          page for an example roleplaying instruction, and it should perform much
          better at those tasks. The only training done with the lora was for roleplaying,
          and it has lost some coherency without a roleplaying task for instructs,
          where it has gained in roleplaying skill.</p>

          </blockquote>

          <p>"Instructions simply using alpaca format are likely to be of lower quality.
          If you want pure general instruct capability I reccomend GPT-4-X-Alpaca
          (the base model of this) - The model responds well to giving it a roleplay
          task in the preprompt, and the actual conversation in the "### Input: "
          field.</p>

          <p>For a better idea of prompting it for roleplay, check out the roleplay
          discord bot code I made here: <a rel="nofollow" href="https://github.com/teknium1/alpaca-roleplay-discordbot">https://github.com/teknium1/alpaca-roleplay-discordbot</a>
          Here is an example:</p>

          <h3 id="instruction">Instruction:</h3>

          <p>Role play as character that is described in the following lines. You
          always stay in character.<br>{"Your name is " + name + "." if name else
          ""}<br>{"Your backstory and history are: " + background if background else
          ""}<br>{"Your personality is: " + personality if personality else ""}<br>{"Your
          current circumstances and situation are: " + circumstances if circumstances
          else ""}<br>{"Your common greetings are: " + common_greeting if common_greeting
          else ""}<br>Remember, you always stay on character. You are the character
          described above.<br>{past_dialogue_formatted}<br>{chat_history if chat_history
          else "Chatbot: Hello!"}</p>

          <p>Always speak with new and unique messages that haven''t been said in
          the chat history.</p>

          <p>Respond to this message as your character would:</p>

          <h3 id="input">Input:</h3>

          <p>{text}</p>

          <h3 id="response">Response:</h3>

          <p>{name}:"</p>

          <p>from model page</p>

          '
        raw: '> Like I state in the model page, it is worse than base gpt4-x-alpaca
          at blank instruct responses. It is tailor made for role playing. See model
          page for an example roleplaying instruction, and it should perform much
          better at those tasks. The only training done with the lora was for roleplaying,
          and it has lost some coherency without a roleplaying task for instructs,
          where it has gained in roleplaying skill.


          "Instructions simply using alpaca format are likely to be of lower quality.
          If you want pure general instruct capability I reccomend GPT-4-X-Alpaca
          (the base model of this) - The model responds well to giving it a roleplay
          task in the preprompt, and the actual conversation in the "### Input: "
          field.


          For a better idea of prompting it for roleplay, check out the roleplay discord
          bot code I made here: https://github.com/teknium1/alpaca-roleplay-discordbot
          Here is an example:


          ### Instruction:

          Role play as character that is described in the following lines. You always
          stay in character.

          {"Your name is " + name + "." if name else ""}

          {"Your backstory and history are: " + background if background else ""}

          {"Your personality is: " + personality if personality else ""}

          {"Your current circumstances and situation are: " + circumstances if circumstances
          else ""}

          {"Your common greetings are: " + common_greeting if common_greeting else
          ""}

          Remember, you always stay on character. You are the character described
          above.

          {past_dialogue_formatted}

          {chat_history if chat_history else "Chatbot: Hello!"}


          Always speak with new and unique messages that haven''t been said in the
          chat history.


          Respond to this message as your character would:

          ### Input:

          {text}

          ### Response:

          {name}:"


          from model page'
        updatedAt: '2023-04-23T07:24:25.933Z'
      numEdits: 0
      reactions: []
    id: 6444dd290f2fc80feb1c9780
    type: comment
  author: teknium
  content: '> Like I state in the model page, it is worse than base gpt4-x-alpaca
    at blank instruct responses. It is tailor made for role playing. See model page
    for an example roleplaying instruction, and it should perform much better at those
    tasks. The only training done with the lora was for roleplaying, and it has lost
    some coherency without a roleplaying task for instructs, where it has gained in
    roleplaying skill.


    "Instructions simply using alpaca format are likely to be of lower quality. If
    you want pure general instruct capability I reccomend GPT-4-X-Alpaca (the base
    model of this) - The model responds well to giving it a roleplay task in the preprompt,
    and the actual conversation in the "### Input: " field.


    For a better idea of prompting it for roleplay, check out the roleplay discord
    bot code I made here: https://github.com/teknium1/alpaca-roleplay-discordbot Here
    is an example:


    ### Instruction:

    Role play as character that is described in the following lines. You always stay
    in character.

    {"Your name is " + name + "." if name else ""}

    {"Your backstory and history are: " + background if background else ""}

    {"Your personality is: " + personality if personality else ""}

    {"Your current circumstances and situation are: " + circumstances if circumstances
    else ""}

    {"Your common greetings are: " + common_greeting if common_greeting else ""}

    Remember, you always stay on character. You are the character described above.

    {past_dialogue_formatted}

    {chat_history if chat_history else "Chatbot: Hello!"}


    Always speak with new and unique messages that haven''t been said in the chat
    history.


    Respond to this message as your character would:

    ### Input:

    {text}

    ### Response:

    {name}:"


    from model page'
  created_at: 2023-04-23 06:24:25+00:00
  edited: false
  hidden: false
  id: 6444dd290f2fc80feb1c9780
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-04-23T11:42:57.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>I am using it with ooba to play characters exclusively. I asked
          it these things as the character GPT-3. But one lora is 400mb and the other
          is 800mb? So is the fp16 model on the first one and the int4 on the second?
          I just d/l both and want to see how they do. It will not help roleplaying
          when the char doesn''t know reality.</p>

          '
        raw: I am using it with ooba to play characters exclusively. I asked it these
          things as the character GPT-3. But one lora is 400mb and the other is 800mb?
          So is the fp16 model on the first one and the int4 on the second? I just
          d/l both and want to see how they do. It will not help roleplaying when
          the char doesn't know reality.
        updatedAt: '2023-04-23T11:42:57.523Z'
      numEdits: 0
      reactions: []
    id: 644519c153ecc52f50ebbf89
    type: comment
  author: autobots
  content: I am using it with ooba to play characters exclusively. I asked it these
    things as the character GPT-3. But one lora is 400mb and the other is 800mb? So
    is the fp16 model on the first one and the int4 on the second? I just d/l both
    and want to see how they do. It will not help roleplaying when the char doesn't
    know reality.
  created_at: 2023-04-23 10:42:57+00:00
  edited: false
  hidden: false
  id: 644519c153ecc52f50ebbf89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-04-23T17:34:49.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>The 2nd lora trained more layers</p>

          '
        raw: The 2nd lora trained more layers
        updatedAt: '2023-04-23T17:34:49.486Z'
      numEdits: 0
      reactions: []
    id: 64456c39d1460e859d1ec3d4
    type: comment
  author: teknium
  content: The 2nd lora trained more layers
  created_at: 2023-04-23 16:34:49+00:00
  edited: false
  hidden: false
  id: 64456c39d1460e859d1ec3d4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2
repo_type: model
status: open
target_branch: null
title: This is made with the larger lora?
