!!python/object:huggingface_hub.community.DiscussionWithDetails
author: asyafiqe
conflicting_files: null
created_at: 2023-08-27 11:11:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd1e828bb3028c5e717c6ff4d4d859c0.svg
      fullname: asyafiqe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asyafiqe
      type: user
    createdAt: '2023-08-27T12:11:35.000Z'
    data:
      edited: false
      editors:
      - asyafiqe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7390140295028687
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd1e828bb3028c5e717c6ff4d4d859c0.svg
          fullname: asyafiqe
          isHf: false
          isPro: false
          name: asyafiqe
          type: user
        html: '<p>Thank you for this fantastic model.</p>

          <p>I used this model to do fine-tuning with orca mini datasets. Checkout
          <a href="https://huggingface.co/asyafiqe/Merak-7B-v3-Mini-Orca-Indo">https://huggingface.co/asyafiqe/Merak-7B-v3-Mini-Orca-Indo</a>!</p>

          <p>It would be great if you can give some feedback.<br>Thanks!</p>

          '
        raw: "Thank you for this fantastic model.\r\n\r\nI used this model to do fine-tuning\
          \ with orca mini datasets. Checkout https://huggingface.co/asyafiqe/Merak-7B-v3-Mini-Orca-Indo!\r\
          \n\r\nIt would be great if you can give some feedback.\r\nThanks!"
        updatedAt: '2023-08-27T12:11:35.992Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Ichsan2895
    id: 64eb3d77cfa36c8ac2d97afa
    type: comment
  author: asyafiqe
  content: "Thank you for this fantastic model.\r\n\r\nI used this model to do fine-tuning\
    \ with orca mini datasets. Checkout https://huggingface.co/asyafiqe/Merak-7B-v3-Mini-Orca-Indo!\r\
    \n\r\nIt would be great if you can give some feedback.\r\nThanks!"
  created_at: 2023-08-27 11:11:35+00:00
  edited: false
  hidden: false
  id: 64eb3d77cfa36c8ac2d97afa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
      fullname: Muhammad Ichsan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Ichsan2895
      type: user
    createdAt: '2023-08-29T14:36:48.000Z'
    data:
      edited: true
      editors:
      - Ichsan2895
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9733129739761353
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
          fullname: Muhammad Ichsan
          isHf: false
          isPro: false
          name: Ichsan2895
          type: user
        html: '<p>Thank you, I have tested it and it works great. :) Now, I also know
          that Orca is a good dataset for my model.</p>

          <p>BTW, since Merak-7B is licensed by CC-BY-SA-NC, that means all derivative
          works of Merak must be same as Merak which is non commercial &amp; share-alike.</p>

          <p>I will try fine tune Merak-v3 with other dataset to make it better.</p>

          <p>Thanks again for contributing in LLM world :)</p>

          '
        raw: 'Thank you, I have tested it and it works great. :) Now, I also know
          that Orca is a good dataset for my model.


          BTW, since Merak-7B is licensed by CC-BY-SA-NC, that means all derivative
          works of Merak must be same as Merak which is non commercial & share-alike.


          I will try fine tune Merak-v3 with other dataset to make it better.


          Thanks again for contributing in LLM world :)'
        updatedAt: '2023-08-29T14:37:48.180Z'
      numEdits: 1
      reactions: []
    id: 64ee0280f801b326e177a86c
    type: comment
  author: Ichsan2895
  content: 'Thank you, I have tested it and it works great. :) Now, I also know that
    Orca is a good dataset for my model.


    BTW, since Merak-7B is licensed by CC-BY-SA-NC, that means all derivative works
    of Merak must be same as Merak which is non commercial & share-alike.


    I will try fine tune Merak-v3 with other dataset to make it better.


    Thanks again for contributing in LLM world :)'
  created_at: 2023-08-29 13:36:48+00:00
  edited: true
  hidden: false
  id: 64ee0280f801b326e177a86c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd1e828bb3028c5e717c6ff4d4d859c0.svg
      fullname: asyafiqe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asyafiqe
      type: user
    createdAt: '2023-08-30T10:59:59.000Z'
    data:
      edited: false
      editors:
      - asyafiqe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9717368483543396
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd1e828bb3028c5e717c6ff4d4d859c0.svg
          fullname: asyafiqe
          isHf: false
          isPro: false
          name: asyafiqe
          type: user
        html: '<p>Great!</p>

          <p>I see. Thank you for the PR. I''ve already merged it.</p>

          <p>Can''t wait to see your next fine-tuning!</p>

          '
        raw: 'Great!


          I see. Thank you for the PR. I''ve already merged it.


          Can''t wait to see your next fine-tuning!'
        updatedAt: '2023-08-30T10:59:59.228Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Ichsan2895
    id: 64ef212fbceb4145b403baf6
    type: comment
  author: asyafiqe
  content: 'Great!


    I see. Thank you for the PR. I''ve already merged it.


    Can''t wait to see your next fine-tuning!'
  created_at: 2023-08-30 09:59:59+00:00
  edited: false
  hidden: false
  id: 64ef212fbceb4145b403baf6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
      fullname: Muhammad Ichsan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Ichsan2895
      type: user
    createdAt: '2023-08-30T15:27:22.000Z'
    data:
      edited: false
      editors:
      - Ichsan2895
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9013208746910095
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
          fullname: Muhammad Ichsan
          isHf: false
          isPro: false
          name: Ichsan2895
          type: user
        html: '<blockquote>

          <p>Great!</p>

          <p>I see. Thank you for the PR. I''ve already merged it.</p>

          <p>Can''t wait to see your next fine-tuning!</p>

          </blockquote>

          <p>Thank you for your understanding.. :)</p>

          <p>BTW, I have a problem that I still find the solution. Both Merak and
          Merak-Orca occur hallucination with history question.</p>

          <p>For example : "Siapa komposer dari lagu kebangsaan Indonesia Raya?"</p>

          <p>My original Merak has been fine tuned by Wikipedia. I sure I have trained
          that. But, The hallucination still occurs.</p>

          <p>My next fine tune, maybe I train again Merak with Wikipedia dataset.
          It will use another hyperparameter or maybe another prompt. So it hopefully
          reduce the hallucination.</p>

          '
        raw: "> Great!\n> \n> I see. Thank you for the PR. I've already merged it.\n\
          > \n> Can't wait to see your next fine-tuning!\n\nThank you for your understanding..\
          \ :)\n\nBTW, I have a problem that I still find the solution. Both Merak\
          \ and Merak-Orca occur hallucination with history question.\n\nFor example\
          \ : \"Siapa komposer dari lagu kebangsaan Indonesia Raya?\"\n\nMy original\
          \ Merak has been fine tuned by Wikipedia. I sure I have trained that. But,\
          \ The hallucination still occurs.\n\nMy next fine tune, maybe I train again\
          \ Merak with Wikipedia dataset. It will use another hyperparameter or maybe\
          \ another prompt. So it hopefully reduce the hallucination.\n\n"
        updatedAt: '2023-08-30T15:27:22.202Z'
      numEdits: 0
      reactions: []
    id: 64ef5fda3cafb8723cef2b6c
    type: comment
  author: Ichsan2895
  content: "> Great!\n> \n> I see. Thank you for the PR. I've already merged it.\n\
    > \n> Can't wait to see your next fine-tuning!\n\nThank you for your understanding..\
    \ :)\n\nBTW, I have a problem that I still find the solution. Both Merak and Merak-Orca\
    \ occur hallucination with history question.\n\nFor example : \"Siapa komposer\
    \ dari lagu kebangsaan Indonesia Raya?\"\n\nMy original Merak has been fine tuned\
    \ by Wikipedia. I sure I have trained that. But, The hallucination still occurs.\n\
    \nMy next fine tune, maybe I train again Merak with Wikipedia dataset. It will\
    \ use another hyperparameter or maybe another prompt. So it hopefully reduce the\
    \ hallucination.\n\n"
  created_at: 2023-08-30 14:27:22+00:00
  edited: false
  hidden: false
  id: 64ef5fda3cafb8723cef2b6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd1e828bb3028c5e717c6ff4d4d859c0.svg
      fullname: asyafiqe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asyafiqe
      type: user
    createdAt: '2023-09-01T21:15:27.000Z'
    data:
      edited: false
      editors:
      - asyafiqe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9614672064781189
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd1e828bb3028c5e717c6ff4d4d859c0.svg
          fullname: asyafiqe
          isHf: false
          isPro: false
          name: asyafiqe
          type: user
        html: '<p>I think that is the problem of fine-tuning. Fine tuning cannot directly
          add knowledge, it''s more of adjusting response style. The knowledge depends
          more on the pretraining. It is often said that for accurate answer, retrieval
          augmented generation (RAG) is more preferable.</p>

          <p>Anyway, Merak v3 already consistently answers in Bahasa Indonesia. Response
          is also much longer than v2.</p>

          '
        raw: 'I think that is the problem of fine-tuning. Fine tuning cannot directly
          add knowledge, it''s more of adjusting response style. The knowledge depends
          more on the pretraining. It is often said that for accurate answer, retrieval
          augmented generation (RAG) is more preferable.


          Anyway, Merak v3 already consistently answers in Bahasa Indonesia. Response
          is also much longer than v2.'
        updatedAt: '2023-09-01T21:15:27.274Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Ichsan2895
    id: 64f2546f7eb3ae0088ce23c6
    type: comment
  author: asyafiqe
  content: 'I think that is the problem of fine-tuning. Fine tuning cannot directly
    add knowledge, it''s more of adjusting response style. The knowledge depends more
    on the pretraining. It is often said that for accurate answer, retrieval augmented
    generation (RAG) is more preferable.


    Anyway, Merak v3 already consistently answers in Bahasa Indonesia. Response is
    also much longer than v2.'
  created_at: 2023-09-01 20:15:27+00:00
  edited: false
  hidden: false
  id: 64f2546f7eb3ae0088ce23c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
      fullname: Muhammad Ichsan
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Ichsan2895
      type: user
    createdAt: '2023-09-02T12:49:14.000Z'
    data:
      edited: true
      editors:
      - Ichsan2895
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9364750981330872
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Uq5V3aIHpSHDPyHBBvEq-.jpeg?w=200&h=200&f=face
          fullname: Muhammad Ichsan
          isHf: false
          isPro: false
          name: Ichsan2895
          type: user
        html: '<blockquote>

          <p>I think that is the problem of fine-tuning. Fine tuning cannot directly
          add knowledge, it''s more of adjusting response style. The knowledge depends
          more on the pretraining. It is often said that for accurate answer, retrieval
          augmented generation (RAG) is more preferable.</p>

          </blockquote>

          <p>I see... I still find any solution since Indonesian language is only
          0,03% in the pretraining dataset (according to the Llama2 paper).<br>RAG
          and RHLF is next possible solution. I will learn it too.</p>

          <blockquote>

          <p>Anyway, Merak v3 already consistently answers in Bahasa Indonesia. Response
          is also much longer than v2.</p>

          </blockquote>

          <p>Praising to the God, I''m happy because v3 has response which is much
          longer than v2.</p>

          <p>I think it was because I have trained it with <a href="https://huggingface.co/datasets/Ichsan2895/OASST_Top1_Indonesian">Ichsan2895/OASST_Top1_Indonesian</a>
          &amp; <a href="https://huggingface.co/datasets/Ichsan2895/alpaca-gpt4-indonesian">Ichsan2895/alpaca-gpt4-indonesian</a></p>

          '
        raw: '> I think that is the problem of fine-tuning. Fine tuning cannot directly
          add knowledge, it''s more of adjusting response style. The knowledge depends
          more on the pretraining. It is often said that for accurate answer, retrieval
          augmented generation (RAG) is more preferable.


          I see... I still find any solution since Indonesian language is only 0,03%
          in the pretraining dataset (according to the Llama2 paper).

          RAG and RHLF is next possible solution. I will learn it too.


          > Anyway, Merak v3 already consistently answers in Bahasa Indonesia. Response
          is also much longer than v2.


          Praising to the God, I''m happy because v3 has response which is much longer
          than v2.


          I think it was because I have trained it with [Ichsan2895/OASST_Top1_Indonesian](https://huggingface.co/datasets/Ichsan2895/OASST_Top1_Indonesian)
          & [Ichsan2895/alpaca-gpt4-indonesian](https://huggingface.co/datasets/Ichsan2895/alpaca-gpt4-indonesian)'
        updatedAt: '2023-09-02T12:51:55.454Z'
      numEdits: 4
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - asyafiqe
        - Sandrian
    id: 64f32f4a9a431dda2282c1b0
    type: comment
  author: Ichsan2895
  content: '> I think that is the problem of fine-tuning. Fine tuning cannot directly
    add knowledge, it''s more of adjusting response style. The knowledge depends more
    on the pretraining. It is often said that for accurate answer, retrieval augmented
    generation (RAG) is more preferable.


    I see... I still find any solution since Indonesian language is only 0,03% in
    the pretraining dataset (according to the Llama2 paper).

    RAG and RHLF is next possible solution. I will learn it too.


    > Anyway, Merak v3 already consistently answers in Bahasa Indonesia. Response
    is also much longer than v2.


    Praising to the God, I''m happy because v3 has response which is much longer than
    v2.


    I think it was because I have trained it with [Ichsan2895/OASST_Top1_Indonesian](https://huggingface.co/datasets/Ichsan2895/OASST_Top1_Indonesian)
    & [Ichsan2895/alpaca-gpt4-indonesian](https://huggingface.co/datasets/Ichsan2895/alpaca-gpt4-indonesian)'
  created_at: 2023-09-02 11:49:14+00:00
  edited: true
  hidden: false
  id: 64f32f4a9a431dda2282c1b0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Ichsan2895/Merak-7B-v3
repo_type: model
status: open
target_branch: null
title: Fine-tuned model
