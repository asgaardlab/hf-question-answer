!!python/object:huggingface_hub.community.DiscussionWithDetails
author: agonzalez
conflicting_files: null
created_at: 2023-05-23 19:46:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d83060ae1625fe0b47b97d54ee4fbdd3.svg
      fullname: Alberto
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: agonzalez
      type: user
    createdAt: '2023-05-23T20:46:18.000Z'
    data:
      edited: false
      editors:
      - agonzalez
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d83060ae1625fe0b47b97d54ee4fbdd3.svg
          fullname: Alberto
          isHf: false
          isPro: false
          name: agonzalez
          type: user
        html: '<p>I have NVIDIA RTX 3060 12GB VRAM and can load 7B version of this
          model and use 6GB RAM, but this modelo keep loading for ever and doesnt
          give anty error any idea?</p>

          <p>INFO:Loading TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ...<br>INFO:Loading
          TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ...<br>INFO:Found the following
          quantized model: models\TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ\Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors</p>

          '
        raw: "I have NVIDIA RTX 3060 12GB VRAM and can load 7B version of this model\
          \ and use 6GB RAM, but this modelo keep loading for ever and doesnt give\
          \ anty error any idea?\r\n\r\nINFO:Loading TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ...\r\
          \nINFO:Loading TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ...\r\nINFO:Found\
          \ the following quantized model: models\\TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ\\\
          Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\r\
          \n"
        updatedAt: '2023-05-23T20:46:18.830Z'
      numEdits: 0
      reactions: []
    id: 646d261ada8e99940b7018e7
    type: comment
  author: agonzalez
  content: "I have NVIDIA RTX 3060 12GB VRAM and can load 7B version of this model\
    \ and use 6GB RAM, but this modelo keep loading for ever and doesnt give anty\
    \ error any idea?\r\n\r\nINFO:Loading TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ...\r\
    \nINFO:Loading TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ...\r\nINFO:Found the\
    \ following quantized model: models\\TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ\\\
    Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\r\n"
  created_at: 2023-05-23 19:46:18+00:00
  edited: false
  hidden: false
  id: 646d261ada8e99940b7018e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-23T23:07:42.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>What version of GPTQ-for-LLaMa do you have in text-generation-webui/repositories?</p>

          '
        raw: What version of GPTQ-for-LLaMa do you have in text-generation-webui/repositories?
        updatedAt: '2023-05-23T23:07:42.098Z'
      numEdits: 0
      reactions: []
    id: 646d473eacc13867a13a4099
    type: comment
  author: TheBloke
  content: What version of GPTQ-for-LLaMa do you have in text-generation-webui/repositories?
  created_at: 2023-05-23 22:07:42+00:00
  edited: false
  hidden: false
  id: 646d473eacc13867a13a4099
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: Model doesnt load
