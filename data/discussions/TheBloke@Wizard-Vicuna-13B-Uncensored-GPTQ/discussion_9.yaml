!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GaymerDanny
conflicting_files: null
created_at: 2023-06-03 14:03:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3bcf9102fec6271a12ca52c345119dd0.svg
      fullname: Danny M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaymerDanny
      type: user
    createdAt: '2023-06-03T15:03:11.000Z'
    data:
      edited: false
      editors:
      - GaymerDanny
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4835876226425171
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3bcf9102fec6271a12ca52c345119dd0.svg
          fullname: Danny M
          isHf: false
          isPro: false
          name: GaymerDanny
          type: user
        html: "<p>I have oobabooga installed on my GPU and so i installed this model\
          \ and i tried to run it and oh god I ran into so many troubles and itried\
          \ many solutions till I ended up with this one</p>\n<p>When I tried to load\
          \ it, it gives me this error.</p>\n<p>Traceback (most recent call last):\
          \ File \u201CC:\\Users\\Dan\\Desktop\\AI\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 71, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Users\\Dan\\Desktop\\AI\\\
          oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
          \ 97, in load_model output = load_func(model_name) File \u201CC:\\Users\\\
          Dan\\Desktop\\AI\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 155, in huggingface_loader model = LoaderClass.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}\"), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\
          \ if shared.args.bf16 else torch.float16, trust_remote_code=shared.args.trust_remote_code)\
          \ File \u201CC:\\Users\\Dan\\Desktop\\AI\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D\
          , line 472, in from_pretrained return model_class.from_pretrained( File\
          \ \u201CC:\\Users\\Dan\\Desktop\\AI\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 2406,\
          \ in from_pretrained raise EnvironmentError( OSError: Error no file named\
          \ pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models\\TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ.</p>\n"
        raw: "I have oobabooga installed on my GPU and so i installed this model and\
          \ i tried to run it and oh god I ran into so many troubles and itried many\
          \ solutions till I ended up with this one\r\n \r\nWhen I tried to load it,\
          \ it gives me this error.\r\n\r\nTraceback (most recent call last): File\
          \ \u201CC:\\Users\\Dan\\Desktop\\AI\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 71, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Users\\Dan\\Desktop\\AI\\\
          oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
          \ 97, in load_model output = load_func(model_name) File \u201CC:\\Users\\\
          Dan\\Desktop\\AI\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 155, in huggingface_loader model = LoaderClass.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}\"), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\
          \ if shared.args.bf16 else torch.float16, trust_remote_code=shared.args.trust_remote_code)\
          \ File \u201CC:\\Users\\Dan\\Desktop\\AI\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D\
          , line 472, in from_pretrained return model_class.from_pretrained( File\
          \ \u201CC:\\Users\\Dan\\Desktop\\AI\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 2406,\
          \ in from_pretrained raise EnvironmentError( OSError: Error no file named\
          \ pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models\\TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ."
        updatedAt: '2023-06-03T15:03:11.302Z'
      numEdits: 0
      reactions: []
    id: 647b562fe3a7d24c8e496897
    type: comment
  author: GaymerDanny
  content: "I have oobabooga installed on my GPU and so i installed this model and\
    \ i tried to run it and oh god I ran into so many troubles and itried many solutions\
    \ till I ended up with this one\r\n \r\nWhen I tried to load it, it gives me this\
    \ error.\r\n\r\nTraceback (most recent call last): File \u201CC:\\Users\\Dan\\\
    Desktop\\AI\\oobabooga_windows\\text-generation-webui\\server.py\u201D, line 71,\
    \ in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
    \ File \u201CC:\\Users\\Dan\\Desktop\\AI\\oobabooga_windows\\text-generation-webui\\\
    modules\\models.py\u201D, line 97, in load_model output = load_func(model_name)\
    \ File \u201CC:\\Users\\Dan\\Desktop\\AI\\oobabooga_windows\\text-generation-webui\\\
    modules\\models.py\u201D, line 155, in huggingface_loader model = LoaderClass.from_pretrained(Path(f\"\
    {shared.args.model_dir}/{model_name}\"), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\
    \ if shared.args.bf16 else torch.float16, trust_remote_code=shared.args.trust_remote_code)\
    \ File \u201CC:\\Users\\Dan\\Desktop\\AI\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D, line\
    \ 472, in from_pretrained return model_class.from_pretrained( File \u201CC:\\\
    Users\\Dan\\Desktop\\AI\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\modeling_utils.py\u201D, line 2406, in from_pretrained raise EnvironmentError(\
    \ OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index\
    \ or flax_model.msgpack found in directory models\\TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ."
  created_at: 2023-06-03 14:03:11+00:00
  edited: false
  hidden: false
  id: 647b562fe3a7d24c8e496897
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3bcf9102fec6271a12ca52c345119dd0.svg
      fullname: Danny M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaymerDanny
      type: user
    createdAt: '2023-06-03T18:34:07.000Z'
    data:
      edited: false
      editors:
      - GaymerDanny
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6518339514732361
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3bcf9102fec6271a12ca52c345119dd0.svg
          fullname: Danny M
          isHf: false
          isPro: false
          name: GaymerDanny
          type: user
        html: '<p>so after I  changed transformers gpu-memory it loaded succefully
          but now when i type it gives back nothing<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/647b555f45616c3de3618f4d/bvcHK1tolK-YnHVwSwerV.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/647b555f45616c3de3618f4d/bvcHK1tolK-YnHVwSwerV.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/647b555f45616c3de3618f4d/Qsooc1lIbls-zzg5RNPoC.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/647b555f45616c3de3618f4d/Qsooc1lIbls-zzg5RNPoC.png"></a></p>

          '
        raw: 'so after I  changed transformers gpu-memory it loaded succefully but
          now when i type it gives back nothing

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/647b555f45616c3de3618f4d/bvcHK1tolK-YnHVwSwerV.png)


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/647b555f45616c3de3618f4d/Qsooc1lIbls-zzg5RNPoC.png)'
        updatedAt: '2023-06-03T18:34:07.098Z'
      numEdits: 0
      reactions: []
    id: 647b879fb31514a4a6e0b8ca
    type: comment
  author: GaymerDanny
  content: 'so after I  changed transformers gpu-memory it loaded succefully but now
    when i type it gives back nothing

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/647b555f45616c3de3618f4d/bvcHK1tolK-YnHVwSwerV.png)


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/647b555f45616c3de3618f4d/Qsooc1lIbls-zzg5RNPoC.png)'
  created_at: 2023-06-03 17:34:07+00:00
  edited: false
  hidden: false
  id: 647b879fb31514a4a6e0b8ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3bcf9102fec6271a12ca52c345119dd0.svg
      fullname: Danny M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GaymerDanny
      type: user
    createdAt: '2023-06-03T22:11:45.000Z'
    data:
      edited: false
      editors:
      - GaymerDanny
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9699708819389343
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3bcf9102fec6271a12ca52c345119dd0.svg
          fullname: Danny M
          isHf: false
          isPro: false
          name: GaymerDanny
          type: user
        html: '<p>ok i chnged pre layer from 0 to 7 and now it is generating very
          very slow responses even tho i had a nvidia rtx 2060 MAX<br>I think all
          these problems coming mainly from my setting or smt i have no idea how to
          worjk with oobabonga like what am i missing</p>

          '
        raw: 'ok i chnged pre layer from 0 to 7 and now it is generating very very
          slow responses even tho i had a nvidia rtx 2060 MAX

          I think all these problems coming mainly from my setting or smt i have no
          idea how to worjk with oobabonga like what am i missing'
        updatedAt: '2023-06-03T22:11:45.873Z'
      numEdits: 0
      reactions: []
    id: 647bbaa1b31514a4a6e6df44
    type: comment
  author: GaymerDanny
  content: 'ok i chnged pre layer from 0 to 7 and now it is generating very very slow
    responses even tho i had a nvidia rtx 2060 MAX

    I think all these problems coming mainly from my setting or smt i have no idea
    how to worjk with oobabonga like what am i missing'
  created_at: 2023-06-03 21:11:45+00:00
  edited: false
  hidden: false
  id: 647bbaa1b31514a4a6e6df44
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: It wont load
