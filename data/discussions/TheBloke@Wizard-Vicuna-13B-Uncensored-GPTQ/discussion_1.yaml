!!python/object:huggingface_hub.community.DiscussionWithDetails
author: VladCorvi
conflicting_files: null
created_at: 2023-05-13 13:12:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e889702b747899cad6655bcf94df1dcf.svg
      fullname: Vladivillian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VladCorvi
      type: user
    createdAt: '2023-05-13T14:12:10.000Z'
    data:
      edited: false
      editors:
      - VladCorvi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e889702b747899cad6655bcf94df1dcf.svg
          fullname: Vladivillian
          isHf: false
          isPro: false
          name: VladCorvi
          type: user
        html: "<p>Cant run this model on my rig with 32 gb ram and rtx 3050 8gb vram</p>\n\
          <p>Getting this error:<br>Traceback (most recent call last):<br>File \u201C\
          G:\\LLM\\oobabooga\\text-generation-webui\\server.py\u201D, line 67, in\
          \ load_model_wrapper<br>shared.model, shared.tokenizer = load_model(shared.model_name)<br>File\
          \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\modules\\models.py\u201D\
          , line 159, in load_model<br>model = load_quantized(model_name)<br>File\
          \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 175, in load_quantized<br>model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, shared.args.pre_layer)<br>File\
          \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
          llama_inference_offload.py\u201D, line 221, in load_quant<br>make_quant(model,\
          \ layers, wbits, groupsize)<br>File \u201CG:\\LLM\\oobabooga\\text-generation-webui\\\
          repositories\\GPTQ-for-LLaMa\\quant.py\u201D, line 148, in make_quant<br>make_quant(child,\
          \ names, bits, groupsize, name + \u2018.\u2019 + name1 if name != \u2018\
          \u2019 else name1)<br>File \u201CG:\\LLM\\oobabooga\\text-generation-webui\\\
          repositories\\GPTQ-for-LLaMa\\quant.py\u201D, line 148, in make_quant<br>make_quant(child,\
          \ names, bits, groupsize, name + \u2018.\u2019 + name1 if name != \u2018\
          \u2019 else name1)<br>File \u201CG:\\LLM\\oobabooga\\text-generation-webui\\\
          repositories\\GPTQ-for-LLaMa\\quant.py\u201D, line 148, in make_quant<br>make_quant(child,\
          \ names, bits, groupsize, name + \u2018.\u2019 + name1 if name != \u2018\
          \u2019 else name1)<br>[Previous line repeated 1 more time]<br>File \u201C\
          G:\\LLM\\oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
          quant.py\u201D, line 146, in make_quant<br>setattr(module, attr, QuantLinear(bits,\
          \ groupsize, tmp.in_features, tmp.out_features, tmp.bias is not None))<br>File\
          \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
          quant.py\u201D, line 161, in init<br>self.register_buffer(\u2018qweight\u2019\
          , torch.zeros((infeatures // 32 * self.bits, outfeatures), dtype=torch.int32))<br>RuntimeError:\
          \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\\
          alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried\
          \ to allocate 13107200 bytes.</p>\n"
        raw: "Cant run this model on my rig with 32 gb ram and rtx 3050 8gb vram\r\
          \n\r\nGetting this error:\r\nTraceback (most recent call last):\r\nFile\
          \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\server.py\u201D, line\
          \ 67, in load_model_wrapper\r\nshared.model, shared.tokenizer = load_model(shared.model_name)\r\
          \nFile \u201CG:\\LLM\\oobabooga\\text-generation-webui\\modules\\models.py\u201D\
          , line 159, in load_model\r\nmodel = load_quantized(model_name)\r\nFile\
          \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 175, in load_quantized\r\nmodel = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, shared.args.pre_layer)\r\
          \nFile \u201CG:\\LLM\\oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
          llama_inference_offload.py\u201D, line 221, in load_quant\r\nmake_quant(model,\
          \ layers, wbits, groupsize)\r\nFile \u201CG:\\LLM\\oobabooga\\text-generation-webui\\\
          repositories\\GPTQ-for-LLaMa\\quant.py\u201D, line 148, in make_quant\r\n\
          make_quant(child, names, bits, groupsize, name + \u2018.\u2019 + name1 if\
          \ name != \u2018\u2019 else name1)\r\nFile \u201CG:\\LLM\\oobabooga\\text-generation-webui\\\
          repositories\\GPTQ-for-LLaMa\\quant.py\u201D, line 148, in make_quant\r\n\
          make_quant(child, names, bits, groupsize, name + \u2018.\u2019 + name1 if\
          \ name != \u2018\u2019 else name1)\r\nFile \u201CG:\\LLM\\oobabooga\\text-generation-webui\\\
          repositories\\GPTQ-for-LLaMa\\quant.py\u201D, line 148, in make_quant\r\n\
          make_quant(child, names, bits, groupsize, name + \u2018.\u2019 + name1 if\
          \ name != \u2018\u2019 else name1)\r\n[Previous line repeated 1 more time]\r\
          \nFile \u201CG:\\LLM\\oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
          quant.py\u201D, line 146, in make_quant\r\nsetattr(module, attr, QuantLinear(bits,\
          \ groupsize, tmp.in_features, tmp.out_features, tmp.bias is not None))\r\
          \nFile \u201CG:\\LLM\\oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
          quant.py\u201D, line 161, in init\r\nself.register_buffer(\u2018qweight\u2019\
          , torch.zeros((infeatures // 32 * self.bits, outfeatures), dtype=torch.int32))\r\
          \nRuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\\
          core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory:\
          \ you tried to allocate 13107200 bytes."
        updatedAt: '2023-05-13T14:12:10.388Z'
      numEdits: 0
      reactions: []
    id: 645f9aba7c7bdadaa2d30d9f
    type: comment
  author: VladCorvi
  content: "Cant run this model on my rig with 32 gb ram and rtx 3050 8gb vram\r\n\
    \r\nGetting this error:\r\nTraceback (most recent call last):\r\nFile \u201CG:\\\
    LLM\\oobabooga\\text-generation-webui\\server.py\u201D, line 67, in load_model_wrapper\r\
    \nshared.model, shared.tokenizer = load_model(shared.model_name)\r\nFile \u201C\
    G:\\LLM\\oobabooga\\text-generation-webui\\modules\\models.py\u201D, line 159,\
    \ in load_model\r\nmodel = load_quantized(model_name)\r\nFile \u201CG:\\LLM\\\
    oobabooga\\text-generation-webui\\modules\\GPTQ_loader.py\u201D, line 175, in\
    \ load_quantized\r\nmodel = load_quant(str(path_to_model), str(pt_path), shared.args.wbits,\
    \ shared.args.groupsize, shared.args.pre_layer)\r\nFile \u201CG:\\LLM\\oobabooga\\\
    text-generation-webui\\repositories\\GPTQ-for-LLaMa\\llama_inference_offload.py\u201D\
    , line 221, in load_quant\r\nmake_quant(model, layers, wbits, groupsize)\r\nFile\
    \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
    quant.py\u201D, line 148, in make_quant\r\nmake_quant(child, names, bits, groupsize,\
    \ name + \u2018.\u2019 + name1 if name != \u2018\u2019 else name1)\r\nFile \u201C\
    G:\\LLM\\oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\u201D\
    , line 148, in make_quant\r\nmake_quant(child, names, bits, groupsize, name +\
    \ \u2018.\u2019 + name1 if name != \u2018\u2019 else name1)\r\nFile \u201CG:\\\
    LLM\\oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\u201D\
    , line 148, in make_quant\r\nmake_quant(child, names, bits, groupsize, name +\
    \ \u2018.\u2019 + name1 if name != \u2018\u2019 else name1)\r\n[Previous line\
    \ repeated 1 more time]\r\nFile \u201CG:\\LLM\\oobabooga\\text-generation-webui\\\
    repositories\\GPTQ-for-LLaMa\\quant.py\u201D, line 146, in make_quant\r\nsetattr(module,\
    \ attr, QuantLinear(bits, groupsize, tmp.in_features, tmp.out_features, tmp.bias\
    \ is not None))\r\nFile \u201CG:\\LLM\\oobabooga\\text-generation-webui\\repositories\\\
    GPTQ-for-LLaMa\\quant.py\u201D, line 161, in init\r\nself.register_buffer(\u2018\
    qweight\u2019, torch.zeros((infeatures // 32 * self.bits, outfeatures), dtype=torch.int32))\r\
    \nRuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\\
    impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried\
    \ to allocate 13107200 bytes."
  created_at: 2023-05-13 13:12:10+00:00
  edited: false
  hidden: false
  id: 645f9aba7c7bdadaa2d30d9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T14:13:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Have you been able to load other 13B GPTQ models OK?</p>

          <p>I''m not quite sure what the best method for CPU offload is meant to
          be right now.  Try using --pre_layer instead, eg --pre_layer 10. That''s
          the CPU offload mode designed for GPTQ specifically</p>

          '
        raw: 'Have you been able to load other 13B GPTQ models OK?


          I''m not quite sure what the best method for CPU offload is meant to be
          right now.  Try using --pre_layer instead, eg --pre_layer 10. That''s the
          CPU offload mode designed for GPTQ specifically'
        updatedAt: '2023-05-13T14:13:53.945Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - VladCorvi
    id: 645f9b2125a4075bcf9b35e0
    type: comment
  author: TheBloke
  content: 'Have you been able to load other 13B GPTQ models OK?


    I''m not quite sure what the best method for CPU offload is meant to be right
    now.  Try using --pre_layer instead, eg --pre_layer 10. That''s the CPU offload
    mode designed for GPTQ specifically'
  created_at: 2023-05-13 13:13:53+00:00
  edited: false
  hidden: false
  id: 645f9b2125a4075bcf9b35e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e889702b747899cad6655bcf94df1dcf.svg
      fullname: Vladivillian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VladCorvi
      type: user
    createdAt: '2023-05-13T14:33:38.000Z'
    data:
      edited: true
      editors:
      - VladCorvi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e889702b747899cad6655bcf94df1dcf.svg
          fullname: Vladivillian
          isHf: false
          isPro: false
          name: VladCorvi
          type: user
        html: "<p>Thank you for a fast response! After increasing Windows page file\
          \ to 128gb it's give me another error log.  Also --pre_layer 10 is changed\
          \ nothing.<br>For 13B GPTQ. I have WizardML-Unc-13b-Q5_1-ggml that running\
          \ only on CPU but works well.</p>\n<p>Traceback (most recent call last):<br>File\
          \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\server.py\u201D, line\
          \ 67, in load_model_wrapper<br>shared.model, shared.tokenizer = load_model(shared.model_name)<br>File\
          \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\modules\\models.py\u201D\
          , line 159, in load_model<br>model = load_quantized(model_name)<br>File\
          \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 175, in load_quantized<br>model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, shared.args.pre_layer)<br>File\
          \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
          llama_inference_offload.py\u201D, line 226, in load_quant<br>model.load_state_dict(safe_load(checkpoint))<br>File\
          \ \u201CG:\\LLM\\oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\u201D, line 2041, in load_state_dict<br>raise RuntimeError(\u2018\
          Error(s) in loading state_dict for {}:\\n\\t{}\u2019.format(<br>RuntimeError:\
          \ Error(s) in loading state_dict for LlamaForCausalLM:<br>Missing key(s)\
          \ in state_dict: \u201Cmodel.layers.0.self_attn.k_proj.g_idx\u201D, \u201C\
          model.layers.0.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.0.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.0.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.0.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.0.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.0.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.1.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.1.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.1.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.2.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.2.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.2.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.2.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.2.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.2.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.2.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.3.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.3.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.3.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.4.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.4.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.4.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.4.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.4.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.4.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.4.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.5.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.5.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.5.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.6.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.6.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.6.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.6.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.6.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.6.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.6.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.7.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.7.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.7.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.8.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.8.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.8.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.8.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.8.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.8.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.8.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.9.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.9.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.9.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.10.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.10.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.10.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.10.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.10.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.10.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.10.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.11.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.11.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.11.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.12.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.12.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.12.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.12.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.12.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.12.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.12.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.13.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.13.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.13.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.14.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.14.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.14.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.14.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.14.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.14.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.14.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.15.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.15.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.15.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.16.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.16.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.16.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.16.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.16.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.16.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.16.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.17.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.17.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.17.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.18.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.18.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.18.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.18.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.18.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.18.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.18.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.19.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.19.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.19.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.20.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.20.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.20.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.20.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.20.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.20.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.20.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.21.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.21.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.21.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.22.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.22.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.22.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.22.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.22.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.22.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.22.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.23.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.23.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.23.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.24.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.24.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.24.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.24.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.24.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.24.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.24.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.25.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.25.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.25.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.26.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.26.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.26.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.26.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.26.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.26.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.26.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.27.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.27.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.27.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.28.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.28.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.28.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.28.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.28.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.28.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.28.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.29.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.29.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.29.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.30.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.30.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.30.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.30.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.30.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.30.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.30.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.31.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.31.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.31.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.32.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.32.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.32.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.32.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.32.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.32.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.32.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.33.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.33.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.33.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.34.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.34.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.34.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.34.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.34.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.34.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.34.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.35.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.35.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.35.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.36.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.36.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.36.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.36.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.36.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.36.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.36.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.37.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.37.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.37.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.38.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.38.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.38.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.38.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.38.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.38.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.38.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.39.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.39.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.39.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.mlp.up_proj.g_idx\u201D.<br>Unexpected key(s) in\
          \ state_dict: \u201Cmodel.layers.0.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.0.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.0.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.0.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.0.mlp.down_proj.bias\u201D, \u201Cmodel.layers.0.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.0.mlp.up_proj.bias\u201D, \u201Cmodel.layers.1.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.1.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.1.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.1.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.1.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.1.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.1.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.2.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.2.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.2.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.2.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.2.mlp.down_proj.bias\u201D, \u201Cmodel.layers.2.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.2.mlp.up_proj.bias\u201D, \u201Cmodel.layers.3.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.3.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.3.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.3.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.3.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.3.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.3.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.4.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.4.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.4.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.4.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.4.mlp.down_proj.bias\u201D, \u201Cmodel.layers.4.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.4.mlp.up_proj.bias\u201D, \u201Cmodel.layers.5.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.5.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.5.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.5.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.5.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.5.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.5.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.6.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.6.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.6.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.6.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.6.mlp.down_proj.bias\u201D, \u201Cmodel.layers.6.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.6.mlp.up_proj.bias\u201D, \u201Cmodel.layers.7.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.7.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.7.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.7.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.7.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.7.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.7.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.8.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.8.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.8.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.8.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.8.mlp.down_proj.bias\u201D, \u201Cmodel.layers.8.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.8.mlp.up_proj.bias\u201D, \u201Cmodel.layers.9.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.9.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.9.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.9.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.9.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.9.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.9.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.10.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.10.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.10.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.10.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.10.mlp.down_proj.bias\u201D, \u201Cmodel.layers.10.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.10.mlp.up_proj.bias\u201D, \u201Cmodel.layers.11.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.11.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.11.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.11.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.11.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.11.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.11.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.12.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.12.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.12.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.12.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.12.mlp.down_proj.bias\u201D, \u201Cmodel.layers.12.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.12.mlp.up_proj.bias\u201D, \u201Cmodel.layers.13.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.13.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.13.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.13.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.13.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.13.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.13.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.14.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.14.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.14.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.14.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.14.mlp.down_proj.bias\u201D, \u201Cmodel.layers.14.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.14.mlp.up_proj.bias\u201D, \u201Cmodel.layers.15.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.15.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.15.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.15.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.15.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.15.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.15.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.16.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.16.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.16.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.16.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.16.mlp.down_proj.bias\u201D, \u201Cmodel.layers.16.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.16.mlp.up_proj.bias\u201D, \u201Cmodel.layers.17.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.17.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.17.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.17.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.17.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.17.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.17.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.18.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.18.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.18.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.18.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.18.mlp.down_proj.bias\u201D, \u201Cmodel.layers.18.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.18.mlp.up_proj.bias\u201D, \u201Cmodel.layers.19.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.19.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.19.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.19.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.19.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.19.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.19.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.20.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.20.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.20.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.20.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.20.mlp.down_proj.bias\u201D, \u201Cmodel.layers.20.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.20.mlp.up_proj.bias\u201D, \u201Cmodel.layers.21.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.21.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.21.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.21.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.21.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.21.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.21.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.22.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.22.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.22.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.22.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.22.mlp.down_proj.bias\u201D, \u201Cmodel.layers.22.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.22.mlp.up_proj.bias\u201D, \u201Cmodel.layers.23.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.23.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.23.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.23.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.23.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.23.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.23.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.24.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.24.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.24.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.24.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.24.mlp.down_proj.bias\u201D, \u201Cmodel.layers.24.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.24.mlp.up_proj.bias\u201D, \u201Cmodel.layers.25.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.25.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.25.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.25.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.25.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.25.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.25.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.26.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.26.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.26.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.26.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.26.mlp.down_proj.bias\u201D, \u201Cmodel.layers.26.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.26.mlp.up_proj.bias\u201D, \u201Cmodel.layers.27.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.27.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.27.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.27.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.27.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.27.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.27.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.28.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.28.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.28.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.28.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.28.mlp.down_proj.bias\u201D, \u201Cmodel.layers.28.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.28.mlp.up_proj.bias\u201D, \u201Cmodel.layers.29.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.29.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.29.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.29.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.29.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.29.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.29.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.30.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.30.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.30.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.30.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.30.mlp.down_proj.bias\u201D, \u201Cmodel.layers.30.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.30.mlp.up_proj.bias\u201D, \u201Cmodel.layers.31.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.31.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.31.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.31.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.31.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.31.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.31.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.32.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.32.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.32.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.32.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.32.mlp.down_proj.bias\u201D, \u201Cmodel.layers.32.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.32.mlp.up_proj.bias\u201D, \u201Cmodel.layers.33.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.33.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.33.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.33.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.33.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.33.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.33.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.34.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.34.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.34.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.34.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.34.mlp.down_proj.bias\u201D, \u201Cmodel.layers.34.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.34.mlp.up_proj.bias\u201D, \u201Cmodel.layers.35.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.35.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.35.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.35.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.35.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.35.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.35.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.36.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.36.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.36.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.36.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.36.mlp.down_proj.bias\u201D, \u201Cmodel.layers.36.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.36.mlp.up_proj.bias\u201D, \u201Cmodel.layers.37.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.37.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.37.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.37.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.37.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.37.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.37.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.38.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.38.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.38.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.38.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.38.mlp.down_proj.bias\u201D, \u201Cmodel.layers.38.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.38.mlp.up_proj.bias\u201D, \u201Cmodel.layers.39.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.39.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.39.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.39.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.39.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.39.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.39.mlp.up_proj.bias\u201D\
          .</p>\n"
        raw: "Thank you for a fast response! After increasing Windows page file to\
          \ 128gb it's give me another error log.  Also --pre_layer 10 is changed\
          \ nothing. \nFor 13B GPTQ. I have WizardML-Unc-13b-Q5_1-ggml that running\
          \ only on CPU but works well.\n\nTraceback (most recent call last):\nFile\
          \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\server.py\u201D, line\
          \ 67, in load_model_wrapper\nshared.model, shared.tokenizer = load_model(shared.model_name)\n\
          File \u201CG:\\LLM\\oobabooga\\text-generation-webui\\modules\\models.py\u201D\
          , line 159, in load_model\nmodel = load_quantized(model_name)\nFile \u201C\
          G:\\LLM\\oobabooga\\text-generation-webui\\modules\\GPTQ_loader.py\u201D\
          , line 175, in load_quantized\nmodel = load_quant(str(path_to_model), str(pt_path),\
          \ shared.args.wbits, shared.args.groupsize, shared.args.pre_layer)\nFile\
          \ \u201CG:\\LLM\\oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
          llama_inference_offload.py\u201D, line 226, in load_quant\nmodel.load_state_dict(safe_load(checkpoint))\n\
          File \u201CG:\\LLM\\oobabooga\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\u201D, line 2041, in load_state_dict\nraise\
          \ RuntimeError(\u2018Error(s) in loading state_dict for {}:\\n\\t{}\u2019\
          .format(\nRuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\n\
          Missing key(s) in state_dict: \u201Cmodel.layers.0.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.0.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.0.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.0.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.0.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.0.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.0.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.1.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.1.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.1.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.1.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.2.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.2.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.2.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.2.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.2.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.2.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.2.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.3.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.3.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.3.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.3.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.4.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.4.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.4.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.4.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.4.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.4.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.4.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.5.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.5.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.5.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.5.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.6.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.6.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.6.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.6.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.6.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.6.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.6.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.7.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.7.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.7.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.7.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.8.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.8.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.8.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.8.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.8.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.8.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.8.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.9.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.9.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.9.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.9.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.10.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.10.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.10.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.10.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.10.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.10.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.10.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.11.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.11.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.11.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.11.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.12.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.12.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.12.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.12.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.12.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.12.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.12.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.13.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.13.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.13.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.13.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.14.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.14.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.14.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.14.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.14.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.14.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.14.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.15.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.15.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.15.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.15.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.16.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.16.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.16.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.16.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.16.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.16.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.16.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.17.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.17.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.17.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.17.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.18.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.18.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.18.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.18.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.18.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.18.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.18.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.19.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.19.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.19.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.19.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.20.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.20.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.20.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.20.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.20.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.20.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.20.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.21.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.21.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.21.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.21.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.22.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.22.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.22.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.22.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.22.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.22.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.22.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.23.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.23.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.23.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.23.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.24.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.24.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.24.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.24.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.24.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.24.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.24.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.25.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.25.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.25.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.25.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.26.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.26.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.26.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.26.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.26.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.26.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.26.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.27.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.27.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.27.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.27.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.28.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.28.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.28.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.28.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.28.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.28.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.28.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.29.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.29.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.29.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.29.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.30.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.30.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.30.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.30.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.30.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.30.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.30.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.31.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.31.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.31.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.31.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.32.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.32.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.32.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.32.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.32.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.32.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.32.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.33.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.33.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.33.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.33.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.34.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.34.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.34.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.34.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.34.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.34.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.34.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.35.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.35.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.35.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.35.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.36.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.36.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.36.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.36.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.36.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.36.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.36.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.37.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.37.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.37.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.37.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.38.self_attn.k_proj.g_idx\u201D\
          , \u201Cmodel.layers.38.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.38.self_attn.q_proj.g_idx\u201D\
          , \u201Cmodel.layers.38.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.38.mlp.down_proj.g_idx\u201D\
          , \u201Cmodel.layers.38.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.38.mlp.up_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.39.self_attn.o_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.39.self_attn.v_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.39.mlp.gate_proj.g_idx\u201D\
          , \u201Cmodel.layers.39.mlp.up_proj.g_idx\u201D.\nUnexpected key(s) in state_dict:\
          \ \u201Cmodel.layers.0.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.0.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.0.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.0.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.0.mlp.down_proj.bias\u201D, \u201Cmodel.layers.0.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.0.mlp.up_proj.bias\u201D, \u201Cmodel.layers.1.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.1.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.1.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.1.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.1.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.1.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.1.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.2.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.2.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.2.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.2.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.2.mlp.down_proj.bias\u201D, \u201Cmodel.layers.2.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.2.mlp.up_proj.bias\u201D, \u201Cmodel.layers.3.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.3.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.3.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.3.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.3.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.3.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.3.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.4.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.4.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.4.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.4.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.4.mlp.down_proj.bias\u201D, \u201Cmodel.layers.4.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.4.mlp.up_proj.bias\u201D, \u201Cmodel.layers.5.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.5.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.5.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.5.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.5.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.5.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.5.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.6.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.6.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.6.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.6.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.6.mlp.down_proj.bias\u201D, \u201Cmodel.layers.6.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.6.mlp.up_proj.bias\u201D, \u201Cmodel.layers.7.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.7.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.7.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.7.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.7.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.7.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.7.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.8.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.8.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.8.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.8.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.8.mlp.down_proj.bias\u201D, \u201Cmodel.layers.8.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.8.mlp.up_proj.bias\u201D, \u201Cmodel.layers.9.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.9.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.9.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.9.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.9.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.9.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.9.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.10.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.10.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.10.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.10.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.10.mlp.down_proj.bias\u201D, \u201Cmodel.layers.10.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.10.mlp.up_proj.bias\u201D, \u201Cmodel.layers.11.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.11.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.11.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.11.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.11.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.11.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.11.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.12.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.12.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.12.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.12.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.12.mlp.down_proj.bias\u201D, \u201Cmodel.layers.12.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.12.mlp.up_proj.bias\u201D, \u201Cmodel.layers.13.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.13.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.13.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.13.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.13.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.13.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.13.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.14.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.14.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.14.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.14.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.14.mlp.down_proj.bias\u201D, \u201Cmodel.layers.14.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.14.mlp.up_proj.bias\u201D, \u201Cmodel.layers.15.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.15.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.15.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.15.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.15.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.15.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.15.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.16.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.16.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.16.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.16.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.16.mlp.down_proj.bias\u201D, \u201Cmodel.layers.16.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.16.mlp.up_proj.bias\u201D, \u201Cmodel.layers.17.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.17.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.17.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.17.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.17.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.17.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.17.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.18.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.18.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.18.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.18.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.18.mlp.down_proj.bias\u201D, \u201Cmodel.layers.18.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.18.mlp.up_proj.bias\u201D, \u201Cmodel.layers.19.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.19.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.19.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.19.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.19.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.19.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.19.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.20.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.20.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.20.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.20.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.20.mlp.down_proj.bias\u201D, \u201Cmodel.layers.20.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.20.mlp.up_proj.bias\u201D, \u201Cmodel.layers.21.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.21.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.21.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.21.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.21.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.21.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.21.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.22.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.22.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.22.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.22.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.22.mlp.down_proj.bias\u201D, \u201Cmodel.layers.22.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.22.mlp.up_proj.bias\u201D, \u201Cmodel.layers.23.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.23.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.23.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.23.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.23.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.23.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.23.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.24.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.24.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.24.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.24.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.24.mlp.down_proj.bias\u201D, \u201Cmodel.layers.24.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.24.mlp.up_proj.bias\u201D, \u201Cmodel.layers.25.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.25.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.25.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.25.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.25.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.25.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.25.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.26.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.26.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.26.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.26.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.26.mlp.down_proj.bias\u201D, \u201Cmodel.layers.26.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.26.mlp.up_proj.bias\u201D, \u201Cmodel.layers.27.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.27.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.27.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.27.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.27.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.27.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.27.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.28.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.28.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.28.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.28.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.28.mlp.down_proj.bias\u201D, \u201Cmodel.layers.28.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.28.mlp.up_proj.bias\u201D, \u201Cmodel.layers.29.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.29.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.29.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.29.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.29.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.29.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.29.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.30.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.30.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.30.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.30.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.30.mlp.down_proj.bias\u201D, \u201Cmodel.layers.30.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.30.mlp.up_proj.bias\u201D, \u201Cmodel.layers.31.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.31.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.31.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.31.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.31.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.31.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.31.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.32.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.32.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.32.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.32.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.32.mlp.down_proj.bias\u201D, \u201Cmodel.layers.32.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.32.mlp.up_proj.bias\u201D, \u201Cmodel.layers.33.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.33.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.33.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.33.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.33.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.33.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.33.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.34.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.34.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.34.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.34.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.34.mlp.down_proj.bias\u201D, \u201Cmodel.layers.34.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.34.mlp.up_proj.bias\u201D, \u201Cmodel.layers.35.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.35.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.35.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.35.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.35.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.35.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.35.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.36.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.36.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.36.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.36.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.36.mlp.down_proj.bias\u201D, \u201Cmodel.layers.36.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.36.mlp.up_proj.bias\u201D, \u201Cmodel.layers.37.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.37.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.37.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.37.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.37.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.37.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.37.mlp.up_proj.bias\u201D\
          , \u201Cmodel.layers.38.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.38.self_attn.o_proj.bias\u201D\
          , \u201Cmodel.layers.38.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.38.self_attn.v_proj.bias\u201D\
          , \u201Cmodel.layers.38.mlp.down_proj.bias\u201D, \u201Cmodel.layers.38.mlp.gate_proj.bias\u201D\
          , \u201Cmodel.layers.38.mlp.up_proj.bias\u201D, \u201Cmodel.layers.39.self_attn.k_proj.bias\u201D\
          , \u201Cmodel.layers.39.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.39.self_attn.q_proj.bias\u201D\
          , \u201Cmodel.layers.39.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.39.mlp.down_proj.bias\u201D\
          , \u201Cmodel.layers.39.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.39.mlp.up_proj.bias\u201D\
          ."
        updatedAt: '2023-05-13T14:35:40.393Z'
      numEdits: 2
      reactions: []
    id: 645f9fc24357049a57f4ee24
    type: comment
  author: VladCorvi
  content: "Thank you for a fast response! After increasing Windows page file to 128gb\
    \ it's give me another error log.  Also --pre_layer 10 is changed nothing. \n\
    For 13B GPTQ. I have WizardML-Unc-13b-Q5_1-ggml that running only on CPU but works\
    \ well.\n\nTraceback (most recent call last):\nFile \u201CG:\\LLM\\oobabooga\\\
    text-generation-webui\\server.py\u201D, line 67, in load_model_wrapper\nshared.model,\
    \ shared.tokenizer = load_model(shared.model_name)\nFile \u201CG:\\LLM\\oobabooga\\\
    text-generation-webui\\modules\\models.py\u201D, line 159, in load_model\nmodel\
    \ = load_quantized(model_name)\nFile \u201CG:\\LLM\\oobabooga\\text-generation-webui\\\
    modules\\GPTQ_loader.py\u201D, line 175, in load_quantized\nmodel = load_quant(str(path_to_model),\
    \ str(pt_path), shared.args.wbits, shared.args.groupsize, shared.args.pre_layer)\n\
    File \u201CG:\\LLM\\oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
    llama_inference_offload.py\u201D, line 226, in load_quant\nmodel.load_state_dict(safe_load(checkpoint))\n\
    File \u201CG:\\LLM\\oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\u201D, line 2041, in load_state_dict\nraise RuntimeError(\u2018\
    Error(s) in loading state_dict for {}:\\n\\t{}\u2019.format(\nRuntimeError: Error(s)\
    \ in loading state_dict for LlamaForCausalLM:\nMissing key(s) in state_dict: \u201C\
    model.layers.0.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.0.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.0.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.0.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.0.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.0.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.0.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.1.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.1.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.1.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.1.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.1.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.1.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.1.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.2.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.2.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.2.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.2.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.2.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.2.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.2.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.3.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.3.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.3.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.3.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.3.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.3.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.3.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.4.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.4.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.4.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.4.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.4.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.4.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.4.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.5.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.5.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.5.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.5.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.5.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.5.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.5.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.6.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.6.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.6.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.6.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.6.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.6.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.6.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.7.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.7.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.7.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.7.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.7.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.7.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.7.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.8.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.8.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.8.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.8.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.8.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.8.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.8.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.9.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.9.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.9.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.9.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.9.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.9.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.9.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.10.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.10.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.10.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.10.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.10.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.10.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.10.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.11.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.11.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.11.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.11.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.11.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.11.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.11.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.12.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.12.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.12.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.12.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.12.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.12.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.12.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.13.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.13.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.13.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.13.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.13.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.13.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.13.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.14.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.14.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.14.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.14.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.14.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.14.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.14.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.15.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.15.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.15.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.15.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.15.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.15.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.15.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.16.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.16.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.16.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.16.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.16.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.16.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.16.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.17.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.17.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.17.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.17.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.17.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.17.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.17.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.18.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.18.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.18.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.18.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.18.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.18.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.18.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.19.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.19.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.19.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.19.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.19.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.19.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.19.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.20.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.20.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.20.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.20.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.20.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.20.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.20.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.21.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.21.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.21.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.21.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.21.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.21.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.21.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.22.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.22.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.22.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.22.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.22.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.22.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.22.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.23.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.23.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.23.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.23.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.23.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.23.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.23.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.24.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.24.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.24.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.24.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.24.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.24.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.24.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.25.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.25.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.25.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.25.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.25.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.25.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.25.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.26.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.26.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.26.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.26.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.26.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.26.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.26.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.27.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.27.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.27.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.27.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.27.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.27.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.27.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.28.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.28.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.28.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.28.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.28.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.28.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.28.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.29.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.29.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.29.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.29.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.29.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.29.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.29.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.30.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.30.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.30.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.30.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.30.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.30.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.30.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.31.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.31.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.31.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.31.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.31.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.31.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.31.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.32.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.32.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.32.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.32.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.32.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.32.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.32.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.33.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.33.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.33.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.33.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.33.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.33.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.33.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.34.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.34.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.34.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.34.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.34.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.34.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.34.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.35.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.35.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.35.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.35.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.35.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.35.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.35.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.36.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.36.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.36.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.36.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.36.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.36.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.36.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.37.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.37.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.37.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.37.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.37.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.37.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.37.mlp.up_proj.g_idx\u201D\
    , \u201Cmodel.layers.38.self_attn.k_proj.g_idx\u201D, \u201Cmodel.layers.38.self_attn.o_proj.g_idx\u201D\
    , \u201Cmodel.layers.38.self_attn.q_proj.g_idx\u201D, \u201Cmodel.layers.38.self_attn.v_proj.g_idx\u201D\
    , \u201Cmodel.layers.38.mlp.down_proj.g_idx\u201D, \u201Cmodel.layers.38.mlp.gate_proj.g_idx\u201D\
    , \u201Cmodel.layers.38.mlp.up_proj.g_idx\u201D, \u201Cmodel.layers.39.self_attn.k_proj.g_idx\u201D\
    , \u201Cmodel.layers.39.self_attn.o_proj.g_idx\u201D, \u201Cmodel.layers.39.self_attn.q_proj.g_idx\u201D\
    , \u201Cmodel.layers.39.self_attn.v_proj.g_idx\u201D, \u201Cmodel.layers.39.mlp.down_proj.g_idx\u201D\
    , \u201Cmodel.layers.39.mlp.gate_proj.g_idx\u201D, \u201Cmodel.layers.39.mlp.up_proj.g_idx\u201D\
    .\nUnexpected key(s) in state_dict: \u201Cmodel.layers.0.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.0.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.0.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.0.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.0.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.0.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.0.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.1.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.1.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.1.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.1.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.1.mlp.down_proj.bias\u201D, \u201Cmodel.layers.1.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.1.mlp.up_proj.bias\u201D, \u201Cmodel.layers.2.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.2.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.2.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.2.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.2.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.2.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.2.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.3.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.3.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.3.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.3.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.3.mlp.down_proj.bias\u201D, \u201Cmodel.layers.3.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.3.mlp.up_proj.bias\u201D, \u201Cmodel.layers.4.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.4.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.4.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.4.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.4.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.4.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.4.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.5.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.5.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.5.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.5.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.5.mlp.down_proj.bias\u201D, \u201Cmodel.layers.5.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.5.mlp.up_proj.bias\u201D, \u201Cmodel.layers.6.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.6.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.6.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.6.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.6.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.6.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.6.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.7.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.7.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.7.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.7.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.7.mlp.down_proj.bias\u201D, \u201Cmodel.layers.7.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.7.mlp.up_proj.bias\u201D, \u201Cmodel.layers.8.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.8.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.8.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.8.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.8.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.8.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.8.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.9.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.9.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.9.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.9.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.9.mlp.down_proj.bias\u201D, \u201Cmodel.layers.9.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.9.mlp.up_proj.bias\u201D, \u201Cmodel.layers.10.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.10.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.10.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.10.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.10.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.10.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.10.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.11.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.11.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.11.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.11.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.11.mlp.down_proj.bias\u201D, \u201Cmodel.layers.11.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.11.mlp.up_proj.bias\u201D, \u201Cmodel.layers.12.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.12.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.12.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.12.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.12.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.12.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.12.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.13.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.13.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.13.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.13.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.13.mlp.down_proj.bias\u201D, \u201Cmodel.layers.13.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.13.mlp.up_proj.bias\u201D, \u201Cmodel.layers.14.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.14.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.14.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.14.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.14.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.14.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.14.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.15.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.15.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.15.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.15.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.15.mlp.down_proj.bias\u201D, \u201Cmodel.layers.15.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.15.mlp.up_proj.bias\u201D, \u201Cmodel.layers.16.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.16.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.16.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.16.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.16.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.16.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.16.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.17.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.17.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.17.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.17.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.17.mlp.down_proj.bias\u201D, \u201Cmodel.layers.17.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.17.mlp.up_proj.bias\u201D, \u201Cmodel.layers.18.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.18.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.18.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.18.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.18.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.18.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.18.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.19.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.19.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.19.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.19.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.19.mlp.down_proj.bias\u201D, \u201Cmodel.layers.19.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.19.mlp.up_proj.bias\u201D, \u201Cmodel.layers.20.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.20.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.20.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.20.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.20.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.20.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.20.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.21.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.21.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.21.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.21.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.21.mlp.down_proj.bias\u201D, \u201Cmodel.layers.21.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.21.mlp.up_proj.bias\u201D, \u201Cmodel.layers.22.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.22.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.22.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.22.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.22.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.22.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.22.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.23.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.23.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.23.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.23.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.23.mlp.down_proj.bias\u201D, \u201Cmodel.layers.23.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.23.mlp.up_proj.bias\u201D, \u201Cmodel.layers.24.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.24.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.24.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.24.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.24.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.24.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.24.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.25.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.25.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.25.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.25.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.25.mlp.down_proj.bias\u201D, \u201Cmodel.layers.25.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.25.mlp.up_proj.bias\u201D, \u201Cmodel.layers.26.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.26.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.26.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.26.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.26.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.26.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.26.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.27.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.27.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.27.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.27.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.27.mlp.down_proj.bias\u201D, \u201Cmodel.layers.27.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.27.mlp.up_proj.bias\u201D, \u201Cmodel.layers.28.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.28.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.28.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.28.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.28.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.28.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.28.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.29.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.29.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.29.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.29.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.29.mlp.down_proj.bias\u201D, \u201Cmodel.layers.29.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.29.mlp.up_proj.bias\u201D, \u201Cmodel.layers.30.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.30.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.30.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.30.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.30.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.30.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.30.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.31.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.31.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.31.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.31.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.31.mlp.down_proj.bias\u201D, \u201Cmodel.layers.31.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.31.mlp.up_proj.bias\u201D, \u201Cmodel.layers.32.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.32.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.32.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.32.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.32.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.32.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.32.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.33.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.33.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.33.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.33.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.33.mlp.down_proj.bias\u201D, \u201Cmodel.layers.33.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.33.mlp.up_proj.bias\u201D, \u201Cmodel.layers.34.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.34.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.34.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.34.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.34.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.34.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.34.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.35.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.35.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.35.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.35.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.35.mlp.down_proj.bias\u201D, \u201Cmodel.layers.35.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.35.mlp.up_proj.bias\u201D, \u201Cmodel.layers.36.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.36.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.36.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.36.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.36.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.36.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.36.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.37.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.37.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.37.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.37.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.37.mlp.down_proj.bias\u201D, \u201Cmodel.layers.37.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.37.mlp.up_proj.bias\u201D, \u201Cmodel.layers.38.self_attn.k_proj.bias\u201D\
    , \u201Cmodel.layers.38.self_attn.o_proj.bias\u201D, \u201Cmodel.layers.38.self_attn.q_proj.bias\u201D\
    , \u201Cmodel.layers.38.self_attn.v_proj.bias\u201D, \u201Cmodel.layers.38.mlp.down_proj.bias\u201D\
    , \u201Cmodel.layers.38.mlp.gate_proj.bias\u201D, \u201Cmodel.layers.38.mlp.up_proj.bias\u201D\
    , \u201Cmodel.layers.39.self_attn.k_proj.bias\u201D, \u201Cmodel.layers.39.self_attn.o_proj.bias\u201D\
    , \u201Cmodel.layers.39.self_attn.q_proj.bias\u201D, \u201Cmodel.layers.39.self_attn.v_proj.bias\u201D\
    , \u201Cmodel.layers.39.mlp.down_proj.bias\u201D, \u201Cmodel.layers.39.mlp.gate_proj.bias\u201D\
    , \u201Cmodel.layers.39.mlp.up_proj.bias\u201D."
  created_at: 2023-05-13 13:33:38+00:00
  edited: true
  hidden: false
  id: 645f9fc24357049a57f4ee24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d83060ae1625fe0b47b97d54ee4fbdd3.svg
      fullname: Alberto
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: agonzalez
      type: user
    createdAt: '2023-05-23T19:40:48.000Z'
    data:
      edited: true
      editors:
      - agonzalez
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d83060ae1625fe0b47b97d54ee4fbdd3.svg
          fullname: Alberto
          isHf: false
          isPro: false
          name: agonzalez
          type: user
        html: '<p>When runnin on nvidia rtx 3060 12GB VRAM and 2CPU + 32GB RAM i get
          this errror. Why is trying to use CPU memory if GPU has 12GB VRAM?</p>

          <p>INFO:Loading TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ...<br>INFO:Found
          the following quantized model: models\TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ\Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors<br>Traceback
          (most recent call last):<br>  File "C:\x\oobabooga_windows\text-generation-webui\server.py",
          line 1063, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\x\oobabooga_windows\text-generation-webui\modules\models.py", line 95,
          in load_model<br>    output = load_func(model_name)<br>  File "C:\x\oobabooga_windows\text-generation-webui\modules\models.py",
          line 275, in GPTQ_loader<br>    model = modules.GPTQ_loader.load_quantized(model_name)<br>  File
          "C:\x\oobabooga_windows\text-generation-webui\modules\GPTQ_loader.py", line
          177, in load_quantized<br>    model = load_quant(str(path_to_model), str(pt_path),
          shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)<br>  File
          "C:\x\oobabooga_windows\text-generation-webui\modules\GPTQ_loader.py", line
          52, in _load_quant<br>    model = AutoModelForCausalLM.from_config(config,
          trust_remote_code=shared.args.trust_remote_code)<br>  File "C:\x\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\auto\auto_factory.py",
          line 414, in from_config<br>    return model_class._from_config(config,
          **kwargs)<br>  File "C:\x\oobabooga_windows\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 1124, in _from_config<br>    model = cls(config, **kwargs)<br>  File
          "C:\x\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 615, in <strong>init</strong><br>    self.model = LlamaModel(config)<br>  File
          "C:\x\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 446, in <strong>init</strong><br>    self.layers = nn.ModuleList([LlamaDecoderLayer(config)
          for _ in range(config.num_hidden_layers)])<br>  File "C:\x\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 446, in <br>    self.layers = nn.ModuleList([LlamaDecoderLayer(config)
          for _ in range(config.num_hidden_layers)])<br>  File "C:\x\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 257, in <strong>init</strong><br>    self.mlp = LlamaMLP(<br>  File
          "C:\x\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 153, in <strong>init</strong><br>    self.down_proj = nn.Linear(intermediate_size,
          hidden_size, bias=False)<br>  File "C:\x\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\linear.py",
          line 96, in <strong>init</strong><br>    self.weight = Parameter(torch.empty((out_features,
          in_features), **factory_kwargs))<br>RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes.</p>

          <p>Done!</p>

          '
        raw: "When runnin on nvidia rtx 3060 12GB VRAM and 2CPU + 32GB RAM i get this\
          \ errror. Why is trying to use CPU memory if GPU has 12GB VRAM?\n\nINFO:Loading\
          \ TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ...\nINFO:Found the following\
          \ quantized model: models\\TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ\\\
          Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\n\
          Traceback (most recent call last):\n  File \"C:\\x\\oobabooga_windows\\\
          text-generation-webui\\server.py\", line 1063, in <module>\n    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)\n  File \"C:\\x\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 95, in load_model\n  \
          \  output = load_func(model_name)\n  File \"C:\\x\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\", line 275, in GPTQ_loader\n    model = modules.GPTQ_loader.load_quantized(model_name)\n\
          \  File \"C:\\x\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\"\
          , line 177, in load_quantized\n    model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
          \  File \"C:\\x\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\"\
          , line 52, in _load_quant\n    model = AutoModelForCausalLM.from_config(config,\
          \ trust_remote_code=shared.args.trust_remote_code)\n  File \"C:\\x\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\"\
          , line 414, in from_config\n    return model_class._from_config(config,\
          \ **kwargs)\n  File \"C:\\x\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\transformers\\modeling_utils.py\", line 1124, in _from_config\n\
          \    model = cls(config, **kwargs)\n  File \"C:\\x\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 615, in __init__\n    self.model = LlamaModel(config)\n\
          \  File \"C:\\x\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 446, in __init__\n\
          \    self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
          \  File \"C:\\x\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 446, in <listcomp>\n\
          \    self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\
          \  File \"C:\\x\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 257, in __init__\n\
          \    self.mlp = LlamaMLP(\n  File \"C:\\x\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
          , line 153, in __init__\n    self.down_proj = nn.Linear(intermediate_size,\
          \ hidden_size, bias=False)\n  File \"C:\\x\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 96, in __init__\n\
          \    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n\
          RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\\
          core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory:\
          \ you tried to allocate 141557760 bytes.\n\nDone!"
        updatedAt: '2023-05-24T21:07:28.454Z'
      numEdits: 1
      reactions: []
    id: 646d16c0acc13867a132dd40
    type: comment
  author: agonzalez
  content: "When runnin on nvidia rtx 3060 12GB VRAM and 2CPU + 32GB RAM i get this\
    \ errror. Why is trying to use CPU memory if GPU has 12GB VRAM?\n\nINFO:Loading\
    \ TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ...\nINFO:Found the following quantized\
    \ model: models\\TheBloke_Wizard-Vicuna-13B-Uncensored-GPTQ\\Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\n\
    Traceback (most recent call last):\n  File \"C:\\x\\oobabooga_windows\\text-generation-webui\\\
    server.py\", line 1063, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"C:\\x\\oobabooga_windows\\text-generation-webui\\modules\\models.py\"\
    , line 95, in load_model\n    output = load_func(model_name)\n  File \"C:\\x\\\
    oobabooga_windows\\text-generation-webui\\modules\\models.py\", line 275, in GPTQ_loader\n\
    \    model = modules.GPTQ_loader.load_quantized(model_name)\n  File \"C:\\x\\\
    oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\", line 177,\
    \ in load_quantized\n    model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n\
    \  File \"C:\\x\\oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\"\
    , line 52, in _load_quant\n    model = AutoModelForCausalLM.from_config(config,\
    \ trust_remote_code=shared.args.trust_remote_code)\n  File \"C:\\x\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\"\
    , line 414, in from_config\n    return model_class._from_config(config, **kwargs)\n\
    \  File \"C:\\x\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\modeling_utils.py\", line 1124, in _from_config\n    model = cls(config,\
    \ **kwargs)\n  File \"C:\\x\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 615, in __init__\n    self.model\
    \ = LlamaModel(config)\n  File \"C:\\x\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 446,\
    \ in __init__\n    self.layers = nn.ModuleList([LlamaDecoderLayer(config) for\
    \ _ in range(config.num_hidden_layers)])\n  File \"C:\\x\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
    , line 446, in <listcomp>\n    self.layers = nn.ModuleList([LlamaDecoderLayer(config)\
    \ for _ in range(config.num_hidden_layers)])\n  File \"C:\\x\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
    , line 257, in __init__\n    self.mlp = LlamaMLP(\n  File \"C:\\x\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
    , line 153, in __init__\n    self.down_proj = nn.Linear(intermediate_size, hidden_size,\
    \ bias=False)\n  File \"C:\\x\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\torch\\nn\\modules\\linear.py\", line 96, in __init__\n    self.weight\
    \ = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\nRuntimeError:\
    \ [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72]\
    \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760\
    \ bytes.\n\nDone!"
  created_at: 2023-05-23 18:40:48+00:00
  edited: true
  hidden: false
  id: 646d16c0acc13867a132dd40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-05T10:35:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5438297986984253
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You need to increase your Windows pagefile size</p>

          <p>Tutorial here: <a rel="nofollow" href="https://mcci.com/support/guides/how-to-change-the-windows-pagefile-size/">https://mcci.com/support/guides/how-to-change-the-windows-pagefile-size/</a></p>

          '
        raw: 'You need to increase your Windows pagefile size


          Tutorial here: https://mcci.com/support/guides/how-to-change-the-windows-pagefile-size/'
        updatedAt: '2023-06-05T10:35:15.093Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - msa523
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - VladCorvi
    id: 647dba63f14eafc3b44b05c9
    type: comment
  author: TheBloke
  content: 'You need to increase your Windows pagefile size


    Tutorial here: https://mcci.com/support/guides/how-to-change-the-windows-pagefile-size/'
  created_at: 2023-06-05 09:35:15+00:00
  edited: false
  hidden: false
  id: 647dba63f14eafc3b44b05c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/445fb86d5839e6f03c1c9a18246663a7.svg
      fullname: Azrael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: azr43l
      type: user
    createdAt: '2023-10-01T15:33:07.000Z'
    data:
      edited: false
      editors:
      - azr43l
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9718700051307678
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/445fb86d5839e6f03c1c9a18246663a7.svg
          fullname: Azrael
          isHf: false
          isPro: false
          name: azr43l
          type: user
        html: '<p>Not sure if that would be the case for you guys, but try changing
          the "Model loader" to "ExLlama". Worked perfectly fine for me, the default
          (AutoGPTQ) wasn''t working for some reason, and was giving this exact same
          error.<br>Might be worth trying other loaders too if ExLlama doesn''t work
          for you.</p>

          '
        raw: 'Not sure if that would be the case for you guys, but try changing the
          "Model loader" to "ExLlama". Worked perfectly fine for me, the default (AutoGPTQ)
          wasn''t working for some reason, and was giving this exact same error.

          Might be worth trying other loaders too if ExLlama doesn''t work for you.'
        updatedAt: '2023-10-01T15:33:07.417Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - VladCorvi
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - VladCorvi
    id: 65199133107446b24cb44804
    type: comment
  author: azr43l
  content: 'Not sure if that would be the case for you guys, but try changing the
    "Model loader" to "ExLlama". Worked perfectly fine for me, the default (AutoGPTQ)
    wasn''t working for some reason, and was giving this exact same error.

    Might be worth trying other loaders too if ExLlama doesn''t work for you.'
  created_at: 2023-10-01 14:33:07+00:00
  edited: false
  hidden: false
  id: 65199133107446b24cb44804
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: 'DefaultCPUAllocator: not enough memory'
