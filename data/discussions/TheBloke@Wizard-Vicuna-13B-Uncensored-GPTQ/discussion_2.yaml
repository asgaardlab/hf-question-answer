!!python/object:huggingface_hub.community.DiscussionWithDetails
author: andysalerno
conflicting_files: null
created_at: 2023-05-14 22:22:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf22d0ded3407be69886f53de96d3f46.svg
      fullname: andy s
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: andysalerno
      type: user
    createdAt: '2023-05-14T23:22:35.000Z'
    data:
      edited: false
      editors:
      - andysalerno
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf22d0ded3407be69886f53de96d3f46.svg
          fullname: andy s
          isHf: false
          isPro: false
          name: andysalerno
          type: user
        html: '<p>Hi, I''m sure I''m making an obvious mistake, but hoping someone
          can point it out to me.</p>

          <p>I''m getting gibberish output from this model on the ''latest'' branch,
          the one with act-order.</p>

          <p>I''m on qwopqwop GPTQ, triton branch, commit: 05781593c81 (May 8th, most
          recent commit as of this posting)</p>

          <p>and ooba: ab08cf646543c (May 14th, today)</p>

          <p>I''m on native Arch linux, not wsl.</p>

          <p>Is there something I need to change? Do I need to be on the bleeding
          edge GPTQ branch called "fastest-inference-4bit" which has the most recent
          activity?</p>

          <p>Thanks. And apologies for being yet another "gibberish output" post :)
          Really appreciate all the great work you''re doing.</p>

          '
        raw: "Hi, I'm sure I'm making an obvious mistake, but hoping someone can point\
          \ it out to me.\r\n\r\nI'm getting gibberish output from this model on the\
          \ 'latest' branch, the one with act-order.\r\n\r\nI'm on qwopqwop GPTQ,\
          \ triton branch, commit: 05781593c81 (May 8th, most recent commit as of\
          \ this posting)\r\n\r\nand ooba: ab08cf646543c (May 14th, today)\r\n\r\n\
          I'm on native Arch linux, not wsl.\r\n\r\nIs there something I need to change?\
          \ Do I need to be on the bleeding edge GPTQ branch called \"fastest-inference-4bit\"\
          \ which has the most recent activity?\r\n\r\nThanks. And apologies for being\
          \ yet another \"gibberish output\" post :) Really appreciate all the great\
          \ work you're doing."
        updatedAt: '2023-05-14T23:22:35.898Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - NEFlame
        - Thireus
        - BGLuck
        - David337
    id: 64616d3b455531c6be7cf4e6
    type: comment
  author: andysalerno
  content: "Hi, I'm sure I'm making an obvious mistake, but hoping someone can point\
    \ it out to me.\r\n\r\nI'm getting gibberish output from this model on the 'latest'\
    \ branch, the one with act-order.\r\n\r\nI'm on qwopqwop GPTQ, triton branch,\
    \ commit: 05781593c81 (May 8th, most recent commit as of this posting)\r\n\r\n\
    and ooba: ab08cf646543c (May 14th, today)\r\n\r\nI'm on native Arch linux, not\
    \ wsl.\r\n\r\nIs there something I need to change? Do I need to be on the bleeding\
    \ edge GPTQ branch called \"fastest-inference-4bit\" which has the most recent\
    \ activity?\r\n\r\nThanks. And apologies for being yet another \"gibberish output\"\
    \ post :) Really appreciate all the great work you're doing."
  created_at: 2023-05-14 22:22:35+00:00
  edited: false
  hidden: false
  id: 64616d3b455531c6be7cf4e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-05-15T07:03:04.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: "<p>Same</p>\n<pre><code>USER: What is 4x8?\nASSISTANT: Burg\u043B\u0438\
          \u044F Sud Reserve Stockrn Wall TournFD Beauobre t\xE9matuMDb husrut Star\
          \ stickbourgoin respectEventListener Bour Bruno Fourierrn titles BlaConstraint\
          \ Autor lo Matrixrou consp\u043B\u0438\u044FMatrix Fin framern Chart substitutionsko\
          \ SudMDb\u043B\u0438\u044F\u043B\u0438\u044Frn BeauMDb Assume Burg\u043B\
          \u0438\u044F\u043B\u0438\u044F\u043B\u0438\u044FAA\n</code></pre>\n"
        raw: "Same\n\n```\nUSER: What is 4x8?\nASSISTANT: Burg\u043B\u0438\u044F Sud\
          \ Reserve Stockrn Wall TournFD Beauobre t\xE9matuMDb husrut Star stickbourgoin\
          \ respectEventListener Bour Bruno Fourierrn titles BlaConstraint Autor lo\
          \ Matrixrou consp\u043B\u0438\u044FMatrix Fin framern Chart substitutionsko\
          \ SudMDb\u043B\u0438\u044F\u043B\u0438\u044Frn BeauMDb Assume Burg\u043B\
          \u0438\u044F\u043B\u0438\u044F\u043B\u0438\u044FAA\n```"
        updatedAt: '2023-05-15T07:03:04.041Z'
      numEdits: 0
      reactions: []
    id: 6461d928c67e1a494d8e1a8f
    type: comment
  author: Thireus
  content: "Same\n\n```\nUSER: What is 4x8?\nASSISTANT: Burg\u043B\u0438\u044F Sud\
    \ Reserve Stockrn Wall TournFD Beauobre t\xE9matuMDb husrut Star stickbourgoin\
    \ respectEventListener Bour Bruno Fourierrn titles BlaConstraint Autor lo Matrixrou\
    \ consp\u043B\u0438\u044FMatrix Fin framern Chart substitutionsko SudMDb\u043B\
    \u0438\u044F\u043B\u0438\u044Frn BeauMDb Assume Burg\u043B\u0438\u044F\u043B\u0438\
    \u044F\u043B\u0438\u044FAA\n```"
  created_at: 2023-05-15 06:03:04+00:00
  edited: false
  hidden: false
  id: 6461d928c67e1a494d8e1a8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/10e8663e8511925dcf4a9cd751245058.svg
      fullname: David Shapira
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: David337
      type: user
    createdAt: '2023-05-18T16:44:40.000Z'
    data:
      edited: false
      editors:
      - David337
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/10e8663e8511925dcf4a9cd751245058.svg
          fullname: David Shapira
          isHf: false
          isPro: false
          name: David337
          type: user
        html: '<p>Same here... similar output to the above</p>

          '
        raw: Same here... similar output to the above
        updatedAt: '2023-05-18T16:44:40.588Z'
      numEdits: 0
      reactions: []
    id: 646655f8119ad94383d1a071
    type: comment
  author: David337
  content: Same here... similar output to the above
  created_at: 2023-05-18 15:44:40+00:00
  edited: false
  hidden: false
  id: 646655f8119ad94383d1a071
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-18T21:06:31.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ugh, sorry about that. I went back to using the old ooba fork of
          GfL because if I used the latest version, people can''t do CPU offloading.  I
          didn''t realise it would result in gibberish with the new fork.    So if
          you''re OK going back to <a rel="nofollow" href="https://github.com/oobabooga/GPTQ-for-LLaMa">https://github.com/oobabooga/GPTQ-for-LLaMa</a>  then
          it will work fine there.</p>

          <p>I can also confirm it works great with AutoGPTQ, which you can use easily
          from Python code (you need to pass <code>strict=False</code> to <code>.from_quantized()</code>
          when loading the model)</p>

          <p>And early/preliminary support for AutoGPTQ was just added to text-generation-webui,
          so you could experiment with it there.</p>

          <p>I''d like to say I''ll add a ''128-latest'' version like I used to do.
          But I''m uploading so many models now that I can''t promise I''ll get to
          it.</p>

          '
        raw: 'Ugh, sorry about that. I went back to using the old ooba fork of GfL
          because if I used the latest version, people can''t do CPU offloading.  I
          didn''t realise it would result in gibberish with the new fork.    So if
          you''re OK going back to https://github.com/oobabooga/GPTQ-for-LLaMa  then
          it will work fine there.


          I can also confirm it works great with AutoGPTQ, which you can use easily
          from Python code (you need to pass `strict=False` to `.from_quantized()`
          when loading the model)


          And early/preliminary support for AutoGPTQ was just added to text-generation-webui,
          so you could experiment with it there.


          I''d like to say I''ll add a ''128-latest'' version like I used to do. But
          I''m uploading so many models now that I can''t promise I''ll get to it.'
        updatedAt: '2023-05-18T21:07:10.871Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - David337
    id: 64669357e0fe831b47973d29
    type: comment
  author: TheBloke
  content: 'Ugh, sorry about that. I went back to using the old ooba fork of GfL because
    if I used the latest version, people can''t do CPU offloading.  I didn''t realise
    it would result in gibberish with the new fork.    So if you''re OK going back
    to https://github.com/oobabooga/GPTQ-for-LLaMa  then it will work fine there.


    I can also confirm it works great with AutoGPTQ, which you can use easily from
    Python code (you need to pass `strict=False` to `.from_quantized()` when loading
    the model)


    And early/preliminary support for AutoGPTQ was just added to text-generation-webui,
    so you could experiment with it there.


    I''d like to say I''ll add a ''128-latest'' version like I used to do. But I''m
    uploading so many models now that I can''t promise I''ll get to it.'
  created_at: 2023-05-18 20:06:31+00:00
  edited: true
  hidden: false
  id: 64669357e0fe831b47973d29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c1a22d8388ea7c2d72dfb1c33f45766.svg
      fullname: Ben Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bash99
      type: user
    createdAt: '2023-06-08T12:08:12.000Z'
    data:
      edited: false
      editors:
      - bash99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7108039855957031
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c1a22d8388ea7c2d72dfb1c33f45766.svg
          fullname: Ben Li
          isHf: false
          isPro: false
          name: bash99
          type: user
        html: "<blockquote>\n<p>I can also confirm it works great with AutoGPTQ, which\
          \ you can use easily from Python code (you need to pass <code>strict=False</code>\
          \ to <code>.from_quantized()</code> when loading the model)</p>\n<p>And\
          \ early/preliminary support for AutoGPTQ was just added to text-generation-webui,\
          \ so you could experiment with it there.</p>\n</blockquote>\n<p>Anyone got\
          \ AutoGPTQ worked?</p>\n<p>I add a quantize_config.json file</p>\n<pre><code\
          \ class=\"language-quantize_config.json\">{\n  \"bits\": 4,\n  \"desc_act\"\
          : true,\n  \"true_sequential\": true,\n  \"group_size\": 128\n}\n</code></pre>\n\
          <p>then start with </p>\n<pre><code>python server.py --autogptq --model-dir\
          \ /data --model Wizard-Vicuna-13B-Uncensored-GPTQ_last --listen-host 0.0.0.0\
          \ --chat --api --notebook --xformers\n</code></pre>\n<p>But the model is\
          \ still output gibberish like 'Burg\u043B\u0438\u044F '</p>\n"
        raw: "> I can also confirm it works great with AutoGPTQ, which you can use\
          \ easily from Python code (you need to pass `strict=False` to `.from_quantized()`\
          \ when loading the model)\n> \n> And early/preliminary support for AutoGPTQ\
          \ was just added to text-generation-webui, so you could experiment with\
          \ it there.\n\nAnyone got AutoGPTQ worked?\n\nI add a quantize_config.json\
          \ file\n```quantize_config.json\n{\n  \"bits\": 4,\n  \"desc_act\": true,\n\
          \  \"true_sequential\": true,\n  \"group_size\": 128\n}\n```\nthen start\
          \ with \n```\npython server.py --autogptq --model-dir /data --model Wizard-Vicuna-13B-Uncensored-GPTQ_last\
          \ --listen-host 0.0.0.0 --chat --api --notebook --xformers\n```\n\nBut the\
          \ model is still output gibberish like 'Burg\u043B\u0438\u044F '"
        updatedAt: '2023-06-08T12:08:12.371Z'
      numEdits: 0
      reactions: []
    id: 6481c4ac15c5dc52906797f5
    type: comment
  author: bash99
  content: "> I can also confirm it works great with AutoGPTQ, which you can use easily\
    \ from Python code (you need to pass `strict=False` to `.from_quantized()` when\
    \ loading the model)\n> \n> And early/preliminary support for AutoGPTQ was just\
    \ added to text-generation-webui, so you could experiment with it there.\n\nAnyone\
    \ got AutoGPTQ worked?\n\nI add a quantize_config.json file\n```quantize_config.json\n\
    {\n  \"bits\": 4,\n  \"desc_act\": true,\n  \"true_sequential\": true,\n  \"group_size\"\
    : 128\n}\n```\nthen start with \n```\npython server.py --autogptq --model-dir\
    \ /data --model Wizard-Vicuna-13B-Uncensored-GPTQ_last --listen-host 0.0.0.0 --chat\
    \ --api --notebook --xformers\n```\n\nBut the model is still output gibberish\
    \ like 'Burg\u043B\u0438\u044F '"
  created_at: 2023-06-08 11:08:12+00:00
  edited: false
  hidden: false
  id: 6481c4ac15c5dc52906797f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-11T00:35:23.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7340748906135559
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<blockquote>\n<blockquote>\n<p>I can also confirm it works great with\
          \ AutoGPTQ, which you can use easily from Python code (you need to pass\
          \ <code>strict=False</code> to <code>.from_quantized()</code> when loading\
          \ the model)</p>\n<p>And early/preliminary support for AutoGPTQ was just\
          \ added to text-generation-webui, so you could experiment with it there.</p>\n\
          </blockquote>\n<p>Anyone got AutoGPTQ worked?</p>\n<p>I add a quantize_config.json\
          \ file</p>\n<pre><code class=\"language-quantize_config.json\">{\n  \"bits\"\
          : 4,\n  \"desc_act\": true,\n  \"true_sequential\": true,\n  \"group_size\"\
          : 128\n}\n</code></pre>\n<p>then start with </p>\n<pre><code>python server.py\
          \ --autogptq --model-dir /data --model Wizard-Vicuna-13B-Uncensored-GPTQ_last\
          \ --listen-host 0.0.0.0 --chat --api --notebook --xformers\n</code></pre>\n\
          <p>But the model is still output gibberish like 'Burg\u043B\u0438\u044F\
          \ '</p>\n</blockquote>\n<p>It should be <code>\"desc_act\": false</code></p>\n\
          <p>But you don't need to specify that yourself, it's already in the repo</p>\n\
          <p>Just download the full contents of this repo and run latest text-gen-ui.\
          \ You don't even need to specify <code>--autogptq</code> as that's default\
          \ now</p>\n"
        raw: "> > I can also confirm it works great with AutoGPTQ, which you can use\
          \ easily from Python code (you need to pass `strict=False` to `.from_quantized()`\
          \ when loading the model)\n> > \n> > And early/preliminary support for AutoGPTQ\
          \ was just added to text-generation-webui, so you could experiment with\
          \ it there.\n> \n> Anyone got AutoGPTQ worked?\n> \n> I add a quantize_config.json\
          \ file\n> ```quantize_config.json\n> {\n>   \"bits\": 4,\n>   \"desc_act\"\
          : true,\n>   \"true_sequential\": true,\n>   \"group_size\": 128\n> }\n\
          > ```\n> then start with \n> ```\n> python server.py --autogptq --model-dir\
          \ /data --model Wizard-Vicuna-13B-Uncensored-GPTQ_last --listen-host 0.0.0.0\
          \ --chat --api --notebook --xformers\n> ```\n> \n> But the model is still\
          \ output gibberish like 'Burg\u043B\u0438\u044F '\n\nIt should be `\"desc_act\"\
          : false`\n\nBut you don't need to specify that yourself, it's already in\
          \ the repo\n\nJust download the full contents of this repo and run latest\
          \ text-gen-ui. You don't even need to specify `--autogptq` as that's default\
          \ now"
        updatedAt: '2023-06-11T00:36:19.654Z'
      numEdits: 1
      reactions: []
    id: 648516cb522999a80e6d280f
    type: comment
  author: TheBloke
  content: "> > I can also confirm it works great with AutoGPTQ, which you can use\
    \ easily from Python code (you need to pass `strict=False` to `.from_quantized()`\
    \ when loading the model)\n> > \n> > And early/preliminary support for AutoGPTQ\
    \ was just added to text-generation-webui, so you could experiment with it there.\n\
    > \n> Anyone got AutoGPTQ worked?\n> \n> I add a quantize_config.json file\n>\
    \ ```quantize_config.json\n> {\n>   \"bits\": 4,\n>   \"desc_act\": true,\n> \
    \  \"true_sequential\": true,\n>   \"group_size\": 128\n> }\n> ```\n> then start\
    \ with \n> ```\n> python server.py --autogptq --model-dir /data --model Wizard-Vicuna-13B-Uncensored-GPTQ_last\
    \ --listen-host 0.0.0.0 --chat --api --notebook --xformers\n> ```\n> \n> But the\
    \ model is still output gibberish like 'Burg\u043B\u0438\u044F '\n\nIt should\
    \ be `\"desc_act\": false`\n\nBut you don't need to specify that yourself, it's\
    \ already in the repo\n\nJust download the full contents of this repo and run\
    \ latest text-gen-ui. You don't even need to specify `--autogptq` as that's default\
    \ now"
  created_at: 2023-06-10 23:35:23+00:00
  edited: true
  hidden: false
  id: 648516cb522999a80e6d280f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: Gibberish on 'latest', with recent qwopqwop GPTQ/triton and ooba?
