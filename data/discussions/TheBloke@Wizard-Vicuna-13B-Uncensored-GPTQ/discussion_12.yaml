!!python/object:huggingface_hub.community.DiscussionWithDetails
author: coyude
conflicting_files: null
created_at: 2023-06-11 09:31:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63df2f2815266dd945fbe4a8/Rvng0MnD2tqSbngoxY1LG.jpeg?w=200&h=200&f=face
      fullname: coyude
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: coyude
      type: user
    createdAt: '2023-06-11T10:31:09.000Z'
    data:
      edited: false
      editors:
      - coyude
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9477136135101318
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63df2f2815266dd945fbe4a8/Rvng0MnD2tqSbngoxY1LG.jpeg?w=200&h=200&f=face
          fullname: coyude
          isHf: false
          isPro: false
          name: coyude
          type: user
        html: "<p>Thank you very much, TheBloke, for your work. I have previously\
          \ used many GPTQ models created by you. Now, I want to try synthesizing\
          \ my own GPTQ model. Currently, I am trying to use the GPTQ-for-LLaMa synthesis\
          \ model, but it is only compatible with GPTQ-for-LLaMa. When I load it with\
          \ AutoGPTQ, it generates incoherent responses. Is there a way to make the\
          \ model compatible with both GPTQ-for-LLaMa and AutoGPTQ? Thank you very\
          \ much!\U0001F60A</p>\n"
        raw: "Thank you very much, TheBloke, for your work. I have previously used\
          \ many GPTQ models created by you. Now, I want to try synthesizing my own\
          \ GPTQ model. Currently, I am trying to use the GPTQ-for-LLaMa synthesis\
          \ model, but it is only compatible with GPTQ-for-LLaMa. When I load it with\
          \ AutoGPTQ, it generates incoherent responses. Is there a way to make the\
          \ model compatible with both GPTQ-for-LLaMa and AutoGPTQ? Thank you very\
          \ much!\U0001F60A"
        updatedAt: '2023-06-11T10:31:09.030Z'
      numEdits: 0
      reactions: []
    id: 6485a26dbf09d7b777830c01
    type: comment
  author: coyude
  content: "Thank you very much, TheBloke, for your work. I have previously used many\
    \ GPTQ models created by you. Now, I want to try synthesizing my own GPTQ model.\
    \ Currently, I am trying to use the GPTQ-for-LLaMa synthesis model, but it is\
    \ only compatible with GPTQ-for-LLaMa. When I load it with AutoGPTQ, it generates\
    \ incoherent responses. Is there a way to make the model compatible with both\
    \ GPTQ-for-LLaMa and AutoGPTQ? Thank you very much!\U0001F60A"
  created_at: 2023-06-11 09:31:09+00:00
  edited: false
  hidden: false
  id: 6485a26dbf09d7b777830c01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-11T10:52:08.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8474567532539368
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>AutoGPTQ should support models made with GPTQ-for-LLaMa.  Did you
          create a <code>quantize_config.json</code> file, or in Python code manually
          pass in a BaseQuantizeConfig() set up appropriately?</p>

          <p>Gibberish output usually occurs when you have a mismatch on the desc_act/--act-order
          setting.  Eg if you made the model with <code>--act-order</code> in GPTQ-for-LLaMa,
          but then didn''t set <code>"desc_act": true</code> in quantize_config.json
          for AutoGPTQ.  Or vice versa.</p>

          '
        raw: 'AutoGPTQ should support models made with GPTQ-for-LLaMa.  Did you create
          a `quantize_config.json` file, or in Python code manually pass in a BaseQuantizeConfig()
          set up appropriately?


          Gibberish output usually occurs when you have a mismatch on the desc_act/--act-order
          setting.  Eg if you made the model with `--act-order` in GPTQ-for-LLaMa,
          but then didn''t set `"desc_act": true` in quantize_config.json for AutoGPTQ.  Or
          vice versa.'
        updatedAt: '2023-06-11T10:52:08.276Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - coyude
    id: 6485a758c6ec4da9c925ba81
    type: comment
  author: TheBloke
  content: 'AutoGPTQ should support models made with GPTQ-for-LLaMa.  Did you create
    a `quantize_config.json` file, or in Python code manually pass in a BaseQuantizeConfig()
    set up appropriately?


    Gibberish output usually occurs when you have a mismatch on the desc_act/--act-order
    setting.  Eg if you made the model with `--act-order` in GPTQ-for-LLaMa, but then
    didn''t set `"desc_act": true` in quantize_config.json for AutoGPTQ.  Or vice
    versa.'
  created_at: 2023-06-11 09:52:08+00:00
  edited: false
  hidden: false
  id: 6485a758c6ec4da9c925ba81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63df2f2815266dd945fbe4a8/Rvng0MnD2tqSbngoxY1LG.jpeg?w=200&h=200&f=face
      fullname: coyude
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: coyude
      type: user
    createdAt: '2023-06-11T11:30:46.000Z'
    data:
      edited: false
      editors:
      - coyude
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8925570249557495
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63df2f2815266dd945fbe4a8/Rvng0MnD2tqSbngoxY1LG.jpeg?w=200&h=200&f=face
          fullname: coyude
          isHf: false
          isPro: false
          name: coyude
          type: user
        html: "<blockquote>\n<p>AutoGPTQ should support models made with GPTQ-for-LLaMa.\
          \  Did you create a <code>quantize_config.json</code> file, or in Python\
          \ code manually pass in a BaseQuantizeConfig() set up appropriately?</p>\n\
          <p>Gibberish output usually occurs when you have a mismatch on the desc_act/--act-order\
          \ setting.  Eg if you made the model with <code>--act-order</code> in GPTQ-for-LLaMa,\
          \ but then didn't set <code>\"desc_act\": true</code> in quantize_config.json\
          \ for AutoGPTQ.  Or vice versa.</p>\n</blockquote>\n<p>Thank you very much!\U0001F601\
          \U0001F44D The problem has been resolved.</p>\n"
        raw: "> AutoGPTQ should support models made with GPTQ-for-LLaMa.  Did you\
          \ create a `quantize_config.json` file, or in Python code manually pass\
          \ in a BaseQuantizeConfig() set up appropriately?\n> \n> Gibberish output\
          \ usually occurs when you have a mismatch on the desc_act/--act-order setting.\
          \  Eg if you made the model with `--act-order` in GPTQ-for-LLaMa, but then\
          \ didn't set `\"desc_act\": true` in quantize_config.json for AutoGPTQ.\
          \  Or vice versa.\n\nThank you very much!\U0001F601\U0001F44D The problem\
          \ has been resolved."
        updatedAt: '2023-06-11T11:30:46.046Z'
      numEdits: 0
      reactions: []
    id: 6485b066a3893fa104f4e277
    type: comment
  author: coyude
  content: "> AutoGPTQ should support models made with GPTQ-for-LLaMa.  Did you create\
    \ a `quantize_config.json` file, or in Python code manually pass in a BaseQuantizeConfig()\
    \ set up appropriately?\n> \n> Gibberish output usually occurs when you have a\
    \ mismatch on the desc_act/--act-order setting.  Eg if you made the model with\
    \ `--act-order` in GPTQ-for-LLaMa, but then didn't set `\"desc_act\": true` in\
    \ quantize_config.json for AutoGPTQ.  Or vice versa.\n\nThank you very much!\U0001F601\
    \U0001F44D The problem has been resolved."
  created_at: 2023-06-11 10:30:46+00:00
  edited: false
  hidden: false
  id: 6485b066a3893fa104f4e277
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63df2f2815266dd945fbe4a8/Rvng0MnD2tqSbngoxY1LG.jpeg?w=200&h=200&f=face
      fullname: coyude
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: coyude
      type: user
    createdAt: '2023-06-11T11:31:02.000Z'
    data:
      status: closed
    id: 6485b076a3893fa104f4e90f
    type: status-change
  author: coyude
  created_at: 2023-06-11 10:31:02+00:00
  id: 6485b076a3893fa104f4e90f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ
repo_type: model
status: closed
target_branch: null
title: Doubts about using the AutoGPTQ conversion model
