!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Mincookie
conflicting_files: null
created_at: 2023-06-26 13:15:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657377941883-noauth.jpeg?w=200&h=200&f=face
      fullname: Min T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mincookie
      type: user
    createdAt: '2023-06-26T14:15:33.000Z'
    data:
      edited: false
      editors:
      - Mincookie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4408705532550812
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657377941883-noauth.jpeg?w=200&h=200&f=face
          fullname: Min T
          isHf: false
          isPro: false
          name: Mincookie
          type: user
        html: "<p>Hi there I'm trying to run an instance of this model on google colab\
          \ with the below code </p>\n<pre><code>from transformers import AutoTokenizer,\
          \ TextGenerationPipeline\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          import logging\n\nquantized_model_dir = \"/content/drive/MyDrive/Wizard-Vicuna-13B-Uncensored-GPTQ\"\
          \n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)\n\
          \ntry:\n   quantize_config = BaseQuantizeConfig.from_pretrained(quantized_model_dir)\n\
          except:\n    quantize_config = BaseQuantizeConfig(\n            bits=4,\n\
          \            group_size=128\n        )\n    \n# download quantized model\
          \ from Hugging Face Hub and load to the first GPU\nmodel = AutoGPTQForCausalLM.from_quantized(\n\
          \    quantized_model_dir, \n    device=\"cuda:0\", \n    use_safetensors=True,\
          \ \n    use_triton=False, \n    quantize_config=quantize_config\n    )\n\
          \npipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n\n\
          print(pipeline(\"auto-gptq is\")[0][\"generated_text\"])\n</code></pre>\n\
          <p>However regardless of whether I set the quantized_model_dir as a location\
          \ in my drive or in the temporary file location it will always return with\
          \ the below traceback:</p>\n<pre><code>\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E\n\u2502 in &lt;cell line: 18&gt;:18                 \
          \                                                           \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:82\
          \ in from_quantized          \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502    79 \u2502   \u2502   model_type = check_and_get_model_type(save_dir\
          \ or model_name_or_path, trust_remo   \u2502\n\u2502    80 \u2502   \u2502\
          \   quant_func = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized   \
          \                \u2502\n\u2502    81 \u2502   \u2502   keywords = {key:\
          \ kwargs[key] for key in signature(quant_func).parameters if key    \u2502\
          \n\u2502 \u2771  82 \u2502   \u2502   return quant_func(               \
          \                                                  \u2502\n\u2502    83\
          \ \u2502   \u2502   \u2502   model_name_or_path=model_name_or_path,    \
          \                                     \u2502\n\u2502    84 \u2502   \u2502\
          \   \u2502   save_dir=save_dir,                                        \
          \                     \u2502\n\u2502    85 \u2502   \u2502   \u2502   device_map=device_map,\
          \                                                         \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:698\
          \ in from_quantized        \u2502\n\u2502                              \
          \                                                                    \u2502\
          \n\u2502   695 \u2502   \u2502   \u2502   \u2502   \u2502   break      \
          \                                                            \u2502\n\u2502\
          \   696 \u2502   \u2502                                                \
          \                                      \u2502\n\u2502   697 \u2502   \u2502\
          \   if resolved_archive_file is None: # Could not find a model file to use\
          \             \u2502\n\u2502 \u2771 698 \u2502   \u2502   \u2502   raise\
          \ FileNotFoundError(f\"Could not find model in {model_name_or_path}\") \
          \      \u2502\n\u2502   699 \u2502   \u2502                            \
          \                                                          \u2502\n\u2502\
          \   700 \u2502   \u2502   model_save_name = resolved_archive_file      \
          \                                      \u2502\n\u2502   701            \
          \                                                                      \
          \          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u256F\nFileNotFoundError: Could not\
          \ find model in /content/drive/MyDrive/Wizard-Vicuna-13B-Uncensored-GPTQ\n\
          </code></pre>\n<p>I've made sure I downloaded all the files from huggingface\
          \ (including the tokenizer_config.json file) and have them in my drive.\
          \ </p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/62c9949bd0b7de324a9c3868/L4W-ABciyqyFAYJPbic9m.png\"\
          ><img alt=\"Screenshot 2023-06-27 001446.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/62c9949bd0b7de324a9c3868/L4W-ABciyqyFAYJPbic9m.png\"\
          ></a></p>\n<p>Does anyone know why it won't detect the model? </p>\n"
        raw: "Hi there I'm trying to run an instance of this model on google colab\
          \ with the below code \r\n\r\n```\r\nfrom transformers import AutoTokenizer,\
          \ TextGenerationPipeline\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\
          \nimport logging\r\n\r\nquantized_model_dir = \"/content/drive/MyDrive/Wizard-Vicuna-13B-Uncensored-GPTQ\"\
          \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)\r\
          \n\r\ntry:\r\n   quantize_config = BaseQuantizeConfig.from_pretrained(quantized_model_dir)\r\
          \nexcept:\r\n    quantize_config = BaseQuantizeConfig(\r\n            bits=4,\r\
          \n            group_size=128\r\n        )\r\n    \r\n# download quantized\
          \ model from Hugging Face Hub and load to the first GPU\r\nmodel = AutoGPTQForCausalLM.from_quantized(\r\
          \n    quantized_model_dir, \r\n    device=\"cuda:0\", \r\n    use_safetensors=True,\
          \ \r\n    use_triton=False, \r\n    quantize_config=quantize_config\r\n\
          \    )\r\n\r\npipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\r\
          \n\r\nprint(pipeline(\"auto-gptq is\")[0][\"generated_text\"])\r\n```\r\n\
          \r\nHowever regardless of whether I set the quantized_model_dir as a location\
          \ in my drive or in the temporary file location it will always return with\
          \ the below traceback:\r\n\r\n```\r\n\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E\r\n\u2502 in <cell line: 18>:18                     \
          \                                                       \u2502\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:82\
          \ in from_quantized          \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502    79 \u2502   \u2502   model_type = check_and_get_model_type(save_dir\
          \ or model_name_or_path, trust_remo   \u2502\r\n\u2502    80 \u2502   \u2502\
          \   quant_func = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized   \
          \                \u2502\r\n\u2502    81 \u2502   \u2502   keywords = {key:\
          \ kwargs[key] for key in signature(quant_func).parameters if key    \u2502\
          \r\n\u2502 \u2771  82 \u2502   \u2502   return quant_func(             \
          \                                                    \u2502\r\n\u2502  \
          \  83 \u2502   \u2502   \u2502   model_name_or_path=model_name_or_path,\
          \                                         \u2502\r\n\u2502    84 \u2502\
          \   \u2502   \u2502   save_dir=save_dir,                               \
          \                              \u2502\r\n\u2502    85 \u2502   \u2502  \
          \ \u2502   device_map=device_map,                                      \
          \                   \u2502\r\n\u2502                                   \
          \                                                               \u2502\r\
          \n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:698\
          \ in from_quantized        \u2502\r\n\u2502                            \
          \                                                                      \u2502\
          \r\n\u2502   695 \u2502   \u2502   \u2502   \u2502   \u2502   break    \
          \                                                              \u2502\r\n\
          \u2502   696 \u2502   \u2502                                           \
          \                                           \u2502\r\n\u2502   697 \u2502\
          \   \u2502   if resolved_archive_file is None: # Could not find a model\
          \ file to use             \u2502\r\n\u2502 \u2771 698 \u2502   \u2502  \
          \ \u2502   raise FileNotFoundError(f\"Could not find model in {model_name_or_path}\"\
          )       \u2502\r\n\u2502   699 \u2502   \u2502                         \
          \                                                             \u2502\r\n\
          \u2502   700 \u2502   \u2502   model_save_name = resolved_archive_file \
          \                                           \u2502\r\n\u2502   701     \
          \                                                                      \
          \                 \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nFileNotFoundError: Could\
          \ not find model in /content/drive/MyDrive/Wizard-Vicuna-13B-Uncensored-GPTQ\r\
          \n``` \r\n\r\nI've made sure I downloaded all the files from huggingface\
          \ (including the tokenizer_config.json file) and have them in my drive.\
          \ \r\n\r\n![Screenshot 2023-06-27 001446.png](https://cdn-uploads.huggingface.co/production/uploads/62c9949bd0b7de324a9c3868/L4W-ABciyqyFAYJPbic9m.png)\r\
          \n\r\nDoes anyone know why it won't detect the model? "
        updatedAt: '2023-06-26T14:15:33.713Z'
      numEdits: 0
      reactions: []
    id: 64999d8516e6180ca2c7c395
    type: comment
  author: Mincookie
  content: "Hi there I'm trying to run an instance of this model on google colab with\
    \ the below code \r\n\r\n```\r\nfrom transformers import AutoTokenizer, TextGenerationPipeline\r\
    \nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\nimport logging\r\
    \n\r\nquantized_model_dir = \"/content/drive/MyDrive/Wizard-Vicuna-13B-Uncensored-GPTQ\"\
    \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)\r\
    \n\r\ntry:\r\n   quantize_config = BaseQuantizeConfig.from_pretrained(quantized_model_dir)\r\
    \nexcept:\r\n    quantize_config = BaseQuantizeConfig(\r\n            bits=4,\r\
    \n            group_size=128\r\n        )\r\n    \r\n# download quantized model\
    \ from Hugging Face Hub and load to the first GPU\r\nmodel = AutoGPTQForCausalLM.from_quantized(\r\
    \n    quantized_model_dir, \r\n    device=\"cuda:0\", \r\n    use_safetensors=True,\
    \ \r\n    use_triton=False, \r\n    quantize_config=quantize_config\r\n    )\r\
    \n\r\npipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\r\n\r\
    \nprint(pipeline(\"auto-gptq is\")[0][\"generated_text\"])\r\n```\r\n\r\nHowever\
    \ regardless of whether I set the quantized_model_dir as a location in my drive\
    \ or in the temporary file location it will always return with the below traceback:\r\
    \n\r\n```\r\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502 in <cell line: 18>:18         \
    \                                                                   \u2502\r\n\
    \u2502                                                                       \
    \                           \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:82\
    \ in from_quantized          \u2502\r\n\u2502                                \
    \                                                                  \u2502\r\n\u2502\
    \    79 \u2502   \u2502   model_type = check_and_get_model_type(save_dir or model_name_or_path,\
    \ trust_remo   \u2502\r\n\u2502    80 \u2502   \u2502   quant_func = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized\
    \                   \u2502\r\n\u2502    81 \u2502   \u2502   keywords = {key:\
    \ kwargs[key] for key in signature(quant_func).parameters if key    \u2502\r\n\
    \u2502 \u2771  82 \u2502   \u2502   return quant_func(                       \
    \                                          \u2502\r\n\u2502    83 \u2502   \u2502\
    \   \u2502   model_name_or_path=model_name_or_path,                          \
    \               \u2502\r\n\u2502    84 \u2502   \u2502   \u2502   save_dir=save_dir,\
    \                                                             \u2502\r\n\u2502\
    \    85 \u2502   \u2502   \u2502   device_map=device_map,                    \
    \                                     \u2502\r\n\u2502                       \
    \                                                                           \u2502\
    \r\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:698\
    \ in from_quantized        \u2502\r\n\u2502                                  \
    \                                                                \u2502\r\n\u2502\
    \   695 \u2502   \u2502   \u2502   \u2502   \u2502   break                   \
    \                                               \u2502\r\n\u2502   696 \u2502\
    \   \u2502                                                                   \
    \                   \u2502\r\n\u2502   697 \u2502   \u2502   if resolved_archive_file\
    \ is None: # Could not find a model file to use             \u2502\r\n\u2502 \u2771\
    \ 698 \u2502   \u2502   \u2502   raise FileNotFoundError(f\"Could not find model\
    \ in {model_name_or_path}\")       \u2502\r\n\u2502   699 \u2502   \u2502    \
    \                                                                            \
    \      \u2502\r\n\u2502   700 \u2502   \u2502   model_save_name = resolved_archive_file\
    \                                            \u2502\r\n\u2502   701          \
    \                                                                            \
    \      \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nFileNotFoundError:\
    \ Could not find model in /content/drive/MyDrive/Wizard-Vicuna-13B-Uncensored-GPTQ\r\
    \n``` \r\n\r\nI've made sure I downloaded all the files from huggingface (including\
    \ the tokenizer_config.json file) and have them in my drive. \r\n\r\n![Screenshot\
    \ 2023-06-27 001446.png](https://cdn-uploads.huggingface.co/production/uploads/62c9949bd0b7de324a9c3868/L4W-ABciyqyFAYJPbic9m.png)\r\
    \n\r\nDoes anyone know why it won't detect the model? "
  created_at: 2023-06-26 13:15:33+00:00
  edited: false
  hidden: false
  id: 64999d8516e6180ca2c7c395
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-26T14:18:46.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6239547729492188
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Yeah you need to specify <code>model_basename</code></p>\n<pre><code\
          \ class=\"language-python\">model_basename = <span class=\"hljs-string\"\
          >\"Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order\"</span>\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(\n    quantized_model_dir,\
          \ \n    model_basename=model_basename\n    device=<span class=\"hljs-string\"\
          >\"cuda:0\"</span>, \n    use_safetensors=<span class=\"hljs-literal\">True</span>,\
          \ \n    use_triton=<span class=\"hljs-literal\">False</span>, \n    quantize_config=quantize_config\n\
          \    )\n</code></pre>\n<p>So that it knows what the name of the safetensors\
          \ file is.</p>\n"
        raw: "Yeah you need to specify `model_basename`\n\n```python\nmodel_basename\
          \ = \"Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order\"\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(\n    quantized_model_dir,\
          \ \n    model_basename=model_basename\n    device=\"cuda:0\", \n    use_safetensors=True,\
          \ \n    use_triton=False, \n    quantize_config=quantize_config\n    )\n\
          ```\n\nSo that it knows what the name of the safetensors file is."
        updatedAt: '2023-06-26T14:19:16.872Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Mincookie
    id: 64999e464e6ad14c39d19af4
    type: comment
  author: TheBloke
  content: "Yeah you need to specify `model_basename`\n\n```python\nmodel_basename\
    \ = \"Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order\"\n\nmodel\
    \ = AutoGPTQForCausalLM.from_quantized(\n    quantized_model_dir, \n    model_basename=model_basename\n\
    \    device=\"cuda:0\", \n    use_safetensors=True, \n    use_triton=False, \n\
    \    quantize_config=quantize_config\n    )\n```\n\nSo that it knows what the\
    \ name of the safetensors file is."
  created_at: 2023-06-26 13:18:46+00:00
  edited: true
  hidden: false
  id: 64999e464e6ad14c39d19af4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657377941883-noauth.jpeg?w=200&h=200&f=face
      fullname: Min T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mincookie
      type: user
    createdAt: '2023-06-26T14:33:12.000Z'
    data:
      edited: false
      editors:
      - Mincookie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9187592267990112
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657377941883-noauth.jpeg?w=200&h=200&f=face
          fullname: Min T
          isHf: false
          isPro: false
          name: Mincookie
          type: user
        html: '<p>Oh I realised I was missing the model_basename argument. All good,
          thanks! </p>

          '
        raw: 'Oh I realised I was missing the model_basename argument. All good, thanks! '
        updatedAt: '2023-06-26T14:33:12.499Z'
      numEdits: 0
      reactions: []
    id: 6499a1a848037fcfa5fb07dc
    type: comment
  author: Mincookie
  content: 'Oh I realised I was missing the model_basename argument. All good, thanks! '
  created_at: 2023-06-26 13:33:12+00:00
  edited: false
  hidden: false
  id: 6499a1a848037fcfa5fb07dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657377941883-noauth.jpeg?w=200&h=200&f=face
      fullname: Min T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mincookie
      type: user
    createdAt: '2023-06-26T14:34:15.000Z'
    data:
      edited: false
      editors:
      - Mincookie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5990511775016785
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657377941883-noauth.jpeg?w=200&h=200&f=face
          fullname: Min T
          isHf: false
          isPro: false
          name: Mincookie
          type: user
        html: "<blockquote>\n<p>Yeah you need to specify <code>model_basename</code></p>\n\
          <pre><code class=\"language-python\">model_basename = <span class=\"hljs-string\"\
          >\"Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order\"</span>\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(\n    quantized_model_dir,\
          \ \n    model_basename=model_basename\n    device=<span class=\"hljs-string\"\
          >\"cuda:0\"</span>, \n    use_safetensors=<span class=\"hljs-literal\">True</span>,\
          \ \n    use_triton=<span class=\"hljs-literal\">False</span>, \n    quantize_config=quantize_config\n\
          \    )\n</code></pre>\n<p>So that it knows what the name of the safetensors\
          \ file is.</p>\n</blockquote>\n<p>Thanks so much, I just realised the legend\
          \ of the forum answered after I posed my last comment. Cheers! </p>\n"
        raw: "> Yeah you need to specify `model_basename`\n> \n> ```python\n> model_basename\
          \ = \"Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order\"\n\
          > \n> model = AutoGPTQForCausalLM.from_quantized(\n>     quantized_model_dir,\
          \ \n>     model_basename=model_basename\n>     device=\"cuda:0\", \n>  \
          \   use_safetensors=True, \n>     use_triton=False, \n>     quantize_config=quantize_config\n\
          >     )\n> ```\n> \n> So that it knows what the name of the safetensors\
          \ file is.\n\nThanks so much, I just realised the legend of the forum answered\
          \ after I posed my last comment. Cheers! "
        updatedAt: '2023-06-26T14:34:15.201Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6499a1e76677f66c2b4b41cc
    id: 6499a1e76677f66c2b4b41cb
    type: comment
  author: Mincookie
  content: "> Yeah you need to specify `model_basename`\n> \n> ```python\n> model_basename\
    \ = \"Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order\"\n> \n\
    > model = AutoGPTQForCausalLM.from_quantized(\n>     quantized_model_dir, \n>\
    \     model_basename=model_basename\n>     device=\"cuda:0\", \n>     use_safetensors=True,\
    \ \n>     use_triton=False, \n>     quantize_config=quantize_config\n>     )\n\
    > ```\n> \n> So that it knows what the name of the safetensors file is.\n\nThanks\
    \ so much, I just realised the legend of the forum answered after I posed my last\
    \ comment. Cheers! "
  created_at: 2023-06-26 13:34:15+00:00
  edited: false
  hidden: false
  id: 6499a1e76677f66c2b4b41cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657377941883-noauth.jpeg?w=200&h=200&f=face
      fullname: Min T
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mincookie
      type: user
    createdAt: '2023-06-26T14:34:15.000Z'
    data:
      status: closed
    id: 6499a1e76677f66c2b4b41cc
    type: status-change
  author: Mincookie
  created_at: 2023-06-26 13:34:15+00:00
  id: 6499a1e76677f66c2b4b41cc
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-26T15:15:42.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9450227618217468
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You''re welcome!</p>

          '
        raw: You're welcome!
        updatedAt: '2023-06-26T15:15:42.306Z'
      numEdits: 0
      reactions: []
    id: 6499ab9ebafe07f02b00983b
    type: comment
  author: TheBloke
  content: You're welcome!
  created_at: 2023-06-26 14:15:42+00:00
  edited: false
  hidden: false
  id: 6499ab9ebafe07f02b00983b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ
repo_type: model
status: closed
target_branch: null
title: 'Model cannot be found when using with the auto_gptq library '
