!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wassname
conflicting_files: null
created_at: 2023-04-09 10:25:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-04-09T11:25:21.000Z'
    data:
      edited: false
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<p>I see you combined a few good instruction datasets. I was considering
          doing the same, but haven''t found the time. How are the results? What''s
          your impression.</p>

          <p>There seems to be a couple of school of thought, one is that is better
          to go smaller, better quality, and cleaner data. Another is to combine lower
          quality datasets to get a large amount of instruction.</p>

          <p>It seems like the alpaca datasets is the cleanest right now. So does
          this get better results that the alpaca_cleaned models?</p>

          '
        raw: "I see you combined a few good instruction datasets. I was considering\
          \ doing the same, but haven't found the time. How are the results? What's\
          \ your impression.\r\n\r\nThere seems to be a couple of school of thought,\
          \ one is that is better to go smaller, better quality, and cleaner data.\
          \ Another is to combine lower quality datasets to get a large amount of\
          \ instruction.\r\n\r\nIt seems like the alpaca datasets is the cleanest\
          \ right now. So does this get better results that the alpaca_cleaned models?"
        updatedAt: '2023-04-09T11:25:21.329Z'
      numEdits: 0
      reactions: []
    id: 6432a0a12bfb2b0ec755e38a
    type: comment
  author: wassname
  content: "I see you combined a few good instruction datasets. I was considering\
    \ doing the same, but haven't found the time. How are the results? What's your\
    \ impression.\r\n\r\nThere seems to be a couple of school of thought, one is that\
    \ is better to go smaller, better quality, and cleaner data. Another is to combine\
    \ lower quality datasets to get a large amount of instruction.\r\n\r\nIt seems\
    \ like the alpaca datasets is the cleanest right now. So does this get better\
    \ results that the alpaca_cleaned models?"
  created_at: 2023-04-09 10:25:21+00:00
  edited: false
  hidden: false
  id: 6432a0a12bfb2b0ec755e38a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jordiclive/gpt4all-alpaca-oa-codealpaca-lora-13b
repo_type: model
status: open
target_branch: null
title: What's your impression?
