!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wahab12
conflicting_files: null
created_at: 2023-11-02 06:36:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a55d8574577d41f6d25cbbb583c56d1.svg
      fullname: Muhammad Wahab
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wahab12
      type: user
    createdAt: '2023-11-02T07:36:46.000Z'
    data:
      edited: false
      editors:
      - wahab12
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6275410652160645
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a55d8574577d41f6d25cbbb583c56d1.svg
          fullname: Muhammad Wahab
          isHf: false
          isPro: false
          name: wahab12
          type: user
        html: '<p>Traceback (most recent call last):<br>  File "main.py", line 44,
          in <br>    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,<br>  File
          "/home/administrator/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py",
          line 563, in from_pretrained<br>    return model_class.from_pretrained(<br>  File
          "/home/administrator/.local/lib/python3.8/site-packages/transformers/modeling_utils.py",
          line 3298, in from_pretrained<br>    ) = cls._load_pretrained_model(<br>  File
          "/home/administrator/.local/lib/python3.8/site-packages/transformers/modeling_utils.py",
          line 3686, in _load_pretrained_model<br>    new_error_msgs, offload_index,
          state_dict_index = _load_state_dict_into_meta_model(<br>  File "/home/administrator/.local/lib/python3.8/site-packages/transformers/modeling_utils.py",
          line 741, in _load_state_dict_into_meta_model<br>    set_module_tensor_to_device(model,
          param_name, param_device, **set_module_kwargs)<br>  File "/home/administrator/.local/lib/python3.8/site-packages/accelerate/utils/modeling.py",
          line 317, in set_module_tensor_to_device<br>    new_value = value.to(device)<br>torch.cuda.OutOfMemoryError:
          CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty
          of 1.95 GiB of which 2.75 MiB is free. Including non-PyTorch memory, this
          process has 1.95 GiB memory in use. Of the allocated memory 1.92 GiB is
          allocated by PyTorch, and 1.91 MiB is reserved by PyTorch but unallocated.
          If reserved but unallocated memory is large try setting max_split_size_mb
          to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          '
        raw: "Traceback (most recent call last):\r\n  File \"main.py\", line 44, in\
          \ <module>\r\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\r\
          \n  File \"/home/administrator/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 563, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n  File \"/home/administrator/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\"\
          , line 3298, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
          \n  File \"/home/administrator/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\"\
          , line 3686, in _load_pretrained_model\r\n    new_error_msgs, offload_index,\
          \ state_dict_index = _load_state_dict_into_meta_model(\r\n  File \"/home/administrator/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\"\
          , line 741, in _load_state_dict_into_meta_model\r\n    set_module_tensor_to_device(model,\
          \ param_name, param_device, **set_module_kwargs)\r\n  File \"/home/administrator/.local/lib/python3.8/site-packages/accelerate/utils/modeling.py\"\
          , line 317, in set_module_tensor_to_device\r\n    new_value = value.to(device)\r\
          \ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00\
          \ MiB. GPU 0 has a total capacty of 1.95 GiB of which 2.75 MiB is free.\
          \ Including non-PyTorch memory, this process has 1.95 GiB memory in use.\
          \ Of the allocated memory 1.92 GiB is allocated by PyTorch, and 1.91 MiB\
          \ is reserved by PyTorch but unallocated. If reserved but unallocated memory\
          \ is large try setting max_split_size_mb to avoid fragmentation.  See documentation\
          \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
        updatedAt: '2023-11-02T07:36:46.710Z'
      numEdits: 0
      reactions: []
    id: 6543518ee5309ae0a1df325b
    type: comment
  author: wahab12
  content: "Traceback (most recent call last):\r\n  File \"main.py\", line 44, in\
    \ <module>\r\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\r\
    \n  File \"/home/administrator/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 563, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
    \  File \"/home/administrator/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\"\
    , line 3298, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File\
    \ \"/home/administrator/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\"\
    , line 3686, in _load_pretrained_model\r\n    new_error_msgs, offload_index, state_dict_index\
    \ = _load_state_dict_into_meta_model(\r\n  File \"/home/administrator/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\"\
    , line 741, in _load_state_dict_into_meta_model\r\n    set_module_tensor_to_device(model,\
    \ param_name, param_device, **set_module_kwargs)\r\n  File \"/home/administrator/.local/lib/python3.8/site-packages/accelerate/utils/modeling.py\"\
    , line 317, in set_module_tensor_to_device\r\n    new_value = value.to(device)\r\
    \ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB.\
    \ GPU 0 has a total capacty of 1.95 GiB of which 2.75 MiB is free. Including non-PyTorch\
    \ memory, this process has 1.95 GiB memory in use. Of the allocated memory 1.92\
    \ GiB is allocated by PyTorch, and 1.91 MiB is reserved by PyTorch but unallocated.\
    \ If reserved but unallocated memory is large try setting max_split_size_mb to\
    \ avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
  created_at: 2023-11-02 06:36:46+00:00
  edited: false
  hidden: false
  id: 6543518ee5309ae0a1df325b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-11-02T11:06:42.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9025553464889526
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>I believe you just have way too little vram. Try using gguf is it
          uses your ram instead of vram.</p>

          '
        raw: I believe you just have way too little vram. Try using gguf is it uses
          your ram instead of vram.
        updatedAt: '2023-11-02T11:06:42.424Z'
      numEdits: 0
      reactions: []
    id: 654382c24539d48070696cce
    type: comment
  author: YaTharThShaRma999
  content: I believe you just have way too little vram. Try using gguf is it uses
    your ram instead of vram.
  created_at: 2023-11-02 10:06:42+00:00
  edited: false
  hidden: false
  id: 654382c24539d48070696cce
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/Mistral-7B-Instruct-v0.1-GPTQ
repo_type: model
status: open
target_branch: null
title: Out of memory
