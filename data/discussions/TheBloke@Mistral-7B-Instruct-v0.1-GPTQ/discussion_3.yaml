!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abrehmaaan
conflicting_files: null
created_at: 2023-10-06 05:24:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60cd54c69021e9ff6b4d5664bb672c49.svg
      fullname: Abdul Rehman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abrehmaaan
      type: user
    createdAt: '2023-10-06T06:24:33.000Z'
    data:
      edited: false
      editors:
      - abrehmaaan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.39453521370887756
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60cd54c69021e9ff6b4d5664bb672c49.svg
          fullname: Abdul Rehman
          isHf: false
          isPro: false
          name: abrehmaaan
          type: user
        html: '<p>I am getting this error:<br>Traceback (most recent call last):<br>  File
          "C:\Users\Administrator\Projects\localGPT\run_localGPT.py", line 267, in
          <br>    main()<br>  File "C:\Program Files\Python310\lib\site-packages\click\core.py",
          line 1157, in <strong>call</strong><br>    return self.main(*args, **kwargs)<br>  File
          "C:\Program Files\Python310\lib\site-packages\click\core.py", line 1078,
          in main<br>    rv = self.invoke(ctx)<br>  File "C:\Program Files\Python310\lib\site-packages\click\core.py",
          line 1434, in invoke<br>    return ctx.invoke(self.callback, **ctx.params)<br>  File
          "C:\Program Files\Python310\lib\site-packages\click\core.py", line 783,
          in invoke<br>    return __callback(*args, **kwargs)<br>  File "C:\Users\Administrator\Projects\localGPT\run_localGPT.py",
          line 237, in main<br>    qa = retrieval_qa_pipline(device_type, use_history,
          promptTemplate_type=model_type)<br>  File "C:\Users\Administrator\Projects\localGPT\run_localGPT.py",
          line 132, in retrieval_qa_pipline<br>    llm = load_model(device_type, model_id=MODEL_ID,
          model_basename=MODEL_BASENAME, LOGGING=logging)<br>  File "C:\Users\Administrator\Projects\localGPT\run_localGPT.py",
          line 66, in load_model<br>    model, tokenizer = load_quantized_model_qptq(model_id,
          model_basename, device_type, LOGGING)<br>  File "C:\Users\Administrator\Projects\localGPT\load_models.py",
          line 95, in load_quantized_model_qptq<br>    model = AutoGPTQForCausalLM.from_quantized(<br>  File
          "C:\Program Files\Python310\lib\site-packages\auto_gptq\modeling\auto.py",
          line 79, in from_quantized<br>    model_type = check_and_get_model_type(save_dir
          or model_name_or_path, trust_remote_code)<br>  File "C:\Program Files\Python310\lib\site-packages\auto_gptq\modeling_utils.py",
          line 125, in check_and_get_model_type<br>    raise TypeError(f"{config.model_type}
          isn''t supported yet.")<br>TypeError: mistral isn''t supported yet.</p>

          <p>I am using localGPT repository from PromptEngineer.</p>

          '
        raw: "I am getting this error:\r\nTraceback (most recent call last):\r\n \
          \ File \"C:\\Users\\Administrator\\Projects\\localGPT\\run_localGPT.py\"\
          , line 267, in <module>\r\n    main()\r\n  File \"C:\\Program Files\\Python310\\\
          lib\\site-packages\\click\\core.py\", line 1157, in __call__\r\n    return\
          \ self.main(*args, **kwargs)\r\n  File \"C:\\Program Files\\Python310\\\
          lib\\site-packages\\click\\core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\
          \n  File \"C:\\Program Files\\Python310\\lib\\site-packages\\click\\core.py\"\
          , line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\
          \n  File \"C:\\Program Files\\Python310\\lib\\site-packages\\click\\core.py\"\
          , line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File\
          \ \"C:\\Users\\Administrator\\Projects\\localGPT\\run_localGPT.py\", line\
          \ 237, in main\r\n    qa = retrieval_qa_pipline(device_type, use_history,\
          \ promptTemplate_type=model_type)\r\n  File \"C:\\Users\\Administrator\\\
          Projects\\localGPT\\run_localGPT.py\", line 132, in retrieval_qa_pipline\r\
          \n    llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME,\
          \ LOGGING=logging)\r\n  File \"C:\\Users\\Administrator\\Projects\\localGPT\\\
          run_localGPT.py\", line 66, in load_model\r\n    model, tokenizer = load_quantized_model_qptq(model_id,\
          \ model_basename, device_type, LOGGING)\r\n  File \"C:\\Users\\Administrator\\\
          Projects\\localGPT\\load_models.py\", line 95, in load_quantized_model_qptq\r\
          \n    model = AutoGPTQForCausalLM.from_quantized(\r\n  File \"C:\\Program\
          \ Files\\Python310\\lib\\site-packages\\auto_gptq\\modeling\\auto.py\",\
          \ line 79, in from_quantized\r\n    model_type = check_and_get_model_type(save_dir\
          \ or model_name_or_path, trust_remote_code)\r\n  File \"C:\\Program Files\\\
          Python310\\lib\\site-packages\\auto_gptq\\modeling\\_utils.py\", line 125,\
          \ in check_and_get_model_type\r\n    raise TypeError(f\"{config.model_type}\
          \ isn't supported yet.\")\r\nTypeError: mistral isn't supported yet.\r\n\
          \r\n\r\nI am using localGPT repository from PromptEngineer."
        updatedAt: '2023-10-06T06:24:33.308Z'
      numEdits: 0
      reactions: []
    id: 651fa8210bb29b2f4ed6f8f3
    type: comment
  author: abrehmaaan
  content: "I am getting this error:\r\nTraceback (most recent call last):\r\n  File\
    \ \"C:\\Users\\Administrator\\Projects\\localGPT\\run_localGPT.py\", line 267,\
    \ in <module>\r\n    main()\r\n  File \"C:\\Program Files\\Python310\\lib\\site-packages\\\
    click\\core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\
    \n  File \"C:\\Program Files\\Python310\\lib\\site-packages\\click\\core.py\"\
    , line 1078, in main\r\n    rv = self.invoke(ctx)\r\n  File \"C:\\Program Files\\\
    Python310\\lib\\site-packages\\click\\core.py\", line 1434, in invoke\r\n    return\
    \ ctx.invoke(self.callback, **ctx.params)\r\n  File \"C:\\Program Files\\Python310\\\
    lib\\site-packages\\click\\core.py\", line 783, in invoke\r\n    return __callback(*args,\
    \ **kwargs)\r\n  File \"C:\\Users\\Administrator\\Projects\\localGPT\\run_localGPT.py\"\
    , line 237, in main\r\n    qa = retrieval_qa_pipline(device_type, use_history,\
    \ promptTemplate_type=model_type)\r\n  File \"C:\\Users\\Administrator\\Projects\\\
    localGPT\\run_localGPT.py\", line 132, in retrieval_qa_pipline\r\n    llm = load_model(device_type,\
    \ model_id=MODEL_ID, model_basename=MODEL_BASENAME, LOGGING=logging)\r\n  File\
    \ \"C:\\Users\\Administrator\\Projects\\localGPT\\run_localGPT.py\", line 66,\
    \ in load_model\r\n    model, tokenizer = load_quantized_model_qptq(model_id,\
    \ model_basename, device_type, LOGGING)\r\n  File \"C:\\Users\\Administrator\\\
    Projects\\localGPT\\load_models.py\", line 95, in load_quantized_model_qptq\r\n\
    \    model = AutoGPTQForCausalLM.from_quantized(\r\n  File \"C:\\Program Files\\\
    Python310\\lib\\site-packages\\auto_gptq\\modeling\\auto.py\", line 79, in from_quantized\r\
    \n    model_type = check_and_get_model_type(save_dir or model_name_or_path, trust_remote_code)\r\
    \n  File \"C:\\Program Files\\Python310\\lib\\site-packages\\auto_gptq\\modeling\\\
    _utils.py\", line 125, in check_and_get_model_type\r\n    raise TypeError(f\"\
    {config.model_type} isn't supported yet.\")\r\nTypeError: mistral isn't supported\
    \ yet.\r\n\r\n\r\nI am using localGPT repository from PromptEngineer."
  created_at: 2023-10-06 05:24:33+00:00
  edited: false
  hidden: false
  id: 651fa8210bb29b2f4ed6f8f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60cd54c69021e9ff6b4d5664bb672c49.svg
      fullname: Abdul Rehman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abrehmaaan
      type: user
    createdAt: '2023-10-06T06:24:46.000Z'
    data:
      edited: false
      editors:
      - abrehmaaan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.816444993019104
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60cd54c69021e9ff6b4d5664bb672c49.svg
          fullname: Abdul Rehman
          isHf: false
          isPro: false
          name: abrehmaaan
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Please help</p>\n"
        raw: '@TheBloke Please help

          '
        updatedAt: '2023-10-06T06:24:46.279Z'
      numEdits: 0
      reactions: []
    id: 651fa82e2f0ff55a7dd74e2c
    type: comment
  author: abrehmaaan
  content: '@TheBloke Please help

    '
  created_at: 2023-10-06 05:24:46+00:00
  edited: false
  hidden: false
  id: 651fa82e2f0ff55a7dd74e2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-06T09:08:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8979666233062744
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes, AutoGPTQ doesn''t support Mistral GPTQ yet. You need to use
          Transformers GPTQ, or ExLlama.</p>

          '
        raw: Yes, AutoGPTQ doesn't support Mistral GPTQ yet. You need to use Transformers
          GPTQ, or ExLlama.
        updatedAt: '2023-10-06T09:08:38.353Z'
      numEdits: 0
      reactions: []
    id: 651fce965d91101181e1773a
    type: comment
  author: TheBloke
  content: Yes, AutoGPTQ doesn't support Mistral GPTQ yet. You need to use Transformers
    GPTQ, or ExLlama.
  created_at: 2023-10-06 08:08:38+00:00
  edited: false
  hidden: false
  id: 651fce965d91101181e1773a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Mistral-7B-Instruct-v0.1-GPTQ
repo_type: model
status: open
target_branch: null
title: Mistral isn't supported yet
