!!python/object:huggingface_hub.community.DiscussionWithDetails
author: g-h-chen
conflicting_files: null
created_at: 2024-01-09 12:17:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ef00b312d2b009c8c7aab21b4b3f258.svg
      fullname: guiminghardychen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: g-h-chen
      type: user
    createdAt: '2024-01-09T12:17:22.000Z'
    data:
      edited: false
      editors:
      - g-h-chen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8477728962898254
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ef00b312d2b009c8c7aab21b4b3f258.svg
          fullname: guiminghardychen
          isHf: false
          isPro: false
          name: g-h-chen
          type: user
        html: '<p>Hi there,<br>    I trained your model with two settings:<br>    1.
          default configs<br>    2. I used <code>config._attn_implementation = "flash_attention_2"</code>
          to enable flash_attn_2.</p>

          <p>Under setting 2, the training speed is doubled compared to setting 1,
          but the <strong>loss goes high and is extremely unstable</strong>.</p>

          <p>What is the correct way of activating flash-attn? Thanks in advance!</p>

          '
        raw: "Hi there,\r\n    I trained your model with two settings:\r\n    1. default\
          \ configs\r\n    2. I used `config._attn_implementation = \"flash_attention_2\"\
          ` to enable flash_attn_2.\r\n\r\nUnder setting 2, the training speed is\
          \ doubled compared to setting 1, but the **loss goes high and is extremely\
          \ unstable**.\r\n\r\nWhat is the correct way of activating flash-attn? Thanks\
          \ in advance!"
        updatedAt: '2024-01-09T12:17:22.462Z'
      numEdits: 0
      reactions: []
    id: 659d3952d5d0c43be4aabeb1
    type: comment
  author: g-h-chen
  content: "Hi there,\r\n    I trained your model with two settings:\r\n    1. default\
    \ configs\r\n    2. I used `config._attn_implementation = \"flash_attention_2\"\
    ` to enable flash_attn_2.\r\n\r\nUnder setting 2, the training speed is doubled\
    \ compared to setting 1, but the **loss goes high and is extremely unstable**.\r\
    \n\r\nWhat is the correct way of activating flash-attn? Thanks in advance!"
  created_at: 2024-01-09 12:17:22+00:00
  edited: false
  hidden: false
  id: 659d3952d5d0c43be4aabeb1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
      fullname: Susnato Dhar
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: susnato
      type: user
    createdAt: '2024-01-09T14:57:50.000Z'
    data:
      edited: false
      editors:
      - susnato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9176436066627502
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
          fullname: Susnato Dhar
          isHf: false
          isPro: false
          name: susnato
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;g-h-chen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/g-h-chen\"\
          >@<span class=\"underline\">g-h-chen</span></a></span>\n\n\t</span></span>,\
          \ thanks for raising the issue!</p>\n<p>Is it possible to share the training\
          \ script?<br>Or maybe an example script to replicate the unstable training?</p>\n"
        raw: 'Hi @g-h-chen, thanks for raising the issue!


          Is it possible to share the training script?

          Or maybe an example script to replicate the unstable training?'
        updatedAt: '2024-01-09T14:57:50.385Z'
      numEdits: 0
      reactions: []
    id: 659d5eee1a0a2cbc10afaf21
    type: comment
  author: susnato
  content: 'Hi @g-h-chen, thanks for raising the issue!


    Is it possible to share the training script?

    Or maybe an example script to replicate the unstable training?'
  created_at: 2024-01-09 14:57:50+00:00
  edited: false
  hidden: false
  id: 659d5eee1a0a2cbc10afaf21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfea587c894154015534dfe3ad308e3b.svg
      fullname: mj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhumj34
      type: user
    createdAt: '2024-01-10T07:12:56.000Z'
    data:
      edited: false
      editors:
      - zhumj34
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7540812492370605
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfea587c894154015534dfe3ad308e3b.svg
          fullname: mj
          isHf: false
          isPro: false
          name: zhumj34
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;g-h-chen&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/g-h-chen\">@<span class=\"\
          underline\">g-h-chen</span></a></span>\n\n\t</span></span> I got the same\
          \ problem ( loss goes high) when set config._attn_implementation = \"flash_attention_2\"\
          .  Is there any way to fix this?</p>\n"
        raw: '@g-h-chen I got the same problem ( loss goes high) when set config._attn_implementation
          = "flash_attention_2".  Is there any way to fix this?'
        updatedAt: '2024-01-10T07:12:56.268Z'
      numEdits: 0
      reactions: []
    id: 659e4378e7ed0764b40a0113
    type: comment
  author: zhumj34
  content: '@g-h-chen I got the same problem ( loss goes high) when set config._attn_implementation
    = "flash_attention_2".  Is there any way to fix this?'
  created_at: 2024-01-10 07:12:56+00:00
  edited: false
  hidden: false
  id: 659e4378e7ed0764b40a0113
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
      fullname: Susnato Dhar
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: susnato
      type: user
    createdAt: '2024-01-10T12:35:45.000Z'
    data:
      edited: false
      editors:
      - susnato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9753134846687317
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
          fullname: Susnato Dhar
          isHf: false
          isPro: false
          name: susnato
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;zhumj34&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zhumj34\">@<span class=\"\
          underline\">zhumj34</span></a></span>\n\n\t</span></span>, could you please\
          \ share the training script? </p>\n<p>I will have a look at it but it is\
          \ much easier when I have the training script available :) .  </p>\n"
        raw: "Hi @zhumj34, could you please share the training script? \n\nI will\
          \ have a look at it but it is much easier when I have the training script\
          \ available :) .  "
        updatedAt: '2024-01-10T12:35:45.413Z'
      numEdits: 0
      reactions: []
    id: 659e8f21887c60124c8c5a74
    type: comment
  author: susnato
  content: "Hi @zhumj34, could you please share the training script? \n\nI will have\
    \ a look at it but it is much easier when I have the training script available\
    \ :) .  "
  created_at: 2024-01-10 12:35:45+00:00
  edited: false
  hidden: false
  id: 659e8f21887c60124c8c5a74
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
      fullname: Susnato Dhar
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: susnato
      type: user
    createdAt: '2024-01-12T13:31:52.000Z'
    data:
      edited: false
      editors:
      - susnato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9134323000907898
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
          fullname: Susnato Dhar
          isHf: false
          isPro: false
          name: susnato
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;g-h-chen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/g-h-chen\"\
          >@<span class=\"underline\">g-h-chen</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;zhumj34&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zhumj34\">@<span class=\"\
          underline\">zhumj34</span></a></span>\n\n\t</span></span>, could you please\
          \ update to transformers main and try to retrain the model again?<br>There\
          \ was a  recent commit to <code>phi</code> model on the library which fixed\
          \ an issue of fp16 logits becoming NaNs.</p>\n<p>Please update to transformers\
          \ main before re running the training script - </p>\n<pre><code>git clone\
          \ https://github.com/huggingface/transformers.git\ncd transformers\npip\
          \ install .\n</code></pre>\n<p>Please let me know if this update fixes the\
          \ issue that you are facing.</p>\n"
        raw: "Hi @g-h-chen @zhumj34, could you please update to transformers main\
          \ and try to retrain the model again?\nThere was a  recent commit to `phi`\
          \ model on the library which fixed an issue of fp16 logits becoming NaNs.\n\
          \nPlease update to transformers main before re running the training script\
          \ - \n\n```\ngit clone https://github.com/huggingface/transformers.git\n\
          cd transformers\npip install .\n```\n\nPlease let me know if this update\
          \ fixes the issue that you are facing."
        updatedAt: '2024-01-12T13:31:52.680Z'
      numEdits: 0
      reactions: []
    id: 65a13f4814ac6d6f819938a7
    type: comment
  author: susnato
  content: "Hi @g-h-chen @zhumj34, could you please update to transformers main and\
    \ try to retrain the model again?\nThere was a  recent commit to `phi` model on\
    \ the library which fixed an issue of fp16 logits becoming NaNs.\n\nPlease update\
    \ to transformers main before re running the training script - \n\n```\ngit clone\
    \ https://github.com/huggingface/transformers.git\ncd transformers\npip install\
    \ .\n```\n\nPlease let me know if this update fixes the issue that you are facing."
  created_at: 2024-01-12 13:31:52+00:00
  edited: false
  hidden: false
  id: 65a13f4814ac6d6f819938a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
      fullname: Susnato Dhar
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: susnato
      type: user
    createdAt: '2024-01-13T07:10:34.000Z'
    data:
      edited: false
      editors:
      - susnato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9530012011528015
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
          fullname: Susnato Dhar
          isHf: false
          isPro: false
          name: susnato
          type: user
        html: '<p>Hi, just a heads up, because of the recent changes made to main
          branch of <code>transformers</code> you can''t load the model properly because
          of the weights mismatch(using this repo). I will most likely fix this today.</p>

          '
        raw: Hi, just a heads up, because of the recent changes made to main branch
          of `transformers` you can't load the model properly because of the weights
          mismatch(using this repo). I will most likely fix this today.
        updatedAt: '2024-01-13T07:10:34.773Z'
      numEdits: 0
      reactions: []
    id: 65a2376a01ed2b702d6e798d
    type: comment
  author: susnato
  content: Hi, just a heads up, because of the recent changes made to main branch
    of `transformers` you can't load the model properly because of the weights mismatch(using
    this repo). I will most likely fix this today.
  created_at: 2024-01-13 07:10:34+00:00
  edited: false
  hidden: false
  id: 65a2376a01ed2b702d6e798d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfea587c894154015534dfe3ad308e3b.svg
      fullname: mj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhumj34
      type: user
    createdAt: '2024-01-15T02:10:40.000Z'
    data:
      edited: true
      editors:
      - zhumj34
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8646003603935242
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfea587c894154015534dfe3ad308e3b.svg
          fullname: mj
          isHf: false
          isPro: false
          name: zhumj34
          type: user
        html: "<blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;zhumj34&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/zhumj34\"\
          >@<span class=\"underline\">zhumj34</span></a></span>\n\n\t</span></span>,\
          \ could you please share the training script? </p>\n<p>I will have a look\
          \ at it but it is much easier when I have the training script available\
          \ :) .</p>\n</blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;susnato&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/susnato\"\
          >@<span class=\"underline\">susnato</span></a></span>\n\n\t</span></span>,\
          \ sorry for the late reply. My code is based on <a rel=\"nofollow\" href=\"\
          https://github.com/haotian-liu/LLaVA\">llava</a>. I just simply replace\
          \ llama with phi-2, and load llava-phi with  flash_attention_2 (similar\
          \ to this code, <a href=\"https://huggingface.co/docs/transformers/v4.36.1/en/model_doc/phi#combining-phi-and-flash-attention-2\"\
          >https://huggingface.co/docs/transformers/v4.36.1/en/model_doc/phi#combining-phi-and-flash-attention-2</a>).\
          \ The training script is identically to llava-v1.5.</p>\n"
        raw: "> Hi @zhumj34, could you please share the training script? \n> \n> I\
          \ will have a look at it but it is much easier when I have the training\
          \ script available :) .\n\nHi @susnato, sorry for the late reply. My code\
          \ is based on [llava](https://github.com/haotian-liu/LLaVA). I just simply\
          \ replace llama with phi-2, and load llava-phi with  flash_attention_2 (similar\
          \ to this code, https://huggingface.co/docs/transformers/v4.36.1/en/model_doc/phi#combining-phi-and-flash-attention-2).\
          \ The training script is identically to llava-v1.5."
        updatedAt: '2024-01-15T02:11:56.850Z'
      numEdits: 1
      reactions: []
    id: 65a49420ea98738768f16feb
    type: comment
  author: zhumj34
  content: "> Hi @zhumj34, could you please share the training script? \n> \n> I will\
    \ have a look at it but it is much easier when I have the training script available\
    \ :) .\n\nHi @susnato, sorry for the late reply. My code is based on [llava](https://github.com/haotian-liu/LLaVA).\
    \ I just simply replace llama with phi-2, and load llava-phi with  flash_attention_2\
    \ (similar to this code, https://huggingface.co/docs/transformers/v4.36.1/en/model_doc/phi#combining-phi-and-flash-attention-2).\
    \ The training script is identically to llava-v1.5."
  created_at: 2024-01-15 02:10:40+00:00
  edited: true
  hidden: false
  id: 65a49420ea98738768f16feb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfea587c894154015534dfe3ad308e3b.svg
      fullname: mj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhumj34
      type: user
    createdAt: '2024-01-15T02:24:45.000Z'
    data:
      edited: false
      editors:
      - zhumj34
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9210747480392456
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfea587c894154015534dfe3ad308e3b.svg
          fullname: mj
          isHf: false
          isPro: false
          name: zhumj34
          type: user
        html: "<blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;g-h-chen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/g-h-chen\"\
          >@<span class=\"underline\">g-h-chen</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;zhumj34&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zhumj34\">@<span class=\"\
          underline\">zhumj34</span></a></span>\n\n\t</span></span>, could you please\
          \ update to transformers main and try to retrain the model again?<br>There\
          \ was a  recent commit to <code>phi</code> model on the library which fixed\
          \ an issue of fp16 logits becoming NaNs.</p>\n<p>Please update to transformers\
          \ main before re running the training script - </p>\n<pre><code>git clone\
          \ https://github.com/huggingface/transformers.git\ncd transformers\npip\
          \ install .\n</code></pre>\n<p>Please let me know if this update fixes the\
          \ issue that you are facing.</p>\n</blockquote>\n<p>OK. About two weeks\
          \ ago, I finetuned llava-phi with llava-v1.5 fine-tuning script. After that\
          \ , at the inference stage, I load the model with fp16 and the output texts\
          \ are  '!!!!! ... !!!!!', which are caused by NaNs.  At that time, I thought\
          \ I'd made a mistake somewhere.</p>\n<p>I'll try this update to verify if\
          \ it solves the NaNs problem.  Sorry for the later reply.</p>\n"
        raw: "> Hi @g-h-chen @zhumj34, could you please update to transformers main\
          \ and try to retrain the model again?\n> There was a  recent commit to `phi`\
          \ model on the library which fixed an issue of fp16 logits becoming NaNs.\n\
          > \n> Please update to transformers main before re running the training\
          \ script - \n> \n> ```\n> git clone https://github.com/huggingface/transformers.git\n\
          > cd transformers\n> pip install .\n> ```\n> \n> Please let me know if this\
          \ update fixes the issue that you are facing.\n\nOK. About two weeks ago,\
          \ I finetuned llava-phi with llava-v1.5 fine-tuning script. After that ,\
          \ at the inference stage, I load the model with fp16 and the output texts\
          \ are  '!!!!! ... !!!!!', which are caused by NaNs.  At that time, I thought\
          \ I'd made a mistake somewhere.\n\nI'll try this update to verify if it\
          \ solves the NaNs problem.  Sorry for the later reply."
        updatedAt: '2024-01-15T02:24:45.569Z'
      numEdits: 0
      reactions: []
    id: 65a4976ddcfd30f7cd284799
    type: comment
  author: zhumj34
  content: "> Hi @g-h-chen @zhumj34, could you please update to transformers main\
    \ and try to retrain the model again?\n> There was a  recent commit to `phi` model\
    \ on the library which fixed an issue of fp16 logits becoming NaNs.\n> \n> Please\
    \ update to transformers main before re running the training script - \n> \n>\
    \ ```\n> git clone https://github.com/huggingface/transformers.git\n> cd transformers\n\
    > pip install .\n> ```\n> \n> Please let me know if this update fixes the issue\
    \ that you are facing.\n\nOK. About two weeks ago, I finetuned llava-phi with\
    \ llava-v1.5 fine-tuning script. After that , at the inference stage, I load the\
    \ model with fp16 and the output texts are  '!!!!! ... !!!!!', which are caused\
    \ by NaNs.  At that time, I thought I'd made a mistake somewhere.\n\nI'll try\
    \ this update to verify if it solves the NaNs problem.  Sorry for the later reply."
  created_at: 2024-01-15 02:24:45+00:00
  edited: false
  hidden: false
  id: 65a4976ddcfd30f7cd284799
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfea587c894154015534dfe3ad308e3b.svg
      fullname: mj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhumj34
      type: user
    createdAt: '2024-01-15T02:53:03.000Z'
    data:
      edited: false
      editors:
      - zhumj34
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9281466007232666
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfea587c894154015534dfe3ad308e3b.svg
          fullname: mj
          isHf: false
          isPro: false
          name: zhumj34
          type: user
        html: '<p>I notice that I pretrain and finetune llava-phi with bfp16. Is there
          anything wrong with this setup? I''ll retrain the model with this version
          of transformers.</p>

          '
        raw: I notice that I pretrain and finetune llava-phi with bfp16. Is there
          anything wrong with this setup? I'll retrain the model with this version
          of transformers.
        updatedAt: '2024-01-15T02:53:03.621Z'
      numEdits: 0
      reactions: []
    id: 65a49e0f3581a68c413444e9
    type: comment
  author: zhumj34
  content: I notice that I pretrain and finetune llava-phi with bfp16. Is there anything
    wrong with this setup? I'll retrain the model with this version of transformers.
  created_at: 2024-01-15 02:53:03+00:00
  edited: false
  hidden: false
  id: 65a49e0f3581a68c413444e9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfea587c894154015534dfe3ad308e3b.svg
      fullname: mj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhumj34
      type: user
    createdAt: '2024-01-15T03:07:14.000Z'
    data:
      edited: false
      editors:
      - zhumj34
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8438124060630798
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfea587c894154015534dfe3ad308e3b.svg
          fullname: mj
          isHf: false
          isPro: false
          name: zhumj34
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/648284f63c609184f6d7be29/denoDWAZsxTkQW24g9aCk.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/648284f63c609184f6d7be29/denoDWAZsxTkQW24g9aCk.png"></a></p>

          <p>After such an update and load model with fp16,  the model''s output is
          not  ''!!!!! ... !!!!!''. I think NaNs problem has been solved nicely. But
          I can''t load the model properly because of the weights mismatch. </p>

          '
        raw: '

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/648284f63c609184f6d7be29/denoDWAZsxTkQW24g9aCk.png)


          After such an update and load model with fp16,  the model''s output is not  ''!!!!!
          ... !!!!!''. I think NaNs problem has been solved nicely. But I can''t load
          the model properly because of the weights mismatch. '
        updatedAt: '2024-01-15T03:07:14.876Z'
      numEdits: 0
      reactions: []
    id: 65a4a1622548c41ad998c0e0
    type: comment
  author: zhumj34
  content: '

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/648284f63c609184f6d7be29/denoDWAZsxTkQW24g9aCk.png)


    After such an update and load model with fp16,  the model''s output is not  ''!!!!!
    ... !!!!!''. I think NaNs problem has been solved nicely. But I can''t load the
    model properly because of the weights mismatch. '
  created_at: 2024-01-15 03:07:14+00:00
  edited: false
  hidden: false
  id: 65a4a1622548c41ad998c0e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
      fullname: Susnato Dhar
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: susnato
      type: user
    createdAt: '2024-01-21T10:45:34.000Z'
    data:
      edited: false
      editors:
      - susnato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9027203321456909
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
          fullname: Susnato Dhar
          isHf: false
          isPro: false
          name: susnato
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;zhumj34&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/zhumj34\">@<span class=\"\
          underline\">zhumj34</span></a></span>\n\n\t</span></span> , I apologise\
          \ for the huge delay.<br>I have updated the checkpoint and it should work\
          \ now. Please install the latest transformers version 4.38.0.dev0 by running\
          \ this command -</p>\n<pre><code class=\"language-bash\">pip uninstall -y\
          \ transformers &amp;&amp; pip install git+https://github.com/huggingface/transformers\n\
          </code></pre>\n<p>After updating the library, you should be able to properly\
          \ load the weights.</p>\n<p>Please let me know if you are facing any issues\
          \ with it.</p>\n"
        raw: 'Hi @zhumj34 , I apologise for the huge delay.

          I have updated the checkpoint and it should work now. Please install the
          latest transformers version 4.38.0.dev0 by running this command -


          ```bash

          pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers

          ```


          After updating the library, you should be able to properly load the weights.


          Please let me know if you are facing any issues with it.

          '
        updatedAt: '2024-01-21T10:45:34.560Z'
      numEdits: 0
      reactions: []
    id: 65acf5ced63812c33efd1bbf
    type: comment
  author: susnato
  content: 'Hi @zhumj34 , I apologise for the huge delay.

    I have updated the checkpoint and it should work now. Please install the latest
    transformers version 4.38.0.dev0 by running this command -


    ```bash

    pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers

    ```


    After updating the library, you should be able to properly load the weights.


    Please let me know if you are facing any issues with it.

    '
  created_at: 2024-01-21 10:45:34+00:00
  edited: false
  hidden: false
  id: 65acf5ced63812c33efd1bbf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
      fullname: Susnato Dhar
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: susnato
      type: user
    createdAt: '2024-01-21T10:51:53.000Z'
    data:
      edited: true
      editors:
      - susnato
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9403104186058044
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e6f6d78aa1e72c5eb39cbfe5da896cb.svg
          fullname: Susnato Dhar
          isHf: false
          isPro: false
          name: susnato
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;g-h-chen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/g-h-chen\"\
          >@<span class=\"underline\">g-h-chen</span></a></span>\n\n\t</span></span>,\
          \ regarding the problem you are facing</p>\n<blockquote>\n<p>Under setting\
          \ 2, the training speed is doubled compared to setting 1, but the loss goes\
          \ high and is extremely unstable.</p>\n</blockquote>\n<p>There is an ongoing\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/28488#issuecomment-1898044816\"\
          >issue</a> at transformers where people are reporting the same thing as\
          \ you said above.  I guess Gugarosa is working on fixing it.<br>In the meantime\
          \ please try to fine-tune it without FA2 (as I have updated the checkpoint\
          \ you can properly load the weights), sorry for the inconvenience.</p>\n"
        raw: "Hi @g-h-chen, regarding the problem you are facing\n> Under setting\
          \ 2, the training speed is doubled compared to setting 1, but the loss goes\
          \ high and is extremely unstable.\n\nThere is an ongoing [issue](https://github.com/huggingface/transformers/issues/28488#issuecomment-1898044816)\
          \ at transformers where people are reporting the same thing as you said\
          \ above.  I guess Gugarosa is working on fixing it. \nIn the meantime please\
          \ try to fine-tune it without FA2 (as I have updated the checkpoint you\
          \ can properly load the weights), sorry for the inconvenience."
        updatedAt: '2024-01-21T10:53:49.154Z'
      numEdits: 1
      reactions: []
    id: 65acf749d6b4c93ba63473a5
    type: comment
  author: susnato
  content: "Hi @g-h-chen, regarding the problem you are facing\n> Under setting 2,\
    \ the training speed is doubled compared to setting 1, but the loss goes high\
    \ and is extremely unstable.\n\nThere is an ongoing [issue](https://github.com/huggingface/transformers/issues/28488#issuecomment-1898044816)\
    \ at transformers where people are reporting the same thing as you said above.\
    \  I guess Gugarosa is working on fixing it. \nIn the meantime please try to fine-tune\
    \ it without FA2 (as I have updated the checkpoint you can properly load the weights),\
    \ sorry for the inconvenience."
  created_at: 2024-01-21 10:51:53+00:00
  edited: true
  hidden: false
  id: 65acf749d6b4c93ba63473a5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: susnato/phi-2
repo_type: model
status: open
target_branch: null
title: different results w/ and w/o flash-attn
