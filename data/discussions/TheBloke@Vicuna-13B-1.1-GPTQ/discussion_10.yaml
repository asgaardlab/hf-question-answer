!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Fimbul
conflicting_files: null
created_at: 2023-04-29 20:41:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ff9ff38c1d3f94ffc76295e8ed3e720.svg
      fullname: Saul Lopez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fimbul
      type: user
    createdAt: '2023-04-29T21:41:21.000Z'
    data:
      edited: false
      editors:
      - Fimbul
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ff9ff38c1d3f94ffc76295e8ed3e720.svg
          fullname: Saul Lopez
          isHf: false
          isPro: false
          name: Fimbul
          type: user
        html: "<p>When I'm trying to use this model with oogabooga web ui I'm getting\
          \ this kind of responses and I don't know why:</p>\n<p>Input:<br>introduce\
          \ yourself<br>Output:<br>/_mysinside phys chairphys AlcUSTontmymoGP\uFFFD\
          \u2260 monuments _ _alu _ _concurrent jsf preced///_mysmysmysmys _ fsmys/_mysmys\
          \ _mys _ _ _ _ _ _ / phys phys/ phys _ mys _mysmys _leep\u0434\u0440\u0430\
          / Phys/_mysmys/_mys _ _mysmys pr\xE9c\xE9d _mysextend _mys _ _mysmys _ _\
          \ _ _ _ _Physmys _mysmys _mysmysmys _ Alcmysmys _ _ Alc _ AlcWF Alc _ _\
          \ _ _ _ _ _ _ _ _ _ _ _ _ _ Alc Alc _g _ _ Alc _ Alc _ _ _ Alc Alc _ _ _\
          \ Alc Alc Alc _ _ Alc Alc _ Alc Alc Alc Alc Alc _ _ _ _ _ _ _ _ Alc _o Alc\
          \ _mymymy _ _ _ _ _ _ _mymymymymymymymymymymymy _PR _ont _ontmyontmymyont\
          \ Alc</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/644d8e496d84e25fad77cea4/WTtGWuhWWduxaqBPmQA6D.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/644d8e496d84e25fad77cea4/WTtGWuhWWduxaqBPmQA6D.png\"\
          ></a></p>\n"
        raw: "When I'm trying to use this model with oogabooga web ui I'm getting\
          \ this kind of responses and I don't know why:\r\n\r\nInput:\r\nintroduce\
          \ yourself\r\nOutput:\r\n/_mysinside phys chairphys AlcUSTontmymoGP\uFFFD\
          \u2260 monuments _ _alu _ _concurrent jsf preced///_mysmysmysmys _ fsmys/_mysmys\
          \ _mys _ _ _ _ _ _ / phys phys/ phys _ mys _mysmys _leep\u0434\u0440\u0430\
          / Phys/_mysmys/_mys _ _mysmys pr\xE9c\xE9d _mysextend _mys _ _mysmys _ _\
          \ _ _ _ _Physmys _mysmys _mysmysmys _ Alcmysmys _ _ Alc _ AlcWF Alc _ _\
          \ _ _ _ _ _ _ _ _ _ _ _ _ _ Alc Alc _g _ _ Alc _ Alc _ _ _ Alc Alc _ _ _\
          \ Alc Alc Alc _ _ Alc Alc _ Alc Alc Alc Alc Alc _ _ _ _ _ _ _ _ Alc _o Alc\
          \ _mymymy _ _ _ _ _ _ _mymymymymymymymymymymymy _PR _ont _ontmyontmymyont\
          \ Alc\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/644d8e496d84e25fad77cea4/WTtGWuhWWduxaqBPmQA6D.png)\r\
          \n"
        updatedAt: '2023-04-29T21:41:21.252Z'
      numEdits: 0
      reactions: []
    id: 644d8f016dfd5f8240dcfd98
    type: comment
  author: Fimbul
  content: "When I'm trying to use this model with oogabooga web ui I'm getting this\
    \ kind of responses and I don't know why:\r\n\r\nInput:\r\nintroduce yourself\r\
    \nOutput:\r\n/_mysinside phys chairphys AlcUSTontmymoGP\uFFFD\u2260 monuments\
    \ _ _alu _ _concurrent jsf preced///_mysmysmysmys _ fsmys/_mysmys _mys _ _ _ _\
    \ _ _ / phys phys/ phys _ mys _mysmys _leep\u0434\u0440\u0430/ Phys/_mysmys/_mys\
    \ _ _mysmys pr\xE9c\xE9d _mysextend _mys _ _mysmys _ _ _ _ _ _Physmys _mysmys\
    \ _mysmysmys _ Alcmysmys _ _ Alc _ AlcWF Alc _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Alc\
    \ Alc _g _ _ Alc _ Alc _ _ _ Alc Alc _ _ _ Alc Alc Alc _ _ Alc Alc _ Alc Alc Alc\
    \ Alc Alc _ _ _ _ _ _ _ _ Alc _o Alc _mymymy _ _ _ _ _ _ _mymymymymymymymymymymymy\
    \ _PR _ont _ontmyontmymyont Alc\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/644d8e496d84e25fad77cea4/WTtGWuhWWduxaqBPmQA6D.png)\r\
    \n"
  created_at: 2023-04-29 20:41:21+00:00
  edited: false
  hidden: false
  id: 644d8f016dfd5f8240dcfd98
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T21:54:07.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please delete the file ending <code>latest.act-order.safetensor</code>
          and load file <code>compat.no-act-order.safetensor</code> instead</p>

          '
        raw: Please delete the file ending `latest.act-order.safetensor` and load
          file `compat.no-act-order.safetensor` instead
        updatedAt: '2023-04-29T21:54:07.560Z'
      numEdits: 0
      reactions: []
    id: 644d91ff328c1aa30e4dfb2b
    type: comment
  author: TheBloke
  content: Please delete the file ending `latest.act-order.safetensor` and load file
    `compat.no-act-order.safetensor` instead
  created_at: 2023-04-29 20:54:07+00:00
  edited: false
  hidden: false
  id: 644d91ff328c1aa30e4dfb2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf0eaf89fa1ea6ad1a350d41a82f03af.svg
      fullname: Shin Asura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kusoge
      type: user
    createdAt: '2023-05-05T06:31:07.000Z'
    data:
      edited: false
      editors:
      - kusoge
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf0eaf89fa1ea6ad1a350d41a82f03af.svg
          fullname: Shin Asura
          isHf: false
          isPro: false
          name: kusoge
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Been using your\
          \ quantized model and it works great. Any chance you'll be making a quantized\
          \ version for the GPT4All-13B-snoozy?</p>\n"
        raw: '@TheBloke Been using your quantized model and it works great. Any chance
          you''ll be making a quantized version for the GPT4All-13B-snoozy?'
        updatedAt: '2023-05-05T06:31:07.890Z'
      numEdits: 0
      reactions: []
    id: 6454a2abd55525a4fedbc4c1
    type: comment
  author: kusoge
  content: '@TheBloke Been using your quantized model and it works great. Any chance
    you''ll be making a quantized version for the GPT4All-13B-snoozy?'
  created_at: 2023-05-05 05:31:07+00:00
  edited: false
  hidden: false
  id: 6454a2abd55525a4fedbc4c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T09:00:25.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh sure, happy to. I kept checking the Nomic repo around the time
          they first released it, but it was never uploaded to HF.  But I see it has
          been now.</p>

          <p>I''m starting the process now!</p>

          '
        raw: 'Oh sure, happy to. I kept checking the Nomic repo around the time they
          first released it, but it was never uploaded to HF.  But I see it has been
          now.


          I''m starting the process now!'
        updatedAt: '2023-05-05T09:01:31.205Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - kusoge
    id: 6454c5a9d55525a4fedf85e5
    type: comment
  author: TheBloke
  content: 'Oh sure, happy to. I kept checking the Nomic repo around the time they
    first released it, but it was never uploaded to HF.  But I see it has been now.


    I''m starting the process now!'
  created_at: 2023-05-05 08:00:25+00:00
  edited: true
  hidden: false
  id: 6454c5a9d55525a4fedf85e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T21:52:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Forgot to come back here and say, it''s done! </p>

          <ul>

          <li><a href="https://huggingface.co/TheBloke/GPT4ALL-13B-snoozy-GPTQ">4bit
          GPTQ models for GPU inference</a>.</li>

          <li><a href="https://huggingface.co/TheBloke/GPT4ALL-13B-snoozy-GGML">4bit
          and 5bit GGML models for GPU inference</a>.</li>

          <li><a href="https://huggingface.co/nomic-ai/gpt4all-13b-snoozy">Nomic.AI''s
          original model in float32 HF for GPU inference</a>.</li>

          </ul>

          '
        raw: "Forgot to come back here and say, it's done! \n\n* [4bit GPTQ models\
          \ for GPU inference](https://huggingface.co/TheBloke/GPT4ALL-13B-snoozy-GPTQ).\n\
          * [4bit and 5bit GGML models for GPU inference](https://huggingface.co/TheBloke/GPT4ALL-13B-snoozy-GGML).\n\
          * [Nomic.AI's original model in float32 HF for GPU inference](https://huggingface.co/nomic-ai/gpt4all-13b-snoozy)."
        updatedAt: '2023-05-05T21:52:16.901Z'
      numEdits: 0
      reactions: []
    id: 64557a90fe2f48cb4b6f899c
    type: comment
  author: TheBloke
  content: "Forgot to come back here and say, it's done! \n\n* [4bit GPTQ models for\
    \ GPU inference](https://huggingface.co/TheBloke/GPT4ALL-13B-snoozy-GPTQ).\n*\
    \ [4bit and 5bit GGML models for GPU inference](https://huggingface.co/TheBloke/GPT4ALL-13B-snoozy-GGML).\n\
    * [Nomic.AI's original model in float32 HF for GPU inference](https://huggingface.co/nomic-ai/gpt4all-13b-snoozy)."
  created_at: 2023-05-05 20:52:16+00:00
  edited: false
  hidden: false
  id: 64557a90fe2f48cb4b6f899c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a9d5b4ec624900991fd8212fb033abff.svg
      fullname: BooBoo Brainsample
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BooBoo992001
      type: user
    createdAt: '2023-05-07T16:13:19.000Z'
    data:
      edited: false
      editors:
      - BooBoo992001
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a9d5b4ec624900991fd8212fb033abff.svg
          fullname: BooBoo Brainsample
          isHf: false
          isPro: false
          name: BooBoo992001
          type: user
        html: '<p>Having the same problem but can''t find the files you specified.
          Where should I look and/or download?</p>

          '
        raw: Having the same problem but can't find the files you specified. Where
          should I look and/or download?
        updatedAt: '2023-05-07T16:13:19.040Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F614"
        users:
        - hussainwali1
        - Kirisuma
    id: 6457ce1f06d739e82b1a8d21
    type: comment
  author: BooBoo992001
  content: Having the same problem but can't find the files you specified. Where should
    I look and/or download?
  created_at: 2023-05-07 15:13:19+00:00
  edited: false
  hidden: false
  id: 6457ce1f06d739e82b1a8d21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49ea25d58acd0682ccbaacb325929aaf.svg
      fullname: Krishnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gowthamkrishnan
      type: user
    createdAt: '2023-07-27T12:19:47.000Z'
    data:
      edited: false
      editors:
      - Gowthamkrishnan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45102566480636597
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49ea25d58acd0682ccbaacb325929aaf.svg
          fullname: Krishnan
          isHf: false
          isPro: false
          name: Gowthamkrishnan
          type: user
        html: '<p>Hey Bloke,  I tried with both 4 bit quantised 7B and 13B .safetensors
          models.<br>The final output looks gibberish. Can u pls let me know what
          am i missing in the below code? </p>

          <p>'' '' ''<br>from transformers import AutoTokenizer, pipeline, logging<br>from
          auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig<br>import argparse</p>

          <p>quantized_model_dir = "/content/drive/MyDrive/Vicuna/FastChat/models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g_actorder"<br>model_basename
          = "/content/drive/MyDrive/Vicuna/FastChat/models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g_actorder/vicuna-7B-1.1-GPTQ-4bit-128g"</p>

          <p>use_triton = False<br>tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,
          use_fast=True)<br>quantize_config = BaseQuantizeConfig(<br>bits=4,<br>group_size=128,<br>desc_act=False<br>)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized( quantized_model_dir,<br>                                            use_safetensors=True,<br>                                            model_basename=model_basename,<br>                                            device="cuda:0",<br>                                            use_triton=use_triton,<br>                                            quantize_config=quantize_config<br>                                          )</p>

          <p>prompt = """ """</p>

          <p>inputs = tokenizer(prompt, return_tensors=''pt'').to(''cuda'')<br>tokens
          = model.generate(<br> **inputs,<br> max_new_tokens=2000,<br> do_sample=True,<br>
          temperature=1.0,<br> top_p=1.0,</p>

          <h1 id="truncationtrue">truncation=True</h1>

          <p>)<br>print(tokenizer.decode(tokens[0], skip_special_tokens=True))</p>

          <p>'' '' ''</p>

          '
        raw: "Hey Bloke,  I tried with both 4 bit quantised 7B and 13B .safetensors\
          \ models.\nThe final output looks gibberish. Can u pls let me know what\
          \ am i missing in the below code? \n\n' ' '\nfrom transformers import AutoTokenizer,\
          \ pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          import argparse\n\nquantized_model_dir = \"/content/drive/MyDrive/Vicuna/FastChat/models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g_actorder\"\
          \nmodel_basename = \"/content/drive/MyDrive/Vicuna/FastChat/models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g_actorder/vicuna-7B-1.1-GPTQ-4bit-128g\"\
          \n\nuse_triton = False\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)\nquantize_config = BaseQuantizeConfig(\nbits=4,\ngroup_size=128,\n\
          desc_act=False\n)\n\nmodel = AutoGPTQForCausalLM.from_quantized( quantized_model_dir,\n\
          \                                            use_safetensors=True,\n   \
          \                                         model_basename=model_basename,\n\
          \                                            device=\"cuda:0\",\n      \
          \                                      use_triton=use_triton,\n        \
          \                                    quantize_config=quantize_config\n \
          \                                         )\n\nprompt = \"\"\" <input query>\"\
          \"\"\n\ninputs = tokenizer(prompt, return_tensors='pt').to('cuda')\ntokens\
          \ = model.generate(\n **inputs,\n max_new_tokens=2000,\n do_sample=True,\n\
          \ temperature=1.0,\n top_p=1.0,\n#  truncation=True\n)\nprint(tokenizer.decode(tokens[0],\
          \ skip_special_tokens=True))\n\n' ' '"
        updatedAt: '2023-07-27T12:19:47.769Z'
      numEdits: 0
      reactions: []
    id: 64c260e3275309cb8bdd322c
    type: comment
  author: Gowthamkrishnan
  content: "Hey Bloke,  I tried with both 4 bit quantised 7B and 13B .safetensors\
    \ models.\nThe final output looks gibberish. Can u pls let me know what am i missing\
    \ in the below code? \n\n' ' '\nfrom transformers import AutoTokenizer, pipeline,\
    \ logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport\
    \ argparse\n\nquantized_model_dir = \"/content/drive/MyDrive/Vicuna/FastChat/models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g_actorder\"\
    \nmodel_basename = \"/content/drive/MyDrive/Vicuna/FastChat/models/TheBloke_vicuna-7B-1.1-GPTQ-4bit-128g_actorder/vicuna-7B-1.1-GPTQ-4bit-128g\"\
    \n\nuse_triton = False\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
    \ use_fast=True)\nquantize_config = BaseQuantizeConfig(\nbits=4,\ngroup_size=128,\n\
    desc_act=False\n)\n\nmodel = AutoGPTQForCausalLM.from_quantized( quantized_model_dir,\n\
    \                                            use_safetensors=True,\n         \
    \                                   model_basename=model_basename,\n         \
    \                                   device=\"cuda:0\",\n                     \
    \                       use_triton=use_triton,\n                             \
    \               quantize_config=quantize_config\n                            \
    \              )\n\nprompt = \"\"\" <input query>\"\"\"\n\ninputs = tokenizer(prompt,\
    \ return_tensors='pt').to('cuda')\ntokens = model.generate(\n **inputs,\n max_new_tokens=2000,\n\
    \ do_sample=True,\n temperature=1.0,\n top_p=1.0,\n#  truncation=True\n)\nprint(tokenizer.decode(tokens[0],\
    \ skip_special_tokens=True))\n\n' ' '"
  created_at: 2023-07-27 11:19:47+00:00
  edited: false
  hidden: false
  id: 64c260e3275309cb8bdd322c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-27T13:59:10.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8879153728485107
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>There was a bug in AutoGPTQ 0.3.0 that causes gibberish when you
          use a model with both group_size and desc_act.</p>

          <p>It can be fixed by updating to AutoGPTQ 0.3.1 or 0.3.2.  I recommend
          to build from source at the moment due to some issues people are having
          installing from PyPi:</p>

          <pre><code>pip3 uninstall -y auto-gptq

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip3 install .

          </code></pre>

          '
        raw: 'There was a bug in AutoGPTQ 0.3.0 that causes gibberish when you use
          a model with both group_size and desc_act.


          It can be fixed by updating to AutoGPTQ 0.3.1 or 0.3.2.  I recommend to
          build from source at the moment due to some issues people are having installing
          from PyPi:

          ```

          pip3 uninstall -y auto-gptq

          git clone https://github.com/PanQiWei/AutoGPTQ

          cd AutoGPTQ

          pip3 install .

          ```'
        updatedAt: '2023-07-27T13:59:10.940Z'
      numEdits: 0
      reactions: []
    id: 64c2782e617b36543de7aac8
    type: comment
  author: TheBloke
  content: 'There was a bug in AutoGPTQ 0.3.0 that causes gibberish when you use a
    model with both group_size and desc_act.


    It can be fixed by updating to AutoGPTQ 0.3.1 or 0.3.2.  I recommend to build
    from source at the moment due to some issues people are having installing from
    PyPi:

    ```

    pip3 uninstall -y auto-gptq

    git clone https://github.com/PanQiWei/AutoGPTQ

    cd AutoGPTQ

    pip3 install .

    ```'
  created_at: 2023-07-27 12:59:10+00:00
  edited: false
  hidden: false
  id: 64c2782e617b36543de7aac8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: TheBloke/Vicuna-13B-1.1-GPTQ
repo_type: model
status: open
target_branch: null
title: Model throws gibberish instead of actual response.
