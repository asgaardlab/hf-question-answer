!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hyperhuzaifa
conflicting_files: null
created_at: 2023-04-23 20:29:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7794a0efd248499a106c679594e206c2.svg
      fullname: Huzaifa Arab
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hyperhuzaifa
      type: user
    createdAt: '2023-04-23T21:29:49.000Z'
    data:
      edited: false
      editors:
      - hyperhuzaifa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7794a0efd248499a106c679594e206c2.svg
          fullname: Huzaifa Arab
          isHf: false
          isPro: false
          name: hyperhuzaifa
          type: user
        html: '<p>Its kinda tiring to either manually clone and wget the act-order
          version.<br>Anyone know how to do that?</p>

          '
        raw: "Its kinda tiring to either manually clone and wget the act-order version.\r\
          \nAnyone know how to do that?"
        updatedAt: '2023-04-23T21:29:49.866Z'
      numEdits: 0
      reactions: []
    id: 6445a34d14d3df8e1301aa40
    type: comment
  author: hyperhuzaifa
  content: "Its kinda tiring to either manually clone and wget the act-order version.\r\
    \nAnyone know how to do that?"
  created_at: 2023-04-23 20:29:49+00:00
  edited: false
  hidden: false
  id: 6445a34d14d3df8e1301aa40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-23T22:17:34.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I don''t. I''ve not looked into how HF''s automatic download system
          works. My first guess would be it looks for pytorch_model.bin.index.json
          to tell it what to grab.</p>

          <p>I don''t know if it''d work to add a file in that format for GPTQs. I
          guess the layers are still structured and named the same even though their
          weight contents is in an format HF can''t process, so it might be worth
          a go.</p>

          '
        raw: 'I don''t. I''ve not looked into how HF''s automatic download system
          works. My first guess would be it looks for pytorch_model.bin.index.json
          to tell it what to grab.


          I don''t know if it''d work to add a file in that format for GPTQs. I guess
          the layers are still structured and named the same even though their weight
          contents is in an format HF can''t process, so it might be worth a go.'
        updatedAt: '2023-04-23T22:17:50.585Z'
      numEdits: 1
      reactions: []
    id: 6445ae7ef993c804b03bd84e
    type: comment
  author: TheBloke
  content: 'I don''t. I''ve not looked into how HF''s automatic download system works.
    My first guess would be it looks for pytorch_model.bin.index.json to tell it what
    to grab.


    I don''t know if it''d work to add a file in that format for GPTQs. I guess the
    layers are still structured and named the same even though their weight contents
    is in an format HF can''t process, so it might be worth a go.'
  created_at: 2023-04-23 21:17:34+00:00
  edited: true
  hidden: false
  id: 6445ae7ef993c804b03bd84e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7794a0efd248499a106c679594e206c2.svg
      fullname: Huzaifa Arab
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hyperhuzaifa
      type: user
    createdAt: '2023-04-24T13:45:28.000Z'
    data:
      edited: false
      editors:
      - hyperhuzaifa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7794a0efd248499a106c679594e206c2.svg
          fullname: Huzaifa Arab
          isHf: false
          isPro: false
          name: hyperhuzaifa
          type: user
        html: '<p>The script downloads both versions and oobabooga tends to load the
          safetensors version which gives gibberish output.<br>Manually deleting that
          file feels like a waste of time and bandwidth.<br>I''ll keep digging</p>

          '
        raw: 'The script downloads both versions and oobabooga tends to load the safetensors
          version which gives gibberish output.

          Manually deleting that file feels like a waste of time and bandwidth.

          I''ll keep digging'
        updatedAt: '2023-04-24T13:45:28.156Z'
      numEdits: 0
      reactions: []
    id: 644687f8a808163cef00e218
    type: comment
  author: hyperhuzaifa
  content: 'The script downloads both versions and oobabooga tends to load the safetensors
    version which gives gibberish output.

    Manually deleting that file feels like a waste of time and bandwidth.

    I''ll keep digging'
  created_at: 2023-04-24 12:45:28+00:00
  edited: false
  hidden: false
  id: 644687f8a808163cef00e218
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-24T14:10:38.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>One way would just be to implement a manual download method that\
          \ could be called at the start of your process?</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> wget\n<span\
          \ class=\"hljs-keyword\">from</span> pathlib <span class=\"hljs-keyword\"\
          >import</span> Path\n\nrepo_files = [ <span class=\"hljs-string\">\"config.json\"\
          </span>, <span class=\"hljs-string\">\"generation_config.json\"</span>,\
          \ <span class=\"hljs-string\">\"special_tokens_map.json\"</span>, <span\
          \ class=\"hljs-string\">\"tokenizer.model\"</span> ]\nmodel_files = [ <span\
          \ class=\"hljs-string\">\"vicuna-13B-1.1-GPTQ-4bit-128g.no-act-order.pt\"\
          </span> ]\nrepo = <span class=\"hljs-string\">\"TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g\"\
          </span>\nbase_folder = <span class=\"hljs-string\">\"/var/tmp/\"</span>\n\
          \ndest_dir = base_folder + <span class=\"hljs-string\">\"/\"</span> + repo.replace(<span\
          \ class=\"hljs-string\">\"/\"</span>, <span class=\"hljs-string\">\"_\"\
          </span>)\nPath(dest_dir).mkdir(exist_ok=<span class=\"hljs-literal\">True</span>)\n\
          \n<span class=\"hljs-keyword\">for</span> get_file <span class=\"hljs-keyword\"\
          >in</span> repo_files + model_files:\n    filename = <span class=\"hljs-string\"\
          >f\"https://huggingface.co/<span class=\"hljs-subst\">{repo}</span>/resolve/main/<span\
          \ class=\"hljs-subst\">{get_file}</span>\"</span>\n    <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f\"\\nDownloading <span class=\"\
          hljs-subst\">{filename}</span>\"</span>)\n    wget.download(filename, out\
          \ = dest_dir)\n</code></pre>\n<pre><code>tomj@Eddie ~/src $ python hf_model_grab.py\n\
          \nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/config.json\n\
          100% [..................................................................................]\
          \ 583 / 583\nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/generation_config.json\n\
          100% [..................................................................................]\
          \ 137 / 137\nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/special_tokens_map.json\n\
          100% [..................................................................................]\
          \ 411 / 411\nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/tokenizer.model\n\
          100% [............................................................................]\
          \ 499723 / 499723\nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/vicuna-13B-1.1-GPTQ-4bit-128g.no-act-order.pt\n\
          \  9% [......                                                          \
          \    ]  694099968 / 7255476788\n</code></pre>\n"
        raw: "One way would just be to implement a manual download method that could\
          \ be called at the start of your process?\n```python\nimport wget\nfrom\
          \ pathlib import Path\n\nrepo_files = [ \"config.json\", \"generation_config.json\"\
          , \"special_tokens_map.json\", \"tokenizer.model\" ]\nmodel_files = [ \"\
          vicuna-13B-1.1-GPTQ-4bit-128g.no-act-order.pt\" ]\nrepo = \"TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g\"\
          \nbase_folder = \"/var/tmp/\"\n\ndest_dir = base_folder + \"/\" + repo.replace(\"\
          /\", \"_\")\nPath(dest_dir).mkdir(exist_ok=True)\n\nfor get_file in repo_files\
          \ + model_files:\n    filename = f\"https://huggingface.co/{repo}/resolve/main/{get_file}\"\
          \n    print(f\"\\nDownloading {filename}\")\n    wget.download(filename,\
          \ out = dest_dir)\n```\n\n```\ntomj@Eddie ~/src $ python hf_model_grab.py\n\
          \nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/config.json\n\
          100% [..................................................................................]\
          \ 583 / 583\nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/generation_config.json\n\
          100% [..................................................................................]\
          \ 137 / 137\nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/special_tokens_map.json\n\
          100% [..................................................................................]\
          \ 411 / 411\nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/tokenizer.model\n\
          100% [............................................................................]\
          \ 499723 / 499723\nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/vicuna-13B-1.1-GPTQ-4bit-128g.no-act-order.pt\n\
          \  9% [......                                                          \
          \    ]  694099968 / 7255476788\n```"
        updatedAt: '2023-04-24T14:14:44.490Z'
      numEdits: 3
      reactions: []
    id: 64468dde1173e85ac7f31279
    type: comment
  author: TheBloke
  content: "One way would just be to implement a manual download method that could\
    \ be called at the start of your process?\n```python\nimport wget\nfrom pathlib\
    \ import Path\n\nrepo_files = [ \"config.json\", \"generation_config.json\", \"\
    special_tokens_map.json\", \"tokenizer.model\" ]\nmodel_files = [ \"vicuna-13B-1.1-GPTQ-4bit-128g.no-act-order.pt\"\
    \ ]\nrepo = \"TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g\"\nbase_folder = \"/var/tmp/\"\
    \n\ndest_dir = base_folder + \"/\" + repo.replace(\"/\", \"_\")\nPath(dest_dir).mkdir(exist_ok=True)\n\
    \nfor get_file in repo_files + model_files:\n    filename = f\"https://huggingface.co/{repo}/resolve/main/{get_file}\"\
    \n    print(f\"\\nDownloading {filename}\")\n    wget.download(filename, out =\
    \ dest_dir)\n```\n\n```\ntomj@Eddie ~/src $ python hf_model_grab.py\n\nDownloading\
    \ https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/config.json\n\
    100% [..................................................................................]\
    \ 583 / 583\nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/generation_config.json\n\
    100% [..................................................................................]\
    \ 137 / 137\nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/special_tokens_map.json\n\
    100% [..................................................................................]\
    \ 411 / 411\nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/tokenizer.model\n\
    100% [............................................................................]\
    \ 499723 / 499723\nDownloading https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/resolve/main/vicuna-13B-1.1-GPTQ-4bit-128g.no-act-order.pt\n\
    \  9% [......                                                              ] \
    \ 694099968 / 7255476788\n```"
  created_at: 2023-04-24 13:10:38+00:00
  edited: true
  hidden: false
  id: 64468dde1173e85ac7f31279
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T09:36:19.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Coming back to this, I understand better now what you meant by auto
          installation.  To achieve this going forward, I am now doing the following
          in my repos:</p>

          <ol>

          <li>I have renamed the files so that the compatible version is called <code>compat.no-act-order.safetensor</code>
          and the one that requires latest code is called <code>latest.act-order.safetensor</code>.  This
          means if you do an automatic installation, the <code>compat</code> file
          will be loaded in preference (because it''s first when sorted alphabetically).</li>

          </ol>

          <p>You still end up with two files which is a bit of a waste of time and
          bandwidth, but at least the compatible one gets loaded and you don''t get
          gibberish.</p>

          <ol start="2">

          <li><p>On my most recent repo (<a href="https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ">https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ</a>),
          I have gone one step further: I have two branches.  The <code>main</code>
          branch contains only the compat file, and then there''s a <code>latest</code>
          branch with the latest file.  Therefore anyone doing an ooba auto install
          will always get just one file downloaded, the compatible version.  And if
          someone wants the act-order version, they can clone the <code>latest</code>
          branch and likewise get only one file downloaded</p>

          </li>

          <li><p>And then I''m starting to add these instructions to all GPTQ repos
          to aid people in automatic downloading:</p>

          </li>

          </ol>

          <h2 id="how-to-easily-download-and-use-this-model-in-text-generation-webui">How
          to easily download and use this model in text-generation-webui</h2>

          <p>Load text-generation-webui as you normally do.</p>

          <ol>

          <li>Click the <strong>Model tab</strong>.</li>

          <li>Under <strong>Download custom model or LoRA</strong>, enter this repo
          name: <code>TheBloke/stable-vicuna-13B-GPTQ</code>.</li>

          <li>Click <strong>Download</strong>.</li>

          <li>Wait until it says it''s finished downloading.</li>

          <li>As this is a GPTQ model, fill in the <code>GPTQ parameters</code> on
          the right: <code>Bits = 4</code>, <code>Groupsize = 128</code>, <code>model_type
          = Llama</code></li>

          <li>Now click the <strong>Refresh</strong> icon next to <strong>Model</strong>
          in the top left.</li>

          <li>In the <strong>Model drop-down</strong>: choose this model: <code>stable-vicuna-13B-GPTQ</code>.</li>

          <li>Click <strong>Reload the Model</strong> in the top right.</li>

          <li>Once it says it''s loaded, click the <strong>Text Generation tab</strong>
          and enter a prompt!</li>

          </ol>

          '
        raw: 'Coming back to this, I understand better now what you meant by auto
          installation.  To achieve this going forward, I am now doing the following
          in my repos:


          1. I have renamed the files so that the compatible version is called `compat.no-act-order.safetensor`
          and the one that requires latest code is called `latest.act-order.safetensor`.  This
          means if you do an automatic installation, the `compat` file will be loaded
          in preference (because it''s first when sorted alphabetically).


          You still end up with two files which is a bit of a waste of time and bandwidth,
          but at least the compatible one gets loaded and you don''t get gibberish.


          2. On my most recent repo (https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ),
          I have gone one step further: I have two branches.  The `main` branch contains
          only the compat file, and then there''s a `latest` branch with the latest
          file.  Therefore anyone doing an ooba auto install will always get just
          one file downloaded, the compatible version.  And if someone wants the act-order
          version, they can clone the `latest` branch and likewise get only one file
          downloaded


          3. And then I''m starting to add these instructions to all GPTQ repos to
          aid people in automatic downloading:


          ## How to easily download and use this model in text-generation-webui


          Load text-generation-webui as you normally do.


          1. Click the **Model tab**.

          2. Under **Download custom model or LoRA**, enter this repo name: `TheBloke/stable-vicuna-13B-GPTQ`.

          3. Click **Download**.

          4. Wait until it says it''s finished downloading.

          5. As this is a GPTQ model, fill in the `GPTQ parameters` on the right:
          `Bits = 4`, `Groupsize = 128`, `model_type = Llama`

          6. Now click the **Refresh** icon next to **Model** in the top left.

          7. In the **Model drop-down**: choose this model: `stable-vicuna-13B-GPTQ`.

          8. Click **Reload the Model** in the top right.

          9. Once it says it''s loaded, click the **Text Generation tab** and enter
          a prompt!'
        updatedAt: '2023-04-29T09:36:19.406Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Squish42
    id: 644ce513fa94e93b0ebcf4b9
    type: comment
  author: TheBloke
  content: 'Coming back to this, I understand better now what you meant by auto installation.  To
    achieve this going forward, I am now doing the following in my repos:


    1. I have renamed the files so that the compatible version is called `compat.no-act-order.safetensor`
    and the one that requires latest code is called `latest.act-order.safetensor`.  This
    means if you do an automatic installation, the `compat` file will be loaded in
    preference (because it''s first when sorted alphabetically).


    You still end up with two files which is a bit of a waste of time and bandwidth,
    but at least the compatible one gets loaded and you don''t get gibberish.


    2. On my most recent repo (https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ),
    I have gone one step further: I have two branches.  The `main` branch contains
    only the compat file, and then there''s a `latest` branch with the latest file.  Therefore
    anyone doing an ooba auto install will always get just one file downloaded, the
    compatible version.  And if someone wants the act-order version, they can clone
    the `latest` branch and likewise get only one file downloaded


    3. And then I''m starting to add these instructions to all GPTQ repos to aid people
    in automatic downloading:


    ## How to easily download and use this model in text-generation-webui


    Load text-generation-webui as you normally do.


    1. Click the **Model tab**.

    2. Under **Download custom model or LoRA**, enter this repo name: `TheBloke/stable-vicuna-13B-GPTQ`.

    3. Click **Download**.

    4. Wait until it says it''s finished downloading.

    5. As this is a GPTQ model, fill in the `GPTQ parameters` on the right: `Bits
    = 4`, `Groupsize = 128`, `model_type = Llama`

    6. Now click the **Refresh** icon next to **Model** in the top left.

    7. In the **Model drop-down**: choose this model: `stable-vicuna-13B-GPTQ`.

    8. Click **Reload the Model** in the top right.

    9. Once it says it''s loaded, click the **Text Generation tab** and enter a prompt!'
  created_at: 2023-04-29 08:36:19+00:00
  edited: false
  hidden: false
  id: 644ce513fa94e93b0ebcf4b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-04-29T10:25:41.000Z'
    data:
      edited: false
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<p>The way your other repo is set up seemed to work really well. The
          only issue I had was having to rename the folder (repository name) to include
          the group size. So, TheBloke_stable-vicuna-13B-4bit-128g-GPTQ. A custom
          config entry works fine too. It probably only matters on first load.<br>Seems
          like a great model by the way. I am looking forward to checking it out some
          more.</p>

          '
        raw: 'The way your other repo is set up seemed to work really well. The only
          issue I had was having to rename the folder (repository name) to include
          the group size. So, TheBloke_stable-vicuna-13B-4bit-128g-GPTQ. A custom
          config entry works fine too. It probably only matters on first load.

          Seems like a great model by the way. I am looking forward to checking it
          out some more.'
        updatedAt: '2023-04-29T10:25:41.597Z'
      numEdits: 0
      reactions: []
    id: 644cf0a5fa94e93b0ebe2a3f
    type: comment
  author: Squish42
  content: 'The way your other repo is set up seemed to work really well. The only
    issue I had was having to rename the folder (repository name) to include the group
    size. So, TheBloke_stable-vicuna-13B-4bit-128g-GPTQ. A custom config entry works
    fine too. It probably only matters on first load.

    Seems like a great model by the way. I am looking forward to checking it out some
    more.'
  created_at: 2023-04-29 09:25:41+00:00
  edited: false
  hidden: false
  id: 644cf0a5fa94e93b0ebe2a3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T11:06:35.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ah ok, so if you rename the model folder it auto detects the gptq
          params?</p>

          '
        raw: Ah ok, so if you rename the model folder it auto detects the gptq params?
        updatedAt: '2023-04-29T11:06:35.499Z'
      numEdits: 0
      reactions: []
    id: 644cfa3bfa94e93b0ebf50ae
    type: comment
  author: TheBloke
  content: Ah ok, so if you rename the model folder it auto detects the gptq params?
  created_at: 2023-04-29 10:06:35+00:00
  edited: false
  hidden: false
  id: 644cfa3bfa94e93b0ebf50ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-04-29T11:27:18.000Z'
    data:
      edited: false
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<p>Yea, with the bit and groupsize in the repository name it doesn''t
          need any config to be set during load. The wildcards for detecting that
          are configured in models/config.yaml. By default anything in a folder named
          <em>4bit-128g</em> gets loaded like you''d expect. The auto installer should
          just load it on the first go that way, rather than setting config and reloading.</p>

          '
        raw: Yea, with the bit and groupsize in the repository name it doesn't need
          any config to be set during load. The wildcards for detecting that are configured
          in models/config.yaml. By default anything in a folder named *4bit-128g*
          gets loaded like you'd expect. The auto installer should just load it on
          the first go that way, rather than setting config and reloading.
        updatedAt: '2023-04-29T11:27:18.209Z'
      numEdits: 0
      reactions: []
    id: 644cff16fa94e93b0ebfe42f
    type: comment
  author: Squish42
  content: Yea, with the bit and groupsize in the repository name it doesn't need
    any config to be set during load. The wildcards for detecting that are configured
    in models/config.yaml. By default anything in a folder named *4bit-128g* gets
    loaded like you'd expect. The auto installer should just load it on the first
    go that way, rather than setting config and reloading.
  created_at: 2023-04-29 10:27:18+00:00
  edited: false
  hidden: false
  id: 644cff16fa94e93b0ebfe42f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T11:36:07.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK thanks! I used to name the model folders GPTQ-4bit-128g but then
          it seemed a bit longwinded given the files had that detail as well.</p>

          <p>Maybe I''ll go back to putting it in the model as well. Or see if I can
          PR a change to ooba to read it from the model file.</p>

          <p>Although I''m hoping the community will move to using AutoGPTQ soon,
          and that provides a quantize_config.json file with all the params in it.</p>

          '
        raw: 'OK thanks! I used to name the model folders GPTQ-4bit-128g but then
          it seemed a bit longwinded given the files had that detail as well.


          Maybe I''ll go back to putting it in the model as well. Or see if I can
          PR a change to ooba to read it from the model file.


          Although I''m hoping the community will move to using AutoGPTQ soon, and
          that provides a quantize_config.json file with all the params in it.'
        updatedAt: '2023-04-29T11:36:07.453Z'
      numEdits: 0
      reactions: []
    id: 644d012797a3b0904a4d8887
    type: comment
  author: TheBloke
  content: 'OK thanks! I used to name the model folders GPTQ-4bit-128g but then it
    seemed a bit longwinded given the files had that detail as well.


    Maybe I''ll go back to putting it in the model as well. Or see if I can PR a change
    to ooba to read it from the model file.


    Although I''m hoping the community will move to using AutoGPTQ soon, and that
    provides a quantize_config.json file with all the params in it.'
  created_at: 2023-04-29 10:36:07+00:00
  edited: false
  hidden: false
  id: 644d012797a3b0904a4d8887
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-04-29T11:49:53.000Z'
    data:
      edited: true
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<p>Sure, glad to help a little.<br>Single GGML .bin files in the main
          models folder do get checked by filename. But for GPTQ and pytorch that
          need the extra files it checks the folder name only I think. There''s definitely
          some room for improvement there somewhere. Reading from the model filename
          does seem preferable even when it is inside a folder. As far as I could
          tell that doesn''t seem to be the current behavior.</p>

          <p>I think so long as people are still using custom config entries, we should
          make sure they are clicking the "Save settings for this model" button. That
          will save the overrides for that specific model to config-user.yaml and
          it will ignore the defaults. Otherwise they''ll have to set the config in
          the UI each time.</p>

          '
        raw: 'Sure, glad to help a little.

          Single GGML .bin files in the main models folder do get checked by filename.
          But for GPTQ and pytorch that need the extra files it checks the folder
          name only I think. There''s definitely some room for improvement there somewhere.
          Reading from the model filename does seem preferable even when it is inside
          a folder. As far as I could tell that doesn''t seem to be the current behavior.


          I think so long as people are still using custom config entries, we should
          make sure they are clicking the "Save settings for this model" button. That
          will save the overrides for that specific model to config-user.yaml and
          it will ignore the defaults. Otherwise they''ll have to set the config in
          the UI each time.'
        updatedAt: '2023-04-29T13:52:34.141Z'
      numEdits: 6
      reactions: []
    id: 644d0461fa94e93b0ec07cd2
    type: comment
  author: Squish42
  content: 'Sure, glad to help a little.

    Single GGML .bin files in the main models folder do get checked by filename. But
    for GPTQ and pytorch that need the extra files it checks the folder name only
    I think. There''s definitely some room for improvement there somewhere. Reading
    from the model filename does seem preferable even when it is inside a folder.
    As far as I could tell that doesn''t seem to be the current behavior.


    I think so long as people are still using custom config entries, we should make
    sure they are clicking the "Save settings for this model" button. That will save
    the overrides for that specific model to config-user.yaml and it will ignore the
    defaults. Otherwise they''ll have to set the config in the UI each time.'
  created_at: 2023-04-29 10:49:53+00:00
  edited: true
  hidden: false
  id: 644d0461fa94e93b0ec07cd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T21:13:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''ve been through the auto download procedure again and revised
          my README to incorporate the advice to save the model.  So here''s the new
          generic instructions for my GPTQ models.  Model names will be set appropriately
          for each respective repo.</p>

          <h2 id="how-to-download-and-use-a-model-in-text-generation-webui">How to
          download and use a model in text-generation-webui</h2>

          <p>Open the text-generation-webui UI as normal.</p>

          <ol>

          <li>Click the <strong>Model tab</strong>.</li>

          <li>Under <strong>Download custom model or LoRA</strong>, enter the repo
          to download, for example: <code>TheBloke/wizardLM-7B-GPTQ</code>.</li>

          <li>Click <strong>Download</strong>.</li>

          <li>Wait until it says it''s finished downloading.</li>

          <li>Click the <strong>Refresh</strong> icon next to <strong>Model</strong>
          in the top left.</li>

          <li>In the <strong>Model drop-down</strong>: choose the model you just downloaded,
          eg <code>wizardLM-7B-GPTQ</code>.</li>

          <li>If you see an error in the bottom right, ignore it - it''s temporary.</li>

          <li>If this is a GPTQ model, fill in the <code>GPTQ parameters</code> on
          the right: <code>Bits = 4</code>, <code>Groupsize = 128</code>, <code>model_type
          = Llama</code></li>

          <li>Click <strong>Save settings for this model</strong> in the top right.</li>

          <li>Click <strong>Reload the Model</strong> in the top right.</li>

          <li>Once it says it''s loaded, click the <strong>Text Generation tab</strong>
          and enter a prompt!</li>

          </ol>

          '
        raw: 'I''ve been through the auto download procedure again and revised my
          README to incorporate the advice to save the model.  So here''s the new
          generic instructions for my GPTQ models.  Model names will be set appropriately
          for each respective repo.


          ## How to download and use a model in text-generation-webui


          Open the text-generation-webui UI as normal.


          1. Click the **Model tab**.

          2. Under **Download custom model or LoRA**, enter the repo to download,
          for example: `TheBloke/wizardLM-7B-GPTQ`.

          3. Click **Download**.

          4. Wait until it says it''s finished downloading.

          5. Click the **Refresh** icon next to **Model** in the top left.

          6. In the **Model drop-down**: choose the model you just downloaded, eg
          `wizardLM-7B-GPTQ`.

          7. If you see an error in the bottom right, ignore it - it''s temporary.

          8. If this is a GPTQ model, fill in the `GPTQ parameters` on the right:
          `Bits = 4`, `Groupsize = 128`, `model_type = Llama`

          9. Click **Save settings for this model** in the top right.

          10. Click **Reload the Model** in the top right.

          11. Once it says it''s loaded, click the **Text Generation tab** and enter
          a prompt!'
        updatedAt: '2023-04-29T21:13:52.174Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Squish42
    id: 644d8890fa94e93b0ecd7f42
    type: comment
  author: TheBloke
  content: 'I''ve been through the auto download procedure again and revised my README
    to incorporate the advice to save the model.  So here''s the new generic instructions
    for my GPTQ models.  Model names will be set appropriately for each respective
    repo.


    ## How to download and use a model in text-generation-webui


    Open the text-generation-webui UI as normal.


    1. Click the **Model tab**.

    2. Under **Download custom model or LoRA**, enter the repo to download, for example:
    `TheBloke/wizardLM-7B-GPTQ`.

    3. Click **Download**.

    4. Wait until it says it''s finished downloading.

    5. Click the **Refresh** icon next to **Model** in the top left.

    6. In the **Model drop-down**: choose the model you just downloaded, eg `wizardLM-7B-GPTQ`.

    7. If you see an error in the bottom right, ignore it - it''s temporary.

    8. If this is a GPTQ model, fill in the `GPTQ parameters` on the right: `Bits
    = 4`, `Groupsize = 128`, `model_type = Llama`

    9. Click **Save settings for this model** in the top right.

    10. Click **Reload the Model** in the top right.

    11. Once it says it''s loaded, click the **Text Generation tab** and enter a prompt!'
  created_at: 2023-04-29 20:13:52+00:00
  edited: false
  hidden: false
  id: 644d8890fa94e93b0ecd7f42
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/Vicuna-13B-1.1-GPTQ
repo_type: model
status: open
target_branch: null
title: How do you force download-model.py to download act-order models by default?
