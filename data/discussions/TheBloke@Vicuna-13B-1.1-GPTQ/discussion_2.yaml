!!python/object:huggingface_hub.community.DiscussionWithDetails
author: horstao
conflicting_files: null
created_at: 2023-04-13 20:33:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643458b17b824748010bd3db/2CQCc3lUYJS705mTcb5cx.jpeg?w=200&h=200&f=face
      fullname: Horst Erdmann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: horstao
      type: user
    createdAt: '2023-04-13T21:33:28.000Z'
    data:
      edited: false
      editors:
      - horstao
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643458b17b824748010bd3db/2CQCc3lUYJS705mTcb5cx.jpeg?w=200&h=200&f=face
          fullname: Horst Erdmann
          isHf: false
          isPro: false
          name: horstao
          type: user
        html: '<p>When trying to load the model directly from HF, I get this error
          :</p>

          <p><code>OSError: TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g does not appear
          to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack</code></p>

          <p>I already have the Vicuna-13b 1.1 model downloaded, so I tried running
          the llama.py command:</p>

          <pre><code>python llama.py \

          &lt;path-to-vicuna-13b&gt; c4 --wbits 4 \

          --true-sequential --act-order --groupsize 128 \

          --save_safetensors vicuna-13B-1.1-GPTQ-4bit-128g.safetensors

          </code></pre>

          <p>And I get this error: </p>

          <pre><code>Token indices sequence length is longer than the specified maximum
          sequence length for this model (3908 &gt; 2048). Running this sequence through
          the model will result in indexing errors

          Starting ...


          KeyError: ''position_ids''

          </code></pre>

          <p>Could you give a hint of what is the problem ? </p>

          '
        raw: "When trying to load the model directly from HF, I get this error :\r\
          \n\r\n```OSError: TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g does not appear\
          \ to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack```\r\
          \n\r\nI already have the Vicuna-13b 1.1 model downloaded, so I tried running\
          \ the llama.py command:\r\n\r\n```\r\npython llama.py \\\r\n<path-to-vicuna-13b>\
          \ c4 --wbits 4 \\\r\n--true-sequential --act-order --groupsize 128 \\\r\n\
          --save_safetensors vicuna-13B-1.1-GPTQ-4bit-128g.safetensors\r\n```\r\n\r\
          \nAnd I get this error: \r\n\r\n```\r\nToken indices sequence length is\
          \ longer than the specified maximum sequence length for this model (3908\
          \ > 2048). Running this sequence through the model will result in indexing\
          \ errors\r\nStarting ...\r\n\r\nKeyError: 'position_ids'\r\n\r\n```\r\n\r\
          \nCould you give a hint of what is the problem ? \r\n"
        updatedAt: '2023-04-13T21:33:28.763Z'
      numEdits: 0
      reactions: []
    id: 64387528b2ea24b52eb920dd
    type: comment
  author: horstao
  content: "When trying to load the model directly from HF, I get this error :\r\n\
    \r\n```OSError: TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g does not appear to have\
    \ a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack```\r\
    \n\r\nI already have the Vicuna-13b 1.1 model downloaded, so I tried running the\
    \ llama.py command:\r\n\r\n```\r\npython llama.py \\\r\n<path-to-vicuna-13b> c4\
    \ --wbits 4 \\\r\n--true-sequential --act-order --groupsize 128 \\\r\n--save_safetensors\
    \ vicuna-13B-1.1-GPTQ-4bit-128g.safetensors\r\n```\r\n\r\nAnd I get this error:\
    \ \r\n\r\n```\r\nToken indices sequence length is longer than the specified maximum\
    \ sequence length for this model (3908 > 2048). Running this sequence through\
    \ the model will result in indexing errors\r\nStarting ...\r\n\r\nKeyError: 'position_ids'\r\
    \n\r\n```\r\n\r\nCould you give a hint of what is the problem ? \r\n"
  created_at: 2023-04-13 20:33:28+00:00
  edited: false
  hidden: false
  id: 64387528b2ea24b52eb920dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf0eaf89fa1ea6ad1a350d41a82f03af.svg
      fullname: Shin Asura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kusoge
      type: user
    createdAt: '2023-04-14T21:28:46.000Z'
    data:
      edited: false
      editors:
      - kusoge
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf0eaf89fa1ea6ad1a350d41a82f03af.svg
          fullname: Shin Asura
          isHf: false
          isPro: false
          name: kusoge
          type: user
        html: '<p>I managed to load the model, but I''m only getting gibberish responses.</p>

          '
        raw: I managed to load the model, but I'm only getting gibberish responses.
        updatedAt: '2023-04-14T21:28:46.780Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - erjo1776
        - Monteclaudio
    id: 6439c58e11e9481b75e4fa44
    type: comment
  author: kusoge
  content: I managed to load the model, but I'm only getting gibberish responses.
  created_at: 2023-04-14 20:28:46+00:00
  edited: false
  hidden: false
  id: 6439c58e11e9481b75e4fa44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-14T21:30:55.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Could you give a hint of what is the problem ?<br>I''ve never tried loading
          it direct from HF.  Are you loading in text-generation-webui, and have you
          set up the --wbits and --groupsize settings?</p>

          </blockquote>

          <p>As for the GPTQ error, not sure, I''ve never seen that KeyError. Something
          is set up wrong, but I can''t say what.</p>

          '
        raw: '> Could you give a hint of what is the problem ?

          I''ve never tried loading it direct from HF.  Are you loading in text-generation-webui,
          and have you set up the --wbits and --groupsize settings?


          As for the GPTQ error, not sure, I''ve never seen that KeyError. Something
          is set up wrong, but I can''t say what.'
        updatedAt: '2023-04-14T21:30:55.126Z'
      numEdits: 0
      reactions: []
    id: 6439c60f3da4490578d8ae89
    type: comment
  author: TheBloke
  content: '> Could you give a hint of what is the problem ?

    I''ve never tried loading it direct from HF.  Are you loading in text-generation-webui,
    and have you set up the --wbits and --groupsize settings?


    As for the GPTQ error, not sure, I''ve never seen that KeyError. Something is
    set up wrong, but I can''t say what.'
  created_at: 2023-04-14 20:30:55+00:00
  edited: false
  hidden: false
  id: 6439c60f3da4490578d8ae89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-14T21:31:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>I managed to load the model, but I''m only getting gibberish responses.</p>

          </blockquote>

          <p>Please see the README. This means you''re using the <code>safetensors</code>
          file without the latest GPTQ-for-LLaMa code. Either update GPTQ-for-LLaMa,
          or else use the <code>pt</code> file instead.</p>

          '
        raw: '> I managed to load the model, but I''m only getting gibberish responses.


          Please see the README. This means you''re using the `safetensors` file without
          the latest GPTQ-for-LLaMa code. Either update GPTQ-for-LLaMa, or else use
          the `pt` file instead.'
        updatedAt: '2023-04-14T21:31:25.370Z'
      numEdits: 0
      reactions: []
    id: 6439c62d5813f0fdd8552d0b
    type: comment
  author: TheBloke
  content: '> I managed to load the model, but I''m only getting gibberish responses.


    Please see the README. This means you''re using the `safetensors` file without
    the latest GPTQ-for-LLaMa code. Either update GPTQ-for-LLaMa, or else use the
    `pt` file instead.'
  created_at: 2023-04-14 20:31:25+00:00
  edited: false
  hidden: false
  id: 6439c62d5813f0fdd8552d0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad8daf7101b78ae4226f3ade0d64bbe6.svg
      fullname: Riggity Wrckd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RiggityWrckd
      type: user
    createdAt: '2023-04-15T08:10:24.000Z'
    data:
      edited: false
      editors:
      - RiggityWrckd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad8daf7101b78ae4226f3ade0d64bbe6.svg
          fullname: Riggity Wrckd
          isHf: false
          isPro: false
          name: RiggityWrckd
          type: user
        html: '<p>Also if you don''t know you need to move your GPTQ-for-LLaMa off
          of the cuda branch and onto the triton (main) branch.  Probably wanna reclone
          the repo for GPTQ in the repositories folder if you''re using oogabooga
          and reinstall all those requirements.  That''s how I stopped the jibberish.</p>

          '
        raw: Also if you don't know you need to move your GPTQ-for-LLaMa off of the
          cuda branch and onto the triton (main) branch.  Probably wanna reclone the
          repo for GPTQ in the repositories folder if you're using oogabooga and reinstall
          all those requirements.  That's how I stopped the jibberish.
        updatedAt: '2023-04-15T08:10:24.667Z'
      numEdits: 0
      reactions: []
    id: 643a5bf021aad82a53139840
    type: comment
  author: RiggityWrckd
  content: Also if you don't know you need to move your GPTQ-for-LLaMa off of the
    cuda branch and onto the triton (main) branch.  Probably wanna reclone the repo
    for GPTQ in the repositories folder if you're using oogabooga and reinstall all
    those requirements.  That's how I stopped the jibberish.
  created_at: 2023-04-15 07:10:24+00:00
  edited: false
  hidden: false
  id: 643a5bf021aad82a53139840
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-15T08:46:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Also if you don''t know you need to move your GPTQ-for-LLaMa off of the
          cuda branch and onto the triton (main) branch.  Probably wanna reclone the
          repo for GPTQ in the repositories folder if you''re using oogabooga and
          reinstall all those requirements.  That''s how I stopped the jibberish.</p>

          </blockquote>

          <p>You shouldn''t have to. The CUDA branch will work, as long as you rebuild
          the CUDA kernel with <code>python setup_cuda.py install --force</code>.  But
          yes the Triton branch is preferable.</p>

          '
        raw: '> Also if you don''t know you need to move your GPTQ-for-LLaMa off of
          the cuda branch and onto the triton (main) branch.  Probably wanna reclone
          the repo for GPTQ in the repositories folder if you''re using oogabooga
          and reinstall all those requirements.  That''s how I stopped the jibberish.


          You shouldn''t have to. The CUDA branch will work, as long as you rebuild
          the CUDA kernel with `python setup_cuda.py install --force`.  But yes the
          Triton branch is preferable.'
        updatedAt: '2023-04-15T08:46:27.558Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RiggityWrckd
    id: 643a64631f162723356df876
    type: comment
  author: TheBloke
  content: '> Also if you don''t know you need to move your GPTQ-for-LLaMa off of
    the cuda branch and onto the triton (main) branch.  Probably wanna reclone the
    repo for GPTQ in the repositories folder if you''re using oogabooga and reinstall
    all those requirements.  That''s how I stopped the jibberish.


    You shouldn''t have to. The CUDA branch will work, as long as you rebuild the
    CUDA kernel with `python setup_cuda.py install --force`.  But yes the Triton branch
    is preferable.'
  created_at: 2023-04-15 07:46:27+00:00
  edited: false
  hidden: false
  id: 643a64631f162723356df876
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a6f2c88fdef10d33790588342ae0a03.svg
      fullname: Artur Cheremkhin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Morivy
      type: user
    createdAt: '2023-04-16T17:46:30.000Z'
    data:
      edited: true
      editors:
      - Morivy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a6f2c88fdef10d33790588342ae0a03.svg
          fullname: Artur Cheremkhin
          isHf: false
          isPro: false
          name: Morivy
          type: user
        html: '<p>I have your model running in the latest text-generation-webui version
          with the original GPTQ-for-LLaMA (cuda) that you specified for Windows in
          the readme, and the latest dependencies. However, with this GPTQ, text-generation-webui
          is frustratingly slower on my system (GTX 1080 Ti) for some reason, this
          affects both models below.<br>With oobabooga''s GPTQ-for-LLaMA fork installed
          and using similar (or not?) "jeremy-costello/vicuna-13b-v1.1-4bit-128g"
          model, token generation speed on my system is several times faster, but
          in case I try to run TheBloke''s solution with this fork installed, I get
          the same gibberish output (token generation speed remains the same though)
          that many people already faced.</p>

          <p>It turns out that at a minimum we need to inform text-generation-webui
          developers about this problem and get them to fix the oobabooga\GPTQ-for-LLaMA
          fork (the one that definitely works with CUDA on Windows when webui is installed
          via "one-click" method), and not only.</p>

          '
        raw: 'I have your model running in the latest text-generation-webui version
          with the original GPTQ-for-LLaMA (cuda) that you specified for Windows in
          the readme, and the latest dependencies. However, with this GPTQ, text-generation-webui
          is frustratingly slower on my system (GTX 1080 Ti) for some reason, this
          affects both models below.

          With oobabooga''s GPTQ-for-LLaMA fork installed and using similar (or not?)
          "jeremy-costello/vicuna-13b-v1.1-4bit-128g" model, token generation speed
          on my system is several times faster, but in case I try to run TheBloke''s
          solution with this fork installed, I get the same gibberish output (token
          generation speed remains the same though) that many people already faced.


          It turns out that at a minimum we need to inform text-generation-webui developers
          about this problem and get them to fix the oobabooga\GPTQ-for-LLaMA fork
          (the one that definitely works with CUDA on Windows when webui is installed
          via "one-click" method), and not only.'
        updatedAt: '2023-04-16T17:49:17.397Z'
      numEdits: 1
      reactions: []
    id: 643c3476e3a7bbe2cf3ceec2
    type: comment
  author: Morivy
  content: 'I have your model running in the latest text-generation-webui version
    with the original GPTQ-for-LLaMA (cuda) that you specified for Windows in the
    readme, and the latest dependencies. However, with this GPTQ, text-generation-webui
    is frustratingly slower on my system (GTX 1080 Ti) for some reason, this affects
    both models below.

    With oobabooga''s GPTQ-for-LLaMA fork installed and using similar (or not?) "jeremy-costello/vicuna-13b-v1.1-4bit-128g"
    model, token generation speed on my system is several times faster, but in case
    I try to run TheBloke''s solution with this fork installed, I get the same gibberish
    output (token generation speed remains the same though) that many people already
    faced.


    It turns out that at a minimum we need to inform text-generation-webui developers
    about this problem and get them to fix the oobabooga\GPTQ-for-LLaMA fork (the
    one that definitely works with CUDA on Windows when webui is installed via "one-click"
    method), and not only.'
  created_at: 2023-04-16 16:46:30+00:00
  edited: true
  hidden: false
  id: 643c3476e3a7bbe2cf3ceec2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T18:01:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Hmm that''s odd.  When did you download the model files from my
          repo here?</p>

          <p>There was an issue with my Vicuna-13B-1.1-HF repo, caused by a bug in
          the Transformers code for converting from the original Llama 13B to HF format.  My
          HF repo was 50% too big as a result. I fixed that about 20 hours ago.  I
          did not think it would affect my GPTQ conversions, but just in case I also
          re-did the GPTQs.</p>

          <p>So when did you download the files?  If it was more than 20 hours ago,
          could you try downloading them again and let me know if you see the same
          slowdown vs Jeremy''s repo?</p>

          <p>I looked at his repo and so far as I can see, he used exactly the same
          parameters as I did.</p>

          '
        raw: 'Hmm that''s odd.  When did you download the model files from my repo
          here?


          There was an issue with my Vicuna-13B-1.1-HF repo, caused by a bug in the
          Transformers code for converting from the original Llama 13B to HF format.  My
          HF repo was 50% too big as a result. I fixed that about 20 hours ago.  I
          did not think it would affect my GPTQ conversions, but just in case I also
          re-did the GPTQs.


          So when did you download the files?  If it was more than 20 hours ago, could
          you try downloading them again and let me know if you see the same slowdown
          vs Jeremy''s repo?


          I looked at his repo and so far as I can see, he used exactly the same parameters
          as I did.'
        updatedAt: '2023-04-16T18:01:15.723Z'
      numEdits: 0
      reactions: []
    id: 643c37ebe3a7bbe2cf3d0460
    type: comment
  author: TheBloke
  content: 'Hmm that''s odd.  When did you download the model files from my repo here?


    There was an issue with my Vicuna-13B-1.1-HF repo, caused by a bug in the Transformers
    code for converting from the original Llama 13B to HF format.  My HF repo was
    50% too big as a result. I fixed that about 20 hours ago.  I did not think it
    would affect my GPTQ conversions, but just in case I also re-did the GPTQs.


    So when did you download the files?  If it was more than 20 hours ago, could you
    try downloading them again and let me know if you see the same slowdown vs Jeremy''s
    repo?


    I looked at his repo and so far as I can see, he used exactly the same parameters
    as I did.'
  created_at: 2023-04-16 17:01:15+00:00
  edited: false
  hidden: false
  id: 643c37ebe3a7bbe2cf3d0460
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a6f2c88fdef10d33790588342ae0a03.svg
      fullname: Artur Cheremkhin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Morivy
      type: user
    createdAt: '2023-04-16T18:34:13.000Z'
    data:
      edited: true
      editors:
      - Morivy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a6f2c88fdef10d33790588342ae0a03.svg
          fullname: Artur Cheremkhin
          isHf: false
          isPro: false
          name: Morivy
          type: user
        html: '<blockquote>

          <p>Hmm that''s odd.  When did you download the model files from my repo
          here?</p>

          <p>There was an issue with my Vicuna-13B-1.1-HF repo, caused by a bug in
          the Transformers code for converting from the original Llama 13B to HF format.  My
          HF repo was 50% too big as a result. I fixed that about 20 hours ago.  I
          did not think it would affect my GPTQ conversions, but just in case I also
          re-did the GPTQs.</p>

          <p>So when did you download the files?  If it was more than 20 hours ago,
          could you try downloading them again and let me know if you see the same
          slowdown vs Jeremy''s repo?</p>

          <p>I looked at his repo and so far as I can see, he used exactly the same
          parameters as I did.</p>

          </blockquote>

          <p>I cloned your model repository 4 hours ago, I already have the latest
          version installed. I checked all possible options, reinstalled all text-generation-webui
          dependencies and GPTQ-for-LLaMA, and with default qwopqwop200/GPTQ-for-LLaMA
          (cuda branch) your model (and also the version from jeremy-costello) seems
          to work fine, though slower than with webui adapted version of GPTQ. This
          version of GPTQ is certainly not optimised, but I don''t know what I can
          do about it.</p>

          '
        raw: "> Hmm that's odd.  When did you download the model files from my repo\
          \ here?\n> \n> There was an issue with my Vicuna-13B-1.1-HF repo, caused\
          \ by a bug in the Transformers code for converting from the original Llama\
          \ 13B to HF format.  My HF repo was 50% too big as a result. I fixed that\
          \ about 20 hours ago.  I did not think it would affect my GPTQ conversions,\
          \ but just in case I also re-did the GPTQs.\n> \n> So when did you download\
          \ the files?  If it was more than 20 hours ago, could you try downloading\
          \ them again and let me know if you see the same slowdown vs Jeremy's repo?\n\
          > \n> I looked at his repo and so far as I can see, he used exactly the\
          \ same parameters as I did.\n\nI cloned your model repository 4 hours ago,\
          \ I already have the latest version installed. I checked all possible options,\
          \ reinstalled all text-generation-webui dependencies and GPTQ-for-LLaMA,\
          \ and with default qwopqwop200/GPTQ-for-LLaMA (cuda branch) your model (and\
          \ also the version from jeremy-costello) seems to work fine, though slower\
          \ than with webui adapted version of GPTQ. This version of GPTQ is certainly\
          \ not optimised, but I don't know what I can do about it."
        updatedAt: '2023-04-16T18:41:01.678Z'
      numEdits: 1
      reactions: []
    id: 643c3fa525c7610a1cd6d1e1
    type: comment
  author: Morivy
  content: "> Hmm that's odd.  When did you download the model files from my repo\
    \ here?\n> \n> There was an issue with my Vicuna-13B-1.1-HF repo, caused by a\
    \ bug in the Transformers code for converting from the original Llama 13B to HF\
    \ format.  My HF repo was 50% too big as a result. I fixed that about 20 hours\
    \ ago.  I did not think it would affect my GPTQ conversions, but just in case\
    \ I also re-did the GPTQs.\n> \n> So when did you download the files?  If it was\
    \ more than 20 hours ago, could you try downloading them again and let me know\
    \ if you see the same slowdown vs Jeremy's repo?\n> \n> I looked at his repo and\
    \ so far as I can see, he used exactly the same parameters as I did.\n\nI cloned\
    \ your model repository 4 hours ago, I already have the latest version installed.\
    \ I checked all possible options, reinstalled all text-generation-webui dependencies\
    \ and GPTQ-for-LLaMA, and with default qwopqwop200/GPTQ-for-LLaMA (cuda branch)\
    \ your model (and also the version from jeremy-costello) seems to work fine, though\
    \ slower than with webui adapted version of GPTQ. This version of GPTQ is certainly\
    \ not optimised, but I don't know what I can do about it."
  created_at: 2023-04-16 17:34:13+00:00
  edited: true
  hidden: false
  id: 643c3fa525c7610a1cd6d1e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T18:36:44.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>What I can''t understand is why Jeremy''s model would be noticeably
          faster than mine. So far as I can tell they''ve been created with identical
          parameters, and using the same GPTQ-for-LLaMa commit.</p>

          <p>The only difference I can notice is that his is a <code>pt</code> file
          and mine is <code>safetensors</code>. I thought safetensors was meant to
          be as fast or even faster, but I don''t know for sure. I do have the <code>no-act-order.pt</code>
          file - did you try that as well?</p>

          '
        raw: 'What I can''t understand is why Jeremy''s model would be noticeably
          faster than mine. So far as I can tell they''ve been created with identical
          parameters, and using the same GPTQ-for-LLaMa commit.


          The only difference I can notice is that his is a `pt` file and mine is
          `safetensors`. I thought safetensors was meant to be as fast or even faster,
          but I don''t know for sure. I do have the `no-act-order.pt` file - did you
          try that as well?'
        updatedAt: '2023-04-16T18:36:44.012Z'
      numEdits: 0
      reactions: []
    id: 643c403c2168686700408005
    type: comment
  author: TheBloke
  content: 'What I can''t understand is why Jeremy''s model would be noticeably faster
    than mine. So far as I can tell they''ve been created with identical parameters,
    and using the same GPTQ-for-LLaMa commit.


    The only difference I can notice is that his is a `pt` file and mine is `safetensors`.
    I thought safetensors was meant to be as fast or even faster, but I don''t know
    for sure. I do have the `no-act-order.pt` file - did you try that as well?'
  created_at: 2023-04-16 17:36:44+00:00
  edited: false
  hidden: false
  id: 643c403c2168686700408005
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a6f2c88fdef10d33790588342ae0a03.svg
      fullname: Artur Cheremkhin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Morivy
      type: user
    createdAt: '2023-04-16T19:05:19.000Z'
    data:
      edited: false
      editors:
      - Morivy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a6f2c88fdef10d33790588342ae0a03.svg
          fullname: Artur Cheremkhin
          isHf: false
          isPro: false
          name: Morivy
          type: user
        html: '<blockquote>

          <p>What I can''t understand is why Jeremy''s model would be noticeably faster
          than mine. So far as I can tell they''ve been created with identical parameters,
          and using the same GPTQ-for-LLaMa commit.</p>

          </blockquote>

          <p>If you compare the performance of both models on one GPTQ and their performance
          on the other, no, not faster.</p>

          <blockquote>

          <p>The only difference I can notice is that his is a <code>pt</code> file
          and mine is <code>safetensors</code>. I thought safetensors was meant to
          be as fast or even faster, but I don''t know for sure.</p>

          </blockquote>

          <p>There are already so many vicuna-13B-v1.1-GPTQ-4bit-128g models on huggingface
          that it''s driving me nuts.<br>But even so - no, using the same dependencies,
          the same webui version and the same GPTQ version, the generation speed is
          almost no different from one model to another.</p>

          <blockquote>

          <p>I do have the <code>no-act-order.pt</code> file - did you try that as
          well?</p>

          </blockquote>

          <p>No, but I will try it now.</p>

          '
        raw: '> What I can''t understand is why Jeremy''s model would be noticeably
          faster than mine. So far as I can tell they''ve been created with identical
          parameters, and using the same GPTQ-for-LLaMa commit.


          If you compare the performance of both models on one GPTQ and their performance
          on the other, no, not faster.

          > The only difference I can notice is that his is a `pt` file and mine is
          `safetensors`. I thought safetensors was meant to be as fast or even faster,
          but I don''t know for sure.


          There are already so many vicuna-13B-v1.1-GPTQ-4bit-128g models on huggingface
          that it''s driving me nuts.

          But even so - no, using the same dependencies, the same webui version and
          the same GPTQ version, the generation speed is almost no different from
          one model to another.


          > I do have the `no-act-order.pt` file - did you try that as well?


          No, but I will try it now.'
        updatedAt: '2023-04-16T19:05:19.837Z'
      numEdits: 0
      reactions: []
    id: 643c46ef49e0fa66dc93e10e
    type: comment
  author: Morivy
  content: '> What I can''t understand is why Jeremy''s model would be noticeably
    faster than mine. So far as I can tell they''ve been created with identical parameters,
    and using the same GPTQ-for-LLaMa commit.


    If you compare the performance of both models on one GPTQ and their performance
    on the other, no, not faster.

    > The only difference I can notice is that his is a `pt` file and mine is `safetensors`.
    I thought safetensors was meant to be as fast or even faster, but I don''t know
    for sure.


    There are already so many vicuna-13B-v1.1-GPTQ-4bit-128g models on huggingface
    that it''s driving me nuts.

    But even so - no, using the same dependencies, the same webui version and the
    same GPTQ version, the generation speed is almost no different from one model
    to another.


    > I do have the `no-act-order.pt` file - did you try that as well?


    No, but I will try it now.'
  created_at: 2023-04-16 18:05:19+00:00
  edited: false
  hidden: false
  id: 643c46ef49e0fa66dc93e10e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T19:09:39.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh! Well if you''re getting the same performance on my model as
          you are on Jeremy''s, then there''s no mystery.</p>

          <p>If your problem is that you wish to use ooba''s fork and you cannot do
          that with the safetensors file, then yes the solution is to use the no-act-order.pt
          which will work fine for you. </p>

          <p>As to why you get poor performance when using the later GPTQ-for-LLaMa
          code  - did you run <code>python setup_cuda.py install</code>? If you installed
          the later GPTQ-for-LLaMa but didn''t compile the CUDA kernel, that would
          certainly explain a significant slowdown.</p>

          <p>But if in doubt, just use the no-act-order.pt file which will work with
          any version of GPTQ-for-LLaMa.</p>

          '
        raw: "Oh! Well if you're getting the same performance on my model as you are\
          \ on Jeremy's, then there's no mystery.\n\nIf your problem is that you wish\
          \ to use ooba's fork and you cannot do that with the safetensors file, then\
          \ yes the solution is to use the no-act-order.pt which will work fine for\
          \ you. \n\nAs to why you get poor performance when using the later GPTQ-for-LLaMa\
          \ code  - did you run `python setup_cuda.py install`? If you installed the\
          \ later GPTQ-for-LLaMa but didn't compile the CUDA kernel, that would certainly\
          \ explain a significant slowdown.\n\nBut if in doubt, just use the no-act-order.pt\
          \ file which will work with any version of GPTQ-for-LLaMa."
        updatedAt: '2023-04-16T19:10:07.524Z'
      numEdits: 1
      reactions: []
    id: 643c47f3216868670040adf0
    type: comment
  author: TheBloke
  content: "Oh! Well if you're getting the same performance on my model as you are\
    \ on Jeremy's, then there's no mystery.\n\nIf your problem is that you wish to\
    \ use ooba's fork and you cannot do that with the safetensors file, then yes the\
    \ solution is to use the no-act-order.pt which will work fine for you. \n\nAs\
    \ to why you get poor performance when using the later GPTQ-for-LLaMa code  -\
    \ did you run `python setup_cuda.py install`? If you installed the later GPTQ-for-LLaMa\
    \ but didn't compile the CUDA kernel, that would certainly explain a significant\
    \ slowdown.\n\nBut if in doubt, just use the no-act-order.pt file which will work\
    \ with any version of GPTQ-for-LLaMa."
  created_at: 2023-04-16 18:09:39+00:00
  edited: true
  hidden: false
  id: 643c47f3216868670040adf0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a6f2c88fdef10d33790588342ae0a03.svg
      fullname: Artur Cheremkhin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Morivy
      type: user
    createdAt: '2023-04-16T20:09:33.000Z'
    data:
      edited: true
      editors:
      - Morivy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a6f2c88fdef10d33790588342ae0a03.svg
          fullname: Artur Cheremkhin
          isHf: false
          isPro: false
          name: Morivy
          type: user
        html: '<blockquote>

          <p>As to why you get poor performance when using the later GPTQ-for-LLaMa
          code  - did you run <code>python setup_cuda.py install</code>? If you installed
          the later GPTQ-for-LLaMa but didn''t compile the CUDA kernel, that would
          certainly explain a significant slowdown.</p>

          </blockquote>

          <p>I uninstalled everything completely and ran the commands "git clone <a
          rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa.git">https://github.com/qwopqwop200/GPTQ-for-LLaMa.git</a>
          -b cuda" and "python setup_cuda.py install --force" via install.bat (setup_cuda.py
          was in the GPTQ-for-LLaMa folder). This .bat can also install from wheel
          "<a rel="nofollow" href="https://github.com/jllllll/GPTQ-for-LLaMa-Wheels/raw/main/quant_cuda-0.0.0-cp310-cp310-win_amd64.whl&quot;">https://github.com/jllllll/GPTQ-for-LLaMa-Wheels/raw/main/quant_cuda-0.0.0-cp310-cp310-win_amd64.whl"</a>
          if "CUDA kernal compilation failed" due to not having Visual C++ Build Tools
          installed, but I had it anyway.<br>The process completed successfully, with
          no errors. But generating tokens with this GPTQ when running no matter what
          model, for reasons unknown to me, was slower (100% GPU load tho) than if
          I ran "git clone <a rel="nofollow" href="https://github.com/oobabooga/GPTQ-for-LLaMa.git">https://github.com/oobabooga/GPTQ-for-LLaMa.git</a>
          -b cuda" and "python setup_cuda.py install --force".</p>

          <blockquote>

          <p>But if in doubt, just use the no-act-order.pt file which will work with
          any version of GPTQ-for-LLaMa.</p>

          </blockquote>

          <p>I checked vicuna-13B-1.1-GPTQ-4bit-128g.no-act-order.pt on oobabooga-fork.
          Everything is fine, no problems. As soon as I change the model to vicuna-13B-1.1-GPTQ-4bit-128g.safetensors,
          I immediately get gibberish on the output.<br>The only thing that bothers
          me a bit is the fact that no-act-order.pt version in theory (from your own
          words) may produce somewhat worse results.</p>

          <hr>

          <p>Here''s some tests of these models.<br>t-g-webui (ooba-GPTQ (cuda) repository),
          default settings and prompt. 100% GPU load everywhere.</p>

          <p>"Hello, say something about yourself."</p>

          <p>Your versions.</p>

          <p>no-act-order.pt:</p>

          <p>Output generated in 7.64 seconds (8.64 tokens/s, 66 tokens, context 41,
          seed 1229294013)</p>

          <p>.safetensors:</p>

          <p>(gibberish) output generated in 21.44 seconds (9.28 tokens/s, 199 tokens,
          context 41, seed 263917108)</p>

          <p>jeremy-costello version.</p>

          <p>vicuna-13b-v1.1-4bit-128g (.pt):</p>

          <p>(gibberish... wait, what. Did I broke something again...) output generated
          in 20.82 seconds (9.56 tokens/s, 199 tokens, context 42, seed 1443451655)</p>

          <hr>

          <p>t-g-webui (original GPTQ (cuda) repository), default settings and prompt.
          100% GPU load everywhere.</p>

          <p>"Hello, say something about yourself."</p>

          <p>Your versions.</p>

          <p>no-act-order.pt:</p>

          <p>Output generated in 15.99 seconds (2.13 tokens/s, 34 tokens, context
          41, seed 198665102)</p>

          <p>.safetensors:</p>

          <p>Output generated in 21.75 seconds (1.98 tokens/s, 43 tokens, context
          41, seed 863730907)</p>

          <p>jeremy-costello version.</p>

          <p>vicuna-13b-v1.1-4bit-128g (.pt):</p>

          <p>Output generated in 58.74 seconds (2.15 tokens/s, 126 tokens, context
          41, seed 47935661)</p>

          '
        raw: "> As to why you get poor performance when using the later GPTQ-for-LLaMa\
          \ code  - did you run `python setup_cuda.py install`? If you installed the\
          \ later GPTQ-for-LLaMa but didn't compile the CUDA kernel, that would certainly\
          \ explain a significant slowdown.\n\nI uninstalled everything completely\
          \ and ran the commands \"git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa.git\
          \ -b cuda\" and \"python setup_cuda.py install --force\" via install.bat\
          \ (setup_cuda.py was in the GPTQ-for-LLaMa folder). This .bat can also install\
          \ from wheel \"https://github.com/jllllll/GPTQ-for-LLaMa-Wheels/raw/main/quant_cuda-0.0.0-cp310-cp310-win_amd64.whl\"\
          \ if \"CUDA kernal compilation failed\" due to not having Visual C++ Build\
          \ Tools installed, but I had it anyway. \nThe process completed successfully,\
          \ with no errors. But generating tokens with this GPTQ when running no matter\
          \ what model, for reasons unknown to me, was slower (100% GPU load tho)\
          \ than if I ran \"git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git\
          \ -b cuda\" and \"python setup_cuda.py install --force\".\n\n> But if in\
          \ doubt, just use the no-act-order.pt file which will work with any version\
          \ of GPTQ-for-LLaMa.\n\nI checked vicuna-13B-1.1-GPTQ-4bit-128g.no-act-order.pt\
          \ on oobabooga-fork. Everything is fine, no problems. As soon as I change\
          \ the model to vicuna-13B-1.1-GPTQ-4bit-128g.safetensors, I immediately\
          \ get gibberish on the output.\nThe only thing that bothers me a bit is\
          \ the fact that no-act-order.pt version in theory (from your own words)\
          \ may produce somewhat worse results.\n\n***\n\nHere's some tests of these\
          \ models.\nt-g-webui (ooba-GPTQ (cuda) repository), default settings and\
          \ prompt. 100% GPU load everywhere.\n\n\"Hello, say something about yourself.\"\
          \n\nYour versions.\n\nno-act-order.pt:\n\nOutput generated in 7.64 seconds\
          \ (8.64 tokens/s, 66 tokens, context 41, seed 1229294013)\n\n.safetensors:\n\
          \n(gibberish) output generated in 21.44 seconds (9.28 tokens/s, 199 tokens,\
          \ context 41, seed 263917108)\n\njeremy-costello version.\n\nvicuna-13b-v1.1-4bit-128g\
          \ (.pt):\n\n(gibberish... wait, what. Did I broke something again...) output\
          \ generated in 20.82 seconds (9.56 tokens/s, 199 tokens, context 42, seed\
          \ 1443451655)\n\n***\n\nt-g-webui (original GPTQ (cuda) repository), default\
          \ settings and prompt. 100% GPU load everywhere.\n\n\"Hello, say something\
          \ about yourself.\"\n\nYour versions.\n\nno-act-order.pt:\n\nOutput generated\
          \ in 15.99 seconds (2.13 tokens/s, 34 tokens, context 41, seed 198665102)\n\
          \n.safetensors:\n\nOutput generated in 21.75 seconds (1.98 tokens/s, 43\
          \ tokens, context 41, seed 863730907)\n\njeremy-costello version.\n\nvicuna-13b-v1.1-4bit-128g\
          \ (.pt):\n\nOutput generated in 58.74 seconds (2.15 tokens/s, 126 tokens,\
          \ context 41, seed 47935661)"
        updatedAt: '2023-04-16T21:00:08.778Z'
      numEdits: 4
      reactions: []
    id: 643c55fd2d67f0ed22f9d4ae
    type: comment
  author: Morivy
  content: "> As to why you get poor performance when using the later GPTQ-for-LLaMa\
    \ code  - did you run `python setup_cuda.py install`? If you installed the later\
    \ GPTQ-for-LLaMa but didn't compile the CUDA kernel, that would certainly explain\
    \ a significant slowdown.\n\nI uninstalled everything completely and ran the commands\
    \ \"git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa.git -b cuda\" and\
    \ \"python setup_cuda.py install --force\" via install.bat (setup_cuda.py was\
    \ in the GPTQ-for-LLaMa folder). This .bat can also install from wheel \"https://github.com/jllllll/GPTQ-for-LLaMa-Wheels/raw/main/quant_cuda-0.0.0-cp310-cp310-win_amd64.whl\"\
    \ if \"CUDA kernal compilation failed\" due to not having Visual C++ Build Tools\
    \ installed, but I had it anyway. \nThe process completed successfully, with no\
    \ errors. But generating tokens with this GPTQ when running no matter what model,\
    \ for reasons unknown to me, was slower (100% GPU load tho) than if I ran \"git\
    \ clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda\" and \"python\
    \ setup_cuda.py install --force\".\n\n> But if in doubt, just use the no-act-order.pt\
    \ file which will work with any version of GPTQ-for-LLaMa.\n\nI checked vicuna-13B-1.1-GPTQ-4bit-128g.no-act-order.pt\
    \ on oobabooga-fork. Everything is fine, no problems. As soon as I change the\
    \ model to vicuna-13B-1.1-GPTQ-4bit-128g.safetensors, I immediately get gibberish\
    \ on the output.\nThe only thing that bothers me a bit is the fact that no-act-order.pt\
    \ version in theory (from your own words) may produce somewhat worse results.\n\
    \n***\n\nHere's some tests of these models.\nt-g-webui (ooba-GPTQ (cuda) repository),\
    \ default settings and prompt. 100% GPU load everywhere.\n\n\"Hello, say something\
    \ about yourself.\"\n\nYour versions.\n\nno-act-order.pt:\n\nOutput generated\
    \ in 7.64 seconds (8.64 tokens/s, 66 tokens, context 41, seed 1229294013)\n\n\
    .safetensors:\n\n(gibberish) output generated in 21.44 seconds (9.28 tokens/s,\
    \ 199 tokens, context 41, seed 263917108)\n\njeremy-costello version.\n\nvicuna-13b-v1.1-4bit-128g\
    \ (.pt):\n\n(gibberish... wait, what. Did I broke something again...) output generated\
    \ in 20.82 seconds (9.56 tokens/s, 199 tokens, context 42, seed 1443451655)\n\n\
    ***\n\nt-g-webui (original GPTQ (cuda) repository), default settings and prompt.\
    \ 100% GPU load everywhere.\n\n\"Hello, say something about yourself.\"\n\nYour\
    \ versions.\n\nno-act-order.pt:\n\nOutput generated in 15.99 seconds (2.13 tokens/s,\
    \ 34 tokens, context 41, seed 198665102)\n\n.safetensors:\n\nOutput generated\
    \ in 21.75 seconds (1.98 tokens/s, 43 tokens, context 41, seed 863730907)\n\n\
    jeremy-costello version.\n\nvicuna-13b-v1.1-4bit-128g (.pt):\n\nOutput generated\
    \ in 58.74 seconds (2.15 tokens/s, 126 tokens, context 41, seed 47935661)"
  created_at: 2023-04-16 19:09:33+00:00
  edited: true
  hidden: false
  id: 643c55fd2d67f0ed22f9d4ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T21:04:17.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p><code>act-order</code> was one of two new methods added to GPTQ
          to improve the quantisation quality. To be exact, they said this about it:</p>

          <pre><code>Two new tricks:--act-order (quantizing columns in order of decreasing
          activation size) and --true-sequential (performing sequential quantization
          even within a single Transformer block). Those fix GPTQ''s strangely bad
          performance on the 7B model (from 7.15 to 6.09 Wiki2 PPL) and lead to slight
          improvements on most models/settings in general.

          </code></pre>

          <p>Given you''re looking at the 13B model, it looks like only ''slight''
          improvements are to be expected.  No further details on what that means
          in practice. </p>

          <p>So I would say that ideally one would use <code>--act-order</code> if
          possible, but if you can''t, there''s no reason to think you''re going to
          get "bad" output on a 13B model.</p>

          <p>I have no idea why you''re getting slow output on the re-compiled CUDA
          branch. I noticed no performance problems when I tested the CUDA branch
          on Linux. The most likely explanation would seem to be that it''s not using
          the compiled CUDA kernel for some reason, but that''s just a guess and as
          I''ve never tried any of this on Windows I couldn''t hazard a guess as to
          what else might be wrong.</p>

          '
        raw: "`act-order` was one of two new methods added to GPTQ to improve the\
          \ quantisation quality. To be exact, they said this about it:\n```\nTwo\
          \ new tricks:--act-order (quantizing columns in order of decreasing activation\
          \ size) and --true-sequential (performing sequential quantization even within\
          \ a single Transformer block). Those fix GPTQ's strangely bad performance\
          \ on the 7B model (from 7.15 to 6.09 Wiki2 PPL) and lead to slight improvements\
          \ on most models/settings in general.\n```\nGiven you're looking at the\
          \ 13B model, it looks like only 'slight' improvements are to be expected.\
          \  No further details on what that means in practice. \n\nSo I would say\
          \ that ideally one would use `--act-order` if possible, but if you can't,\
          \ there's no reason to think you're going to get \"bad\" output on a 13B\
          \ model.\n\nI have no idea why you're getting slow output on the re-compiled\
          \ CUDA branch. I noticed no performance problems when I tested the CUDA\
          \ branch on Linux. The most likely explanation would seem to be that it's\
          \ not using the compiled CUDA kernel for some reason, but that's just a\
          \ guess and as I've never tried any of this on Windows I couldn't hazard\
          \ a guess as to what else might be wrong."
        updatedAt: '2023-04-16T21:04:17.967Z'
      numEdits: 0
      reactions: []
    id: 643c62d1e3a7bbe2cf3e07da
    type: comment
  author: TheBloke
  content: "`act-order` was one of two new methods added to GPTQ to improve the quantisation\
    \ quality. To be exact, they said this about it:\n```\nTwo new tricks:--act-order\
    \ (quantizing columns in order of decreasing activation size) and --true-sequential\
    \ (performing sequential quantization even within a single Transformer block).\
    \ Those fix GPTQ's strangely bad performance on the 7B model (from 7.15 to 6.09\
    \ Wiki2 PPL) and lead to slight improvements on most models/settings in general.\n\
    ```\nGiven you're looking at the 13B model, it looks like only 'slight' improvements\
    \ are to be expected.  No further details on what that means in practice. \n\n\
    So I would say that ideally one would use `--act-order` if possible, but if you\
    \ can't, there's no reason to think you're going to get \"bad\" output on a 13B\
    \ model.\n\nI have no idea why you're getting slow output on the re-compiled CUDA\
    \ branch. I noticed no performance problems when I tested the CUDA branch on Linux.\
    \ The most likely explanation would seem to be that it's not using the compiled\
    \ CUDA kernel for some reason, but that's just a guess and as I've never tried\
    \ any of this on Windows I couldn't hazard a guess as to what else might be wrong."
  created_at: 2023-04-16 20:04:17+00:00
  edited: false
  hidden: false
  id: 643c62d1e3a7bbe2cf3e07da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T21:06:59.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I just saw your edit. I''d say it''s expected that you''d get gibberish
          from Jeremy''s model. I was surprised when you said that you didn''t.  According
          to his README he''s using the same settings as me, including <code>--act-order</code>,
          which means it requires newer GPTQ-for-LLaMa code to use.  He has not provided
          a no-act-order version like I did.</p>

          '
        raw: I just saw your edit. I'd say it's expected that you'd get gibberish
          from Jeremy's model. I was surprised when you said that you didn't.  According
          to his README he's using the same settings as me, including `--act-order`,
          which means it requires newer GPTQ-for-LLaMa code to use.  He has not provided
          a no-act-order version like I did.
        updatedAt: '2023-04-16T21:07:26.327Z'
      numEdits: 1
      reactions: []
    id: 643c637349e0fa66dc948f52
    type: comment
  author: TheBloke
  content: I just saw your edit. I'd say it's expected that you'd get gibberish from
    Jeremy's model. I was surprised when you said that you didn't.  According to his
    README he's using the same settings as me, including `--act-order`, which means
    it requires newer GPTQ-for-LLaMa code to use.  He has not provided a no-act-order
    version like I did.
  created_at: 2023-04-16 20:06:59+00:00
  edited: true
  hidden: false
  id: 643c637349e0fa66dc948f52
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T21:08:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>By the way, in case you didn''t know you''ll usually get higher
          quality responses if you use this prompt template:</p>

          <pre><code>Below is an instruction that describes a task. Write a response
          that appropriately completes the request.

          ### Instruction:

          prompt goes here

          ### Response:

          </code></pre>

          '
        raw: 'By the way, in case you didn''t know you''ll usually get higher quality
          responses if you use this prompt template:


          ```

          Below is an instruction that describes a task. Write a response that appropriately
          completes the request.

          ### Instruction:

          prompt goes here

          ### Response:

          ```'
        updatedAt: '2023-04-16T21:08:52.646Z'
      numEdits: 0
      reactions: []
    id: 643c63e48c901ebc901f078d
    type: comment
  author: TheBloke
  content: 'By the way, in case you didn''t know you''ll usually get higher quality
    responses if you use this prompt template:


    ```

    Below is an instruction that describes a task. Write a response that appropriately
    completes the request.

    ### Instruction:

    prompt goes here

    ### Response:

    ```'
  created_at: 2023-04-16 20:08:52+00:00
  edited: false
  hidden: false
  id: 643c63e48c901ebc901f078d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-04-16T23:16:21.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Morivy&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Morivy\">@<span class=\"\
          underline\">Morivy</span></a></span>\n\n\t</span></span> - just wanted to\
          \ point out that there is also <a rel=\"nofollow\" href=\"https://github.com/jllllll/GPTQ-for-LLaMa-Wheels/raw/main/610fdae6588c2b17bcf2726cacaaf795cd45077e/quant_cuda-0.0.0-cp310-cp310-win_amd64.whl\"\
          >https://github.com/jllllll/GPTQ-for-LLaMa-Wheels/raw/main/610fdae6588c2b17bcf2726cacaaf795cd45077e/quant_cuda-0.0.0-cp310-cp310-win_amd64.whl</a>\
          \ \"610fdae wheel compiled from latest (as of writing) commit of GPTQ-for-LLaMa\"\
          \ - <a rel=\"nofollow\" href=\"https://github.com/jllllll/GPTQ-for-LLaMa-Wheels\"\
          >https://github.com/jllllll/GPTQ-for-LLaMa-Wheels</a></p>\n<p>Your issue\
          \ is described here: <a rel=\"nofollow\" href=\"https://github.com/qwopqwop200/GPTQ-for-LLaMa/issues/128\"\
          >https://github.com/qwopqwop200/GPTQ-for-LLaMa/issues/128</a><br>Also, cf.\
          \ README \"Unless you are using 3bit, i recommend using a branch that currently\
          \ supports triton.\" - <a rel=\"nofollow\" href=\"https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda\"\
          >https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda</a></p>\n<p>I recommend\
          \ you switch to the triton branch via WSL.</p>\n"
        raw: '@Morivy - just wanted to point out that there is also https://github.com/jllllll/GPTQ-for-LLaMa-Wheels/raw/main/610fdae6588c2b17bcf2726cacaaf795cd45077e/quant_cuda-0.0.0-cp310-cp310-win_amd64.whl
          "610fdae wheel compiled from latest (as of writing) commit of GPTQ-for-LLaMa"
          - https://github.com/jllllll/GPTQ-for-LLaMa-Wheels


          Your issue is described here: https://github.com/qwopqwop200/GPTQ-for-LLaMa/issues/128

          Also, cf. README "Unless you are using 3bit, i recommend using a branch
          that currently supports triton." - https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda


          I recommend you switch to the triton branch via WSL.'
        updatedAt: '2023-04-16T23:16:21.109Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 643c81c5f925266fb59d5f95
    type: comment
  author: Thireus
  content: '@Morivy - just wanted to point out that there is also https://github.com/jllllll/GPTQ-for-LLaMa-Wheels/raw/main/610fdae6588c2b17bcf2726cacaaf795cd45077e/quant_cuda-0.0.0-cp310-cp310-win_amd64.whl
    "610fdae wheel compiled from latest (as of writing) commit of GPTQ-for-LLaMa"
    - https://github.com/jllllll/GPTQ-for-LLaMa-Wheels


    Your issue is described here: https://github.com/qwopqwop200/GPTQ-for-LLaMa/issues/128

    Also, cf. README "Unless you are using 3bit, i recommend using a branch that currently
    supports triton." - https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda


    I recommend you switch to the triton branch via WSL.'
  created_at: 2023-04-16 22:16:21+00:00
  edited: false
  hidden: false
  id: 643c81c5f925266fb59d5f95
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b37d0ca991cec7656ef9519c2032bdf5.svg
      fullname: Bow Wow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tsumeone
      type: user
    createdAt: '2023-04-17T17:24:45.000Z'
    data:
      edited: false
      editors:
      - tsumeone
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b37d0ca991cec7656ef9519c2032bdf5.svg
          fullname: Bow Wow
          isHf: false
          isPro: false
          name: tsumeone
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Morivy&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Morivy\">@<span class=\"\
          underline\">Morivy</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;Thireus&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Thireus\">@<span class=\"underline\">Thireus</span></a></span>\n\
          \n\t</span></span><br>Sadly, qwop's cuda branch has huge performance regressions\
          \ after implementing the ability for simultaneous use of true-sequential/act-order/groupsize,\
          \ even when you don't run a model using those 3 together.  With development\
          \ focusing on Triton now (despite the lack of Windows compatibility and\
          \ the lack of compatibility with pre-RTX Nvidia cards) that's left a lot\
          \ of people like myself stuck on older versions of GPTQ.  Neither WSL2 nor\
          \ Linux have proven to be great solutions for me.  WSL2 takes 7+ minutes\
          \ to load a 30b model into VRAM even when I configure it for full access\
          \ to system ram and a huge swap space.  Running natively on Linux with the\
          \ Triton branch of GPTQ on Ooba causes a 4bit non-groupsized 30b model to\
          \ run out of VRAM at only 1300ctx on my 3090, even when the only other active\
          \ process using VRAM is X (using only 4MiB).  The same model on the same\
          \ system is fine at full ctx with an older GPTQ cuda in Windows.</p>\n<p>The\
          \ no-act-order model provided here works fine in the Ooba fork of GPTQ.\
          \  Unfortunately, it's too new to work in the 0cc4m fork of GPTQ.</p>\n"
        raw: "@Morivy @Thireus \nSadly, qwop's cuda branch has huge performance regressions\
          \ after implementing the ability for simultaneous use of true-sequential/act-order/groupsize,\
          \ even when you don't run a model using those 3 together.  With development\
          \ focusing on Triton now (despite the lack of Windows compatibility and\
          \ the lack of compatibility with pre-RTX Nvidia cards) that's left a lot\
          \ of people like myself stuck on older versions of GPTQ.  Neither WSL2 nor\
          \ Linux have proven to be great solutions for me.  WSL2 takes 7+ minutes\
          \ to load a 30b model into VRAM even when I configure it for full access\
          \ to system ram and a huge swap space.  Running natively on Linux with the\
          \ Triton branch of GPTQ on Ooba causes a 4bit non-groupsized 30b model to\
          \ run out of VRAM at only 1300ctx on my 3090, even when the only other active\
          \ process using VRAM is X (using only 4MiB).  The same model on the same\
          \ system is fine at full ctx with an older GPTQ cuda in Windows.\n\nThe\
          \ no-act-order model provided here works fine in the Ooba fork of GPTQ.\
          \  Unfortunately, it's too new to work in the 0cc4m fork of GPTQ."
        updatedAt: '2023-04-17T17:24:45.640Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - Thireus
    id: 643d80dde951e76f9a76262e
    type: comment
  author: tsumeone
  content: "@Morivy @Thireus \nSadly, qwop's cuda branch has huge performance regressions\
    \ after implementing the ability for simultaneous use of true-sequential/act-order/groupsize,\
    \ even when you don't run a model using those 3 together.  With development focusing\
    \ on Triton now (despite the lack of Windows compatibility and the lack of compatibility\
    \ with pre-RTX Nvidia cards) that's left a lot of people like myself stuck on\
    \ older versions of GPTQ.  Neither WSL2 nor Linux have proven to be great solutions\
    \ for me.  WSL2 takes 7+ minutes to load a 30b model into VRAM even when I configure\
    \ it for full access to system ram and a huge swap space.  Running natively on\
    \ Linux with the Triton branch of GPTQ on Ooba causes a 4bit non-groupsized 30b\
    \ model to run out of VRAM at only 1300ctx on my 3090, even when the only other\
    \ active process using VRAM is X (using only 4MiB).  The same model on the same\
    \ system is fine at full ctx with an older GPTQ cuda in Windows.\n\nThe no-act-order\
    \ model provided here works fine in the Ooba fork of GPTQ.  Unfortunately, it's\
    \ too new to work in the 0cc4m fork of GPTQ."
  created_at: 2023-04-17 16:24:45+00:00
  edited: false
  hidden: false
  id: 643d80dde951e76f9a76262e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb8c7a0e1860f38f72b58f165e447bed.svg
      fullname: Nonya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DryIceX
      type: user
    createdAt: '2023-04-28T22:31:13.000Z'
    data:
      edited: false
      editors:
      - DryIceX
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb8c7a0e1860f38f72b58f165e447bed.svg
          fullname: Nonya
          isHf: false
          isPro: false
          name: DryIceX
          type: user
        html: '<p>Model is worthless and spits out gibberish.</p>

          '
        raw: Model is worthless and spits out gibberish.
        updatedAt: '2023-04-28T22:31:13.941Z'
      numEdits: 0
      reactions: []
    id: 644c4931ed08a4fdf4e5b037
    type: comment
  author: DryIceX
  content: Model is worthless and spits out gibberish.
  created_at: 2023-04-28 21:31:13+00:00
  edited: false
  hidden: false
  id: 644c4931ed08a4fdf4e5b037
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T22:33:00.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Model is worthless and spits out gibberish.</p>

          </blockquote>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/bO6e1UJc0rWMq-wVEmRnO.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/bO6e1UJc0rWMq-wVEmRnO.png"></a></p>

          '
        raw: '> Model is worthless and spits out gibberish.


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/bO6e1UJc0rWMq-wVEmRnO.png)'
        updatedAt: '2023-04-28T22:33:00.030Z'
      numEdits: 0
      reactions: []
    id: 644c499c194e124dacc18bed
    type: comment
  author: TheBloke
  content: '> Model is worthless and spits out gibberish.


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/bO6e1UJc0rWMq-wVEmRnO.png)'
  created_at: 2023-04-28 21:33:00+00:00
  edited: false
  hidden: false
  id: 644c499c194e124dacc18bed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-28T22:36:19.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I just added instructions for easily downloading the model with
          a few clicks from text-generation-webui</p>

          <p>I also renamed the model files so the compatible file will load in preference
          to the one that requires latest GPTQ-for-LLaMa code.</p>

          <p>Delete the model you already downloaded and follow these instructions:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/XVJ_HizL3YNMRt4MFlQPV.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/XVJ_HizL3YNMRt4MFlQPV.png"></a></p>

          '
        raw: 'I just added instructions for easily downloading the model with a few
          clicks from text-generation-webui


          I also renamed the model files so the compatible file will load in preference
          to the one that requires latest GPTQ-for-LLaMa code.


          Delete the model you already downloaded and follow these instructions:



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/XVJ_HizL3YNMRt4MFlQPV.png)'
        updatedAt: '2023-04-28T22:36:19.013Z'
      numEdits: 0
      reactions: []
    id: 644c4a63ed08a4fdf4e5cd2c
    type: comment
  author: TheBloke
  content: 'I just added instructions for easily downloading the model with a few
    clicks from text-generation-webui


    I also renamed the model files so the compatible file will load in preference
    to the one that requires latest GPTQ-for-LLaMa code.


    Delete the model you already downloaded and follow these instructions:



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/XVJ_HizL3YNMRt4MFlQPV.png)'
  created_at: 2023-04-28 21:36:19+00:00
  edited: false
  hidden: false
  id: 644c4a63ed08a4fdf4e5cd2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb8c7a0e1860f38f72b58f165e447bed.svg
      fullname: Nonya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DryIceX
      type: user
    createdAt: '2023-04-28T22:48:35.000Z'
    data:
      edited: false
      editors:
      - DryIceX
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb8c7a0e1860f38f72b58f165e447bed.svg
          fullname: Nonya
          isHf: false
          isPro: false
          name: DryIceX
          type: user
        html: '<blockquote>

          <p>I just added instructions for easily downloading the model with a few
          clicks from text-generation-webui</p>

          <p>I also renamed the model files so the compatible file will load in preference
          to the one that requires latest GPTQ-for-LLaMa code.</p>

          <p>Delete the model you already downloaded and follow these instructions:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/XVJ_HizL3YNMRt4MFlQPV.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/XVJ_HizL3YNMRt4MFlQPV.png"></a></p>

          </blockquote>

          <p>I updated already. Still getting gibberish.</p>

          '
        raw: "> I just added instructions for easily downloading the model with a\
          \ few clicks from text-generation-webui\n> \n> I also renamed the model\
          \ files so the compatible file will load in preference to the one that requires\
          \ latest GPTQ-for-LLaMa code.\n> \n> Delete the model you already downloaded\
          \ and follow these instructions:\n> \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/XVJ_HizL3YNMRt4MFlQPV.png)\n\
          \nI updated already. Still getting gibberish."
        updatedAt: '2023-04-28T22:48:35.369Z'
      numEdits: 0
      reactions: []
    id: 644c4d430ce4f8fb51739e3e
    type: comment
  author: DryIceX
  content: "> I just added instructions for easily downloading the model with a few\
    \ clicks from text-generation-webui\n> \n> I also renamed the model files so the\
    \ compatible file will load in preference to the one that requires latest GPTQ-for-LLaMa\
    \ code.\n> \n> Delete the model you already downloaded and follow these instructions:\n\
    > \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/XVJ_HizL3YNMRt4MFlQPV.png)\n\
    \nI updated already. Still getting gibberish."
  created_at: 2023-04-28 21:48:35+00:00
  edited: false
  hidden: false
  id: 644c4d430ce4f8fb51739e3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7426b0eb3f4f9971907c9ef71db2da40.svg
      fullname: Leszek Hanusz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leszekhanusz
      type: user
    createdAt: '2023-05-11T21:49:33.000Z'
    data:
      edited: false
      editors:
      - leszekhanusz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7426b0eb3f4f9971907c9ef71db2da40.svg
          fullname: Leszek Hanusz
          isHf: false
          isPro: false
          name: leszekhanusz
          type: user
        html: '<p>Alright I had the same problem with gibberish but did not understand
          the explanation to update GPTQ-for-LLaMa as it seems at first I was up-to-date
          with the latest version.<br>But I realized that my repository in <code>repositories/GPTQ-for-LLaMa</code>
          was in fact pointing to a fork for oobabooga: <code>https://github.com/oobabooga/GPTQ-for-LLaMa</code>which
          was not updated for some time.<br>The solution then was to rename the existing
          GPTQ-for-LLaMa folder to something else, then clone from the correct repo:<br><code>git
          clone https://github.com/qwopqwop200/GPTQ-for-LLaMa</code></p>

          <p>I did then a <code>pip install -r requirements.txt --upgrade</code> in
          my Python env and after restarting the server now it seems to work correctly.</p>

          '
        raw: 'Alright I had the same problem with gibberish but did not understand
          the explanation to update GPTQ-for-LLaMa as it seems at first I was up-to-date
          with the latest version.

          But I realized that my repository in `repositories/GPTQ-for-LLaMa` was in
          fact pointing to a fork for oobabooga: `https://github.com/oobabooga/GPTQ-for-LLaMa`which
          was not updated for some time.

          The solution then was to rename the existing GPTQ-for-LLaMa folder to something
          else, then clone from the correct repo:

          `git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa`


          I did then a `pip install -r requirements.txt --upgrade` in my Python env
          and after restarting the server now it seems to work correctly.'
        updatedAt: '2023-05-11T21:49:33.932Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 645d62ed8ce4443cae741529
    type: comment
  author: leszekhanusz
  content: 'Alright I had the same problem with gibberish but did not understand the
    explanation to update GPTQ-for-LLaMa as it seems at first I was up-to-date with
    the latest version.

    But I realized that my repository in `repositories/GPTQ-for-LLaMa` was in fact
    pointing to a fork for oobabooga: `https://github.com/oobabooga/GPTQ-for-LLaMa`which
    was not updated for some time.

    The solution then was to rename the existing GPTQ-for-LLaMa folder to something
    else, then clone from the correct repo:

    `git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa`


    I did then a `pip install -r requirements.txt --upgrade` in my Python env and
    after restarting the server now it seems to work correctly.'
  created_at: 2023-05-11 20:49:33+00:00
  edited: false
  hidden: false
  id: 645d62ed8ce4443cae741529
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb8c7a0e1860f38f72b58f165e447bed.svg
      fullname: Nonya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DryIceX
      type: user
    createdAt: '2023-05-13T01:34:34.000Z'
    data:
      edited: false
      editors:
      - DryIceX
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb8c7a0e1860f38f72b58f165e447bed.svg
          fullname: Nonya
          isHf: false
          isPro: false
          name: DryIceX
          type: user
        html: '<blockquote>

          <p>Alright I had the same problem with gibberish but did not understand
          the explanation to update GPTQ-for-LLaMa as it seems at first I was up-to-date
          with the latest version.<br>But I realized that my repository in <code>repositories/GPTQ-for-LLaMa</code>
          was in fact pointing to a fork for oobabooga: <code>https://github.com/oobabooga/GPTQ-for-LLaMa</code>which
          was not updated for some time.<br>The solution then was to rename the existing
          GPTQ-for-LLaMa folder to something else, then clone from the correct repo:<br><code>git
          clone https://github.com/qwopqwop200/GPTQ-for-LLaMa</code></p>

          <p>I did then a <code>pip install -r requirements.txt --upgrade</code> in
          my Python env and after restarting the server now it seems to work correctly.</p>

          </blockquote>

          <p>Something must be up, I followed your instructions to a tee, still getting
          gibberish.</p>

          <p>Pip install is 2.23.1, einops have been updated, oobabooga updated...
          what am I missing?</p>

          '
        raw: "> Alright I had the same problem with gibberish but did not understand\
          \ the explanation to update GPTQ-for-LLaMa as it seems at first I was up-to-date\
          \ with the latest version.\n> But I realized that my repository in `repositories/GPTQ-for-LLaMa`\
          \ was in fact pointing to a fork for oobabooga: `https://github.com/oobabooga/GPTQ-for-LLaMa`which\
          \ was not updated for some time.\n> The solution then was to rename the\
          \ existing GPTQ-for-LLaMa folder to something else, then clone from the\
          \ correct repo:\n> `git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa`\n\
          > \n> I did then a `pip install -r requirements.txt --upgrade` in my Python\
          \ env and after restarting the server now it seems to work correctly.\n\n\
          Something must be up, I followed your instructions to a tee, still getting\
          \ gibberish.\n\nPip install is 2.23.1, einops have been updated, oobabooga\
          \ updated... what am I missing?"
        updatedAt: '2023-05-13T01:34:34.013Z'
      numEdits: 0
      reactions: []
    id: 645ee92abafcebea7d9fb894
    type: comment
  author: DryIceX
  content: "> Alright I had the same problem with gibberish but did not understand\
    \ the explanation to update GPTQ-for-LLaMa as it seems at first I was up-to-date\
    \ with the latest version.\n> But I realized that my repository in `repositories/GPTQ-for-LLaMa`\
    \ was in fact pointing to a fork for oobabooga: `https://github.com/oobabooga/GPTQ-for-LLaMa`which\
    \ was not updated for some time.\n> The solution then was to rename the existing\
    \ GPTQ-for-LLaMa folder to something else, then clone from the correct repo:\n\
    > `git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa`\n> \n> I did then\
    \ a `pip install -r requirements.txt --upgrade` in my Python env and after restarting\
    \ the server now it seems to work correctly.\n\nSomething must be up, I followed\
    \ your instructions to a tee, still getting gibberish.\n\nPip install is 2.23.1,\
    \ einops have been updated, oobabooga updated... what am I missing?"
  created_at: 2023-05-13 00:34:34+00:00
  edited: false
  hidden: false
  id: 645ee92abafcebea7d9fb894
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T12:55:20.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>What model files do you have in my model dir?</p>

          <p>If you updated GPTQ-for-LLaMa like leszekhanusz described then you should
          be able to use either model file.  But try removing the latest.safetensors
          file and trying again. Then you''ll be using the compatible file which should
          work with any/all versions of GPTQ-for-LLaMa.</p>

          '
        raw: 'What model files do you have in my model dir?


          If you updated GPTQ-for-LLaMa like leszekhanusz described then you should
          be able to use either model file.  But try removing the latest.safetensors
          file and trying again. Then you''ll be using the compatible file which should
          work with any/all versions of GPTQ-for-LLaMa.'
        updatedAt: '2023-05-13T12:55:54.515Z'
      numEdits: 1
      reactions: []
    id: 645f88b8d27fa024aa84a75b
    type: comment
  author: TheBloke
  content: 'What model files do you have in my model dir?


    If you updated GPTQ-for-LLaMa like leszekhanusz described then you should be able
    to use either model file.  But try removing the latest.safetensors file and trying
    again. Then you''ll be using the compatible file which should work with any/all
    versions of GPTQ-for-LLaMa.'
  created_at: 2023-05-13 11:55:20+00:00
  edited: true
  hidden: false
  id: 645f88b8d27fa024aa84a75b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb8c7a0e1860f38f72b58f165e447bed.svg
      fullname: Nonya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DryIceX
      type: user
    createdAt: '2023-05-17T23:34:20.000Z'
    data:
      edited: false
      editors:
      - DryIceX
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb8c7a0e1860f38f72b58f165e447bed.svg
          fullname: Nonya
          isHf: false
          isPro: false
          name: DryIceX
          type: user
        html: '<blockquote>

          <p>What model files do you have in my model dir?</p>

          <p>If you updated GPTQ-for-LLaMa like leszekhanusz described then you should
          be able to use either model file.  But try removing the latest.safetensors
          file and trying again. Then you''ll be using the compatible file which should
          work with any/all versions of GPTQ-for-LLaMa.</p>

          </blockquote>

          <p>Finally got it to work... Don''t know if it was because I removed the
          old "repositories/GPTQ-for-LLaMa" folder completely or putting the "vicuna-13B-1.1-GPTQ-4bit-128g.compat.no-act-order"
          file in the models folder but whatever did it, it''s finally not putting
          out gibberish.</p>

          '
        raw: "> What model files do you have in my model dir?\n> \n> If you updated\
          \ GPTQ-for-LLaMa like leszekhanusz described then you should be able to\
          \ use either model file.  But try removing the latest.safetensors file and\
          \ trying again. Then you'll be using the compatible file which should work\
          \ with any/all versions of GPTQ-for-LLaMa.\n\nFinally got it to work...\
          \ Don't know if it was because I removed the old \"repositories/GPTQ-for-LLaMa\"\
          \ folder completely or putting the \"vicuna-13B-1.1-GPTQ-4bit-128g.compat.no-act-order\"\
          \ file in the models folder but whatever did it, it's finally not putting\
          \ out gibberish."
        updatedAt: '2023-05-17T23:34:20.602Z'
      numEdits: 0
      reactions: []
    id: 6465647ce8e31202cb5403ee
    type: comment
  author: DryIceX
  content: "> What model files do you have in my model dir?\n> \n> If you updated\
    \ GPTQ-for-LLaMa like leszekhanusz described then you should be able to use either\
    \ model file.  But try removing the latest.safetensors file and trying again.\
    \ Then you'll be using the compatible file which should work with any/all versions\
    \ of GPTQ-for-LLaMa.\n\nFinally got it to work... Don't know if it was because\
    \ I removed the old \"repositories/GPTQ-for-LLaMa\" folder completely or putting\
    \ the \"vicuna-13B-1.1-GPTQ-4bit-128g.compat.no-act-order\" file in the models\
    \ folder but whatever did it, it's finally not putting out gibberish."
  created_at: 2023-05-17 22:34:20+00:00
  edited: false
  hidden: false
  id: 6465647ce8e31202cb5403ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-18T21:15:31.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great, glad it''s working now!</p>

          '
        raw: Great, glad it's working now!
        updatedAt: '2023-05-18T21:15:31.430Z'
      numEdits: 0
      reactions: []
    id: 646695733b99ed9970015434
    type: comment
  author: TheBloke
  content: Great, glad it's working now!
  created_at: 2023-05-18 20:15:31+00:00
  edited: false
  hidden: false
  id: 646695733b99ed9970015434
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Vicuna-13B-1.1-GPTQ
repo_type: model
status: open
target_branch: null
title: Cannot load the model
