!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Thireus
conflicting_files: null
created_at: 2023-04-19 18:32:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-04-19T19:32:58.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>For some reason your model has slightly better PPL than any of the 4bit-128g\
          \ versions I recently converted. I say \"any\" because I tried various combinations\
          \ of GPTQ commits and transformers versions.</p>\n<p>Some metrics for some\
          \ versions I've tried:</p>\n<div class=\"max-w-full overflow-auto\">\n\t\
          <table>\n\t\t<thead><tr>\n<th>Model</th>\n<th>wikitext2 PPL</th>\n<th>ptb-new\
          \ PPL</th>\n<th>c4-new PPL</th>\n</tr>\n\n\t\t</thead><tbody><tr>\n<td>4bit-GPTQ\
          \ - TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g</td>\n<td><strong>7.119165420532227</strong></td>\n\
          <td>35.637290954589844</td>\n<td><strong>9.550592422485352</strong></td>\n\
          </tr>\n<tr>\n<td>4bit-GPTQ - Thireus/Vicuna13B-v1.1-4bit-128g (GPTQ commit\
          \ 508de42ff45ec560a4504e12b0d42114d599cf38)</td>\n<td>7.129854202270508</td>\n\
          <td>35.848060607910156</td>\n<td>9.568032264709473</td>\n</tr>\n<tr>\n<td>4bit-GPTQ\
          \ - Thireus/Vicuna13B-v1.1-4bit-128g (GPTQ commit d89cdcd8b53f61346290a28d326816af6a028434)</td>\n\
          <td>7.137491226196289</td>\n<td>35.530372619628906</td>\n<td>9.597953796386719</td>\n\
          </tr>\n<tr>\n<td>4bit-GPTQ - Thireus/Vicuna13B-v1.1-4bit-128g (GPTQ commit\
          \ f3f7a6910fd6778548cdafe7f0d5155411b1696c)</td>\n<td>7.137701988220215</td>\n\
          <td><strong>35.52903366088867</strong></td>\n<td>9.597844123840332</td>\n\
          </tr>\n<tr>\n<td>4bit-GPTQ - Thireus/Vicuna13B-v1.1-4bit-128g (GPTQ commit\
          \ 49ffd9ab085004978a6bdc8e2dff7510f2458e71)</td>\n<td>7.137701988220215</td>\n\
          <td><strong>35.52903366088867</strong></td>\n<td>9.597844123840332</td>\n\
          </tr>\n</tbody>\n\t</table>\n</div>\n<pre><code>pip freeze | grep transformers\n\
          transformers @ git+https://github.com/huggingface/transformers@5bb4ec6233d6414a922ad2818f0bcf879de81c28\n\
          </code></pre>\n<p>Would you have an idea why that is and what can influence\
          \ this? Did you use Triton or Cuda?</p>\n"
        raw: "Hi @TheBloke,\r\n\r\nFor some reason your model has slightly better\
          \ PPL than any of the 4bit-128g versions I recently converted. I say \"\
          any\" because I tried various combinations of GPTQ commits and transformers\
          \ versions.\r\n\r\nSome metrics for some versions I've tried:\r\n| Model\
          \  | wikitext2 PPL  | ptb-new PPL  | c4-new PPL  |\r\n|---|---|---|---|\r\
          \n| 4bit-GPTQ - TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g  | **7.119165420532227**\
          \  | 35.637290954589844  | **9.550592422485352**  |\r\n| 4bit-GPTQ - Thireus/Vicuna13B-v1.1-4bit-128g\
          \ (GPTQ commit 508de42ff45ec560a4504e12b0d42114d599cf38) | 7.129854202270508\
          \ | 35.848060607910156 | 9.568032264709473 | \r\n| 4bit-GPTQ - Thireus/Vicuna13B-v1.1-4bit-128g\
          \ (GPTQ commit d89cdcd8b53f61346290a28d326816af6a028434) | 7.137491226196289\
          \ | 35.530372619628906 | 9.597953796386719 | \r\n| 4bit-GPTQ - Thireus/Vicuna13B-v1.1-4bit-128g\
          \ (GPTQ commit f3f7a6910fd6778548cdafe7f0d5155411b1696c) | 7.137701988220215\
          \ | **35.52903366088867** | 9.597844123840332 | \r\n| 4bit-GPTQ - Thireus/Vicuna13B-v1.1-4bit-128g\
          \ (GPTQ commit 49ffd9ab085004978a6bdc8e2dff7510f2458e71) | 7.137701988220215\
          \ | **35.52903366088867** | 9.597844123840332 | \r\n\r\n```\r\npip freeze\
          \ | grep transformers\r\ntransformers @ git+https://github.com/huggingface/transformers@5bb4ec6233d6414a922ad2818f0bcf879de81c28\r\
          \n```\r\n\r\nWould you have an idea why that is and what can influence this?\
          \ Did you use Triton or Cuda?"
        updatedAt: '2023-04-19T19:32:58.778Z'
      numEdits: 0
      reactions: []
    id: 644041ea7841867cd5b4dce6
    type: comment
  author: Thireus
  content: "Hi @TheBloke,\r\n\r\nFor some reason your model has slightly better PPL\
    \ than any of the 4bit-128g versions I recently converted. I say \"any\" because\
    \ I tried various combinations of GPTQ commits and transformers versions.\r\n\r\
    \nSome metrics for some versions I've tried:\r\n| Model  | wikitext2 PPL  | ptb-new\
    \ PPL  | c4-new PPL  |\r\n|---|---|---|---|\r\n| 4bit-GPTQ - TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g\
    \  | **7.119165420532227**  | 35.637290954589844  | **9.550592422485352**  |\r\
    \n| 4bit-GPTQ - Thireus/Vicuna13B-v1.1-4bit-128g (GPTQ commit 508de42ff45ec560a4504e12b0d42114d599cf38)\
    \ | 7.129854202270508 | 35.848060607910156 | 9.568032264709473 | \r\n| 4bit-GPTQ\
    \ - Thireus/Vicuna13B-v1.1-4bit-128g (GPTQ commit d89cdcd8b53f61346290a28d326816af6a028434)\
    \ | 7.137491226196289 | 35.530372619628906 | 9.597953796386719 | \r\n| 4bit-GPTQ\
    \ - Thireus/Vicuna13B-v1.1-4bit-128g (GPTQ commit f3f7a6910fd6778548cdafe7f0d5155411b1696c)\
    \ | 7.137701988220215 | **35.52903366088867** | 9.597844123840332 | \r\n| 4bit-GPTQ\
    \ - Thireus/Vicuna13B-v1.1-4bit-128g (GPTQ commit 49ffd9ab085004978a6bdc8e2dff7510f2458e71)\
    \ | 7.137701988220215 | **35.52903366088867** | 9.597844123840332 | \r\n\r\n```\r\
    \npip freeze | grep transformers\r\ntransformers @ git+https://github.com/huggingface/transformers@5bb4ec6233d6414a922ad2818f0bcf879de81c28\r\
    \n```\r\n\r\nWould you have an idea why that is and what can influence this? Did\
    \ you use Triton or Cuda?"
  created_at: 2023-04-19 18:32:58+00:00
  edited: false
  hidden: false
  id: 644041ea7841867cd5b4dce6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-19T20:28:58.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Oh, that's interesting.</p>\n<p>To be honest I don't think I did\
          \ anything special. The commands I use are in my READMEs, but for example:</p>\n\
          <pre><code>CUDA_VISIBLE_DEVICES=0 python3 llama.py vicuna-13B-1.1-HF c4\
          \ --wbits 4 --true-sequential --act-order --groupsize 128 --save_safetensors\
          \ /workspace/vicuna-13B-1.1-GPTQ-4bit-128g.safetensors \n</code></pre>\n\
          <p>Lately I have always been using the Triton branch for making GPTQs. \
          \ And I made this GPTQ - and all my recent ones - with commit <code>58c8ab4c7aaccc50f507fd08cce941976affe5e0</code>on\
          \ the qwopqwop repo. Which was the last commit on April 13th, before he\
          \ started the refactor that broke everything.</p>\n<p>In terms of pip versions.\
          \ I can't check precise commits because I do all of this in the cloud and\
          \ each instance gets destroyed afterwards.  Until recently I was pulling\
          \ peft and transformers from their respective Githubs, but after transformers\
          \ finally released 4.28.0 a couple of days ago I started using the standard\
          \ versions.  I did this GPTQ four days ago so I think that would have been\
          \ using Github transformers.  PyTorch is 2.0.0+cu118.  Triton is 2.0.0.</p>\n\
          <p>Here's my initialisation script that installs all my dependencies:</p>\n\
          <pre><code>echo -n \"PIP OTHERS: \"\n(pip3 uninstall -qy transformers peft\
          \ datasets loralib sentencepiece safetensors accelerate triton bitsandbytes\
          \ huggingface_hub flexgen rwkv quant-cuda &amp;&amp; \\\npip3 install -q\
          \ datasets==2.10.1 loralib sentencepiece safetensors==0.3.0 accelerate==0.18.0\
          \ triton==2.0.0 huggingface_hub &amp;&amp; \\\npip3 install -q transformers\
          \ &amp;&amp; \\\npip3 install -q peft &amp;&amp; \\\npip3 install -q bitsandbytes==0.37.2\
          \ xformers &amp;&amp; \\\npip3 install -q markdown pyyaml tqdm requests\
          \ gradio==3.24.1 flexgen==0.1.7 rwkv==0.7.3 ninja ) &gt;/dev/null 2&gt;errors.pip\
          \ &amp;&amp; echo \" DONE\" || cat errors.pip\n\necho -n \"GIT SETUP: \"\
          \n( git config --global credential.helper store &amp;&amp; \\\ngit config\
          \ --global user.email \"XXX\" &amp;&amp; \\\ngit config --global user.name\
          \ \"TheBloke\" &amp;&amp; \\\nhuggingface-cli login --add-to-git-credential\
          \ --token 'XXX' &amp;&amp; \\\ngit lfs install ) &gt;/dev/null 2&gt;errors.gitsetup\
          \ &amp;&amp; echo \" DONE\" || cat errors.gitsetup\n\necho -n \"GIT GPTQ:\
          \ \"\n( git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa gptq-llama\
          \ &amp;&amp; \\\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ -b cuda gptq-llama-cuda &amp;&amp; \\\ngit clone https://github.com/oobabooga/GPTQ-for-LLaMa\
          \ ooba-gptq-llama &amp;&amp; \\\ngit clone -n  https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ gptq-safe &amp;&amp; cd gptq-safe &amp;&amp; git checkout 58c8ab4c7aaccc50f507fd08cce941976affe5e0\
          \ ) &gt;/dev/null 2&gt;errors.gitgptq &amp;&amp; echo \" DONE\" || cat errors.gitgptq\n\
          \necho -n \"WEBUI SETUP: \"\n( rm -rf /content/text-generation-webui &amp;&amp;\
          \ \\\ngit clone https://github.com/oobabooga/text-generation-webui &amp;&amp;\
          \ \\\nmkdir -p text-generation-webui/repositories &amp;&amp; \\\nln -s /content/gptq-safe\
          \ text-generation-webui/repositories/GPTQ-for-LLaMa   ) &gt;/dev/null 2&gt;errors.webui\
          \ &amp;&amp; echo \" DONE\" || cat errors.webui \n</code></pre>\n<p>I actually\
          \ re-made this particular GPTQ four days ago, because I had realised that\
          \ my original <code>vicuna-13B-1.1-HF</code> repo had been converted to\
          \ HF with a buggy version of the transformers <code>models/llama/convert_llama_weights_to_hf.py</code>\
          \ script which caused the 13B models to use 37GB on disk instead of 26GB.\
          \  So I re-converted my <code>vicuna-13B-1.1-HF</code> repo, and then just\
          \ in case that affected the GPTQ I also re-made the GPTQs.</p>\n<p>No idea\
          \ if any of that would affect this, but that's what I did! I suppose that\
          \ might mean I used a later version of GPTQ-for-LLaMa than you did?</p>\n\
          <p>Question for you if you've got a sec: what code/method do you use to\
          \ produce those metrics?  So far the only evaluation I've done on models\
          \ has been for GGML models, using llama.cpp's perplexity binary. I've been\
          \ meaning to try evaluating GPU models but haven't looked into it yet.</p>\n"
        raw: "Oh, that's interesting.\n\nTo be honest I don't think I did anything\
          \ special. The commands I use are in my READMEs, but for example:\n```\n\
          CUDA_VISIBLE_DEVICES=0 python3 llama.py vicuna-13B-1.1-HF c4 --wbits 4 --true-sequential\
          \ --act-order --groupsize 128 --save_safetensors /workspace/vicuna-13B-1.1-GPTQ-4bit-128g.safetensors\
          \ \n```\n\nLately I have always been using the Triton branch for making\
          \ GPTQs.  And I made this GPTQ - and all my recent ones - with commit `58c8ab4c7aaccc50f507fd08cce941976affe5e0`on\
          \ the qwopqwop repo. Which was the last commit on April 13th, before he\
          \ started the refactor that broke everything.\n\nIn terms of pip versions.\
          \ I can't check precise commits because I do all of this in the cloud and\
          \ each instance gets destroyed afterwards.  Until recently I was pulling\
          \ peft and transformers from their respective Githubs, but after transformers\
          \ finally released 4.28.0 a couple of days ago I started using the standard\
          \ versions.  I did this GPTQ four days ago so I think that would have been\
          \ using Github transformers.  PyTorch is 2.0.0+cu118.  Triton is 2.0.0.\n\
          \nHere's my initialisation script that installs all my dependencies:\n```\n\
          echo -n \"PIP OTHERS: \"\n(pip3 uninstall -qy transformers peft datasets\
          \ loralib sentencepiece safetensors accelerate triton bitsandbytes huggingface_hub\
          \ flexgen rwkv quant-cuda && \\\npip3 install -q datasets==2.10.1 loralib\
          \ sentencepiece safetensors==0.3.0 accelerate==0.18.0 triton==2.0.0 huggingface_hub\
          \ && \\\npip3 install -q transformers && \\\npip3 install -q peft && \\\n\
          pip3 install -q bitsandbytes==0.37.2 xformers && \\\npip3 install -q markdown\
          \ pyyaml tqdm requests gradio==3.24.1 flexgen==0.1.7 rwkv==0.7.3 ninja )\
          \ >/dev/null 2>errors.pip && echo \" DONE\" || cat errors.pip\n\necho -n\
          \ \"GIT SETUP: \"\n( git config --global credential.helper store && \\\n\
          git config --global user.email \"XXX\" && \\\ngit config --global user.name\
          \ \"TheBloke\" && \\\nhuggingface-cli login --add-to-git-credential --token\
          \ 'XXX' && \\\ngit lfs install ) >/dev/null 2>errors.gitsetup && echo \"\
          \ DONE\" || cat errors.gitsetup\n\necho -n \"GIT GPTQ: \"\n( git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ gptq-llama && \\\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ -b cuda gptq-llama-cuda && \\\ngit clone https://github.com/oobabooga/GPTQ-for-LLaMa\
          \ ooba-gptq-llama && \\\ngit clone -n  https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ gptq-safe && cd gptq-safe && git checkout 58c8ab4c7aaccc50f507fd08cce941976affe5e0\
          \ ) >/dev/null 2>errors.gitgptq && echo \" DONE\" || cat errors.gitgptq\n\
          \necho -n \"WEBUI SETUP: \"\n( rm -rf /content/text-generation-webui &&\
          \ \\\ngit clone https://github.com/oobabooga/text-generation-webui && \\\
          \nmkdir -p text-generation-webui/repositories && \\\nln -s /content/gptq-safe\
          \ text-generation-webui/repositories/GPTQ-for-LLaMa   ) >/dev/null 2>errors.webui\
          \ && echo \" DONE\" || cat errors.webui \n```\n\nI actually re-made this\
          \ particular GPTQ four days ago, because I had realised that my original\
          \ `vicuna-13B-1.1-HF` repo had been converted to HF with a buggy version\
          \ of the transformers `models/llama/convert_llama_weights_to_hf.py` script\
          \ which caused the 13B models to use 37GB on disk instead of 26GB.  So I\
          \ re-converted my `vicuna-13B-1.1-HF` repo, and then just in case that affected\
          \ the GPTQ I also re-made the GPTQs.\n\nNo idea if any of that would affect\
          \ this, but that's what I did! I suppose that might mean I used a later\
          \ version of GPTQ-for-LLaMa than you did?\n\nQuestion for you if you've\
          \ got a sec: what code/method do you use to produce those metrics?  So far\
          \ the only evaluation I've done on models has been for GGML models, using\
          \ llama.cpp's perplexity binary. I've been meaning to try evaluating GPU\
          \ models but haven't looked into it yet."
        updatedAt: '2023-04-19T20:28:58.568Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Thireus
    id: 64404f0a2113f7dfcb5701c4
    type: comment
  author: TheBloke
  content: "Oh, that's interesting.\n\nTo be honest I don't think I did anything special.\
    \ The commands I use are in my READMEs, but for example:\n```\nCUDA_VISIBLE_DEVICES=0\
    \ python3 llama.py vicuna-13B-1.1-HF c4 --wbits 4 --true-sequential --act-order\
    \ --groupsize 128 --save_safetensors /workspace/vicuna-13B-1.1-GPTQ-4bit-128g.safetensors\
    \ \n```\n\nLately I have always been using the Triton branch for making GPTQs.\
    \  And I made this GPTQ - and all my recent ones - with commit `58c8ab4c7aaccc50f507fd08cce941976affe5e0`on\
    \ the qwopqwop repo. Which was the last commit on April 13th, before he started\
    \ the refactor that broke everything.\n\nIn terms of pip versions. I can't check\
    \ precise commits because I do all of this in the cloud and each instance gets\
    \ destroyed afterwards.  Until recently I was pulling peft and transformers from\
    \ their respective Githubs, but after transformers finally released 4.28.0 a couple\
    \ of days ago I started using the standard versions.  I did this GPTQ four days\
    \ ago so I think that would have been using Github transformers.  PyTorch is 2.0.0+cu118.\
    \  Triton is 2.0.0.\n\nHere's my initialisation script that installs all my dependencies:\n\
    ```\necho -n \"PIP OTHERS: \"\n(pip3 uninstall -qy transformers peft datasets\
    \ loralib sentencepiece safetensors accelerate triton bitsandbytes huggingface_hub\
    \ flexgen rwkv quant-cuda && \\\npip3 install -q datasets==2.10.1 loralib sentencepiece\
    \ safetensors==0.3.0 accelerate==0.18.0 triton==2.0.0 huggingface_hub && \\\n\
    pip3 install -q transformers && \\\npip3 install -q peft && \\\npip3 install -q\
    \ bitsandbytes==0.37.2 xformers && \\\npip3 install -q markdown pyyaml tqdm requests\
    \ gradio==3.24.1 flexgen==0.1.7 rwkv==0.7.3 ninja ) >/dev/null 2>errors.pip &&\
    \ echo \" DONE\" || cat errors.pip\n\necho -n \"GIT SETUP: \"\n( git config --global\
    \ credential.helper store && \\\ngit config --global user.email \"XXX\" && \\\n\
    git config --global user.name \"TheBloke\" && \\\nhuggingface-cli login --add-to-git-credential\
    \ --token 'XXX' && \\\ngit lfs install ) >/dev/null 2>errors.gitsetup && echo\
    \ \" DONE\" || cat errors.gitsetup\n\necho -n \"GIT GPTQ: \"\n( git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
    \ gptq-llama && \\\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa -b\
    \ cuda gptq-llama-cuda && \\\ngit clone https://github.com/oobabooga/GPTQ-for-LLaMa\
    \ ooba-gptq-llama && \\\ngit clone -n  https://github.com/qwopqwop200/GPTQ-for-LLaMa\
    \ gptq-safe && cd gptq-safe && git checkout 58c8ab4c7aaccc50f507fd08cce941976affe5e0\
    \ ) >/dev/null 2>errors.gitgptq && echo \" DONE\" || cat errors.gitgptq\n\necho\
    \ -n \"WEBUI SETUP: \"\n( rm -rf /content/text-generation-webui && \\\ngit clone\
    \ https://github.com/oobabooga/text-generation-webui && \\\nmkdir -p text-generation-webui/repositories\
    \ && \\\nln -s /content/gptq-safe text-generation-webui/repositories/GPTQ-for-LLaMa\
    \   ) >/dev/null 2>errors.webui && echo \" DONE\" || cat errors.webui \n```\n\n\
    I actually re-made this particular GPTQ four days ago, because I had realised\
    \ that my original `vicuna-13B-1.1-HF` repo had been converted to HF with a buggy\
    \ version of the transformers `models/llama/convert_llama_weights_to_hf.py` script\
    \ which caused the 13B models to use 37GB on disk instead of 26GB.  So I re-converted\
    \ my `vicuna-13B-1.1-HF` repo, and then just in case that affected the GPTQ I\
    \ also re-made the GPTQs.\n\nNo idea if any of that would affect this, but that's\
    \ what I did! I suppose that might mean I used a later version of GPTQ-for-LLaMa\
    \ than you did?\n\nQuestion for you if you've got a sec: what code/method do you\
    \ use to produce those metrics?  So far the only evaluation I've done on models\
    \ has been for GGML models, using llama.cpp's perplexity binary. I've been meaning\
    \ to try evaluating GPU models but haven't looked into it yet."
  created_at: 2023-04-19 19:28:58+00:00
  edited: false
  hidden: false
  id: 64404f0a2113f7dfcb5701c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-04-19T20:40:13.000Z'
    data:
      edited: true
      editors:
      - Thireus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: '<p>Thank you for the detailed answer. I''ll look into this!</p>

          <p>I am using cuda117 instead of cuda118, but I doubt that could be it...
          I also use a more recent version of bitsandbytes==0.38.1.<br>All on WSL,
          but I''m thinking of giving it a try in Google Colab (I believe this is
          what you''re using).</p>

          <p>To generate the metrics, enter the directory where you have your safetensor
          and execute:<br><code>python /content/text-generation-webui/repositories/GPTQ-for-LLaMa/llama.py
          . c4 --wbits 4 --groupsize 128 --load vicuna-13B-1.1-GPTQ-4bit-128g.safetensors
          --new-eval --eval</code></p>

          <p>You can also try instead of <code>--new-eval --eval</code> to use <code>--eval</code>
          alone or <code>--benchmark 2048 --check</code>.</p>

          <p>I remember our conversation about the size difference. ;)</p>

          '
        raw: 'Thank you for the detailed answer. I''ll look into this!


          I am using cuda117 instead of cuda118, but I doubt that could be it... I
          also use a more recent version of bitsandbytes==0.38.1.

          All on WSL, but I''m thinking of giving it a try in Google Colab (I believe
          this is what you''re using).


          To generate the metrics, enter the directory where you have your safetensor
          and execute:

          `python /content/text-generation-webui/repositories/GPTQ-for-LLaMa/llama.py
          . c4 --wbits 4 --groupsize 128 --load vicuna-13B-1.1-GPTQ-4bit-128g.safetensors
          --new-eval --eval`


          You can also try instead of `--new-eval --eval` to use `--eval` alone or
          `--benchmark 2048 --check`.


          I remember our conversation about the size difference. ;)'
        updatedAt: '2023-04-19T20:44:15.223Z'
      numEdits: 2
      reactions: []
    id: 644051addbd88206a83f3a32
    type: comment
  author: Thireus
  content: 'Thank you for the detailed answer. I''ll look into this!


    I am using cuda117 instead of cuda118, but I doubt that could be it... I also
    use a more recent version of bitsandbytes==0.38.1.

    All on WSL, but I''m thinking of giving it a try in Google Colab (I believe this
    is what you''re using).


    To generate the metrics, enter the directory where you have your safetensor and
    execute:

    `python /content/text-generation-webui/repositories/GPTQ-for-LLaMa/llama.py .
    c4 --wbits 4 --groupsize 128 --load vicuna-13B-1.1-GPTQ-4bit-128g.safetensors
    --new-eval --eval`


    You can also try instead of `--new-eval --eval` to use `--eval` alone or `--benchmark
    2048 --check`.


    I remember our conversation about the size difference. ;)'
  created_at: 2023-04-19 19:40:13+00:00
  edited: true
  hidden: false
  id: 644051addbd88206a83f3a32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-19T20:58:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh that was you! Sorry. I''ve had so many discussions here since
          I started uploading models that I can''t remember all the different usernames
          :)</p>

          <p>Thanks for the details on the evaluation, I''ll have to try that.</p>

          <p>And yes I have used Google Colab a lot as I don''t have an NVidia GPU
          at home yet - my home GPU is an AMD 6900XT 16GB on macOS, and that''s not
          well supported at all for this sort of thing.  Also my internet upload is
          only 4MB/s which takes forever when dealing with larger models. Uploads
          from the cloud are way quicker. And if I need to reboot my PC or something,
          it won''t interrupt anything.</p>

          <p>Most of my GPTQs I did with Colab, but lately I''ve started to move over
          to Runpod. They have a lot more hardware options, and they support SSH and
          allow you to open any TCP ports if you want which Colab doesn''t officially
          support. On Google Colab there''s only two GPU options: T5 15GB or A100
          40GB.  I found that for most of what I was doing, T5 was too small and slow,
          and A100 was more than I needed.  On Runpod I can pay $0.29/hr for a 3090,
          $0.69/hr for a 4090, $0.89/hr for an A100 40GB or $1.89/hr for an A100 80GB.  And
          then when it''s done it''s usually way faster to upload to HF from a server
          than it is from my home internet.</p>

          '
        raw: 'Oh that was you! Sorry. I''ve had so many discussions here since I started
          uploading models that I can''t remember all the different usernames :)


          Thanks for the details on the evaluation, I''ll have to try that.


          And yes I have used Google Colab a lot as I don''t have an NVidia GPU at
          home yet - my home GPU is an AMD 6900XT 16GB on macOS, and that''s not well
          supported at all for this sort of thing.  Also my internet upload is only
          4MB/s which takes forever when dealing with larger models. Uploads from
          the cloud are way quicker. And if I need to reboot my PC or something, it
          won''t interrupt anything.


          Most of my GPTQs I did with Colab, but lately I''ve started to move over
          to Runpod. They have a lot more hardware options, and they support SSH and
          allow you to open any TCP ports if you want which Colab doesn''t officially
          support. On Google Colab there''s only two GPU options: T5 15GB or A100
          40GB.  I found that for most of what I was doing, T5 was too small and slow,
          and A100 was more than I needed.  On Runpod I can pay $0.29/hr for a 3090,
          $0.69/hr for a 4090, $0.89/hr for an A100 40GB or $1.89/hr for an A100 80GB.  And
          then when it''s done it''s usually way faster to upload to HF from a server
          than it is from my home internet.'
        updatedAt: '2023-04-19T20:58:21.914Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F917"
        users:
        - Thireus
        - wsuff
        - mikolodz
    id: 644055ed2113f7dfcb579255
    type: comment
  author: TheBloke
  content: 'Oh that was you! Sorry. I''ve had so many discussions here since I started
    uploading models that I can''t remember all the different usernames :)


    Thanks for the details on the evaluation, I''ll have to try that.


    And yes I have used Google Colab a lot as I don''t have an NVidia GPU at home
    yet - my home GPU is an AMD 6900XT 16GB on macOS, and that''s not well supported
    at all for this sort of thing.  Also my internet upload is only 4MB/s which takes
    forever when dealing with larger models. Uploads from the cloud are way quicker.
    And if I need to reboot my PC or something, it won''t interrupt anything.


    Most of my GPTQs I did with Colab, but lately I''ve started to move over to Runpod.
    They have a lot more hardware options, and they support SSH and allow you to open
    any TCP ports if you want which Colab doesn''t officially support. On Google Colab
    there''s only two GPU options: T5 15GB or A100 40GB.  I found that for most of
    what I was doing, T5 was too small and slow, and A100 was more than I needed.  On
    Runpod I can pay $0.29/hr for a 3090, $0.69/hr for a 4090, $0.89/hr for an A100
    40GB or $1.89/hr for an A100 80GB.  And then when it''s done it''s usually way
    faster to upload to HF from a server than it is from my home internet.'
  created_at: 2023-04-19 19:58:21+00:00
  edited: false
  hidden: false
  id: 644055ed2113f7dfcb579255
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Vicuna-13B-1.1-GPTQ
repo_type: model
status: open
target_branch: null
title: Your converted model performs better, and I don't understand why
