!!python/object:huggingface_hub.community.DiscussionWithDetails
author: blooscrn
conflicting_files: null
created_at: 2023-06-08 17:35:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c91fc45163476a7c86a0050035f90a74.svg
      fullname: Kuba
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: blooscrn
      type: user
    createdAt: '2023-06-08T18:35:03.000Z'
    data:
      edited: false
      editors:
      - blooscrn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.32563531398773193
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c91fc45163476a7c86a0050035f90a74.svg
          fullname: Kuba
          isHf: false
          isPro: false
          name: blooscrn
          type: user
        html: "<p>Traceback (most recent call last): File \u201CC:\\Users\\zolte\\\
          Downloads\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
          server.py\u201D, line 69, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201CC:\\Users\\zolte\\Downloads\\\
          oobabooga_windows\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 94, in load_model output = load_func(model_name) File \u201CC:\\\
          Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 296, in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
          \ File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D, line 53, in load_quantized\
          \ model = AutoGPTQForCausalLM.from_quantized(path_to_model, **params) File\
          \ \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\auto_gptq\\modeling\\auto.py\u201D\
          , line 83, in from_quantized return quant_func( File \u201CC:\\Users\\zolte\\\
          Downloads\\oobabooga_windows\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\auto_gptq\\modeling_base.py\u201D, line 749, in from_quantized\
          \ make_quant( File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
          modeling_utils.py\u201D, line 92, in make_quant make_quant( File \u201C\
          C:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D, line 92, in\
          \ make_quant make_quant( File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
          modeling_utils.py\u201D, line 92, in make_quant make_quant( [Previous line\
          \ repeated 1 more time] File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
          modeling_utils.py\u201D, line 84, in make_quant new_layer = QuantLinear(\
          \ File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\auto_gptq\\nn_modules\\qlinear\\\
          qlinear_cuda_old.py\u201D, line 83, in init self.autogptq_cuda = autogptq_cuda_256\
          \ NameError: name \u2018autogptq_cuda_256\u2019 is not defined</p>\n"
        raw: "Traceback (most recent call last): File \u201CC:\\Users\\zolte\\Downloads\\\
          oobabooga_windows\\oobabooga_windows\\text-generation-webui\\server.py\u201D\
          , line 69, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\u201D, line 94, in load_model\
          \ output = load_func(model_name) File \u201CC:\\Users\\zolte\\Downloads\\\
          oobabooga_windows\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 296, in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
          \ File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D, line 53, in load_quantized\
          \ model = AutoGPTQForCausalLM.from_quantized(path_to_model, **params) File\
          \ \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\auto_gptq\\modeling\\auto.py\u201D\
          , line 83, in from_quantized return quant_func( File \u201CC:\\Users\\zolte\\\
          Downloads\\oobabooga_windows\\oobabooga_windows\\installer_files\\env\\\
          lib\\site-packages\\auto_gptq\\modeling_base.py\u201D, line 749, in from_quantized\
          \ make_quant( File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
          modeling_utils.py\u201D, line 92, in make_quant make_quant( File \u201C\
          C:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D, line 92, in\
          \ make_quant make_quant( File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
          modeling_utils.py\u201D, line 92, in make_quant make_quant( [Previous line\
          \ repeated 1 more time] File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
          modeling_utils.py\u201D, line 84, in make_quant new_layer = QuantLinear(\
          \ File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\auto_gptq\\nn_modules\\qlinear\\\
          qlinear_cuda_old.py\u201D, line 83, in init self.autogptq_cuda = autogptq_cuda_256\
          \ NameError: name \u2018autogptq_cuda_256\u2019 is not defined"
        updatedAt: '2023-06-08T18:35:03.719Z'
      numEdits: 0
      reactions: []
    id: 64821f57cca6af5cd2f06219
    type: comment
  author: blooscrn
  content: "Traceback (most recent call last): File \u201CC:\\Users\\zolte\\Downloads\\\
    oobabooga_windows\\oobabooga_windows\\text-generation-webui\\server.py\u201D,\
    \ line 69, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
    \ File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\u201D, line 94, in load_model output\
    \ = load_func(model_name) File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\\
    oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line 296,\
    \ in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
    \ File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
    text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D, line 53, in load_quantized\
    \ model = AutoGPTQForCausalLM.from_quantized(path_to_model, **params) File \u201C\
    C:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 83, in from_quantized\
    \ return quant_func( File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\\
    oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\modeling_base.py\u201D\
    , line 749, in from_quantized make_quant( File \u201CC:\\Users\\zolte\\Downloads\\\
    oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    auto_gptq\\modeling_utils.py\u201D, line 92, in make_quant make_quant( File \u201C\
    C:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D, line 92, in make_quant\
    \ make_quant( File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D,\
    \ line 92, in make_quant make_quant( [Previous line repeated 1 more time] File\
    \ \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D, line 84, in make_quant\
    \ new_layer = QuantLinear( File \u201CC:\\Users\\zolte\\Downloads\\oobabooga_windows\\\
    oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\nn_modules\\\
    qlinear\\qlinear_cuda_old.py\u201D, line 83, in init self.autogptq_cuda = autogptq_cuda_256\
    \ NameError: name \u2018autogptq_cuda_256\u2019 is not defined"
  created_at: 2023-06-08 17:35:03+00:00
  edited: false
  hidden: false
  id: 64821f57cca6af5cd2f06219
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1643acd09fe8924ab3ce9244c2b89392.svg
      fullname: JOAN CASTRO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jocastroc
      type: user
    createdAt: '2023-06-11T03:23:20.000Z'
    data:
      edited: true
      editors:
      - jocastroc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8349567651748657
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1643acd09fe8924ab3ce9244c2b89392.svg
          fullname: JOAN CASTRO
          isHf: false
          isPro: false
          name: jocastroc
          type: user
        html: '<p>same here...im trying to run "TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ"</p>

          <p>edit:<br>Same here... I''m trying to run "TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ"</p>

          <p>edit:<br>I can load the model setting this</p>

          <pre><code class="language-python">model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,
          device=<span class="hljs-string">"cuda:0"</span>, use_triton=<span class="hljs-literal">True</span>,
          use_safetensors=<span class="hljs-literal">True</span>, torch_dtype=torch.float32,
          trust_remote_code=<span class="hljs-literal">True</span>)

          </code></pre>

          <p>so for me was<code>use_triton=True</code></p>

          '
        raw: 'same here...im trying to run "TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ"


          edit:

          Same here... I''m trying to run "TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ"


          edit:

          I can load the model setting this

          ```python

          model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0",
          use_triton=True, use_safetensors=True, torch_dtype=torch.float32, trust_remote_code=True)

          ```

          so for me was`use_triton=True`'
        updatedAt: '2023-06-11T03:32:58.355Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Drshafi
    id: 64853e28984dbfde8e682465
    type: comment
  author: jocastroc
  content: 'same here...im trying to run "TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ"


    edit:

    Same here... I''m trying to run "TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ"


    edit:

    I can load the model setting this

    ```python

    model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0",
    use_triton=True, use_safetensors=True, torch_dtype=torch.float32, trust_remote_code=True)

    ```

    so for me was`use_triton=True`'
  created_at: 2023-06-11 02:23:20+00:00
  edited: true
  hidden: false
  id: 64853e28984dbfde8e682465
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c34150f6c59565bcfe075729ad9605b.svg
      fullname: juret
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SuperAimodelsNow11
      type: user
    createdAt: '2023-07-23T14:24:56.000Z'
    data:
      edited: false
      editors:
      - SuperAimodelsNow11
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6945778131484985
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c34150f6c59565bcfe075729ad9605b.svg
          fullname: juret
          isHf: false
          isPro: false
          name: SuperAimodelsNow11
          type: user
        html: '<p> for  me  settging  use_triton=True    does  not solve it , any
          ideas?</p>

          '
        raw: ' for  me  settging  use_triton=True    does  not solve it , any ideas?'
        updatedAt: '2023-07-23T14:24:56.476Z'
      numEdits: 0
      reactions: []
    id: 64bd3838b1a618880988528d
    type: comment
  author: SuperAimodelsNow11
  content: ' for  me  settging  use_triton=True    does  not solve it , any ideas?'
  created_at: 2023-07-23 13:24:56+00:00
  edited: false
  hidden: false
  id: 64bd3838b1a618880988528d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba33ef37bc70a0261e100c596ef8e3f9.svg
      fullname: Alessandro Roncatti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roncatti
      type: user
    createdAt: '2023-07-23T22:45:15.000Z'
    data:
      edited: false
      editors:
      - roncatti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49389028549194336
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba33ef37bc70a0261e100c596ef8e3f9.svg
          fullname: Alessandro Roncatti
          isHf: false
          isPro: false
          name: roncatti
          type: user
        html: '<p>Hello!<br>I''m experiencing problem to load the model.<br>The command
          is executed but the load stops after a few seconds and the Killed message
          is retuned.<br>See bellow:<br>~/text-generation-webui$ python3 server.py
          --loader autogptq --gpu-memory 5000MiB --model TheBloke_vicuna-13B-1.1-GPTQ-4bit-128g<br>2023-07-23
          19:42:13 INFO:Loading TheBloke_vicuna-13B-1.1-GPTQ-4bit-128g...<br>2023-07-23
          19:42:13 INFO:The AutoGPTQ params are: {''model_basename'': ''vicuna-13B-1.1-GPTQ-4bit-128g.compat.no-act-order'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'':
          True, ''inject_fused_mlp'': True, ''use_safetensors'': False, ''trust_remote_code'':
          False, ''max_memory'': {0: ''5000MiB'', ''cpu'': ''99GiB''}, ''quantize_config'':
          None, ''use_cuda_fp16'': True}<br>Killed</p>

          <p>Has anyone else gone through this? Any suggestion?</p>

          <p>Thx!</p>

          '
        raw: "Hello!\nI'm experiencing problem to load the model. \nThe command is\
          \ executed but the load stops after a few seconds and the Killed message\
          \ is retuned.\nSee bellow:\n~/text-generation-webui$ python3 server.py --loader\
          \ autogptq --gpu-memory 5000MiB --model TheBloke_vicuna-13B-1.1-GPTQ-4bit-128g\n\
          2023-07-23 19:42:13 INFO:Loading TheBloke_vicuna-13B-1.1-GPTQ-4bit-128g...\n\
          2023-07-23 19:42:13 INFO:The AutoGPTQ params are: {'model_basename': 'vicuna-13B-1.1-GPTQ-4bit-128g.compat.no-act-order',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': False, 'trust_remote_code':\
          \ False, 'max_memory': {0: '5000MiB', 'cpu': '99GiB'}, 'quantize_config':\
          \ None, 'use_cuda_fp16': True}\nKilled\n\nHas anyone else gone through this?\
          \ Any suggestion?\n\nThx!"
        updatedAt: '2023-07-23T22:45:15.836Z'
      numEdits: 0
      reactions: []
    id: 64bdad7b38953777fe8e1853
    type: comment
  author: roncatti
  content: "Hello!\nI'm experiencing problem to load the model. \nThe command is executed\
    \ but the load stops after a few seconds and the Killed message is retuned.\n\
    See bellow:\n~/text-generation-webui$ python3 server.py --loader autogptq --gpu-memory\
    \ 5000MiB --model TheBloke_vicuna-13B-1.1-GPTQ-4bit-128g\n2023-07-23 19:42:13\
    \ INFO:Loading TheBloke_vicuna-13B-1.1-GPTQ-4bit-128g...\n2023-07-23 19:42:13\
    \ INFO:The AutoGPTQ params are: {'model_basename': 'vicuna-13B-1.1-GPTQ-4bit-128g.compat.no-act-order',\
    \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp':\
    \ True, 'use_safetensors': False, 'trust_remote_code': False, 'max_memory': {0:\
    \ '5000MiB', 'cpu': '99GiB'}, 'quantize_config': None, 'use_cuda_fp16': True}\n\
    Killed\n\nHas anyone else gone through this? Any suggestion?\n\nThx!"
  created_at: 2023-07-23 21:45:15+00:00
  edited: false
  hidden: false
  id: 64bdad7b38953777fe8e1853
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-24T09:21:43.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9443296194076538
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>"Killed" means you ran out of RAM.  GPTQ models need to load in
          to RAM first, and then they move to GPU.  So you need to run on a system
          with more available RAM, or add some swap space if you''re able to.</p>

          '
        raw: '"Killed" means you ran out of RAM.  GPTQ models need to load in to RAM
          first, and then they move to GPU.  So you need to run on a system with more
          available RAM, or add some swap space if you''re able to.'
        updatedAt: '2023-07-24T09:21:43.024Z'
      numEdits: 0
      reactions: []
    id: 64be42a7e90972519d99ed49
    type: comment
  author: TheBloke
  content: '"Killed" means you ran out of RAM.  GPTQ models need to load in to RAM
    first, and then they move to GPU.  So you need to run on a system with more available
    RAM, or add some swap space if you''re able to.'
  created_at: 2023-07-24 08:21:43+00:00
  edited: false
  hidden: false
  id: 64be42a7e90972519d99ed49
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba33ef37bc70a0261e100c596ef8e3f9.svg
      fullname: Alessandro Roncatti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roncatti
      type: user
    createdAt: '2023-07-24T14:49:42.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/ba33ef37bc70a0261e100c596ef8e3f9.svg
          fullname: Alessandro Roncatti
          isHf: false
          isPro: false
          name: roncatti
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-07-24T15:05:08.011Z'
      numEdits: 0
      reactions: []
    id: 64be8f86140491ca9f7a7f83
    type: comment
  author: roncatti
  content: This comment has been hidden
  created_at: 2023-07-24 13:49:42+00:00
  edited: true
  hidden: true
  id: 64be8f86140491ca9f7a7f83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba33ef37bc70a0261e100c596ef8e3f9.svg
      fullname: Alessandro Roncatti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roncatti
      type: user
    createdAt: '2023-07-28T21:12:15.000Z'
    data:
      edited: false
      editors:
      - roncatti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9671376347541809
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba33ef37bc70a0261e100c596ef8e3f9.svg
          fullname: Alessandro Roncatti
          isHf: false
          isPro: false
          name: roncatti
          type: user
        html: '<p>I''ve increased the SWAP file size that solved the RAM issue, now
          the restriction is my GPU GTX 1660 S that has only 6GB of DDR. I will try
          a smaller model. Thx!</p>

          '
        raw: I've increased the SWAP file size that solved the RAM issue, now the
          restriction is my GPU GTX 1660 S that has only 6GB of DDR. I will try a
          smaller model. Thx!
        updatedAt: '2023-07-28T21:12:15.570Z'
      numEdits: 0
      reactions: []
    id: 64c42f2f8f31d1e6c6734ebe
    type: comment
  author: roncatti
  content: I've increased the SWAP file size that solved the RAM issue, now the restriction
    is my GPU GTX 1660 S that has only 6GB of DDR. I will try a smaller model. Thx!
  created_at: 2023-07-28 20:12:15+00:00
  edited: false
  hidden: false
  id: 64c42f2f8f31d1e6c6734ebe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: TheBloke/Vicuna-13B-1.1-GPTQ
repo_type: model
status: open
target_branch: null
title: Error while trying to load this model
