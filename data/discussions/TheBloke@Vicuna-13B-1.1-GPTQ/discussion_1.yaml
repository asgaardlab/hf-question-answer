!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Fenfel
conflicting_files: null
created_at: 2023-04-13 00:34:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e3ce7099a032b1c952ea09/GBKxnHY-OkGXhzxEJK-AD.png?w=200&h=200&f=face
      fullname: Anton Meller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fenfel
      type: user
    createdAt: '2023-04-13T01:34:12.000Z'
    data:
      edited: false
      editors:
      - Fenfel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e3ce7099a032b1c952ea09/GBKxnHY-OkGXhzxEJK-AD.png?w=200&h=200&f=face
          fullname: Anton Meller
          isHf: false
          isPro: false
          name: Fenfel
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63e3ce7099a032b1c952ea09/LFcIoiv-ge-BkSydy-TJr.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63e3ce7099a032b1c952ea09/LFcIoiv-ge-BkSydy-TJr.png"></a><br>I
          just updated webui.</p>

          '
        raw: "\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63e3ce7099a032b1c952ea09/LFcIoiv-ge-BkSydy-TJr.png)\r\
          \nI just updated webui."
        updatedAt: '2023-04-13T01:34:12.066Z'
      numEdits: 0
      reactions: []
    id: 64375c1401429ecf25557ad1
    type: comment
  author: Fenfel
  content: "\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63e3ce7099a032b1c952ea09/LFcIoiv-ge-BkSydy-TJr.png)\r\
    \nI just updated webui."
  created_at: 2023-04-13 00:34:12+00:00
  edited: false
  hidden: false
  id: 64375c1401429ecf25557ad1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e3ce7099a032b1c952ea09/GBKxnHY-OkGXhzxEJK-AD.png?w=200&h=200&f=face
      fullname: Anton Meller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fenfel
      type: user
    createdAt: '2023-04-13T01:38:27.000Z'
    data:
      from: model is broken
      to: The model is broken
    id: 64375d13b1071789ec6e23b4
    type: title-change
  author: Fenfel
  created_at: 2023-04-13 00:38:27+00:00
  id: 64375d13b1071789ec6e23b4
  new_title: The model is broken
  old_title: model is broken
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e3ce7099a032b1c952ea09/GBKxnHY-OkGXhzxEJK-AD.png?w=200&h=200&f=face
      fullname: Anton Meller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fenfel
      type: user
    createdAt: '2023-04-13T02:53:53.000Z'
    data:
      edited: false
      editors:
      - Fenfel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e3ce7099a032b1c952ea09/GBKxnHY-OkGXhzxEJK-AD.png?w=200&h=200&f=face
          fullname: Anton Meller
          isHf: false
          isPro: false
          name: Fenfel
          type: user
        html: '<p>Okay, I fixed it.</p>

          '
        raw: Okay, I fixed it.
        updatedAt: '2023-04-13T02:53:53.418Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64376ec19e8d019de8377455
    id: 64376ec19e8d019de8377454
    type: comment
  author: Fenfel
  content: Okay, I fixed it.
  created_at: 2023-04-13 01:53:53+00:00
  edited: false
  hidden: false
  id: 64376ec19e8d019de8377454
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e3ce7099a032b1c952ea09/GBKxnHY-OkGXhzxEJK-AD.png?w=200&h=200&f=face
      fullname: Anton Meller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fenfel
      type: user
    createdAt: '2023-04-13T02:53:53.000Z'
    data:
      status: closed
    id: 64376ec19e8d019de8377455
    type: status-change
  author: Fenfel
  created_at: 2023-04-13 01:53:53+00:00
  id: 64376ec19e8d019de8377455
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9077421ef078294241a42f477217ffe3.svg
      fullname: Neil Milne
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MilneyAth
      type: user
    createdAt: '2023-04-13T18:49:44.000Z'
    data:
      edited: false
      editors:
      - MilneyAth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9077421ef078294241a42f477217ffe3.svg
          fullname: Neil Milne
          isHf: false
          isPro: false
          name: MilneyAth
          type: user
        html: '<p>Care to share what was wrong?</p>

          '
        raw: Care to share what was wrong?
        updatedAt: '2023-04-13T18:49:44.983Z'
      numEdits: 0
      reactions: []
    id: 64384ec81d7a5675bf516c60
    type: comment
  author: MilneyAth
  content: Care to share what was wrong?
  created_at: 2023-04-13 17:49:44+00:00
  edited: false
  hidden: false
  id: 64384ec81d7a5675bf516c60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-13T18:50:57.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>They probably hadn''t followed the details in the README regarding
          which GPTQ-for-LLaMa code to use.  If you use the <code>safetensors</code>
          file you must use the latest GPTQ-for-LLaMa code for inference.  If you
          don''t want to update GPTQ, you can use the <code>pt</code> file instead.</p>

          '
        raw: They probably hadn't followed the details in the README regarding which
          GPTQ-for-LLaMa code to use.  If you use the `safetensors` file you must
          use the latest GPTQ-for-LLaMa code for inference.  If you don't want to
          update GPTQ, you can use the `pt` file instead.
        updatedAt: '2023-04-13T18:50:57.718Z'
      numEdits: 0
      reactions: []
    id: 64384f1107583375d77ec460
    type: comment
  author: TheBloke
  content: They probably hadn't followed the details in the README regarding which
    GPTQ-for-LLaMa code to use.  If you use the `safetensors` file you must use the
    latest GPTQ-for-LLaMa code for inference.  If you don't want to update GPTQ, you
    can use the `pt` file instead.
  created_at: 2023-04-13 17:50:57+00:00
  edited: false
  hidden: false
  id: 64384f1107583375d77ec460
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e3ce7099a032b1c952ea09/GBKxnHY-OkGXhzxEJK-AD.png?w=200&h=200&f=face
      fullname: Anton Meller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fenfel
      type: user
    createdAt: '2023-04-13T19:45:37.000Z'
    data:
      edited: false
      editors:
      - Fenfel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63e3ce7099a032b1c952ea09/GBKxnHY-OkGXhzxEJK-AD.png?w=200&h=200&f=face
          fullname: Anton Meller
          isHf: false
          isPro: false
          name: Fenfel
          type: user
        html: '<blockquote>

          <p>They probably hadn''t followed the details in the README regarding which
          GPTQ-for-LLaMa code to use.  If you use the <code>safetensors</code> file
          you must use the latest GPTQ-for-LLaMa code for inference.  If you don''t
          want to update GPTQ, you can use the <code>pt</code> file instead.</p>

          </blockquote>

          <p>Yes it turned out that you need to completely remove GPTQ and compile
          it again</p>

          '
        raw: '> They probably hadn''t followed the details in the README regarding
          which GPTQ-for-LLaMa code to use.  If you use the `safetensors` file you
          must use the latest GPTQ-for-LLaMa code for inference.  If you don''t want
          to update GPTQ, you can use the `pt` file instead.


          Yes it turned out that you need to completely remove GPTQ and compile it
          again'
        updatedAt: '2023-04-13T19:45:37.418Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - TheBloke
        - Thireus
    id: 64385be1f79e7f4fd93bd478
    type: comment
  author: Fenfel
  content: '> They probably hadn''t followed the details in the README regarding which
    GPTQ-for-LLaMa code to use.  If you use the `safetensors` file you must use the
    latest GPTQ-for-LLaMa code for inference.  If you don''t want to update GPTQ,
    you can use the `pt` file instead.


    Yes it turned out that you need to completely remove GPTQ and compile it again'
  created_at: 2023-04-13 18:45:37+00:00
  edited: false
  hidden: false
  id: 64385be1f79e7f4fd93bd478
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/477717f056f808eb5e77e6e5c2d6be43.svg
      fullname: Fabio Angela
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MithrilMan
      type: user
    createdAt: '2023-04-15T17:33:10.000Z'
    data:
      edited: false
      editors:
      - MithrilMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/477717f056f808eb5e77e6e5c2d6be43.svg
          fullname: Fabio Angela
          isHf: false
          isPro: false
          name: MithrilMan
          type: user
        html: '<p>I''m using oobabooga using its dockerfile and I had to use .pt file
          to make it work</p>

          '
        raw: I'm using oobabooga using its dockerfile and I had to use .pt file to
          make it work
        updatedAt: '2023-04-15T17:33:10.129Z'
      numEdits: 0
      reactions: []
    id: 643adfd6f6ef50c310d804db
    type: comment
  author: MithrilMan
  content: I'm using oobabooga using its dockerfile and I had to use .pt file to make
    it work
  created_at: 2023-04-15 16:33:10+00:00
  edited: false
  hidden: false
  id: 643adfd6f6ef50c310d804db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/_A1RXNvyotvhd_3NsA7YB.jpeg?w=200&h=200&f=face
      fullname: Starlento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Starlento
      type: user
    createdAt: '2023-04-22T01:55:56.000Z'
    data:
      edited: false
      editors:
      - Starlento
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/_A1RXNvyotvhd_3NsA7YB.jpeg?w=200&h=200&f=face
          fullname: Starlento
          isHf: false
          isPro: false
          name: Starlento
          type: user
        html: '<p>For me, it is strange. Because <code>vicuna-13b-GPTQ-4bit-128g</code>
          works well which is safetensor as well, but the 1.1 does not.</p>

          <pre><code>git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b
          cuda

          cd GPTQ-for-LLaMa

          python setup_cuda.py install

          </code></pre>

          <p>And I just tried to delete the folder and execute the above commands,
          it still does not work. Maybe I use a wrong readme?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/5lnIoouHy-K33Bcza64Gy.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/5lnIoouHy-K33Bcza64Gy.png"></a></p>

          '
        raw: "For me, it is strange. Because `vicuna-13b-GPTQ-4bit-128g` works well\
          \ which is safetensor as well, but the 1.1 does not.\n``` \ngit clone https://github.com/oobabooga/GPTQ-for-LLaMa.git\
          \ -b cuda\ncd GPTQ-for-LLaMa\npython setup_cuda.py install\n```\nAnd I just\
          \ tried to delete the folder and execute the above commands, it still does\
          \ not work. Maybe I use a wrong readme?\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/5lnIoouHy-K33Bcza64Gy.png)"
        updatedAt: '2023-04-22T01:55:56.650Z'
      numEdits: 0
      reactions: []
    id: 64433eac856997843202b65e
    type: comment
  author: Starlento
  content: "For me, it is strange. Because `vicuna-13b-GPTQ-4bit-128g` works well\
    \ which is safetensor as well, but the 1.1 does not.\n``` \ngit clone https://github.com/oobabooga/GPTQ-for-LLaMa.git\
    \ -b cuda\ncd GPTQ-for-LLaMa\npython setup_cuda.py install\n```\nAnd I just tried\
    \ to delete the folder and execute the above commands, it still does not work.\
    \ Maybe I use a wrong readme?\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/5lnIoouHy-K33Bcza64Gy.png)"
  created_at: 2023-04-22 00:55:56+00:00
  edited: false
  hidden: false
  id: 64433eac856997843202b65e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/_A1RXNvyotvhd_3NsA7YB.jpeg?w=200&h=200&f=face
      fullname: Starlento
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Starlento
      type: user
    createdAt: '2023-04-22T01:58:29.000Z'
    data:
      edited: false
      editors:
      - Starlento
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/_A1RXNvyotvhd_3NsA7YB.jpeg?w=200&h=200&f=face
          fullname: Starlento
          isHf: false
          isPro: false
          name: Starlento
          type: user
        html: '<blockquote>

          <p>For me, it is strange. Because <code>vicuna-13b-GPTQ-4bit-128g</code>
          works well which is safetensor as well, but the 1.1 does not.</p>

          <pre><code>git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b
          cuda

          cd GPTQ-for-LLaMa

          python setup_cuda.py install

          </code></pre>

          <p>And I just tried to delete the folder and execute the above commands,
          it still does not work. Maybe I use a wrong readme?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/5lnIoouHy-K33Bcza64Gy.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/5lnIoouHy-K33Bcza64Gy.png"></a></p>

          </blockquote>

          <p>OK. Seems there is a guide in this model card''s readme...</p>

          '
        raw: "> For me, it is strange. Because `vicuna-13b-GPTQ-4bit-128g` works well\
          \ which is safetensor as well, but the 1.1 does not.\n> ``` \n> git clone\
          \ https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda\n> cd GPTQ-for-LLaMa\n\
          > python setup_cuda.py install\n> ```\n> And I just tried to delete the\
          \ folder and execute the above commands, it still does not work. Maybe I\
          \ use a wrong readme?\n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/5lnIoouHy-K33Bcza64Gy.png)\n\
          \nOK. Seems there is a guide in this model card's readme..."
        updatedAt: '2023-04-22T01:58:29.325Z'
      numEdits: 0
      reactions: []
    id: 64433f45856997843202c053
    type: comment
  author: Starlento
  content: "> For me, it is strange. Because `vicuna-13b-GPTQ-4bit-128g` works well\
    \ which is safetensor as well, but the 1.1 does not.\n> ``` \n> git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git\
    \ -b cuda\n> cd GPTQ-for-LLaMa\n> python setup_cuda.py install\n> ```\n> And I\
    \ just tried to delete the folder and execute the above commands, it still does\
    \ not work. Maybe I use a wrong readme?\n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/5lnIoouHy-K33Bcza64Gy.png)\n\
    \nOK. Seems there is a guide in this model card's readme..."
  created_at: 2023-04-22 00:58:29+00:00
  edited: false
  hidden: false
  id: 64433f45856997843202c053
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Vicuna-13B-1.1-GPTQ
repo_type: model
status: closed
target_branch: null
title: The model is broken
