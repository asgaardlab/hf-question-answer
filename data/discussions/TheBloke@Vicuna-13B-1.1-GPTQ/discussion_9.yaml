!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ukro
conflicting_files: null
created_at: 2023-04-29 07:24:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-29T08:24:21.000Z'
    data:
      edited: false
      editors:
      - Ukro
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
          fullname: Ukrovic
          isHf: false
          isPro: false
          name: Ukro
          type: user
        html: '<p>Hello  :)<br>Updated oobogabooga through install<br>And downloaded
          GPTQ-for-LLaMa-58c8ab4c7aaccc50f507fd08cce941976affe5e0<br>Copied to repositories<br>This
          is the error log:<br> File "h:\0_oobabooga\text-generation-webui\modules\text_generation.py",
          line 290, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "h:\0_oobabooga\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 1485, in generate<br>    return self.sample(<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 2524, in sample<br>    outputs = self(<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "h:\0_oobabooga\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 687, in forward<br>    outputs = self.model(<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "h:\0_oobabooga\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 577, in forward<br>    layer_outputs = decoder_layer(<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "h:\0_oobabooga\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 292, in forward<br>    hidden_states, self_attn_weights, present_key_value
          = self.self_attn(<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "h:\0_oobabooga\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 196, in forward<br>    query_states = self.q_proj(hidden_states).view(bsz,
          q_len, self.num_heads, self.head_dim).transpose(1, 2)<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "h:\0_oobabooga\text-generation-webui\repositories\GPTQ-for-LLaMa\quant.py",
          line 493, in forward<br>    out = QuantLinearFunction.apply(x.reshape(-1,x.shape[-1]),
          self.qweight, self.scales,<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\autograd\function.py",
          line 506, in apply<br>    return super().apply(*args, **kwargs)  # type:
          ignore[misc]<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\cuda\amp\autocast_mode.py",
          line 106, in decorate_fwd    return fwd(*args, **kwargs)<br>  File "h:\0_oobabooga\text-generation-webui\repositories\GPTQ-for-LLaMa\quant.py",
          line 407, in forward<br>    output = matmul248(input, qweight, scales, qzeros,
          g_idx, bits, maxq)<br>  File "h:\0_oobabooga\text-generation-webui\repositories\GPTQ-for-LLaMa\quant.py",
          line 380, in matmul248<br>    matmul_248_kernel[grid](input, qweight, output,<br>NameError:
          name ''matmul_248_kernel'' is not defined<br>Output generated in 0.38 seconds
          (0.00 tokens/s, 0 tokens, context 23, seed 1336889877)</p>

          '
        raw: "Hello  :)\r\nUpdated oobogabooga through install\r\nAnd downloaded GPTQ-for-LLaMa-58c8ab4c7aaccc50f507fd08cce941976affe5e0\r\
          \nCopied to repositories\r\nThis is the error log:\r\n File \"h:\\0_oobabooga\\\
          text-generation-webui\\modules\\text_generation.py\", line 290, in generate_with_callback\r\
          \n    shared.model.generate(**kwargs)\r\n  File \"h:\\0_oobabooga\\installer_files\\\
          env\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\
          \n    return func(*args, **kwargs)\r\n  File \"h:\\0_oobabooga\\installer_files\\\
          env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1485,\
          \ in generate\r\n    return self.sample(\r\n  File \"h:\\0_oobabooga\\installer_files\\\
          env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2524,\
          \ in sample\r\n    outputs = self(\r\n  File \"h:\\0_oobabooga\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"h:\\\
          0_oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\models\\\
          llama\\modeling_llama.py\", line 687, in forward\r\n    outputs = self.model(\r\
          \n  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 577, in forward\r\
          \n    layer_outputs = decoder_layer(\r\n  File \"h:\\0_oobabooga\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"h:\\\
          0_oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\models\\\
          llama\\modeling_llama.py\", line 292, in forward\r\n    hidden_states, self_attn_weights,\
          \ present_key_value = self.self_attn(\r\n  File \"h:\\0_oobabooga\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"h:\\\
          0_oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\models\\\
          llama\\modeling_llama.py\", line 196, in forward\r\n    query_states = self.q_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\r\n  File \"h:\\\
          0_oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"h:\\0_oobabooga\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\quant.py\", line 493, in forward\r\n    out = QuantLinearFunction.apply(x.reshape(-1,x.shape[-1]),\
          \ self.qweight, self.scales,\r\n  File \"h:\\0_oobabooga\\installer_files\\\
          env\\lib\\site-packages\\torch\\autograd\\function.py\", line 506, in apply\r\
          \n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File\
          \ \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\cuda\\\
          amp\\autocast_mode.py\", line 106, in decorate_fwd    return fwd(*args,\
          \ **kwargs)\r\n  File \"h:\\0_oobabooga\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\quant.py\", line 407, in forward\r\n    output = matmul248(input,\
          \ qweight, scales, qzeros, g_idx, bits, maxq)\r\n  File \"h:\\0_oobabooga\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\", line 380,\
          \ in matmul248\r\n    matmul_248_kernel[grid](input, qweight, output,\r\n\
          NameError: name 'matmul_248_kernel' is not defined\r\nOutput generated in\
          \ 0.38 seconds (0.00 tokens/s, 0 tokens, context 23, seed 1336889877)"
        updatedAt: '2023-04-29T08:24:21.287Z'
      numEdits: 0
      reactions: []
    id: 644cd4351e0cef198712ad08
    type: comment
  author: Ukro
  content: "Hello  :)\r\nUpdated oobogabooga through install\r\nAnd downloaded GPTQ-for-LLaMa-58c8ab4c7aaccc50f507fd08cce941976affe5e0\r\
    \nCopied to repositories\r\nThis is the error log:\r\n File \"h:\\0_oobabooga\\\
    text-generation-webui\\modules\\text_generation.py\", line 290, in generate_with_callback\r\
    \n    shared.model.generate(**kwargs)\r\n  File \"h:\\0_oobabooga\\installer_files\\\
    env\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\
    \n    return func(*args, **kwargs)\r\n  File \"h:\\0_oobabooga\\installer_files\\\
    env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1485, in generate\r\
    \n    return self.sample(\r\n  File \"h:\\0_oobabooga\\installer_files\\env\\\
    lib\\site-packages\\transformers\\generation\\utils.py\", line 2524, in sample\r\
    \n    outputs = self(\r\n  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n \
    \   return forward_call(*args, **kwargs)\r\n  File \"h:\\0_oobabooga\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line\
    \ 687, in forward\r\n    outputs = self.model(\r\n  File \"h:\\0_oobabooga\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"h:\\0_oobabooga\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line\
    \ 577, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"h:\\0_oobabooga\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"\
    h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\models\\\
    llama\\modeling_llama.py\", line 292, in forward\r\n    hidden_states, self_attn_weights,\
    \ present_key_value = self.self_attn(\r\n  File \"h:\\0_oobabooga\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"h:\\0_oobabooga\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line\
    \ 196, in forward\r\n    query_states = self.q_proj(hidden_states).view(bsz, q_len,\
    \ self.num_heads, self.head_dim).transpose(1, 2)\r\n  File \"h:\\0_oobabooga\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"\
    h:\\0_oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\"\
    , line 493, in forward\r\n    out = QuantLinearFunction.apply(x.reshape(-1,x.shape[-1]),\
    \ self.qweight, self.scales,\r\n  File \"h:\\0_oobabooga\\installer_files\\env\\\
    lib\\site-packages\\torch\\autograd\\function.py\", line 506, in apply\r\n   \
    \ return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"h:\\\
    0_oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py\"\
    , line 106, in decorate_fwd    return fwd(*args, **kwargs)\r\n  File \"h:\\0_oobabooga\\\
    text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\", line 407, in\
    \ forward\r\n    output = matmul248(input, qweight, scales, qzeros, g_idx, bits,\
    \ maxq)\r\n  File \"h:\\0_oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
    quant.py\", line 380, in matmul248\r\n    matmul_248_kernel[grid](input, qweight,\
    \ output,\r\nNameError: name 'matmul_248_kernel' is not defined\r\nOutput generated\
    \ in 0.38 seconds (0.00 tokens/s, 0 tokens, context 23, seed 1336889877)"
  created_at: 2023-04-29 07:24:21+00:00
  edited: false
  hidden: false
  id: 644cd4351e0cef198712ad08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-29T08:37:48.000Z'
    data:
      edited: false
      editors:
      - Ukro
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
          fullname: Ukrovic
          isHf: false
          isPro: false
          name: Ukro
          type: user
        html: '<p>Tryed the PT file, the output are realy good :) but loved to try
          the ST</p>

          '
        raw: Tryed the PT file, the output are realy good :) but loved to try the
          ST
        updatedAt: '2023-04-29T08:37:48.910Z'
      numEdits: 0
      reactions: []
    id: 644cd75cfa94e93b0ebb86dc
    type: comment
  author: Ukro
  content: Tryed the PT file, the output are realy good :) but loved to try the ST
  created_at: 2023-04-29 07:37:48+00:00
  edited: false
  hidden: false
  id: 644cd75cfa94e93b0ebb86dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T08:44:01.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry, the README instructions were out of date. We don''t need
          to use that commit any more. I have updated the README.</p>

          <p>Now if you are on Linux or WSL2 and can use Triton, you can use the latest
          commit of GPTQ-for-LlaMa:</p>

          <pre><code># Clone text-generation-webui, if you don''t already have it

          git clone https://github.com/oobabooga/text-generation-webui

          # Make a repositories directory

          mkdir text-generation-webui/repositories

          cd text-generation-webui/repositories

          # Clone the latest GPTQ-for-LLaMa code inside text-generation-webui

          git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa

          </code></pre>

          <p>Please try again with latest GPTQ-for-LLaMa commit and let me know!</p>

          <p>To update, try:</p>

          <pre><code>cd text-generation-webui/repositories

          rm -rf GPTQ-for-LLaMa

          git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa

          </code></pre>

          '
        raw: 'Sorry, the README instructions were out of date. We don''t need to use
          that commit any more. I have updated the README.


          Now if you are on Linux or WSL2 and can use Triton, you can use the latest
          commit of GPTQ-for-LlaMa:

          ```

          # Clone text-generation-webui, if you don''t already have it

          git clone https://github.com/oobabooga/text-generation-webui

          # Make a repositories directory

          mkdir text-generation-webui/repositories

          cd text-generation-webui/repositories

          # Clone the latest GPTQ-for-LLaMa code inside text-generation-webui

          git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa

          ```

          Please try again with latest GPTQ-for-LLaMa commit and let me know!


          To update, try:

          ```

          cd text-generation-webui/repositories

          rm -rf GPTQ-for-LLaMa

          git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa

          ```'
        updatedAt: '2023-04-29T08:47:25.009Z'
      numEdits: 5
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Ukro
    id: 644cd8d1328c1aa30e3b4491
    type: comment
  author: TheBloke
  content: 'Sorry, the README instructions were out of date. We don''t need to use
    that commit any more. I have updated the README.


    Now if you are on Linux or WSL2 and can use Triton, you can use the latest commit
    of GPTQ-for-LlaMa:

    ```

    # Clone text-generation-webui, if you don''t already have it

    git clone https://github.com/oobabooga/text-generation-webui

    # Make a repositories directory

    mkdir text-generation-webui/repositories

    cd text-generation-webui/repositories

    # Clone the latest GPTQ-for-LLaMa code inside text-generation-webui

    git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa

    ```

    Please try again with latest GPTQ-for-LLaMa commit and let me know!


    To update, try:

    ```

    cd text-generation-webui/repositories

    rm -rf GPTQ-for-LLaMa

    git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa

    ```'
  created_at: 2023-04-29 07:44:01+00:00
  edited: true
  hidden: false
  id: 644cd8d1328c1aa30e3b4491
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-29T08:51:33.000Z'
    data:
      edited: false
      editors:
      - Ukro
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
          fullname: Ukrovic
          isHf: false
          isPro: false
          name: Ukro
          type: user
        html: '<p>Wow thank you for fast response.<br>I use Windows unfortunately.<br>I
          have downloaded triton as per your linkg qwopqwop200<br>It gives this error
          on model load:<br>bin h:\0_oobabooga\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cuda117.dll<br>Loading
          TheBloke_vicuna-13B-1.1-GPTQ-4bit-128gst...<br>trioton not installed.<br>triton
          not installed.<br>Traceback (most recent call last):<br>  File "h:\0_oobabooga\text-generation-webui\server.py",
          line 914, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "h:\0_oobabooga\text-generation-webui\modules\models.py", line 156, in load_model<br>    from
          modules.GPTQ_loader import load_quantized<br>  File "h:\0_oobabooga\text-generation-webui\modules\GPTQ_loader.py",
          line 14, in <br>    import llama_inference_offload<br>  File "h:\0_oobabooga\text-generation-webui\repositories\GPTQ-for-LLaMa\llama_inference_offload.py",
          line 4, in <br>    from gptq import GPTQ<br>  File "h:\0_oobabooga\text-generation-webui\repositories\GPTQ-for-LLaMa\gptq.py",
          line 8, in <br>    from texttable import Texttable<br>ModuleNotFoundError:
          No module named ''texttable''<br>Press any key to continue . . .</p>

          <hr>

          <p>And when i take cude branch it gives this error when i promted "hello":<br>  File
          "h:\0_oobabooga\text-generation-webui\modules\callbacks.py", line 66, in
          gentask<br>    ret = self.mfunc(callback=_callback, **self.kwargs)<br>  File
          "h:\0_oobabooga\text-generation-webui\modules\text_generation.py", line
          290, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "h:\0_oobabooga\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 1485, in generate<br>    return self.sample(<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 2524, in sample<br>    outputs = self(<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "h:\0_oobabooga\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 687, in forward<br>    outputs = self.model(<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "h:\0_oobabooga\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 577, in forward<br>    layer_outputs = decoder_layer(<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "h:\0_oobabooga\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 292, in forward<br>    hidden_states, self_attn_weights, present_key_value
          = self.self_attn(<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "h:\0_oobabooga\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 196, in forward<br>    query_states = self.q_proj(hidden_states).view(bsz,
          q_len, self.num_heads, self.head_dim).transpose(1, 2)<br>  File "h:\0_oobabooga\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "h:\0_oobabooga\text-generation-webui\repositories\GPTQ-for-LLaMa\quant.py",
          line 279, in forward<br>    quant_cuda.vecquant4matmul(x.float(), self.qweight,
          out, self.scales.float(), self.qzeros, self.g_idx)<br>TypeError: vecquant4matmul():
          incompatible function arguments. The following argument types are supported:<br>    1.
          (arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor,
          arg4: torch.Tensor, arg5: int) -&gt; None</p>

          <p>Invoked with: tensor([[ 0.0097, -0.0423,  0.2747,  ..., -0.0144,  0.0021,  0.0083],<br>        [
          0.0172,  0.0039, -0.0247,  ..., -0.0062, -0.0020, -0.0056],<br>        [
          0.0144,  0.0142, -0.0514,  ...,  0.0037,  0.0072,  0.0195],<br>        ...,<br>        [
          0.0117, -0.0166,  0.0213,  ...,  0.0200,  0.0124,  0.0093],<br>        [
          0.0219, -0.0053,  0.0230,  ..., -0.0189,  0.0629,  0.0051],<br>        [-0.0053,
          -0.0219, -0.0596,  ...,  0.0373, -0.0200,  0.0070]],<br>       device=''cuda:0''),
          tensor([[ 1818725976,   138849433, -1535478587,  ..., -1789286762,<br>          2075669608,  1987818610],<br>        [
          2053732233, -1672951194, -2035853562,  ..., -1133934939,<br>           901286473,
          -1369270681],<br>        [-1234789703,  1448792681, -1977252477,  ...,  -110598569,<br>           564566347,
          -1382511956],<br>        ...,<br>        [ 1461565612,   696546725, -1785359048,  ...,
          -1767143063,<br>           875526984,  -375875459],<br>        [-1297377942,
          -1419229274,  1521908069,  ..., -1665447735,<br>         -2127055527, -1432790902],<br>        [  141912649,
          -1888199995, -1181763453,  ...,  1097831093,<br>          2058911093, -1488278902]],
          device=''cuda:0'', dtype=torch.int32), tensor([[0., 0., 0.,  ..., 0., 0.,
          0.],<br>        [0., 0., 0.,  ..., 0., 0., 0.],<br>        [0., 0., 0.,  ...,
          0., 0., 0.],<br>        ...,<br>        [0., 0., 0.,  ..., 0., 0., 0.],<br>        [0.,
          0., 0.,  ..., 0., 0., 0.],<br>        [0., 0., 0.,  ..., 0., 0., 0.]], device=''cuda:0''),
          tensor([[0.0319, 0.0154, 0.0293,  ..., 0.0321, 0.0178, 0.0309],<br>        [0.0106,
          0.0081, 0.0083,  ..., 0.0220, 0.0143, 0.0172],<br>        [0.0074, 0.0074,
          0.0071,  ..., 0.0160, 0.0173, 0.0110],<br>        ...,<br>        [0.0053,
          0.0058, 0.0036,  ..., 0.0119, 0.0083, 0.0072],<br>        [0.0055, 0.0047,
          0.0044,  ..., 0.0125, 0.0083, 0.0078],<br>        [0.0043, 0.0049, 0.0042,  ...,
          0.0097, 0.0085, 0.0066]],<br>       device=''cuda:0''), tensor([[ 1719170664,  1484158581,  2004248422,  ...,  1720083575,<br>          1987601783,  1986492247],<br>        [-2006485145,  2004309623,  1987467092,  ...,  1182168967,<br>          1466398328,  1466402407],<br>        [
          1717921624,  1987475030,  1987475302,  ..., -2022156713,<br>          1466328694,  2003003511],<br>        ...,<br>        [
          1734834023,  2020051061,  1985443159,  ...,  2002090086,<br>          1182103143,  2003203703],<br>        [
          1449682551,  1751611254,  2004182903,  ...,  1970759015,<br>          1716947079,  1718974310],<br>        [
          1720145766, -2040043401,  2021950838,  ...,  1988523894,<br>          2004248166,  1733785702]],
          device=''cuda:0'', dtype=torch.int32), tensor([38,  6,  1,  ..., 23,  9,
          17], device=''cuda:0'', dtype=torch.int32)</p>

          '
        raw: "Wow thank you for fast response.\nI use Windows unfortunately.\nI have\
          \ downloaded triton as per your linkg qwopqwop200\nIt gives this error on\
          \ model load:\nbin h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\\
          bitsandbytes\\libbitsandbytes_cuda117.dll\nLoading TheBloke_vicuna-13B-1.1-GPTQ-4bit-128gst...\n\
          trioton not installed.\ntriton not installed.\nTraceback (most recent call\
          \ last):\n  File \"h:\\0_oobabooga\\text-generation-webui\\server.py\",\
          \ line 914, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"h:\\0_oobabooga\\text-generation-webui\\modules\\models.py\",\
          \ line 156, in load_model\n    from modules.GPTQ_loader import load_quantized\n\
          \  File \"h:\\0_oobabooga\\text-generation-webui\\modules\\GPTQ_loader.py\"\
          , line 14, in <module>\n    import llama_inference_offload\n  File \"h:\\\
          0_oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\llama_inference_offload.py\"\
          , line 4, in <module>\n    from gptq import GPTQ\n  File \"h:\\0_oobabooga\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa\\gptq.py\", line 8,\
          \ in <module>\n    from texttable import Texttable\nModuleNotFoundError:\
          \ No module named 'texttable'\nPress any key to continue . . .\n---\nAnd\
          \ when i take cude branch it gives this error when i promted \"hello\":\n\
          \  File \"h:\\0_oobabooga\\text-generation-webui\\modules\\callbacks.py\"\
          , line 66, in gentask\n    ret = self.mfunc(callback=_callback, **self.kwargs)\n\
          \  File \"h:\\0_oobabooga\\text-generation-webui\\modules\\text_generation.py\"\
          , line 290, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\\
          utils\\_contextlib.py\", line 115, in decorate_context\n    return func(*args,\
          \ **kwargs)\n  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\\
          transformers\\generation\\utils.py\", line 1485, in generate\n    return\
          \ self.sample(\n  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\\
          transformers\\generation\\utils.py\", line 2524, in sample\n    outputs\
          \ = self(\n  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 687, in forward\n\
          \    outputs = self.model(\n  File \"h:\\0_oobabooga\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl\n    return forward_call(*args, **kwargs)\n  File \"h:\\0_oobabooga\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 577, in forward\n    layer_outputs = decoder_layer(\n\
          \  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 292, in forward\n\
          \    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\
          \  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 196, in forward\n\
          \    query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads,\
          \ self.head_dim).transpose(1, 2)\n  File \"h:\\0_oobabooga\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl\n    return forward_call(*args, **kwargs)\n  File \"h:\\0_oobabooga\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\", line 279,\
          \ in forward\n    quant_cuda.vecquant4matmul(x.float(), self.qweight, out,\
          \ self.scales.float(), self.qzeros, self.g_idx)\nTypeError: vecquant4matmul():\
          \ incompatible function arguments. The following argument types are supported:\n\
          \    1. (arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3:\
          \ torch.Tensor, arg4: torch.Tensor, arg5: int) -> None\n\nInvoked with:\
          \ tensor([[ 0.0097, -0.0423,  0.2747,  ..., -0.0144,  0.0021,  0.0083],\n\
          \        [ 0.0172,  0.0039, -0.0247,  ..., -0.0062, -0.0020, -0.0056],\n\
          \        [ 0.0144,  0.0142, -0.0514,  ...,  0.0037,  0.0072,  0.0195],\n\
          \        ...,\n        [ 0.0117, -0.0166,  0.0213,  ...,  0.0200,  0.0124,\
          \  0.0093],\n        [ 0.0219, -0.0053,  0.0230,  ..., -0.0189,  0.0629,\
          \  0.0051],\n        [-0.0053, -0.0219, -0.0596,  ...,  0.0373, -0.0200,\
          \  0.0070]],\n       device='cuda:0'), tensor([[ 1818725976,   138849433,\
          \ -1535478587,  ..., -1789286762,\n          2075669608,  1987818610],\n\
          \        [ 2053732233, -1672951194, -2035853562,  ..., -1133934939,\n  \
          \         901286473, -1369270681],\n        [-1234789703,  1448792681, -1977252477,\
          \  ...,  -110598569,\n           564566347, -1382511956],\n        ...,\n\
          \        [ 1461565612,   696546725, -1785359048,  ..., -1767143063,\n  \
          \         875526984,  -375875459],\n        [-1297377942, -1419229274, \
          \ 1521908069,  ..., -1665447735,\n         -2127055527, -1432790902],\n\
          \        [  141912649, -1888199995, -1181763453,  ...,  1097831093,\n  \
          \        2058911093, -1488278902]], device='cuda:0', dtype=torch.int32),\
          \ tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0.,\
          \ 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n    \
          \    [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0.,\
          \ 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), tensor([[0.0319,\
          \ 0.0154, 0.0293,  ..., 0.0321, 0.0178, 0.0309],\n        [0.0106, 0.0081,\
          \ 0.0083,  ..., 0.0220, 0.0143, 0.0172],\n        [0.0074, 0.0074, 0.0071,\
          \  ..., 0.0160, 0.0173, 0.0110],\n        ...,\n        [0.0053, 0.0058,\
          \ 0.0036,  ..., 0.0119, 0.0083, 0.0072],\n        [0.0055, 0.0047, 0.0044,\
          \  ..., 0.0125, 0.0083, 0.0078],\n        [0.0043, 0.0049, 0.0042,  ...,\
          \ 0.0097, 0.0085, 0.0066]],\n       device='cuda:0'), tensor([[ 1719170664,\
          \  1484158581,  2004248422,  ...,  1720083575,\n          1987601783,  1986492247],\n\
          \        [-2006485145,  2004309623,  1987467092,  ...,  1182168967,\n  \
          \        1466398328,  1466402407],\n        [ 1717921624,  1987475030, \
          \ 1987475302,  ..., -2022156713,\n          1466328694,  2003003511],\n\
          \        ...,\n        [ 1734834023,  2020051061,  1985443159,  ...,  2002090086,\n\
          \          1182103143,  2003203703],\n        [ 1449682551,  1751611254,\
          \  2004182903,  ...,  1970759015,\n          1716947079,  1718974310],\n\
          \        [ 1720145766, -2040043401,  2021950838,  ...,  1988523894,\n  \
          \        2004248166,  1733785702]], device='cuda:0', dtype=torch.int32),\
          \ tensor([38,  6,  1,  ..., 23,  9, 17], device='cuda:0', dtype=torch.int32)"
        updatedAt: '2023-04-29T08:51:33.878Z'
      numEdits: 0
      reactions: []
    id: 644cda9597a3b0904a49b003
    type: comment
  author: Ukro
  content: "Wow thank you for fast response.\nI use Windows unfortunately.\nI have\
    \ downloaded triton as per your linkg qwopqwop200\nIt gives this error on model\
    \ load:\nbin h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\bitsandbytes\\\
    libbitsandbytes_cuda117.dll\nLoading TheBloke_vicuna-13B-1.1-GPTQ-4bit-128gst...\n\
    trioton not installed.\ntriton not installed.\nTraceback (most recent call last):\n\
    \  File \"h:\\0_oobabooga\\text-generation-webui\\server.py\", line 914, in <module>\n\
    \    shared.model, shared.tokenizer = load_model(shared.model_name)\n  File \"\
    h:\\0_oobabooga\\text-generation-webui\\modules\\models.py\", line 156, in load_model\n\
    \    from modules.GPTQ_loader import load_quantized\n  File \"h:\\0_oobabooga\\\
    text-generation-webui\\modules\\GPTQ_loader.py\", line 14, in <module>\n    import\
    \ llama_inference_offload\n  File \"h:\\0_oobabooga\\text-generation-webui\\repositories\\\
    GPTQ-for-LLaMa\\llama_inference_offload.py\", line 4, in <module>\n    from gptq\
    \ import GPTQ\n  File \"h:\\0_oobabooga\\text-generation-webui\\repositories\\\
    GPTQ-for-LLaMa\\gptq.py\", line 8, in <module>\n    from texttable import Texttable\n\
    ModuleNotFoundError: No module named 'texttable'\nPress any key to continue .\
    \ . .\n---\nAnd when i take cude branch it gives this error when i promted \"\
    hello\":\n  File \"h:\\0_oobabooga\\text-generation-webui\\modules\\callbacks.py\"\
    , line 66, in gentask\n    ret = self.mfunc(callback=_callback, **self.kwargs)\n\
    \  File \"h:\\0_oobabooga\\text-generation-webui\\modules\\text_generation.py\"\
    , line 290, in generate_with_callback\n    shared.model.generate(**kwargs)\n \
    \ File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\utils\\\
    _contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n\
    \  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\\
    generation\\utils.py\", line 1485, in generate\n    return self.sample(\n  File\
    \ \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\\
    generation\\utils.py\", line 2524, in sample\n    outputs = self(\n  File \"h:\\\
    0_oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\models\\\
    llama\\modeling_llama.py\", line 687, in forward\n    outputs = self.model(\n\
    \  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\nn\\\
    modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 577, in forward\n    layer_outputs\
    \ = decoder_layer(\n  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"h:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 292, in forward\n    hidden_states,\
    \ self_attn_weights, present_key_value = self.self_attn(\n  File \"h:\\0_oobabooga\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"h:\\\
    0_oobabooga\\installer_files\\env\\lib\\site-packages\\transformers\\models\\\
    llama\\modeling_llama.py\", line 196, in forward\n    query_states = self.q_proj(hidden_states).view(bsz,\
    \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\n  File \"h:\\0_oobabooga\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"h:\\\
    0_oobabooga\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\quant.py\",\
    \ line 279, in forward\n    quant_cuda.vecquant4matmul(x.float(), self.qweight,\
    \ out, self.scales.float(), self.qzeros, self.g_idx)\nTypeError: vecquant4matmul():\
    \ incompatible function arguments. The following argument types are supported:\n\
    \    1. (arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor,\
    \ arg4: torch.Tensor, arg5: int) -> None\n\nInvoked with: tensor([[ 0.0097, -0.0423,\
    \  0.2747,  ..., -0.0144,  0.0021,  0.0083],\n        [ 0.0172,  0.0039, -0.0247,\
    \  ..., -0.0062, -0.0020, -0.0056],\n        [ 0.0144,  0.0142, -0.0514,  ...,\
    \  0.0037,  0.0072,  0.0195],\n        ...,\n        [ 0.0117, -0.0166,  0.0213,\
    \  ...,  0.0200,  0.0124,  0.0093],\n        [ 0.0219, -0.0053,  0.0230,  ...,\
    \ -0.0189,  0.0629,  0.0051],\n        [-0.0053, -0.0219, -0.0596,  ...,  0.0373,\
    \ -0.0200,  0.0070]],\n       device='cuda:0'), tensor([[ 1818725976,   138849433,\
    \ -1535478587,  ..., -1789286762,\n          2075669608,  1987818610],\n     \
    \   [ 2053732233, -1672951194, -2035853562,  ..., -1133934939,\n           901286473,\
    \ -1369270681],\n        [-1234789703,  1448792681, -1977252477,  ...,  -110598569,\n\
    \           564566347, -1382511956],\n        ...,\n        [ 1461565612,   696546725,\
    \ -1785359048,  ..., -1767143063,\n           875526984,  -375875459],\n     \
    \   [-1297377942, -1419229274,  1521908069,  ..., -1665447735,\n         -2127055527,\
    \ -1432790902],\n        [  141912649, -1888199995, -1181763453,  ...,  1097831093,\n\
    \          2058911093, -1488278902]], device='cuda:0', dtype=torch.int32), tensor([[0.,\
    \ 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n      \
    \  [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0.,\
    \ 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ...,\
    \ 0., 0., 0.]], device='cuda:0'), tensor([[0.0319, 0.0154, 0.0293,  ..., 0.0321,\
    \ 0.0178, 0.0309],\n        [0.0106, 0.0081, 0.0083,  ..., 0.0220, 0.0143, 0.0172],\n\
    \        [0.0074, 0.0074, 0.0071,  ..., 0.0160, 0.0173, 0.0110],\n        ...,\n\
    \        [0.0053, 0.0058, 0.0036,  ..., 0.0119, 0.0083, 0.0072],\n        [0.0055,\
    \ 0.0047, 0.0044,  ..., 0.0125, 0.0083, 0.0078],\n        [0.0043, 0.0049, 0.0042,\
    \  ..., 0.0097, 0.0085, 0.0066]],\n       device='cuda:0'), tensor([[ 1719170664,\
    \  1484158581,  2004248422,  ...,  1720083575,\n          1987601783,  1986492247],\n\
    \        [-2006485145,  2004309623,  1987467092,  ...,  1182168967,\n        \
    \  1466398328,  1466402407],\n        [ 1717921624,  1987475030,  1987475302,\
    \  ..., -2022156713,\n          1466328694,  2003003511],\n        ...,\n    \
    \    [ 1734834023,  2020051061,  1985443159,  ...,  2002090086,\n          1182103143,\
    \  2003203703],\n        [ 1449682551,  1751611254,  2004182903,  ...,  1970759015,\n\
    \          1716947079,  1718974310],\n        [ 1720145766, -2040043401,  2021950838,\
    \  ...,  1988523894,\n          2004248166,  1733785702]], device='cuda:0', dtype=torch.int32),\
    \ tensor([38,  6,  1,  ..., 23,  9, 17], device='cuda:0', dtype=torch.int32)"
  created_at: 2023-04-29 07:51:33+00:00
  edited: false
  hidden: false
  id: 644cda9597a3b0904a49b003
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-29T08:52:52.000Z'
    data:
      edited: false
      editors:
      - Ukro
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
          fullname: Ukrovic
          isHf: false
          isPro: false
          name: Ukro
          type: user
        html: '<p>To be clear, i am just downloading the zip file and putting to repositories
          folder that''s it. I guess that is fine right? Don''t need to change something
          in env or somewhere. Ofcource i am changing the folder name from GPTQ-for-LLaMa-cuda
          to GPTQ-for-LLaMa for example</p>

          '
        raw: To be clear, i am just downloading the zip file and putting to repositories
          folder that's it. I guess that is fine right? Don't need to change something
          in env or somewhere. Ofcource i am changing the folder name from GPTQ-for-LLaMa-cuda
          to GPTQ-for-LLaMa for example
        updatedAt: '2023-04-29T08:52:52.469Z'
      numEdits: 0
      reactions: []
    id: 644cdae4328c1aa30e3b7bc0
    type: comment
  author: Ukro
  content: To be clear, i am just downloading the zip file and putting to repositories
    folder that's it. I guess that is fine right? Don't need to change something in
    env or somewhere. Ofcource i am changing the folder name from GPTQ-for-LLaMa-cuda
    to GPTQ-for-LLaMa for example
  created_at: 2023-04-29 07:52:52+00:00
  edited: false
  hidden: false
  id: 644cdae4328c1aa30e3b7bc0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T08:54:10.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ok yeah, no Triton on Windows unless you use WSL2.</p>

          <p>So you must do: <code>git clone -b cuda https://github.com/qwopqwop200/GPTQ-for-LLaMa</code></p>

          <p>That error you got with the CUDA version means that you need to re-compile
          the CUDA package for GPTQ-for-LLaMa</p>

          <p>Please try this:</p>

          <pre><code>pip uninstall quant-cuda

          cd text-generation-webui/repositories/GPTQ-for-LLaMa

          python setup_cuda.py install

          </code></pre>

          <p>And let me know</p>

          '
        raw: 'Ok yeah, no Triton on Windows unless you use WSL2.


          So you must do: `git clone -b cuda https://github.com/qwopqwop200/GPTQ-for-LLaMa`


          That error you got with the CUDA version means that you need to re-compile
          the CUDA package for GPTQ-for-LLaMa


          Please try this:


          ```

          pip uninstall quant-cuda

          cd text-generation-webui/repositories/GPTQ-for-LLaMa

          python setup_cuda.py install

          ```


          And let me know'
        updatedAt: '2023-04-29T08:54:21.943Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Ukro
    id: 644cdb32328c1aa30e3b83c2
    type: comment
  author: TheBloke
  content: 'Ok yeah, no Triton on Windows unless you use WSL2.


    So you must do: `git clone -b cuda https://github.com/qwopqwop200/GPTQ-for-LLaMa`


    That error you got with the CUDA version means that you need to re-compile the
    CUDA package for GPTQ-for-LLaMa


    Please try this:


    ```

    pip uninstall quant-cuda

    cd text-generation-webui/repositories/GPTQ-for-LLaMa

    python setup_cuda.py install

    ```


    And let me know'
  created_at: 2023-04-29 07:54:10+00:00
  edited: true
  hidden: false
  id: 644cdb32328c1aa30e3b83c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T08:54:35.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>PS. If you installed WSL2 then you could use Triton, and all of
          this could be a lot easier! :)</p>

          '
        raw: PS. If you installed WSL2 then you could use Triton, and all of this
          could be a lot easier! :)
        updatedAt: '2023-04-29T08:54:35.436Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Ukro
    id: 644cdb4b0dc952d2459568ac
    type: comment
  author: TheBloke
  content: PS. If you installed WSL2 then you could use Triton, and all of this could
    be a lot easier! :)
  created_at: 2023-04-29 07:54:35+00:00
  edited: false
  hidden: false
  id: 644cdb4b0dc952d2459568ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T08:59:48.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Actually maybe you shouldn''t run those commands. You will need
          a C/C++ compiler and CUDA toolkit installed - do you have those?</p>

          <p>If you don''t, you will need to stick with the compat.no-act-order file</p>

          '
        raw: 'Actually maybe you shouldn''t run those commands. You will need a C/C++
          compiler and CUDA toolkit installed - do you have those?


          If you don''t, you will need to stick with the compat.no-act-order file'
        updatedAt: '2023-04-29T08:59:48.870Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Ukro
    id: 644cdc840dc952d245958969
    type: comment
  author: TheBloke
  content: 'Actually maybe you shouldn''t run those commands. You will need a C/C++
    compiler and CUDA toolkit installed - do you have those?


    If you don''t, you will need to stick with the compat.no-act-order file'
  created_at: 2023-04-29 07:59:48+00:00
  edited: false
  hidden: false
  id: 644cdc840dc952d245958969
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-29T09:05:45.000Z'
    data:
      edited: false
      editors:
      - Ukro
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
          fullname: Ukrovic
          isHf: false
          isPro: false
          name: Ukro
          type: user
        html: '<p>Yeah<br>h:\0_oobabooga\installer_files\env\lib\site-packages\torch\utils\cpp_extension.py:359:
          UserWarning: Error checking compiler version for cl: [WinError 2] The system
          cannot find the file specified<br>  warnings.warn(f''Error checking compiler
          version for {compiler}: {error}'')<br>building ''quant_cuda'' extension<br>error:
          Microsoft Visual C++ 14.0 or greater is required. Get it with "Microsoft
          C++ Build Tools": <a rel="nofollow" href="https://visualstudio.microsoft.com/visual-cpp-build-tools/">https://visualstudio.microsoft.com/visual-cpp-build-tools/</a><br>--<br>Will
          check what is wsl2 ^^</p>

          '
        raw: "Yeah\nh:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\\
          utils\\cpp_extension.py:359: UserWarning: Error checking compiler version\
          \ for cl: [WinError 2] The system cannot find the file specified\n  warnings.warn(f'Error\
          \ checking compiler version for {compiler}: {error}')\nbuilding 'quant_cuda'\
          \ extension\nerror: Microsoft Visual C++ 14.0 or greater is required. Get\
          \ it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n\
          --\nWill check what is wsl2 ^^"
        updatedAt: '2023-04-29T09:05:45.235Z'
      numEdits: 0
      reactions: []
    id: 644cdde997a3b0904a49fdd8
    type: comment
  author: Ukro
  content: "Yeah\nh:\\0_oobabooga\\installer_files\\env\\lib\\site-packages\\torch\\\
    utils\\cpp_extension.py:359: UserWarning: Error checking compiler version for\
    \ cl: [WinError 2] The system cannot find the file specified\n  warnings.warn(f'Error\
    \ checking compiler version for {compiler}: {error}')\nbuilding 'quant_cuda' extension\n\
    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft\
    \ C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n\
    --\nWill check what is wsl2 ^^"
  created_at: 2023-04-29 08:05:45+00:00
  edited: false
  hidden: false
  id: 644cdde997a3b0904a49fdd8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-29T09:06:59.000Z'
    data:
      edited: false
      editors:
      - Ukro
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
          fullname: Ukrovic
          isHf: false
          isPro: false
          name: Ukro
          type: user
        html: '<p>Lol linux subs in windows, going for that right away xD<br>Need
          to do some house stuff, be back when have some response &lt;3 thank you!</p>

          '
        raw: 'Lol linux subs in windows, going for that right away xD

          Need to do some house stuff, be back when have some response <3 thank you!'
        updatedAt: '2023-04-29T09:06:59.229Z'
      numEdits: 0
      reactions: []
    id: 644cde330dc952d24595b7d1
    type: comment
  author: Ukro
  content: 'Lol linux subs in windows, going for that right away xD

    Need to do some house stuff, be back when have some response <3 thank you!'
  created_at: 2023-04-29 08:06:59+00:00
  edited: false
  hidden: false
  id: 644cde330dc952d24595b7d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T09:07:40.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK yeah, please go back to using the compat.no-act-order file. You
          can''t use the act-order file without being able to compile the latest CUDA
          code.</p>

          <p>WSL2 is Linux on Windows. You can install it from Microsoft and with
          a bit of work it can support your CUDA GPU</p>

          <p><a rel="nofollow" href="https://learn.microsoft.com/en-us/windows/wsl/install">https://learn.microsoft.com/en-us/windows/wsl/install</a></p>

          <p><a rel="nofollow" href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html">https://docs.nvidia.com/cuda/wsl-user-guide/index.html</a></p>

          '
        raw: 'OK yeah, please go back to using the compat.no-act-order file. You can''t
          use the act-order file without being able to compile the latest CUDA code.


          WSL2 is Linux on Windows. You can install it from Microsoft and with a bit
          of work it can support your CUDA GPU


          https://learn.microsoft.com/en-us/windows/wsl/install


          https://docs.nvidia.com/cuda/wsl-user-guide/index.html'
        updatedAt: '2023-04-29T09:07:40.460Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Ukro
    id: 644cde5c0dc952d24595bcec
    type: comment
  author: TheBloke
  content: 'OK yeah, please go back to using the compat.no-act-order file. You can''t
    use the act-order file without being able to compile the latest CUDA code.


    WSL2 is Linux on Windows. You can install it from Microsoft and with a bit of
    work it can support your CUDA GPU


    https://learn.microsoft.com/en-us/windows/wsl/install


    https://docs.nvidia.com/cuda/wsl-user-guide/index.html'
  created_at: 2023-04-29 08:07:40+00:00
  edited: false
  hidden: false
  id: 644cde5c0dc952d24595bcec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-29T19:00:44.000Z'
    data:
      edited: false
      editors:
      - Ukro
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
          fullname: Ukrovic
          isHf: false
          isPro: false
          name: Ukro
          type: user
        html: '<p>Can you please advice, if you said this "PS. If you installed WSL2
          then you could use Triton, and all of this could be a lot easier! :)" did
          you meant that i need to run the oobabooga in WSL2? or just the triton thing?
          &lt;3</p>

          '
        raw: Can you please advice, if you said this "PS. If you installed WSL2 then
          you could use Triton, and all of this could be a lot easier! :)" did you
          meant that i need to run the oobabooga in WSL2? or just the triton thing?
          <3
        updatedAt: '2023-04-29T19:00:44.142Z'
      numEdits: 0
      reactions: []
    id: 644d695cfa94e93b0ecabcff
    type: comment
  author: Ukro
  content: Can you please advice, if you said this "PS. If you installed WSL2 then
    you could use Triton, and all of this could be a lot easier! :)" did you meant
    that i need to run the oobabooga in WSL2? or just the triton thing? <3
  created_at: 2023-04-29 18:00:44+00:00
  edited: false
  hidden: false
  id: 644d695cfa94e93b0ecabcff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-29T19:24:36.000Z'
    data:
      edited: false
      editors:
      - Ukro
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
          fullname: Ukrovic
          isHf: false
          isPro: false
          name: Ukro
          type: user
        html: '<p>I am also trying the CUDA with theese commands:<br>pip uninstall
          quant-cuda<br>cd repositories/GPTQ-for-LLaMa<br>python setup_cuda.py install<br>I
          have installed the c++ tools<br>build is doing okay<br>The model is loading
          okay, but gives giberish output :-/<br>Will try again with different  GPTQ-for-LLaMa</p>

          '
        raw: 'I am also trying the CUDA with theese commands:

          pip uninstall quant-cuda

          cd repositories/GPTQ-for-LLaMa

          python setup_cuda.py install

          I have installed the c++ tools

          build is doing okay

          The model is loading okay, but gives giberish output :-/

          Will try again with different  GPTQ-for-LLaMa'
        updatedAt: '2023-04-29T19:24:36.396Z'
      numEdits: 0
      reactions: []
    id: 644d6ef40dc952d245a4cc39
    type: comment
  author: Ukro
  content: 'I am also trying the CUDA with theese commands:

    pip uninstall quant-cuda

    cd repositories/GPTQ-for-LLaMa

    python setup_cuda.py install

    I have installed the c++ tools

    build is doing okay

    The model is loading okay, but gives giberish output :-/

    Will try again with different  GPTQ-for-LLaMa'
  created_at: 2023-04-29 18:24:36+00:00
  edited: false
  hidden: false
  id: 644d6ef40dc952d245a4cc39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-29T19:44:56.000Z'
    data:
      edited: false
      editors:
      - Ukro
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
          fullname: Ukrovic
          isHf: false
          isPro: false
          name: Ukro
          type: user
        html: "<p>It's working !!! Thank you very much.<br>So last post i had wrong\
          \ GPTQ-for-LLama as i have tested a lot variations and i had lost count.<br>So\
          \ the solution as you described with cuda.<br>Thank you again!!!!!!! peace!\
          \ &lt;3<br><em><strong>For further refference</strong></em><br><a rel=\"\
          nofollow\" href=\"https://stackoverflow.com/questions/64261546/how-to-solve-error-microsoft-visual-c-14-0-or-greater-is-required-when-inst\"\
          >https://stackoverflow.com/questions/64261546/how-to-solve-error-microsoft-visual-c-14-0-or-greater-is-required-when-inst</a><br>edit\
          \ for first run start-webui.bat:<br>Change this:<br><span data-props=\"\
          {&quot;user&quot;:&quot;rem&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/rem\">@<span class=\"underline\">rem</span></a></span>\n\
          \n\t</span></span> set default cuda toolkit to the one in the environment<br>set\
          \ \"CUDA_PATH=%INSTALL_ENV_DIR%\"<br>to this:<br><span data-props=\"{&quot;user&quot;:&quot;rem&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rem\"\
          >@<span class=\"underline\">rem</span></a></span>\n\n\t</span></span> set\
          \ default cuda toolkit to the one in the environment<br>set \"CUDA_PATH=%INSTALL_ENV_DIR%\"\
          <br>pip uninstall quant-cuda<br>cd repositories/GPTQ-for-LLaMa<br>python\
          \ setup_cuda.py install<br>pause<br>cd ..<br>cd ..</p>\n<hr>\n<p>after first\
          \ run, the added rows can be removed</p>\n"
        raw: 'It''s working !!! Thank you very much.

          So last post i had wrong GPTQ-for-LLama as i have tested a lot variations
          and i had lost count.

          So the solution as you described with cuda.

          Thank you again!!!!!!! peace! <3

          ___For further refference___

          https://stackoverflow.com/questions/64261546/how-to-solve-error-microsoft-visual-c-14-0-or-greater-is-required-when-inst

          edit for first run start-webui.bat:

          Change this:

          @rem set default cuda toolkit to the one in the environment

          set "CUDA_PATH=%INSTALL_ENV_DIR%"

          to this:

          @rem set default cuda toolkit to the one in the environment

          set "CUDA_PATH=%INSTALL_ENV_DIR%"

          pip uninstall quant-cuda

          cd repositories/GPTQ-for-LLaMa

          python setup_cuda.py install

          pause

          cd ..

          cd ..

          -----

          after first run, the added rows can be removed

          '
        updatedAt: '2023-04-29T19:44:56.917Z'
      numEdits: 0
      reactions: []
      relatedEventId: 644d73b80dc952d245a53625
    id: 644d73b80dc952d245a53624
    type: comment
  author: Ukro
  content: 'It''s working !!! Thank you very much.

    So last post i had wrong GPTQ-for-LLama as i have tested a lot variations and
    i had lost count.

    So the solution as you described with cuda.

    Thank you again!!!!!!! peace! <3

    ___For further refference___

    https://stackoverflow.com/questions/64261546/how-to-solve-error-microsoft-visual-c-14-0-or-greater-is-required-when-inst

    edit for first run start-webui.bat:

    Change this:

    @rem set default cuda toolkit to the one in the environment

    set "CUDA_PATH=%INSTALL_ENV_DIR%"

    to this:

    @rem set default cuda toolkit to the one in the environment

    set "CUDA_PATH=%INSTALL_ENV_DIR%"

    pip uninstall quant-cuda

    cd repositories/GPTQ-for-LLaMa

    python setup_cuda.py install

    pause

    cd ..

    cd ..

    -----

    after first run, the added rows can be removed

    '
  created_at: 2023-04-29 18:44:56+00:00
  edited: false
  hidden: false
  id: 644d73b80dc952d245a53624
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-29T19:44:56.000Z'
    data:
      status: closed
    id: 644d73b80dc952d245a53625
    type: status-change
  author: Ukro
  created_at: 2023-04-29 18:44:56+00:00
  id: 644d73b80dc952d245a53625
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T20:23:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Can you please advice, if you said this "PS. If you installed WSL2 then
          you could use Triton, and all of this could be a lot easier! :)" did you
          meant that i need to run the oobabooga in WSL2? or just the triton thing?
          &lt;3</p>

          </blockquote>

          <p>Glad you''ve got it working now!</p>

          <p>FYI I meant you should run everything in WSL2 - oobabooga with GPTQ-for-LLaMa
          using Triton.   So you would be running the whole ooba UI inside Linux.</p>

          <p>But if you''ve got it working in Windows now, that''s great!</p>

          '
        raw: '> Can you please advice, if you said this "PS. If you installed WSL2
          then you could use Triton, and all of this could be a lot easier! :)" did
          you meant that i need to run the oobabooga in WSL2? or just the triton thing?
          <3


          Glad you''ve got it working now!


          FYI I meant you should run everything in WSL2 - oobabooga with GPTQ-for-LLaMa
          using Triton.   So you would be running the whole ooba UI inside Linux.


          But if you''ve got it working in Windows now, that''s great!'
        updatedAt: '2023-04-29T20:23:33.329Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Ukro
    id: 644d7cc5fa94e93b0ecc7480
    type: comment
  author: TheBloke
  content: '> Can you please advice, if you said this "PS. If you installed WSL2 then
    you could use Triton, and all of this could be a lot easier! :)" did you meant
    that i need to run the oobabooga in WSL2? or just the triton thing? <3


    Glad you''ve got it working now!


    FYI I meant you should run everything in WSL2 - oobabooga with GPTQ-for-LLaMa
    using Triton.   So you would be running the whole ooba UI inside Linux.


    But if you''ve got it working in Windows now, that''s great!'
  created_at: 2023-04-29 19:23:33+00:00
  edited: false
  hidden: false
  id: 644d7cc5fa94e93b0ecc7480
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-29T20:43:03.000Z'
    data:
      edited: false
      editors:
      - Ukro
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
          fullname: Ukrovic
          isHf: false
          isPro: false
          name: Ukro
          type: user
        html: '<p>Thank you for explanation &lt;3<br>In the future i will definitely
          move to WSL2<br>Downloading your stable-vicuna for testing :&gt;</p>

          '
        raw: "Thank you for explanation <3\nIn the future i will definitely move to\
          \ WSL2 \nDownloading your stable-vicuna for testing :>"
        updatedAt: '2023-04-29T20:43:03.378Z'
      numEdits: 0
      reactions: []
    id: 644d8157fa94e93b0eccdc8f
    type: comment
  author: Ukro
  content: "Thank you for explanation <3\nIn the future i will definitely move to\
    \ WSL2 \nDownloading your stable-vicuna for testing :>"
  created_at: 2023-04-29 19:43:03+00:00
  edited: false
  hidden: false
  id: 644d8157fa94e93b0eccdc8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
      fullname: Ukrovic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ukro
      type: user
    createdAt: '2023-04-29T20:46:12.000Z'
    data:
      edited: true
      editors:
      - Ukro
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441002bb3e14d9ba7b4d433/0jSyuhsWBrFOg6IOVqzDp.jpeg?w=200&h=200&f=face
          fullname: Ukrovic
          isHf: false
          isPro: false
          name: Ukro
          type: user
        html: '<p>FYI :)<br>StableVicuna is NOW the UNSTOPPABLE 13B LLM KING! Bye
          Vicuna!<br><a rel="nofollow" href="https://youtu.be/QeBmeHg8s5Y">https://youtu.be/QeBmeHg8s5Y</a></p>

          '
        raw: 'FYI :)

          StableVicuna is NOW the UNSTOPPABLE 13B LLM KING! Bye Vicuna!

          https://youtu.be/QeBmeHg8s5Y'
        updatedAt: '2023-04-29T20:46:34.919Z'
      numEdits: 1
      reactions: []
    id: 644d82140dc952d245a686bf
    type: comment
  author: Ukro
  content: 'FYI :)

    StableVicuna is NOW the UNSTOPPABLE 13B LLM KING! Bye Vicuna!

    https://youtu.be/QeBmeHg8s5Y'
  created_at: 2023-04-29 19:46:12+00:00
  edited: true
  hidden: false
  id: 644d82140dc952d245a686bf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: TheBloke/Vicuna-13B-1.1-GPTQ
repo_type: model
status: closed
target_branch: null
title: Detailed problem, need support <3
