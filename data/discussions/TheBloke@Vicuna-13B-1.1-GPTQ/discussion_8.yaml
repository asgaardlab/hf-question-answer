!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AnzacExodus
conflicting_files: null
created_at: 2023-04-26 06:52:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/30b8e939e6e68dd14b79210ab613d0c0.svg
      fullname: Caleb Wycliff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AnzacExodus
      type: user
    createdAt: '2023-04-26T07:52:37.000Z'
    data:
      edited: false
      editors:
      - AnzacExodus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/30b8e939e6e68dd14b79210ab613d0c0.svg
          fullname: Caleb Wycliff
          isHf: false
          isPro: false
          name: AnzacExodus
          type: user
        html: '<p>Hi, I''m currently trying to get the vicuna-13B-1.1-GPTQ-4bit-128g
          model working on my PC. I''m using a Windows 10 computer and I''m also using
          Ooba. I''ve downloaded the repository including the  no-act-order version
          as I have an Nvidia GPU (RTX 3070), but when I type in the chat, my message
          briefly shows for about a second and then disappears. It seems the only
          model that works for me is the "TheBloke_vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g"
          version.<br>I''m currently new to all the AI stuff so if anyone can point
          me in the right direction, I appreciate it.</p>

          <p>Thank you.</p>

          '
        raw: "Hi, I'm currently trying to get the vicuna-13B-1.1-GPTQ-4bit-128g model\
          \ working on my PC. I'm using a Windows 10 computer and I'm also using Ooba.\
          \ I've downloaded the repository including the  no-act-order version as\
          \ I have an Nvidia GPU (RTX 3070), but when I type in the chat, my message\
          \ briefly shows for about a second and then disappears. It seems the only\
          \ model that works for me is the \"TheBloke_vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g\"\
          \ version. \r\nI'm currently new to all the AI stuff so if anyone can point\
          \ me in the right direction, I appreciate it.\r\n\r\nThank you."
        updatedAt: '2023-04-26T07:52:37.393Z'
      numEdits: 0
      reactions: []
    id: 6448d845b6ac93fe6512c2d8
    type: comment
  author: AnzacExodus
  content: "Hi, I'm currently trying to get the vicuna-13B-1.1-GPTQ-4bit-128g model\
    \ working on my PC. I'm using a Windows 10 computer and I'm also using Ooba. I've\
    \ downloaded the repository including the  no-act-order version as I have an Nvidia\
    \ GPU (RTX 3070), but when I type in the chat, my message briefly shows for about\
    \ a second and then disappears. It seems the only model that works for me is the\
    \ \"TheBloke_vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g\" version. \r\nI'm currently\
    \ new to all the AI stuff so if anyone can point me in the right direction, I\
    \ appreciate it.\r\n\r\nThank you."
  created_at: 2023-04-26 06:52:37+00:00
  edited: false
  hidden: false
  id: 6448d845b6ac93fe6512c2d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90d2587a45ac080f101ef09dc404a279.svg
      fullname: Niichan Haou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Niichanhaou
      type: user
    createdAt: '2023-04-30T13:37:52.000Z'
    data:
      edited: false
      editors:
      - Niichanhaou
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90d2587a45ac080f101ef09dc404a279.svg
          fullname: Niichan Haou
          isHf: false
          isPro: false
          name: Niichanhaou
          type: user
        html: '<p>Same here. After I built the latest possible (for windows) GPTQ-for-LlaMa
          and using the latest ooba, I get this exception when I type something (when
          the prompt disappears):</p>

          <p>GPTQ-for-LLaMa\quant.py", line 279, in forward<br>    quant_cuda.vecquant4matmul(x.float(),
          self.qweight, out, self.scales.float(), self.qzeros, self.g_idx)<br>TypeError:
          vecquant4matmul(): incompatible function arguments. The following argument
          types are supported:<br>    1. (arg0: torch.Tensor, arg1: torch.Tensor,
          arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: int) -&gt;
          None</p>

          <p>Invoked with: tensor([[ 0.0097, -0.0423,  0.2747,  ..., -0.0144,  0.0021,  0.0083],<br>        [
          0.0172,  0.0039, -0.0247,  ..., -0.0062, -0.0020, -0.0056],<br>        [
          0.0144,  0.0142, -0.0514,  ...,  0.0037,  0.0072,  0.0195],</p>

          <p>(a bunch of tensor stuff)</p>

          <p> device=''cuda:0'', dtype=torch.int32), tensor([38,  6,  1,  ..., 23,  9,
          17], device=''cuda:0'', dtype=torch.int32)</p>

          <p>That''s the exception I get. It happens both for safetensors and the
          pt one. Basically it loads the model fine, but throws the exception when
          trying any input.</p>

          <p>Since what you describe is exactly the same issue, I believe it''s the
          same. I tried to build a GPTQ-for-LlaMa from an earlier commit (like 1 month
          ago) but then it fails because another function is called with a mismatch
          in the number of arguments.</p>

          <p>I don''t remember having any better luck running the .pt version with
          the "installer" version of ooba, but I''m going to try and see if it does
          any better.</p>

          '
        raw: "Same here. After I built the latest possible (for windows) GPTQ-for-LlaMa\
          \ and using the latest ooba, I get this exception when I type something\
          \ (when the prompt disappears):\n\nGPTQ-for-LLaMa\\quant.py\", line 279,\
          \ in forward\n    quant_cuda.vecquant4matmul(x.float(), self.qweight, out,\
          \ self.scales.float(), self.qzeros, self.g_idx)\nTypeError: vecquant4matmul():\
          \ incompatible function arguments. The following argument types are supported:\n\
          \    1. (arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3:\
          \ torch.Tensor, arg4: torch.Tensor, arg5: int) -> None\n\nInvoked with:\
          \ tensor([[ 0.0097, -0.0423,  0.2747,  ..., -0.0144,  0.0021,  0.0083],\n\
          \        [ 0.0172,  0.0039, -0.0247,  ..., -0.0062, -0.0020, -0.0056],\n\
          \        [ 0.0144,  0.0142, -0.0514,  ...,  0.0037,  0.0072,  0.0195],\n\
          \n(a bunch of tensor stuff)\n\n device='cuda:0', dtype=torch.int32), tensor([38,\
          \  6,  1,  ..., 23,  9, 17], device='cuda:0', dtype=torch.int32)\n\nThat's\
          \ the exception I get. It happens both for safetensors and the pt one. Basically\
          \ it loads the model fine, but throws the exception when trying any input.\n\
          \nSince what you describe is exactly the same issue, I believe it's the\
          \ same. I tried to build a GPTQ-for-LlaMa from an earlier commit (like 1\
          \ month ago) but then it fails because another function is called with a\
          \ mismatch in the number of arguments.\n\nI don't remember having any better\
          \ luck running the .pt version with the \"installer\" version of ooba, but\
          \ I'm going to try and see if it does any better."
        updatedAt: '2023-04-30T13:37:52.748Z'
      numEdits: 0
      reactions: []
    id: 644e6f30bf9683cba45ff715
    type: comment
  author: Niichanhaou
  content: "Same here. After I built the latest possible (for windows) GPTQ-for-LlaMa\
    \ and using the latest ooba, I get this exception when I type something (when\
    \ the prompt disappears):\n\nGPTQ-for-LLaMa\\quant.py\", line 279, in forward\n\
    \    quant_cuda.vecquant4matmul(x.float(), self.qweight, out, self.scales.float(),\
    \ self.qzeros, self.g_idx)\nTypeError: vecquant4matmul(): incompatible function\
    \ arguments. The following argument types are supported:\n    1. (arg0: torch.Tensor,\
    \ arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor,\
    \ arg5: int) -> None\n\nInvoked with: tensor([[ 0.0097, -0.0423,  0.2747,  ...,\
    \ -0.0144,  0.0021,  0.0083],\n        [ 0.0172,  0.0039, -0.0247,  ..., -0.0062,\
    \ -0.0020, -0.0056],\n        [ 0.0144,  0.0142, -0.0514,  ...,  0.0037,  0.0072,\
    \  0.0195],\n\n(a bunch of tensor stuff)\n\n device='cuda:0', dtype=torch.int32),\
    \ tensor([38,  6,  1,  ..., 23,  9, 17], device='cuda:0', dtype=torch.int32)\n\
    \nThat's the exception I get. It happens both for safetensors and the pt one.\
    \ Basically it loads the model fine, but throws the exception when trying any\
    \ input.\n\nSince what you describe is exactly the same issue, I believe it's\
    \ the same. I tried to build a GPTQ-for-LlaMa from an earlier commit (like 1 month\
    \ ago) but then it fails because another function is called with a mismatch in\
    \ the number of arguments.\n\nI don't remember having any better luck running\
    \ the .pt version with the \"installer\" version of ooba, but I'm going to try\
    \ and see if it does any better."
  created_at: 2023-04-30 12:37:52+00:00
  edited: false
  hidden: false
  id: 644e6f30bf9683cba45ff715
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90d2587a45ac080f101ef09dc404a279.svg
      fullname: Niichan Haou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Niichanhaou
      type: user
    createdAt: '2023-04-30T14:11:05.000Z'
    data:
      edited: false
      editors:
      - Niichanhaou
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90d2587a45ac080f101ef09dc404a279.svg
          fullname: Niichan Haou
          isHf: false
          isPro: false
          name: Niichanhaou
          type: user
        html: '<p>I got .pt to work with the latest installer of ooba but sadly it
          doesn''t allow to use pre_layers, so I run out of VRAM pretty quick. However,
          I can''t make it work with the latest CUDA branch of GPTQ-for-LlaMa either,
          so pretty much stuck</p>

          '
        raw: I got .pt to work with the latest installer of ooba but sadly it doesn't
          allow to use pre_layers, so I run out of VRAM pretty quick. However, I can't
          make it work with the latest CUDA branch of GPTQ-for-LlaMa either, so pretty
          much stuck
        updatedAt: '2023-04-30T14:11:05.644Z'
      numEdits: 0
      reactions: []
    id: 644e76f9cf72e60a5b7a29b6
    type: comment
  author: Niichanhaou
  content: I got .pt to work with the latest installer of ooba but sadly it doesn't
    allow to use pre_layers, so I run out of VRAM pretty quick. However, I can't make
    it work with the latest CUDA branch of GPTQ-for-LlaMa either, so pretty much stuck
  created_at: 2023-04-30 13:11:05+00:00
  edited: false
  hidden: false
  id: 644e76f9cf72e60a5b7a29b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-30T14:59:25.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You get this error if you haven''t re-compiled the CUDA branch of
          GPTQ-for-LLaMa after git cloning it. You need to run</p>

          <pre><code>cd GPTQ-for-LLaMa

          pip uninstall quant-cuda

          python setup_cuda.py install

          </code></pre>

          <p>To compile it, you will need a C/C++ compiler and the CUDA toolkit installed.  This
          may be tricky to do on Windows, but check out this post from another user
          who did it recently: <a href="https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/discussions/9#644d73b80dc952d245a53624">https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/discussions/9#644d73b80dc952d245a53624</a></p>

          '
        raw: 'You get this error if you haven''t re-compiled the CUDA branch of GPTQ-for-LLaMa
          after git cloning it. You need to run

          ```

          cd GPTQ-for-LLaMa

          pip uninstall quant-cuda

          python setup_cuda.py install

          ```


          To compile it, you will need a C/C++ compiler and the CUDA toolkit installed.  This
          may be tricky to do on Windows, but check out this post from another user
          who did it recently: https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/discussions/9#644d73b80dc952d245a53624'
        updatedAt: '2023-04-30T14:59:55.461Z'
      numEdits: 1
      reactions: []
    id: 644e824da00f4b11d393b491
    type: comment
  author: TheBloke
  content: 'You get this error if you haven''t re-compiled the CUDA branch of GPTQ-for-LLaMa
    after git cloning it. You need to run

    ```

    cd GPTQ-for-LLaMa

    pip uninstall quant-cuda

    python setup_cuda.py install

    ```


    To compile it, you will need a C/C++ compiler and the CUDA toolkit installed.  This
    may be tricky to do on Windows, but check out this post from another user who
    did it recently: https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/discussions/9#644d73b80dc952d245a53624'
  created_at: 2023-04-30 13:59:25+00:00
  edited: true
  hidden: false
  id: 644e824da00f4b11d393b491
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-30T15:00:26.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Note that life is a lot easier in WSL2. Then you can use the Triton
          branch.</p>

          '
        raw: Note that life is a lot easier in WSL2. Then you can use the Triton branch.
        updatedAt: '2023-04-30T15:00:26.776Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Niichanhaou
    id: 644e828abf9683cba461e314
    type: comment
  author: TheBloke
  content: Note that life is a lot easier in WSL2. Then you can use the Triton branch.
  created_at: 2023-04-30 14:00:26+00:00
  edited: false
  hidden: false
  id: 644e828abf9683cba461e314
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90d2587a45ac080f101ef09dc404a279.svg
      fullname: Niichan Haou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Niichanhaou
      type: user
    createdAt: '2023-04-30T16:57:48.000Z'
    data:
      edited: false
      editors:
      - Niichanhaou
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90d2587a45ac080f101ef09dc404a279.svg
          fullname: Niichan Haou
          isHf: false
          isPro: false
          name: Niichanhaou
          type: user
        html: '<blockquote>

          <p>Note that life is a lot easier in WSL2. Then you can use the Triton branch.</p>

          </blockquote>

          <p>Does it work well? I mean, it looks like some kind of virtualization,
          I''m not sure if the performance will be degraded or if I can even use my
          GPU (nvidia). But if you recommend it, I will give it a try</p>

          '
        raw: '> Note that life is a lot easier in WSL2. Then you can use the Triton
          branch.


          Does it work well? I mean, it looks like some kind of virtualization, I''m
          not sure if the performance will be degraded or if I can even use my GPU
          (nvidia). But if you recommend it, I will give it a try'
        updatedAt: '2023-04-30T16:57:48.944Z'
      numEdits: 0
      reactions: []
    id: 644e9e0cbf9683cba4645817
    type: comment
  author: Niichanhaou
  content: '> Note that life is a lot easier in WSL2. Then you can use the Triton
    branch.


    Does it work well? I mean, it looks like some kind of virtualization, I''m not
    sure if the performance will be degraded or if I can even use my GPU (nvidia).
    But if you recommend it, I will give it a try'
  created_at: 2023-04-30 15:57:48+00:00
  edited: false
  hidden: false
  id: 644e9e0cbf9683cba4645817
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-30T17:01:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes it''s basically virtualised Linux on Windows. It performs very
          well. You can definitely use a NVidia GPU in WSL and they have a guide to
          doing it: <a rel="nofollow" href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html">https://docs.nvidia.com/cuda/wsl-user-guide/index.html</a></p>

          '
        raw: 'Yes it''s basically virtualised Linux on Windows. It performs very well.
          You can definitely use a NVidia GPU in WSL and they have a guide to doing
          it: https://docs.nvidia.com/cuda/wsl-user-guide/index.html'
        updatedAt: '2023-04-30T17:01:45.401Z'
      numEdits: 0
      reactions: []
    id: 644e9ef9ddf20748b05b2c86
    type: comment
  author: TheBloke
  content: 'Yes it''s basically virtualised Linux on Windows. It performs very well.
    You can definitely use a NVidia GPU in WSL and they have a guide to doing it:
    https://docs.nvidia.com/cuda/wsl-user-guide/index.html'
  created_at: 2023-04-30 16:01:45+00:00
  edited: false
  hidden: false
  id: 644e9ef9ddf20748b05b2c86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90d2587a45ac080f101ef09dc404a279.svg
      fullname: Niichan Haou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Niichanhaou
      type: user
    createdAt: '2023-04-30T18:57:49.000Z'
    data:
      edited: false
      editors:
      - Niichanhaou
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90d2587a45ac080f101ef09dc404a279.svg
          fullname: Niichan Haou
          isHf: false
          isPro: false
          name: Niichanhaou
          type: user
        html: '<p>Thanks!<br>It worked perfectly. It''s a life saver, because nowadays
          most 4bit quantized models weren''t working with ooba, and although I knew
          that running it on linux might solve it, I honestly didn''t want to do any
          dual boot (I''m a bit old fashioned so that''s the first thing that springs
          to mind)</p>

          '
        raw: 'Thanks!

          It worked perfectly. It''s a life saver, because nowadays most 4bit quantized
          models weren''t working with ooba, and although I knew that running it on
          linux might solve it, I honestly didn''t want to do any dual boot (I''m
          a bit old fashioned so that''s the first thing that springs to mind)'
        updatedAt: '2023-04-30T18:57:49.723Z'
      numEdits: 0
      reactions: []
    id: 644eba2dddf20748b05d58f1
    type: comment
  author: Niichanhaou
  content: 'Thanks!

    It worked perfectly. It''s a life saver, because nowadays most 4bit quantized
    models weren''t working with ooba, and although I knew that running it on linux
    might solve it, I honestly didn''t want to do any dual boot (I''m a bit old fashioned
    so that''s the first thing that springs to mind)'
  created_at: 2023-04-30 17:57:49+00:00
  edited: false
  hidden: false
  id: 644eba2dddf20748b05d58f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-30T19:55:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great! Glad you got it working</p>

          '
        raw: Great! Glad you got it working
        updatedAt: '2023-04-30T19:55:25.593Z'
      numEdits: 0
      reactions: []
    id: 644ec7adbf9683cba467bac8
    type: comment
  author: TheBloke
  content: Great! Glad you got it working
  created_at: 2023-04-30 18:55:25+00:00
  edited: false
  hidden: false
  id: 644ec7adbf9683cba467bac8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9afb6855674631132f4041e2599cbd0e.svg
      fullname: Varun Mathur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: varun500
      type: user
    createdAt: '2023-05-04T05:40:13.000Z'
    data:
      edited: false
      editors:
      - varun500
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9afb6855674631132f4041e2599cbd0e.svg
          fullname: Varun Mathur
          isHf: false
          isPro: false
          name: varun500
          type: user
        html: '<p>It takes very long to load in a kaggle notebook? Is there any way
          to load the whole model faster?</p>

          '
        raw: It takes very long to load in a kaggle notebook? Is there any way to
          load the whole model faster?
        updatedAt: '2023-05-04T05:40:13.734Z'
      numEdits: 0
      reactions: []
    id: 6453453d8fe6558e32904ddd
    type: comment
  author: varun500
  content: It takes very long to load in a kaggle notebook? Is there any way to load
    the whole model faster?
  created_at: 2023-05-04 04:40:13+00:00
  edited: false
  hidden: false
  id: 6453453d8fe6558e32904ddd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T16:11:14.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>It takes very long to load in a kaggle notebook? Is there any way to
          load the whole model faster?</p>

          </blockquote>

          <p>Are you definitely loading it on the GPU?   What is the GPU?</p>

          '
        raw: '> It takes very long to load in a kaggle notebook? Is there any way
          to load the whole model faster?


          Are you definitely loading it on the GPU?   What is the GPU?'
        updatedAt: '2023-05-05T16:11:14.774Z'
      numEdits: 0
      reactions: []
    id: 64552aa2f61f10d69dcadfe2
    type: comment
  author: TheBloke
  content: '> It takes very long to load in a kaggle notebook? Is there any way to
    load the whole model faster?


    Are you definitely loading it on the GPU?   What is the GPU?'
  created_at: 2023-05-05 15:11:14+00:00
  edited: false
  hidden: false
  id: 64552aa2f61f10d69dcadfe2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9afb6855674631132f4041e2599cbd0e.svg
      fullname: Varun Mathur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: varun500
      type: user
    createdAt: '2023-05-05T16:44:48.000Z'
    data:
      edited: false
      editors:
      - varun500
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9afb6855674631132f4041e2599cbd0e.svg
          fullname: Varun Mathur
          isHf: false
          isPro: false
          name: varun500
          type: user
        html: '<p>+-----------------------------------------------------------------------------+<br>|
          NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |<br>|-------------------------------+----------------------+----------------------+<br>|
          GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.
          ECC |<br>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute
          M. |<br>|                               |                      |               MIG
          M. |<br>|===============================+======================+======================|<br>|   0  Tesla
          P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |<br>|
          N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default
          |<br>|                               |                      |                  N/A
          |<br>+-------------------------------+----------------------+----------------------+<br>It
          is a Tesla P100 with 16GB RAM.</p>

          '
        raw: '+-----------------------------------------------------------------------------+

          | NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |

          |-------------------------------+----------------------+----------------------+

          | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.
          ECC |

          | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute
          M. |

          |                               |                      |               MIG
          M. |

          |===============================+======================+======================|

          |   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0
          |

          | N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default
          |

          |                               |                      |                  N/A
          |

          +-------------------------------+----------------------+----------------------+

          It is a Tesla P100 with 16GB RAM.'
        updatedAt: '2023-05-05T16:44:48.770Z'
      numEdits: 0
      reactions: []
    id: 64553280f61f10d69dcbc96d
    type: comment
  author: varun500
  content: '+-----------------------------------------------------------------------------+

    | NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |

    |-------------------------------+----------------------+----------------------+

    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC
    |

    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M.
    |

    |                               |                      |               MIG M.
    |

    |===============================+======================+======================|

    |   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0
    |

    | N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default
    |

    |                               |                      |                  N/A
    |

    +-------------------------------+----------------------+----------------------+

    It is a Tesla P100 with 16GB RAM.'
  created_at: 2023-05-05 15:44:48+00:00
  edited: false
  hidden: false
  id: 64553280f61f10d69dcbc96d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T22:58:36.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>What version of GPTQ-for-LLaMa are you using, do you know?</p>

          '
        raw: What version of GPTQ-for-LLaMa are you using, do you know?
        updatedAt: '2023-05-05T22:58:36.570Z'
      numEdits: 0
      reactions: []
    id: 64558a1cfe2f48cb4b70856c
    type: comment
  author: TheBloke
  content: What version of GPTQ-for-LLaMa are you using, do you know?
  created_at: 2023-05-05 21:58:36+00:00
  edited: false
  hidden: false
  id: 64558a1cfe2f48cb4b70856c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9afb6855674631132f4041e2599cbd0e.svg
      fullname: Varun Mathur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: varun500
      type: user
    createdAt: '2023-05-06T18:31:10.000Z'
    data:
      edited: false
      editors:
      - varun500
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9afb6855674631132f4041e2599cbd0e.svg
          fullname: Varun Mathur
          isHf: false
          isPro: false
          name: varun500
          type: user
        html: '<p>I am using the latest version.</p>

          '
        raw: I am using the latest version.
        updatedAt: '2023-05-06T18:31:10.100Z'
      numEdits: 0
      reactions: []
    id: 64569ceed10badc9555f5247
    type: comment
  author: varun500
  content: I am using the latest version.
  created_at: 2023-05-06 17:31:10+00:00
  edited: false
  hidden: false
  id: 64569ceed10badc9555f5247
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-06T20:15:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You can try adding these command line arguments to text-generation-webui:
          <code>--quant_attn --fused_mlp</code>, it may provide some speed boost</p>

          '
        raw: 'You can try adding these command line arguments to text-generation-webui:
          `--quant_attn --fused_mlp`, it may provide some speed boost'
        updatedAt: '2023-05-06T20:15:53.008Z'
      numEdits: 0
      reactions: []
    id: 6456b579cd6567f52fba4bea
    type: comment
  author: TheBloke
  content: 'You can try adding these command line arguments to text-generation-webui:
    `--quant_attn --fused_mlp`, it may provide some speed boost'
  created_at: 2023-05-06 19:15:53+00:00
  edited: false
  hidden: false
  id: 6456b579cd6567f52fba4bea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/Vicuna-13B-1.1-GPTQ
repo_type: model
status: open
target_branch: null
title: 'vicuna-13B-1.1-GPTQ-4bit-128g not working properly '
