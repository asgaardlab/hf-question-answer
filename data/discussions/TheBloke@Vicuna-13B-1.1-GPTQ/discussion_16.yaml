!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AIDran
conflicting_files: null
created_at: 2023-06-09 11:50:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7f5581563553cde6829b8465d1ae2a7.svg
      fullname: JJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIDran
      type: user
    createdAt: '2023-06-09T12:50:39.000Z'
    data:
      edited: false
      editors:
      - AIDran
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9976145029067993
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7f5581563553cde6829b8465d1ae2a7.svg
          fullname: JJ
          isHf: false
          isPro: false
          name: AIDran
          type: user
        html: '<p>Why you deleted this model ? it was pretty good.</p>

          '
        raw: Why you deleted this model ? it was pretty good.
        updatedAt: '2023-06-09T12:50:39.117Z'
      numEdits: 0
      reactions: []
    id: 6483201f58902fd029e8bad0
    type: comment
  author: AIDran
  content: Why you deleted this model ? it was pretty good.
  created_at: 2023-06-09 11:50:39+00:00
  edited: false
  hidden: false
  id: 6483201f58902fd029e8bad0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7f5581563553cde6829b8465d1ae2a7.svg
      fullname: JJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIDran
      type: user
    createdAt: '2023-06-09T13:10:51.000Z'
    data:
      edited: false
      editors:
      - AIDran
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.957416832447052
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7f5581563553cde6829b8465d1ae2a7.svg
          fullname: JJ
          isHf: false
          isPro: false
          name: AIDran
          type: user
        html: '<p>it was working in Colab, and --gptq-for-llama flag.</p>

          '
        raw: it was working in Colab, and --gptq-for-llama flag.
        updatedAt: '2023-06-09T13:10:51.232Z'
      numEdits: 0
      reactions: []
    id: 648324db97a133e1f2283339
    type: comment
  author: AIDran
  content: it was working in Colab, and --gptq-for-llama flag.
  created_at: 2023-06-09 12:10:51+00:00
  edited: false
  hidden: false
  id: 648324db97a133e1f2283339
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-09T13:16:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9896017909049988
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry, it was causing lots of support requests because it doesn''t
          work with many GPTQ-for-LLaMa setups.</p>

          <p>I will put it back in a new branch. Give me a few mins.</p>

          '
        raw: 'Sorry, it was causing lots of support requests because it doesn''t work
          with many GPTQ-for-LLaMa setups.


          I will put it back in a new branch. Give me a few mins.'
        updatedAt: '2023-06-09T13:16:25.012Z'
      numEdits: 0
      reactions: []
    id: 64832629ccbb52063fb7e547
    type: comment
  author: TheBloke
  content: 'Sorry, it was causing lots of support requests because it doesn''t work
    with many GPTQ-for-LLaMa setups.


    I will put it back in a new branch. Give me a few mins.'
  created_at: 2023-06-09 12:16:25+00:00
  edited: false
  hidden: false
  id: 64832629ccbb52063fb7e547
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7f5581563553cde6829b8465d1ae2a7.svg
      fullname: JJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIDran
      type: user
    createdAt: '2023-06-09T13:40:34.000Z'
    data:
      edited: false
      editors:
      - AIDran
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8859408497810364
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7f5581563553cde6829b8465d1ae2a7.svg
          fullname: JJ
          isHf: false
          isPro: false
          name: AIDran
          type: user
        html: '<p>In Colab to make it works, the correct CUDA branch must be installed.<br>git
          clone <a rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">https://github.com/qwopqwop200/GPTQ-for-LLaMa</a>
          -b cuda<br>or use old CUDA branch <a rel="nofollow" href="https://github.com/oobabooga/GPTQ-for-LLaMa">https://github.com/oobabooga/GPTQ-for-LLaMa</a><br>The
          last one is much faster, but supposedly a little bit less precise.<br>Thanks,</p>

          '
        raw: "In Colab to make it works, the correct CUDA branch must be installed.\
          \ \ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa -b cuda \nor\
          \ use old CUDA branch https://github.com/oobabooga/GPTQ-for-LLaMa \nThe\
          \ last one is much faster, but supposedly a little bit less precise.\nThanks,"
        updatedAt: '2023-06-09T13:40:34.372Z'
      numEdits: 0
      reactions: []
    id: 64832bd2e3bd340cdca7cb0b
    type: comment
  author: AIDran
  content: "In Colab to make it works, the correct CUDA branch must be installed.\
    \ \ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa -b cuda \nor use old\
    \ CUDA branch https://github.com/oobabooga/GPTQ-for-LLaMa \nThe last one is much\
    \ faster, but supposedly a little bit less precise.\nThanks,"
  created_at: 2023-06-09 12:40:34+00:00
  edited: false
  hidden: false
  id: 64832bd2e3bd340cdca7cb0b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-09T13:54:26.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9439858794212341
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The latest.act-order.safetensors file won''t work with ooba GPTQ-for-LLaMA?
          That''s the whole reason I made two files, because groupsize + act-order
          won''t work with ooba GPTQ-for-LLaMa.</p>

          <p>It does work with latest qwopqwop GPTQ-for-LLaMa. And it does work with
          AutoGPTQ, which is what I now recommend.</p>

          <p>Anyway I got the latest file back, which is now in a new branch called
          <code>actorder</code> : <a href="https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/actorder">https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/actorder</a></p>

          <p>So if you clone that branch you will have it.</p>

          <p>But yeah, it is much slower to have both groupsize + act_order.  Unless
          you use Triton GPTQ-for-LLaMA instead, then it is a bit quicker (but still
          not as fast as CUDA branch will be with group_size or act-order on their
          own)</p>

          '
        raw: 'The latest.act-order.safetensors file won''t work with ooba GPTQ-for-LLaMA?
          That''s the whole reason I made two files, because groupsize + act-order
          won''t work with ooba GPTQ-for-LLaMa.


          It does work with latest qwopqwop GPTQ-for-LLaMa. And it does work with
          AutoGPTQ, which is what I now recommend.


          Anyway I got the latest file back, which is now in a new branch called `actorder`
          : https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/actorder


          So if you clone that branch you will have it.


          But yeah, it is much slower to have both groupsize + act_order.  Unless
          you use Triton GPTQ-for-LLaMA instead, then it is a bit quicker (but still
          not as fast as CUDA branch will be with group_size or act-order on their
          own)'
        updatedAt: '2023-06-09T13:55:23.822Z'
      numEdits: 1
      reactions: []
    id: 64832f12823496a7c30f2dc5
    type: comment
  author: TheBloke
  content: 'The latest.act-order.safetensors file won''t work with ooba GPTQ-for-LLaMA?
    That''s the whole reason I made two files, because groupsize + act-order won''t
    work with ooba GPTQ-for-LLaMa.


    It does work with latest qwopqwop GPTQ-for-LLaMa. And it does work with AutoGPTQ,
    which is what I now recommend.


    Anyway I got the latest file back, which is now in a new branch called `actorder`
    : https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/actorder


    So if you clone that branch you will have it.


    But yeah, it is much slower to have both groupsize + act_order.  Unless you use
    Triton GPTQ-for-LLaMA instead, then it is a bit quicker (but still not as fast
    as CUDA branch will be with group_size or act-order on their own)'
  created_at: 2023-06-09 12:54:26+00:00
  edited: true
  hidden: false
  id: 64832f12823496a7c30f2dc5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7f5581563553cde6829b8465d1ae2a7.svg
      fullname: JJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIDran
      type: user
    createdAt: '2023-06-09T14:35:40.000Z'
    data:
      edited: false
      editors:
      - AIDran
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7917607426643372
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7f5581563553cde6829b8465d1ae2a7.svg
          fullname: JJ
          isHf: false
          isPro: false
          name: AIDran
          type: user
        html: '<p>Confirmed the case,  latest.act-order.safetensors does not work
          with ooba GPTQ-for-LLaMa, it gives gibberish output.  But it does work with
          qwopqwop GPTQ-for-LLaMa.<br>Thx</p>

          '
        raw: 'Confirmed the case,  latest.act-order.safetensors does not work with
          ooba GPTQ-for-LLaMa, it gives gibberish output.  But it does work with qwopqwop
          GPTQ-for-LLaMa.

          Thx'
        updatedAt: '2023-06-09T14:35:40.386Z'
      numEdits: 0
      reactions: []
    id: 648338bc2874a8e21e8a52bd
    type: comment
  author: AIDran
  content: 'Confirmed the case,  latest.act-order.safetensors does not work with ooba
    GPTQ-for-LLaMa, it gives gibberish output.  But it does work with qwopqwop GPTQ-for-LLaMa.

    Thx'
  created_at: 2023-06-09 13:35:40+00:00
  edited: false
  hidden: false
  id: 648338bc2874a8e21e8a52bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b7f5581563553cde6829b8465d1ae2a7.svg
      fullname: JJ
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIDran
      type: user
    createdAt: '2023-06-09T14:41:11.000Z'
    data:
      status: closed
    id: 64833a07a5402eb30b6a8cb9
    type: status-change
  author: AIDran
  created_at: 2023-06-09 13:41:11+00:00
  id: 64833a07a5402eb30b6a8cb9
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: TheBloke/Vicuna-13B-1.1-GPTQ
repo_type: model
status: closed
target_branch: null
title: vicuna-13B-1.1-GPTQ-4bit-128g.latest.safetensors
