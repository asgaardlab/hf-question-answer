!!python/object:huggingface_hub.community.DiscussionWithDetails
author: disarmyouwitha
conflicting_files: null
created_at: 2023-05-28 15:38:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-05-28T16:38:27.000Z'
    data:
      edited: true
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: "<p>I downloaded your <code>guanaco-13B-GPTQ</code> model and I get\
          \ nearly 80 tokens/sec using exllama:<br><a rel=\"nofollow\" href=\"https://github.com/turboderp/exllama\"\
          >https://github.com/turboderp/exllama</a></p>\n<pre><code>python test_benchmark_inference_log.py\
          \ -d ~/llm_models/guanaco-13B-GPTQ\n -- Loading model\n -- Tokenizer: /home/nap/llm_models/guanaco-13B-GPTQ/tokenizer.model\n\
          \ -- Model config: /home/nap/llm_models/guanaco-13B-GPTQ/config.json\n --\
          \ Model: /home/nap/llm_models/guanaco-13B-GPTQ/Guanaco-13B-GPTQ-4bit-128g.no-act-order.safetensors\n\
          \ -- Sequence length: 2048\n -- Options: ['attention: switched', 'matmul:\
          \ switched', 'mlp: switched', 'perf', 'perplexity']\n ** Time, Load model:\
          \ 1.53 seconds\n -- Groupsize (inferred): 128\n -- Act-order (inferred):\
          \ no\n ** VRAM, Model: [cuda:0] 6,683.17 MB\n -- Inference, first pass.\n\
          \ ** Time, Inference: 0.75 seconds\n ** Speed: 2576.64 tokens/second\n --\
          \ Generating 128 tokens, 1920 token prompt...\n ** Speed: 79.34 tokens/second\n\
          \ -- Generating 128 tokens, 4 token prompt...\n ** Speed: 79.71 tokens/second\n\
          \ ** VRAM, Inference: [cuda:0] 2,254.17 MB\n ** VRAM, Total: [cuda:0] 8,937.34\
          \ MB\n -- Loading dataset...\n -- Testing..........\n ** Perplexity: 6.3734\n\
          </code></pre>\n<p><strong>I also downloaded the hugging face repo (this\
          \ one) and did the quantization myself, like how you listed in your model\
          \ card:</strong></p>\n<pre><code> CUDA_VISIBLE_DEVICES=0 python llama.py\
          \ /home/nap/llm_models/guanaco-13B-HF  wikitext2 --wbits 4 --true-sequential\
          \ --groupsize 128 --save_safetensors /home/nap/llm_models/guanaco-13B-HF/guanaco-13B-4bit-128g-no-act-order.safetensors\n\
          </code></pre>\n<p><strong>However my benchmarks are much lower (50tokens/sec)\
          \ with my quantized version?</strong></p>\n<pre><code>CUDA_VISIBLE_DEVICES=0\
          \ python test_benchmark_inference_log.py -d ~/llm_models/guanaco-13B-HF/\n\
          \ -- Loading model\n -- Tokenizer: /home/nap/llm_models/guanaco-13B-HF/tokenizer.model\n\
          \ -- Model config: /home/nap/llm_models/guanaco-13B-HF/config.json\n --\
          \ Model: /home/nap/llm_models/guanaco-13B-HF/guanaco-13B-4bit-128g-no-act-order.safetensors\n\
          \ -- Sequence length: 2048\n -- Options: ['attention: switched', 'matmul:\
          \ switched', 'mlp: switched', 'perf', 'perplexity']\n ** Time, Load model:\
          \ 1.91 seconds\n -- Groupsize (inferred): 128\n -- Act-order (inferred):\
          \ yes\n ** VRAM, Model: [cuda:0] 6,689.96 MB\n -- Inference, first pass.\n\
          \ ** Time, Inference: 0.83 seconds\n ** Speed: 2319.07 tokens/second\n --\
          \ Generating 128 tokens, 1920 token prompt...\n ** Speed: 50.98 tokens/second\n\
          \ -- Generating 128 tokens, 4 token prompt...\n ** Speed: 50.99 tokens/second\n\
          \ ** VRAM, Inference: [cuda:0] 2,254.17 MB\n ** VRAM, Total: [cuda:0] 8,944.13\
          \ MB\n -- Loading dataset...\n -- Testing..........\n ** Perplexity: 6.3615\n\
          </code></pre>\n<p>I am using the latest current cuda branch for quantizing\
          \ -- what branch are you using? (Do you think it matters?) Thanks!</p>\n"
        raw: "I downloaded your `guanaco-13B-GPTQ` model and I get nearly 80 tokens/sec\
          \ using exllama:\nhttps://github.com/turboderp/exllama\n\n```\npython test_benchmark_inference_log.py\
          \ -d ~/llm_models/guanaco-13B-GPTQ\n -- Loading model\n -- Tokenizer: /home/nap/llm_models/guanaco-13B-GPTQ/tokenizer.model\n\
          \ -- Model config: /home/nap/llm_models/guanaco-13B-GPTQ/config.json\n --\
          \ Model: /home/nap/llm_models/guanaco-13B-GPTQ/Guanaco-13B-GPTQ-4bit-128g.no-act-order.safetensors\n\
          \ -- Sequence length: 2048\n -- Options: ['attention: switched', 'matmul:\
          \ switched', 'mlp: switched', 'perf', 'perplexity']\n ** Time, Load model:\
          \ 1.53 seconds\n -- Groupsize (inferred): 128\n -- Act-order (inferred):\
          \ no\n ** VRAM, Model: [cuda:0] 6,683.17 MB\n -- Inference, first pass.\n\
          \ ** Time, Inference: 0.75 seconds\n ** Speed: 2576.64 tokens/second\n --\
          \ Generating 128 tokens, 1920 token prompt...\n ** Speed: 79.34 tokens/second\n\
          \ -- Generating 128 tokens, 4 token prompt...\n ** Speed: 79.71 tokens/second\n\
          \ ** VRAM, Inference: [cuda:0] 2,254.17 MB\n ** VRAM, Total: [cuda:0] 8,937.34\
          \ MB\n -- Loading dataset...\n -- Testing..........\n ** Perplexity: 6.3734\n\
          ```\n\n**I also downloaded the hugging face repo (this one) and did the\
          \ quantization myself, like how you listed in your model card:**\n```\n\
          \ CUDA_VISIBLE_DEVICES=0 python llama.py /home/nap/llm_models/guanaco-13B-HF\
          \  wikitext2 --wbits 4 --true-sequential --groupsize 128 --save_safetensors\
          \ /home/nap/llm_models/guanaco-13B-HF/guanaco-13B-4bit-128g-no-act-order.safetensors\n\
          ```\n\n**However my benchmarks are much lower (50tokens/sec) with my quantized\
          \ version?**\n```\nCUDA_VISIBLE_DEVICES=0 python test_benchmark_inference_log.py\
          \ -d ~/llm_models/guanaco-13B-HF/\n -- Loading model\n -- Tokenizer: /home/nap/llm_models/guanaco-13B-HF/tokenizer.model\n\
          \ -- Model config: /home/nap/llm_models/guanaco-13B-HF/config.json\n --\
          \ Model: /home/nap/llm_models/guanaco-13B-HF/guanaco-13B-4bit-128g-no-act-order.safetensors\n\
          \ -- Sequence length: 2048\n -- Options: ['attention: switched', 'matmul:\
          \ switched', 'mlp: switched', 'perf', 'perplexity']\n ** Time, Load model:\
          \ 1.91 seconds\n -- Groupsize (inferred): 128\n -- Act-order (inferred):\
          \ yes\n ** VRAM, Model: [cuda:0] 6,689.96 MB\n -- Inference, first pass.\n\
          \ ** Time, Inference: 0.83 seconds\n ** Speed: 2319.07 tokens/second\n --\
          \ Generating 128 tokens, 1920 token prompt...\n ** Speed: 50.98 tokens/second\n\
          \ -- Generating 128 tokens, 4 token prompt...\n ** Speed: 50.99 tokens/second\n\
          \ ** VRAM, Inference: [cuda:0] 2,254.17 MB\n ** VRAM, Total: [cuda:0] 8,944.13\
          \ MB\n -- Loading dataset...\n -- Testing..........\n ** Perplexity: 6.3615\n\
          ```\n\nI am using the latest current cuda branch for quantizing -- what\
          \ branch are you using? (Do you think it matters?) Thanks!"
        updatedAt: '2023-05-28T17:52:07.534Z'
      numEdits: 1
      reactions: []
    id: 647383837afa69c3c7a6bac8
    type: comment
  author: disarmyouwitha
  content: "I downloaded your `guanaco-13B-GPTQ` model and I get nearly 80 tokens/sec\
    \ using exllama:\nhttps://github.com/turboderp/exllama\n\n```\npython test_benchmark_inference_log.py\
    \ -d ~/llm_models/guanaco-13B-GPTQ\n -- Loading model\n -- Tokenizer: /home/nap/llm_models/guanaco-13B-GPTQ/tokenizer.model\n\
    \ -- Model config: /home/nap/llm_models/guanaco-13B-GPTQ/config.json\n -- Model:\
    \ /home/nap/llm_models/guanaco-13B-GPTQ/Guanaco-13B-GPTQ-4bit-128g.no-act-order.safetensors\n\
    \ -- Sequence length: 2048\n -- Options: ['attention: switched', 'matmul: switched',\
    \ 'mlp: switched', 'perf', 'perplexity']\n ** Time, Load model: 1.53 seconds\n\
    \ -- Groupsize (inferred): 128\n -- Act-order (inferred): no\n ** VRAM, Model:\
    \ [cuda:0] 6,683.17 MB\n -- Inference, first pass.\n ** Time, Inference: 0.75\
    \ seconds\n ** Speed: 2576.64 tokens/second\n -- Generating 128 tokens, 1920 token\
    \ prompt...\n ** Speed: 79.34 tokens/second\n -- Generating 128 tokens, 4 token\
    \ prompt...\n ** Speed: 79.71 tokens/second\n ** VRAM, Inference: [cuda:0] 2,254.17\
    \ MB\n ** VRAM, Total: [cuda:0] 8,937.34 MB\n -- Loading dataset...\n -- Testing..........\n\
    \ ** Perplexity: 6.3734\n```\n\n**I also downloaded the hugging face repo (this\
    \ one) and did the quantization myself, like how you listed in your model card:**\n\
    ```\n CUDA_VISIBLE_DEVICES=0 python llama.py /home/nap/llm_models/guanaco-13B-HF\
    \  wikitext2 --wbits 4 --true-sequential --groupsize 128 --save_safetensors /home/nap/llm_models/guanaco-13B-HF/guanaco-13B-4bit-128g-no-act-order.safetensors\n\
    ```\n\n**However my benchmarks are much lower (50tokens/sec) with my quantized\
    \ version?**\n```\nCUDA_VISIBLE_DEVICES=0 python test_benchmark_inference_log.py\
    \ -d ~/llm_models/guanaco-13B-HF/\n -- Loading model\n -- Tokenizer: /home/nap/llm_models/guanaco-13B-HF/tokenizer.model\n\
    \ -- Model config: /home/nap/llm_models/guanaco-13B-HF/config.json\n -- Model:\
    \ /home/nap/llm_models/guanaco-13B-HF/guanaco-13B-4bit-128g-no-act-order.safetensors\n\
    \ -- Sequence length: 2048\n -- Options: ['attention: switched', 'matmul: switched',\
    \ 'mlp: switched', 'perf', 'perplexity']\n ** Time, Load model: 1.91 seconds\n\
    \ -- Groupsize (inferred): 128\n -- Act-order (inferred): yes\n ** VRAM, Model:\
    \ [cuda:0] 6,689.96 MB\n -- Inference, first pass.\n ** Time, Inference: 0.83\
    \ seconds\n ** Speed: 2319.07 tokens/second\n -- Generating 128 tokens, 1920 token\
    \ prompt...\n ** Speed: 50.98 tokens/second\n -- Generating 128 tokens, 4 token\
    \ prompt...\n ** Speed: 50.99 tokens/second\n ** VRAM, Inference: [cuda:0] 2,254.17\
    \ MB\n ** VRAM, Total: [cuda:0] 8,944.13 MB\n -- Loading dataset...\n -- Testing..........\n\
    \ ** Perplexity: 6.3615\n```\n\nI am using the latest current cuda branch for\
    \ quantizing -- what branch are you using? (Do you think it matters?) Thanks!"
  created_at: 2023-05-28 15:38:27+00:00
  edited: true
  hidden: false
  id: 647383837afa69c3c7a6bac8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-28T20:45:56.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''m currently quantising with the <a rel="nofollow" href="https://github.com/oobabooga/GPTQ-for-LLaMa">oobabooga
          GPTQ-for-LLaMA fork</a>, to ensure compatibility for as many users as possible.</p>

          <p>In the near future I expect to move to quantising with AutoGPTQ instead,
          when it is mature enough to be the default for users.</p>

          <p>That''s odd that you would get lower performance when quantising yourself
          versus the file I uploaded.  Normally performance is affected by the code
          used to do inference, not by the code used to quantise in the first place.</p>

          <p>Which GPTQ-for-LLaMa did you use specifically?</p>

          '
        raw: 'I''m currently quantising with the [oobabooga GPTQ-for-LLaMA fork](https://github.com/oobabooga/GPTQ-for-LLaMa),
          to ensure compatibility for as many users as possible.


          In the near future I expect to move to quantising with AutoGPTQ instead,
          when it is mature enough to be the default for users.


          That''s odd that you would get lower performance when quantising yourself
          versus the file I uploaded.  Normally performance is affected by the code
          used to do inference, not by the code used to quantise in the first place.


          Which GPTQ-for-LLaMa did you use specifically?'
        updatedAt: '2023-05-28T20:45:56.755Z'
      numEdits: 0
      reactions: []
    id: 6473bd846cff2f86720a2845
    type: comment
  author: TheBloke
  content: 'I''m currently quantising with the [oobabooga GPTQ-for-LLaMA fork](https://github.com/oobabooga/GPTQ-for-LLaMa),
    to ensure compatibility for as many users as possible.


    In the near future I expect to move to quantising with AutoGPTQ instead, when
    it is mature enough to be the default for users.


    That''s odd that you would get lower performance when quantising yourself versus
    the file I uploaded.  Normally performance is affected by the code used to do
    inference, not by the code used to quantise in the first place.


    Which GPTQ-for-LLaMa did you use specifically?'
  created_at: 2023-05-28 19:45:56+00:00
  edited: false
  hidden: false
  id: 6473bd846cff2f86720a2845
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-05-29T00:57:05.000Z'
    data:
      edited: false
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: '<p>I was using the master cuda branch from qwopqwop200/GPTQ-for-LLaMa,
          I think. </p>

          <p>Pulling the Ooba cuda branch of GPTQ-for-llama and requantizing with
          the same command gave results that matched yours - Thank you!</p>

          <p>I am going to pull qwopqwop200''s latest cuda/triton branches and try
          again to see if I can replicate the slow results.</p>

          '
        raw: "I was using the master cuda branch from qwopqwop200/GPTQ-for-LLaMa,\
          \ I think. \n\nPulling the Ooba cuda branch of GPTQ-for-llama and requantizing\
          \ with the same command gave results that matched yours - Thank you!\n\n\
          I am going to pull qwopqwop200's latest cuda/triton branches and try again\
          \ to see if I can replicate the slow results."
        updatedAt: '2023-05-29T00:57:05.075Z'
      numEdits: 0
      reactions: []
    id: 6473f8616cff2f86720e52b7
    type: comment
  author: disarmyouwitha
  content: "I was using the master cuda branch from qwopqwop200/GPTQ-for-LLaMa, I\
    \ think. \n\nPulling the Ooba cuda branch of GPTQ-for-llama and requantizing with\
    \ the same command gave results that matched yours - Thank you!\n\nI am going\
    \ to pull qwopqwop200's latest cuda/triton branches and try again to see if I\
    \ can replicate the slow results."
  created_at: 2023-05-28 23:57:05+00:00
  edited: false
  hidden: false
  id: 6473f8616cff2f86720e52b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-29T00:59:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK yeah CUDA branch is known to be slow.  Either try the ooba text-gen-ui
          CUDA fork I listed, or try the latest Triton AutoGPTQ (if you''re on Linux)</p>

          <p>Or switch to AutoGPTQ which is the future anyway</p>

          <p>What are you trying to do specifically? Like are you quantising just
          to test, or quantising because you want quantised models for yourself? Or
          because you want to quantise models for others to use?</p>

          <p>Depending on what you want to do will vary my answer as to what I''d
          recommend. So let me know and I can advise more tomorrow.</p>

          '
        raw: 'OK yeah CUDA branch is known to be slow.  Either try the ooba text-gen-ui
          CUDA fork I listed, or try the latest Triton AutoGPTQ (if you''re on Linux)


          Or switch to AutoGPTQ which is the future anyway


          What are you trying to do specifically? Like are you quantising just to
          test, or quantising because you want quantised models for yourself? Or because
          you want to quantise models for others to use?


          Depending on what you want to do will vary my answer as to what I''d recommend.
          So let me know and I can advise more tomorrow.'
        updatedAt: '2023-05-29T00:59:51.379Z'
      numEdits: 0
      reactions: []
    id: 6473f90763001a0002d4d9ff
    type: comment
  author: TheBloke
  content: 'OK yeah CUDA branch is known to be slow.  Either try the ooba text-gen-ui
    CUDA fork I listed, or try the latest Triton AutoGPTQ (if you''re on Linux)


    Or switch to AutoGPTQ which is the future anyway


    What are you trying to do specifically? Like are you quantising just to test,
    or quantising because you want quantised models for yourself? Or because you want
    to quantise models for others to use?


    Depending on what you want to do will vary my answer as to what I''d recommend.
    So let me know and I can advise more tomorrow.'
  created_at: 2023-05-28 23:59:51+00:00
  edited: false
  hidden: false
  id: 6473f90763001a0002d4d9ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-05-29T01:17:49.000Z'
    data:
      edited: false
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: '<p>Well, 2 reasons - I would like to be able to quantize models for
          myself just incase you don''t have it up</p>

          <p>But what has really pulled me down this rabbit hole is that I am using
          this code for inference:<br><a rel="nofollow" href="https://github.com/turboderp/exllama">https://github.com/turboderp/exllama</a></p>

          <p>It''s lighting fast custom CUDA voodoo as far as I can tell. It is much
          faster than any of the other branches I have tested (Though I haven''t tried
          AutoGPTQ.. If you have resources?)</p>

          <p>BUT.. On my go-to model (TheBloke/koala-13B-GPTQ-4bit-128g) I was only
          getting 50 tokens/sec compared to the creators 80-90 t/s, and he mentioned
          PyTorch has a CPU bottleneck so I just thought it was that</p>

          <p>UNTIL I tried your (TheBloke/Project-Baize-v2-13B-GPTQ) model and got
          70 tokens/sec and then I tried (TheBloke/guanaco-13B-GPTQ) ~70 tokens/sec..
          so I discovered it had something to do with the model.</p>

          <p>So, I grabbed the HF model and quantized it myself only to get 50 tokens/sec,
          just like I had got quantizing my own koala.</p>

          <p>Using the repo you listed I am getting very consistently fast speeds,
          so thank you!</p>

          '
        raw: 'Well, 2 reasons - I would like to be able to quantize models for myself
          just incase you don''t have it up


          But what has really pulled me down this rabbit hole is that I am using this
          code for inference:

          https://github.com/turboderp/exllama


          It''s lighting fast custom CUDA voodoo as far as I can tell. It is much
          faster than any of the other branches I have tested (Though I haven''t tried
          AutoGPTQ.. If you have resources?)


          BUT.. On my go-to model (TheBloke/koala-13B-GPTQ-4bit-128g) I was only getting
          50 tokens/sec compared to the creators 80-90 t/s, and he mentioned PyTorch
          has a CPU bottleneck so I just thought it was that


          UNTIL I tried your (TheBloke/Project-Baize-v2-13B-GPTQ) model and got 70
          tokens/sec and then I tried (TheBloke/guanaco-13B-GPTQ) ~70 tokens/sec..
          so I discovered it had something to do with the model.


          So, I grabbed the HF model and quantized it myself only to get 50 tokens/sec,
          just like I had got quantizing my own koala.


          Using the repo you listed I am getting very consistently fast speeds, so
          thank you!'
        updatedAt: '2023-05-29T01:17:49.958Z'
      numEdits: 0
      reactions: []
    id: 6473fd3d2a74fb43cce82346
    type: comment
  author: disarmyouwitha
  content: 'Well, 2 reasons - I would like to be able to quantize models for myself
    just incase you don''t have it up


    But what has really pulled me down this rabbit hole is that I am using this code
    for inference:

    https://github.com/turboderp/exllama


    It''s lighting fast custom CUDA voodoo as far as I can tell. It is much faster
    than any of the other branches I have tested (Though I haven''t tried AutoGPTQ..
    If you have resources?)


    BUT.. On my go-to model (TheBloke/koala-13B-GPTQ-4bit-128g) I was only getting
    50 tokens/sec compared to the creators 80-90 t/s, and he mentioned PyTorch has
    a CPU bottleneck so I just thought it was that


    UNTIL I tried your (TheBloke/Project-Baize-v2-13B-GPTQ) model and got 70 tokens/sec
    and then I tried (TheBloke/guanaco-13B-GPTQ) ~70 tokens/sec.. so I discovered
    it had something to do with the model.


    So, I grabbed the HF model and quantized it myself only to get 50 tokens/sec,
    just like I had got quantizing my own koala.


    Using the repo you listed I am getting very consistently fast speeds, so thank
    you!'
  created_at: 2023-05-29 00:17:49+00:00
  edited: false
  hidden: false
  id: 6473fd3d2a74fb43cce82346
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-05-29T06:38:07.000Z'
    data:
      edited: true
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: '<p>A bit of follow up:</p>

          <p>I requantized the weights 3 times, using Ooba_cuda, Qwop_cuda, and Qwop_triton
          and Ooba_cuda was the only one to hit this speed~</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64123681ac08ffb707922dfb/X_GOD8y9cwDx1k6yTPvUm.png"><img
          alt="Screenshot 2023-05-29 at 1.06.55 AM.png" src="https://cdn-uploads.huggingface.co/production/uploads/64123681ac08ffb707922dfb/X_GOD8y9cwDx1k6yTPvUm.png"></a></p>

          <p>I was able to get the <code>basic_example.py</code> code to work for
          AutoGPTQ so I will check this out as well~<br>I like that it already supports
          quantization of Falcon (and so many other models!)</p>

          <p>Thanks, again!</p>

          '
        raw: "A bit of follow up:\n\nI requantized the weights 3 times, using Ooba_cuda,\
          \ Qwop_cuda, and Qwop_triton and Ooba_cuda was the only one to hit this\
          \ speed~\n\n![Screenshot 2023-05-29 at 1.06.55 AM.png](https://cdn-uploads.huggingface.co/production/uploads/64123681ac08ffb707922dfb/X_GOD8y9cwDx1k6yTPvUm.png)\n\
          \nI was able to get the `basic_example.py` code to work for AutoGPTQ so\
          \ I will check this out as well~ \nI like that it already supports quantization\
          \ of Falcon (and so many other models!)\n\nThanks, again!"
        updatedAt: '2023-05-29T06:38:59.953Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - barl2021
        - wj2015
      relatedEventId: 6474484fd56974d0c055b0a8
    id: 6474484fd56974d0c055b0a7
    type: comment
  author: disarmyouwitha
  content: "A bit of follow up:\n\nI requantized the weights 3 times, using Ooba_cuda,\
    \ Qwop_cuda, and Qwop_triton and Ooba_cuda was the only one to hit this speed~\n\
    \n![Screenshot 2023-05-29 at 1.06.55 AM.png](https://cdn-uploads.huggingface.co/production/uploads/64123681ac08ffb707922dfb/X_GOD8y9cwDx1k6yTPvUm.png)\n\
    \nI was able to get the `basic_example.py` code to work for AutoGPTQ so I will\
    \ check this out as well~ \nI like that it already supports quantization of Falcon\
    \ (and so many other models!)\n\nThanks, again!"
  created_at: 2023-05-29 05:38:07+00:00
  edited: true
  hidden: false
  id: 6474484fd56974d0c055b0a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-05-29T06:38:07.000Z'
    data:
      status: closed
    id: 6474484fd56974d0c055b0a8
    type: status-change
  author: disarmyouwitha
  created_at: 2023-05-29 05:38:07+00:00
  id: 6474484fd56974d0c055b0a8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/guanaco-13B-HF
repo_type: model
status: closed
target_branch: null
title: What is your environment like for quantizing models?
