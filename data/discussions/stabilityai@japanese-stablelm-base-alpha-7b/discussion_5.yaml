!!python/object:huggingface_hub.community.DiscussionWithDetails
author: konbraphat51
conflicting_files: null
created_at: 2023-08-19 17:46:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-UlFWOcBW4ffyyF81ULPT.png?w=200&h=200&f=face
      fullname: konbraphat51
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: konbraphat51
      type: user
    createdAt: '2023-08-19T18:46:53.000Z'
    data:
      edited: false
      editors:
      - konbraphat51
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5369614362716675
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-UlFWOcBW4ffyyF81ULPT.png?w=200&h=200&f=face
          fullname: konbraphat51
          isHf: false
          isPro: false
          name: konbraphat51
          type: user
        html: "<p>I tried fine tune this model by PEFT library and Trainer from transformers\
          \ library</p>\n<p>I got this error:</p>\n<blockquote>\n<p>japanese-stablelm-base-alpha-7b\\\
          69e1599948909cccca2369385fb6c82ef59086f4\\modeling_japanese_stablelm_alpha.py\"\
          , line 383, in apply_rotary_pos_emb<br>   q_embed = (q * cos) + (rotate_half(q)\
          \ * sin)<br>RuntimeError: The size of tensor a (32) must match the size\
          \ of tensor b (64) at non-singleton dimension 3</p>\n</blockquote>\n<p>The\
          \ loading of the model is:</p>\n<pre><code>tokenizer = LlamaTokenizer.from_pretrained(\"\
          novelai/nerdstash-tokenizer-v1\", load_in_8bit=self.finetuner_properties.useint8,\
          \ device_map=\"auto\")\n\nlm_model = AutoModelForCausalLM.from_pretrained(\"\
          stabilityai/japanese-stablelm-base-alpha-7b\", load_in_8bit=true, device_map=\"\
          auto\", trust_remote_code=True)\n</code></pre>\n<p>My arguments are:</p>\n\
          <pre><code>#for LoraConfig\n        lora_r:int = 8,\n        lora_alpha:int\
          \ = 16,\n        lora_dropout:float = 0.05,\n        lora_bias:str = \"\
          none\",\n        \n#for Trainer\nta_epochs:int = 4,\n        ta_logging_steps:int\
          \ = 200,\n        ta_save_steps:int = 100000,\n        ta_save_total_limit:int\
          \ = 3,\n        ta_train_batch_size:int = 8,\n        ta_warmup_steps:int\
          \ = 200,\n        ta_weight_decay:float = 0.1,\n        ta_learning_rate:float\
          \ = 5e-4,\n</code></pre>\n<p>What's wrong? This model unable to fine-tuning\
          \ in int8 condition?</p>\n"
        raw: "I tried fine tune this model by PEFT library and Trainer from transformers\
          \ library\r\n\r\nI got this error:\r\n>japanese-stablelm-base-alpha-7b\\\
          69e1599948909cccca2369385fb6c82ef59086f4\\modeling_japanese_stablelm_alpha.py\"\
          , line 383, in apply_rotary_pos_emb    \r\n>    q_embed = (q * cos) + (rotate_half(q)\
          \ * sin)\r\n>RuntimeError: The size of tensor a (32) must match the size\
          \ of tensor b (64) at non-singleton dimension 3\r\n\r\nThe loading of the\
          \ model is:\r\n```\r\ntokenizer = LlamaTokenizer.from_pretrained(\"novelai/nerdstash-tokenizer-v1\"\
          , load_in_8bit=self.finetuner_properties.useint8, device_map=\"auto\")\r\
          \n\r\nlm_model = AutoModelForCausalLM.from_pretrained(\"stabilityai/japanese-stablelm-base-alpha-7b\"\
          , load_in_8bit=true, device_map=\"auto\", trust_remote_code=True)\r\n```\r\
          \n\r\nMy arguments are:\r\n```\r\n#for LoraConfig\r\n        lora_r:int\
          \ = 8,\r\n        lora_alpha:int = 16,\r\n        lora_dropout:float = 0.05,\r\
          \n        lora_bias:str = \"none\",\r\n        \r\n#for Trainer\r\nta_epochs:int\
          \ = 4,\r\n        ta_logging_steps:int = 200,\r\n        ta_save_steps:int\
          \ = 100000,\r\n        ta_save_total_limit:int = 3,\r\n        ta_train_batch_size:int\
          \ = 8,\r\n        ta_warmup_steps:int = 200,\r\n        ta_weight_decay:float\
          \ = 0.1,\r\n        ta_learning_rate:float = 5e-4,\r\n```\r\n\r\nWhat's\
          \ wrong? This model unable to fine-tuning in int8 condition?"
        updatedAt: '2023-08-19T18:46:53.462Z'
      numEdits: 0
      reactions: []
    id: 64e10e1dcccd823564f2e752
    type: comment
  author: konbraphat51
  content: "I tried fine tune this model by PEFT library and Trainer from transformers\
    \ library\r\n\r\nI got this error:\r\n>japanese-stablelm-base-alpha-7b\\69e1599948909cccca2369385fb6c82ef59086f4\\\
    modeling_japanese_stablelm_alpha.py\", line 383, in apply_rotary_pos_emb    \r\
    \n>    q_embed = (q * cos) + (rotate_half(q) * sin)\r\n>RuntimeError: The size\
    \ of tensor a (32) must match the size of tensor b (64) at non-singleton dimension\
    \ 3\r\n\r\nThe loading of the model is:\r\n```\r\ntokenizer = LlamaTokenizer.from_pretrained(\"\
    novelai/nerdstash-tokenizer-v1\", load_in_8bit=self.finetuner_properties.useint8,\
    \ device_map=\"auto\")\r\n\r\nlm_model = AutoModelForCausalLM.from_pretrained(\"\
    stabilityai/japanese-stablelm-base-alpha-7b\", load_in_8bit=true, device_map=\"\
    auto\", trust_remote_code=True)\r\n```\r\n\r\nMy arguments are:\r\n```\r\n#for\
    \ LoraConfig\r\n        lora_r:int = 8,\r\n        lora_alpha:int = 16,\r\n  \
    \      lora_dropout:float = 0.05,\r\n        lora_bias:str = \"none\",\r\n   \
    \     \r\n#for Trainer\r\nta_epochs:int = 4,\r\n        ta_logging_steps:int =\
    \ 200,\r\n        ta_save_steps:int = 100000,\r\n        ta_save_total_limit:int\
    \ = 3,\r\n        ta_train_batch_size:int = 8,\r\n        ta_warmup_steps:int\
    \ = 200,\r\n        ta_weight_decay:float = 0.1,\r\n        ta_learning_rate:float\
    \ = 5e-4,\r\n```\r\n\r\nWhat's wrong? This model unable to fine-tuning in int8\
    \ condition?"
  created_at: 2023-08-19 17:46:53+00:00
  edited: false
  hidden: false
  id: 64e10e1dcccd823564f2e752
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-UlFWOcBW4ffyyF81ULPT.png?w=200&h=200&f=face
      fullname: konbraphat51
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: konbraphat51
      type: user
    createdAt: '2023-08-19T19:28:09.000Z'
    data:
      edited: false
      editors:
      - konbraphat51
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9973456263542175
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-UlFWOcBW4ffyyF81ULPT.png?w=200&h=200&f=face
          fullname: konbraphat51
          isHf: false
          isPro: false
          name: konbraphat51
          type: user
        html: '<p>I also tried <code>variant="int8"</code> but didn''t solve</p>

          '
        raw: I also tried `variant="int8"` but didn't solve
        updatedAt: '2023-08-19T19:28:09.535Z'
      numEdits: 0
      reactions: []
    id: 64e117c958076dcc987feb6f
    type: comment
  author: konbraphat51
  content: I also tried `variant="int8"` but didn't solve
  created_at: 2023-08-19 18:28:09+00:00
  edited: false
  hidden: false
  id: 64e117c958076dcc987feb6f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: stabilityai/japanese-stablelm-base-alpha-7b
repo_type: model
status: open
target_branch: null
title: Fine tuning by LoRA
