!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mahranxo
conflicting_files: null
created_at: 2023-09-16 19:09:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9dd2438998e87ec6876d430e310f4466.svg
      fullname: Ali Mahran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mahranxo
      type: user
    createdAt: '2023-09-16T20:09:46.000Z'
    data:
      edited: false
      editors:
      - mahranxo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9602323174476624
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9dd2438998e87ec6876d430e310f4466.svg
          fullname: Ali Mahran
          isHf: false
          isPro: false
          name: mahranxo
          type: user
        html: '<p>i downloaded the model but when i try to load it the RAM can''t
          fit it</p>

          '
        raw: i downloaded the model but when i try to load it the RAM can't fit it
        updatedAt: '2023-09-16T20:09:46.352Z'
      numEdits: 0
      reactions: []
    id: 65060b8a423b46492ea83e13
    type: comment
  author: mahranxo
  content: i downloaded the model but when i try to load it the RAM can't fit it
  created_at: 2023-09-16 19:09:46+00:00
  edited: false
  hidden: false
  id: 65060b8a423b46492ea83e13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/626237d9bbcbd1c34f1bb231/EJrOjvAL-68qMCYdnvOrq.png?w=200&h=200&f=face
      fullname: Ali Elfilali
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Ali-C137
      type: user
    createdAt: '2023-09-16T20:13:09.000Z'
    data:
      edited: false
      editors:
      - Ali-C137
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.23563359677791595
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/626237d9bbcbd1c34f1bb231/EJrOjvAL-68qMCYdnvOrq.png?w=200&h=200&f=face
          fullname: Ali Elfilali
          isHf: false
          isPro: false
          name: Ali-C137
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mahranxo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mahranxo\">@<span class=\"\
          underline\">mahranxo</span></a></span>\n\n\t</span></span> what GPU are\
          \ you using ?</p>\n"
        raw: '@mahranxo what GPU are you using ?'
        updatedAt: '2023-09-16T20:13:09.064Z'
      numEdits: 0
      reactions: []
    id: 65060c554a8839a8bd6c4038
    type: comment
  author: Ali-C137
  content: '@mahranxo what GPU are you using ?'
  created_at: 2023-09-16 19:13:09+00:00
  edited: false
  hidden: false
  id: 65060c554a8839a8bd6c4038
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9dd2438998e87ec6876d430e310f4466.svg
      fullname: Ali Mahran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mahranxo
      type: user
    createdAt: '2023-09-16T21:12:58.000Z'
    data:
      edited: true
      editors:
      - mahranxo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48513463139533997
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9dd2438998e87ec6876d430e310f4466.svg
          fullname: Ali Mahran
          isHf: false
          isPro: false
          name: mahranxo
          type: user
        html: "<p>Nvidia RTX 3050.</p>\n<p>Update:<br>i loaded the model using the\
          \ following code:<br>model = AutoModelForCausalLM.from_pretrained(r\"Jais\"\
          ,device_map = 'cpu',offload_folder=\"offload\", offload_state_dict = True,trust_remote_code=True)<br>but\
          \ when i try to run inference this happens:</p>\n<hr>\n<pre><code>  9 inputs\
          \ = input_ids.to(device)\n 10 input_len = inputs.shape[-1]\n</code></pre>\n\
          <p>---&gt; 11 generate_ids = model.generate(<br>     12     inputs,<br>\
          \     13     top_p=0.9,<br>     14     temperature=0.3,<br>     15     max_length=2048-input_len,<br>\
          \     16     min_length=input_len + 4,<br>     17     repetition_penalty=1.2,<br>\
          \     18     do_sample=True,<br>     19 )<br>     20 response = tokenizer.batch_decode(<br>\
          \     21     generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True<br>\
          \     22 )[0]<br>     23 response = response.split(\"### Response: [|AI|]\"\
          )<br>...<br>   2513         layer_norm, (input, weight, bias), input, normalized_shape,\
          \ weight=weight, bias=bias, eps=eps<br>   2514     )<br>-&gt; 2515 return\
          \ torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)</p>\n\
          <h2 id=\"runtimeerror-layernormkernelimpl-not-implemented-for-half\">RuntimeError:\
          \ \"LayerNormKernelImpl\" not implemented for 'Half' </h2>\n"
        raw: "Nvidia RTX 3050.\n\nUpdate: \ni loaded the model using the following\
          \ code:\nmodel = AutoModelForCausalLM.from_pretrained(r\"Jais\",device_map\
          \ = 'cpu',offload_folder=\"offload\", offload_state_dict = True,trust_remote_code=True)\n\
          but when i try to run inference this happens:\n---\n      9 inputs = input_ids.to(device)\n\
          \     10 input_len = inputs.shape[-1]\n---> 11 generate_ids = model.generate(\n\
          \     12     inputs,\n     13     top_p=0.9,\n     14     temperature=0.3,\n\
          \     15     max_length=2048-input_len,\n     16     min_length=input_len\
          \ + 4,\n     17     repetition_penalty=1.2,\n     18     do_sample=True,\n\
          \     19 )\n     20 response = tokenizer.batch_decode(\n     21     generate_ids,\
          \ skip_special_tokens=True, clean_up_tokenization_spaces=True\n     22 )[0]\n\
          \     23 response = response.split(\"### Response: [|AI|]\")\n...\n   2513\
          \         layer_norm, (input, weight, bias), input, normalized_shape, weight=weight,\
          \ bias=bias, eps=eps\n   2514     )\n-> 2515 return torch.layer_norm(input,\
          \ normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n\n\
          RuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half' \n---"
        updatedAt: '2023-09-16T21:13:27.945Z'
      numEdits: 1
      reactions: []
    id: 65061a5a7de4710bafd76fbc
    type: comment
  author: mahranxo
  content: "Nvidia RTX 3050.\n\nUpdate: \ni loaded the model using the following code:\n\
    model = AutoModelForCausalLM.from_pretrained(r\"Jais\",device_map = 'cpu',offload_folder=\"\
    offload\", offload_state_dict = True,trust_remote_code=True)\nbut when i try to\
    \ run inference this happens:\n---\n      9 inputs = input_ids.to(device)\n  \
    \   10 input_len = inputs.shape[-1]\n---> 11 generate_ids = model.generate(\n\
    \     12     inputs,\n     13     top_p=0.9,\n     14     temperature=0.3,\n \
    \    15     max_length=2048-input_len,\n     16     min_length=input_len + 4,\n\
    \     17     repetition_penalty=1.2,\n     18     do_sample=True,\n     19 )\n\
    \     20 response = tokenizer.batch_decode(\n     21     generate_ids, skip_special_tokens=True,\
    \ clean_up_tokenization_spaces=True\n     22 )[0]\n     23 response = response.split(\"\
    ### Response: [|AI|]\")\n...\n   2513         layer_norm, (input, weight, bias),\
    \ input, normalized_shape, weight=weight, bias=bias, eps=eps\n   2514     )\n\
    -> 2515 return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n\
    \nRuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half' \n---"
  created_at: 2023-09-16 20:12:58+00:00
  edited: true
  hidden: false
  id: 65061a5a7de4710bafd76fbc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/626237d9bbcbd1c34f1bb231/EJrOjvAL-68qMCYdnvOrq.png?w=200&h=200&f=face
      fullname: Ali Elfilali
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Ali-C137
      type: user
    createdAt: '2023-09-18T19:24:28.000Z'
    data:
      edited: true
      editors:
      - Ali-C137
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9317365884780884
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/626237d9bbcbd1c34f1bb231/EJrOjvAL-68qMCYdnvOrq.png?w=200&h=200&f=face
          fullname: Ali Elfilali
          isHf: false
          isPro: false
          name: Ali-C137
          type: user
        html: '<p>generally the CPU memory is used to offload the chunks of the model
          and not process it , the whole calculation should stays in the GPU, apparently
          this layer "LayerNormKernelImpl" can''t fit into your GPU memory even if
          the rest of the model is offloaded in the CPU (maybe) ! i suggest to use
          some cloud solution instead ! Also i admit there is something off with the
          implementation of this model ! even i could''t load it in a T4 with bnb
          nf4 quantization ! i had to upgrade to an A100 40GB. Good Luck and i hope
          i helped a little </p>

          <p>PS : please don''t forget to close this issue when you resolve the problem.</p>

          '
        raw: "generally the CPU memory is used to offload the chunks of the model\
          \ and not process it , the whole calculation should stays in the GPU, apparently\
          \ this layer \"LayerNormKernelImpl\" can't fit into your GPU memory even\
          \ if the rest of the model is offloaded in the CPU (maybe) ! i suggest to\
          \ use some cloud solution instead ! Also i admit there is something off\
          \ with the implementation of this model ! even i could't load it in a T4\
          \ with bnb nf4 quantization ! i had to upgrade to an A100 40GB. Good Luck\
          \ and i hope i helped a little \n\nPS : please don't forget to close this\
          \ issue when you resolve the problem."
        updatedAt: '2023-09-18T22:17:27.791Z'
      numEdits: 2
      reactions: []
    id: 6508a3ec7ee07e274b1744a6
    type: comment
  author: Ali-C137
  content: "generally the CPU memory is used to offload the chunks of the model and\
    \ not process it , the whole calculation should stays in the GPU, apparently this\
    \ layer \"LayerNormKernelImpl\" can't fit into your GPU memory even if the rest\
    \ of the model is offloaded in the CPU (maybe) ! i suggest to use some cloud solution\
    \ instead ! Also i admit there is something off with the implementation of this\
    \ model ! even i could't load it in a T4 with bnb nf4 quantization ! i had to\
    \ upgrade to an A100 40GB. Good Luck and i hope i helped a little \n\nPS : please\
    \ don't forget to close this issue when you resolve the problem."
  created_at: 2023-09-18 18:24:28+00:00
  edited: true
  hidden: false
  id: 6508a3ec7ee07e274b1744a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9dd2438998e87ec6876d430e310f4466.svg
      fullname: Ali Mahran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mahranxo
      type: user
    createdAt: '2023-09-18T21:38:55.000Z'
    data:
      edited: false
      editors:
      - mahranxo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.942927360534668
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9dd2438998e87ec6876d430e310f4466.svg
          fullname: Ali Mahran
          isHf: false
          isPro: false
          name: mahranxo
          type: user
        html: '<p>Thank you for your help and i hope the creators of this models release
          a smaller version of the models that can run on any device</p>

          '
        raw: Thank you for your help and i hope the creators of this models release
          a smaller version of the models that can run on any device
        updatedAt: '2023-09-18T21:38:55.482Z'
      numEdits: 0
      reactions: []
    id: 6508c36f7e0d56c2714c9164
    type: comment
  author: mahranxo
  content: Thank you for your help and i hope the creators of this models release
    a smaller version of the models that can run on any device
  created_at: 2023-09-18 20:38:55+00:00
  edited: false
  hidden: false
  id: 6508c36f7e0d56c2714c9164
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9dd2438998e87ec6876d430e310f4466.svg
      fullname: Ali Mahran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mahranxo
      type: user
    createdAt: '2023-09-18T21:39:00.000Z'
    data:
      status: closed
    id: 6508c374f36bb51c50fd035b
    type: status-change
  author: mahranxo
  created_at: 2023-09-18 20:39:00+00:00
  id: 6508c374f36bb51c50fd035b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: asas-ai/jais-13b-chat-8bit
repo_type: model
status: closed
target_branch: null
title: 'Can''t load the model for inference '
