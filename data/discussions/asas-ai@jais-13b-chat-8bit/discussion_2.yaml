!!python/object:huggingface_hub.community.DiscussionWithDetails
author: luogen
conflicting_files: null
created_at: 2023-10-17 07:41:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/038741dc09346826a817cce11679825c.svg
      fullname: wei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luogen
      type: user
    createdAt: '2023-10-17T08:41:32.000Z'
    data:
      edited: false
      editors:
      - luogen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5503142476081848
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/038741dc09346826a817cce11679825c.svg
          fullname: wei
          isHf: false
          isPro: false
          name: luogen
          type: user
        html: "<p>My computer does not have a GPU, so the CPU should be used for calculations.\
          \ But it will report an error every time I run it.<br>Below is the output\
          \ error\uFF1A<br>===================================BUG REPORT===================================<br>Welcome\
          \ to bitsandbytes. For bug reports, please run</p>\n<p>python -m bitsandbytes</p>\n\
          <h1 id=\"and-submit-this-information-together-with-your-error-trace-to-httpsgithubcomtimdettmersbitsandbytesissues\"\
          > and submit this information together with your error trace to: <a rel=\"\
          nofollow\" href=\"https://github.com/TimDettmers/bitsandbytes/issues\">https://github.com/TimDettmers/bitsandbytes/issues</a></h1>\n\
          <p>bin F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\bitsandbytes\\\
          libbitsandbytes_cpu.so<br>F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\\
          bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes\
          \ was compiled without GPU support. 8-bit optimizers, 8-bit multiplication,\
          \ and GPU quantization are unavailable.<br>  warn(\"The installed version\
          \ of bitsandbytes was compiled without GPU support. \"<br>'NoneType' object\
          \ has no attribute 'cadam32bit_grad_fp32'<br>CUDA SETUP: Loading binary\
          \ F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...<br>argument\
          \ of type 'WindowsPath' is not iterable<br>Detected the presence of a <code>quantization_config</code>\
          \ attribute in the model's configuration but you don't have the correct\
          \ <code>bitsandbytes</code> version to support int8 serialization. Please\
          \ install the latest version of <code>bitsandbytes</code> with  <code>pip\
          \ install --upgrade bitsandbytes</code>.<br>Loading checkpoint shards: \
          \  0%|                                                                 |\
          \ 0/2 [00:07&lt;?, ?it/s]<br>Traceback (most recent call last):<br>  File\
          \ \"jais8bit.py\", line 18, in <br>    model = AutoModelForCausalLM.from_pretrained(\"\
          F:\\code\\chat-py\\jais-13b-chat-8bit\", device_map=\"auto\", offload_folder=\"\
          offload\", trust_remote_code=True)<br>  File \"F:\\anaconda\\envs\\godel-env\\\
          lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line\
          \ 560, in from_pretrained<br>    return model_class.from_pretrained(<br>\
          \  File \"F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\", line 3307, in from_pretrained<br>    ) = cls._load_pretrained_model(<br>\
          \  File \"F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\", line 3695, in _load_pretrained_model<br>    new_error_msgs,\
          \ offload_index, state_dict_index = _load_state_dict_into_meta_model(<br>\
          \  File \"F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\", line 749, in _load_state_dict_into_meta_model<br> \
          \   set_module_quantized_tensor_to_device(<br>  File \"F:\\anaconda\\envs\\\
          godel-env\\lib\\site-packages\\transformers\\integrations\\bitsandbytes.py\"\
          , line 58, in set_module_quantized_tensor_to_device<br>    if old_value.device\
          \ == torch.device(\"meta\") and device not in [\"meta\", torch.device(\"\
          meta\")] and value is None:<br>NameError: name 'torch' is not defined</p>\n"
        raw: "My computer does not have a GPU, so the CPU should be used for calculations.\
          \ But it will report an error every time I run it.\r\nBelow is the output\
          \ error\uFF1A\r\n===================================BUG REPORT===================================\r\
          \nWelcome to bitsandbytes. For bug reports, please run\r\n\r\npython -m\
          \ bitsandbytes\r\n\r\n and submit this information together with your error\
          \ trace to: https://github.com/TimDettmers/bitsandbytes/issues\r\n================================================================================\r\
          \nbin F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\bitsandbytes\\\
          libbitsandbytes_cpu.so\r\nF:\\anaconda\\envs\\godel-env\\lib\\site-packages\\\
          bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes\
          \ was compiled without GPU support. 8-bit optimizers, 8-bit multiplication,\
          \ and GPU quantization are unavailable.\r\n  warn(\"The installed version\
          \ of bitsandbytes was compiled without GPU support. \"\r\n'NoneType' object\
          \ has no attribute 'cadam32bit_grad_fp32'\r\nCUDA SETUP: Loading binary\
          \ F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\r\
          \nargument of type 'WindowsPath' is not iterable\r\nDetected the presence\
          \ of a `quantization_config` attribute in the model's configuration but\
          \ you don't have the correct `bitsandbytes` version to support int8 serialization.\
          \ Please install the latest version of `bitsandbytes` with  `pip install\
          \ --upgrade bitsandbytes`.\r\nLoading checkpoint shards:   0%|         \
          \                                                        | 0/2 [00:07<?,\
          \ ?it/s]\r\nTraceback (most recent call last):\r\n  File \"jais8bit.py\"\
          , line 18, in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(\"\
          F:\\\\code\\\\chat-py\\\\jais-13b-chat-8bit\", device_map=\"auto\", offload_folder=\"\
          offload\", trust_remote_code=True)\r\n  File \"F:\\anaconda\\envs\\godel-env\\\
          lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line\
          \ 560, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
          \  File \"F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\", line 3307, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
          \n  File \"F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\", line 3695, in _load_pretrained_model\r\n    new_error_msgs,\
          \ offload_index, state_dict_index = _load_state_dict_into_meta_model(\r\n\
          \  File \"F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\", line 749, in _load_state_dict_into_meta_model\r\n \
          \   set_module_quantized_tensor_to_device(\r\n  File \"F:\\anaconda\\envs\\\
          godel-env\\lib\\site-packages\\transformers\\integrations\\bitsandbytes.py\"\
          , line 58, in set_module_quantized_tensor_to_device\r\n    if old_value.device\
          \ == torch.device(\"meta\") and device not in [\"meta\", torch.device(\"\
          meta\")] and value is None:\r\nNameError: name 'torch' is not defined"
        updatedAt: '2023-10-17T08:41:32.313Z'
      numEdits: 0
      reactions: []
    id: 652e48bc4f4fec02a77fdc3d
    type: comment
  author: luogen
  content: "My computer does not have a GPU, so the CPU should be used for calculations.\
    \ But it will report an error every time I run it.\r\nBelow is the output error\uFF1A\
    \r\n===================================BUG REPORT===================================\r\
    \nWelcome to bitsandbytes. For bug reports, please run\r\n\r\npython -m bitsandbytes\r\
    \n\r\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\r\
    \n================================================================================\r\
    \nbin F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so\r\
    \nF:\\anaconda\\envs\\godel-env\\lib\\site-packages\\bitsandbytes\\cextension.py:34:\
    \ UserWarning: The installed version of bitsandbytes was compiled without GPU\
    \ support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\
    \n  warn(\"The installed version of bitsandbytes was compiled without GPU support.\
    \ \"\r\n'NoneType' object has no attribute 'cadam32bit_grad_fp32'\r\nCUDA SETUP:\
    \ Loading binary F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\bitsandbytes\\\
    libbitsandbytes_cpu.so...\r\nargument of type 'WindowsPath' is not iterable\r\n\
    Detected the presence of a `quantization_config` attribute in the model's configuration\
    \ but you don't have the correct `bitsandbytes` version to support int8 serialization.\
    \ Please install the latest version of `bitsandbytes` with  `pip install --upgrade\
    \ bitsandbytes`.\r\nLoading checkpoint shards:   0%|                         \
    \                                        | 0/2 [00:07<?, ?it/s]\r\nTraceback (most\
    \ recent call last):\r\n  File \"jais8bit.py\", line 18, in <module>\r\n    model\
    \ = AutoModelForCausalLM.from_pretrained(\"F:\\\\code\\\\chat-py\\\\jais-13b-chat-8bit\"\
    , device_map=\"auto\", offload_folder=\"offload\", trust_remote_code=True)\r\n\
    \  File \"F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\transformers\\models\\\
    auto\\auto_factory.py\", line 560, in from_pretrained\r\n    return model_class.from_pretrained(\r\
    \n  File \"F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\transformers\\modeling_utils.py\"\
    , line 3307, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File\
    \ \"F:\\anaconda\\envs\\godel-env\\lib\\site-packages\\transformers\\modeling_utils.py\"\
    , line 3695, in _load_pretrained_model\r\n    new_error_msgs, offload_index, state_dict_index\
    \ = _load_state_dict_into_meta_model(\r\n  File \"F:\\anaconda\\envs\\godel-env\\\
    lib\\site-packages\\transformers\\modeling_utils.py\", line 749, in _load_state_dict_into_meta_model\r\
    \n    set_module_quantized_tensor_to_device(\r\n  File \"F:\\anaconda\\envs\\\
    godel-env\\lib\\site-packages\\transformers\\integrations\\bitsandbytes.py\",\
    \ line 58, in set_module_quantized_tensor_to_device\r\n    if old_value.device\
    \ == torch.device(\"meta\") and device not in [\"meta\", torch.device(\"meta\"\
    )] and value is None:\r\nNameError: name 'torch' is not defined"
  created_at: 2023-10-17 07:41:32+00:00
  edited: false
  hidden: false
  id: 652e48bc4f4fec02a77fdc3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a781d9dbb9e288665ce943/cjCeFsfMnWVSulNVeEuqG.jpeg?w=200&h=200&f=face
      fullname: Saied Alshahrani
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: SaiedAlshahrani
      type: user
    createdAt: '2023-10-24T14:25:31.000Z'
    data:
      edited: false
      editors:
      - SaiedAlshahrani
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9075735807418823
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a781d9dbb9e288665ce943/cjCeFsfMnWVSulNVeEuqG.jpeg?w=200&h=200&f=face
          fullname: Saied Alshahrani
          isHf: false
          isPro: false
          name: SaiedAlshahrani
          type: user
        html: '<p>Have you tried to install <code>torch</code> (<code>pip install
          torch</code>)? If you have it installed already, try to import it as a package
          in your code (<code>import torch</code>).</p>

          '
        raw: Have you tried to install `torch` (`pip install torch`)? If you have
          it installed already, try to import it as a package in your code (`import
          torch`).
        updatedAt: '2023-10-24T14:25:31.764Z'
      numEdits: 0
      reactions: []
    id: 6537d3db568d8be8fa106f6b
    type: comment
  author: SaiedAlshahrani
  content: Have you tried to install `torch` (`pip install torch`)? If you have it
    installed already, try to import it as a package in your code (`import torch`).
  created_at: 2023-10-24 13:25:31+00:00
  edited: false
  hidden: false
  id: 6537d3db568d8be8fa106f6b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: asas-ai/jais-13b-chat-8bit
repo_type: model
status: open
target_branch: null
title: 'NameError: name ''torch'' is not defined'
