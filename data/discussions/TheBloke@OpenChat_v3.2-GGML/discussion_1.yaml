!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dranger003
conflicting_files: null
created_at: 2023-07-31 18:00:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4cd276aaa319b12d0beaf23c65630769.svg
      fullname: "DAN\u2122"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dranger003
      type: user
    createdAt: '2023-07-31T19:00:44.000Z'
    data:
      edited: true
      editors:
      - dranger003
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7476370930671692
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4cd276aaa319b12d0beaf23c65630769.svg
          fullname: "DAN\u2122"
          isHf: false
          isPro: false
          name: dranger003
          type: user
        html: '<p>All your models are working fine, but this one (tried openchat_v3.2.ggmlv3.q8_0.bin
          and openchat_v3.2.ggmlv3.q4_K_M.bin) is giving me an error (see below) -
          this is using the latest llama.cpp compiled from source:<br>GGML_ASSERT:
          C:\llama.cpp\ggml-cuda.cu:4749: i01_high == rows_per_iter || g_device_count
          &gt; 1</p>

          <p>Edit: something to do with CUDA as it works fine running on CPU only.
          This may be helpful, too (the proposed fix works for me although I inserted
          @ line 4744 instead of 4381 - probably not same commit):<br><a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/pull/2160#issuecomment-1657203763">https://github.com/ggerganov/llama.cpp/pull/2160#issuecomment-1657203763</a></p>

          <p>Edit #2: I tried converting from the source model and I get the same
          error. Could it be the 32002 vocab size maybe?</p>

          '
        raw: 'All your models are working fine, but this one (tried openchat_v3.2.ggmlv3.q8_0.bin
          and openchat_v3.2.ggmlv3.q4_K_M.bin) is giving me an error (see below) -
          this is using the latest llama.cpp compiled from source:

          GGML_ASSERT: C:\llama.cpp\ggml-cuda.cu:4749: i01_high == rows_per_iter ||
          g_device_count > 1


          Edit: something to do with CUDA as it works fine running on CPU only. This
          may be helpful, too (the proposed fix works for me although I inserted @
          line 4744 instead of 4381 - probably not same commit):

          https://github.com/ggerganov/llama.cpp/pull/2160#issuecomment-1657203763


          Edit #2: I tried converting from the source model and I get the same error.
          Could it be the 32002 vocab size maybe?'
        updatedAt: '2023-07-31T19:32:49.212Z'
      numEdits: 7
      reactions: []
    id: 64c804dc4515835c4d7aae9c
    type: comment
  author: dranger003
  content: 'All your models are working fine, but this one (tried openchat_v3.2.ggmlv3.q8_0.bin
    and openchat_v3.2.ggmlv3.q4_K_M.bin) is giving me an error (see below) - this
    is using the latest llama.cpp compiled from source:

    GGML_ASSERT: C:\llama.cpp\ggml-cuda.cu:4749: i01_high == rows_per_iter || g_device_count
    > 1


    Edit: something to do with CUDA as it works fine running on CPU only. This may
    be helpful, too (the proposed fix works for me although I inserted @ line 4744
    instead of 4381 - probably not same commit):

    https://github.com/ggerganov/llama.cpp/pull/2160#issuecomment-1657203763


    Edit #2: I tried converting from the source model and I get the same error. Could
    it be the 32002 vocab size maybe?'
  created_at: 2023-07-31 18:00:44+00:00
  edited: true
  hidden: false
  id: 64c804dc4515835c4d7aae9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-31T19:39:44.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8575283885002136
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Hmm, yes the non-standard vocab size has caused problems with CUDA\
          \ in GGML in the past.  But it's working fine for me with offloading all\
          \ layers:</p>\n<pre><code> [pytorch2] tomj@d442126f7dde:/workspace/git/llama.cpp\
          \ (master \u2714) \u1405 ./main -m /workspace/process/openchat_v3.2/ggml/openchat_v3.2.ggmlv3.q8_0.bin\
          \ -ngl 100 -t 1 -p \"GPT4 User: write a story about llamas&lt;|end_of_turn|&gt;GPT4\
          \ Assistant:\"\nmain: build = 933 (0728c5a)\nmain: seed  = 1690831897\n\
          ggml_init_cublas: found 4 CUDA devices:\n  Device 0: NVIDIA L40, compute\
          \ capability 8.9\n  Device 1: NVIDIA L40, compute capability 8.9\n  Device\
          \ 2: NVIDIA L40, compute capability 8.9\n  Device 3: NVIDIA L40, compute\
          \ capability 8.9\nllama.cpp: loading model from /workspace/process/openchat_v3.2/ggml/openchat_v3.2.ggmlv3.q8_0.bin\n\
          llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
          \ n_vocab    = 32002\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal:\
          \ n_embd     = 5120\nllama_model_load_internal: n_mult     = 6912\nllama_model_load_internal:\
          \ n_head     = 40\nllama_model_load_internal: n_head_kv  = 40\nllama_model_load_internal:\
          \ n_layer    = 40\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal:\
          \ n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 5.0e-06\nllama_model_load_internal:\
          \ n_ff       = 13824\nllama_model_load_internal: freq_base  = 10000.0\n\
          llama_model_load_internal: freq_scale = 1\nllama_model_load_internal: ftype\
          \      = 7 (mostly Q8_0)\nllama_model_load_internal: model size = 13B\n\
          llama_model_load_internal: ggml ctx size =    0.11 MB\nllama_model_load_internal:\
          \ using CUDA for GPU acceleration\nggml_cuda_set_main_device: using device\
          \ 0 (NVIDIA L40) as main device\nllama_model_load_internal: mem required\
          \  =  532.14 MB (+  400.00 MB per state)\nllama_model_load_internal: allocating\
          \ batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n\
          llama_model_load_internal: offloading 40 repeating layers to GPU\nllama_model_load_internal:\
          \ offloading non-repeating layers to GPU\nllama_model_load_internal: offloading\
          \ v cache to GPU\nllama_model_load_internal: offloading k cache to GPU\n\
          llama_model_load_internal: offloaded 43/43 layers to GPU\nllama_model_load_internal:\
          \ total VRAM used: 13784 MB\nllama_new_context_with_model: kv self size\
          \  =  400.00 MB\n\nsystem_info: n_threads = 1 / 256 | AVX = 1 | AVX2 = 1\
          \ | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0\
          \ | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3\
          \ = 1 | VSX = 0 |\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000,\
          \ presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40,\
          \ tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000,\
          \ mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate:\
          \ n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n GPT4 User:\
          \ write a story about llamas&lt;|end_of_turn|&gt;GPT4 Assistant: In the\
          \ high plains of Peru, nestled between the Andes Mountains and the sprawling\
          \ grasslands, lay a mysterious valley. For centuries, this secluded spot\
          \ had remained hidden from the outside world, its secrets kept by the ancient\
          \ Incas who once inhabited these lands. The valley was home to a small herd\
          \ of llamas, gentle creatures with long necks and comical faces, whose wool\
          \ provided warmth for the people of the Andes. But these llamas were not\
          \ just any ordinary animals; they held a special power that had been passed\
          \ down from generation to generation.\n\nOne day, two travelers stumbled\
          \ upon the valley while searching for a lost Incan city rumored to contain\
          \ untold riches and treasures. As they walked through the lush greenery,\
          \ they heard the sound of bells ringing in the distance. Curious, they followed\
          \ the melodic tune until they came upon a herd of llamas grazing peacefully.\
          \ But something about these llamas was different\u2014they were glowing\
          \ with an otherworldly light, and the travelers felt a sense of calmness\
          \ wash over them.\n\nOne of the travelers, a man named Jackson, approached\
          \ the llamas cautiously, unsure of what to expect. To his surprise, the\
          \ llamas didn't flee but instead allowed him to pet their soft fur. As he\
          \ did so, Jackson felt a strange tingling sensation course through his body.\
          \ Suddenly, he found himself understanding the llama's language as if it\
          \ were his own. The llamas were communicating with him, revealing that they\
          \ held the key to the lost Incan city.\n\nJackson and his companion, a woman\
          \ named Emily, spent the next few days learning from the wise llamas. They\
          \ discovered that the animals possessed ancient knowledge and wisdom, passed\
          \ down through generations, which allowed them to connect with the spiritual\
          \ realm. The llamas explained that they were guardians of the valley, protecting\
          \ its secrets until the right people came along.\n\nWith the help of the\
          \ llamas, Jackson and Emily finally found the lost city, hidden deep within\
          \ a labyrinth of caves. Inside, they discovered a treasure trove of artifacts,\
          \ each one holding a piece of the ancient Incan culture's history and wisdom.\
          \ The llamas had been right - there was a connection between them and the\
          \ Inca people that went beyond mere coincidence.\n\nAs Jackson and Emily\
          \ returned to their homes, they couldn't shake the sense that something\
          \ profound had happened. They knew that they had been changed by their encounter\
          \ with the llamas, and that this experience would stay with them for the\
          \ rest of their lives. And so, they continued to study and share the wisdom\
          \ of the Incan civilization, passing it on to future generations.\n\nIn\
          \ a world where technology often overshadows ancient traditions, the llamas\
          \ remind us that there is still much to learn from our past. Their presence\
          \ in the valley serves as a reminder to seek out knowledge and embrace the\
          \ wisdom of those who came before us. And who knows? Maybe one day, we too\
          \ might find ourselves standing face-to-face with a wise llama, ready to\
          \ share their ancient secrets.\n\n## Contributors\n\n* [@juliangmolina](https://twitter.com/juliangmolina)\
          \ - Story and Art\n* [@diegoberruecos](https://twitter.com/diegoberruecos)\
          \ - Illustrations\n* [@elisaferreira](https://twitter.com/elisaferreira)\
          \ - Layouts\n* [@julianaponzo](https://twitter.com/julianaponzo) - Proofreading\
          \ and Production\n [end of text]\n</code></pre>\n<p>Also worked fine when\
          \ I only offloaded some layers.</p>\n<p>So I'm not quite sure why you're\
          \ getting these problems, but I am confident the GGMLs are OK.  Maybe report\
          \ it to llama.cpp Github?</p>\n"
        raw: "Hmm, yes the non-standard vocab size has caused problems with CUDA in\
          \ GGML in the past.  But it's working fine for me with offloading all layers:\n\
          ```\n [pytorch2] tomj@d442126f7dde:/workspace/git/llama.cpp (master \u2714\
          ) \u1405 ./main -m /workspace/process/openchat_v3.2/ggml/openchat_v3.2.ggmlv3.q8_0.bin\
          \ -ngl 100 -t 1 -p \"GPT4 User: write a story about llamas<|end_of_turn|>GPT4\
          \ Assistant:\"\nmain: build = 933 (0728c5a)\nmain: seed  = 1690831897\n\
          ggml_init_cublas: found 4 CUDA devices:\n  Device 0: NVIDIA L40, compute\
          \ capability 8.9\n  Device 1: NVIDIA L40, compute capability 8.9\n  Device\
          \ 2: NVIDIA L40, compute capability 8.9\n  Device 3: NVIDIA L40, compute\
          \ capability 8.9\nllama.cpp: loading model from /workspace/process/openchat_v3.2/ggml/openchat_v3.2.ggmlv3.q8_0.bin\n\
          llama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal:\
          \ n_vocab    = 32002\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal:\
          \ n_embd     = 5120\nllama_model_load_internal: n_mult     = 6912\nllama_model_load_internal:\
          \ n_head     = 40\nllama_model_load_internal: n_head_kv  = 40\nllama_model_load_internal:\
          \ n_layer    = 40\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal:\
          \ n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 5.0e-06\nllama_model_load_internal:\
          \ n_ff       = 13824\nllama_model_load_internal: freq_base  = 10000.0\n\
          llama_model_load_internal: freq_scale = 1\nllama_model_load_internal: ftype\
          \      = 7 (mostly Q8_0)\nllama_model_load_internal: model size = 13B\n\
          llama_model_load_internal: ggml ctx size =    0.11 MB\nllama_model_load_internal:\
          \ using CUDA for GPU acceleration\nggml_cuda_set_main_device: using device\
          \ 0 (NVIDIA L40) as main device\nllama_model_load_internal: mem required\
          \  =  532.14 MB (+  400.00 MB per state)\nllama_model_load_internal: allocating\
          \ batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n\
          llama_model_load_internal: offloading 40 repeating layers to GPU\nllama_model_load_internal:\
          \ offloading non-repeating layers to GPU\nllama_model_load_internal: offloading\
          \ v cache to GPU\nllama_model_load_internal: offloading k cache to GPU\n\
          llama_model_load_internal: offloaded 43/43 layers to GPU\nllama_model_load_internal:\
          \ total VRAM used: 13784 MB\nllama_new_context_with_model: kv self size\
          \  =  400.00 MB\n\nsystem_info: n_threads = 1 / 256 | AVX = 1 | AVX2 = 1\
          \ | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0\
          \ | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3\
          \ = 1 | VSX = 0 |\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000,\
          \ presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40,\
          \ tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000,\
          \ mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate:\
          \ n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n GPT4 User:\
          \ write a story about llamas<|end_of_turn|>GPT4 Assistant: In the high plains\
          \ of Peru, nestled between the Andes Mountains and the sprawling grasslands,\
          \ lay a mysterious valley. For centuries, this secluded spot had remained\
          \ hidden from the outside world, its secrets kept by the ancient Incas who\
          \ once inhabited these lands. The valley was home to a small herd of llamas,\
          \ gentle creatures with long necks and comical faces, whose wool provided\
          \ warmth for the people of the Andes. But these llamas were not just any\
          \ ordinary animals; they held a special power that had been passed down\
          \ from generation to generation.\n\nOne day, two travelers stumbled upon\
          \ the valley while searching for a lost Incan city rumored to contain untold\
          \ riches and treasures. As they walked through the lush greenery, they heard\
          \ the sound of bells ringing in the distance. Curious, they followed the\
          \ melodic tune until they came upon a herd of llamas grazing peacefully.\
          \ But something about these llamas was different\u2014they were glowing\
          \ with an otherworldly light, and the travelers felt a sense of calmness\
          \ wash over them.\n\nOne of the travelers, a man named Jackson, approached\
          \ the llamas cautiously, unsure of what to expect. To his surprise, the\
          \ llamas didn't flee but instead allowed him to pet their soft fur. As he\
          \ did so, Jackson felt a strange tingling sensation course through his body.\
          \ Suddenly, he found himself understanding the llama's language as if it\
          \ were his own. The llamas were communicating with him, revealing that they\
          \ held the key to the lost Incan city.\n\nJackson and his companion, a woman\
          \ named Emily, spent the next few days learning from the wise llamas. They\
          \ discovered that the animals possessed ancient knowledge and wisdom, passed\
          \ down through generations, which allowed them to connect with the spiritual\
          \ realm. The llamas explained that they were guardians of the valley, protecting\
          \ its secrets until the right people came along.\n\nWith the help of the\
          \ llamas, Jackson and Emily finally found the lost city, hidden deep within\
          \ a labyrinth of caves. Inside, they discovered a treasure trove of artifacts,\
          \ each one holding a piece of the ancient Incan culture's history and wisdom.\
          \ The llamas had been right - there was a connection between them and the\
          \ Inca people that went beyond mere coincidence.\n\nAs Jackson and Emily\
          \ returned to their homes, they couldn't shake the sense that something\
          \ profound had happened. They knew that they had been changed by their encounter\
          \ with the llamas, and that this experience would stay with them for the\
          \ rest of their lives. And so, they continued to study and share the wisdom\
          \ of the Incan civilization, passing it on to future generations.\n\nIn\
          \ a world where technology often overshadows ancient traditions, the llamas\
          \ remind us that there is still much to learn from our past. Their presence\
          \ in the valley serves as a reminder to seek out knowledge and embrace the\
          \ wisdom of those who came before us. And who knows? Maybe one day, we too\
          \ might find ourselves standing face-to-face with a wise llama, ready to\
          \ share their ancient secrets.\n\n## Contributors\n\n* [@juliangmolina](https://twitter.com/juliangmolina)\
          \ - Story and Art\n* [@diegoberruecos](https://twitter.com/diegoberruecos)\
          \ - Illustrations\n* [@elisaferreira](https://twitter.com/elisaferreira)\
          \ - Layouts\n* [@julianaponzo](https://twitter.com/julianaponzo) - Proofreading\
          \ and Production\n [end of text]\n```\n\nAlso worked fine when I only offloaded\
          \ some layers.\n\nSo I'm not quite sure why you're getting these problems,\
          \ but I am confident the GGMLs are OK.  Maybe report it to llama.cpp Github?"
        updatedAt: '2023-07-31T19:39:44.793Z'
      numEdits: 0
      reactions: []
    id: 64c80e00e761f470610d33c2
    type: comment
  author: TheBloke
  content: "Hmm, yes the non-standard vocab size has caused problems with CUDA in\
    \ GGML in the past.  But it's working fine for me with offloading all layers:\n\
    ```\n [pytorch2] tomj@d442126f7dde:/workspace/git/llama.cpp (master \u2714) \u1405\
    \ ./main -m /workspace/process/openchat_v3.2/ggml/openchat_v3.2.ggmlv3.q8_0.bin\
    \ -ngl 100 -t 1 -p \"GPT4 User: write a story about llamas<|end_of_turn|>GPT4\
    \ Assistant:\"\nmain: build = 933 (0728c5a)\nmain: seed  = 1690831897\nggml_init_cublas:\
    \ found 4 CUDA devices:\n  Device 0: NVIDIA L40, compute capability 8.9\n  Device\
    \ 1: NVIDIA L40, compute capability 8.9\n  Device 2: NVIDIA L40, compute capability\
    \ 8.9\n  Device 3: NVIDIA L40, compute capability 8.9\nllama.cpp: loading model\
    \ from /workspace/process/openchat_v3.2/ggml/openchat_v3.2.ggmlv3.q8_0.bin\nllama_model_load_internal:\
    \ format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32002\n\
    llama_model_load_internal: n_ctx      = 512\nllama_model_load_internal: n_embd\
    \     = 5120\nllama_model_load_internal: n_mult     = 6912\nllama_model_load_internal:\
    \ n_head     = 40\nllama_model_load_internal: n_head_kv  = 40\nllama_model_load_internal:\
    \ n_layer    = 40\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal:\
    \ n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 5.0e-06\nllama_model_load_internal:\
    \ n_ff       = 13824\nllama_model_load_internal: freq_base  = 10000.0\nllama_model_load_internal:\
    \ freq_scale = 1\nllama_model_load_internal: ftype      = 7 (mostly Q8_0)\nllama_model_load_internal:\
    \ model size = 13B\nllama_model_load_internal: ggml ctx size =    0.11 MB\nllama_model_load_internal:\
    \ using CUDA for GPU acceleration\nggml_cuda_set_main_device: using device 0 (NVIDIA\
    \ L40) as main device\nllama_model_load_internal: mem required  =  532.14 MB (+\
    \  400.00 MB per state)\nllama_model_load_internal: allocating batch_size x (640\
    \ kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\nllama_model_load_internal:\
    \ offloading 40 repeating layers to GPU\nllama_model_load_internal: offloading\
    \ non-repeating layers to GPU\nllama_model_load_internal: offloading v cache to\
    \ GPU\nllama_model_load_internal: offloading k cache to GPU\nllama_model_load_internal:\
    \ offloaded 43/43 layers to GPU\nllama_model_load_internal: total VRAM used: 13784\
    \ MB\nllama_new_context_with_model: kv self size  =  400.00 MB\n\nsystem_info:\
    \ n_threads = 1 / 256 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
    \ = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
    \ = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\nsampling: repeat_last_n = 64, repeat_penalty\
    \ = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k\
    \ = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000,\
    \ mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx\
    \ = 512, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n GPT4 User: write a story\
    \ about llamas<|end_of_turn|>GPT4 Assistant: In the high plains of Peru, nestled\
    \ between the Andes Mountains and the sprawling grasslands, lay a mysterious valley.\
    \ For centuries, this secluded spot had remained hidden from the outside world,\
    \ its secrets kept by the ancient Incas who once inhabited these lands. The valley\
    \ was home to a small herd of llamas, gentle creatures with long necks and comical\
    \ faces, whose wool provided warmth for the people of the Andes. But these llamas\
    \ were not just any ordinary animals; they held a special power that had been\
    \ passed down from generation to generation.\n\nOne day, two travelers stumbled\
    \ upon the valley while searching for a lost Incan city rumored to contain untold\
    \ riches and treasures. As they walked through the lush greenery, they heard the\
    \ sound of bells ringing in the distance. Curious, they followed the melodic tune\
    \ until they came upon a herd of llamas grazing peacefully. But something about\
    \ these llamas was different\u2014they were glowing with an otherworldly light,\
    \ and the travelers felt a sense of calmness wash over them.\n\nOne of the travelers,\
    \ a man named Jackson, approached the llamas cautiously, unsure of what to expect.\
    \ To his surprise, the llamas didn't flee but instead allowed him to pet their\
    \ soft fur. As he did so, Jackson felt a strange tingling sensation course through\
    \ his body. Suddenly, he found himself understanding the llama's language as if\
    \ it were his own. The llamas were communicating with him, revealing that they\
    \ held the key to the lost Incan city.\n\nJackson and his companion, a woman named\
    \ Emily, spent the next few days learning from the wise llamas. They discovered\
    \ that the animals possessed ancient knowledge and wisdom, passed down through\
    \ generations, which allowed them to connect with the spiritual realm. The llamas\
    \ explained that they were guardians of the valley, protecting its secrets until\
    \ the right people came along.\n\nWith the help of the llamas, Jackson and Emily\
    \ finally found the lost city, hidden deep within a labyrinth of caves. Inside,\
    \ they discovered a treasure trove of artifacts, each one holding a piece of the\
    \ ancient Incan culture's history and wisdom. The llamas had been right - there\
    \ was a connection between them and the Inca people that went beyond mere coincidence.\n\
    \nAs Jackson and Emily returned to their homes, they couldn't shake the sense\
    \ that something profound had happened. They knew that they had been changed by\
    \ their encounter with the llamas, and that this experience would stay with them\
    \ for the rest of their lives. And so, they continued to study and share the wisdom\
    \ of the Incan civilization, passing it on to future generations.\n\nIn a world\
    \ where technology often overshadows ancient traditions, the llamas remind us\
    \ that there is still much to learn from our past. Their presence in the valley\
    \ serves as a reminder to seek out knowledge and embrace the wisdom of those who\
    \ came before us. And who knows? Maybe one day, we too might find ourselves standing\
    \ face-to-face with a wise llama, ready to share their ancient secrets.\n\n##\
    \ Contributors\n\n* [@juliangmolina](https://twitter.com/juliangmolina) - Story\
    \ and Art\n* [@diegoberruecos](https://twitter.com/diegoberruecos) - Illustrations\n\
    * [@elisaferreira](https://twitter.com/elisaferreira) - Layouts\n* [@julianaponzo](https://twitter.com/julianaponzo)\
    \ - Proofreading and Production\n [end of text]\n```\n\nAlso worked fine when\
    \ I only offloaded some layers.\n\nSo I'm not quite sure why you're getting these\
    \ problems, but I am confident the GGMLs are OK.  Maybe report it to llama.cpp\
    \ Github?"
  created_at: 2023-07-31 18:39:44+00:00
  edited: false
  hidden: false
  id: 64c80e00e761f470610d33c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4cd276aaa319b12d0beaf23c65630769.svg
      fullname: "DAN\u2122"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dranger003
      type: user
    createdAt: '2023-07-31T19:43:06.000Z'
    data:
      edited: false
      editors:
      - dranger003
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7679656744003296
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4cd276aaa319b12d0beaf23c65630769.svg
          fullname: "DAN\u2122"
          isHf: false
          isPro: false
          name: dranger003
          type: user
        html: '<p>Right, the problem is on Windows only, see: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/2160#issuecomment-1657203763">https://github.com/ggerganov/llama.cpp/pull/2160#issuecomment-1657203763</a></p>

          '
        raw: 'Right, the problem is on Windows only, see: https://github.com/ggerganov/llama.cpp/pull/2160#issuecomment-1657203763'
        updatedAt: '2023-07-31T19:43:06.874Z'
      numEdits: 0
      reactions: []
    id: 64c80eca4524c2aea7f5a88c
    type: comment
  author: dranger003
  content: 'Right, the problem is on Windows only, see: https://github.com/ggerganov/llama.cpp/pull/2160#issuecomment-1657203763'
  created_at: 2023-07-31 18:43:06+00:00
  edited: false
  hidden: false
  id: 64c80eca4524c2aea7f5a88c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
      fullname: Cross Nastasi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dillfrescott
      type: user
    createdAt: '2023-08-01T21:39:39.000Z'
    data:
      edited: false
      editors:
      - dillfrescott
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6820484399795532
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6215ce9abfcb3893344dd0a2/8nZkcC2lhaFHFSGcgf01T.png?w=200&h=200&f=face
          fullname: Cross Nastasi
          isHf: false
          isPro: false
          name: dillfrescott
          type: user
        html: '<p>Same issue. And yes I am on windows :/</p>

          '
        raw: Same issue. And yes I am on windows :/
        updatedAt: '2023-08-01T21:39:39.714Z'
      numEdits: 0
      reactions: []
    id: 64c97b9b203170e13c3b473c
    type: comment
  author: dillfrescott
  content: Same issue. And yes I am on windows :/
  created_at: 2023-08-01 20:39:39+00:00
  edited: false
  hidden: false
  id: 64c97b9b203170e13c3b473c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4cd276aaa319b12d0beaf23c65630769.svg
      fullname: "DAN\u2122"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dranger003
      type: user
    createdAt: '2023-08-02T15:05:43.000Z'
    data:
      edited: false
      editors:
      - dranger003
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7399359345436096
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4cd276aaa319b12d0beaf23c65630769.svg
          fullname: "DAN\u2122"
          isHf: false
          isPro: false
          name: dranger003
          type: user
        html: '<p>Latest commit of llama.cpp (4f6b60c) should resolve the issue.</p>

          '
        raw: Latest commit of llama.cpp (4f6b60c) should resolve the issue.
        updatedAt: '2023-08-02T15:05:43.617Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - TheBloke
        - algorithm
        - dillfrescott
    id: 64ca70c75dd61155625810d0
    type: comment
  author: dranger003
  content: Latest commit of llama.cpp (4f6b60c) should resolve the issue.
  created_at: 2023-08-02 14:05:43+00:00
  edited: false
  hidden: false
  id: 64ca70c75dd61155625810d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-03T09:59:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9172558188438416
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;dranger003&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/dranger003\">@<span class=\"\
          underline\">dranger003</span></a></span>\n\n\t</span></span> could you let\
          \ me know how well this model is working for you with GGML, in terms of\
          \ prompt templating, and getting the model to stop generating at the right\
          \ time?</p>\n<p>Could you let me know the command line parameters/options\
          \ you use for this model?</p>\n"
        raw: '@dranger003 could you let me know how well this model is working for
          you with GGML, in terms of prompt templating, and getting the model to stop
          generating at the right time?


          Could you let me know the command line parameters/options you use for this
          model?'
        updatedAt: '2023-08-03T09:59:51.844Z'
      numEdits: 0
      reactions: []
    id: 64cb7a972e592905f9f1ea65
    type: comment
  author: TheBloke
  content: '@dranger003 could you let me know how well this model is working for you
    with GGML, in terms of prompt templating, and getting the model to stop generating
    at the right time?


    Could you let me know the command line parameters/options you use for this model?'
  created_at: 2023-08-03 08:59:51+00:00
  edited: false
  hidden: false
  id: 64cb7a972e592905f9f1ea65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4cd276aaa319b12d0beaf23c65630769.svg
      fullname: "DAN\u2122"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dranger003
      type: user
    createdAt: '2023-08-03T11:22:11.000Z'
    data:
      edited: false
      editors:
      - dranger003
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7015483975410461
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4cd276aaa319b12d0beaf23c65630769.svg
          fullname: "DAN\u2122"
          isHf: false
          isPro: false
          name: dranger003
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Sure! This appears\
          \ to work well for me:</p>\n<p><code>bin\\Release\\main.exe -t 1 -ngl 63\
          \ -c 4096 -e -p \"GPT4 User: Tell me all about how awesome TheBloke is.&lt;|end_of_turn|&gt;GPT4\
          \ Assistant:\" -r \"&lt;|end_of_turn|&gt;\" -m models\\openchat_v3.2.ggmlv3.q8_0.bin</code></p>\n"
        raw: '@TheBloke Sure! This appears to work well for me:


          `bin\Release\main.exe -t 1 -ngl 63 -c 4096 -e -p "GPT4 User: Tell me all
          about how awesome TheBloke is.<|end_of_turn|>GPT4 Assistant:" -r "<|end_of_turn|>"
          -m models\openchat_v3.2.ggmlv3.q8_0.bin`'
        updatedAt: '2023-08-03T11:22:11.372Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
    id: 64cb8de338837b12d519eb1d
    type: comment
  author: dranger003
  content: '@TheBloke Sure! This appears to work well for me:


    `bin\Release\main.exe -t 1 -ngl 63 -c 4096 -e -p "GPT4 User: Tell me all about
    how awesome TheBloke is.<|end_of_turn|>GPT4 Assistant:" -r "<|end_of_turn|>" -m
    models\openchat_v3.2.ggmlv3.q8_0.bin`'
  created_at: 2023-08-03 10:22:11+00:00
  edited: false
  hidden: false
  id: 64cb8de338837b12d519eb1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-03T11:35:01.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9566392302513123
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thanks very much!</p>

          '
        raw: Thanks very much!
        updatedAt: '2023-08-03T11:35:01.416Z'
      numEdits: 0
      reactions: []
    id: 64cb90e578fad0f5b6841a6c
    type: comment
  author: TheBloke
  content: Thanks very much!
  created_at: 2023-08-03 10:35:01+00:00
  edited: false
  hidden: false
  id: 64cb90e578fad0f5b6841a6c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/OpenChat_v3.2-GGML
repo_type: model
status: open
target_branch: null
title: Something wrong with this model I think.
