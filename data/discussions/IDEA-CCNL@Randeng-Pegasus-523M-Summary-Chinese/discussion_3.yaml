!!python/object:huggingface_hub.community.DiscussionWithDetails
author: XYHHY
conflicting_files: null
created_at: 2023-04-15 02:08:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679044659905-noauth.jpeg?w=200&h=200&f=face
      fullname: '0'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XYHHY
      type: user
    createdAt: '2023-04-15T03:08:51.000Z'
    data:
      edited: false
      editors:
      - XYHHY
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679044659905-noauth.jpeg?w=200&h=200&f=face
          fullname: '0'
          isHf: false
          isPro: false
          name: XYHHY
          type: user
        html: "<h2 id=\"cod\">cod</h2>\n<p>from transformers import PegasusForConditionalGeneration</p>\n\
          <h1 id=\"need-to-download-tokenizers_pegasuspy-and-other-python-script-from-fengshenbang-lm-github-repo-in-advance\"\
          >Need to download tokenizers_pegasus.py and other Python script from Fengshenbang-LM\
          \ github repo in advance,</h1>\n<h1 id=\"or-you-can-download-tokenizers_pegasuspy-and-data_utilspy-in-httpshuggingfacecoidea-ccnlrandeng_pegasus_523mtreemain\"\
          >or you can download tokenizers_pegasus.py and data_utils.py in <a href=\"\
          https://huggingface.co/IDEA-CCNL/Randeng_Pegasus_523M/tree/main\">https://huggingface.co/IDEA-CCNL/Randeng_Pegasus_523M/tree/main</a></h1>\n\
          <h1 id=\"strongly-recommend-you-git-clone-the-fengshenbang-lm-repo\">Strongly\
          \ recommend you git clone the Fengshenbang-LM repo:</h1>\n<h1 id=\"1-git-clone-httpsgithubcomidea-ccnlfengshenbang-lm\"\
          >1. git clone <a rel=\"nofollow\" href=\"https://github.com/IDEA-CCNL/Fengshenbang-LM\"\
          >https://github.com/IDEA-CCNL/Fengshenbang-LM</a></h1>\n<h1 id=\"2-cd-fengshenbang-lmfengshenexamplespegasus\"\
          >2. cd Fengshenbang-LM/fengshen/examples/pegasus/</h1>\n<h1 id=\"and-then-you-will-see-the-tokenizers_pegasuspy-and-data_utilspy-which-are-needed-by-pegasus-model\"\
          >and then you will see the tokenizers_pegasus.py and data_utils.py which\
          \ are needed by pegasus model</h1>\n<p>from tokenizers_pegasus import PegasusTokenizer</p>\n\
          <p>model = PegasusForConditionalGeneration.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-523M-Chinese\"\
          )<br>tokenizer = PegasusTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-523M-Chinese\"\
          )</p>\n<p>text = \"\u636E\u5FAE\u4FE1\u516C\u4F17\u53F7\u201C\u754C\u9762\
          \u201D\u62A5\u9053\uFF0C4\u65E5\u4E0A\u534810\u70B9\u5DE6\u53F3\uFF0C\u4E2D\
          \u56FD\u53D1\u6539\u59D4\u53CD\u5784\u65AD\u8C03\u67E5\u5C0F\u7EC4\u7A81\
          \u51FB\u67E5\u8BBF\u5954\u9A70\u4E0A\u6D77\u529E\u4E8B\u5904\uFF0C\u8C03\
          \u53D6\u6570\u636E\u6750\u6599\uFF0C\u5E76\u5BF9\u591A\u540D\u5954\u9A70\
          \u9AD8\u7BA1\u8FDB\u884C\u4E86\u7EA6\u8C08\u3002\u622A\u6B62\u6628\u65E5\
          \u665A9\u70B9\uFF0C\u5305\u62EC\u5317\u4EAC\u6885\u8D5B\u5FB7\u65AF-\u5954\
          \u9A70\u9500\u552E\u670D\u52A1\u6709\u9650\u516C\u53F8\u4E1C\u533A\u603B\
          \u7ECF\u7406\u5728\u5185\u7684\u591A\u540D\u7BA1\u7406\u4EBA\u5458\u4ECD\
          \u7559\u5728\u4E0A\u6D77\u529E\u516C\u5BA4\u5185\"<br>inputs = tokenizer(text,\
          \ max_length=1024, return_tensors=\"pt\")</p>\n<h1 id=\"generate-summary\"\
          >Generate Summary</h1>\n<p>summary_ids = model.generate(inputs[\"input_ids\"\
          ])<br>tokenizer.encode_plus(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]</p>\n\
          <h1 id=\"model-output-\u622A\u6B62\u6628\u65E5\u665A9\u70B9\uFF0C\u5305\u62EC\
          \u5317\u4EAC\u6885\u8D5B\u5FB7\u65AF-\u5954\u9A70\u9500\u552E\u670D\u52A1\
          \u6709\u9650\u516C\u53F8\u4E1C\u533A\u603B\u7ECF\u7406\u5728\u5185\u7684\
          \u591A\u540D\u7BA1\u7406\u4EBA\u5458\u4ECD\u7559\u5728\u4E0A\u6D77\u529E\
          \u516C\u5BA4\u5185\">model Output: \u622A\u6B62\u6628\u65E5\u665A9\u70B9\
          \uFF0C\u5305\u62EC\u5317\u4EAC\u6885\u8D5B\u5FB7\u65AF-\u5954\u9A70\u9500\
          \u552E\u670D\u52A1\u6709\u9650\u516C\u53F8\u4E1C\u533A\u603B\u7ECF\u7406\
          \u5728\u5185\u7684\u591A\u540D\u7BA1\u7406\u4EBA\u5458\u4ECD\u7559\u5728\
          \u4E0A\u6D77\u529E\u516C\u5BA4\u5185</h1>\n<h2 id=\"error\">Error:</h2>\n\
          <p>/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1273:\
          \ UserWarning: Neither <code>max_length</code> nor <code>max_new_tokens</code>\
          \ has been set, <code>max_length</code> will default to 256 (<code>generation_config.max_length</code>).\
          \ Controlling <code>max_length</code> via the config is deprecated and <code>max_length</code>\
          \ will be removed from the config in v5 of Transformers -- we recommend\
          \ using <code>max_new_tokens</code> to control the maximum length of the\
          \ generation.<br>  warnings.warn(</p>\n<p>I tested adding truncation=True\
          \ in tokenizer.encode, but it didn't work</p>\n"
        raw: "## cod\r\nfrom transformers import PegasusForConditionalGeneration\r\
          \n# Need to download tokenizers_pegasus.py and other Python script from\
          \ Fengshenbang-LM github repo in advance,\r\n# or you can download tokenizers_pegasus.py\
          \ and data_utils.py in https://huggingface.co/IDEA-CCNL/Randeng_Pegasus_523M/tree/main\r\
          \n# Strongly recommend you git clone the Fengshenbang-LM repo:\r\n# 1. git\
          \ clone https://github.com/IDEA-CCNL/Fengshenbang-LM\r\n# 2. cd Fengshenbang-LM/fengshen/examples/pegasus/\r\
          \n# and then you will see the tokenizers_pegasus.py and data_utils.py which\
          \ are needed by pegasus model\r\nfrom tokenizers_pegasus import PegasusTokenizer\r\
          \n\r\nmodel = PegasusForConditionalGeneration.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-523M-Chinese\"\
          )\r\ntokenizer = PegasusTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-523M-Chinese\"\
          )\r\n\r\ntext = \"\u636E\u5FAE\u4FE1\u516C\u4F17\u53F7\u201C\u754C\u9762\
          \u201D\u62A5\u9053\uFF0C4\u65E5\u4E0A\u534810\u70B9\u5DE6\u53F3\uFF0C\u4E2D\
          \u56FD\u53D1\u6539\u59D4\u53CD\u5784\u65AD\u8C03\u67E5\u5C0F\u7EC4\u7A81\
          \u51FB\u67E5\u8BBF\u5954\u9A70\u4E0A\u6D77\u529E\u4E8B\u5904\uFF0C\u8C03\
          \u53D6\u6570\u636E\u6750\u6599\uFF0C\u5E76\u5BF9\u591A\u540D\u5954\u9A70\
          \u9AD8\u7BA1\u8FDB\u884C\u4E86\u7EA6\u8C08\u3002\u622A\u6B62\u6628\u65E5\
          \u665A9\u70B9\uFF0C\u5305\u62EC\u5317\u4EAC\u6885\u8D5B\u5FB7\u65AF-\u5954\
          \u9A70\u9500\u552E\u670D\u52A1\u6709\u9650\u516C\u53F8\u4E1C\u533A\u603B\
          \u7ECF\u7406\u5728\u5185\u7684\u591A\u540D\u7BA1\u7406\u4EBA\u5458\u4ECD\
          \u7559\u5728\u4E0A\u6D77\u529E\u516C\u5BA4\u5185\"\r\ninputs = tokenizer(text,\
          \ max_length=1024, return_tensors=\"pt\")\r\n\r\n# Generate Summary\r\n\
          summary_ids = model.generate(inputs[\"input_ids\"])\r\ntokenizer.encode_plus(summary_ids,\
          \ skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\r\n\r\
          \n# model Output: \u622A\u6B62\u6628\u65E5\u665A9\u70B9\uFF0C\u5305\u62EC\
          \u5317\u4EAC\u6885\u8D5B\u5FB7\u65AF-\u5954\u9A70\u9500\u552E\u670D\u52A1\
          \u6709\u9650\u516C\u53F8\u4E1C\u533A\u603B\u7ECF\u7406\u5728\u5185\u7684\
          \u591A\u540D\u7BA1\u7406\u4EBA\u5458\u4ECD\u7559\u5728\u4E0A\u6D77\u529E\
          \u516C\u5BA4\u5185\r\n\r\n## Error:\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1273:\
          \ UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length`\
          \ will default to 256 (`generation_config.max_length`). Controlling `max_length`\
          \ via the config is deprecated and `max_length` will be removed from the\
          \ config in v5 of Transformers -- we recommend using `max_new_tokens` to\
          \ control the maximum length of the generation.\r\n  warnings.warn(\r\n\r\
          \n\r\nI tested adding truncation=True in tokenizer.encode, but it didn't\
          \ work"
        updatedAt: '2023-04-15T03:08:51.474Z'
      numEdits: 0
      reactions: []
    id: 643a15430cc94e53f8b88055
    type: comment
  author: XYHHY
  content: "## cod\r\nfrom transformers import PegasusForConditionalGeneration\r\n\
    # Need to download tokenizers_pegasus.py and other Python script from Fengshenbang-LM\
    \ github repo in advance,\r\n# or you can download tokenizers_pegasus.py and data_utils.py\
    \ in https://huggingface.co/IDEA-CCNL/Randeng_Pegasus_523M/tree/main\r\n# Strongly\
    \ recommend you git clone the Fengshenbang-LM repo:\r\n# 1. git clone https://github.com/IDEA-CCNL/Fengshenbang-LM\r\
    \n# 2. cd Fengshenbang-LM/fengshen/examples/pegasus/\r\n# and then you will see\
    \ the tokenizers_pegasus.py and data_utils.py which are needed by pegasus model\r\
    \nfrom tokenizers_pegasus import PegasusTokenizer\r\n\r\nmodel = PegasusForConditionalGeneration.from_pretrained(\"\
    IDEA-CCNL/Randeng-Pegasus-523M-Chinese\")\r\ntokenizer = PegasusTokenizer.from_pretrained(\"\
    IDEA-CCNL/Randeng-Pegasus-523M-Chinese\")\r\n\r\ntext = \"\u636E\u5FAE\u4FE1\u516C\
    \u4F17\u53F7\u201C\u754C\u9762\u201D\u62A5\u9053\uFF0C4\u65E5\u4E0A\u534810\u70B9\
    \u5DE6\u53F3\uFF0C\u4E2D\u56FD\u53D1\u6539\u59D4\u53CD\u5784\u65AD\u8C03\u67E5\
    \u5C0F\u7EC4\u7A81\u51FB\u67E5\u8BBF\u5954\u9A70\u4E0A\u6D77\u529E\u4E8B\u5904\
    \uFF0C\u8C03\u53D6\u6570\u636E\u6750\u6599\uFF0C\u5E76\u5BF9\u591A\u540D\u5954\
    \u9A70\u9AD8\u7BA1\u8FDB\u884C\u4E86\u7EA6\u8C08\u3002\u622A\u6B62\u6628\u65E5\
    \u665A9\u70B9\uFF0C\u5305\u62EC\u5317\u4EAC\u6885\u8D5B\u5FB7\u65AF-\u5954\u9A70\
    \u9500\u552E\u670D\u52A1\u6709\u9650\u516C\u53F8\u4E1C\u533A\u603B\u7ECF\u7406\
    \u5728\u5185\u7684\u591A\u540D\u7BA1\u7406\u4EBA\u5458\u4ECD\u7559\u5728\u4E0A\
    \u6D77\u529E\u516C\u5BA4\u5185\"\r\ninputs = tokenizer(text, max_length=1024,\
    \ return_tensors=\"pt\")\r\n\r\n# Generate Summary\r\nsummary_ids = model.generate(inputs[\"\
    input_ids\"])\r\ntokenizer.encode_plus(summary_ids, skip_special_tokens=True,\
    \ clean_up_tokenization_spaces=False)[0]\r\n\r\n# model Output: \u622A\u6B62\u6628\
    \u65E5\u665A9\u70B9\uFF0C\u5305\u62EC\u5317\u4EAC\u6885\u8D5B\u5FB7\u65AF-\u5954\
    \u9A70\u9500\u552E\u670D\u52A1\u6709\u9650\u516C\u53F8\u4E1C\u533A\u603B\u7ECF\
    \u7406\u5728\u5185\u7684\u591A\u540D\u7BA1\u7406\u4EBA\u5458\u4ECD\u7559\u5728\
    \u4E0A\u6D77\u529E\u516C\u5BA4\u5185\r\n\r\n## Error:\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1273:\
    \ UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length`\
    \ will default to 256 (`generation_config.max_length`). Controlling `max_length`\
    \ via the config is deprecated and `max_length` will be removed from the config\
    \ in v5 of Transformers -- we recommend using `max_new_tokens` to control the\
    \ maximum length of the generation.\r\n  warnings.warn(\r\n\r\n\r\nI tested adding\
    \ truncation=True in tokenizer.encode, but it didn't work"
  created_at: 2023-04-15 02:08:51+00:00
  edited: false
  hidden: false
  id: 643a15430cc94e53f8b88055
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d22e8fa76187af6a78e04d037985ad8c.svg
      fullname: dongxiaoqun
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: dongxq
      type: user
    createdAt: '2023-04-25T05:12:15.000Z'
    data:
      edited: true
      editors:
      - dongxq
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d22e8fa76187af6a78e04d037985ad8c.svg
          fullname: dongxiaoqun
          isHf: false
          isPro: false
          name: dongxq
          type: user
        html: '<p>You could check our example carefully again, the error you report
          is only a warning , which is not cause real error.<br>The reason why you
          can''t really run this example is because you have use "encode_plus" indead
          of batch_decode.</p>

          '
        raw: 'You could check our example carefully again, the error you report is
          only a warning , which is not cause real error.

          The reason why you can''t really run this example is because you have use
          "encode_plus" indead of batch_decode.'
        updatedAt: '2023-04-25T05:12:54.258Z'
      numEdits: 1
      reactions: []
    id: 6447612fc614eaeed52d18df
    type: comment
  author: dongxq
  content: 'You could check our example carefully again, the error you report is only
    a warning , which is not cause real error.

    The reason why you can''t really run this example is because you have use "encode_plus"
    indead of batch_decode.'
  created_at: 2023-04-25 04:12:15+00:00
  edited: true
  hidden: false
  id: 6447612fc614eaeed52d18df
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese
repo_type: model
status: open
target_branch: null
title: Unable to run
