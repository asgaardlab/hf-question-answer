!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Toaster496
conflicting_files: null
created_at: 2023-07-20 08:21:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43e0b7a30062bdafc8fa2984c7ae1f4.svg
      fullname: Taj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Toaster496
      type: user
    createdAt: '2023-07-20T09:21:22.000Z'
    data:
      edited: false
      editors:
      - Toaster496
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.277005136013031
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43e0b7a30062bdafc8fa2984c7ae1f4.svg
          fullname: Taj
          isHf: false
          isPro: false
          name: Toaster496
          type: user
        html: "<p>doesn't load . what have i missed?</p>\n<p>Traceback (most recent\
          \ call last): File \u201CC:\\Users\\User\\text-generation-webui\\server.py\u201D\
          , line 68, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u201CC:\\Users\\User\\\
          text-generation-webui\\modules\\models.py\u201D, line 74, in load_model\
          \ output = load_func_maploader ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u201C\
          C:\\Users\\User\\text-generation-webui\\modules\\models.py\u201D, line 288,\
          \ in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
          \ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u201CC:\\Users\\\
          User\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D, line 56,\
          \ in load_quantized model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\
          \ File \u201CC:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 94, in from_quantized\
          \ return quant_func( ^^^^^^^^^^^ File \u201CC:\\Users\\User\\AppData\\Local\\\
          Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\modeling_base.py\u201D\
          , line 749, in from_quantized make_quant( File \u201CC:\\Users\\User\\AppData\\\
          Local\\Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D\
          , line 92, in make_quant make_quant( File \u201CC:\\Users\\User\\AppData\\\
          Local\\Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D\
          , line 92, in make_quant make_quant( File \u201CC:\\Users\\User\\AppData\\\
          Local\\Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D\
          , line 92, in make_quant make_quant( [Previous line repeated 1 more time]\
          \ File \u201CC:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D, line 84, in make_quant\
          \ new_layer = QuantLinear( ^^^^^^^^^^^^ File \u201CC:\\Users\\User\\AppData\\\
          Local\\Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\nn_modules\\\
          qlinear\\qlinear_cuda_old.py\u201D, line 83, in init self.autogptq_cuda\
          \ = autogptq_cuda_256 ^^^^^^^^^^^^^^^^^ NameError: name \u2018autogptq_cuda_256\u2019\
          \ is not defined</p>\n"
        raw: "doesn't load . what have i missed?\r\n\r\nTraceback (most recent call\
          \ last): File \u201CC:\\Users\\User\\text-generation-webui\\server.py\u201D\
          , line 68, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u201CC:\\Users\\User\\\
          text-generation-webui\\modules\\models.py\u201D, line 74, in load_model\
          \ output = load_func_maploader ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u201C\
          C:\\Users\\User\\text-generation-webui\\modules\\models.py\u201D, line 288,\
          \ in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
          \ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u201CC:\\Users\\\
          User\\text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D, line 56,\
          \ in load_quantized model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\
          \ File \u201CC:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 94, in from_quantized\
          \ return quant_func( ^^^^^^^^^^^ File \u201CC:\\Users\\User\\AppData\\Local\\\
          Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\modeling_base.py\u201D\
          , line 749, in from_quantized make_quant( File \u201CC:\\Users\\User\\AppData\\\
          Local\\Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D\
          , line 92, in make_quant make_quant( File \u201CC:\\Users\\User\\AppData\\\
          Local\\Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D\
          , line 92, in make_quant make_quant( File \u201CC:\\Users\\User\\AppData\\\
          Local\\Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D\
          , line 92, in make_quant make_quant( [Previous line repeated 1 more time]\
          \ File \u201CC:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D, line 84, in make_quant\
          \ new_layer = QuantLinear( ^^^^^^^^^^^^ File \u201CC:\\Users\\User\\AppData\\\
          Local\\Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\nn_modules\\\
          qlinear\\qlinear_cuda_old.py\u201D, line 83, in init self.autogptq_cuda\
          \ = autogptq_cuda_256 ^^^^^^^^^^^^^^^^^ NameError: name \u2018autogptq_cuda_256\u2019\
          \ is not defined"
        updatedAt: '2023-07-20T09:21:22.225Z'
      numEdits: 0
      reactions: []
    id: 64b8fc92e436bbca16709fcf
    type: comment
  author: Toaster496
  content: "doesn't load . what have i missed?\r\n\r\nTraceback (most recent call\
    \ last): File \u201CC:\\Users\\User\\text-generation-webui\\server.py\u201D, line\
    \ 68, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u201CC:\\Users\\User\\text-generation-webui\\\
    modules\\models.py\u201D, line 74, in load_model output = load_func_maploader\
    \ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u201CC:\\Users\\User\\text-generation-webui\\\
    modules\\models.py\u201D, line 288, in AutoGPTQ_loader return modules.AutoGPTQ_loader.load_quantized(model_name)\
    \ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u201CC:\\Users\\User\\\
    text-generation-webui\\modules\\AutoGPTQ_loader.py\u201D, line 56, in load_quantized\
    \ model = AutoGPTQForCausalLM.from_quantized(path_to_model, **params) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\
    \ File \u201CC:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\\
    site-packages\\auto_gptq\\modeling\\auto.py\u201D, line 94, in from_quantized\
    \ return quant_func( ^^^^^^^^^^^ File \u201CC:\\Users\\User\\AppData\\Local\\\
    Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\modeling_base.py\u201D\
    , line 749, in from_quantized make_quant( File \u201CC:\\Users\\User\\AppData\\\
    Local\\Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D\
    , line 92, in make_quant make_quant( File \u201CC:\\Users\\User\\AppData\\Local\\\
    Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D\
    , line 92, in make_quant make_quant( File \u201CC:\\Users\\User\\AppData\\Local\\\
    Programs\\Python\\Python311\\Lib\\site-packages\\auto_gptq\\modeling_utils.py\u201D\
    , line 92, in make_quant make_quant( [Previous line repeated 1 more time] File\
    \ \u201CC:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\\
    auto_gptq\\modeling_utils.py\u201D, line 84, in make_quant new_layer = QuantLinear(\
    \ ^^^^^^^^^^^^ File \u201CC:\\Users\\User\\AppData\\Local\\Programs\\Python\\\
    Python311\\Lib\\site-packages\\auto_gptq\\nn_modules\\qlinear\\qlinear_cuda_old.py\u201D\
    , line 83, in init self.autogptq_cuda = autogptq_cuda_256 ^^^^^^^^^^^^^^^^^ NameError:\
    \ name \u2018autogptq_cuda_256\u2019 is not defined"
  created_at: 2023-07-20 08:21:22+00:00
  edited: false
  hidden: false
  id: 64b8fc92e436bbca16709fcf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-20T09:27:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9062615633010864
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This issue is caused by AutoGPTQ not being correctly compiled. </p>

          <p>In general, as you''re using text-generation-webui, I suggest you use
          ExLlama instead if you can.  It''s faster, uses less VRAM, and is automatically
          compiled for you by text-generation-webui.  The only restriction is that
          it can''t load the 8-bit quants I provided.</p>

          <p>You''re getting this AutoGPTQ issue because you''re using Python 3.11,
          and there are pre-compiled AutoGPTQ wheels only for Python 3.8, 3.9 and
          3.10.</p>

          <p>You have two options:</p>

          <ol>

          <li>Install and use Python 3.10 instead, ideally in a virtual environment
          like MiniConda.</li>

          </ol>

          <p>In fact the text-generation-webui already provides a one-click installer
          which installs a Python 3.10 conda environment, with everything installed
          in it that you need. </p>

          <p>So that is my recommendation to you: download and use the text-generation-webui
          one click installer instead.  It should handle everything for you automatically,
          providing both ExLlama and a working AutoGPTQ.</p>

          <ol start="2">

          <li>If you really want to try and get AutoGPTQ running in your manual install
          with Python 3.11, you would need to try to build it yourself. This requires
          the CUDA toolkit installed, and is often much more challenging on Windows
          than it is on Linux.  These are the commands you would run:</li>

          </ol>

          <pre><code>pip3 uninstall -y auto-gptq

          set GITHUB_ACTIONS=true

          pip3 install -v auto-gptq

          </code></pre>

          '
        raw: "This issue is caused by AutoGPTQ not being correctly compiled. \n\n\
          In general, as you're using text-generation-webui, I suggest you use ExLlama\
          \ instead if you can.  It's faster, uses less VRAM, and is automatically\
          \ compiled for you by text-generation-webui.  The only restriction is that\
          \ it can't load the 8-bit quants I provided.\n\nYou're getting this AutoGPTQ\
          \ issue because you're using Python 3.11, and there are pre-compiled AutoGPTQ\
          \ wheels only for Python 3.8, 3.9 and 3.10.\n\nYou have two options:\n1.\
          \ Install and use Python 3.10 instead, ideally in a virtual environment\
          \ like MiniConda.  \n\nIn fact the text-generation-webui already provides\
          \ a one-click installer which installs a Python 3.10 conda environment,\
          \ with everything installed in it that you need. \n\nSo that is my recommendation\
          \ to you: download and use the text-generation-webui one click installer\
          \ instead.  It should handle everything for you automatically, providing\
          \ both ExLlama and a working AutoGPTQ.\n\n2. If you really want to try and\
          \ get AutoGPTQ running in your manual install with Python 3.11, you would\
          \ need to try to build it yourself. This requires the CUDA toolkit installed,\
          \ and is often much more challenging on Windows than it is on Linux.  These\
          \ are the commands you would run:\n```\npip3 uninstall -y auto-gptq\nset\
          \ GITHUB_ACTIONS=true\npip3 install -v auto-gptq\n```\n\n"
        updatedAt: '2023-07-20T09:27:50.612Z'
      numEdits: 0
      reactions: []
    id: 64b8fe1625b0493d517d8551
    type: comment
  author: TheBloke
  content: "This issue is caused by AutoGPTQ not being correctly compiled. \n\nIn\
    \ general, as you're using text-generation-webui, I suggest you use ExLlama instead\
    \ if you can.  It's faster, uses less VRAM, and is automatically compiled for\
    \ you by text-generation-webui.  The only restriction is that it can't load the\
    \ 8-bit quants I provided.\n\nYou're getting this AutoGPTQ issue because you're\
    \ using Python 3.11, and there are pre-compiled AutoGPTQ wheels only for Python\
    \ 3.8, 3.9 and 3.10.\n\nYou have two options:\n1. Install and use Python 3.10\
    \ instead, ideally in a virtual environment like MiniConda.  \n\nIn fact the text-generation-webui\
    \ already provides a one-click installer which installs a Python 3.10 conda environment,\
    \ with everything installed in it that you need. \n\nSo that is my recommendation\
    \ to you: download and use the text-generation-webui one click installer instead.\
    \  It should handle everything for you automatically, providing both ExLlama and\
    \ a working AutoGPTQ.\n\n2. If you really want to try and get AutoGPTQ running\
    \ in your manual install with Python 3.11, you would need to try to build it yourself.\
    \ This requires the CUDA toolkit installed, and is often much more challenging\
    \ on Windows than it is on Linux.  These are the commands you would run:\n```\n\
    pip3 uninstall -y auto-gptq\nset GITHUB_ACTIONS=true\npip3 install -v auto-gptq\n\
    ```\n\n"
  created_at: 2023-07-20 08:27:50+00:00
  edited: false
  hidden: false
  id: 64b8fe1625b0493d517d8551
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4344ff4d2d8b7aa06ef91ccb538f717a.svg
      fullname: Marcelo Roberto Bianchi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0mrb
      type: user
    createdAt: '2023-07-26T21:51:58.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/4344ff4d2d8b7aa06ef91ccb538f717a.svg
          fullname: Marcelo Roberto Bianchi
          isHf: false
          isPro: false
          name: 0mrb
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-07-26T22:18:06.304Z'
      numEdits: 1
      reactions: []
    id: 64c1957ed1ca220e3030c2dc
    type: comment
  author: 0mrb
  content: This comment has been hidden
  created_at: 2023-07-26 20:51:58+00:00
  edited: true
  hidden: true
  id: 64c1957ed1ca220e3030c2dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e1150772de0f50923a293d5174e87d1.svg
      fullname: cesareswift
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cesareswift
      type: user
    createdAt: '2023-07-27T11:39:38.000Z'
    data:
      edited: false
      editors:
      - cesareswift
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5607813000679016
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e1150772de0f50923a293d5174e87d1.svg
          fullname: cesareswift
          isHf: false
          isPro: false
          name: cesareswift
          type: user
        html: '<p>using lm-evaluation-harness autogptq mode, Could not find model<br>
          File "/root/anaconda3/envs/infer/lib/python3.9/site-packages/auto_gptq/modeling/_base.py",
          line 768, in from_quantized<br>    raise FileNotFoundError(f"Could not find
          model in {model_name_or_path}")<br>FileNotFoundError: Could not find model
          in TheBloke/Llama-2-70B-GPTQ</p>

          '
        raw: "using lm-evaluation-harness autogptq mode, Could not find model \n File\
          \ \"/root/anaconda3/envs/infer/lib/python3.9/site-packages/auto_gptq/modeling/_base.py\"\
          , line 768, in from_quantized\n    raise FileNotFoundError(f\"Could not\
          \ find model in {model_name_or_path}\")\nFileNotFoundError: Could not find\
          \ model in TheBloke/Llama-2-70B-GPTQ\n"
        updatedAt: '2023-07-27T11:39:38.548Z'
      numEdits: 0
      reactions: []
    id: 64c2577ab73c3980be5a709a
    type: comment
  author: cesareswift
  content: "using lm-evaluation-harness autogptq mode, Could not find model \n File\
    \ \"/root/anaconda3/envs/infer/lib/python3.9/site-packages/auto_gptq/modeling/_base.py\"\
    , line 768, in from_quantized\n    raise FileNotFoundError(f\"Could not find model\
    \ in {model_name_or_path}\")\nFileNotFoundError: Could not find model in TheBloke/Llama-2-70B-GPTQ\n"
  created_at: 2023-07-27 10:39:38+00:00
  edited: false
  hidden: false
  id: 64c2577ab73c3980be5a709a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Llama-2-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: Does not load
