!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cashinvoice
conflicting_files: null
created_at: 2024-01-19 08:33:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4f10ad09a67112b22fa5c27616ab0d74.svg
      fullname: "DanieleFan\xEC"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cashinvoice
      type: user
    createdAt: '2024-01-19T08:33:41.000Z'
    data:
      edited: false
      editors:
      - cashinvoice
      hidden: false
      identifiedLanguage:
        language: it
        probability: 0.9874303340911865
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4f10ad09a67112b22fa5c27616ab0d74.svg
          fullname: "DanieleFan\xEC"
          isHf: false
          isPro: false
          name: cashinvoice
          type: user
        html: "<p>Ciao, \xE8 possibile utilizzare questo modello con [LoLLMS] (<a\
          \ rel=\"nofollow\" href=\"https://github.com/ParisNeo/lollms-webui\">https://github.com/ParisNeo/lollms-webui</a>)?<br>Sono\
          \ nuovo nel campo, e sto cercando di capire come far funzionare tutto. LoLLMS\
          \ mi sembra ottimo, visto che mi permette di selezionare diversi modelli\
          \ e (in parte almeno) configurarli, tutto in un unico posto. Ma in realt\xE0\
          \ sto riuscendo a far funzionare ben poco, oltre Ollama e llama-2</p>\n"
        raw: "Ciao, \xE8 possibile utilizzare questo modello con [LoLLMS] (https://github.com/ParisNeo/lollms-webui)?\r\
          \nSono nuovo nel campo, e sto cercando di capire come far funzionare tutto.\
          \ LoLLMS mi sembra ottimo, visto che mi permette di selezionare diversi\
          \ modelli e (in parte almeno) configurarli, tutto in un unico posto. Ma\
          \ in realt\xE0 sto riuscendo a far funzionare ben poco, oltre Ollama e llama-2"
        updatedAt: '2024-01-19T08:33:41.284Z'
      numEdits: 0
      reactions: []
    id: 65aa33e5356bf23b4a4f3a4f
    type: comment
  author: cashinvoice
  content: "Ciao, \xE8 possibile utilizzare questo modello con [LoLLMS] (https://github.com/ParisNeo/lollms-webui)?\r\
    \nSono nuovo nel campo, e sto cercando di capire come far funzionare tutto. LoLLMS\
    \ mi sembra ottimo, visto che mi permette di selezionare diversi modelli e (in\
    \ parte almeno) configurarli, tutto in un unico posto. Ma in realt\xE0 sto riuscendo\
    \ a far funzionare ben poco, oltre Ollama e llama-2"
  created_at: 2024-01-19 08:33:41+00:00
  edited: false
  hidden: false
  id: 65aa33e5356bf23b4a4f3a4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/63545a8e91a88bd8464a5683054e48ff.svg
      fullname: Marco Polignano
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: m-polignano-uniba
      type: user
    createdAt: '2024-01-19T16:17:06.000Z'
    data:
      edited: false
      editors:
      - m-polignano-uniba
      hidden: false
      identifiedLanguage:
        language: it
        probability: 0.9043896198272705
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/63545a8e91a88bd8464a5683054e48ff.svg
          fullname: Marco Polignano
          isHf: false
          isPro: false
          name: m-polignano-uniba
          type: user
        html: "<p>Ciao, sinceramente non conosco questa libreria. Internamente, per\xF2\
          , siamo riusciti a lanciare un'interfaccia basata su Gradio ( <a rel=\"\
          nofollow\" href=\"https://www.gradio.app/\">https://www.gradio.app/</a>\
          \ ) senza grossi problemi. Abbiamo usato questa libreria: <a rel=\"nofollow\"\
          \ href=\"https://github.com/oobabooga/text-generation-webui\">https://github.com/oobabooga/text-generation-webui</a></p>\n"
        raw: "Ciao, sinceramente non conosco questa libreria. Internamente, per\xF2\
          , siamo riusciti a lanciare un'interfaccia basata su Gradio ( https://www.gradio.app/\
          \ ) senza grossi problemi. Abbiamo usato questa libreria: https://github.com/oobabooga/text-generation-webui"
        updatedAt: '2024-01-19T16:17:06.124Z'
      numEdits: 0
      reactions: []
    id: 65aaa08286f88a686be2c781
    type: comment
  author: m-polignano-uniba
  content: "Ciao, sinceramente non conosco questa libreria. Internamente, per\xF2\
    , siamo riusciti a lanciare un'interfaccia basata su Gradio ( https://www.gradio.app/\
    \ ) senza grossi problemi. Abbiamo usato questa libreria: https://github.com/oobabooga/text-generation-webui"
  created_at: 2024-01-19 16:17:06+00:00
  edited: false
  hidden: false
  id: 65aaa08286f88a686be2c781
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: swap-uniba/LLaMAntino-2-7b-hf-ITA
repo_type: model
status: open
target_branch: null
title: utilizzare il modello su LoLLMS
