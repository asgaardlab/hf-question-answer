!!python/object:huggingface_hub.community.DiscussionWithDetails
author: chenxi118
conflicting_files: null
created_at: 2023-12-02 11:25:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a70e96e668e945cdc3940f1ec36ddba6.svg
      fullname: chenxi wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chenxi118
      type: user
    createdAt: '2023-12-02T11:25:40.000Z'
    data:
      edited: false
      editors:
      - chenxi118
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9441766142845154
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a70e96e668e945cdc3940f1ec36ddba6.svg
          fullname: chenxi wu
          isHf: false
          isPro: false
          name: chenxi118
          type: user
        html: '<p>Typically, models based on LLaMA-2 have a parameter size of 4K,
          but why is it 2K here? Will this lead to a shorter effective understanding
          of the context by the model?</p>

          '
        raw: Typically, models based on LLaMA-2 have a parameter size of 4K, but why
          is it 2K here? Will this lead to a shorter effective understanding of the
          context by the model?
        updatedAt: '2023-12-02T11:25:40.736Z'
      numEdits: 0
      reactions: []
    id: 656b1434fe7fe0b1e9b5efa1
    type: comment
  author: chenxi118
  content: Typically, models based on LLaMA-2 have a parameter size of 4K, but why
    is it 2K here? Will this lead to a shorter effective understanding of the context
    by the model?
  created_at: 2023-12-02 11:25:40+00:00
  edited: false
  hidden: false
  id: 656b1434fe7fe0b1e9b5efa1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619266745847-607838a98794cdeedcb7b04d.jpeg?w=200&h=200&f=face
      fullname: Ye Chen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yechen
      type: user
    createdAt: '2023-12-03T01:18:55.000Z'
    data:
      edited: false
      editors:
      - yechen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.943588137626648
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1619266745847-607838a98794cdeedcb7b04d.jpeg?w=200&h=200&f=face
          fullname: Ye Chen
          isHf: false
          isPro: false
          name: yechen
          type: user
        html: '<p>this is because we fine tuned this version of model using 2048 max
          length to group data, we found almost all demonstration data within this
          length. However, the model should work fine with 4k length or even longer,
          RoPE can extrapolate well due to its functional form.</p>

          '
        raw: this is because we fine tuned this version of model using 2048 max length
          to group data, we found almost all demonstration data within this length.
          However, the model should work fine with 4k length or even longer, RoPE
          can extrapolate well due to its functional form.
        updatedAt: '2023-12-03T01:18:55.912Z'
      numEdits: 0
      reactions: []
    id: 656bd77f79a834846bc95493
    type: comment
  author: yechen
  content: this is because we fine tuned this version of model using 2048 max length
    to group data, we found almost all demonstration data within this length. However,
    the model should work fine with 4k length or even longer, RoPE can extrapolate
    well due to its functional form.
  created_at: 2023-12-03 01:18:55+00:00
  edited: false
  hidden: false
  id: 656bd77f79a834846bc95493
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a70e96e668e945cdc3940f1ec36ddba6.svg
      fullname: chenxi wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chenxi118
      type: user
    createdAt: '2023-12-03T05:25:06.000Z'
    data:
      edited: false
      editors:
      - chenxi118
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.6382389664649963
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a70e96e668e945cdc3940f1ec36ddba6.svg
          fullname: chenxi wu
          isHf: false
          isPro: false
          name: chenxi118
          type: user
        html: "<p>Thanks\uFF01</p>\n"
        raw: "Thanks\uFF01"
        updatedAt: '2023-12-03T05:25:06.033Z'
      numEdits: 0
      reactions: []
      relatedEventId: 656c11323dbac3a83d041dfa
    id: 656c11323dbac3a83d041df6
    type: comment
  author: chenxi118
  content: "Thanks\uFF01"
  created_at: 2023-12-03 05:25:06+00:00
  edited: false
  hidden: false
  id: 656c11323dbac3a83d041df6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a70e96e668e945cdc3940f1ec36ddba6.svg
      fullname: chenxi wu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chenxi118
      type: user
    createdAt: '2023-12-03T05:25:06.000Z'
    data:
      status: closed
    id: 656c11323dbac3a83d041dfa
    type: status-change
  author: chenxi118
  created_at: 2023-12-03 05:25:06+00:00
  id: 656c11323dbac3a83d041dfa
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TigerResearch/tigerbot-70b-chat-v2
repo_type: model
status: closed
target_branch: null
title: 'WHY  "max_position_embeddings": 2048'
