!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tomashoufek
conflicting_files: null
created_at: 2023-07-03 13:55:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e7d436b469bd0c232d8557f3ff93ca5.svg
      fullname: "Tom\xE1\u0161 Houfek"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomashoufek
      type: user
    createdAt: '2023-07-03T14:55:03.000Z'
    data:
      edited: false
      editors:
      - tomashoufek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4862958490848541
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e7d436b469bd0c232d8557f3ff93ca5.svg
          fullname: "Tom\xE1\u0161 Houfek"
          isHf: false
          isPro: false
          name: tomashoufek
          type: user
        html: "<p>When trying to further pre-train the model on a specific domain\
          \ I encountered an error:</p>\n<p>When tokenizing using the robeczech-base\
          \ tokenizer warning accours:<br><code>The OrderedVocab you are attempting\
          \ to save contains a hole for index 51959, your vocabulary could be corrupted\
          \ !</code></p>\n<p>When I start training the model python throws following\
          \ error <em><strong>PyTroch</strong></em>:</p>\n<pre><code class=\"language-Python\"\
          >../aten/src/ATen/native/cuda/Indexing.cu:<span class=\"hljs-number\">1146</span>:\
          \ indexSelectLargeIndex: block: [<span class=\"hljs-number\">94</span>,<span\
          \ class=\"hljs-number\">0</span>,<span class=\"hljs-number\">0</span>],\
          \ thread: [<span class=\"hljs-number\">96</span>,<span class=\"hljs-number\"\
          >0</span>,<span class=\"hljs-number\">0</span>] Assertion `srcIndex &lt;\
          \ srcSelectDimSize` failed.\n<span class=\"hljs-comment\"># This error is\
          \ repeated for various block and thread values.</span>\n\nTraceback (most\
          \ recent call last):\n  File <span class=\"hljs-string\">\"/home/jovyan/tomas/medical-lm/PyTorch/TrainLM-masked-pytorch.py\"\
          </span>, line <span class=\"hljs-number\">98</span>, <span class=\"hljs-keyword\"\
          >in</span> &lt;module&gt;\n    result = trainer.train()\n  File <span class=\"\
          hljs-string\">\"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\"\
          </span>, line <span class=\"hljs-number\">1645</span>, <span class=\"hljs-keyword\"\
          >in</span> train\n    <span class=\"hljs-keyword\">return</span> inner_training_loop(\n\
          \  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\"\
          </span>, line <span class=\"hljs-number\">1938</span>, <span class=\"hljs-keyword\"\
          >in</span> _inner_training_loop\n    tr_loss_step = self.training_step(model,\
          \ inputs)\n  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\"\
          </span>, line <span class=\"hljs-number\">2759</span>, <span class=\"hljs-keyword\"\
          >in</span> training_step\n    loss = self.compute_loss(model, inputs)\n\
          \  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\"\
          </span>, line <span class=\"hljs-number\">2784</span>, <span class=\"hljs-keyword\"\
          >in</span> compute_loss\n    outputs = model(**inputs)\n  File <span class=\"\
          hljs-string\">\"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          </span>, line <span class=\"hljs-number\">1501</span>, <span class=\"hljs-keyword\"\
          >in</span> _call_impl\n    <span class=\"hljs-keyword\">return</span> forward_call(*args,\
          \ **kwargs)\n  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/accelerate/utils/operations.py\"\
          </span>, line <span class=\"hljs-number\">553</span>, <span class=\"hljs-keyword\"\
          >in</span> forward\n    <span class=\"hljs-keyword\">return</span> model_forward(*args,\
          \ **kwargs)\n  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/accelerate/utils/operations.py\"\
          </span>, line <span class=\"hljs-number\">541</span>, <span class=\"hljs-keyword\"\
          >in</span> __call__\n    <span class=\"hljs-keyword\">return</span> convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))\n  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/torch/amp/autocast_mode.py\"\
          </span>, line <span class=\"hljs-number\">14</span>, <span class=\"hljs-keyword\"\
          >in</span> decorate_autocast\n    <span class=\"hljs-keyword\">return</span>\
          \ func(*args, **kwargs)\n  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\"\
          </span>, line <span class=\"hljs-number\">1100</span>, <span class=\"hljs-keyword\"\
          >in</span> forward\n    outputs = self.roberta(\n  File <span class=\"hljs-string\"\
          >\"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"</span>,\
          \ line <span class=\"hljs-number\">1501</span>, <span class=\"hljs-keyword\"\
          >in</span> _call_impl\n    <span class=\"hljs-keyword\">return</span> forward_call(*args,\
          \ **kwargs)\n  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\"\
          </span>, line <span class=\"hljs-number\">845</span>, <span class=\"hljs-keyword\"\
          >in</span> forward\n    embedding_output = self.embeddings(\n  File <span\
          \ class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          </span>, line <span class=\"hljs-number\">1501</span>, <span class=\"hljs-keyword\"\
          >in</span> _call_impl\n    <span class=\"hljs-keyword\">return</span> forward_call(*args,\
          \ **kwargs)\n  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\"\
          </span>, line <span class=\"hljs-number\">123</span>, <span class=\"hljs-keyword\"\
          >in</span> forward\n    inputs_embeds = self.word_embeddings(input_ids)\n\
          \  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          </span>, line <span class=\"hljs-number\">1501</span>, <span class=\"hljs-keyword\"\
          >in</span> _call_impl\n    <span class=\"hljs-keyword\">return</span> forward_call(*args,\
          \ **kwargs)\n  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py\"\
          </span>, line <span class=\"hljs-number\">162</span>, <span class=\"hljs-keyword\"\
          >in</span> forward\n    <span class=\"hljs-keyword\">return</span> F.embedding(\n\
          \  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\"\
          </span>, line <span class=\"hljs-number\">2210</span>, <span class=\"hljs-keyword\"\
          >in</span> embedding\n    <span class=\"hljs-keyword\">return</span> torch.embedding(weight,\
          \ <span class=\"hljs-built_in\">input</span>, padding_idx, scale_grad_by_freq,\
          \ sparse)\nRuntimeError: CUDA error: device-side <span class=\"hljs-keyword\"\
          >assert</span> triggered\nCompile <span class=\"hljs-keyword\">with</span>\
          \ `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n</code></pre>\n\
          <p>Code I use to pre-train the model:</p>\n<pre><code class=\"language-Python\"\
          ><span class=\"hljs-keyword\">import</span> torch\n\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ AutoModelForMaskedLM\n\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> Trainer, TrainingArguments\n\
          <span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> DataCollatorForLanguageModeling\n<span class=\"hljs-keyword\"\
          >from</span> datasets <span class=\"hljs-keyword\">import</span> load_dataset\n\
          <span class=\"hljs-keyword\">import</span> math\n<span class=\"hljs-keyword\"\
          >import</span> evaluate\n\n<span class=\"hljs-keyword\">from</span> pynvml\
          \ <span class=\"hljs-keyword\">import</span> *\n\nmodel_name = <span class=\"\
          hljs-string\">\"ufal/robeczech-base\"</span> \n\n<span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">print_gpu_utilization</span>():\n\
          \    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(<span class=\"\
          hljs-number\">0</span>)\n    info = nvmlDeviceGetMemoryInfo(handle)\n  \
          \  <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"GPU memory occupied: <span class=\"hljs-subst\">{info.used//<span class=\"\
          hljs-number\">1024</span>**<span class=\"hljs-number\">2</span>}</span>\
          \ MB.\"</span>)\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">print_summary</span>(<span class=\"hljs-params\"\
          >result</span>):\n    <span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">f\"Time: <span class=\"hljs-subst\">{result.metrics[<span\
          \ class=\"hljs-string\">'train_runtime'</span>]:<span class=\"hljs-number\"\
          >.2</span>f}</span>\"</span>)\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"Samples/second: <span class=\"hljs-subst\">{result.metrics[<span\
          \ class=\"hljs-string\">'train_samples_per_second'</span>]:<span class=\"\
          hljs-number\">.2</span>f}</span>\"</span>)\n    print_gpu_utilization()\n\
          \n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndataset = load_dataset(<span\
          \ class=\"hljs-string\">'text'</span>, data_dir=<span class=\"hljs-string\"\
          >\"data/\"</span>)\ndatasets = dataset[<span class=\"hljs-string\">\"train\"\
          </span>].train_test_split(test_size = <span class=\"hljs-number\">0.1</span>)\n\
          \n<span class=\"hljs-built_in\">print</span>(tokenizer.model_max_length)\n\
          \nprint_gpu_utilization()\n\n<span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">tokenize_function</span>(<span class=\"\
          hljs-params\">examples</span>):\n    <span class=\"hljs-keyword\">return</span>\
          \ tokenizer(examples[<span class=\"hljs-string\">\"text\"</span>], truncation=<span\
          \ class=\"hljs-literal\">True</span>, max_length=<span class=\"hljs-number\"\
          >512</span>)\n\ntokenized_datasets = datasets.<span class=\"hljs-built_in\"\
          >map</span>(tokenize_function, batched=<span class=\"hljs-literal\">True</span>,\
          \ num_proc=<span class=\"hljs-number\">16</span>, remove_columns=[<span\
          \ class=\"hljs-string\">\"text\"</span>])\n\nblock_size=<span class=\"hljs-number\"\
          >512</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\
          \ function_\">group_texts</span>(<span class=\"hljs-params\">examples</span>):\n\
          \    concatenated_examples = {k: <span class=\"hljs-built_in\">sum</span>(examples[k],\
          \ []) <span class=\"hljs-keyword\">for</span> k <span class=\"hljs-keyword\"\
          >in</span> examples.keys()}\n    total_length = <span class=\"hljs-built_in\"\
          >len</span>(concatenated_examples[<span class=\"hljs-built_in\">list</span>(examples.keys())[<span\
          \ class=\"hljs-number\">0</span>]])\n    total_length = (total_length //\
          \ block_size) * block_size\n    result = {\n        k: [t[i : i + block_size]\
          \ <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\"\
          >in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\"\
          >0</span>, total_length, block_size)]\n        <span class=\"hljs-keyword\"\
          >for</span> k, t <span class=\"hljs-keyword\">in</span> concatenated_examples.items()\n\
          \    }\n    result[<span class=\"hljs-string\">\"labels\"</span>] = result[<span\
          \ class=\"hljs-string\">\"input_ids\"</span>].copy()\n    <span class=\"\
          hljs-keyword\">return</span> result\n\nlm_datasets = tokenized_datasets.<span\
          \ class=\"hljs-built_in\">map</span>(\n    group_texts,\n    batched=<span\
          \ class=\"hljs-literal\">True</span>,\n    batch_size=<span class=\"hljs-number\"\
          >1000</span>,\n    num_proc=<span class=\"hljs-number\">16</span>,\n)\n\n\
          model = AutoModelForMaskedLM.from_pretrained(model_name)\n\ndata_collator\
          \ = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=<span\
          \ class=\"hljs-number\">0.15</span>)\n\ntraining_args = TrainingArguments(\n\
          \    <span class=\"hljs-string\">f\"<span class=\"hljs-subst\">{model_name}</span>-pre-trained-med\"\
          </span>,\n    learning_rate=<span class=\"hljs-number\">2e-5</span>,\n \
          \   weight_decay=<span class=\"hljs-number\">0.01</span>,\n    warmup_steps=<span\
          \ class=\"hljs-number\">2000</span>,\n    per_device_train_batch_size=<span\
          \ class=\"hljs-number\">32</span>,\n    per_device_eval_batch_size=<span\
          \ class=\"hljs-number\">32</span>,\n    fp16=<span class=\"hljs-literal\"\
          >True</span>,\n    logging_dir=<span class=\"hljs-string\">f\"<span class=\"\
          hljs-subst\">{model_name}</span>-pre-trained-med\"</span>,\n    logging_strategy=<span\
          \ class=\"hljs-string\">\"steps\"</span>,\n    num_train_epochs=<span class=\"\
          hljs-number\">10</span>,\n    logging_steps=<span class=\"hljs-number\"\
          >100</span>,\n    save_strategy=<span class=\"hljs-string\">\"epoch\"</span>,\n\
          \    save_total_limit=<span class=\"hljs-number\">3</span>,\n)\n\ntrainer\
          \ = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_datasets[<span\
          \ class=\"hljs-string\">\"train\"</span>],\n    eval_dataset=lm_datasets[<span\
          \ class=\"hljs-string\">\"test\"</span>],\n    data_collator=data_collator,\n\
          )\n\nresult = trainer.train()\nprint_summary(result)\n\neval_results = trainer.evaluate()\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"\
          Perplexity: <span class=\"hljs-subst\">{math.exp(eval_results[<span class=\"\
          hljs-string\">'eval_loss'</span>]):<span class=\"hljs-number\">.2</span>f}</span>\"\
          </span>)\n\ntrainer.save_model(<span class=\"hljs-string\">f\"py<span class=\"\
          hljs-subst\">{model_name}</span>-med-pretrained\"</span>)\n</code></pre>\n\
          <p>I tried to download the model and tokenizer locally, \"fill\" the vocab\
          \ hole and reload the tokenizer, but I was not able to reload the fixed\
          \ tokenizer.</p>\n<p>Are there any possible ways how to fix this?</p>\n"
        raw: "When trying to further pre-train the model on a specific domain I encountered\
          \ an error:\r\n\r\nWhen tokenizing using the robeczech-base tokenizer warning\
          \ accours:\r\n`The OrderedVocab you are attempting to save contains a hole\
          \ for index 51959, your vocabulary could be corrupted !`\r\n\r\nWhen I start\
          \ training the model python throws following error ***PyTroch***:\r\n```Python\r\
          \n../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex:\
          \ block: [94,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize`\
          \ failed.\r\n# This error is repeated for various block and thread values.\r\
          \n\r\nTraceback (most recent call last):\r\n  File \"/home/jovyan/tomas/medical-lm/PyTorch/TrainLM-masked-pytorch.py\"\
          , line 98, in <module>\r\n    result = trainer.train()\r\n  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\"\
          , line 1645, in train\r\n    return inner_training_loop(\r\n  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\"\
          , line 1938, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model,\
          \ inputs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\"\
          , line 2759, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\"\
          , line 2784, in compute_loss\r\n    outputs = model(**inputs)\r\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/accelerate/utils/operations.py\"\
          , line 553, in forward\r\n    return model_forward(*args, **kwargs)\r\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/accelerate/utils/operations.py\"\
          , line 541, in __call__\r\n    return convert_to_fp32(self.model_forward(*args,\
          \ **kwargs))\r\n  File \"/opt/conda/lib/python3.9/site-packages/torch/amp/autocast_mode.py\"\
          , line 14, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\"\
          , line 1100, in forward\r\n    outputs = self.roberta(\r\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\"\
          , line 845, in forward\r\n    embedding_output = self.embeddings(\r\n  File\
          \ \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\"\
          , line 123, in forward\r\n    inputs_embeds = self.word_embeddings(input_ids)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py\"\
          , line 162, in forward\r\n    return F.embedding(\r\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\"\
          , line 2210, in embedding\r\n    return torch.embedding(weight, input, padding_idx,\
          \ scale_grad_by_freq, sparse)\r\nRuntimeError: CUDA error: device-side assert\
          \ triggered\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\
          \n```\r\nCode I use to pre-train the model:\r\n\r\n```Python\r\nimport torch\r\
          \n\r\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\r\n\r\
          \nfrom transformers import Trainer, TrainingArguments\r\nfrom transformers\
          \ import DataCollatorForLanguageModeling\r\nfrom datasets import load_dataset\r\
          \nimport math\r\nimport evaluate\r\n\r\nfrom pynvml import *\r\n\r\nmodel_name\
          \ = \"ufal/robeczech-base\" \r\n\r\ndef print_gpu_utilization():\r\n   \
          \ nvmlInit()\r\n    handle = nvmlDeviceGetHandleByIndex(0)\r\n    info =\
          \ nvmlDeviceGetMemoryInfo(handle)\r\n    print(f\"GPU memory occupied: {info.used//1024**2}\
          \ MB.\")\r\n\r\n\r\ndef print_summary(result):\r\n    print(f\"Time: {result.metrics['train_runtime']:.2f}\"\
          )\r\n    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\"\
          )\r\n    print_gpu_utilization()\r\n\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\
          \n\r\ndataset = load_dataset('text', data_dir=\"data/\")\r\ndatasets = dataset[\"\
          train\"].train_test_split(test_size = 0.1)\r\n\r\nprint(tokenizer.model_max_length)\r\
          \n\r\nprint_gpu_utilization()\r\n\r\ndef tokenize_function(examples):\r\n\
          \    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\r\
          \n\r\ntokenized_datasets = datasets.map(tokenize_function, batched=True,\
          \ num_proc=16, remove_columns=[\"text\"])\r\n\r\nblock_size=512\r\ndef group_texts(examples):\r\
          \n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\r\
          \n    total_length = len(concatenated_examples[list(examples.keys())[0]])\r\
          \n    total_length = (total_length // block_size) * block_size\r\n    result\
          \ = {\r\n        k: [t[i : i + block_size] for i in range(0, total_length,\
          \ block_size)]\r\n        for k, t in concatenated_examples.items()\r\n\
          \    }\r\n    result[\"labels\"] = result[\"input_ids\"].copy()\r\n    return\
          \ result\r\n\r\nlm_datasets = tokenized_datasets.map(\r\n    group_texts,\r\
          \n    batched=True,\r\n    batch_size=1000,\r\n    num_proc=16,\r\n)\r\n\
          \r\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\r\n\r\ndata_collator\
          \ = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\r\
          \n\r\ntraining_args = TrainingArguments(\r\n    f\"{model_name}-pre-trained-med\"\
          ,\r\n    learning_rate=2e-5,\r\n    weight_decay=0.01,\r\n    warmup_steps=2000,\r\
          \n    per_device_train_batch_size=32,\r\n    per_device_eval_batch_size=32,\r\
          \n    fp16=True,\r\n    logging_dir=f\"{model_name}-pre-trained-med\",\r\
          \n    logging_strategy=\"steps\",\r\n    num_train_epochs=10,\r\n    logging_steps=100,\r\
          \n    save_strategy=\"epoch\",\r\n    save_total_limit=3,\r\n)\r\n\r\ntrainer\
          \ = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=lm_datasets[\"\
          train\"],\r\n    eval_dataset=lm_datasets[\"test\"],\r\n    data_collator=data_collator,\r\
          \n)\r\n\r\nresult = trainer.train()\r\nprint_summary(result)\r\n\r\neval_results\
          \ = trainer.evaluate()\r\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\"\
          )\r\n\r\ntrainer.save_model(f\"py{model_name}-med-pretrained\")\r\n```\r\
          \nI tried to download the model and tokenizer locally, \"fill\" the vocab\
          \ hole and reload the tokenizer, but I was not able to reload the fixed\
          \ tokenizer.\r\n\r\nAre there any possible ways how to fix this?"
        updatedAt: '2023-07-03T14:55:03.063Z'
      numEdits: 0
      reactions: []
    id: 64a2e14702457a5b1ce50430
    type: comment
  author: tomashoufek
  content: "When trying to further pre-train the model on a specific domain I encountered\
    \ an error:\r\n\r\nWhen tokenizing using the robeczech-base tokenizer warning\
    \ accours:\r\n`The OrderedVocab you are attempting to save contains a hole for\
    \ index 51959, your vocabulary could be corrupted !`\r\n\r\nWhen I start training\
    \ the model python throws following error ***PyTroch***:\r\n```Python\r\n../aten/src/ATen/native/cuda/Indexing.cu:1146:\
    \ indexSelectLargeIndex: block: [94,0,0], thread: [96,0,0] Assertion `srcIndex\
    \ < srcSelectDimSize` failed.\r\n# This error is repeated for various block and\
    \ thread values.\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jovyan/tomas/medical-lm/PyTorch/TrainLM-masked-pytorch.py\"\
    , line 98, in <module>\r\n    result = trainer.train()\r\n  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\"\
    , line 1645, in train\r\n    return inner_training_loop(\r\n  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\"\
    , line 1938, in _inner_training_loop\r\n    tr_loss_step = self.training_step(model,\
    \ inputs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\"\
    , line 2759, in training_step\r\n    loss = self.compute_loss(model, inputs)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\", line\
    \ 2784, in compute_loss\r\n    outputs = model(**inputs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/accelerate/utils/operations.py\", line\
    \ 553, in forward\r\n    return model_forward(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/accelerate/utils/operations.py\"\
    , line 541, in __call__\r\n    return convert_to_fp32(self.model_forward(*args,\
    \ **kwargs))\r\n  File \"/opt/conda/lib/python3.9/site-packages/torch/amp/autocast_mode.py\"\
    , line 14, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\"\
    , line 1100, in forward\r\n    outputs = self.roberta(\r\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\"\
    , line 845, in forward\r\n    embedding_output = self.embeddings(\r\n  File \"\
    /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501,\
    \ in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\"\
    , line 123, in forward\r\n    inputs_embeds = self.word_embeddings(input_ids)\r\
    \n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py\", line\
    \ 162, in forward\r\n    return F.embedding(\r\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\"\
    , line 2210, in embedding\r\n    return torch.embedding(weight, input, padding_idx,\
    \ scale_grad_by_freq, sparse)\r\nRuntimeError: CUDA error: device-side assert\
    \ triggered\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\
    \n```\r\nCode I use to pre-train the model:\r\n\r\n```Python\r\nimport torch\r\
    \n\r\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\r\n\r\nfrom\
    \ transformers import Trainer, TrainingArguments\r\nfrom transformers import DataCollatorForLanguageModeling\r\
    \nfrom datasets import load_dataset\r\nimport math\r\nimport evaluate\r\n\r\n\
    from pynvml import *\r\n\r\nmodel_name = \"ufal/robeczech-base\" \r\n\r\ndef print_gpu_utilization():\r\
    \n    nvmlInit()\r\n    handle = nvmlDeviceGetHandleByIndex(0)\r\n    info = nvmlDeviceGetMemoryInfo(handle)\r\
    \n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\r\n\r\n\r\ndef\
    \ print_summary(result):\r\n    print(f\"Time: {result.metrics['train_runtime']:.2f}\"\
    )\r\n    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\"\
    )\r\n    print_gpu_utilization()\r\n\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\
    \n\r\ndataset = load_dataset('text', data_dir=\"data/\")\r\ndatasets = dataset[\"\
    train\"].train_test_split(test_size = 0.1)\r\n\r\nprint(tokenizer.model_max_length)\r\
    \n\r\nprint_gpu_utilization()\r\n\r\ndef tokenize_function(examples):\r\n    return\
    \ tokenizer(examples[\"text\"], truncation=True, max_length=512)\r\n\r\ntokenized_datasets\
    \ = datasets.map(tokenize_function, batched=True, num_proc=16, remove_columns=[\"\
    text\"])\r\n\r\nblock_size=512\r\ndef group_texts(examples):\r\n    concatenated_examples\
    \ = {k: sum(examples[k], []) for k in examples.keys()}\r\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\r\
    \n    total_length = (total_length // block_size) * block_size\r\n    result =\
    \ {\r\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\r\
    \n        for k, t in concatenated_examples.items()\r\n    }\r\n    result[\"\
    labels\"] = result[\"input_ids\"].copy()\r\n    return result\r\n\r\nlm_datasets\
    \ = tokenized_datasets.map(\r\n    group_texts,\r\n    batched=True,\r\n    batch_size=1000,\r\
    \n    num_proc=16,\r\n)\r\n\r\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\r\
    \n\r\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\r\
    \n\r\ntraining_args = TrainingArguments(\r\n    f\"{model_name}-pre-trained-med\"\
    ,\r\n    learning_rate=2e-5,\r\n    weight_decay=0.01,\r\n    warmup_steps=2000,\r\
    \n    per_device_train_batch_size=32,\r\n    per_device_eval_batch_size=32,\r\n\
    \    fp16=True,\r\n    logging_dir=f\"{model_name}-pre-trained-med\",\r\n    logging_strategy=\"\
    steps\",\r\n    num_train_epochs=10,\r\n    logging_steps=100,\r\n    save_strategy=\"\
    epoch\",\r\n    save_total_limit=3,\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\
    \n    args=training_args,\r\n    train_dataset=lm_datasets[\"train\"],\r\n   \
    \ eval_dataset=lm_datasets[\"test\"],\r\n    data_collator=data_collator,\r\n\
    )\r\n\r\nresult = trainer.train()\r\nprint_summary(result)\r\n\r\neval_results\
    \ = trainer.evaluate()\r\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\"\
    )\r\n\r\ntrainer.save_model(f\"py{model_name}-med-pretrained\")\r\n```\r\nI tried\
    \ to download the model and tokenizer locally, \"fill\" the vocab hole and reload\
    \ the tokenizer, but I was not able to reload the fixed tokenizer.\r\n\r\nAre\
    \ there any possible ways how to fix this?"
  created_at: 2023-07-03 13:55:03+00:00
  edited: false
  hidden: false
  id: 64a2e14702457a5b1ce50430
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618772050171-noauth.jpeg?w=200&h=200&f=face
      fullname: Milan Straka
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: foxik
      type: user
    createdAt: '2023-07-20T08:56:07.000Z'
    data:
      edited: false
      editors:
      - foxik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9537954926490784
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618772050171-noauth.jpeg?w=200&h=200&f=face
          fullname: Milan Straka
          isHf: false
          isPro: false
          name: foxik
          type: user
        html: '<p>Hi,</p>

          <p>yes, the tokenizer of RobeCzech model is unfortunately a bit non-standard.
          Notably, there are multiple subwords with the same ID 3 (originally the
          ID of an <code>&lt;unk&gt;</code> token).</p>

          <ul>

          <li><p>The problem was caused by the following. We first created a ByteBPE
          tokenizer, remapped the inputs, and then trained the model using FairSeq.
          However, that again renumbered the subwords and embeddings were created
          only for subwords that actually appeared in the training data. After training,
          we "composed" the two mappings, arriving at the final tokenizer. However,
          the ByteBPE tokenizer requires the 256 special tokens representing the 0-255
          byte values, and some of them were not present in the training data, so
          they did not get an embedding. However, without some of the subwords for
          0-255 byte values the ByteBPE tokenizer does not even load.</p>

          <p>Unfortunately, we "solved" the issue by mapping the missing subwords
          to index 3 (<code>&lt;unk&gt;</code>).</p>

          </li>

          <li><p>We provide both the "fast" and "slow" tokenizers in this repo, mapping
          multiple tokens to ID 3. However, they cannot be saved (as the tokenizers
          are expected to be injective), so you must refrain from saving them. Furthermore,
          the number of embeddings is not the same as the number of subwords in the
          tokenizer.</p>

          </li>

          <li><p>Other than that, the model works fine, and we have finetuned it successfully
          both in PyTorch and in TensorFlow.</p>

          </li>

          <li><p>Retrospectively, a much better fix would be to actually <em>append</em>
          the embeddings for the missing tokens (and initialize them to the value
          of <code>&lt;unk&gt;</code>) -- the tokenizer would be injective and standard.</p>

          <p>However, changing <code>ufal/robeczech-base</code> this way would not
          be backward compatible, so if people finetuned the original version and
          then tried to load it into the updated model, it would fail (because the
          number of embeddings would be different) -- which is why we haven''t done
          it in the first place. We could release a model with a name like <code>ufal/robeczech-standardtokenizer-base</code>
          with a "normal" tokenizer, but we do not currently think it is worth it.</p>

          </li>

          </ul>

          <p>Sorry for the trouble and cheers!</p>

          '
        raw: "Hi,\n\nyes, the tokenizer of RobeCzech model is unfortunately a bit\
          \ non-standard. Notably, there are multiple subwords with the same ID 3\
          \ (originally the ID of an `<unk>` token).\n\n- The problem was caused by\
          \ the following. We first created a ByteBPE tokenizer, remapped the inputs,\
          \ and then trained the model using FairSeq. However, that again renumbered\
          \ the subwords and embeddings were created only for subwords that actually\
          \ appeared in the training data. After training, we \"composed\" the two\
          \ mappings, arriving at the final tokenizer. However, the ByteBPE tokenizer\
          \ requires the 256 special tokens representing the 0-255 byte values, and\
          \ some of them were not present in the training data, so they did not get\
          \ an embedding. However, without some of the subwords for 0-255 byte values\
          \ the ByteBPE tokenizer does not even load.\n\n  Unfortunately, we \"solved\"\
          \ the issue by mapping the missing subwords to index 3 (`<unk>`).\n\n- We\
          \ provide both the \"fast\" and \"slow\" tokenizers in this repo, mapping\
          \ multiple tokens to ID 3. However, they cannot be saved (as the tokenizers\
          \ are expected to be injective), so you must refrain from saving them. Furthermore,\
          \ the number of embeddings is not the same as the number of subwords in\
          \ the tokenizer.\n\n- Other than that, the model works fine, and we have\
          \ finetuned it successfully both in PyTorch and in TensorFlow.\n\n- Retrospectively,\
          \ a much better fix would be to actually _append_ the embeddings for the\
          \ missing tokens (and initialize them to the value of `<unk>`) -- the tokenizer\
          \ would be injective and standard.\n\n  However, changing `ufal/robeczech-base`\
          \ this way would not be backward compatible, so if people finetuned the\
          \ original version and then tried to load it into the updated model, it\
          \ would fail (because the number of embeddings would be different) -- which\
          \ is why we haven't done it in the first place. We could release a model\
          \ with a name like `ufal/robeczech-standardtokenizer-base` with a \"normal\"\
          \ tokenizer, but we do not currently think it is worth it.\n\nSorry for\
          \ the trouble and cheers!"
        updatedAt: '2023-07-20T08:56:07.486Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64b8f6a7f1f8e6ea5860b315
    id: 64b8f6a7f1f8e6ea5860b314
    type: comment
  author: foxik
  content: "Hi,\n\nyes, the tokenizer of RobeCzech model is unfortunately a bit non-standard.\
    \ Notably, there are multiple subwords with the same ID 3 (originally the ID of\
    \ an `<unk>` token).\n\n- The problem was caused by the following. We first created\
    \ a ByteBPE tokenizer, remapped the inputs, and then trained the model using FairSeq.\
    \ However, that again renumbered the subwords and embeddings were created only\
    \ for subwords that actually appeared in the training data. After training, we\
    \ \"composed\" the two mappings, arriving at the final tokenizer. However, the\
    \ ByteBPE tokenizer requires the 256 special tokens representing the 0-255 byte\
    \ values, and some of them were not present in the training data, so they did\
    \ not get an embedding. However, without some of the subwords for 0-255 byte values\
    \ the ByteBPE tokenizer does not even load.\n\n  Unfortunately, we \"solved\"\
    \ the issue by mapping the missing subwords to index 3 (`<unk>`).\n\n- We provide\
    \ both the \"fast\" and \"slow\" tokenizers in this repo, mapping multiple tokens\
    \ to ID 3. However, they cannot be saved (as the tokenizers are expected to be\
    \ injective), so you must refrain from saving them. Furthermore, the number of\
    \ embeddings is not the same as the number of subwords in the tokenizer.\n\n-\
    \ Other than that, the model works fine, and we have finetuned it successfully\
    \ both in PyTorch and in TensorFlow.\n\n- Retrospectively, a much better fix would\
    \ be to actually _append_ the embeddings for the missing tokens (and initialize\
    \ them to the value of `<unk>`) -- the tokenizer would be injective and standard.\n\
    \n  However, changing `ufal/robeczech-base` this way would not be backward compatible,\
    \ so if people finetuned the original version and then tried to load it into the\
    \ updated model, it would fail (because the number of embeddings would be different)\
    \ -- which is why we haven't done it in the first place. We could release a model\
    \ with a name like `ufal/robeczech-standardtokenizer-base` with a \"normal\" tokenizer,\
    \ but we do not currently think it is worth it.\n\nSorry for the trouble and cheers!"
  created_at: 2023-07-20 07:56:07+00:00
  edited: false
  hidden: false
  id: 64b8f6a7f1f8e6ea5860b314
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618772050171-noauth.jpeg?w=200&h=200&f=face
      fullname: Milan Straka
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: foxik
      type: user
    createdAt: '2023-07-20T08:56:07.000Z'
    data:
      status: closed
    id: 64b8f6a7f1f8e6ea5860b315
    type: status-change
  author: foxik
  created_at: 2023-07-20 07:56:07+00:00
  id: 64b8f6a7f1f8e6ea5860b315
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: ufal/robeczech-base
repo_type: model
status: closed
target_branch: null
title: Vocabulary contains hole for index 51959
